{
  "assignees": [],
  "author": {
    "id": "MDQ6VXNlcjUzMjEwOA==",
    "is_bot": false,
    "login": "Spikhalskiy",
    "name": "Dmitry Spikhalsky"
  },
  "body": "# Problem\r\n\r\nAn investigation of flaky `InterceptorsExceptionsTests#testExceptionOnStart` test from https://github.com/temporalio/sdk-java/issues/455 ended up inside GRPC code that swallows `InterruptedException` in intention to perform a graceful shutdown, but instead peeking up a long poll task submitted by another poller thread (from another non-shutdown thread pool) and falling into a long poll inside in-memory GRPC server implementation. The stacktrace of the hanging Poller thread AFTER receiving an interruption:\r\n```\r\n\"Host Local Workflow Poller: 5@3606\" daemon prio=5 tid=0x1c nid=NA waiting\r\n  java.lang.Thread.State: WAITING\r\n\t  at jdk.internal.misc.Unsafe.park(Unsafe.java:-1)\r\n\t  at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)\r\n\t  at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1661)\r\n\t  at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)\r\n\t  at io.temporal.internal.testservice.TestWorkflowStoreImpl.pollWorkflowTaskQueue(TestWorkflowStoreImpl.java:347)\r\n\t  at io.temporal.internal.testservice.TestWorkflowService.pollWorkflowTaskQueue(TestWorkflowService.java:448)\r\n\t  at io.temporal.api.workflowservice.v1.WorkflowServiceGrpc$MethodHandlers.invoke(WorkflowServiceGrpc.java:3625)\r\n\t  at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\r\n\t  at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)\r\n\t  at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:797)\r\n\t  at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\n\t  at io.grpc.internal.SerializeReentrantCallsDirectExecutor.execute(SerializeReentrantCallsDirectExecutor.java:49)\r\n\t  at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener.halfClosed(ServerImpl.java:808)\r\n\t  at io.grpc.inprocess.InProcessTransport$InProcessStream$InProcessClientStream.halfClose(InProcessTransport.java:793)\r\n\t  - locked <0xf41> (a io.grpc.inprocess.InProcessTransport$InProcessStream$InProcessClientStream)\r\n\t  at io.grpc.internal.ForwardingClientStream.halfClose(ForwardingClientStream.java:72)\r\n\t  at io.grpc.internal.DelayedStream$9.run(DelayedStream.java:344)\r\n\t  at io.grpc.internal.DelayedStream.drainPendingCalls(DelayedStream.java:181)\r\n\t  at io.grpc.internal.DelayedStream.access$100(DelayedStream.java:43)\r\n\t  at io.grpc.internal.DelayedStream$4.run(DelayedStream.java:147)\r\n\t  at io.grpc.stub.ClientCalls$ThreadlessExecutor.waitAndDrain(ClientCalls.java:740)\r\n\t  at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:149)\r\n\t  at io.temporal.api.workflowservice.v1.WorkflowServiceGrpc$WorkflowServiceBlockingStub.pollWorkflowTaskQueue(WorkflowServiceGrpc.java:2639)\r\n\t  at io.temporal.internal.worker.WorkflowPollTask.poll(WorkflowPollTask.java:81)\r\n\t  at io.temporal.internal.worker.WorkflowPollTask.poll(WorkflowPollTask.java:37)\r\n\t  at io.temporal.internal.worker.Poller$PollExecutionTask.run(Poller.java:270)\r\n\t  at io.temporal.internal.worker.Poller$PollLoopTask.run(Poller.java:235)\r\n\t  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\t  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t  at java.lang.Thread.run(Thread.java:832)\r\n```\r\n\r\n# Partial notes made during an investigation\r\n\r\nhttps://github.com/grpc/grpc-java/blob/9b803f3/stub/src/main/java/io/grpc/stub/ClientCalls.java#L149\r\nGRPC catches the InterruptedException, puts it into a flag var, and goes to a new iteration of the loop. (based on comments waiting for “onClose” to be called)\r\nWhat actually happens at the moment when we experience problems with shutdown - it finds a job that actually calls a long poll inside `waitAndDrain`, starts to execute it, and hangs there, because the interrupted status has been flushed.\r\nThis happens only in a case when a long-poll task ends up in this executor AFTER the thread is interrupted. A situation is likely to happen only when we shut down the environment right after the start. That’s why most tests are ok - they actually take stuff from the server, so long poll is already triggered. \r\nI made a branch (https://github.com/Spikhalskiy/java-sdk/commit/f543651432f0f7d0173e2461ec00287219c91b12) where I threw a bunch of debug prints inside GRPC code around this waitAndDrain and it shows one more strange/unexpected thing - the long poll task that blocks the intercepted thread is put to the executor of interrupted Poller by another non-yet-interrupted Poller thread.  Basically, we have an intercepted sticky poller thread and the non-intercepted workflow poller thread publishes a long poll task into the intercepted sticky poller executor.\r\n\r\n# Assumptions\r\n\r\nIt looks like this situation is only happening when we have a GRPC client + a GRPC in-process server that performs the long poll. Interruption in the client thread ends up ignored and a Poller thread falls into a long poll inside the in-process GRPC server implementation. Also, timing is very important for it, because the long poll task needs to be submitted right after threads get terminated.\r\nWe were unable to reproduce an issue with an actual dockerized Temporal server.\r\nAn investigation of this issue is not finished. The best bet right now is: it's a bug in GRPC happening when GRPC client & GRPC in-process server & long polls are used and the right timing is needed for the problem to reproduce.\r\n\r\n# Mitigation \r\n\r\nTaking into account that further investigation of this issue is very time-consuming, we decided to mitigate the problem in this specific flaky test by triggering a shutdown of GRPC in-process Temporal server implementation before shutdown of workers. This way GRPC client doesn't have a long poll to fall into after getting an interception. This workaround is implemented in: https://github.com/temporalio/sdk-java/pull/601 and affects only tests code.\r\nThe last master commit where `InterceptorsExceptionsTests#testExceptionOnStart` is able to often reproduce the described problem is b4cbaa13f32590e2337a03305dfb6a0af6628bdc.\r\n\r\n\r\n\r\n",
  "closedAt": null,
  "comments": [],
  "createdAt": "2021-07-26T21:56:48Z",
  "labels": [
    {
      "id": "MDU6TGFiZWwxNjIyNzE5ODQx",
      "name": "bug",
      "description": "Something isn't working",
      "color": "d73a4a"
    },
    {
      "id": "LA_kwDODN12PM8AAAABzzHCJQ",
      "name": "test server",
      "description": "Related to the test server",
      "color": "A71076"
    }
  ],
  "milestone": null,
  "number": 608,
  "reactionGroups": [],
  "state": "OPEN",
  "title": "GRPC falling into in-process server long polls after interruption breaks graceful Worker Pollers shutdown",
  "updatedAt": "2025-05-17T14:56:04Z",
  "url": "https://github.com/temporalio/sdk-java/issues/608"
}
