{"assignees":[],"author":{"id":"MDQ6VXNlcjIwNjM5Ng==","is_bot":false,"login":"cretz","name":"Chad Retz"},"body":"### Describe the bug\r\n\r\nOn short-lived client-only process after `execute_workflow` client call completes, one user reported getting:\r\n\r\n```\r\nFatal Python error: PyGILState_Release: thread state 0x***************** must be current when releasing\r\nPython runtime state: finalizing (tstate=0x*****************)\r\nThread 0x****************** (most recent call first):\r\n  <no Python frame>\r\n```\r\n\r\nMaybe this is caused by process death while waiting on client call complete? Try to replicate.\r\n\r\nThere is some discussion at https://github.com/PyO3/pyo3/issues/1274 that predates pyo3-asyncio. Maybe I am not implementing our custom Tokio pyo3 asyncio extension properly?\r\n\r\nIt looks like we shouldn't be calling `Python::with_gil` in callbacks (i.e. not in Python-owned thread) for any reason, so we need to work around that. But https://pyo3.rs/main/ecosystem/async-await.html#awaiting-a-rust-future-in-python shows it used in a callback.\r\n\r\nFirst thing is a replication, then we can see whether an pyo3-asyncio upgrade can help.","closedAt":null,"comments":[{"id":"IC_kwDOGusT1c6Nej_2","author":{"login":"TiunovNN"},"authorAssociation":"NONE","body":"Hi @cretz . I've came across the similar problem. Here is a simple code to reproduce bug:\r\n\r\n```python\r\nimport asyncio\r\nimport logging\r\n\r\nfrom temporalio.client import Client\r\nfrom temporalio.exceptions import WorkflowAlreadyStartedError\r\nfrom temporalio.runtime import (LoggingConfig, Runtime, TelemetryConfig, TelemetryFilter)\r\n\r\n\r\nasync def connect_client(host):\r\n    temporal_client: Client = await Client.connect(\r\n        host,\r\n        namespace='default',\r\n        runtime=Runtime(\r\n            telemetry=TelemetryConfig(\r\n                logging=LoggingConfig(\r\n                    filter=TelemetryFilter(\r\n                        core_level='DEBUG',\r\n                        other_level='DEBUG',\r\n                    ),\r\n                ),\r\n            ),\r\n        ),\r\n    )\r\n\r\n    try:\r\n        await temporal_client.start_workflow('PackagerWorkflow', 'test', id='Pack',\r\n                                             task_queue='default_queue')\r\n    except WorkflowAlreadyStartedError:\r\n        pass\r\n\r\n\r\ndef run(self):\r\n    logging.basicConfig(level=logging.DEBUG, force=True)\r\n    logging.info('start app')\r\n    asyncio.run(connect_client('localhost:7233'))\r\n    logging.info('finish app')\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nlog from gdb:\r\n```\r\n2024-09-25T09:32:59.616938Z DEBUG hyper_util::client::legacy::connect::dns: resolving host=\"localhost\"                                                                                                            \r\n2024-09-25T09:32:59.657156Z DEBUG hyper_util::client::legacy::connect::http: connecting to [::1]:7233                                                                                                             \r\n2024-09-25T09:32:59.657644Z DEBUG hyper_util::client::legacy::connect::http: connecting to 127.0.0.1:7233                                                                                                         \r\n2024-09-25T09:32:59.657936Z DEBUG hyper_util::client::legacy::connect::http: connected to 127.0.0.1:7233                                                                                                          \r\n2024-09-25T09:32:59.657996Z DEBUG h2::client: binding client connection                                                                                                                                           \r\n2024-09-25T09:32:59.658088Z DEBUG h2::client: client connection bound                                                                                                                                             \r\n2024-09-25T09:32:59.658129Z DEBUG h2::codec::framed_write: send frame=Settings { flags: (0x0), enable_push: 0, initial_window_size: 2097152, max_frame_size: 16384, max_header_list_size: 16384 }\r\n2024-09-25T09:32:59.658413Z DEBUG Connection: h2::codec::framed_write: send frame=WindowUpdate { stream_id: StreamId(0), size_increment: 5177345 } peer=Client\r\n2024-09-25T09:32:59.658430Z DEBUG tower::buffer::worker: service.ready=true processing request                                                                                                                    \r\n2024-09-25T09:32:59.658655Z DEBUG Connection: h2::codec::framed_read: received frame=Settings { flags: (0x0), max_frame_size: 16384 } peer=Client      \r\n2024-09-25T09:32:59.658704Z DEBUG Connection: h2::codec::framed_write: send frame=Settings { flags: (0x1: ACK) } peer=Client      \r\n2024-09-25T09:32:59.658715Z DEBUG Connection: h2::codec::framed_write: send frame=Headers { stream_id: StreamId(1), flags: (0x4: END_HEADERS) } peer=Client                                                       \r\n2024-09-25T09:32:59.658763Z DEBUG Connection: h2::codec::framed_write: send frame=Data { stream_id: StreamId(1) } peer=Client                                                                                     \r\n2024-09-25T09:32:59.658773Z DEBUG Connection: h2::codec::framed_write: send frame=Data { stream_id: StreamId(1), flags: (0x1: END_STREAM) } peer=Client    \r\n2024-09-25T09:32:59.658843Z DEBUG Connection: h2::codec::framed_read: received frame=Settings { flags: (0x1: ACK) } peer=Client                                                                                   \r\n2024-09-25T09:32:59.658872Z DEBUG Connection: h2::proto::settings: received settings ACK; applying Settings { flags: (0x0), enable_push: 0, initial_window_size: 2097152, max_frame_size: 16384, max_header_list_s\r\nize: 16384 } peer=Client                                                                                                                                                                                          \r\n2024-09-25T09:32:59.659016Z DEBUG Connection: h2::codec::framed_read: received frame=WindowUpdate { stream_id: StreamId(0), size_increment: 5 } peer=Client\r\n2024-09-25T09:32:59.659059Z DEBUG Connection: h2::codec::framed_read: received frame=Ping { ack: false, payload: [2, 4, 16, 16, 9, 14, 7, 7] } peer=Client                 \r\n2024-09-25T09:32:59.659073Z DEBUG Connection: h2::codec::framed_write: send frame=Ping { ack: true, payload: [2, 4, 16, 16, 9, 14, 7, 7] } peer=Client                                                            \r\n2024-09-25T09:32:59.659470Z DEBUG Connection: h2::codec::framed_read: received frame=Headers { stream_id: StreamId(1), flags: (0x4: END_HEADERS) } peer=Client\r\n2024-09-25T09:32:59.659492Z DEBUG Connection: h2::codec::framed_read: received frame=Data { stream_id: StreamId(1) } peer=Client  \r\n2024-09-25T09:32:59.659506Z DEBUG Connection: h2::codec::framed_read: received frame=Headers { stream_id: StreamId(1), flags: (0x5: END_HEADERS | END_STREAM) } peer=Client\r\n2024-09-25T09:32:59.660903Z DEBUG tower::buffer::worker: service.ready=true processing request                                                                                                                    \r\n2024-09-25T09:32:59.661020Z DEBUG Connection: h2::codec::framed_write: send frame=Headers { stream_id: StreamId(3), flags: (0x4: END_HEADERS) } peer=Client                                                       \r\n2024-09-25T09:32:59.661100Z DEBUG Connection: h2::codec::framed_write: send frame=Data { stream_id: StreamId(3) } peer=Client                                                                                     \r\n2024-09-25T09:32:59.661111Z DEBUG Connection: h2::codec::framed_write: send frame=Data { stream_id: StreamId(3), flags: (0x1: END_STREAM) } peer=Client                                                           \r\n2024-09-25T09:32:59.661419Z DEBUG Connection: h2::codec::framed_read: received frame=WindowUpdate { stream_id: StreamId(0), size_increment: 170 } peer=Client                                                     \r\n2024-09-25T09:32:59.661438Z DEBUG Connection: h2::codec::framed_read: received frame=Ping { ack: false, payload: [2, 4, 16, 16, 9, 14, 7, 7] } peer=Client                                                        \r\n2024-09-25T09:32:59.661444Z DEBUG Connection: h2::codec::framed_write: send frame=Ping { ack: true, payload: [2, 4, 16, 16, 9, 14, 7, 7] } peer=Client                                                            \r\n2024-09-25T09:32:59.664355Z DEBUG Connection: h2::codec::framed_read: received frame=Headers { stream_id: StreamId(3), flags: (0x5: END_HEADERS | END_STREAM) } peer=Client                                       \r\nINFO:root:finish app                                                                                                                                                                                              \r\nFatal Python error: This thread state must be current when releasing                                                                                                                                              \r\nPython runtime state: finalizing (tstate=0x95d720)                                                                                                                                                                \r\n                                                                                                                                                                                                                  \r\nCurrent thread 0x00007ffedaff7700 (most recent call first):                                                                                                                                                       \r\n  File \"/usr/lib/python3.8/asyncio/selector_events.py\", line 140 in _write_to_self                                                                                                                                \r\n  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 770 in call_soon_threadsafe                                                                                                                              \r\n                                                                                                                                                                                                                  \r\nThread 0x00007ffff7bfd740 (most recent call first):                                                                                                                                                               \r\n<no Python frame>                                                                                                                                                                                                 \r\n                                                                                                                                                                                                                  \r\nThread 79 \"tokio-runtime-w\" received signal SIGABRT, Aborted.                                            \r\n```\r\n\r\nFull stack trace of failed thread: https://gist.github.com/TiunovNN/84c253d62f03f11270a49facee6c7b52\r\n\r\nI suppose that root cause is the python does not call destructor immediately, and as written in docs `A Client does not have an explicit \"close\"`. When cpython is finalizing code, it acquires GIL and executes destructor of tokio::runtime::Runtime which is expecting while all tasks have finished. At the same time tokio::runtime got ping message and run python code to handle message, which is trying to acquire GIL as well.\r\n\r\nIt would be better if there was an explicit way to close client.","createdAt":"2024-09-25T09:47:15Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/300#issuecomment-2373599222","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6NgJiq","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"> Here is a simple code to reproduce bug:\r\n\r\nI am struggling to replicate. There is a slight bug where `run` accepts `self`, but fixing that, I didn't see it. Can you confirm your OS, `temporalio` SDK version, and Python version?\r\n\r\n> It would be better if there was an explicit way to close client.\r\n\r\nI don't think this would solve it if the issue is Tokio runtime cleanup. A better solution may be like what https://github.com/hydro-project/hydroflow/pull/699 did (linked from https://github.com/hydro-project/hydroflow/issues/619 who linked here as having a common issue). Specifically using something like `atexit.register` to register runtime drop.\r\n\r\nBut ideally we could get a reliable replication first so we can confirm it is fixed once we make the change. I am struggling to get this currently.","createdAt":"2024-09-25T12:57:25Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/300#issuecomment-2374015146","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6Ngxhx","author":{"login":"TiunovNN"},"authorAssociation":"NONE","body":"> I am struggling to replicate.\r\n\r\nMy mistake. I have forgot to mention that this bug appears not every run, but rarely. So you have to rerun script several times.\r\nI used gdb in the following manner:\r\n```shell\r\n$ gdb python\r\n(gdb) set pagination off\r\n(gdb) break _exit\r\n(gdb) command\r\n> run\r\n> end\r\n(gdb) run script.py\r\n```\r\nAnd wait for several minutes.\r\n\r\n> Can you confirm your OS, temporalio SDK version, and Python version?\r\n\r\nThe bug appears on the following sets:\r\n* Ubuntu 20.04.6 LTS + python 3.12 + temporal 1.5.0\r\n* Ubuntu 22.04.5 LTS + python 3.12 + temporal 1.5.0\r\n* Ubuntu 20.04.6 LTS + python 3.12 + temporal 1.7.1\r\n* Ubuntu 22.04.5 LTS + python 3.12 + temporal 1.7.1\r\n* Ubuntu 20.04.6 LTS + python 3.8 + temporal 1.7.1\r\n\r\n\r\n","createdAt":"2024-09-25T14:00:30Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/300#issuecomment-2374178929","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6NhGiF","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"Thanks! We will increase the priority on this.","createdAt":"2024-09-25T14:31:49Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/300#issuecomment-2374264965","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6OVU7u","author":{"login":"TiunovNN"},"authorAssociation":"NONE","body":"@cretz \r\n> Specifically using something like atexit.register to register runtime drop\r\n\r\nPlease, take into account, that client destructor might be called even during execution process (not only at exit), when there is no references to the object. I would recommend to use context manager as appropriate solution for python.","createdAt":"2024-10-02T08:52:46Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/300#issuecomment-2387955438","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6OXypz","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"> that client destructor might be called even during execution process (not only at exit), when there is no references to the object.\r\n\r\nThis is not just about client destructor, this is about when it is dropped Rust side. If the client is in use in any way (Python or Rust), it is not closed/disconnected. We can't just use context manager because it is passed around (e.g. to a worker) and may only retain Rust references. I think this error is specific to the Python-Rust bridge and PyO3.","createdAt":"2024-10-02T13:07:23Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/300#issuecomment-2388601459","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6P6xxv","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"Update here, I am still struggling to replicate. I suspect this is actually a Tokio runtime lifetime issue and not specific to the client, but we are still trying to replicate reliably (running the script under gdb for me, at least on 3.10, just continually works forever).","createdAt":"2024-10-15T16:58:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/300#issuecomment-2414550127","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6P8L3b","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"I can replicate a similar issue changing the code to just run a task in the background and then completing the app before waiting on complete, e.g.:\r\n\r\n```python\r\nimport asyncio\r\nimport logging\r\nimport uuid\r\n\r\nfrom temporalio.api.workflowservice.v1 import StartWorkflowExecutionRequest\r\nfrom temporalio.client import Client\r\nfrom temporalio.exceptions import WorkflowAlreadyStartedError\r\nfrom temporalio.runtime import LoggingConfig, Runtime, TelemetryConfig, TelemetryFilter\r\n\r\n\r\nasync def connect_client(host):\r\n    client = await Client.connect(\r\n        host,\r\n        namespace=\"default\",\r\n        runtime=Runtime(\r\n            telemetry=TelemetryConfig(\r\n                logging=LoggingConfig(\r\n                    filter=TelemetryFilter(\r\n                        core_level=\"DEBUG\",\r\n                        other_level=\"DEBUG\",\r\n                    ),\r\n                ),\r\n            ),\r\n        ),\r\n    )\r\n\r\n    req = StartWorkflowExecutionRequest(\r\n        namespace=\"default\",\r\n        workflow_id=f\"my-workflow-id-{uuid.uuid4()}\",\r\n        identity=\"my-identity\",\r\n        request_id=str(uuid.uuid4()),\r\n    )\r\n    req.workflow_type.name = \"my-workflow\"\r\n    req.task_queue.name = \"my-task-queue\"\r\n\r\n    asyncio.create_task(client.workflow_service.start_workflow_execution(req))\r\n\r\n\r\ndef run():\r\n    logging.basicConfig(level=logging.DEBUG, force=True)\r\n    logging.info(\"start app\")\r\n    asyncio.run(connect_client(\"localhost:7233\"))\r\n    logging.info(\"finish app\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nThis will give:\r\n\r\n```\r\nFATAL: exception not rethrown\r\nAborted\r\n```\r\n\r\nMy current guess is this is because pyo3-asyncio assumes `Python::with_gil` will succeed, but that delegates to `PyGILState_Ensure` which documents at https://docs.python.org/3/c-api/init.html#c.PyGILState_Ensure:\r\n\r\n> Calling this function from a thread when the runtime is finalizing will terminate the thread, even if the thread was not created by Python. You can use [Py_IsFinalizing()](https://docs.python.org/3/c-api/init.html#c.Py_IsFinalizing) or [sys.is_finalizing()](https://docs.python.org/3/library/sys.html#sys.is_finalizing) to check if the interpreter is in process of being finalized before calling this function to avoid unwanted termination.\r\n\r\nUnfortunately `Py_IsFinalizing` is 3.13+ and ideally we don't want the overhead of invoking `sys.is_finalizing`, but we may have to (and it's not that bad considering pyo3-asyncio already uses things like `call_soon_threadsafe`).\r\n\r\nUnfortunately https://github.com/awestlake87/pyo3-asyncio is mostly abandoned, https://github.com/PyO3/pyo3-async-runtimes is just a simple fork, and built-in PyO3 asyncio with Rust futures is still being worked on. So we probably would just end up vendoring what we haven't already from pyo3-asyncio (we already copy and slightly alter some code to use the current Tokio runtime instead of one big static global).","createdAt":"2024-10-15T20:14:26Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/300#issuecomment-2414919131","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6UX1hV","author":{"login":"i3Cheese"},"authorAssociation":"NONE","body":"@cretz\r\nHello! I encountered the same issue, and I would like to help fix it. Where do I start?\r\n","createdAt":"2024-11-20T18:24:54Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/300#issuecomment-2489276501","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6UZfL9","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"Assuming our guess at the issue is correct, fixing this is daunting because it probably requires rewriting/vendoring a Rust library (pyo3-asyncio). Can you confirm this only happens on process/event-loop shutdown? The best way to avoid this is to not attempt to gracefully exit the program while the client is in use somewhere.","createdAt":"2024-11-20T23:03:32Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/300#issuecomment-2489709309","viewerDidAuthor":false}],"createdAt":"2023-03-22T14:13:35Z","labels":[{"id":"LA_kwDOGusT1c7gQgHK","name":"bug","description":"Something isn't working","color":"d73a4a"}],"milestone":null,"number":300,"reactionGroups":[],"state":"OPEN","title":"[Bug] Investigate PyGILState_Release issue for client","updatedAt":"2024-11-20T23:03:33Z","url":"https://github.com/temporalio/sdk-python/issues/300"}
