{"assignees":[],"author":{"id":"MDQ6VXNlcjgxNzQzMDcw","is_bot":false,"login":"kelkawi-a","name":"Ali Kelkawi"},"body":"### What are you really trying to do?\r\n\r\nTo connect a worker to a Temporal server with an `authorization` header.\r\n\r\n### Describe the bug\r\n\r\nStarting a worker with an incorrect token, causing the server to respond with `Request unauthorized`, causes the worker to hang indefinitely.\r\n\r\n### Minimal Reproduction\r\n\r\n```python\r\nimport asyncio\r\n\r\nfrom temporalio import activity, client, workflow, worker\r\n\r\n\r\n@activity.defn\r\nasync def a() -> None:\r\n    pass\r\n\r\n\r\n@workflow.defn\r\nclass Workflow:\r\n    @workflow.run\r\n    async def run(self) -> None:\r\n        pass\r\n\r\n\r\nasync def main():\r\n    c = await client.Client.connect(\r\n        \"my_temporal_host:7233\",\r\n        rpc_metadata={\"authorization\": \"wrong_token\"},\r\n        tls=True,\r\n    )\r\n    w = worker.Worker(\r\n        c,\r\n        task_queue=\"default\",\r\n        activities=[a],\r\n        workflows=[Workflow],\r\n    )\r\n    await w.run()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    asyncio.run(main())\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n2024-09-03T07:26:48.707Z [temporal-worker] 2024-09-03T07:26:48.707200Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 52 times error=Status { code: Internal, message: \"protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:26:48 GMT\", \"content-type\": \"text/html\", \"content-length\": \"160\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None }\r\n2024-09-03T07:26:49.544Z [temporal-worker] 2024-09-03T07:26:49.544345Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 52 times error=Status { code: Internal, message: \"protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:26:49 GMT\", \"content-type\": \"text/html\", \"content-length\": \"160\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None }\r\n2024-09-03T07:27:51.707Z [temporal-worker] 2024-09-03T07:27:51.707200Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 53 times error=Status { code: Internal, message: \"protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:27:51 GMT\", \"content-type\": \"text/html\", \"content-length\": \"160\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None }\r\n2024-09-03T07:27:52.968Z [temporal-worker] 2024-09-03T07:27:52.967933Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 52 times error=Status { code: Internal, message: \"protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:27:52 GMT\", \"content-type\": \"text/html\", \"content-length\": \"160\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None }\r\n2024-09-03T07:28:00.036Z [temporal-worker] 2024-09-03T07:28:00.036155Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 53 times error=Status { code: Internal, message: \"protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:28:00 GMT\", \"content-type\": \"text/html\", \"content-length\": \"160\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None }\r\n2024-09-03T07:28:00.687Z [temporal-worker] 2024-09-03T07:28:00.687484Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 53 times error=Status { code: Internal, message: \"protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:28:00 GMT\", \"content-type\": \"text/html\", \"content-length\": \"160\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None }\r\n2024-09-03T07:28:03.579Z [temporal-worker] 2024-09-03T07:28:03.578940Z  WARN temporal_sdk_core::worker::workflow::wft_poller: Error while polling for workflow tasks error=Status { code: PermissionDenied, message: \"Request unauthorized.\", details: b\"\\x08\\x07\\x12\\x15Request unauthorized.\\x1aJ\\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:28:03 GMT\", \"content-type\": \"application/grpc\", \"content-length\": \"0\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None }\r\n2024-09-03T07:28:03.970Z [temporal-worker] 2024-09-03T07:28:03.583738Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: PermissionDenied, message: \"Request unauthorized.\", details: b\"\\x08\\x07\\x12\\x15Request unauthorized.\\x1aJ\\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:28:03 GMT\", \"content-type\": \"application/grpc\", \"content-length\": \"0\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None })\r\n2024-09-03T07:28:03.970Z [temporal-worker] 2024-09-03T07:28:03.969933Z  WARN temporal_sdk_core::worker::workflow::wft_poller: Error while polling for workflow tasks error=Status { code: PermissionDenied, message: \"Request unauthorized.\", details: b\"\\x08\\x07\\x12\\x15Request unauthorized.\\x1aJ\\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:28:03 GMT\", \"content-type\": \"application/grpc\", \"content-length\": \"0\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None }\r\n2024-09-03T07:28:03.970Z [temporal-worker] 2024-09-03T07:28:03.969960Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: PermissionDenied, message: \"Request unauthorized.\", details: b\"\\x08\\x07\\x12\\x15Request unauthorized.\\x1aJ\\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:28:03 GMT\", \"content-type\": \"application/grpc\", \"content-length\": \"0\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None })\r\n2024-09-03T07:28:04.023Z [temporal-worker] Worker failed, shutting down\r\n2024-09-03T07:28:04.023Z [temporal-worker] Traceback (most recent call last):\r\n2024-09-03T07:28:04.023Z [temporal-worker]   File \"/lib/python3.10/site-packages/temporalio/worker/_workflow.py\", line 143, in run\r\n2024-09-03T07:28:04.023Z [temporal-worker]     act = await self._bridge_worker().poll_workflow_activation()\r\n2024-09-03T07:28:04.023Z [temporal-worker]   File \"/lib/python3.10/site-packages/temporalio/bridge/worker.py\", line 141, in poll_workflow_activation\r\n2024-09-03T07:28:04.023Z [temporal-worker]     await self._ref.poll_workflow_activation()\r\n2024-09-03T07:28:04.023Z [temporal-worker] RuntimeError: Poll failure: Unhandled grpc error when workflow polling: Status { code: PermissionDenied, message: \"Request unauthorized.\", details: b\"\\x08\\x07\\x12\\x15Request unauthorized.\\x1aJ\\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure\", metadata: MetadataMap { headers: {\"date\": \"Tue, 03 Sep 2024 07:28:03 GMT\", \"content-type\": \"application/grpc\", \"content-length\": \"0\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None }\r\n2024-09-03T07:28:04.023Z [temporal-worker] \r\n2024-09-03T07:28:04.023Z [temporal-worker] The above exception was the direct cause of the following exception:\r\n2024-09-03T07:28:04.023Z [temporal-worker] \r\n2024-09-03T07:28:04.023Z [temporal-worker] Traceback (most recent call last):\r\n2024-09-03T07:28:04.023Z [temporal-worker]   File \"/lib/python3.10/site-packages/temporalio/worker/_workflow.py\", line 153, in run\r\n2024-09-03T07:28:04.023Z [temporal-worker]     raise RuntimeError(\"Workflow worker failed\") from err\r\n2024-09-03T07:28:04.023Z [temporal-worker] RuntimeError: Workflow worker failed\r\n```\r\n\r\n### Environment/Versions\r\n\r\n- Temporal Version: Server v1.23.1, SDK v1.7.0\r\n\r\n### Additional context\r\n\r\nRelated issue: #459 ","closedAt":"2025-04-03T13:13:50Z","comments":[{"id":"IC_kwDOGusT1c6KqtC4","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"> causes the worker to hang indefinitely.\r\n\r\nWe do check that the worker can connect to the namespace using \"describe namespace\". So you have a token that works on some calls but not others? Is this self-hosted or cloud?\r\n\r\nThe stack trace seems to be showing it raising an exception. Are you sure that `worker.run()` does not raise an exception here? It does take a minute because we retry all polling errors just in case they are spurious.","createdAt":"2024-09-03T12:53:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2326450360","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6KtODQ","author":{"login":"kelkawi-a"},"authorAssociation":"NONE","body":"This issue is two-fold, I reported the one relevant to the SDK here:\r\n1. I have a worker which sends the correct token to a self-hosted Temporal server (v1.23.1). There is an issue at the ingress level which sometimes causes the server to respond with a `504 Bad Gateway`.\r\n2. As the worker continues to receive this response from the ingress, it exhibits the behavior reported in this bug, eventually hanging and failing to re-connect.\r\n\r\nThe second issue is what i'm reporting here. The worker ends up raising a `RuntimeError` and not recovering from it.","createdAt":"2024-09-03T17:56:11Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2327109840","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6Kte-j","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"> The worker ends up raising a RuntimeError and not recovering from it.\r\n\r\nSome client errors we can detect as recoverable. For ones we can't, we still try to recover for a minute before failing the worker. We intentionally fail the worker instead of letting it operate in a failed state on something that is not quickly/obviously recoverable. Can you confirm whether `worker.run()` does or does not raise an exception here? We start a worker shutdown, but a worker shutdown sends cancellation to activities and has to wait for activities to complete (see https://github.com/temporalio/sdk-python?tab=readme-ov-file#worker-shutdown).","createdAt":"2024-09-03T18:39:18Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2327179171","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6Ku2c2","author":{"login":"kelkawi-a"},"authorAssociation":"NONE","body":"I'm not sure if you're referring to something different, but the logs shared in the original bug report show that `worker.run()` is raising a `RuntimeError`.","createdAt":"2024-09-03T22:15:37Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2327537462","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6Ku97J","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"Ah, I see it in the trace now. This is intentional behavior. A fatal error (or at least one we can't tell is non-fatal) that doesn't fix itself after a minute will cause the worker to fail and shutdown instead of pretending to work silently while continuing to fail. You may restart the worker if you wish, though many prefer not to blindly restart but rather investigate.","createdAt":"2024-09-03T22:45:06Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2327568073","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6Ku-7j","author":{"login":"kelkawi-a"},"authorAssociation":"NONE","body":"In our case the worker is failing after a number of intermittent network issues, so we would rather it restarts instead. Is there any recommendation around how to restarting the worker? I tried a simple `while True` loop to catch the `RuntimeError` exception raised and re-run `await worker.run()`, which seemed to complain (I don't have the logs from that right now but I can get them).","createdAt":"2024-09-03T22:49:16Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2327572195","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6K1QMz","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"> I tried a simple `while True` loop\r\n\r\nThis should work. Similarly you can consider having whatever is monitoring the process/pod/container do the restart at an outer level. I would recommend at least also alerting or something on fatal worker error or you won't know your worker isn't working.","createdAt":"2024-09-04T14:25:38Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2329215795","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6K8uq6","author":{"login":"kelkawi-a"},"authorAssociation":"NONE","body":"I did try the following to retry the worker:\r\n```\r\n    while True:\r\n        try:\r\n            await worker.run()\r\n        except RuntimeError as e:\r\n            print(f\"RuntimeError caught: {e}. Retrying...\")\r\n            await asyncio.sleep(5)  # wait for 5 seconds before retrying\r\n        except Exception as e:\r\n            print(f\"An unexpected error occurred: {e}\")\r\n            raise\r\n```\r\n\r\nAfter running for a while, the worker crashed with the following error when trying to restart:\r\n\r\n```\r\n  File \"/home/kelkawi/.cache/pypoetry/virtualenvs/boarding-w3anQpca-py3.10/lib/python3.10/site-packages/temporalio/worker/_workflow.py\", line 143, in run\r\n    act = await self._bridge_worker().poll_workflow_activation()\r\n  File \"/home/kelkawi/.cache/pypoetry/virtualenvs/boarding-w3anQpca-py3.10/lib/python3.10/site-packages/temporalio/bridge/worker.py\", line 141, in poll_workflow_activation\r\n    await self._ref.poll_workflow_activation()\r\nRuntimeError: Poll failure: Unhandled grpc error when workflow polling: Status { code: PermissionDenied, message: \"Request unauthorized.\", details: b\"\\x08\\x07\\x12\\x15Request unauthorized.\\x1aJ\\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure\", metadata: MetadataMap { headers: {\"date\": \"Thu, 05 Sep 2024 09:43:21 GMT\", \"content-type\": \"application/grpc\", \"content-length\": \"0\", \"strict-transport-security\": \"max-age=15724800; includeSubDomains\"} }, source: None }\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/kelkawi/.cache/pypoetry/virtualenvs/boarding-w3anQpca-py3.10/lib/python3.10/site-packages/temporalio/worker/_workflow.py\", line 153, in run\r\n    raise RuntimeError(\"Workflow worker failed\") from err\r\nRuntimeError: Workflow worker failed\r\nRuntimeError caught: Workflow worker failed. Retrying...\r\nAn unexpected error occurred: 'NoneType' object has no attribute 'validate'\r\nTraceback (most recent call last):\r\n  File \"/home/kelkawi/Desktop/CanonicalRepos/hr-automation/boarding/worker.py\", line 219, in <module>\r\n    asyncio.run(run_worker())\r\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/home/kelkawi/Desktop/CanonicalRepos/hr-automation/boarding/worker.py\", line 207, in run_worker\r\n    await worker.run()\r\n  File \"/home/kelkawi/.cache/pypoetry/virtualenvs/boarding-w3anQpca-py3.10/lib/python3.10/site-packages/temporalio/worker/_worker.py\", line 478, in run\r\n    await self._bridge_worker.validate()\r\n  File \"/home/kelkawi/.cache/pypoetry/virtualenvs/boarding-w3anQpca-py3.10/lib/python3.10/site-packages/temporalio/bridge/worker.py\", line 133, in validate\r\n    await self._ref.validate()\r\nAttributeError: 'NoneType' object has no attribute 'validate'\r\n```","createdAt":"2024-09-05T10:30:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2331175610","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6K_FsY","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"A worker is meant for one run/shutdown. If you want to run a new worker you will need to recreate it again. Unfortunately some validation we added is happening before this check, we will fix that.","createdAt":"2024-09-05T14:12:51Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2331794200","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6gIhet","author":{"login":"hebrd"},"authorAssociation":"NONE","body":"> This issue is two-fold, I reported the one relevant to the SDK here:\n> \n> 1. I have a worker which sends the correct token to a self-hosted Temporal server (v1.23.1). There is an issue at the ingress level which sometimes causes the server to respond with a `504 Bad Gateway`.\n> 2. As the worker continues to receive this response from the ingress, it exhibits the behavior reported in this bug, eventually hanging and failing to re-connect.\n> \n> The second issue is what i'm reporting here. The worker ends up raising a `RuntimeError` and not recovering from it.\n\nI have the same issues, too. Would you please share the solution to issue 1?","createdAt":"2025-02-27T01:38:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2686588845","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6gLbY8","author":{"login":"kelkawi-a"},"authorAssociation":"NONE","body":"@hebrd This was related to the ingress timing out when the worker does long polls of 60 seconds. One workaround we implemented was reducing the long polling interval through the Temporal server dynamic config:\n\n```\nmatching.longPollExpirationInterval:\n  - value: \"50s\"\n```","createdAt":"2025-02-27T09:16:16Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2687350332","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6gNQ65","author":{"login":"hebrd"},"authorAssociation":"NONE","body":"Many thanks. It help a lot!!!","createdAt":"2025-02-27T12:33:36Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2687831737","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6lcpaZ","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"Closing this issue as it seems to have been a ingress configuration issue (Temporal needs to be able to make 70s calls).","createdAt":"2025-04-03T13:13:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/631#issuecomment-2775750297","viewerDidAuthor":false}],"createdAt":"2024-09-03T10:18:14Z","labels":[{"id":"LA_kwDOGusT1c7gQgHK","name":"bug","description":"Something isn't working","color":"d73a4a"}],"milestone":null,"number":631,"reactionGroups":[],"state":"CLOSED","title":"[Bug] Worker hangs after polling workflow task queue","updatedAt":"2025-04-03T13:13:51Z","url":"https://github.com/temporalio/sdk-python/issues/631"}
