{"assignees":[],"author":{"id":"MDQ6VXNlcjk1NTk2NTU=","is_bot":false,"login":"gregbrowndev","name":"Greg Brown"},"body":"### Is your feature request related to a problem? Please describe.\n\n<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->\n\nCurrently, the SDK doesn't extend custom metric support via the Runtime's MetricMeter to activities running within a multi-processed worker. This means users have to figure out how to support this themselves, which adds complexity to the project and may be infeasible for some.\n\n### Describe the solution you'd like\n\n<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->\n\nIdeally, I'd hope to see metrics wired up across the process pool out of the box. So following a similar approach to the existing heartbeat manager, metrics emitted by the child process would be sent via a multiprocessing queue to the parent worker process.\n\nPossible high-level approach:\n\n- Provide `temporalio.common.MetricMeter` implementation alternate to [_MetricMeter](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/runtime.py#L480) that is set in [`_Context`](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/activity.py#L190) automatically in process pool workers.\n- Instead of being backed by `temporalio.bridge.metric.MetricMeter`, they would push messages onto a queue consumed by the parent process, which would then rely those actions against the real `temporalio.bridge.metric.MetricMeter`.\n\nThis might be easier Python side, than trying to wire up the Rust code under the hood.\n\nWorth pointing out there are on-going proposals to do this natively in OTEL: https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/4968.\n\nI'd be happy to support this work, if you think the plan sounds reasonable. \n\n### Additional context\n\n<!-- Add any other context or screenshots about the feature request here. -->\n\n#### **Background**\n\nThe previous work to add custom metric support, https://github.com/temporalio/sdk-python/pull/384, at the time didn't include support for multi-processed worker:\n\n> NOTE: This does not return a metric meter for sync activities that are non-threaded (i.e. multiprocess) because a meter cannot be shared across processes and it's a large effort to support metric shipping across processes for this rarely used activity type\n\n#### **Case to support this feature**\n\nThe multi-processed worker provides a lot of value, at least on my project which uses Temporal for its typical system integration use cases but also for managing some reasonably heavy CPU bound workloads (e.g. processing PDFs, NLP, small batch jobs, etc.) all within one tool. \n\nIt might be the least frequently used worker type, but it provides several key benefits of its own:\n\n- It highlights a clear design principle to split up CPU bound workloads at the workflow level, i.e. fanning out, and then keep each activity single threaded/single processed to optimise performance, avoid under/over utilisation of the underlying CPU cores, and support better auto-scaling decisions. Its easier to understand when size and scheduling of tasks on the process pool is managed by the worker itself, rather than a free-for-all that could occur if async/threaded activities were to manage their own/shared processes or pools.\n- It provides out of the box support for heartbeating in activities running in the process pool, which users would need to wire up themselves if managing processes in the asyncio/threaded workers.\n- Python is a common data science language, so a process pool based worker would be the go to over async/threaded\n\n#### **Existing solution** \n\nBesides having to set up and configure a second OTEL metrics toolchain (`MeterProvider`, processors, exporters, and using different `meter` and `metric` classes), users would need to:\n\n- ensure the toolchain is initialised correctly to properly configure the process pool\n- be concerned with different default `mp_context`  behaviours on different OSs, e.g. maybe they develop on MacOS which defaults to `spawn`, but deploy in a linux based image, which defaults to `fork` ([the default will change to `forkserver` Python 3.14](https://docs.python.org/3.14/whatsnew/3.14.html#incompatible-changes)) \n- ensure with the global metric provider isn't accidentally inherited by the child process, which can cause deadlocks, or understand fork pre init hooks. \n\nOnce the process pool is configured, we'd then need to get metrics out of the process correctly, e.g.:\n\n- Tagging metrics with the process ID `pid` and using `OTLPMetricExporter` provides a simple solution, since each process would just push its own metrics to a collector. However, this would lead to high-cardinality, increased costs, and further complexity down the line.\n\n  > Note: the metric payloads scraped/pushed to the backend need to be uniquely labeled, otherwise you get out-of-order writes, conflicts, and are otherwise left with meaningless metric values interleaved from multiple ambiguous sources. (It can also cause you to exceed the platform's data points per minute (DPM), which in Grafana Clouds case DPM=1, i.e. scrape/emit every 60s. \n\n- The initial simplicity would then come at the cost of having to set up some external metric aggregation. E.g. within an OTEL collector's [transformprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor#supported-functions) using [aggregate_on_attributes](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/v0.128.0/processor/transformprocessor#aggregate_on_attributes) to aggregate out the PID label. This also requires sensitive handling using `interval`, `batch`, `metricstarttime`, etc. to ensure you capture all the metrics within one interval.","closedAt":null,"comments":[{"id":"IC_kwDOGusT1c6__k-n","author":{"login":"tconley1428"},"authorAssociation":"MEMBER","body":"This does seem like a valuable feature for multi process workers, but that isn't a very large scenario at the moment and we have limited bandwidth. We'll put it up for triage, but it isn't very likely to be prioritized soon. ","createdAt":"2025-08-25T17:24:29Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/1049#issuecomment-3221114791","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c7ASEcX","author":{"login":"gregbrowndev"},"authorAssociation":"NONE","body":"Thanks for considering the request @tconley1428 ðŸ™ðŸ» \n\nI understand the bandwidth constraints (I hope the Nexus work is going well!). Would it be helpful to receive contributions in this area? I'd quite like to drive forward observability related features for my own project's sake and to give back to the project. \n\n---\n\nI did spend quite a bit of time over the weekend digging into the Rust SDK code to better understand whats happening under the hood. \n\nFor me, most of the SDK's latency histogram metric are basically a bit useless and expensive, since we can't specify bucket overrides in `OtelConfig` like you can in the Prometheus config and all of my tasks are long running and even `scheduled_to_start` latencies go higher than the [default settings](https://github.com/temporalio/sdk-core/blob/d34c1d6d393462a816baf2469c256a21ffbaf196/core/src/telemetry/metrics.rs#L675-L725) (very much specific to my own use case, I know). So even if I solved this issue, I'd still be constrained by the bucket settings in the Rust code. Though, ideally, we'd be able to export exponential histogram formats.\n\nAdditionally, the currently metric meter doesn't support other advanced features like [capturing trace exemplars](https://github.com/open-telemetry/opentelemetry-python/blob/1aaa2a25872c36aee208442ff654a67f5daa5736/opentelemetry-sdk/src/opentelemetry/sdk/metrics/_internal/__init__.py#L427-L437) (trace-to-metrics), as it would mean adding that to the abstraction / dynamically loading OTEL's tracer within the metric meter some how...\n\nSo I'm just wondering now whether the SDK would be better off with a Python-first OTEL integration built on top of the `MetricBuffer` to access the raw SDK datapoints and then have access to all the features, like `views`, `resource`, tracing-integration, etc. all configurable within the standard Python OTEL SDK. \n\nIs that something that is being considered? Otherwise, I'd be happy to be able to contribute towards these missing features if the [language] -> Rust approach is the intended solution going forward.\n\nCheers","createdAt":"2025-08-26T22:46:42Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/1049#issuecomment-3225962263","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c7ByIvT","author":{"login":"Sushisource"},"authorAssociation":"MEMBER","body":"@gregbrowndev Thanks for all this great feedback.\n\nSo, I agree that probably the best option here is to use the `MetricBuffer` as you suggest, so that you can directly do the more advanced things you'd like to do in Python using the exporter directly. It is, indeed, exactly what `MetricBuffer` is intended for.\n\nSomething using it to export via OTel could I think be added to contrib here: https://github.com/temporalio/sdk-python/blob/main/temporalio/contrib/opentelemetry.py and would be a welcome contribution","createdAt":"2025-09-03T23:46:02Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/1049#issuecomment-3251145683","viewerDidAuthor":false}],"createdAt":"2025-08-24T16:52:34Z","labels":[{"id":"LA_kwDOGusT1c7gQgHN","name":"enhancement","description":"New feature or request","color":"a2eeef"}],"milestone":null,"number":1049,"reactionGroups":[],"state":"OPEN","title":"[Feature Request] Support access to metric meter in multi-processed workers","updatedAt":"2025-09-03T23:46:02Z","url":"https://github.com/temporalio/sdk-python/issues/1049"}
