{"assignees":[],"author":{"id":"MDQ6VXNlcjQxMTI1MjMw","is_bot":false,"login":"databill86","name":""},"body":"### What are you really trying to do?\n\nI'm building a multi-tenant application using Temporal workflows with OpenAI agents where different agents need to use different proxy configurations (different API keys and base URLs). I want each agent to have its own model configuration rather than being forced to use a global OpenAI client configuration.\n\n### Describe the bug\n\nTemporal workflows with OpenAI agents require the model name to be a string, but this prevents using custom model objects like `LitellmModel` with specific API keys and base URLs for individual agents. This limitation forces all agents to use the same global OpenAI client configuration, making it impossible to have different proxy configurations per agent.\n\nWhen using Temporal workflows with OpenAI agents, the `convert_agent` function in `temporalio/contrib/openai_agents/_openai_runner.py` enforces that the model must be a string:\n\n```python\ndef _model_name(agent):\n    # ...\n    raise ValueError(\n        \"Temporal workflows require a model name to be a string in the agent.\"\n    )\n```\n\nThis prevents using custom model objects like `LitellmModel` that allow per-agent configuration of API keys and base URLs.\n\n**Expected Behavior:**\nAgents should be able to use custom model objects that implement the `Model` interface, allowing for:\n- Per-agent API key configuration\n- Per-agent base URL configuration (for proxy setups)\n- Flexible model provider selection per agent\n\n**Current Behavior:**\nUsing a custom `LitellmModel` object in an agent results in:\n```\nValueError: Temporal workflows require a model name to be a string in the agent.\n```\n\n### Minimal Reproduction\n\n**1. Activity Definition (Tool ‚Üí Activity conversion):**\n```python\nfrom temporalio import activity\n\n@activity.defn\nasync def simple_tool_activity(message: str) -> str:\n    \"\"\"Simple tool activity that processes a message.\"\"\"\n    logger.info(f\"üîß Simple tool activity processing: {message}\")\n    response = f\"Tool processed: '{message}' - Response from activity!\"\n    return response\n\n# The activity is automatically converted to a tool when passed to Agent\ntools = [simple_tool_activity]\n```\n\n**2. Workflow with Agent:**\n```python\nfrom agents import Agent\nfrom agents.extensions.models.litellm_model import LitellmModel\nfrom temporalio import workflow\n\n@workflow.defn\nclass MyWorkflow:\n    @workflow.run\n    async def run(self) -> str:\n        # Inside the workflow\n        \n        agent = Agent(\n            name=\"Triage Agent\",\n            instructions=\"Your instructions here\",\n            model=LitellmModel(model=\"model_name\", api_key=\"proxy_api_key\", base_url=\"proxy_base_url\"),\n            # or this: model=OpenAIChatCompletionsModel(model=\"model_name\", openai_client=AsyncOpenAI(api_key=key, base_url=base_url)),\n            # model=\"model_name\", # This works if the client is created with plugins\n            tools=tools,  # Activities are automatically converted to tools\n            handoffs=handoffs,\n        )\n        \n        # This is where the error occurs - agent.run() internally calls convert_agent()\n        result = await agent.run(\"Your message here\")\n        return result\n```\n\n**3. Worker Setup:**\n```python\nfrom temporalio.worker import Worker\n\nasync def main():\n    # Create Temporal client with plugins (required for agents)\n    client = await create_temporal_client(include_plugins=True)\n    \n    # Run the worker\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as activity_executor:\n        worker = Worker(\n            client,\n            task_queue=\"demo-task-queue\",\n            workflows=[MyWorkflow],\n            activities=[simple_tool_activity],  # Register the activity\n            activity_executor=activity_executor,\n        )\n        await worker.run()\n\n# This fails when client is created without plugins\nClient.connect(\n    \"localhost:7233\",\n    namespace=\"default\",\n    tls=False,\n    plugins=[],  # Empty plugins causes the error\n)\n\n# Start the workflow\nworkflow_handle = await client.start_workflow(\n    MyWorkflow.run,\n    id=\"my-workflow-id\",\n    task_queue=\"demo-task-queue\",\n)\n```\n\n**Workaround that works:**\n\n```python\nplugins = [\n    OpenAIAgentsPlugin(\n        model_params=ModelActivityParameters(\n            start_to_close_timeout=timedelta(seconds=30)\n        ),\n        model_provider=CustomLitellmProvider(\n            base_url=PROXY_BASE_URL,\n            api_key=PROXY_API_KEY,\n        ),\n    ),\n]\n\nClient.connect(\n    \"localhost:7233\",\n    namespace=\"default\",\n    tls=False,\n    plugins=plugins,  # With plugins it works\n)\n\n# In the workflow: use model=\"model_name\" instead of LitellmModel\n# The agent.run() call succeeds because convert_agent() can handle string model names\n```\n\n\nThis is the CustomLitellmProvider implementation:\n\n```python\nfrom agents.extensions.models.litellm_model import LitellmModel\nfrom agents.models.interface import Model, ModelProvider\n\n\nclass CustomLitellmProvider(ModelProvider):\n    \"\"\"\n    A custom ModelProvider that uses LiteLLM with configurable base_url and api_key.\n    \"\"\"\n\n    def __init__(self, base_url: str | None = None, api_key: str | None = None):\n        \"\"\"\n        Initialize the custom Litellm provider.\n\n        Args:\n            base_url: The base URL for the API (e.g., proxy endpoint)\n            api_key: The API key for authentication\n        \"\"\"\n        self.base_url = base_url\n        self.api_key = api_key\n\n    @property\n    def model_class(self) -> type[Model]:\n        \"\"\"Get the model class used by this provider.\"\"\"\n        return LitellmModel\n\n    @property\n    def provider_name(self) -> str:\n        \"\"\"Get the name of this provider.\"\"\"\n        return \"CustomLitellmProvider\"\n\n    @property\n    def is_proxy_configured(self) -> bool:\n        \"\"\"Check if proxy configuration is set.\"\"\"\n        return self.base_url is not None and self.api_key is not None\n\n    def get_model(self, model_name: str) -> Model:\n        \"\"\"\n        Get a LitellmModel instance with the configured base_url and api_key.\n\n        Args:\n            model_name: The name of the model to use\n\n        Returns:\n            A LitellmModel instance configured with the provider's settings\n        \"\"\"\n        if not model_name:\n            raise ValueError(\"Model name is required\")\n        return LitellmModel(\n            model=model_name,\n            base_url=self.base_url,\n            api_key=self.api_key,\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Return a string representation of the provider.\"\"\"\n        proxy_status = \"configured\" if self.is_proxy_configured else \"not configured\"\n        return (\n            f\"CustomLitellmProvider(\"\n            f\"base_url={self.base_url!r}, \"\n            f\"api_key={'***' if self.api_key else None!r}, \"\n            f\"proxy={proxy_status})\"\n        )\n\n    def __str__(self) -> str:\n        \"\"\"Return a user-friendly string representation.\"\"\"\n        return (\n            f\"CustomLitellmProvider with \"\n            f\"base_url={self.base_url or 'default'}, \"\n            f\"proxy={'enabled' if self.is_proxy_configured else 'disabled'}\"\n        )\n\n```\n\n### Environment/Versions\n\n- OS and processor: Linux\n- Temporal Version: temporalio==1.18.0\n- OpenAI SDK: openai==1.109.0\n- OpenAI Agents: openai-agents==0.3.2\n- Python: 3.11\n- Are you using Docker or Kubernetes or building Temporal from source? Using Docker\n\n### Additional context\n\n**Use Case:**\nIn a multi-tenant application with different proxy configurations per agent:\n- Agent A needs to use Proxy 1 with specific API key\n- Agent B needs to use Proxy 2 with different API key  \n- Agent C needs to use direct OpenAI API\n\nCurrently, this is impossible because all agents must use the same global OpenAI client configuration.\n\n**Important Note:**\nOne of the most important reasons I don't want to use the global OpenAI client configuration is to use logfire to trace the agents. When I used the plugins, the logfire tracing was not working at all.\n\n## Tracing Conflict Issue\n\nThe plugin approach conflicts with observability/tracing setup. Here's the tracing configuration that doesn't work with plugins:\n\n```python\nfrom agents import set_trace_processors\nimport logfire\nimport nest_asyncio\n\ndef init_tracing():\n    \"\"\"Initialize tracing and observability.\"\"\"\n    # Set Langfuse env vars from settings (user can override via real env)\n    os.environ.setdefault(\"LANGFUSE_PUBLIC_KEY\", LANGFUSE_PUBLIC_KEY)\n    os.environ.setdefault(\"LANGFUSE_SECRET_KEY\", LANGFUSE_SECRET_KEY)\n    os.environ.setdefault(\"LANGFUSE_HOST\", LANGFUSE_HOST)\n\n    set_trace_processors([])  # only disable OpenAI tracing\n\n    # Instrument OpenAI Agents SDK via pydantic-ai logfire\n    try:\n        nest_asyncio.apply()\n        logfire.configure(service_name=\"proxy\", send_to_logfire=False)\n        # This method automatically patches the OpenAI Agents SDK to send logs via OTLP to Langfuse.\n        logfire.instrument_openai_agents()\n    except Exception as exc:  # noqa: BLE001\n        logger.error(f\"Logfire instrumentation not available: {exc}\")\n\n# Worker setup with tracing\nasync def main():\n    # Initialize tracing (conflicts with plugins)\n    init_tracing()  # Sets up logfire ‚Üí OTLP ‚Üí Langfuse\n    \n    # Create Temporal client with plugins (required for agents)\n    client = await create_temporal_client(include_plugins=True)\n    \n    # Run the worker\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as activity_executor:\n        worker = Worker(\n            client,\n            task_queue=\"demo-task-queue\",\n            workflows=[MyWorkflow],\n            activities=[simple_tool_activity],\n            activity_executor=activity_executor,\n        )\n        await worker.run()\n```\n\n**The Problem:** When using plugins, the logfire tracing instrumentation doesn't work properly, making it impossible to trace agent execution in Langfuse. This forces a choice between:\n1. Using plugins (works with string models) but losing observability\n2. Using custom model objects (maybe better observability, not tested because of the string model requirement).\n\nIf you've run into this same issue or have any insights on how to work around it, I'd really appreciate hearing from you. \n\nThanks üôè\n\n","closedAt":"2025-09-30T15:08:11Z","comments":[{"id":"IC_kwDOGusT1c7H0Rku","author":{"login":"tconley1428"},"authorAssociation":"MEMBER","body":"Using a string to represent the model is a hard requirement. Because the models need to be invoked inside an activity, we need to be able to acquire the model there. Because `Model` is generally not serializable, this means that we need an alternate way of acquiring the model rather than being provided an instantiated model. The answer to this is a model name, combined with a `ModelProvider` which can be instantiated on the worker side, where the activities are executed.\n\nThis does _not_ however mean that this is true:\n> This prevents using custom model objects like LitellmModel that allow per-agent configuration of API keys and base URLs.\n\nYou can either use an existing `ModelProvider` if it gives you the semantics you need, like `MultiProvider` or `LitellmProvider`, or you can always create your own to provide an arbitrary mapping from model name to the model you desire, however complex. Consider this sample: https://github.com/temporalio/samples-python/blob/main/openai_agents/model_providers/run_gpt_oss_worker.py\nIt has a simple implementation of a custom model provider, but in your case you can define the name to model mapping however you want. The tenant can be included in the name in some fashion and you can construct an appropriate LiteLLM model. (You can look at the definition of `LitellmProvider` as a reference). \n\nIf you need additional assistance in making that work, I would recommend the community slack. https://t.mp/slack allows better back and forth than github issues.\n\nThe tracing concern appears to be an entirely separate issue. Unless you see reason there is a substantial intersection between the two, please open a new issue instead on that concern. That way any conversation or changes to address it will not be interleaved with this.\n","createdAt":"2025-09-30T14:06:20Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/sdk-python/issues/1134#issuecomment-3352369454","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c7H07rV","author":{"login":"databill86"},"authorAssociation":"NONE","body":"Thank you for you response.\n\n\nAfter further investigation, I conducted additional testing to explore alternative approaches and understand the behavior better. Here are my findings: \n\n1. Interestingly, when I don't use plugins, but specify the model name as a string, I have the traces in langfuse, but the workflow fails with the error:\n\n\n```log\n[EDITED]\nü§ñ Running agent without plugins + string model...\n16:10:26.073 OpenAI Agents trace: Agent workflow\n16:55:53.393   Agent run: 'Demo Agent Name'\nGetting response for model: gpt-4.1-mini-2025-04-14\nModel settings: ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), base_url: https://api.openai.com/v1/, api_key: sk-proj-******\n16:55:53.435     Responses API with 'gpt-4.1-mini-2025-04-14'\nTraceback (most recent call last):\n  File \"/lib/python3.11/site-packages/agents/models/openai_responses.py\", line 92, in get_response\n    response = await self._fetch_response(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/lib/python3.11/site-packages/agents/models/openai_responses.py\", line 309, in _fetch_response\n    return await self._client.responses.create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/lib/python3.11/site-packages/openai/resources/responses/responses.py\", line 2259, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/lib/python3.11/site-packages/openai/_base_client.py\", line 1794, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/lib/python3.11/site-packages/openai/_base_client.py\", line 1494, in request\n    self._platform = await asyncify(get_platform)()\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/lib/python3.11/site-packages/openai/_utils/_sync.py\", line 84, in wrapper\n    return await to_thread(function, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/lib/python3.11/site-packages/openai/_utils/_sync.py\", line 45, in to_thread\n    return await _asyncio_to_thread(func, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n    return await loop.run_in_executor(None, func_call)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/lib/python3.11/asyncio/events.py\", line 290, in run_in_executor\n    raise NotImplementedError\nNotImplementedError\nERROR:openai.agents:Error getting response: . (request_id: None)\n```\n\nThe code used in the workflow is:\n\n```python\ntool = temporal_agents.workflow.activity_as_tool(\n    simple_tool_activity,\n    start_to_close_timeout=timedelta(minutes=2),\n)\nos.environ.setdefault(\"OPENAI_API_KEY\", OPENAI_API_KEY)\nagent = Agent(\n    name=\"Demo Agent\",\n    instructions=\"instructions here\",\n    model=model_name,  # String model name\n    tools=[tool],\n\n# Run the agent\ninput_items = cast(\n    List[TResponseInputItem], [{\"role\": \"user\", \"content\": \"test message\"}]\n)\n\nlogger.info(\"Running agent without plugins + string model...\")\nresult = await Runner.run(agent, input_items, context={})\n\n)\n```\n\n2. I also tried to use the `LitellmModel` object in the Agent, but without plugins in the client, and surprisingly I got the traces in langfuse, but the workflow failed with the error:\n\n```log\n- ü§ñ Running agent without plugins + LitellmModel...\n16:20:32.620 OpenAI Agents trace: Agent workflow\n16:20:32.622   Agent run: 'Demo Agent'\n16:20:32.625     Chat completion with 'gpt-4.1-mini-2025-04-14' [LLM]\n16:20:32 - LiteLLM:DEBUG: utils.py:359 - \n\nDEBUG:LiteLLM:\n\n\n\n16:20:32 - LiteLLM:DEBUG: utils.py:359 - Request to litellm:\nDEBUG:LiteLLM:Request to litellm:\nRequest to litellm:\n16:20:32 - LiteLLM:DEBUG: utils.py:359 - litellm.acompletion(model='gpt-4.1-mini-2025-04-14', messages=[{'content': '# System context\\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_<agent_name>`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\\n\\nYou are a demo agent. Use the simple_tool_activity to process user messages.', 'role': 'system'}, {'role': 'user', 'content': 'Hello, this is a test message for the demo!'}], tools=[{'type': 'function', 'function': {'name': 'simple_tool_activity', 'description': '', 'parameters': {'properties': {'message': {'title': 'Message', 'type': 'string'}}, 'required': ['message'], 'title': 'simple_tool_activity_args', 'type': 'object', 'additionalProperties': False}}}], temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.3.2'}, api_key='sk-6******', base_url='http://0.0.0.0:4004')\nDEBUG:LiteLLM:litellm.acompletion(model='gpt-4.1-mini-2025-04-14', messages=[{'content': '# System context\\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_<agent_name>`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\\n\\nYou are a demo agent. Use the simple_tool_activity to process user messages.', 'role': 'system'}, {'role': 'user', 'content': 'Hello, this is a test message for the demo!'}], tools=[{'type': 'function', 'function': {'name': 'simple_tool_activity', 'description': '', 'parameters': {'properties': {'message': {'title': 'Message', 'type': 'string'}}, 'required': ['message'], 'title': 'simple_tool_activity_args', 'type': 'object', 'additionalProperties': False}}}], temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.3.2'}, api_key='sk-*****', base_url='http://0.0.0.0:4004')\nlitellm.acompletion(model='gpt-4.1-mini-2025-04-14', messages=[{'content': '# System context\\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_<agent_name>`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\\n\\nYou are a demo agent. Use the simple_tool_activity to process user messages.', 'role': 'system'}, {'role': 'user', 'content': 'Hello, this is a test message for the demo!'}], tools=[{'type': 'function', 'function': {'name': 'simple_tool_activity', 'description': '', 'parameters': {'properties': {'message': {'title': 'Message', 'type': 'string'}}, 'required': ['message'], 'title': 'simple_tool_activity_args', 'type': 'object', 'additionalProperties': False}}}], temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.3.2'}, api_key='sk-*****', base_url='http://0.0.0.0:4004')\n16:20:32 - LiteLLM:DEBUG: utils.py:359 - \n\nDEBUG:LiteLLM:\n\n\n\n16:20:32 - LiteLLM:WARNING: utils.py:538 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\nWARNING:LiteLLM:`litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n16:20:32 - LiteLLM:DEBUG: litellm_logging.py:476 - self.optional_params: {}\nDEBUG:LiteLLM:self.optional_params: {}\n16:20:32 - LiteLLM:DEBUG: utils.py:359 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG:LiteLLM:ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n16:20:32 - LiteLLM:DEBUG: caching_handler.py:233 - CACHE RESULT: None\nDEBUG:LiteLLM:CACHE RESULT: None\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n16:20:32 - LiteLLM:DEBUG: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\n16:20:32 - LiteLLM:DEBUG: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\nDEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n2025-09-30 16:20:32.668 | ERROR    | temporal_demo.workflows.demo_workflow:run:78 - ‚ùå Demo workflow failed: litellm.APIConnectionError: APIConnectionError: OpenAIException - \n```\n\n\nThe code used in the workflow is:   \n\n```python\n# Create a simple tool\ntool = temporal_agents.workflow.activity_as_tool(\n    simple_tool_activity,\n    start_to_close_timeout=timedelta(minutes=2),\n)\n\n# Create agent with LitellmModel but no plugins\nagent = Agent(\n    name=\"Demo Agent\",\n    instructions=\"instructions here\",\n    model=LitellmModel(\n        model=model_name,\n        api_key=\"sk-*****\",\n        base_url=\"http://0.0.0.0:4004\",\n    ),\n    tools=[tool],\n)\n\n# Run the agent\ninput_items = cast(\n    List[TResponseInputItem], [{\"role\": \"user\", \"content\": \"test message\"}]\n)\n\nlogger.info(\"Running agent without plugins + LitellmModel...\")\nresult = await Runner.run(agent, input_items, context={})\n```\n\nNote that the same litellm config is working when used outside of the workflow.\n\nSo, I think that specifying the plugins in the worker have something to do with the traces not being sent to langfuse here.","createdAt":"2025-09-30T14:41:36Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/1134#issuecomment-3352541909","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c7H1YG2","author":{"login":"tconley1428"},"authorAssociation":"MEMBER","body":"You can't use the agents SDK in a workflow without the plugin. It simply will not work. The plugin does a bunch of things to make it work, and without it the agents SDK has no provision for running the models in activities. It simply isn't an option without essentially doing everything the plugin does anyway. Langfuse tracing not working should be a separate issue which we can investigate.","createdAt":"2025-09-30T15:08:01Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/1134#issuecomment-3352658358","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c7H1b_d","author":{"login":"databill86"},"authorAssociation":"NONE","body":"Thanks, I will create a separate issue for the tracing. ","createdAt":"2025-09-30T15:11:28Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/1134#issuecomment-3352674269","viewerDidAuthor":false}],"createdAt":"2025-09-30T13:27:01Z","labels":[{"id":"LA_kwDOGusT1c7gQgHK","name":"bug","description":"Something isn't working","color":"d73a4a"}],"milestone":null,"number":1134,"reactionGroups":[],"state":"CLOSED","title":"[Bug] Temporal Workflows Require Model Name to be String - LitellmModel Not Supported","updatedAt":"2025-09-30T15:15:10Z","url":"https://github.com/temporalio/sdk-python/issues/1134"}
