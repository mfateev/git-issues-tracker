{"assignees":[],"author":{"id":"MDQ6VXNlcjQxNjMxOTU=","is_bot":false,"login":"florin-chelaru","name":"Florin Chelaru"},"body":"### What are you really trying to do?\n\nTrying to start a worker with a resource-based tuner rather than a fixed one. When setting `target_memory_usage` and `target_cpu_usage` to values between 0 and 1, the resulting worker only allocates the minimum number of slots for workflows and activities. \n\nTo work around the issue, values between 0 and 100 have to be used instead.\n\n### Describe the bug\n\nUsing the resource-based tuner for the worker as indicated [in the documentation](https://github.com/temporalio/sdk-python/blob/154aab9999bad7fc58e3e481d5a5986aa08d72db/temporalio/worker/_tuning.py#L37-L39) ends up creating a worker which always allocates the minimum for the Workflow and Activity slots.\n\n```py\nWorker(\n    client,\n    tuner=WorkerTuner.create_resource_based(\n        target_memory_usage=0.80,  # 80% of available memory\n        target_cpu_usage=0.8,\n    ),\n    ...\n)\n```\n\nThis is because the underlying Rust implementation interprets `target_memory_usage` and `target_cpu_usage` as percentages with values between 0 and 100 (rather than between 0 and 1): https://github.com/temporalio/sdk-core/blob/f0418936c4eec664faab931d0b174b9e57c65668/core/src/worker/tuner/resource_based.rs#L206-L218.\n\nWhen setting values as percentages, i.e. 80.0 rather than 0.8, the tuner behaves as expected.\n\n### Environment/Versions\n\n- OS and processor: M2 Mac\n- Temporal Version: 1.9.0\n- Deploying Temporal to Kubernetes using the default Helm chart\n\n","closedAt":"2025-01-22T12:42:33Z","comments":[{"id":"IC_kwDOGusT1c6bUyLl","author":{"login":"Sushisource"},"authorAssociation":"MEMBER","body":"So that line isn't saying it's a value between 0 and 100. It's saying the absolute limit for the PID controller is 100, which is not related to the underlying value's range.\n\nWhat's relevant is that the PID attempts to reach the set point (the value you provide, between 0 and 1) and that it is comparing that to the measured value, which comes from [here](https://github.com/temporalio/sdk-core/blob/8fd1499ca9069328ef65c78b9638c1806f5e67ba/core/src/worker/tuner/resource_based.rs#L252) and is also between 0 and 1.\n\n> ends up creating a worker which always allocates the minimum for the Workflow and Activity slots.\n\nIf this is indeed true it would be good to know what else is going on - but the problem is not the range of this value.","createdAt":"2025-01-21T23:02:28Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/743#issuecomment-2605916901","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c6bZfHj","author":{"login":"florin-chelaru"},"authorAssociation":"NONE","body":"My bad. I did a closer read of the code and now I understand it. The issue was likely lack of memory in my container. When I set the `target_memory_usage` to `80.0`, it effectively set a target to 8000%, which immediately triggered a spike in available slots. That's what made me suspect this was the issue in the first place. \n\nThanks for the quick response!","createdAt":"2025-01-22T12:42:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/sdk-python/issues/743#issuecomment-2607149539","viewerDidAuthor":false}],"createdAt":"2025-01-21T22:09:44Z","labels":[{"id":"LA_kwDOGusT1c7gQgHK","name":"bug","description":"Something isn't working","color":"d73a4a"}],"milestone":null,"number":743,"reactionGroups":[{"content":"HEART","users":{"totalCount":1}}],"state":"CLOSED","title":"[Bug] ResourceBasedSlotOptions fields not consistent with the underlying Rust implementation","updatedAt":"2025-01-22T13:16:57Z","url":"https://github.com/temporalio/sdk-python/issues/743"}
