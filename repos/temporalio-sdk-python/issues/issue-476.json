{"assignees":[],"author":{"id":"MDQ6VXNlcjE0NjQ1MTk=","is_bot":false,"login":"wilsoniya","name":"Michael Wilson"},"body":"### What are you really trying to do?\r\n\r\nOperate a heavily-loaded multiprocess worker. \r\n\r\n\r\n### Describe the bug\r\n\r\nIn a heavily-loaded multiprocess worker with activities that may possibly be issuing heartbeats too frequently, I observe logging indicating the parent process times out reading from the  `_MultiprocessingSharedStateManager._heartbeat_queue` thus the worker tree effectively hangs, i.e., exhibits no further liveness in its ability to process tasks.  If this situation is unrecoverable, I would expect the worker process tree to terminate, thus allowing a restart (i.e., via systemd, k8s policy, etc.).\r\n\r\nIt's not clear to me why this timeout is happening, but it also results in the heartbeat queue filling up and blocking further progress on activities which heartbeat.\r\n\r\nI see the following logging. First, presumably logs emitted by the parent of the process tree (i.e., the reader of the heartbeat queue): \r\n\r\n```json\r\n{\r\n  \"git\": \"0bd697f642d775670c808da1e216c44764ddb854\",\r\n  \"version\": \"1.2.0\",\r\n  \"event\": \"Failed during multiprocess queue poll for heartbeat\",\r\n  \"timestamp\": \"2024-02-13T19:33:27.414756Z\",\r\n  \"logger\": \"temporalio.worker._activity\",\r\n  \"level\": \"error\",\r\n  \"exception\": [\r\n    {\r\n      \"exc_type\": \"TimeoutError\",\r\n      \"exc_value\": \"\",\r\n      \"syntax_error\": null,\r\n      \"is_cause\": false,\r\n      \"frames\": [\r\n        {\r\n          \"filename\": \"/opt/DPS/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py\",\r\n          \"lineno\": 905,\r\n          \"name\": \"_heartbeat_processor\",\r\n          \"line\": \"\",\r\n          \"locals\": {\r\n            \"self\": \"'<temporalio.worker._activity._MultiprocessingSharedStateManager object at 0x7f23'+9\",\r\n            \"item\": \"\\\"(b'\\\\\\\\n$bc8c0612-11a9-4a58-ab6b-43d37f064f56\\\\\\\\x12\\\\\\\\x18replicate-treenode-20060\\\\\\\\x1a$8\\\"+115\",\r\n            \"completion\": \"'<function _MultiprocessingSharedStateManager.unregister_heartbeater.<locals>.<la'+24\",\r\n            \"fn\": \"'<function _ActivityInboundImpl.execute_activity.<locals>.<lambda> at 0x7f23412cb'+4\"\r\n          }\r\n        },\r\n        {\r\n          \"filename\": \"/opt/DPS/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py\",\r\n          \"lineno\": 644,\r\n          \"name\": \"<lambda>\",\r\n          \"line\": \"\",\r\n          \"locals\": {\r\n            \"details\": \"()\",\r\n            \"heartbeat_with_context\": \"'<function _ActivityInboundImpl.execute_activity.<locals>.heartbeat_with_context '+18\",\r\n            \"loop\": \"<_UnixSelectorEventLoop running=True closed=False debug=False>\"\r\n          }\r\n        },\r\n        {\r\n          \"filename\": \"/usr/lib/python3.8/concurrent/futures/_base.py\",\r\n          \"lineno\": 446,\r\n          \"name\": \"result\",\r\n          \"line\": \"\",\r\n          \"locals\": {\r\n            \"self\": \"None\",\r\n            \"timeout\": \"10\"\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n}\r\n```\r\nAnd second, from the a child in the process tree, as it attempts to push heartbeats to the queue:\r\n\r\n```json\r\n{\r\n  \"git\": \"0bd697f642d775670c808da1e216c44764ddb854\",\r\n  \"version\": \"1.2.0\",\r\n  \"event\": \"ProgressPipe callback raised exception\",\r\n  \"timestamp\": \"2024-02-08T21:26:47.729792Z\",\r\n  \"logger\": \"vault.storage_manager.stream\",\r\n  \"level\": \"error\",\r\n  \"exception\": [\r\n    {\r\n      \"exc_type\": \"Full\",\r\n      \"exc_value\": \"\",\r\n      \"syntax_error\": null,\r\n      \"is_cause\": false,\r\n      \"frames\": [\r\n        {\r\n          \"filename\": \"/opt/DPS/vault-site/vault/storage_manager/stream.py\",\r\n          \"lineno\": 250,\r\n          \"name\": \"_invoke_callback\",\r\n          \"line\": \"\",\r\n          \"locals\": {\r\n            \"self\": \"<vault.storage_manager.stream.ProgressPipe object at 0x7f78d9ab2eb0>\",\r\n            \"e\": \"Full()\"\r\n          }\r\n        },\r\n        {\r\n          \"filename\": \"/opt/DPS/vault-site/vault/temporal/replication/activities.py\",\r\n          \"lineno\": 225,\r\n          \"name\": \"<lambda>\",\r\n          \"line\": \"\",\r\n          \"locals\": {\r\n            \"n\": \"1960837120\",\r\n            \"tree_node\": \"<TreeNode: TreeNode object (278873)>\"\r\n          }\r\n        },\r\n        {\r\n          \"filename\": \"/opt/DPS/venv/lib/python3.8/site-packages/temporalio/activity.py\",\r\n          \"lineno\": 274,\r\n          \"name\": \"heartbeat\",\r\n          \"line\": \"\",\r\n          \"locals\": {\r\n            \"details\": \"(1960837120, 2077125573)\",\r\n            \"heartbeat_fn\": \"<function _execute_sync_activity.<locals>.<lambda> at 0x7f78dab99790>\"\r\n          }\r\n        },\r\n        {\r\n          \"filename\": \"/opt/DPS/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py\",\r\n          \"lineno\": 743,\r\n          \"name\": \"<lambda>\",\r\n          \"line\": \"\",\r\n          \"locals\": {\r\n            \"details\": \"(1960837120, 2077125573)\",\r\n            \"heartbeat_sender\": \"'<temporalio.worker._activity._MultiprocessingSharedHeartbeatSender object at 0x7'+12\",\r\n            \"info\": \"\\\"Info(activity_id='3', activity_type='adjust_replica', attempt=1, current_attempt\\\"+875\"\r\n          }\r\n        },\r\n        {\r\n          \"filename\": \"/opt/DPS/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py\",\r\n          \"lineno\": 924,\r\n          \"name\": \"send_heartbeat\",\r\n          \"line\": \"\",\r\n          \"locals\": {\r\n            \"self\": \"'<temporalio.worker._activity._MultiprocessingSharedHeartbeatSender object at 0x7'+12\",\r\n            \"task_token\": \"\\\"b'\\\\\\\\n$bc8c0612-11a9-4a58-ab6b-43d37f064f56\\\\\\\\x12\\\\\\\\x19replicate-treenode-278873\\\\\\\\x1a$5\\\"+110\",\r\n            \"details\": \"(1960837120, 2077125573)\"\r\n          }\r\n        },\r\n        {\r\n          \"filename\": \"<string>\",\r\n          \"lineno\": 2,\r\n          \"name\": \"put\",\r\n          \"line\": \"\",\r\n          \"locals\": {\r\n            \"self\": \"<AutoProxy[Queue] object, typeid 'Queue' at 0x7f78dab2b070>\",\r\n            \"args\": \"\\\"((b'\\\\\\\\n$bc8c0612-11a9-4a58-ab6b-43d37f064f56\\\\\\\\x12\\\\\\\\x19replicate-treenode-278873\\\\\\\\x1a\\\"+150\",\r\n            \"kwds\": \"{}\"\r\n          }\r\n        },\r\n        {\r\n          \"filename\": \"/usr/lib/python3.8/multiprocessing/managers.py\",\r\n          \"lineno\": 850,\r\n          \"name\": \"_callmethod\",\r\n          \"line\": \"\",\r\n          \"locals\": {\r\n            \"self\": \"<AutoProxy[Queue] object, typeid 'Queue' at 0x7f78dab2b070>\",\r\n            \"methodname\": \"put\",\r\n            \"args\": \"\\\"((b'\\\\\\\\n$bc8c0612-11a9-4a58-ab6b-43d37f064f56\\\\\\\\x12\\\\\\\\x19replicate-treenode-278873\\\\\\\\x1a\\\"+150\",\r\n            \"kwds\": \"{}\",\r\n            \"conn\": \"<multiprocessing.connection.Connection object at 0x7f78e1c62460>\",\r\n            \"kind\": \"#ERROR\",\r\n            \"result\": \"Full()\"\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Minimal Reproduction\r\n\r\nUnfortunately, I have no reproduction, but I do have some analysis to share:\r\n\r\nI think the log message `Failed during multiprocess queue poll for heartbeat` is potentially misleading, as [raised here](https://github.com/temporalio/sdk-python/blob/1.5.0/temporalio/worker/_activity.py#L908-L909) on the top `Exception` type. Digging into the first stack trace above, the exception is a `TimeoutError` and the point of origin appears to be here: https://github.com/temporalio/sdk-python/blob/1.5.0/temporalio/worker/_activity.py#L644 - the body of that expression ends with `Future.result(10)`, which can raise `TimeoutError`.  So the `TimeoutError` arises not from queue polling, but from executing a coroutine on an event loop. As to why the `Future.result(10)` call times out, I can't understand why.\r\n\r\n### Environment/Versions\r\n\r\n<!-- Please complete the following information where relevant. -->\r\n\r\n- OS and processor: `5.4.0-42-generic #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux`\r\n- Temporal Version: 1.22.1, sdk-python 1.5.0\r\n- Using the binary distribution, operated natively, i.e., non-containerized.\r\n\r\n","closedAt":null,"comments":[{"id":"IC_kwDOGusT1c51FxWX","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"Hrmm, so in a well-running multiprocess setup, a heartbeat should be immediate and barely queued. Because the primary process should be processing the cross-process heartbeat almost immediately.\r\n\r\n> So the TimeoutError arises not from queue polling, but from executing a coroutine on an event loop.\r\n\r\nWell, it's more like from the lack of queue polling. I believe if the heartbeat queue is not being polled/processed fast enough, this is where this call gets hung.\r\n\r\n> As to why the Future.result(10) call times out, I can't understand why.\r\n\r\nThe only reason why I can think this would fail is if your primary process is not processing asyncio quickly enough. This often happens if you're doing a blocking call in some `async def` somewhere but I suppose could be caused by sending too many heartbeats too fast (or a combination of both things).\r\n\r\nI am not sure there is much that can be done here. I believe the worker is not processing heartbeats faster than you are sending them and at some point it's a timeout. We can't queue heartbeats indefinitely. Open to suggestions though. For whatever suggestion, we likely need a replication to even confirm it solves your issue.\r\n\r\nAlternatively, you may consider standard threaded (or async) activities that you, inside the activity, invoke multiprocessing.","createdAt":"2024-02-26T15:39:57Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/476#issuecomment-1964447127","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c51i4Qg","author":{"login":"wilsoniya"},"authorAssociation":"NONE","body":"Thanks for replying and generally being so responsive!\r\n\r\n> The only reason why I can think this would fail is if your primary process is not processing asyncio quickly enough. This often happens if you're doing a blocking call in some async def somewhere but I suppose could be caused by sending too many heartbeats too fast (or a combination of both things).\r\n\r\nI suspect you're right. I'm using Pydantic 2.x, and [using the custom data converter recipe you provide](https://github.com/temporalio/samples-python/tree/main/pydantic_converter). The activity with frequent heartbeats is sending the details parameter, [thus invoking the data converter](https://github.com/temporalio/sdk-python/blob/1.5.0/temporalio/worker/_activity.py#L250-L252) (if I understand correctly). Pydantic 2.x appears to have some serious unresolved perf issues, so I'm wondering if the primary process is bogged down doing Pydantic serde of heartbeat details. I'm going to experiment with omitting heartbeat details and see if that resolves the issue.\r\n\r\n> I am not sure there is much that can be done here. I believe the worker is not processing heartbeats faster than you are sending them and at some point it's a timeout. We can't queue heartbeats indefinitely. Open to suggestions though. For whatever suggestion, we likely need a replication to even confirm it solves your issue.\r\n\r\nIs this `return` call potentially problematic? \r\n\r\nhttps://github.com/temporalio/sdk-python/blob/4037dd4ce57461124612a9536f534abc05a7eb79/temporalio/worker/_activity.py#L908-L910\r\n\r\nIt seems like returning from `_MultiprocessingSharedStateManager._heartbeat_processor()` when we catch a `TimeoutError` when evaluating the heartbeat future here \r\n\r\nhttps://github.com/temporalio/sdk-python/blob/4037dd4ce57461124612a9536f534abc05a7eb79/temporalio/worker/_activity.py#L644-L646\r\n\r\nwill result in processing no further heartbeats, and thus the heartbeat queue filling up and blocking further activity liveness. I have an incomplete understanding of this code, but I suspect that the error I'm describing is unrecoverable. ","createdAt":"2024-02-29T22:27:42Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/476#issuecomment-1972077600","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c51nNIW","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"> Is this return call potentially problematic?\r\n\r\nI fear it will encourage lossy heartbeats as opposed to just stopping the processor. If we had an easy way to surface the exception we would, but in the meantime I am worried about just accepting some heartbeats and not others\r\n\r\n> will result in processing no further heartbeats, and thus the heartbeat queue filling up and blocking further activity liveness\r\n\r\nYes, this is a dangerous state to get in. I am not exactly sure what we should do when the shared heartbeat processor fails. It should never fail. I do want to avoid adding some kind of advanced code (e.g. draining just this specific activity's heartbeat queue somehow) to this never-should-happen failure state. Really it shouldn't even be a queue, it should be almost immediate where the queue never has a size more than one.\r\n\r\nOpen to suggestions about how to better surface this. Should I try to find a way to throw an exception and shut the whole worker down?","createdAt":"2024-03-01T13:31:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/476#issuecomment-1973211670","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c54peKw","author":{"login":"wilsoniya"},"authorAssociation":"NONE","body":"Sorry for the delayed response.\r\n\r\n> Should I try to find a way to throw an exception and shut the whole worker down?\r\n\r\nThat would be ideal for my use case. But more generally, I think encountering situations one has consciously conceived of as \"impossible\" merits a hard crash. And given a choice between a process crashing or technically persisting in a dysfunctional state, I think the former is more conducive to detecting and recovering from a problem. \r\n\r\nThanks!","createdAt":"2024-03-27T23:17:14Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/476#issuecomment-2024137392","viewerDidAuthor":false},{"id":"IC_kwDOGusT1c54tHwd","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"With Temporal, we often favor partially disfunctional workers that still work for other things (e.g. things that aren't heartbeating and workflows) over fail-fast. But maybe we can make it an option.\r\n\r\nBut I am now wondering if we should deprecate multiprocess activities and instead tell users they can use normal Python subprocessing in their activities if they _must_ have separate processes. It doesn't have much value in the SDK anymore than any other form of remote activity would. The details of dealing with out-of-process activities is probably best left to the (advanced) user.","createdAt":"2024-03-28T12:41:02Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-python/issues/476#issuecomment-2025094173","viewerDidAuthor":false}],"createdAt":"2024-02-22T20:33:09Z","labels":[{"id":"LA_kwDOGusT1c7gQgHK","name":"bug","description":"Something isn't working","color":"d73a4a"}],"milestone":null,"number":476,"reactionGroups":[],"state":"OPEN","title":"[Bug] Failed during multiprocess queue poll for heartbeat","updatedAt":"2024-03-28T12:42:23Z","url":"https://github.com/temporalio/sdk-python/issues/476"}
