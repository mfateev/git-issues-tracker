# temporalio/sdk-python - Complete Issue Dump

**Generated:** 2026-01-09
**Total Issues:** 589
**Total Upvotes:** 119
**Total Comments:** 982

## Table of Contents

- [Summary](#summary)
- [Top Labels](#top-labels)
- [Issue Index](#issue-index)
- [All Issues](#all-issues)

## Summary

| Metric | Value |
|--------|-------|
| Open Issues | 589 |
| Issues with Upvotes | 62 (11%) |
| Total Upvotes | 119 |
| Total Comments | 982 |

## Top Labels

| Label | Count |
|-------|-------|
| enhancement | 313 |
| bug | 220 |
| Mend: dependency security vulnerability | 12 |
| question | 2 |

## Issue Index

| # | ðŸ‘ | ðŸ’¬ | Title |
|---|-----|-----|-------|
| [#621](#621) | 20 | 4 | [Feature Request] Create official Pydantic contrib module |
| [#58](#58) | 0 | 27 | Deserialize traceback from stack trace string in Temporal failures if able |
| [#441](#441) | 4 | 9 | [Bug] GeneratorExit possibly causing issues on context detach in OTel finally |
| [#487](#487) | 6 | 3 | [Feature Request] Allow workflow logger to opt-out of sandboxing |
| [#638](#638) | 1 | 12 | Importing 3rd party package `ruamel.yaml` anywhere causes failure to validate workflow |
| [#631](#631) | 0 | 13 | [Bug] Worker hangs after polling workflow task queue |
| [#412](#412) | 0 | 13 | [Bug] protoc-wheel-0 dev dependency causing issues when building on musl-based distros like Alpine |
| [#207](#207) | 0 | 13 | [Bug] SandboxWorkflowRunner doesn't use correct Pydantic field types in some cases |
| [#792](#792) | 0 | 12 | [Bug] Wait_condition timeout makes tests impossible |
| [#688](#688) | 0 | 12 | [Bug] Workflow sandbox issues with Protobuf |
| [#190](#190) | 5 | 2 | [Feature Request] Upsert memo support |
| [#160](#160) | 0 | 12 | [Bug] OpenTelemetry interceptors report errors |
| [#1048](#1048) | 0 | 11 | [Feature Request] Support raising cancellation in sync multiprocessed activities |
| [#686](#686) | 0 | 11 | [Feature Request] Timeout in Workflow definition not in execution |
| [#238](#238) | 3 | 5 | [Bug] Can't debugging workflow using vscode |
| [#893](#893) | 4 | 2 | [Bug] Pyright (strict mode) fails on `start`/`execute` activity/workflow calls |
| [#733](#733) | 0 | 10 | [Bug] Unable to run workflows with OpenTelemetry and ddtrace |
| [#700](#700) | 0 | 10 | [Bug] No clean way to cancel an activity and wait until it's cancelled |
| [#300](#300) | 0 | 10 | [Bug] Investigate PyGILState_Release issue for client |
| [#274](#274) | 4 | 2 | mutate workflow instance attributes from within activities that it executes |
| [#59](#59) | 0 | 10 | Investigate issues with gevent.monkey.patch_all() |
| [#54](#54) | 0 | 10 | temporal-sdk-core-api-0.1.0: 1 vulnerabilities (highest severity is: 5.5) - autoclosed |
| [#747](#747) | 0 | 9 | How to Get the Worker ID Executing the Current Activity? |
| [#639](#639) | 1 | 7 | [Bug] Operation was canceled when start_workflow |
| [#306](#306) | 0 | 9 | No module named 'temporalio.bridge.temporal_sdk_bridge' |
| [#276](#276) | 0 | 9 | [Bug] logging adapter does not include extra attributes? |
| [#894](#894) | 0 | 8 | [Feature Request] Create anaconda package |
| [#815](#815) | 1 | 6 | [Feature Request] Fail workflow upon pydantic.ValidationError when using the pydantic_data_converter |
| [#775](#775) | 0 | 8 | [Bug] sdk metrics do not include tags added with "telemetry.global_tags" |
| [#758](#758) | 1 | 6 | [Feature Request] Suggest Providing activities extraction utilities to get them from a class and a module |
| [#679](#679) | 0 | 8 | [Bug/Question] non_retryable_error_types seems not working |
| [#672](#672) | 1 | 6 | [Feature Request] Drop 3.8 support, confirm 3.13 support |
| [#232](#232) | 0 | 8 | [Bug] Python 3.11 TypeError: Passing coroutines is forbidden, use tasks explicitly. |
| [#78](#78) | 0 | 8 | Improve dataclass type hinting |
| [#1224](#1224) | 2 | 3 | Cannot reliably catch workflow cancellation with try/except pattern |
| [#1107](#1107) | 3 | 1 | [Feature Request] Update dependency constraint for protobuf to protobuf>=3.20 |
| [#1104](#1104) | 0 | 7 | Setting debug_mode in a Worker still doesn't allow the user of breakpoints |
| [#1032](#1032) | 0 | 7 | [Bug] value_to_type fails with custom string types as dictionary keys |
| [#807](#807) | 2 | 3 | [Feature Request] Loss of stack trace information |
| [#691](#691) | 3 | 1 | [Feature Request] Make option for "all passthrough" (i.e. disable import isolation part of sandbox) |
| [#681](#681) | 1 | 5 | [Feature Request] Support for test server without internet access |
| [#676](#676) | 0 | 7 | [Feature Request] Support `(str, Enum)` like `StrEnum` |
| [#603](#603) | 3 | 1 | [Feature Request] Investigate/document how to use PyCharm debugger with workflows |
| [#572](#572) | 0 | 7 | [Bug] Upserting datetime search attribute fails |
| [#512](#512) | 0 | 7 | [Feature Request] Reload client certificate  |
| [#309](#309) | 0 | 7 | [Bug] Unable to install temporalio within [image: sonarsource/sonar-scanner-cli:4.6] container |
| [#194](#194) | 0 | 7 | [Feature Request] Support patch search attribute |
| [#143](#143) | 1 | 5 | Pydantic models with nested non-JSON-friendly field types cannot be serialized |
| [#68](#68) | 1 | 5 | [Feature Request] Inject dependencies into activities |
| [#62](#62) | 0 | 7 | [Bug] retry_policy not availably in activity info |
| [#1154](#1154) | 2 | 2 | [Feature Request] Support InterpreterPoolExecutor from py3.14 |
| [#837](#837) | 0 | 6 | [Feature Request] Make Temporal logger adapter accomodate to OpenTelemetry |
| [#646](#646) | 0 | 6 | [Bug] _convert_payloads fails with One or more mappers failed to initialize |
| [#627](#627) | 3 | 0 | [Feature Request] Support serialization of `datetime` values in default JSON converter |
| [#429](#429) | 0 | 6 | [Bug] asyncio.wait is non-deterministic when used with coroutines instead of tasks |
| [#393](#393) | 1 | 4 | [Feature Request] Support ExceptionGroup traceback |
| [#235](#235) | 0 | 6 | [Feature Request] Raise exception for payloads that violate gRPC message max size |
| [#181](#181) | 1 | 4 | [Feature Request] Review options for allowing protobuf 3.x to work with this library |
| [#134](#134) | 0 | 6 | [Feature Request] Extend readme contents about exception from activity |
| [#1215](#1215) | 0 | 5 | [Bug] must only be set once: TaskLocals { event_loop: Py(0x1165b6120), context: Py(0x116ad1d40) } |
| [#1136](#1136) | 0 | 5 | [Bug] Langfuse Tracing Not Working with Temporal OpenAI Agents Plugin |
| [#1009](#1009) | 0 | 5 | [Feature Request] OpenAI Agents Streaming support |
| [#875](#875) | 1 | 3 | [Feature Request] Expose SDK metric for `worker._count_not_evict_count` |
| [#855](#855) | 0 | 5 | [Bug] Helper execute activity doesn't work as expected |
| [#823](#823) | 0 | 5 | [Feature Request] Disabling Deadlock detection in data converters |
| [#796](#796) | 2 | 1 | [Feature Request] Serialization context for codecs and converters |
| [#777](#777) | 0 | 5 | [Feature Request] Allow custom metric buckets |
| [#641](#641) | 1 | 3 | [Bug] Sending a query that is not registered in the workflow results in an "Encoded failure" when encode_common_attributes=True |
| [#588](#588) | 0 | 5 | [Feature Request] Default Workflow\Activity Params In Declaration |
| [#586](#586) | 0 | 5 | [Bug] workflow.start_child_workflow() behaving differently than workflow.execute_child_workflow() in tests |
| [#568](#568) | 0 | 5 | [Bug] Fix flaky tests |
| [#554](#554) | 1 | 3 | Document workflow-friendly concurrency control |
| [#540](#540) | 0 | 5 | [Bug] Exception in converter encoding doesn't propagate as expected |
| [#476](#476) | 0 | 5 | [Bug] Failed during multiprocess queue poll for heartbeat |
| [#459](#459) | 0 | 5 | [Bug] Worker hangs after polling error |
| [#426](#426) | 1 | 3 | [Feature Request] Implement `is_running` for `_WorkflowInstanceImpl` |
| [#321](#321) | 2 | 1 | [Feature Request] Temporal exceptions from converters and codecs should fail the workflow not task |
| [#315](#315) | 2 | 1 | [Feature Request] Support constructing JSONProtoPayloadConverter with google.protobuf.json_format.Parse options |
| [#311](#311) | 0 | 5 | [Feature Request] Log Forwarding Support for python-SDK |
| [#113](#113) | 0 | 5 | [Feature Request] UUID support as an arg |
| [#85](#85) | 0 | 5 | [Feature Request] Ability to install without having Rust installed  |
| [#61](#61) | 0 | 5 | temporal-sdk-core-0.1.0: 1 vulnerabilities (highest severity is: 7.5) - autoclosed |
| [#1267](#1267) | 2 | 0 | [Bug] Logs not emitted during workflow queries |
| [#1250](#1250) | 1 | 2 | RuntimeError when starting Temporal test server in Python SDK |
| [#1194](#1194) | 0 | 4 | [Feature Request] Set Temporal-Namespace header on every namespace-specific gRPC request |
| [#1134](#1134) | 0 | 4 | [Bug] Temporal Workflows Require Model Name to be String - LitellmModel Not Supported |
| [#1113](#1113) | 1 | 2 | [Bug] Client connect with TLS does not work anymore |
| [#1050](#1050) | 1 | 2 | [Feature Request] Configurable Deadlock Detection Timeout in Python SDK |
| [#1020](#1020) | 1 | 2 | [Bug] Using TaskGroup for child workflow execution does not correctly detect failure |
| [#928](#928) | 2 | 0 | [Feature Request] Research and make SDK free-threading capable/compatible |
| [#927](#927) | 0 | 4 | [Bug]  Batch operation feature in the Temporal Python SDK doesn't work |
| [#795](#795) | 1 | 2 | [Bug] Pyright typing issue |
| [#783](#783) | 0 | 4 | [Bug] Sticky execution after Worker shutdown causes "Workflow Task Timed Out" |
| [#716](#716) | 0 | 4 | [Bug] Test server for ARM64 not available (404) |
| [#697](#697) | 0 | 4 | [Feature Request] Handle edge case of recursive exceptions in failure converter |
| [#666](#666) | 0 | 4 | [Feature Request] Worker instance lifespan |
| [#647](#647) | 0 | 4 | [Feature Request] OpenTelemetry Metrics OLTP/HTTP support |
| [#622](#622) | 1 | 2 | [Bug] unable to install temporal on python 3.11-alpine3.16 |
| [#578](#578) | 0 | 4 | activity returning dict object forcefully |
| [#448](#448) | 1 | 2 | [Bug] Cut a new release please |
| [#445](#445) | 0 | 4 | [Bug] delete_workflow_execution not properly available on workflow_service |
| [#383](#383) | 2 | 0 | [Feature Request] Provide last result and last failure to workflows |
| [#333](#333) | 0 | 4 | [Bug] BaseException exceptions are not caught when raised in an activity |
| [#322](#322) | 0 | 4 | [Bug] Python generics for function type hints |
| [#317](#317) | 0 | 4 | [Bug] await call in async query function would cause the function to return immediately and cause an error on client side |
| [#272](#272) | 0 | 4 | Temporal CLI based ephemeral server |
| [#262](#262) | 0 | 4 | [Feature Request] Reset Workflow |
| [#220](#220) | 0 | 4 | [Bug] Conflict using temporal-sdk with loguru |
| [#213](#213) | 2 | 0 | [Feature Request] Confirm vscode debuggability within sandbox |
| [#153](#153) | 0 | 4 | [Feature Request] Rework search attributes and add list endpoint to client |
| [#128](#128) | 0 | 4 | Add support for Pydantic's secret types |
| [#126](#126) | 0 | 4 | The SDK is failing to serialize objects from Pydantic classes |
| [#64](#64) | 0 | 4 | [Feature Request] Get workflow handle/send signals from non-async code |
| [#52](#52) | 0 | 4 | [Feature Request] Support non-function callables as activities |
| [#1207](#1207) | 0 | 3 | [Feature Request] Add a clean way to extract failure cause from client side |
| [#1186](#1186) | 0 | 3 | [Bug] Incomplete fix for #1091 - conversation_id still required in _TemporalModelStub.get_response() |
| [#1184](#1184) | 0 | 3 | Feature request: Call activities as decorated functions inside workflows (sugar over execute_activity) |
| [#1177](#1177) | 0 | 3 | [Bug] OpenAI agent sdk non retryable error on liteLLM 503 |
| [#1144](#1144) | 0 | 3 | [Bug] Revisit activity/nexus worker error handling on shutdown |
| [#1120](#1120) | 0 | 3 | [Question] LIFO logic in task queue or workflow Type |
| [#1049](#1049) | 0 | 3 | [Feature Request] Support access to metric meter in multi-processed workers |
| [#1047](#1047) | 0 | 3 | [Feature Request] Set OpenTelemetry span status to failed for heartbeat timeout cancellations |
| [#1041](#1041) | 0 | 3 | [Feature Request] Reclassify Benign Application errors in OpenTelemetry |
| [#891](#891) | 0 | 3 | [Feature Request] Support executing Workflow directly |
| [#878](#878) | 1 | 1 | [Bug] workflow.in_workflow() expects to be called in a thread with a running event loop |
| [#863](#863) | 0 | 3 | [Feature Request] Confirm support for Python 3.14 |
| [#782](#782) | 1 | 1 | [Bug] cancelled timer callback causes asyncio.exceptions.InvalidStateError |
| [#663](#663) | 0 | 3 | [Feature Request] Activity specific worker tuning |
| [#661](#661) | 0 | 3 | [Bug] Child workflow execution stuck when running in testing.WorkflowEnvironment and workflow update is involved. |
| [#659](#659) | 0 | 3 | [Feature Request] Some kind of registry for workflows and activities to get classes/function with their names |
| [#587](#587) | 0 | 3 | [Feature Request] add "id_conflict_policy" arg for "execute_child_workflow" |
| [#520](#520) | 0 | 3 | [Feature Request] Update rustls and mio in Rust dependencies |
| [#454](#454) | 0 | 3 | [Feature Request] Schedule creation should create tracing span by default |
| [#449](#449) | 0 | 3 | [Bug] Scheduled workflow did not receive OpenTelemetry tracing |
| [#444](#444) | 0 | 3 | [Bug] Fix lint warnings for Client.execute_workflow |
| [#432](#432) | 0 | 3 | [Bug] Exception raise from workflow causes indefinite retry |
| [#420](#420) | 0 | 3 | [Feature Request] Make start_update/execute_update work with vscode/pyright and add pyright to CI |
| [#415](#415) | 1 | 1 | [Feature Request] Add `temporalio.workflow.in_workflow()` call |
| [#399](#399) | 0 | 3 | [Bug] Older Python versions cannot auto-import activity return class in sandbox |
| [#394](#394) | 0 | 3 | [Bug] Compilation fails for ARCH `s390x` and `ppc64el` |
| [#385](#385) | 0 | 3 | [Bug] Client stuck connecting or starting a workflow |
| [#367](#367) | 0 | 3 | [Bug] Temporal graceful shutdown does not prevent activity failure |
| [#362](#362) | 0 | 3 | [Feature Request] Make sure OTel baggage propagates properly throughout activities |
| [#338](#338) | 0 | 3 | [Bug] Wrong types when using "workflow.unsafe.imports_passed_through" |
| [#313](#313) | 0 | 3 | [Bug] Retry Policy on start_workflow |
| [#286](#286) | 0 | 3 | [Bug] Worker started state is not reset after shutdown |
| [#211](#211) | 0 | 3 | [Feature Request] Integration in django application |
| [#144](#144) | 0 | 3 | Search attribute Datetime need to support full ISO 8601 (and maybe more) |
| [#130](#130) | 0 | 3 | [Feature Request] client.OutboundInterceptor should support get_workflow_execution_history() |
| [#116](#116) | 0 | 3 | [Bug] Search attributes require specific non-documented format |
| [#92](#92) | 0 | 3 | [Feature Request] Lazy connectivity for client and healthCheck |
| [#82](#82) | 0 | 3 | Replayer |
| [#1252](#1252) | 0 | 2 | [Feature Request] add client support for sync activities run by `ThreadPoolExecutor` |
| [#1203](#1203) | 0 | 2 | [Bug] `retry_policy` not set in `ativity.info()` despite being defined in the `workflow.execute_activity()` call |
| [#1196](#1196) | 1 | 0 | [Feature Request] Enable Worker Heartbeating |
| [#1190](#1190) | 0 | 2 | OpenTelemetry spans missing workflow input/output attributes |
| [#1168](#1168) | 0 | 2 | [Bug] When the task volume is greater than the number of concurrent activities, the subsequent activity execution time does not reach the start_to_close_timeout and is directly canceled |
| [#1166](#1166) | 0 | 2 | [Feature Request] Suggest improving type hint for activity in execute_activity to work with acitvity methods |
| [#1146](#1146) | 1 | 0 | [Feature Request] Allow providing additional data to MCP server construction |
| [#1129](#1129) | 0 | 2 | [Bug] Child workflow search attributes aren't decrypted in 1.18.0 |
| [#1108](#1108) | 0 | 2 | [Feature Request] Update dependency constraint openai-agents to  v0.3.0 |
| [#1106](#1106) | 0 | 2 | [Bug] TLS error from `poll_workflow_task_queue` |
| [#1098](#1098) | 0 | 2 | [Bug] Error running hello world workflow |
| [#1091](#1091) | 0 | 2 | [Bug] `_TemporalModelStub.get_response` incompatible with latest `openai-agents-python` (`conversation_id` kwarg) |
| [#1089](#1089) | 1 | 0 | [Feature Request] Provide supported Session implementations for OpenAI Agents |
| [#1024](#1024) | 0 | 2 | [Feature Request] Display Agent name in the timeline |
| [#1012](#1012) | 0 | 2 | [Bug]  The second time the process exits, the activity will not be re-executed immediately |
| [#980](#980) | 0 | 2 | [Bug] Inaccessible type in NamespaceSpec |
| [#979](#979) | 0 | 2 | [Bug] [temporal python sdk release] invoke_model_activity is missing in latest release |
| [#971](#971) | 0 | 2 | [Feature Request] Configure logging without info on message |
| [#848](#848) | 0 | 2 | [Bug] Updates conflict with replay |
| [#824](#824) | 0 | 2 | [Release] Cut a new release to address CVE-2025-29787 |
| [#791](#791) | 0 | 2 | [Bug] We assume an element is present in an error details list |
| [#790](#790) | 1 | 0 | [Feature Request] Helpers to prevent accidental non-passed-through imports |
| [#786](#786) | 0 | 2 | [Feature Request] Initial Nexus implementation |
| [#780](#780) | 0 | 2 | [Bug] Not able to debug code [python] |
| [#743](#743) | 0 | 2 | [Bug] ResourceBasedSlotOptions fields not consistent with the underlying Rust implementation |
| [#732](#732) | 0 | 2 | [Feature Request] Please add workflow.execute_activity which has a typed resonse |
| [#726](#726) | 0 | 2 | [Feature Request] Pydantic V2 support in converter |
| [#677](#677) | 0 | 2 | Question: Logging of uncaught exceptions when running under a threadpool executor |
| [#673](#673) | 0 | 2 | [Bug] NondeterminismError when replaying overlapping signals and updates |
| [#671](#671) | 0 | 2 | [Feature Request] Fix tonic Vulnerability by Updating Rust Package to Version 0.12.3 |
| [#643](#643) | 1 | 0 | UpdateWithStart SDK API |
| [#633](#633) | 0 | 2 | Cannot execute workflow update from a temporal activity |
| [#618](#618) | 0 | 2 | [Feature Request] Improve wait_condition semantics |
| [#601](#601) | 0 | 2 | [Feature Request] Reset workflow feature |
| [#592](#592) | 0 | 2 | [Bug] Activity logger context not populated when `activity_info_on_message` False |
| [#576](#576) | 0 | 2 | [Feature Request] better workflow testing in multi-worker scenarios |
| [#564](#564) | 1 | 0 | [Bug] Converting payloads and other exception-possible actions done after command creation |
| [#531](#531) | 1 | 0 | [Feature Request] Include stack trace on deadlock detection exception |
| [#523](#523) | 0 | 2 | [Bug] Make sure `is_replaying` is `True` upon eviction, and only throw certain exception types out on eviction |
| [#497](#497) | 0 | 2 | [Feature Request] Check Protobuf 5.x Support |
| [#489](#489) | 1 | 0 | [Feature Request] Make log "extra" have dict instead of info |
| [#482](#482) | 1 | 0 | [Feature Request] API key client option |
| [#463](#463) | 0 | 2 | [Feature Request] Support disabling host verification |
| [#462](#462) | 0 | 2 | [Bug] Handle possibility of update-before-start |
| [#452](#452) | 0 | 2 | [Feature Request] Decode json/plain as json/protobuf if type_hint is a subclass of protobuf.Message |
| [#450](#450) | 0 | 2 | pathlib RestrictedWorkflowAccessError |
| [#446](#446) | 1 | 0 | [Feature Request] Configurable workflow failure exception types |
| [#401](#401) | 0 | 2 | [Feature Request] Trigger Activity Retry when workflow got signal |
| [#390](#390) | 0 | 2 | PoC - Static analyzer for Python |
| [#371](#371) | 0 | 2 | [Bug] mTLS doesn't support non-EC private keys |
| [#368](#368) | 1 | 0 | [Bug] Activity completions may be dropped on worker shutdown |
| [#319](#319) | 0 | 2 | [Bug] Continue as new doesn't work without specifying `task_queue` and `workflow` |
| [#312](#312) | 0 | 2 | [Feature Request] Validate type of timedelta() parameters in validate methods? |
| [#299](#299) | 0 | 2 | [Feature Request] tokio update for vuln |
| [#298](#298) | 0 | 2 | [Feature Request] bzip2 update for vuln |
| [#297](#297) | 0 | 2 | [Feature Request] remove_dir_all update for vuln |
| [#296](#296) | 0 | 2 | [Feature Request] bumpalo update for vuln |
| [#288](#288) | 0 | 2 | [Bug] Enum HistoryEventFilterType has no value defined for name 'ValueType' |
| [#277](#277) | 0 | 2 | types-protobuf dependency is still on 3.x, while package supports protobuf 4.x |
| [#275](#275) | 0 | 2 | [Feature Request] Access to invoking workflow in activity method |
| [#264](#264) | 0 | 2 | [Feature Request] Support JSON Type Converters |
| [#259](#259) | 0 | 2 | [Feature Request] Workflow Versioning Documentation |
| [#245](#245) | 0 | 2 | [Bug] BrokenProcessPool completely disables ProcessPoolExecutor  |
| [#242](#242) | 1 | 0 | [Feature Request] Support dynamic workflows, activities, signals, and queries |
| [#223](#223) | 0 | 2 | [Bug] Deprecation Warning During Install |
| [#216](#216) | 0 | 2 | [Bug] Importing gevent causes sandbox errors |
| [#200](#200) | 0 | 2 | [Bug] Identity equality for int enum fails inside sandbox |
| [#195](#195) | 0 | 2 | [Feature Request] Iterable Intermediate Results |
| [#162](#162) | 1 | 0 | [Bug] Wrap query RPC error with friendlier error |
| [#157](#157) | 0 | 2 | [Feature Request] Integration test for Otel gRPC metrics collector |
| [#150](#150) | 0 | 2 | [Bug] Potential issue with parallel activities |
| [#146](#146) | 0 | 2 | Clarify story around sync activity cancellation |
| [#141](#141) | 1 | 0 | [Feature Request] Support Otel temporality config |
| [#129](#129) | 1 | 0 | [Bug] conditions are triggered when processing queries and patch notifications |
| [#123](#123) | 0 | 2 | Update Python proto library to latest |
| [#109](#109) | 0 | 2 | Support timedelta | float | int for all user-facing duration input |
| [#100](#100) | 0 | 2 | Linux aarch64 wheel |
| [#87](#87) | 1 | 0 | High level API for schedules |
| [#36](#36) | 0 | 2 | Publish to pip |
| [#23](#23) | 0 | 2 | Stress tests |
| [#9](#9) | 1 | 0 | Support macOS arm64 |
| [#1238](#1238) | 0 | 1 | [Bug] `create_schedule` doesn't use static user metadata |
| [#1220](#1220) | 0 | 1 | [Feature Request] Add typing for signal for PyCharm |
| [#1212](#1212) | 0 | 1 | Add pascalcase to WorkflowExecutionStatus |
| [#1202](#1202) | 0 | 1 | [Feature Request] Advanced, unsafe ability to close the client |
| [#1180](#1180) | 0 | 1 | [Bug] OpenAI Agents SDK serialization fails with complex type annotations |
| [#1162](#1162) | 0 | 1 | [Feature Request] [OpenAI] Allow activity_as_tool to execute as local_activity |
| [#1130](#1130) | 0 | 1 | [Feature Request] Support for custom tracing providers |
| [#1112](#1112) | 0 | 1 | [Feature Request] Add WorkflowIdConflictPolicy for child workflows |
| [#1109](#1109) | 0 | 1 | [Bug] calling workflow.random().randint before workflow.uuid4 breaks replay with [TMPRL1100] Nondeterminism error |
| [#1103](#1103) | 0 | 1 | [Bug] There are no Workers polling the X Task Queue. |
| [#1087](#1087) | 0 | 1 | [Bug] Unable to pickle proto classes, module name is 'temporal' instead of 'temporalio' |
| [#1081](#1081) | 0 | 1 | [Feature Request] Expose `strict_json_schema` argument in `activity(nexus)_as_tool` in OpenAI SDK integration |
| [#1054](#1054) | 0 | 1 | [Bug] OTEL metadata missing in `__init__` method of a workflow |
| [#1053](#1053) | 0 | 1 | [Bug] OTEL metadata missing for execute_update_with_start_workflow  |
| [#1033](#1033) | 0 | 1 | [Bug] heartbeat_timeout=_proto_to_non_zero_timedelta(start.WAIT_TIMEOUT) |
| [#1028](#1028) | 0 | 1 | [Documentation Request] GetLastCompletionResult equivalent in Python SDK |
| [#1018](#1018) | 0 | 1 | [Bug] Replayer not working when workflow_runner is set to UnsandboxedWorkflowRunner(). |
| [#1013](#1013) | 0 | 1 | [Bug] Missing overload for start causing Pyright type check issue |
| [#1002](#1002) | 0 | 1 | Over 1000 commits in workflow non active state |
| [#989](#989) | 0 | 1 | [Feature Request] Support for specifying WorkflowIDConflictPolicy in child workflows |
| [#961](#961) | 0 | 1 | [Feature Request] Add optional headers parameter to WorkflowHandle.signal() and Workflow.execute_update()/Workflow.start_update() |
| [#951](#951) | 0 | 1 | [Bug] OpenAI Agents - trace spans sometimes fail to complete |
| [#925](#925) | 0 | 1 | [Feature Request] Opt-in support for applying codec to headers |
| [#890](#890) | 0 | 1 | [Bug] pydantic_data_converter change the value of SecretStr to "*" |
| [#873](#873) | 0 | 1 | [Bug] Workflow Task Timed out |
| [#856](#856) | 0 | 1 | Integration of Temporal Python SDK Metrics with FastAPI and Prometheus FastAPI Instrumentator |
| [#852](#852) | 0 | 1 | [Bug] Serializing Literal fails |
| [#851](#851) | 0 | 1 | [Bug] Test server in time-skipping mode does return userMetadata.summary |
| [#850](#850) | 0 | 1 | [Bug] _RestrictedProxy does not invoke __format__ correctly when called on class |
| [#839](#839) | 0 | 1 | [Feature Request] Warn when the server-provided task start time is fairly different than time SDK begins processing the task |
| [#816](#816) | 0 | 1 | [Feature Request] Upgrade PyO3 to latest and use pyo3-async-runtimes instead of pyo3-asyncio |
| [#814](#814) | 0 | 1 | [Bug] Python client not able to connect to self-hosted Temporal server via proxy using authorization header |
| [#803](#803) | 0 | 1 | Run different worker types in separate threads |
| [#779](#779) | 0 | 1 | [Feature Request] Adding type checking on workflow_execute activity when a variable number of positional args exist in the callable activity |
| [#778](#778) | 0 | 1 | [Bug] strange workflow task timeout |
| [#776](#776) | 0 | 1 | [Feature Request] Implement run_in_executor on _WorkflowInstanceImpl |
| [#765](#765) | 0 | 1 | [Feature Request] Allow failures to have the traditional pre-3.11 traceback with less information |
| [#731](#731) | 0 | 1 | [Feature Request] Special behavior for Temporal built-in prefixes |
| [#721](#721) | 0 | 1 | [Feature Request] Provide client from activity context |
| [#719](#719) | 0 | 1 | [Feature Request] Provide "workflow local" |
| [#699](#699) | 0 | 1 | [Bug] `workflow.wait` broken with `asyncio.FIRST_COMPLETED` and local activities |
| [#655](#655) | 0 | 1 | [Bug] KeyError: 'warnings' due to sandbox |
| [#654](#654) | 0 | 1 | MultiProcess executor not working as expected |
| [#652](#652) | 0 | 1 | [Bug] `ScheduleOverlapPolicy` has broken __eq__ |
| [#632](#632) | 0 | 1 | [Feature Request] OpenTelemetry export fails over TLS |
| [#629](#629) | 0 | 1 | [Bug] workflow.upsert_search_attributes() will bork SDK on empty inputs |
| [#625](#625) | 0 | 1 | [Feature Request] Fix pyright type-checking violations |
| [#624](#624) | 0 | 1 | [Bug] RPCStatusCode.ALREADY_EXISTS can be swallowed |
| [#620](#620) | 0 | 1 | [Bug] Workflow cancellation can be swallowed |
| [#597](#597) | 0 | 1 | [Bug] Cant build wheel temporalio when installing with pip |
| [#590](#590) | 0 | 1 | [Feature Request] Update schedule default values |
| [#583](#583) | 0 | 1 | Tweak error message on workflow cancellation |
| [#563](#563) | 0 | 1 | [Feature Request] Update test running to replay |
| [#561](#561) | 0 | 1 | [Bug] Workflow Task Timeout only supports upto 2 minutes |
| [#560](#560) | 0 | 1 | [Feature Request] Propagate changes made with updated Rust dependencies |
| [#529](#529) | 0 | 1 | Wrap GRPC::CANCELED and DEADLINE_EXCEEDED in an SDK Timeout exception for Update |
| [#518](#518) | 0 | 1 | [Feature Request] Investigate asyncio.as_completed and asyncio.wait usage of `set` as non-deterministic |
| [#504](#504) | 0 | 1 | Support for WorkflowIdConflictPolicy |
| [#494](#494) | 0 | 1 | [Bug] Commands sent during finally of a cache eviction may cross workflow contexts |
| [#493](#493) | 0 | 1 | [Feature Request] Support metric option for using seconds (and other recent core metric changes) |
| [#453](#453) | 0 | 1 | [Bug] Parent packages of a package are automatically imported via the Sandbox |
| [#451](#451) | 0 | 1 | upsert_search_attribute does not insert new attributes |
| [#447](#447) | 0 | 1 | [Bug] Wrong line number in workflow.logger |
| [#443](#443) | 0 | 1 | [Bug] Coroutine Raised StopIteration when triggering  workflow execution from CLI for workflows which call workflow.upsert_search_attributes |
| [#425](#425) | 0 | 1 | [Feature Request] Add usedforsecurity=False for build-ID MD5 use |
| [#407](#407) | 0 | 1 | [Feature Request] Add utility or at least sample to incorporate core logs into Python logging |
| [#397](#397) | 0 | 1 | [Bug] Client may not be reconnecting on server shutdown/restart |
| [#382](#382) | 0 | 1 | tonic-0.9.2.crate: 1 vulnerabilities (highest severity is: 7.5) - autoclosed |
| [#381](#381) | 0 | 1 | [Feature Request] Typed search attributes |
| [#360](#360) | 0 | 1 | [Feature Request] Allow use of type hints even if arg count mismatches in case of default params |
| [#358](#358) | 0 | 1 | [Feature Request] Default to suggesting threaded activities (and other async helper docs) |
| [#357](#357) | 0 | 1 | [Feature Request] TracingInterceptor span kinds |
| [#354](#354) | 0 | 1 | [Feature Request] Metric fowarding support and custom metric support |
| [#345](#345) | 0 | 1 | [Feature Request] Support in-workflow OpenTelemetry span creation |
| [#336](#336) | 0 | 1 | [Python] Workflow update support |
| [#331](#331) | 0 | 1 | [Bug] Local activity without schedule_to_close_timeout is failing immediately |
| [#327](#327) | 0 | 1 | Use stable pydoctor version instead of devel version |
| [#325](#325) | 0 | 1 | [Bug] GeneratorExit inadvertently caught, and also core timer mismatch  |
| [#316](#316) | 0 | 1 | [Bug] ScheduleState.limited_actions doc is invalid |
| [#302](#302) | 0 | 1 | [Feature Request] Fully qualify README logo image URL |
| [#295](#295) | 0 | 1 | [Feature Request] Support for dependency injection (pinject) |
| [#292](#292) | 0 | 1 | Support dynamic activity and workflow implementations |
| [#244](#244) | 0 | 1 | tokio-1.21.2.crate: 2 vulnerabilities (highest severity is: 9.8) - autoclosed |
| [#241](#241) | 0 | 1 | [Feature Request] Opt-in to having stack trace query use custom data converter |
| [#233](#233) | 0 | 1 | setuptools-65.3.0-py3-none-any.whl: 1 vulnerabilities (highest severity is: 5.9) - autoclosed |
| [#229](#229) | 0 | 1 | [Feature Request] Confirm PyThreadState_SetAsyncExc is ok on non-GIL threads |
| [#227](#227) | 0 | 1 | [Feature Request] Dynamic activity/workflow support |
| [#214](#214) | 0 | 1 | [Feature Request] RPC error improvements |
| [#210](#210) | 0 | 1 | [Bug] Sandboxed classes should be able to extend restricted classes |
| [#208](#208) | 0 | 1 | [Bug] Sandbox importer not respecting relative levels for pass through |
| [#201](#201) | 0 | 1 | [Bug] Cannot even import concurrent.futures.ProcessPoolExecutor in sandbox |
| [#182](#182) | 0 | 1 | [Feature Request] Unit test for protos as input/output of workflows in sandbox |
| [#173](#173) | 0 | 1 | V1 - is there any esitimation date for V1 ? |
| [#171](#171) | 0 | 1 | [Bug] Investigate "legacy query response along with other commands" |
| [#163](#163) | 0 | 1 | [Bug] Query return type hint not being validated against |
| [#161](#161) | 0 | 1 | github.com/temporalio/temporalite-v0.0.0-20220801155940-a4b633da82bc: 1 vulnerabilities (highest severity is: 7.5) - autoclosed |
| [#155](#155) | 0 | 1 | [Feature Request] Add metric temporality to telemetry config |
| [#154](#154) | 0 | 1 | [Feature Request] Support multi-workflow replayer |
| [#152](#152) | 0 | 1 | [Feature Request] Custom failure converters |
| [#151](#151) | 0 | 1 | [Bug] Telemetry not initialized for replayer |
| [#149](#149) | 0 | 1 | [Bug] "History is out of order" reported when running two workers for the same queue |
| [#147](#147) | 0 | 1 | Clarify story around using a client from a different asyncio loop than it was created in |
| [#145](#145) | 0 | 1 | [Bug] Cancelled workflow returns WorkflowExecutionStatus.FAILED status in workflow description |
| [#142](#142) | 0 | 1 | [Feature Request] Support FailureConverter concept |
| [#140](#140) | 0 | 1 | [Feature Request] Better title and favicon for python.temporal.io |
| [#139](#139) | 0 | 1 | Expose default sequence of payload converters for custom converter creators |
| [#133](#133) | 0 | 1 | temporal-sdk-core-protos-0.1.0: 1 vulnerabilities (highest severity is: 7.5) - autoclosed |
| [#131](#131) | 0 | 1 | [Feature Request] Apply codecs to `Failure.encodedAttributes` |
| [#120](#120) | 0 | 1 | Convert all tests to use test framework and multiple server forms |
| [#115](#115) | 0 | 1 | [Bug] Setting memo on executing child flow |
| [#114](#114) | 0 | 1 | Add max task queue activities per second |
| [#111](#111) | 0 | 1 | Change child workflow cancel to issue the one command |
| [#110](#110) | 0 | 1 | Ensure accepted iterables are iterated eagerly and stored as lists |
| [#105](#105) | 0 | 1 | go.temporal.io/server-v1.17.1: 6 vulnerabilities (highest severity is: 7.5) - autoclosed |
| [#103](#103) | 0 | 1 | [Feature Request] `retry_policy` on `continue_as_new` |
| [#88](#88) | 0 | 1 | github.com/temporalio/temporal-v1.17.1: 3 vulnerabilities (highest severity is: 7.5) - autoclosed |
| [#84](#84) | 0 | 1 | [Bug] M1 Mac grpcio error |
| [#83](#83) | 0 | 1 | Add README info for how to disable deadlock detector |
| [#81](#81) | 0 | 1 | Test framework |
| [#79](#79) | 0 | 1 | Accept `host:port` for client instead of only URL |
| [#40](#40) | 0 | 1 | Add more robust docs to docstrings |
| [#37](#37) | 0 | 1 | Put public doc links in docstrings |
| [#30](#30) | 0 | 1 | github.com/temporalio/temporal-v1.14.4: 4 vulnerabilities (highest severity is: 7.5) - autoclosed |
| [#25](#25) | 0 | 1 | Propagate fatal worker errors |
| [#19](#19) | 0 | 1 | Roadmap |
| [#18](#18) | 0 | 1 | temporal-client-0.1.0: 1 vulnerabilities (highest severity is: 7.5) - autoclosed |
| [#17](#17) | 0 | 1 | go.temporal.io/server-v1.14.4: 4 vulnerabilities (highest severity is: 7.5) - autoclosed |
| [#12](#12) | 0 | 1 | Multiprocess heartbeat and cancel improvement research |
| [#1268](#1268) | 0 | 0 | [Bug] Activity/workflow pollers drop to 0 ignoring minimum/maximum settings |
| [#1262](#1262) | 0 | 0 | [Bug] Add ability to obtain `ApplicationError` details with type hint |
| [#1255](#1255) | 0 | 0 | [Bug] Update openai-agents dependency |
| [#1254](#1254) | 0 | 0 | [Bug] `SandboxImportNotificationPolicy.WARN_ON_UNINTENTIONAL_PASSTHROUGH` warns on import of the workflow itself to the sandbox |
| [#1244](#1244) | 0 | 0 | [Bug] Build Binaries Github Actions Workflow Fails on nexus-rpc mismatch |
| [#1239](#1239) | 0 | 0 | [Bug] `WorkflowHistoryEventAsyncIterator` page_size override is ignored. |
| [#1237](#1237) | 0 | 0 | [Bug] Converter returns bad value converting `dict[None, Any]` |
| [#1232](#1232) | 0 | 0 | [Feature Request] Add BasedPyright as a required typecheck |
| [#1230](#1230) | 0 | 0 | [Feature Request] Support non-workflow activities |
| [#1225](#1225) | 0 | 0 | [Bug] Client.config() and Worker.config() should represent immutable copies of exact kwargs passed in |
| [#1209](#1209) | 0 | 0 | [Feature Request] Allow overriding of the Worker Deployment Version when invoking a Child Workflow |
| [#1201](#1201) | 0 | 0 | [Feature Request] Better prevention of accidental `Runtime` use over forks |
| [#1191](#1191) | 0 | 0 | [Bug] Failures in activites are not decoded in workflow when custom codec is used |
| [#1188](#1188) | 0 | 0 | [Bug] converter.py: TypeError on Python 3.13 when using isinstance with typing.Literal |
| [#1176](#1176) | 0 | 0 | [Feature Request] Clean up pyright exclude list |
| [#1171](#1171) | 0 | 0 | [Feature Request] Check integration support with agents 0.4.0 |
| [#1160](#1160) | 0 | 0 | Modify code comment for Worker build_id field |
| [#1159](#1159) | 0 | 0 | [Feature Request] Create test to verify OpenAI plugin works in activity-only and workflow-only modes |
| [#1133](#1133) | 0 | 0 | [Feature Request] Provide an easier way to create simple Plugins |
| [#1132](#1132) | 0 | 0 | [Feature Request] Properly set Python programming language versions and license in wheel/PyPI |
| [#1121](#1121) | 0 | 0 | [Feature Request] Document runtime telemetry config fields |
| [#1090](#1090) | 0 | 0 | [Feature Request] Support `mcp_server` in OpenAI integration |
| [#1084](#1084) | 0 | 0 | [Feature Request] Do not clobber user payload codec in OpenAI plugin |
| [#1080](#1080) | 0 | 0 | [Bug] Unable to import generated protocol buffer code |
| [#1079](#1079) | 0 | 0 | [Feature Request] Ensure tests exist to confirm custom slot supplier slot info has proper fields |
| [#1077](#1077) | 0 | 0 | [Feature Request] Extend typing improvements from #938 |
| [#1071](#1071) | 0 | 0 | [Bug] Ensure every inbound and outbound workflow payload goes through codec |
| [#1063](#1063) | 0 | 0 | [Feature Request] Ensure gRPC binary metadata headers are supported |
| [#1058](#1058) | 0 | 0 | [Bug] OpenAI Agent SDK wraps Temporal ApplicationError preventing workflow acting on it |
| [#1056](#1056) | 0 | 0 | [Feature Request] Add workflow MCP client |
| [#1025](#1025) | 0 | 0 | [Feature Request] Add first execution run ID to workflow info |
| [#994](#994) | 0 | 0 | [Feature Request] Worker plugin support for replayers |
| [#978](#978) | 0 | 0 | [Feature Request] Add Summary to LocalActivityConfig |
| [#965](#965) | 0 | 0 | [Bug] pydantic_core._pydantic_core.PydanticSerializationError: Error serializing to JSON: TypeError: 'MockValSer' object cannot be converted to 'SchemaSerializer' |
| [#950](#950) | 0 | 0 | [Feature Request] Plugins to support controlling multiple configuration points at once |
| [#921](#921) | 0 | 0 | Remove eval_type_backport dependency |
| [#916](#916) | 0 | 0 | Add tests for the tracing intercepter to OpenAI tests |
| [#915](#915) | 0 | 0 | Add tests for heartbeating in OpenAI tests |
| [#914](#914) | 0 | 0 | Fix OpenAI extension issues on python 3.9-3.10 |
| [#913](#913) | 0 | 0 | Move OpenAI overrides to a worker interecepter |
| [#912](#912) | 0 | 0 | Validate OpenAI data converter necessity |
| [#911](#911) | 0 | 0 | Add named parameters to OpenAI APIs |
| [#892](#892) | 0 | 0 | [Feature Request] Support `merge_extra` in custom adapters |
| [#889](#889) | 0 | 0 | [Bug] Improperly accessing current activity context in outbound heartbeat |
| [#874](#874) | 0 | 0 | [Bug] Workflow interceptors initialized before some attributes set in instance constructor |
| [#870](#870) | 0 | 0 | [Python] Worker Versioning high level client |
| [#865](#865) | 0 | 0 | [Bug] Flaky test: `tests/worker/test_workflow.py::test_workflow_return_is_honored_when_it_precedes_signal_completion_command` |
| [#864](#864) | 0 | 0 | [Feature Request] Add some kind of extra on some logs for easier identification |
| [#849](#849) | 0 | 0 | [Feature Request] Add workflow.Info.workflow_start_time |
| [#847](#847) | 0 | 0 | [python] SDK support for activity reset  |
| [#845](#845) | 0 | 0 | [Bug] Missing Local Activites should fail WFT rather than retry-looping |
| [#840](#840) | 0 | 0 | [Feature Request] Change get static summary/details on workflow in client to be lazy |
| [#835](#835) | 0 | 0 | [Feature Request] Environment configuration |
| [#834](#834) | 0 | 0 | [Bug] Python 3.13.3 on macOS Intel hangs post-test |
| [#831](#831) | 0 | 0 | [Feature Request] Show retry policy on activity info |
| [#827](#827) | 0 | 0 | [Bug] Importing uvicorn causes exception |
| [#826](#826) | 0 | 0 | [Bug] test_unfinished_handler_on_workflow_termination fails in some situations with time skipping and update |
| [#820](#820) | 0 | 0 | Apply application failure logging and metrics behaviour according to ApplicationErrorCategory |
| [#812](#812) | 0 | 0 | Heartbeating activities should be interrupted when the activities are paused. |
| [#810](#810) | 0 | 0 | [Feature Request] Implement proper behavior for `cancelled`, `uncancel`, and `cancelling` of activities and child workflows |
| [#798](#798) | 0 | 0 | [Feature Request] Consider TimeoutError as a workflow failure exception |
| [#794](#794) | 0 | 0 | [Feature Request] Make option for OTel workflow spans even if client span not present |
| [#793](#793) | 0 | 0 | [Feature Request] Support New Worker Versioning API |
| [#787](#787) | 0 | 0 | [Feature Request] Add summary to Nexus operation |
| [#784](#784) | 0 | 0 | [Bug] Avoid losing worker slot on error while processing cache eviction |
| [#774](#774) | 0 | 0 | [Feature Request] Expose root workflow execution |
| [#773](#773) | 0 | 0 | [Feature Request] Built-in query responses should use "RawValue" |
| [#772](#772) | 0 | 0 | [Bug] Investigate unused bind_f function |
| [#764](#764) | 0 | 0 | Expose Workflow cancel cause/reason |
| [#763](#763) | 0 | 0 | [Feature Request] Remove experimental warning from a few no-longer-experimental APIs |
| [#762](#762) | 0 | 0 | [Feature Request] Enforce no-mutable-defaults lint rule |
| [#761](#761) | 0 | 0 | [Feature Request] Publish musl wheel |
| [#760](#760) | 0 | 0 | [Feature Request] Support Priority Annotations |
| [#754](#754) | 0 | 0 | [Feature Request] Add explicit memoization support to the `patched()` API |
| [#748](#748) | 0 | 0 | [Feature Request] Allow customization of dev server UI port |
| [#728](#728) | 0 | 0 | [Feature Request] Update Ruff target version to 3.9 |
| [#722](#722) | 0 | 0 | [Feature Request] Stack trace on deadlock exception improvements |
| [#720](#720) | 0 | 0 | [Feature Request] Access to workflow instance from context |
| [#708](#708) | 0 | 0 | [Bug] Stop skipping update tests under the Java test server |
| [#704](#704) | 0 | 0 | [Bug] Run ID should be set in update handle |
| [#689](#689) | 0 | 0 | [Feature Request] Do runtime check to ensure update validator has the same parameter signature as update it validates |
| [#685](#685) | 0 | 0 | [Bug] Issue when failing to build activity failure |
| [#682](#682) | 0 | 0 | [Bug] Update requests should set first_execution_run_id |
| [#678](#678) | 0 | 0 | [Bug] Logic bug for create_schedule + backfill w/out trigger immediately |
| [#675](#675) | 0 | 0 | [Feature Request] Auto-skip time in time-skipping testing environment when waiting on update result |
| [#670](#670) | 0 | 0 | [Feature Request] Support user metadata |
| [#669](#669) | 0 | 0 | [Feature Request] Support / provide guidance on using OpenTelemetry logging + metrics SDKs with process-pool workers  |
| [#657](#657) | 0 | 0 | [Bug] Cannot replace worker client if default Runtime is used |
| [#648](#648) | 0 | 0 | Show update handler and ID in logging context |
| [#642](#642) | 0 | 0 | [Feature Request] Fix quinn-proto Vulnerability by Updating Rust Package to Version 0.11.7  |
| [#637](#637) | 0 | 0 | [Bug] Windows 3.12.5 builds are broken |
| [#636](#636) | 0 | 0 | [Feature Request] Add type-level tests |
| [#613](#613) | 0 | 0 | [Feature Request] Investigate adding allure to GitHub Actions CI workflows |
| [#611](#611) | 0 | 0 | [Feature Request] Better error message when trying to upsert a datetime search attribute with non-tz-aware datetime. |
| [#610](#610) | 0 | 0 | [Feature Request] Support a `limit` parameter in `replay_workflows` and `workflow_replay_iterator` |
| [#608](#608) | 0 | 0 | [Feature Request] Make payload conversion errors on workflow input more discriminable |
| [#607](#607) | 0 | 0 | [Bug] Prevent hot-reload of workflow definitions from disk |
| [#606](#606) | 0 | 0 | [Feature Request] Consider aligning activation job application with TS changes |
| [#600](#600) | 0 | 0 | Workflow-init support |
| [#594](#594) | 0 | 0 | [Feature Request] Support schedule search attribute update |
| [#585](#585) | 0 | 0 | [Bug] Test flake, may be sandbox import issue |
| [#584](#584) | 0 | 0 | Unfinished handlers: continue-as-new and dynamic handlers |
| [#577](#577) | 0 | 0 | [Bug] cibuildwheel using older manylinux 2014 images using out of date Centos 7 repo |
| [#575](#575) | 0 | 0 | [Feature Request] Workflow cancellation should cancel handler executions |
| [#573](#573) | 0 | 0 | [Feature Request] Support "query" when listing schedules |
| [#565](#565) | 0 | 0 | [Feature Request] Replace BuildJet ARM runner with GH runner |
| [#558](#558) | 0 | 0 | [Feature Request] Accept search attributes in dev server startup |
| [#553](#553) | 0 | 0 | Testing |
| [#552](#552) | 0 | 0 | Testing |
| [#543](#543) | 0 | 0 | [Bug] Generating protos via docker and manually are causing different whitespace inside of multi-paragraph docs |
| [#542](#542) | 0 | 0 | Expose UpdateID in an update handler |
| [#539](#539) | 0 | 0 | Provide methods to wait for all handlers to finish |
| [#538](#538) | 0 | 0 | Warn or error when update handlers dangle across CAN or workflow exit |
| [#536](#536) | 0 | 0 | [Feature Request] Clarify exception on return type decoding failure |
| [#535](#535) | 0 | 0 | [Feature Request] Turn invalid asyncio call warnings into errors |
| [#532](#532) | 0 | 0 | [Feature Request] Call Core worker.validate |
| [#528](#528) | 0 | 0 | [Feature Request] Do not set workflow completion until after all coroutines have settled in the task  |
| [#527](#527) | 0 | 0 | [Bug] Can crash SDK with non-halting workflow |
| [#525](#525) | 0 | 0 | [Bug] Returning given payload object in codec causes issues |
| [#522](#522) | 0 | 0 | [Feature Request] Cloud Operations API Client |
| [#514](#514) | 0 | 0 | [Feature Request] Make start_update users aware that it's synchronous w/ worker |
| [#513](#513) | 0 | 0 | [Feature Request] Client replacement in worker |
| [#509](#509) | 0 | 0 | Expose defaultConnection() method to switch between localhost and cloud based on env |
| [#503](#503) | 0 | 0 | [Bug] Do not mutate `extra` in activity/workflow loggers |
| [#500](#500) | 0 | 0 | [Feature Request] Add extra context on task failure logs |
| [#496](#496) | 0 | 0 | [Bug] Fix explanation of Pydantic datetime issue |
| [#495](#495) | 0 | 0 | [Feature Request] Use macOS M1/ARM runners for all SDK CI |
| [#488](#488) | 0 | 0 | [Bug] Fix typo in restriction set |
| [#485](#485) | 0 | 0 | [Feature Request] SDK should not return an update handle if the update has not reached the desired state |
| [#484](#484) | 0 | 0 | [Feature Request] SDK clients should set a UpdateID on any update request even if the user did not specify one |
| [#479](#479) | 0 | 0 | [Feature Request] Reflection-based payload codec test to ensure all non-search-attribute payloads are covered |
| [#477](#477) | 0 | 0 | [Bug] Updates not run through data converters |
| [#473](#473) | 0 | 0 | [Feature Request] Clarify in README that --all-extras is needed for build |
| [#470](#470) | 0 | 0 | [Feature Request] Clarify option immutability for application failures |
| [#468](#468) | 0 | 0 | Expose next retry delay on Application Failure in all SDKs |
| [#467](#467) | 0 | 0 | [Feature Request] Support workflow metadata query |
| [#439](#439) | 0 | 0 | [Feature Request] Allow continue as new from update handler |
| [#438](#438) | 0 | 0 | [Feature Request] Allow `RawValue` to be used for Search Attibute Upsert |
| [#436](#436) | 0 | 0 | [Bug] Ensure continue as new raised from update handler is a task failure |
| [#435](#435) | 0 | 0 | [Bug] Overload checking not catching bad parameters anymore |
| [#433](#433) | 0 | 0 | [Feature Request] Remove activation job sorting and confirm update order |
| [#427](#427) | 0 | 0 | [Bug] dataclasses require even defaulted fields in the JSON dict |
| [#424](#424) | 0 | 0 | [Feature Request] add get update handle and set run ID |
| [#421](#421) | 0 | 0 | [Feature Request] Move from black/isort to Ruff |
| [#419](#419) | 0 | 0 | [Bug] README issues |
| [#416](#416) | 0 | 0 | [Feature Request] Ensure activity failure warning log has contextual workflow information |
| [#409](#409) | 0 | 0 | Remove OutboundInterceptor.poll_workflow_update |
| [#408](#408) | 0 | 0 | Remove timeout from WorkflowUpdateHandle.result |
| [#404](#404) | 0 | 0 | [Feature Request] Add StartDelay to Workflow Options |
| [#398](#398) | 0 | 0 | [Feature Request] Drop 3.7 support, update CI to Python 3.12, and other build improvements |
| [#395](#395) | 0 | 0 | [Feature Request] Client keep alive support |
| [#388](#388) | 0 | 0 | [Feature Request] Eager workflow start |
| [#379](#379) | 0 | 0 | Document that time skipping test server does not currently work on ARM |
| [#377](#377) | 0 | 0 | [Feature Request] Warn when non-static activity method registered as static |
| [#369](#369) | 0 | 0 | [Feature Request] Advanced metrics support |
| [#363](#363) | 0 | 0 | [Bug] Testing environment overrides set interceptors |
| [#355](#355) | 0 | 0 | [Feature Request] Log and drop signals that pass the wrong number of arguments |
| [#352](#352) | 0 | 0 | [Bug] Cancel received in between awaitables not cancelling workflow |
| [#348](#348) | 0 | 0 | [Feature Request] Make all exposed modules be packages |
| [#347](#347) | 0 | 0 | [Bug] Signals that can't be deserialized should be dropped/ignored |
| [#335](#335) | 0 | 0 | Worker versioning support |
| [#328](#328) | 0 | 0 | Remove "similar to Pedantic" from no-generic part of README |
| [#326](#326) | 0 | 0 | [Feature Request] Add helper for checking whether exception represents cancellation |
| [#320](#320) | 0 | 0 | [Feature Request] Remove experimental warning from Schedules |
| [#318](#318) | 0 | 0 | [Bug] If self parameter is type hinted, it is not properly ignored when deserializing parameters |
| [#301](#301) | 0 | 0 | [Bug] Proxied classes are not properly hashable |
| [#294](#294) | 0 | 0 | Add API to count workflows |
| [#293](#293) | 0 | 0 | [Feature Request] Clarify activity_id parameter is not recommended |
| [#291](#291) | 0 | 0 | Add disclaimer about gevent incompatibility |
| [#290](#290) | 0 | 0 | [Feature Request] Support result_type on child workflows and activities |
| [#289](#289) | 0 | 0 | [Bug] JSONTypeConverter not applied recursively |
| [#285](#285) | 0 | 0 | [Bug] temporalio.worker.WorkflowReplayResult(s) not visible |
| [#282](#282) | 0 | 0 | [Feature Request] Clarify requirement of heartbeat timeout wrt cancellation |
| [#273](#273) | 0 | 0 | [Feature Request] Improve workflow sandboxing suggestions |
| [#267](#267) | 0 | 0 | [Bug] Health proto clashes with grpc_health library |
| [#266](#266) | 0 | 0 | [Feature Request] Confirm async activity completion can give null value for Optional-return activity |
| [#265](#265) | 0 | 0 | [Feature Request] Properly list queries in query-handler-not-found error |
| [#263](#263) | 0 | 0 | [Feature Request] Populate contextvars for threaded activities |
| [#258](#258) | 0 | 0 | [Bug] Make sure start workflows throws WorkflowAlreadyStartedError if already running |
| [#255](#255) | 0 | 0 | [Bug] PEP 604 style optionals not working properly |
| [#254](#254) | 0 | 0 | [Bug] with_child_unrestricted not properly removing restriction |
| [#250](#250) | 0 | 0 | [Feature Request] Warn/error on invalid query use |
| [#247](#247) | 0 | 0 | [Bug] Failure.encoded_attributes not being subjected to codec |
| [#246](#246) | 0 | 0 | [Feature Request] Mark local activities as experimental |
| [#237](#237) | 0 | 0 | [Bug] Activity cancellation error issues |
| [#236](#236) | 0 | 0 | [Bug] Optional[X] type not being handled properly as parameter |
| [#234](#234) | 0 | 0 | [Bug] Generic Nested Dataclasses not desrialized correctly |
| [#230](#230) | 0 | 0 | [Feature Request] Move the user-facing parts in temporalio.bridge.runtime to somewhere stable |
| [#228](#228) | 0 | 0 | [Feature Request] Eagerly error when attempting to register a callable activity class instead of its instance |
| [#226](#226) | 0 | 0 | [Bug] Protobuf version 3.19 does not have ValueType enum wrapper |
| [#224](#224) | 0 | 0 | [Bug] grpcio not being treated as optional dependency |
| [#218](#218) | 0 | 0 | [Feature Request] Disallow cross-namespace child workflow calls |
| [#212](#212) | 0 | 0 | [Bug] Replayer README inaccurate |
| [#204](#204) | 0 | 0 | [Feature Request] Make it easier to pass through third party modules |
| [#203](#203) | 0 | 0 | [Bug] Fix README to use workflow_runner instead of runner in sandbox examples |
| [#199](#199) | 0 | 0 | [Bug] OpenTelemetry sandbox issue on first load inside sandbox |
| [#198](#198) | 0 | 0 | [Bug] Bad metrics config type |
| [#187](#187) | 0 | 0 | [Feature Request] Allow client headers to be mutated after creation |
| [#178](#178) | 0 | 0 | [Feature Request] Prefix private files/modules' filenames with underscore |
| [#176](#176) | 0 | 0 | [Feature Request] Support string-based enum |
| [#167](#167) | 0 | 0 | [Bug] Do not require timer to be present when fire-timer received |
| [#165](#165) | 0 | 0 | [Feature Request] Tests for Python 3.11 support |
| [#159](#159) | 0 | 0 | [Bug] Function type hint lookup cache key is bad |
| [#156](#156) | 0 | 0 | [Feature Request] Remove custom Go server |
| [#125](#125) | 0 | 0 | Mark global tracing options as experimental |
| [#124](#124) | 0 | 0 | Do not require ID for child workflow invocation |
| [#122](#122) | 0 | 0 | Remove abbreviations in options |
| [#119](#119) | 0 | 0 | Remove callable as accepted client interceptor type |
| [#118](#118) | 0 | 0 | Do not convert history JSON enums if already done |
| [#117](#117) | 0 | 0 | [Bug] Client.get_workflow_handle_for not properly setting result type |
| [#101](#101) | 0 | 0 | Per-call gRPC options |
| [#93](#93) | 0 | 0 | Expose operator service raw grpc API |
| [#91](#91) | 0 | 0 | Immediate start-then-cancel not working properly |
| [#89](#89) | 0 | 0 | Timed out wait_condition futures still attempt setting result |
| [#80](#80) | 0 | 0 | Create workflow sandbox |
| [#70](#70) | 0 | 0 | Properly treat activity/child cancellation errors bubbled as workflow cancel |
| [#60](#60) | 0 | 0 | Interceptor header and OpenTelemetry support |
| [#56](#56) | 0 | 0 | Remove gRPC dependency |
| [#47](#47) | 0 | 0 | Properly report version |
| [#45](#45) | 0 | 0 | Improve workflow stack trace query result |
| [#39](#39) | 0 | 0 | Add build docs to README |
| [#29](#29) | 0 | 0 | Move from Sphinx API docs to pydoctor |
| [#28](#28) | 0 | 0 | More workflow tests |
| [#27](#27) | 0 | 0 | Versioning/Patching |
| [#26](#26) | 0 | 0 | Search attributes |
| [#24](#24) | 0 | 0 | Async activity support |
| [#11](#11) | 0 | 0 | Test client/worker/core behavior across fork |
| [#10](#10) | 0 | 0 | Investigate more generic manylinux support |

---

## All Issues

Issues are sorted by priority score (upvotes Ã— 2 + comments).

---

<a id="621"></a>

### #621: [Feature Request] Create official Pydantic contrib module

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/621 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-08-26 12:08:46.000 UTC (1y 4m ago) |
| **Updated** | 2025-02-18 17:49:58.000 UTC |
| **Closed** | 2025-02-14 04:42:19.000 UTC |
| **Upvotes** | 20 |
| **Comments** | 4 |
| **Priority Score** | 44 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 20 ðŸš€ 10 |

#### Description

### Describe the solution you'd like

Pydantic is popular/common enough and people are struggling with https://github.com/temporalio/samples-python/tree/main/pydantic_converter enough (or just that we haven't shown V2 yet: https://github.com/temporalio/samples-python/issues/97), that we should just make a `temporalio.contrib.pydantic` module as a `pydantic` extra similar to what we have done with `opentelemetry`.

It should contain:

* A JSON encoding payload converter that works with Pydantic models to/from JSON
* A default payload converter that puts Pydantic converter just above the default JSON encoding payload converter in the list
* A default data converter using that default payload converter
* Some way to solve the datetime issue mentioned in the sample's README (maybe just a built-in Pydantic-safe worker runner that customizes the sandbox for them if there's no better way)
  * We might also want the converter to warn if it can when it sees a Pydantic datetime and the fix is not enabled in the sandbox and there's no better way to fix the issue

#### Comments (4)

<details>
<summary><strong>Kilo59</strong> commented on 2025-01-04 03:33:04.000 UTC</summary>

@cretz Is there any movement on this?
I almost passed on Temporal in favor of [FastStream](https://faststream.airt.ai/latest/) and [Tasik](https://faststream.airt.ai/latest/scheduling/#taskiq-faststream) because of the relative friction of using the Python SDK in comparison.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-01-06 15:25:58.000 UTC</summary>

I am afraid not yet, but it is on the near-term backlog. We will look into prioritizing.

Reactions: â¤ï¸ 4

</details>

<details>
<summary><strong>drewhoskins-temporal</strong> commented on 2025-02-04 21:10:56.000 UTC</summary>

@Kilo59 as you can see from the above PR, we are working on it!

Reactions: â¤ï¸ 2

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-02-18 17:49:55.000 UTC</summary>

@Kilo59 the latest version of the SDK now has official Pydantic (v2) support: https://github.com/temporalio/sdk-python/releases/tag/1.10.0

Reactions: â¤ï¸ 3

</details>


---

<a id="58"></a>

### #58: Deserialize traceback from stack trace string in Temporal failures if able

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/58 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-06-28 20:29:55.000 UTC (3y 6m ago) |
| **Updated** | 2022-08-17 19:06:50.000 UTC |
| **Closed** | 2022-08-17 19:06:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 27 |
| **Priority Score** | 27 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

~~We chain errors when converting from failures by setting `__cause__`, but there is a report that the chained errors are not logged like normally chained errors~~

EDIT: We don't rehydrate the traceback from the stack trace string

### Describe the solution you'd like

~~Make sure we log chained errors normally and write a test to ensure it~~

EDIT: Java parses their string stack trace back to stack trace elements, so we should too. See https://github.com/ionelmc/python-tblib/blob/dd926c1e5dc5bbe5e1fc494443bbac8970c7d3ee/src/tblib/__init__.py#L200 for an example of how to do this.



#### Comments (27)

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-06-28 20:54:10.000 UTC</summary>

Ok, I looked into this bit further and I realized that the chaining _is_ working, but you don't get any stack traces. To be honest I just didn't notice the exception chain without associated stack traces, particularly because the top level `WorkflowError` had a big stack trace.

I know that the stack trace for `ApplicationError`s is available (`e.cause.cause.failure.stack_trace`) I wonder if there's a way to "rehydrate" the stack trace so it displays as if the exception was raised locally. It looks like https://github.com/ionelmc/python-tblib does this sort of thing.

From what I see there is no stack trace attached for `ActivityError`s. It would be nice to have stack traces for these as well, but I guess it's less important and I imagine if it's not in the error prototbuf you're not about to add it.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-06-28 21:31:11.000 UTC</summary>

> I wonder if there's a way to "rehydrate" the stack trace so it displays as if the exception was raised locally

My fear of "rehydrating" the stack trace is if it's not from Python. These stack traces may come from any language. It is normal to have a Python workflow execute a Go activity and vice-versa. Let me check what other languages do here.

> From what I see there is no stack trace attached for ActivityErrors.

This is because `ActivityError` is never "raised", it is only used as a wrapper by the server. It'd be the actual raised error (i.e. `ApplicationError`) that would have a stack trace.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-06-28 22:15:24.000 UTC</summary>

> My fear of "rehydrating" the stack trace is if it's not from Python. These stack traces may come from any language. It is normal to have a Python workflow execute a Go activity and vice-versa. Let me check what other languages do here.

I suppose you could just try "rehydration" and then fallback to no traceback or a string representation.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-06-28 22:51:07.000 UTC</summary>

> I suppose you could just try "rehydration" and then fallback to no traceback or a string representation.

I have confirmed this is _exactly_ what Java does. My only concern now is how to parse the traceback. I would rather not have a third party library it at all possible just for this use case. And I also would rather not come up with my own string format for tracebacks that will work on all versions of Python (because I doubt I can trust default traceback string formats from version to version).

The question now becomes how important is it to rehydrate Python stack traces compared to the effort and brittleness of the solution?

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-06-28 23:04:52.000 UTC</summary>

> The question now becomes how important is it to rehydrate Python stack traces compared to the effort and brittleness of the solution?

I say, if Java did it why not Python? I think it would be a nice thing. You could just pull the minimal code needed from tblib into sdk-python. I'd be up for working on something like this, but I haven't been able to get the development setup working on my M1 Mac ðŸ˜”

Alternatively we could just rely on logging. This is what I'm used to with [Dramatiq](https://dramatiq.io/) which is what I currently use for task processing and it's not _soo_ bad.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-06-29 14:02:58.000 UTC</summary>

Java's stack trace element string format is a bit more stable from version to version.

Regardless, yeah we can do this. I think we'd just take `tblib.Traceback.from_string` which seems to have an expectation on Python traceback string format. So maybe it is more stable than I realize. And of course we'd have some tests to ensure it continues to work. I will change the title of this issue to add this functionality. This shouldn't be hard to implement, I'll probably get to it before next release.

Reactions: â¤ï¸ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-06 20:25:06.000 UTC</summary>

I am bit concerned about doing this now as I see the string format is changing in 3.11 to include more detail. It seems relying on traceback string format is rough. It breaks on 3.11 with the library you linked at https://github.com/ionelmc/python-tblib and other traceback parsers I've seen (e.g. bolton). I wonder if we can just ask that users:

```python
err = my_exception
while isinstance(err, FailureError):
    if err.failure.stack_trace:
        try:
            err.__traceback__ = tblib.Traceback.from_string(err.failure.stack_trace)
        except TracebackParseError:
            pass
    err = err.__cause__
```

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-06 21:36:48.000 UTC</summary>

You are so right, I can see that being a big problem. Maybe the thing is just to think of the developer story ("as a developer I want to see where in my code an error is occurring when I am testing locally") and make sure it's covered in some way?

For example, I'm pretty happy with my custom activity decorator that logs exceptions in activities. So maybe improving the default logging would work too, but I feel like having clear stack traces be displayed in stdout/stderr by default is important.

Under the same developer story, it's still slightly bugs me that you don't get any stack trace for the workflow portion of errors. I feel like the activities are more important as that's were the errors should mostly be, but it would be very nice to have stack traces everywhere.
 

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-06 22:02:21.000 UTC</summary>

Any suggestions here? Options as I see them:

* Deserialize into traceback - brittle because traceback strings are non-standard across versions
* Ask the user to extract the traces themselves - may be asking too much of the user
* Have custom logger - we need to respect their choice on logging
* Put the stack trace inside the exception args - then normal `str()` calls return way more than the message
* Our own equivalent of https://docs.python.org/3/library/traceback.html#traceback.print_exception - still requires users actually call it

Are there any options I missed? Any preferred option there?

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-07 15:49:58.000 UTC</summary>

I think I'm partial to logging. Maybe a way to think of it is "default logging" and then user's can still setup their own system if they like. I personally use [loguru](https://loguru.readthedocs.io/en/stable/) but it's fairly interoperable/based on the core Python logging, and I feel like that's how things generally are in Python logging. I also use [the rich logging handler](https://rich.readthedocs.io/en/stable/logging.html) as my handler for nicely formatted and colored output. So I think it's OK to depend on Python logging. I feel like if you have nice default logging out of the box it also gives a great feel to first time users while power users still know how to reconfigure things how they like it.

I'm not sure where you would log exceptions from (worker process? workflow process?) but I'm looking forward to seeing what you decide.






</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-07 16:34:54.000 UTC</summary>

The concern is not depending on Python logging, the concern is that the caller calls their _own_ logger at their _own_ time to log the exception. We can't inject the stack trace only at logging time without forcing the user to use some kind of helper explicitly.

How do I make `logger.exception("OH NO")` print these stack traces without including them in the exception args (i.e. what `str()`) would show)? We can make helpers they can call, but I am not aware of a way to do this implicitly. Do you have a more concrete suggestion? Can you show code of how a user might use their logger to log this exception with string stack traces?

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-07 17:04:46.000 UTC</summary>

> The concern is not depending on Python logging, the concern is that the caller calls their own logger at their own time to log the exception. We can't inject the stack trace only at logging time without forcing the user to use some kind of helper explicitly.

If they want to do that, they can just disable the temporal section of the logging tree. I feel it's not so widely known that you can do this, but you can always add a note to the SDK docs the help point people in the right direction.

> How do I make logger.exception("OH NO") print these stack traces without including them in the exception args (i.e. what str()) would show)? We can make helpers they can call, but I am not aware of a way to do this implicitly. Do you have a more concrete suggestion? Can you show code of how a user might use their logger to log this exception with string stack traces?

Possibly you could log exceptions for actions directly from the action worker, and then this issue goes away. Then you lose the exception chain, but I personally don't find the current exception chain very useful because it basically says "A workflow failed and the source was an application error with this message" with no other information about how those things are linked. So, as a developer, if I see "my workflow failed due to an ApplicationError" along with a separate, logged, stack trace for the ApplicationError I likely understand what happened.

Logging application errors local to the activity feels less pure, but might be more useful in practice for local development and then in production you would probably rely more on the temporal UI anyway.

As to logging as string stack trace, I think you would just have to use `logger.error` rather than `logger.exception`. As I understand it, `logger.exception` calls `logger.error` but passes exception information to be formatted. We have already formatted stack trace, so `logger.error` might be fine.


</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-07 17:50:16.000 UTC</summary>

> Possibly you could log exceptions for actions directly from the action worker, and then this issue goes away.

I do log as debug with full stack traces at https://github.com/temporalio/sdk-python/blob/0392468f21d9604c828d50d34daf8c9be03b31cd/temporalio/worker/workflow_instance.py#L1120-L1123 and https://github.com/temporalio/sdk-python/blob/0392468f21d9604c828d50d34daf8c9be03b31cd/temporalio/worker/activity.py#L442-L444, but errors are a normal thing and I don't think we should log each one at a higher log level by default. But of course any user can catch/log their own exceptions or use an interceptor to log.

The issue of client not seeing stack traces does not go away though, because the client does not get the stack trace for a failed activity or workflow in the client-side log by default.

> Then you lose the exception chain

Exception chain will always remain. We serialize hierarchical exception causes in all SDKs.

> Logging application errors local [...]

Sure, nobody is stopping users from doing that. Granted it shouldn't be the default I don't think.

> As to logging as string stack trace [...]

I am not following the concrete suggestion here. Regardless of what `exc_info` is set as (`True` on `logger.exception` by default, `False` otherwise), the point is the traceback is not present.

Here is a simple problem request:

"Can you make Temporal stack traces appear on the client when they use their logger?"

Everything else about how a user may want to log on the worker side or manually extract the stack trace string out is fine and doable, but it does not solve the problem. I think we either need to solve this problem or tell users their alternatives (e.g. "use X helper when logging" and/or "log on the worker side and just accept client side logging doesn't have stack traces", etc).

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-07 18:08:20.000 UTC</summary>

After internal discussions, I can change those default worker-side log levels from debug to warning (but doesn't change the fact that client-side is missing stack traces on their logs by default).

Also, after consulting with others, I think we'll just populate the traceback knowing the limitations and brittleness.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-07 19:47:49.000 UTC</summary>

>After internal discussions, I can change those default worker-side log levels from debug to warning (but doesn't change the fact that client-side is missing stack traces on their logs by default).
I like it!

I guess I personally don't care _so much_ that the stack traces I see are client side vs. just seeing stack traces by default when testing, but I can see how it's important to have both. Well actually, it might get a little weird if we have both (duplicate output for testing case) but it's important to at least have both options!

You and the rest of the team are doing such a great job of nailing all the details, I really enjoy following along. Thank you for engaging with my questions/issues.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-07 21:24:55.000 UTC</summary>

Well, unfortunately trying to create a `types.TracebackType` with my own frame type gives:

> TypeError: TracebackType() argument 'tb_frame' must be frame, not _TracebackFrame

So I am not sure I can do this even if I wanted to (I am using `types.TracebackType` so that methods like `inspect.istraceback` would work). Still researching...

(attached [exceptions.py](https://github.com/temporalio/sdk-python/files/9067459/exceptions.py.txt) with the parsing code at the bottom in case we want to revisit this in the future)

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-13 12:36:23.000 UTC</summary>

After research, I can't find anyone that re-populates `__traceback__` from a remote exception. https://github.com/ionelmc/python-tblib, https://github.com/mahmoud/boltons/blob/master/boltons/tbutils.py, https://github.com/celery/billiard/blob/master/billiard/einfo.py, etc all have structure that mimics traceback and can serialize/deserialize traceback and such, but nothing I have found puts the traceback _back on the exception_ for use in logging.

If there is any example in the Python world of anyone doing this, please link. Otherwise, I am afraid it's not really doable.

We _could_ put the traceback string in the message, but that's unreasonable for `str()` users.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-13 15:41:44.000 UTC</summary>

Gotcha. Well maybe small logging improvements are the way to a better experience then.

Here's another small improvement I've been thinking of. It would be nice for ApplicationError's to include the root exception name. Right now the message/value whatever of the application error is just that of the underlying message, but then the exception name is not visible at all. So if root error is `MyException("something went wrong")`, currently `ApplicationError: something went wrong` is raised but I feel it would be better to raise `ApplicationError: MyException: something went wrong`. Often the name of the exception is an important piece, and right now it is not displayed but can be accessed via `.type` I think. I feel like prepending it to the exception message might be a pragmatic solution. What do you think?

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-13 15:51:38.000 UTC</summary>

> Well maybe small logging improvements are the way to a better experience then.

Any concrete suggestions here? I am struggling to think of the best way.

> I feel like prepending it to the exception message might be a pragmatic solution. What do you think?

I am hesitant to fiddle with anyone's exception message string. People should be able to throw a string message and assert equality against that string message I think. This is the case with all of our SDKs I believe.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-13 16:02:13.000 UTC</summary>

> Any concrete suggestions here? I am struggling to think of the best way.

I think switching the logging level for action exceptions to warning (as you all decided) is a good one. If I think of more than the below, I'll let you know!

That's a good point, but can't they access the underlying message via `.message`? I feel like it's reasonably common to raise a specific exception when the name of the exception describes the issue and the message is empty. For this case, you would currently get an error like `ApplicationError: ''` which is not so helpful and then you have to drop into a debugger to check that the `.type` is. Also, from a consistency standpoint, I feel like `ApplicationError` is different then the root error, so it's OK for the message to be different.

I feel like the the default output should do its best to help the developer _understand_ the error, and then they have these other attributes to do programmatic checks and dig deeper.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-13 16:05:06.000 UTC</summary>

> That's a good point, but can't they access the underlying message via .message?

While true, often people expect `str(my_error)` to have the message, not the type and then the message. But we might be able to compromise here. Will discuss with team.

Note I think that this concern exists with all of our SDKs today, so it may be a bit difficult to convince the team that Python should be special.

EDIT: Also, are you suggesting prepending the string type name on the client side or the worker side? If on the client side, how does this output look when I manually `raise ApplicationError("some error")` which we _often_ ask people to do (e.g. it's the only way to raise a specific non-retryable exception for instance).

> I feel like the the default output should do its best to help the developer understand the error

I agree when it comes to logger output, but not every string representation of the error. If we can do this just for loggers that'd be ideal.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-13 17:01:37.000 UTC</summary>

> Note I think that this concern exists with all of our SDKs today, so it may be a bit difficult to convince the team that Python should be special.

I can understand that. It would be annoying if you wrote in a couple different SDK languages to have these types of differences. As far as matching the pattern of checking the `str(my_error)` case, I don't see this as a big problem because:

- The exception model already takes some understanding, so I feel like it's unreasonable/unlikely for people to ship code without some testing and exploration where they would be confronted with this behaviour
- They can always use `str(my_error).endswith(...)` or just check for the string value with the exception name in it. It's almost a feature, check type and message in one line!
- And they have that `.message` as well

> EDIT: Also, are you suggesting prepending the string type name on the client side or the worker side? If on the client side, how does this output look when I manually raise ApplicationError("some error") which we often ask people to do (e.g. it's the only way to raise a specific non-retryable exception for instance).

In this case there would be no underlying exception name to prepend to the message, so maybe just don't prepend anything and this is a non-issue? I guess I was thinking of prepending on the client side. So it's just a presentation level thing, and what's sent over the wire does not change.

> I agree when it comes to logger output, but not every string representation of the error. If we can do this just for loggers that'd be ideal.

Again, I don't really care how it gets on the screen, I just want to have clear exception output when I write my tests! I know you will find the best way to accomplish that.




</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-13 17:16:41.000 UTC</summary>

I think I am starting to come around here...I don't personally like that `str(error)` is not a simple message like it is with all the standard Python errors, but whatever.

> They can always use `str(my_error).endswith(...)` ...

Do you think we should also append the stack trace to the message? I'm thinking so.

I'm thinking I'll open a PR that sets the base exception args as a string format of optional `f"Stack:\n{stack}\n", optional f"{type}: ", then the message. Sound about right? I'll open a PR and see how it looks.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-13 17:26:14.000 UTC</summary>

> Do you think we should also append the stack trace to the message? I'm thinking so.

Then people _really_ have to go to `.message` to have logic based on the exception message, but maybe that's a good thing.

For me, I guess this depends on how it displays, but I guess this could tick all the boxes if it looks OK and it's intuitive what's going on. I can compile temporal locally now, so if you come up with something I'd be happy to try it out and share any thoughts.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-13 21:31:21.000 UTC</summary>

After internal discussion, I think we will just prefix with type and document a way to get stack traces in the log formatter. Will update with PR.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-14 15:10:03.000 UTC</summary>

I have opened #75. In addition to other things we have suggested, it contains a test that appends the stack to all Temporal failure errors. Basically you can use this helper:

```python
def append_temporal_stack(exc: Optional[BaseException]) -> None:
    while exc:
        # Only append if it doesn't appear already there
        if (
            isinstance(exc, FailureError)
            and exc.failure
            and exc.failure.stack_trace
            and len(exc.args) == 1
            and "\nStack:\n" not in exc.args[0]
        ):
            exc.args = (f"{exc}\nStack:\n{exc.failure.stack_trace.rstrip()}",)
        exc = exc.__cause__
```

I am hesitant to add it as a supported public utility at the moment (it's very simple, only a couple of lines if the formatter didn't break it out).

Since we cannot put the traceback back on the error and we don't want stack to be on every string representation of the error, opt-in on the user part is the only way.

I researched custom logging formatters, adapters, etc and it boiled down to just being easier to alter the exceptions in the chain than altering the logging calls. The `traceback.format_exception` and similar calls by logging and others handle the chain for you, so you can't add the stack after the fact there. And I don't want to recreate my own chaining string formatter because I want to reuse Python's. I would have preferred shallow copying all the exceptions (e.g. `copy.copy()`) and only adding stack to the shallow copies, but that had problems maintaining the chain too. Same with customizing log record and other approaches.

So basically, altering the exception is easiest. I chose to put the stack after the message instead of before because the internal Python exception formatter always put's the exception class name first which means I _can't_ inject anything before that.

Using that helper above, given the following:

```python
    try:
        raise ValueError("error1")
    except Exception as err:
        raise RuntimeError("error2") from err
```

Once serialized and sent back from Temporal, after running through `append_temporal_stack`, your output might look like:

```
temporalio.exceptions.ApplicationError: ValueError: error1
Stack:
  File "/path/to/file.py", line 100, in my_function
    raise ValueError("error1")

The above exception was the direct cause of the following exception:

temporalio.exceptions.ApplicationError: RuntimeError: error2
Stack:
  File "/path/to/file.py", line 201, in my_function
    raise RuntimeError("error2") from err
```

Which I think is about the best we can do.

Reactions: ðŸ‘ 2

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-08-17 19:06:49.000 UTC</summary>

I am closing this for now, but will refer back to it anytime anyone is curious why we can't deserialize tracebacks.

</details>


---

<a id="441"></a>

### #441: [Bug] GeneratorExit possibly causing issues on context detach in OTel finally

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/441 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-12-07 13:00:54.000 UTC (2y 1m ago) |
| **Updated** | 2025-10-22 18:57:18.000 UTC |
| **Closed** | 2025-10-22 18:57:18.000 UTC |
| **Upvotes** | 4 |
| **Comments** | 9 |
| **Priority Score** | 17 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 4 |

#### Description

### Describe the bug

Reports of:

```
Failed to detach context

Traceback (most recent call last):
  File "/path/to/temporalio/contrib/opentelemetry.py", line 427, in _top_level_workflow_context
    yield None
  File "/path/to/temporalio/contrib/opentelemetry.py", line 340, in execute_workflow
    return await super().execute_workflow(input)
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/path/to/opentelemetry/context/__init__.py", line 157, in detach
    _RUNTIME_CONTEXT.detach(token)  # type: ignore
  File "/path/to/opentelemetry/context/contextvars_context.py", line 50, in detach
    self._current_context.reset(token)  # type: ignore
ValueError: <Token var=<ContextVar name='current_context' default={} at 0x79340011d590> at 0x7933a4b4c9c0> was created in a different Context"
```

There are outstanding issues like https://github.com/open-telemetry/opentelemetry-python/issues/2606 that may be related.

#### Comments (9)

<details>
<summary><strong>jpcarlino</strong> commented on 2024-04-19 16:16:53.000 UTC</summary>

Is there any workaround for this? We have tons of these errors in our logs forcing us to disable tracing in our workers.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-04-19 16:23:16.000 UTC</summary>

I am not aware of an obvious one (see linked upstream ticket). We may be able to make sure everywhere we `detach` we aren't doing so in a `GeneratorExit`.

However, we have found that `GeneratorExit` has other problems, so we have made a fairly substantial change to workflow teardown in #499 that should prevent `GeneratorExit` from occurring within common workflow use cases (i.e. we try to force tasks to complete before letting garbage collector take over and raise this). This will come out in next release, but if you'd like you can build `main` now and test it. Your errors may go away.

I wouldn't be surprised if this is some OpenTelemetry users' issue too - `GeneratorExit` can occur in a completely different thread than the async code was originally run within and may have lost all contextual information. It's a very dangerous thing Python does here (waking up tasks on GC in any thread).

Reactions: ðŸ‘ 2

</details>

<details>
<summary><strong>benliddicott</strong> commented on 2024-04-21 14:21:45.000 UTC</summary>

@cretz  As I commented on #2606 I was able to work around a similar issue. Feel free to contact me if you are interested in more details.
https://github.com/open-telemetry/opentelemetry-python/issues/2606#issuecomment-1789515786


I suspect that if the otel teardown_request hook "opentelemetry-flask.activation_key" could somehow know that it was not in the correct context (e.g. on the wrong thread, or different async context) and therefore decide to do nothing, this might resolve the issue.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>matthewgrossman</strong> commented on 2024-04-22 19:56:44.000 UTC</summary>

Hey @benliddicott I've been following this issue for python-otel, and put out a flask-specific fix awhile back: https://github.com/open-telemetry/opentelemetry-python-contrib/pull/1692. Since you're still recently having issues, I'm imagining that didn't resolve it for you (unless you're on an old version?)

Maybe the changes / PR desc could be helpful debugging your issue?

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-04-22 20:15:51.000 UTC</summary>

For the Temporal SDK here in this GH issue, we don't even use Flask. The GC can wake up the coroutine in any thread, including the same one it may have started in but is currently being used for something else by the thread pool. There's not much you can do but avoid letting coroutines/generators get GC'd (i.e. cancel and wait for graceful completion).

</details>

<details>
<summary><strong>matthewgrossman</strong> commented on 2024-04-22 20:22:12.000 UTC</summary>

Ya sorry about the flask noise, probably better to post what I did back in https://github.com/open-telemetry/opentelemetry-python/issues/2606

I'm now on a new project where we're running into the same error, and it's moreso related to generators/coroutines that you are mentioning; that's why I'm also following this thread

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-04-22 20:24:23.000 UTC</summary>

No prob, yeah I flat out could not find a good solution besides not ever reaching `GeneratorExit` (or coroutine GC). There's no way to disable this behavior, no way to intercept, and with threading (especially thread pools) no guarantee where it may wake up.

So that's how we solved this, just making sure all tasks complete (but leaving open until we release it)

</details>

<details>
<summary><strong>benliddicott</strong> commented on 2024-04-22 21:16:46.000 UTC</summary>

@matthewgrossman I am not having issues, as I have a workaround, as described in https://github.com/open-telemetry/opentelemetry-python/issues/2606#issuecomment-1789515786 so there is no urgency from my side.

@cretz  I haven't tried to find out if my workaround is still necessary in the latest release - all I can say is that the version we are using hasn't broken the workaround.

</details>

<details>
<summary><strong>hellais</strong> commented on 2024-04-26 11:05:14.000 UTC</summary>

@benliddicott would you be available to share the code you used as a solution to this? I am running into the same issue as well.

</details>


---

<a id="487"></a>

### #487: [Feature Request] Allow workflow logger to opt-out of sandboxing

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/487 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-03-12 17:58:04.000 UTC (1y 10m ago) |
| **Updated** | 2025-10-16 00:06:49.000 UTC |
| **Upvotes** | 6 |
| **Comments** | 3 |
| **Priority Score** | 15 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 6 ðŸ‘€ 2 |

#### Description

### Describe the solution you'd like

Some users have advanced logger handlers that cannot work in sandbox mode. I think we can add a `temporalio.workflow.logger.unsafe_disable_sandbox()` call on the logger and if set, the `process` call can be wrapped in `with temporalio.workflow.unsafe.sandbox_unrestricted()` and `workflow.unsafe.imports_passed_through()`.

#### Comments (3)

<details>
<summary><strong>millerick</strong> commented on 2025-10-15 19:21:46.000 UTC</summary>

Until this is implemented, would it be reasonable for users that run into issues with their logger to add the underlying logging libraries a `with_passthrough_modules()` call to suppress these types of messages?  E.g. to ignore sandbox restrictions for `structlog`?

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-10-15 19:37:48.000 UTC</summary>

Depends on how the underlying logging library is loaded and used on whether that would be enough. I would probably recommend something like:

```python
class SandboxDisabledLoggerAdapter(workflow.LoggerAdapter):
    def process(
        self, msg: Any, kwargs: MutableMapping[str, Any]
    ) -> Tuple[Any, MutableMapping[str, Any]]:
        with temporalio.workflow.unsafe.sandbox_unrestricted():
            with temporalio.workflow.unsafe.imports_passed_through():
                return super().process(msg, kwargs)

temporalio.workflow.logger = SandboxDisabledLoggerAdapter(logging.getLogger("temporalio.workflow"), None)
```

Which is basically what this issue is suggesting we offer a helper for. But I just typed this here in GH issue, this would have to be tested and confirmed to work as expected.


</details>

<details>
<summary><strong>millerick</strong> commented on 2025-10-16 00:06:48.000 UTC</summary>

Our issues stem from formatting the record and not from processing the record.  I'll try to find a similar way to wrap the format calls in `structlog`, but adding this as a datapoint for if/when the helpers this issue represents in the SDK are implemented.
```
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.11/logging/__init__.py", line 1110, in emit
    msg = self.format(record)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/logging/__init__.py", line 953, in format
    return fmt.format(record)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/structlog/stdlib.py", line 1098, in format
    ed = p(logger, meth_name, cast(EventDict, ed))
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/structlog/processors.py", line 346, in __call__
    return self._dumps(event_dict, **self._dumps_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/encoder.py", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/structlog/processors.py", line 354, in _json_fallback_handler
    from structlog.threadlocal import _ThreadLocalDictWrapper
  File "/usr/local/lib/python3.11/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 442, in __call__
    return self.current(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 234, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
```

Reactions: ðŸ‘ 1

</details>


---

<a id="638"></a>

### #638: Importing 3rd party package `ruamel.yaml` anywhere causes failure to validate workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/638 |
| **State** | OPEN |
| **Author** | adamh-oai (Adam Hupp) |
| **Created** | 2024-09-08 07:44:18.000 UTC (1y 4m ago) |
| **Updated** | 2024-10-16 20:56:09.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 12 |
| **Priority Score** | 14 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description


To reproduce:
  1. Checkout https://github.com/temporalio/samples-python
  2. `poetry add ruamel.yaml`
  3. Add `import ruamel.yaml` to the top of `hello/hello_activity.py`
  4. `poetry run hello/hello_activity.py`

This produces the error here: https://gist.github.com/adamh-oai/dfdb9b07b89bf3cee10da34ba2582805

Important parts:

```
  File "/Users/adamh/.virtualenvs/temporalio-samples-G-Ux_4V2-py3.11/lib/python3.11/site-packages/ruamel/yaml/representer.py", line 1132, in <module>
    RoundTripRepresenter.add_representer(TimeStamp, RoundTripRepresenter.represent_datetime)
  File "/Users/adamh/.virtualenvs/temporalio-samples-G-Ux_4V2-py3.11/lib/python3.11/site-packages/ruamel/yaml/representer.py", line 135, in add_representer
    cls.yaml_representers[data_type] = representer
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
TypeError: unhashable type: '_RestrictedProxy'

The above exception was the direct cause of the following exception:
...
  File "/Users/adamh/.virtualenvs/temporalio-samples-G-Ux_4V2-py3.11/lib/python3.11/site-packages/temporalio/worker/_workflow.py", line 118, in __init__
    raise RuntimeError(f"Failed validating workflow {defn.name}") from err
RuntimeError: Failed validating workflow GreetingWorkflow
```

This behavior is surprising to me because the workflow/activity is not actually using the `ruamel` package.  The error persists if I move the workflow/activity to a separate module and wrap in a `imports_passed_through` block.  Wrapping the `ruamel` import in this same block *does* resolve the error.  It seems like the `ruamel` import may have side effects on other modules.  So, I have a workaround, but (short of disabling sandboxing entirely) I'm concerned that in a large codebase with unpredictable 3rd party libraries this will be a recurring issue.

#### Comments (12)

<details>
<summary><strong>cretz</strong> commented on 2024-09-09 13:32:38.000 UTC</summary>

This is a sandbox issue. Despite those short "hello" samples being in the same file, we actually recommend workflows be in their own file. Can you replicate if the workflows are in a separate file and import the activities via "passthrough" similar to what's shown in the quickstart of the README: https://github.com/temporalio/sdk-python?tab=readme-ov-file#implementing-a-workflow?

</details>

<details>
<summary><strong>adamh-oai</strong> commented on 2024-09-09 17:36:40.000 UTC</summary>

Yes, it happens if the workflows are in their own module and imported via an unsafe block:

> The error persists if I move the workflow/activity to a separate module and wrap in a imports_passed_through block. Wrapping the ruamel import in this same block does resolve the error. It seems like the ruamel import may have side effects on other modules. So, I have a workaround, but (short of disabling sandboxing entirely) I'm concerned that in a large codebase with unpredictable 3rd party libraries this will be a recurring issue.

</details>

<details>
<summary><strong>adamh-oai</strong> commented on 2024-09-09 17:37:18.000 UTC</summary>

 A related issue: https://github.com/temporalio/sdk-python/issues/301

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-09 17:47:01.000 UTC</summary>

> Yes, it happens if the workflows are in their own module and imported via an unsafe block:

Ah, I see the bug already opened for making the proxy item hashable. I wonder then if this is a duplicate of that. In this case our wrapping of datetime causes this issue.

>  Wrapping the ruamel import in this same block does resolve the error. It seems like the ruamel import may have side effects on other modules.

To confirm, are you using this library _inside_ the workflow and want the import reloaded every time the workflow is run? We recommend passing through all imports you know you'll use deterministically and/or you don't want completely reloaded every time. It's going to be a performance issue to reload third party libraries for every workflow run, but I understand the safety benefit.

> I'm concerned that in a large codebase with unpredictable 3rd party libraries this will be a recurring issue.

Yes, most do not use many third party libraries _inside_ their workflows, mostly only in activities and other code. But yes, our goal is to try our best to make third party libraries usable inside workflows but these hiccups happen here and there and we need to fix the linked bug.

</details>

<details>
<summary><strong>adamh-oai</strong> commented on 2024-09-09 18:09:58.000 UTC</summary>

> To confirm, are you using this library inside the workflow and want the import reloaded every time the workflow is run? 

This import is not used inside the workflow; in the real case where I hit this I was using it to load a yaml file for configuring the temporal client, but the workflows themselves don't import it or use it.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-09 18:43:18.000 UTC</summary>

> but the workflows themselves don't import it or use it.

This error only occurs in workflow code when loading it in a sandbox, so somehow this yaml code is getting executed/loaded in a workflow. Maybe the activity imports weren't being passed through when you split the workflow file off onto its own?

</details>

<details>
<summary><strong>creatorrr</strong> commented on 2024-10-15 22:41:45.000 UTC</summary>

Same issue. I ended up removing `ruamel` entirely but I found that adding the following to the _very first_ `__init__.py` file seems to help:

```python
from temporalio import workflow

with workflow.unsafe.imports_passed_through():
    import ruamel.yaml

    # also- msgpack if installed
    import msgpack
```

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>creatorrr</strong> commented on 2024-10-15 22:42:46.000 UTC</summary>

@cretz it would be really helpful to add this suggestion / explanation to the error though. It took up hours to track this down and fix :(

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>creatorrr</strong> commented on 2024-10-15 22:43:57.000 UTC</summary>

or maybe a whitelist of common libraries in the temporal python sdk itself? that's probably not super wise but would save so much confusion

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-16 15:47:29.000 UTC</summary>

We don't really control the error here unfortunately. We do have docs at https://github.com/temporalio/sdk-python?tab=readme-ov-file#workflow-sandbox that say in bold:

> **For performance and behavior reasons, users are encouraged to pass through all third party modules whose calls will be deterministic**

</details>

<details>
<summary><strong>adamh-oai</strong> commented on 2024-10-16 18:35:30.000 UTC</summary>

> This error only occurs in workflow code when loading it in a sandbox, so somehow this yaml code is getting executed/loaded in a workflow. 

The way I interpreted it is that because ruamel patches datetime, it's implicitly used in the the workflow even though it's not imported.  FWIW, I can't repro this right now but it was weirdly inconsistent earlier as well so not sure what's going on; since someone else ran into it I assume this is a real issue.  Ideally ruamel would just Not Do That, but I'm also not sure why it does or what other options there are.  

The main issue is that an innocuous change in another part of the codebase that doesn't touch anything temporal related can cause temporal sandbox failures.  

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-16 20:45:07.000 UTC</summary>

> because ruamel patches datetime

Ah, I was not aware of this (unfamiliar with the library). So we proxy datetime to avoid people doing things like calling `now()`. But that proxying can be disabled. We have to do this for Pydantic because of an issue in Python subclass checking. So you can create a new sandbox runner without datetime proxied like https://github.com/temporalio/samples-python/blob/171b5e5b205167fdff4231978857c4efe1cd6225/pydantic_converter/worker.py#L45-L65 and pass that runner to the worker and `datetime` will not be affected usually.

However, you may have to use `with temporalio.workflow.unsafe.sandbox_unrestricted():` for any ruamel-using code (even if it uses it indirectly, like with datetime) to avoid our other checks.

> The main issue is that an innocuous change in another part of the codebase that doesn't touch anything temporal related can cause temporal sandbox failures.

I am not sure any Python library can help this if patching is occurring. If you patch Python stdlib and some unrelated code expects it to work the stdlib way, it may fail. But between passing through imports, unrestricting sandbox for certain code, and disabling proxying for the `datetime` module, you should be able to remove all Temporal checks and wrappings.

</details>


---

<a id="631"></a>

### #631: [Bug] Worker hangs after polling workflow task queue

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/631 |
| **State** | CLOSED |
| **Author** | kelkawi-a (Ali Kelkawi) |
| **Created** | 2024-09-03 10:18:14.000 UTC (1y 4m ago) |
| **Updated** | 2025-04-03 13:13:51.000 UTC |
| **Closed** | 2025-04-03 13:13:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 13 |
| **Priority Score** | 13 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

To connect a worker to a Temporal server with an `authorization` header.

### Describe the bug

Starting a worker with an incorrect token, causing the server to respond with `Request unauthorized`, causes the worker to hang indefinitely.

### Minimal Reproduction

```python
import asyncio

from temporalio import activity, client, workflow, worker


@activity.defn
async def a() -> None:
    pass


@workflow.defn
class Workflow:
    @workflow.run
    async def run(self) -> None:
        pass


async def main():
    c = await client.Client.connect(
        "my_temporal_host:7233",
        rpc_metadata={"authorization": "wrong_token"},
        tls=True,
    )
    w = worker.Worker(
        c,
        task_queue="default",
        activities=[a],
        workflows=[Workflow],
    )
    await w.run()


if __name__ == "__main__":
    asyncio.run(main())
```

Output:

```
2024-09-03T07:26:48.707Z [temporal-worker] 2024-09-03T07:26:48.707200Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 52 times error=Status { code: Internal, message: "protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:26:48 GMT", "content-type": "text/html", "content-length": "160", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None }
2024-09-03T07:26:49.544Z [temporal-worker] 2024-09-03T07:26:49.544345Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 52 times error=Status { code: Internal, message: "protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:26:49 GMT", "content-type": "text/html", "content-length": "160", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None }
2024-09-03T07:27:51.707Z [temporal-worker] 2024-09-03T07:27:51.707200Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 53 times error=Status { code: Internal, message: "protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:27:51 GMT", "content-type": "text/html", "content-length": "160", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None }
2024-09-03T07:27:52.968Z [temporal-worker] 2024-09-03T07:27:52.967933Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 52 times error=Status { code: Internal, message: "protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:27:52 GMT", "content-type": "text/html", "content-length": "160", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None }
2024-09-03T07:28:00.036Z [temporal-worker] 2024-09-03T07:28:00.036155Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 53 times error=Status { code: Internal, message: "protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:28:00 GMT", "content-type": "text/html", "content-length": "160", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None }
2024-09-03T07:28:00.687Z [temporal-worker] 2024-09-03T07:28:00.687484Z ERROR temporal_client::retry: gRPC call poll_workflow_task_queue retried 53 times error=Status { code: Internal, message: "protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 504 Gateway Timeout", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:28:00 GMT", "content-type": "text/html", "content-length": "160", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None }
2024-09-03T07:28:03.579Z [temporal-worker] 2024-09-03T07:28:03.578940Z  WARN temporal_sdk_core::worker::workflow::wft_poller: Error while polling for workflow tasks error=Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:28:03 GMT", "content-type": "application/grpc", "content-length": "0", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None }
2024-09-03T07:28:03.970Z [temporal-worker] 2024-09-03T07:28:03.583738Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:28:03 GMT", "content-type": "application/grpc", "content-length": "0", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None })
2024-09-03T07:28:03.970Z [temporal-worker] 2024-09-03T07:28:03.969933Z  WARN temporal_sdk_core::worker::workflow::wft_poller: Error while polling for workflow tasks error=Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:28:03 GMT", "content-type": "application/grpc", "content-length": "0", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None }
2024-09-03T07:28:03.970Z [temporal-worker] 2024-09-03T07:28:03.969960Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:28:03 GMT", "content-type": "application/grpc", "content-length": "0", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None })
2024-09-03T07:28:04.023Z [temporal-worker] Worker failed, shutting down
2024-09-03T07:28:04.023Z [temporal-worker] Traceback (most recent call last):
2024-09-03T07:28:04.023Z [temporal-worker]   File "/lib/python3.10/site-packages/temporalio/worker/_workflow.py", line 143, in run
2024-09-03T07:28:04.023Z [temporal-worker]     act = await self._bridge_worker().poll_workflow_activation()
2024-09-03T07:28:04.023Z [temporal-worker]   File "/lib/python3.10/site-packages/temporalio/bridge/worker.py", line 141, in poll_workflow_activation
2024-09-03T07:28:04.023Z [temporal-worker]     await self._ref.poll_workflow_activation()
2024-09-03T07:28:04.023Z [temporal-worker] RuntimeError: Poll failure: Unhandled grpc error when workflow polling: Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"date": "Tue, 03 Sep 2024 07:28:03 GMT", "content-type": "application/grpc", "content-length": "0", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None }
2024-09-03T07:28:04.023Z [temporal-worker] 
2024-09-03T07:28:04.023Z [temporal-worker] The above exception was the direct cause of the following exception:
2024-09-03T07:28:04.023Z [temporal-worker] 
2024-09-03T07:28:04.023Z [temporal-worker] Traceback (most recent call last):
2024-09-03T07:28:04.023Z [temporal-worker]   File "/lib/python3.10/site-packages/temporalio/worker/_workflow.py", line 153, in run
2024-09-03T07:28:04.023Z [temporal-worker]     raise RuntimeError("Workflow worker failed") from err
2024-09-03T07:28:04.023Z [temporal-worker] RuntimeError: Workflow worker failed
```

### Environment/Versions

- Temporal Version: Server v1.23.1, SDK v1.7.0

### Additional context

Related issue: #459 

#### Comments (13)

<details>
<summary><strong>cretz</strong> commented on 2024-09-03 12:53:19.000 UTC</summary>

> causes the worker to hang indefinitely.

We do check that the worker can connect to the namespace using "describe namespace". So you have a token that works on some calls but not others? Is this self-hosted or cloud?

The stack trace seems to be showing it raising an exception. Are you sure that `worker.run()` does not raise an exception here? It does take a minute because we retry all polling errors just in case they are spurious.

</details>

<details>
<summary><strong>kelkawi-a</strong> commented on 2024-09-03 17:56:11.000 UTC</summary>

This issue is two-fold, I reported the one relevant to the SDK here:
1. I have a worker which sends the correct token to a self-hosted Temporal server (v1.23.1). There is an issue at the ingress level which sometimes causes the server to respond with a `504 Bad Gateway`.
2. As the worker continues to receive this response from the ingress, it exhibits the behavior reported in this bug, eventually hanging and failing to re-connect.

The second issue is what i'm reporting here. The worker ends up raising a `RuntimeError` and not recovering from it.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-03 18:39:18.000 UTC</summary>

> The worker ends up raising a RuntimeError and not recovering from it.

Some client errors we can detect as recoverable. For ones we can't, we still try to recover for a minute before failing the worker. We intentionally fail the worker instead of letting it operate in a failed state on something that is not quickly/obviously recoverable. Can you confirm whether `worker.run()` does or does not raise an exception here? We start a worker shutdown, but a worker shutdown sends cancellation to activities and has to wait for activities to complete (see https://github.com/temporalio/sdk-python?tab=readme-ov-file#worker-shutdown).

</details>

<details>
<summary><strong>kelkawi-a</strong> commented on 2024-09-03 22:15:37.000 UTC</summary>

I'm not sure if you're referring to something different, but the logs shared in the original bug report show that `worker.run()` is raising a `RuntimeError`.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-03 22:45:06.000 UTC</summary>

Ah, I see it in the trace now. This is intentional behavior. A fatal error (or at least one we can't tell is non-fatal) that doesn't fix itself after a minute will cause the worker to fail and shutdown instead of pretending to work silently while continuing to fail. You may restart the worker if you wish, though many prefer not to blindly restart but rather investigate.

</details>

<details>
<summary><strong>kelkawi-a</strong> commented on 2024-09-03 22:49:16.000 UTC</summary>

In our case the worker is failing after a number of intermittent network issues, so we would rather it restarts instead. Is there any recommendation around how to restarting the worker? I tried a simple `while True` loop to catch the `RuntimeError` exception raised and re-run `await worker.run()`, which seemed to complain (I don't have the logs from that right now but I can get them).

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-04 14:25:38.000 UTC</summary>

> I tried a simple `while True` loop

This should work. Similarly you can consider having whatever is monitoring the process/pod/container do the restart at an outer level. I would recommend at least also alerting or something on fatal worker error or you won't know your worker isn't working.

</details>

<details>
<summary><strong>kelkawi-a</strong> commented on 2024-09-05 10:30:55.000 UTC</summary>

I did try the following to retry the worker:
```
    while True:
        try:
            await worker.run()
        except RuntimeError as e:
            print(f"RuntimeError caught: {e}. Retrying...")
            await asyncio.sleep(5)  # wait for 5 seconds before retrying
        except Exception as e:
            print(f"An unexpected error occurred: {e}")
            raise
```

After running for a while, the worker crashed with the following error when trying to restart:

```
  File "/home/kelkawi/.cache/pypoetry/virtualenvs/boarding-w3anQpca-py3.10/lib/python3.10/site-packages/temporalio/worker/_workflow.py", line 143, in run
    act = await self._bridge_worker().poll_workflow_activation()
  File "/home/kelkawi/.cache/pypoetry/virtualenvs/boarding-w3anQpca-py3.10/lib/python3.10/site-packages/temporalio/bridge/worker.py", line 141, in poll_workflow_activation
    await self._ref.poll_workflow_activation()
RuntimeError: Poll failure: Unhandled grpc error when workflow polling: Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"date": "Thu, 05 Sep 2024 09:43:21 GMT", "content-type": "application/grpc", "content-length": "0", "strict-transport-security": "max-age=15724800; includeSubDomains"} }, source: None }

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/kelkawi/.cache/pypoetry/virtualenvs/boarding-w3anQpca-py3.10/lib/python3.10/site-packages/temporalio/worker/_workflow.py", line 153, in run
    raise RuntimeError("Workflow worker failed") from err
RuntimeError: Workflow worker failed
RuntimeError caught: Workflow worker failed. Retrying...
An unexpected error occurred: 'NoneType' object has no attribute 'validate'
Traceback (most recent call last):
  File "/home/kelkawi/Desktop/CanonicalRepos/hr-automation/boarding/worker.py", line 219, in <module>
    asyncio.run(run_worker())
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/home/kelkawi/Desktop/CanonicalRepos/hr-automation/boarding/worker.py", line 207, in run_worker
    await worker.run()
  File "/home/kelkawi/.cache/pypoetry/virtualenvs/boarding-w3anQpca-py3.10/lib/python3.10/site-packages/temporalio/worker/_worker.py", line 478, in run
    await self._bridge_worker.validate()
  File "/home/kelkawi/.cache/pypoetry/virtualenvs/boarding-w3anQpca-py3.10/lib/python3.10/site-packages/temporalio/bridge/worker.py", line 133, in validate
    await self._ref.validate()
AttributeError: 'NoneType' object has no attribute 'validate'
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-05 14:12:51.000 UTC</summary>

A worker is meant for one run/shutdown. If you want to run a new worker you will need to recreate it again. Unfortunately some validation we added is happening before this check, we will fix that.

</details>

<details>
<summary><strong>hebrd</strong> commented on 2025-02-27 01:38:33.000 UTC</summary>

> This issue is two-fold, I reported the one relevant to the SDK here:
> 
> 1. I have a worker which sends the correct token to a self-hosted Temporal server (v1.23.1). There is an issue at the ingress level which sometimes causes the server to respond with a `504 Bad Gateway`.
> 2. As the worker continues to receive this response from the ingress, it exhibits the behavior reported in this bug, eventually hanging and failing to re-connect.
> 
> The second issue is what i'm reporting here. The worker ends up raising a `RuntimeError` and not recovering from it.

I have the same issues, too. Would you please share the solution to issue 1?

</details>

<details>
<summary><strong>kelkawi-a</strong> commented on 2025-02-27 09:16:16.000 UTC</summary>

@hebrd This was related to the ingress timing out when the worker does long polls of 60 seconds. One workaround we implemented was reducing the long polling interval through the Temporal server dynamic config:

```
matching.longPollExpirationInterval:
  - value: "50s"
```

</details>

<details>
<summary><strong>hebrd</strong> commented on 2025-02-27 12:33:36.000 UTC</summary>

Many thanks. It help a lot!!!

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-03 13:13:50.000 UTC</summary>

Closing this issue as it seems to have been a ingress configuration issue (Temporal needs to be able to make 70s calls).

</details>


---

<a id="412"></a>

### #412: [Bug] protoc-wheel-0 dev dependency causing issues when building on musl-based distros like Alpine

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/412 |
| **State** | CLOSED |
| **Author** | bryanlalexander (Bryan Alexander) |
| **Created** | 2023-11-02 21:14:19.000 UTC (2y 2m ago) |
| **Updated** | 2024-08-26 12:25:57.000 UTC |
| **Closed** | 2024-06-10 18:13:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 13 |
| **Priority Score** | 13 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm attempting to install `pip install temporalio` in Alpine v3.18 (Python 3.11.6). This doesn't seem to be possible given [protoc-wheel-0](https://pypi.org/project/protoc-wheel-0/) does not yet support newer versions (locked at 3.9). However, it does seem a minimal dependency that could be easily be isolated and eliminated.

Is this on the roadmap for removal/deprecation?


### Describe the bug

```console
ERROR: Could not find a version that satisfies the requirement protoc-wheel-0 (from versions: none)
ERROR: No matching distribution found for protoc-wheel-0
```


```console
Collecting temporalio (from -r requirements.txt (line 1))
  Downloading temporalio-1.3.0.tar.gz (944 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 944.6/944.6 kB 19.6 MB/s eta 0:00:00
  Installing build dependencies: started
  Installing build dependencies: finished with status 'error'
  error: subprocess-exited-with-error
  
  Ã— pip subprocess to install build dependencies did not run successfully.
  â”‚ exit code: 1
  â•°â”€> [17 lines of output]
      Collecting poetry-core>=1.0.0
        Obtaining dependency information for poetry-core>=1.0.0 from https://files.pythonhosted.org/packages/99/bc/058b8ff87871fce6615ad032d62c773272f243266b110f7b86d146cf78d8/poetry_core-1.8.1-py3-none-any.whl.metadata
        Downloading poetry_core-1.8.1-py3-none-any.whl.metadata (3.5 kB)
      Collecting setuptools
        Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/bb/26/7945080113158354380a12ce26873dd6c1ebd88d47f5bc24e2c5bb38c16a/setuptools-68.2.2-py3-none-any.whl.metadata
        Downloading setuptools-68.2.2-py3-none-any.whl.metadata (6.3 kB)
      Collecting wheel
        Obtaining dependency information for wheel from https://files.pythonhosted.org/packages/fa/7f/4c07234086edbce4a0a446209dc0cb08a19bb206a3ea53b2f56a403f983b/wheel-0.41.3-py3-none-any.whl.metadata
        Downloading wheel-0.41.3-py3-none-any.whl.metadata (2.2 kB)
      Collecting setuptools-rust
        Obtaining dependency information for setuptools-rust from https://files.pythonhosted.org/packages/e0/34/d88a7ceb193fbcee6c8992d1b1e33ed20361027e07fea1676efc45ec7a43/setuptools_rust-1.8.1-py3-none-any.whl.metadata
        Downloading setuptools_rust-1.8.1-py3-none-any.whl.metadata (9.4 kB)
      ERROR: Could not find a version that satisfies the requirement protoc-wheel-0 (from versions: none)
      ERROR: No matching distribution found for protoc-wheel-0
      
Notice:       [notice] A new release of pip is available: 23.2.1 -> 23.3.1
Notice:       [notice] To update, run: pip install --upgrade pip
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

Ã— pip subprocess to install build dependencies did not run successfully.
â”‚ exit code: 1
â•°â”€> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.

Notice:  A new release of pip is available: 23.2.1 -> 23.3.1
Notice:  To update, run: pip install --upgrade pip
```


### Minimal Reproduction

`repro.dockerfile`
```Dockerfile
FROM public.ecr.aws/docker/library/alpine:latest

RUN apk update && apk add --no-cache python3 py3-pip

RUN pip3 install temporalio
```
`docker build -f repro.dockerfile .`


### Environment/Versions

|program|version|
|-------|-------|
|Alpine|3.18|
|Python|3.11.6|
|pip|23.1.2|


#### Comments (13)

<details>
<summary><strong>cretz</strong> commented on 2023-11-03 15:20:49.000 UTC</summary>

What happens on musl-based platforms is that your system attempts to rebuild the wheel from scratch. We don't expect runtime users to have this, it is a dev dependency. You're also going to have to have Rust and protoc on the path and everything else that comes with building this project since we don't pre-build a musl wheel.

We do require this dependency for proto generation to put `protoc` on the `PATH`, but I'm ok requiring it on the `PATH` already since we do for building the Rust part anyways. So we can probably remove this dev dependency (or make it optional), but you're still going to have to have certain things installed when building on an environment we don't prebuild a wheel for.

</details>

<details>
<summary><strong>bryanlalexander</strong> commented on 2023-11-03 17:40:58.000 UTC</summary>

This seems to be perfectly reasonable to me, this will give those without a choice a path forward. I will move this job onto a debian-based distro for now. Will musl be supported in the future with this sdk?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-11-03 18:12:12.000 UTC</summary>

I am unsure whether it will be supported in the future, but for now (as with windows arm, 32-bit, etc) it requires a custom build

</details>

<details>
<summary><strong>bryanlalexander</strong> commented on 2023-11-03 18:19:40.000 UTC</summary>

Sounds good, thanks for the help!

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>Carnewal</strong> commented on 2024-05-08 10:53:10.000 UTC</summary>

> So we can probably remove this dev dependency (or make it optional), but you're still going to have to have certain things installed when building on an environment we don't prebuild a wheel for.

I'm running into a similar issue, do you have any news with regards to this, any way to circumvent this?

```
 > [service 7/8] RUN poetry install --without dev:                                                                                                                                                        
1.643 Skipping virtualenv creation, as specified in config file.                                                                                                                                                               
2.530 Installing dependencies from lock file                                                                                                                                                                                   
3.263                                                                                                                                                                                                                          
3.264 Package operations: 38 installs, 0 updates, 0 removals                                                                                                                                                                   
3.264 
...
...
25.12   - Installing temporalio (1.6.0)
25.19   - Installing uvloop (0.19.0)
32.92 
32.92   ChefInstallError
32.92 
32.92   Failed to install poetry-core>=1.0.0, setuptools, wheel, setuptools-rust, protoc-wheel-0.
32.92   
32.92   Output:
32.92   Updating dependencies
32.92   Resolving dependencies...
32.92   
32.92   Package operations: 6 installs, 0 updates, 0 removals
32.92   
32.92     - Installing semantic-version (2.10.0)
32.92     - Installing setuptools (69.5.1)
32.92     - Installing poetry-core (1.9.0)
32.92     - Installing protoc-wheel-0 (25.3)
32.92     - Installing setuptools-rust (1.9.0)
32.92     - Installing wheel (0.43.0)
32.92   
32.92     RuntimeError
32.92   
32.92     Unable to find installation candidates for protoc-wheel-0 (25.3)
32.92   
32.92     at /usr/local/lib/python3.12/site-packages/poetry/installation/chooser.py:74 in choose_for
32.92          70â”‚ 
32.92          71â”‚             links.append(link)
32.92          72â”‚ 
32.92          73â”‚         if not links:
32.92       â†’  74â”‚             raise RuntimeError(f"Unable to find installation candidates for {package}")
32.92          75â”‚ 
32.92          76â”‚         # Get the best link
32.92          77â”‚         chosen = max(links, key=lambda link: self._sort_key(package, link))
32.92          78â”‚ 
32.92   
32.92   Cannot install protoc-wheel-0.
32.92   
32.92   
32.92   
32.92   Error:
32.92   
32.93 
32.93   at /usr/local/lib/python3.12/site-packages/poetry/installation/chef.py:102 in install
32.95        98â”‚             InstalledRepository.load(self._env),
32.95        99â”‚         )
32.95       100â”‚         installer.update(True)
32.95       101â”‚         if installer.run() != 0:
32.95     â†’ 102â”‚             raise ChefInstallError(requirements, io.fetch_output(), io.fetch_error())
32.95       103â”‚ 
32.95       104â”‚ 
32.95       105â”‚ class Chef:
32.95       106â”‚     def __init__(
32.95 
32.95 Cannot install build-system.requires for temporalio.
32.95 
------
```

Moving to python-slim images removes the error but alpine would be preferred

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-05-08 13:10:42.000 UTC</summary>

I think this happens because we don't have a prebuilt musl wheel and `protoc-wheel-0` doesn't either. I am not aware of a way to circumvent this at this time short of removing the dev dependency before building. I am reopening this issue to see if it's reasonable for us to remove this dependency (e.g. maybe put it behind a "extra" or use `protoc` on the `PATH` more directly).

Reactions: â¤ï¸ 1

</details>

<details>
<summary><strong>ymandge</strong> commented on 2024-05-29 07:22:44.000 UTC</summary>

I have same issue when I try to install Temporal SDK on FreeBSD

`# python -m pip install temporalio
Collecting temporalio
  Using cached temporalio-1.6.0.tar.gz (1.0 MB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  Ã— pip subprocess to install build dependencies did not run successfully.
  â”‚ exit code: 1
  â•°â”€> [10 lines of output]
      Collecting poetry-core>=1.0.0
        Using cached poetry_core-1.9.0-py3-none-any.whl.metadata (3.5 kB)
      Collecting setuptools
        Using cached setuptools-70.0.0-py3-none-any.whl.metadata (5.9 kB)
      Collecting wheel
        Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)
      Collecting setuptools-rust
        Using cached setuptools_rust-1.9.0-py3-none-any.whl.metadata (9.3 kB)
      ERROR: Could not find a version that satisfies the requirement protoc-wheel-0 (from versions: none)
      ERROR: No matching distribution found for protoc-wheel-0
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

Ã— pip subprocess to install build dependencies did not run successfully.
â”‚ exit code: 1
â•°â”€> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.`

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-05-29 13:33:25.000 UTC</summary>

You can probably remove the https://github.com/temporalio/sdk-python/blob/afadc15ba74d6101109316fd24a6747e8a558187/pyproject.toml#L50 dependency and rebuild. We need to remove this dependency ourselves or make it optional (we will look into doing this). It's only for letting us do proto generation which is not needed at build time.

</details>

<details>
<summary><strong>ymandge</strong> commented on 2024-05-30 09:47:36.000 UTC</summary>

Hi @cretz,

After removing the above line, I was successfully able to build the dependencies using "poetry install --no-root --all-extras" command  But when I try to build the release build using "poetry build" command , I'm getting the following errors, 

`
# poetry build
Preparing build environment with build-system requirements poetry-core>=1.0.0, setuptools, wheel, setuptools-rust, protoc-wheel-0

Command ['/tmp/tmpfe6rso36/.venv/bin/python', '/usr/local/lib/python3.8/site-packages/virtualenv/seed/wheels/embed/pip-24.0-py3-none-any.whl/pip', 'install', '--disable-pip-version-check', '--ignore-installed', '--no-input', 'poetry-core>=1.0.0', 'setuptools', 'wheel', 'setuptools-rust', 'protoc-wheel-0'] errored with the following return code 1

Output:
Collecting poetry-core>=1.0.0
  Using cached poetry_core-1.9.0-py3-none-any.whl.metadata (3.5 kB)
Collecting setuptools
  Using cached setuptools-70.0.0-py3-none-any.whl.metadata (5.9 kB)
Collecting wheel
  Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)
Collecting setuptools-rust
  Using cached setuptools_rust-1.9.0-py3-none-any.whl.metadata (9.3 kB)
ERROR: Could not find a version that satisfies the requirement protoc-wheel-0 (from versions: none)
ERROR: No matching distribution found for protoc-wheel-0
`

Fyi - I'm referring below doc 

https://github.com/temporalio/sdk-python?tab=readme-ov-file#development:~:text=written%20in%20Rust.-,Building,-Prepare

for building the Temporal Python SDK on FreeBSD.




</details>

<details>
<summary><strong>cretz</strong> commented on 2024-05-30 14:05:01.000 UTC</summary>

To remove a dependency with Poetry, you it needs to be removed from toml and lock file. `poetry remove` command can probably do this.

</details>

<details>
<summary><strong>ymandge</strong> commented on 2024-06-06 06:30:20.000 UTC</summary>

Hey everyone, fyi, I was able to build and install the Temporal Python SDK on FreeBSD by removing the protoc-wheel-0 and adding the protobuf package. Thank you Temporal Team for your prompt help :)

#protoc-wheel-0 = "^21.1" ---> remove this line
protobuf = "^4.25.3" ---> add this one

</details>

<details>
<summary><strong>hansraj1999</strong> commented on 2024-08-26 11:58:42.000 UTC</summary>

 Using cached temporalio-1.7.0.tar.gz (1.3 MB)
Installing build dependencies ... error
 error: subprocess-exited-with-error
  Ã— pip subprocess to install build dependencies did not run successfully.
 â”‚ exit code: 1
 â•°â”€> [10 lines of output]
Collecting poetry-core>=1.0.0
Using cached poetry_core-1.9.0-py3-none-any.whl.metadata (3.5 kB)
Collecting setuptools
Using cached setuptools-73.0.1-py3-none-any.whl.metadata (6.6 kB)
 Collecting wheel
Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)
Collecting setuptools-rust
Using cached setuptools_rust-1.10.1-py3-none-any.whl.metadata (9.2 kB)
ERROR: Could not find a version that satisfies the requirement protoc-wheel-0 (from versions: none)
 ERROR: No matching distribution found for protoc-wheel-0
end of output]

 note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

base image python 3.11-alpine3.16
getting this error while running python -m pip install temporalio command

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-08-26 12:25:56.000 UTC</summary>

I see an issue was opened at https://github.com/temporalio/sdk-python/issues/622 and a new Slack thread was started. No need to double/triple post.

</details>


---

<a id="207"></a>

### #207: [Bug] SandboxWorkflowRunner doesn't use correct Pydantic field types in some cases

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/207 |
| **State** | CLOSED |
| **Author** | michael-cybrid |
| **Created** | 2022-11-18 14:22:40.000 UTC (3y 1m ago) |
| **Updated** | 2024-12-09 18:39:13.000 UTC |
| **Closed** | 2024-01-29 13:46:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 13 |
| **Priority Score** | 13 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I want to be able to pass in Pydantic models with `datetime` fields.

To encode `datetime`fields, I am using the `pydantic.json.pydantic_encoder` but can hit this behaviour without any data conversion occurring.

### Describe the bug

The `SandboxWorkflowRunner` restrictions are resulting in Pydantic models being created with incorrect field types. 

Specifically, I am seeing models derived from Pydantic's `BaseModel` class (let's say `MyDatetimeWrapper`) that contain a `datetime` field being instantiated with a `date` object instead when a `MyDatetimeWrapper` object is created within a workflow.

### Minimal Reproduction

I've created a code sample to illustrate the issue:
```python
import asyncio
import dataclasses
from datetime import datetime
from uuid import uuid4

from pydantic import BaseModel
from temporalio import workflow
from temporalio.client import Client
from temporalio.worker import Worker
from temporalio.worker.workflow_sandbox import SandboxRestrictions, SandboxedWorkflowRunner

class Message(BaseModel):
    content: datetime


@workflow.defn
class Workflow:

    @workflow.run
    async def run(self, context: str) -> None:
        message = Message(content=workflow.now())
        print(f"Message in workflow with {context}: {message} (`content` type is{type(message.content)})\n")


def default_worker(client: Client, task_queue: str) -> Worker:
    return Worker(client, workflows=[Workflow], activities=[], task_queue=task_queue)


def relaxed_worker(client: Client, task_queue: str) -> Worker:
    restrictions = dataclasses.replace(
        SandboxRestrictions.default,
        invalid_module_members=dataclasses.replace(
            SandboxRestrictions.invalid_module_members_default,
            children={
                k: v for k, v in SandboxRestrictions.invalid_module_members_default.children.items() if k != "datetime"
            }
        )
    )
    runner = SandboxedWorkflowRunner(restrictions=restrictions)
    return Worker(client, workflows=[Workflow], activities=[], task_queue=task_queue, workflow_runner=runner)


async def main():
    task_queue = str(uuid4())
    client = await Client.connect("temporal:7233", namespace="default-namespace")

    async with default_worker(client, task_queue):
        context = "default sandbox"
        message = Message(content=datetime.utcnow())
        print(f"Message in main with {context}: {message} (`content` type is{type(message.content)})\n")
        handle = await client.start_workflow(Workflow.run, context, id=str(uuid4()), task_queue=task_queue)
        await handle.result()


    async with relaxed_worker(client, task_queue):
        context = "relaxed sandbox"
        message = Message(content=datetime.utcnow())
        print(f"Message in main with {context}: {message} (`content` type is{type(message.content)})\n")
        handle = await client.start_workflow(Workflow.run, context, id=str(uuid4()), task_queue=task_queue)
        await handle.result()


if __name__ == "__main__":
    asyncio.run(main())
```

The output from this snippet it:
```
Message in main with default sandbox: content=datetime.datetime(2022, 11, 18, 14, 13, 25, 326269) (`content` type is<class 'datetime.datetime'>)

Message in workflow with default sandbox: content=datetime.date(2022, 11, 18) (`content` type is<class 'datetime.date'>)

Message in main with relaxed sandbox: content=datetime.datetime(2022, 11, 18, 14, 13, 25, 360676) (`content` type is<class 'datetime.datetime'>)

Message in workflow with relaxed sandbox: content=datetime.datetime(2022, 11, 18, 14, 13, 25, 369645, tzinfo=datetime.timezone.utc) (`content` type is<class 'datetime.datetime'>)
```

**As you can see, the default sandbox runner is resulting in `content` being converted to just a `date` despite being instantiated with a `datetime`.**

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: M1 Pro Mac
- Temporal Version: 1.8.4 and SDK version 0.1b3 (Pydantic 1.10.2)
- Using Docker to run Temporal server
- Python Version: Python 3.10.7


#### Comments (13)

<details>
<summary><strong>michael-cybrid</strong> commented on 2022-11-18 14:27:34.000 UTC</summary>

Sorry, some additional context I forgot to mention.

If you install Pydantic with source only (i.e. without Cython binary), you get an error on startup:
```
Traceback (most recent call last):
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/_workflow.py", line 113, in __init__
    workflow_runner.prepare_workflow(defn)
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 53, in prepare_workflow
    self.create_instance(
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 87, in create_instance
    return _Instance(det, self._runner_class, self._restrictions)
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 107, in __init__
    self._create_instance()
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 118, in _create_instance
    self._run_code(
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 160, in _run_code
    exec(code, self.globals_and_locals, self.globals_and_locals)
  File "<string>", line 2, in <module>
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 199, in _import
    new_spec.loader.exec_module(new_mod)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/user/Development/temporal/worker-hang/main.py", line 6, in <module>
    from pydantic import BaseModel
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 201, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)
  File "<frozen importlib._bootstrap>", line 1129, in __import__
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/pydantic/__init__.py", line 2, in <module>
    from . import dataclasses
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 201, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)
  File "<frozen importlib._bootstrap>", line 1149, in __import__
  File "<frozen importlib._bootstrap>", line 1078, in _handle_fromlist
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/pydantic/dataclasses.py", line 56, in <module>
    from .error_wrappers import ValidationError
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 201, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)
  File "<frozen importlib._bootstrap>", line 1133, in __import__
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/pydantic/error_wrappers.py", line 4, in <module>
    from .json import pydantic_encoder
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 201, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)
  File "<frozen importlib._bootstrap>", line 1133, in __import__
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/pydantic/json.py", line 14, in <module>
    from .types import SecretBytes, SecretStr
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 201, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)
  File "<frozen importlib._bootstrap>", line 1133, in __import__
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/pydantic/types.py", line 764, in <module>
    class FilePath(Path):
TypeError: _RestrictedProxy.__init__() missing 1 required positional argument: 'matcher'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/user/Development/temporal/worker-hang/main.py", line 64, in <module>
    asyncio.run(main())
  File "/opt/homebrew/Cellar/python@3.10/3.10.7/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/homebrew/Cellar/python@3.10/3.10.7/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py", line 646, in run_until_complete
    return future.result()
  File "/Users/user/Development/temporal/worker-hang/main.py", line 47, in main
    async with default_worker(client, task_queue):
  File "/Users/user/Development/temporal/worker-hang/main.py", line 26, in default_worker
    return Worker(client, workflows=[Workflow], activities=[], task_queue=task_queue)
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/_worker.py", line 261, in __init__
    self._workflow_worker = _WorkflowWorker(
  File "/Users/user/.local/share/virtualenvs/worker-hang-grTap8Tq/lib/python3.10/site-packages/temporalio/worker/_workflow.py", line 117, in __init__
    raise RuntimeError(f"Failed validating workflow {defn.name}") from err
RuntimeError: Failed validating workflow Workflow
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-18 14:38:25.000 UTC</summary>

>  I am using the pydantic.json.pydantic_encoder

I do not see where this is used in the reproduction you provided. I don't see any custom data conversion.  Can you update the sample code? I will also test it to see why it's choosing one type and not the other. There is a fix we just made in #202 that may have affected type hints on either side of the sandbox.

By default we use Pydantic's `dict()` which doesn't always return a JSONable dict. Date time is not supported by our JSON encoder natively. We have to cross languages and date time is not a supported type in our other SDKs.

There is discussion of this at #143. But basically Pydantic doesn't provide a way to get a JSON-able dict but they've been actively talking about it lately. It provides a way to get JSON string (which we'd have to convert back at a perf loss) and a way to get a dict Pydantic can convert to JSON (but is not cross-language compatible for Temporal benefit).

I am planning on writing a sample at https://github.com/temporalio/samples-python/issues/25 that will show how to use full Pydantic JSON conversion.

> If you install Pydantic with source only (i.e. without Cython binary), you get an error on startup:

This is a known issue at https://github.com/temporalio/sdk-python#extending-restricted-classes and something we hope to fix. In the meantime you can mark `pydantic` as a pass-through module (we plan on making this easier too).

Also, feel free to join us in `#python-sdk` at https://temporal.io/slack or https://community.temporal.io too.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-18 14:46:37.000 UTC</summary>

I have replicated with your code and am looking in detail

</details>

<details>
<summary><strong>michael-cybrid</strong> commented on 2022-11-18 14:49:24.000 UTC</summary>

Thanks for the speedy reply! I was in the middle of typing out a response but see you are looking in detail. Thanks very much.

For context, I was hesitant to mention `I am using the pydantic.json.pydantic_encoder` because it is just context for how I came across this issue but you can see in the sample code, this behaviour is hit **without** any conversion. I am happy to show how I've got that bit working separately.

I just tried  adding `pydantic` to the `passthrough_modules` modules list but it still results in the same erroneous behaviour.

Update sample here:

```python
import asyncio
import dataclasses
from datetime import datetime
from uuid import uuid4

from pydantic import BaseModel
from temporalio import workflow
from temporalio.client import Client
from temporalio.worker import Worker
from temporalio.worker.workflow_sandbox import SandboxRestrictions, SandboxedWorkflowRunner, SandboxMatcher

class Message(BaseModel):
    content: datetime


@workflow.defn
class Workflow:

    @workflow.run
    async def run(self, context: str) -> None:
        message = Message(content=workflow.now())
        print(f"Message in workflow with {context}: {message} (`content` type is{type(message.content)})\n")


def default_worker(client: Client, task_queue: str) -> Worker:
    return Worker(client, workflows=[Workflow], activities=[], task_queue=task_queue)


def relaxed_worker(client: Client, task_queue: str) -> Worker:
    restrictions = dataclasses.replace(
        SandboxRestrictions.default,
        invalid_module_members=dataclasses.replace(
            SandboxRestrictions.invalid_module_members_default,
            children={
                k: v for k, v in SandboxRestrictions.invalid_module_members_default.children.items() if k != "datetime"
            }
        )
    )
    runner = SandboxedWorkflowRunner(restrictions=restrictions)
    return Worker(client, workflows=[Workflow], activities=[], task_queue=task_queue, workflow_runner=runner)


def passthrough_worker(client: Client, task_queue: str) -> Worker:
    restrictions = dataclasses.replace(
        SandboxRestrictions.default,
        passthrough_modules=SandboxRestrictions.passthrough_modules_default | SandboxMatcher(access={"pydantic"}),
    )
    runner = SandboxedWorkflowRunner(restrictions=restrictions)
    return Worker(client, workflows=[Workflow], activities=[], task_queue=task_queue, workflow_runner=runner)


async def run_the_bug(client: Client, task_queue: str, context: str) -> None:
    message = Message(content=datetime.utcnow())
    print(f"Message in main with {context}: {message} (`content` type is{type(message.content)})\n")
    handle = await client.start_workflow(Workflow.run, context, id=str(uuid4()), task_queue=task_queue)
    await handle.result()


async def main():
    task_queue = str(uuid4())
    client = await Client.connect("temporal:7233", namespace="default-namespace")

    async with default_worker(client, task_queue):
        await run_the_bug(client, task_queue, "default sandbox")

    async with relaxed_worker(client, task_queue):
        await run_the_bug(client, task_queue, "relaxed sandbox")

    async with passthrough_worker(client, task_queue):
        await run_the_bug(client, task_queue, "passthrough sandbox")


if __name__ == "__main__":
    asyncio.run(main())

```


</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-18 15:03:54.000 UTC</summary>

Here's my small reproduction:

```python
import asyncio
from datetime import datetime
from uuid import uuid4

from pydantic import BaseModel
from temporalio import workflow
from temporalio.exceptions import ApplicationError
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import Worker


class Message(BaseModel):
    content: datetime


@workflow.defn
class Workflow:
    @workflow.run
    async def run(self) -> None:
        message = Message(content=workflow.now())
        if not isinstance(message.content, datetime):
            raise ApplicationError(f"was type {type(message.content)}")


async def main():
    async with await WorkflowEnvironment.start_local() as env:
        task_queue = str(uuid4())
        async with Worker(env.client, workflows=[Workflow], task_queue=task_queue):
            await env.client.execute_workflow(
                Workflow.run, id=str(uuid4()), task_queue=task_queue
            )


if __name__ == "__main__":
    asyncio.run(main())
```

Debugging now...

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-18 18:58:27.000 UTC</summary>

I have hunted this issue down. This is caused by https://github.com/temporalio/sdk-python#is_subclass-of-abc-based-restricted-classes:

> Due to https://bugs.python.org/issue44847, classes that are wrapped and then checked to see if they are subclasses of another via is_subclass may fail (see also https://github.com/GrahamDumpleton/wrapt/issues/130).

Also see https://github.com/python/cpython/issues/89010

Basically at https://github.com/pydantic/pydantic/blob/bc74342b39e2b1d0115364cf2aa90d8f874cb9b5/pydantic/validators.py#L751, `pydantic` is checking `is_subclass` to determine whether it's `datetime` before falling through to `date`. This call fails for proxied objects. I'll look into workarounds, but in the meantime you'd have to remove `datetime` from being a restricted.

It might look something like (untested):
```python
my_restrictions = dataclasses.replace(
    SandboxRestrictions.default,
    invalid_module_members=SandboxMatcher(
        children={k: v for SandboxRestrictions.invalid_module_members_default.children.items() if k != "datetime"}
    ),
)
my_worker = Worker(..., workflow_runner=SandboxedWorkflowRunner(restrictions=my_restrictions))
```

But this removes our protections against calling something like `datetime.now()` which is forbidden in workflows, so be careful.

</details>

<details>
<summary><strong>michael-cybrid</strong> commented on 2022-11-21 14:19:46.000 UTC</summary>

Thanks, @cretz, for chasing this down.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-12-02 17:16:31.000 UTC</summary>

While a fix such as patching `issubclass` would fix this for non-binary Pydantic, installing the binary form bypasses any stdlib patches (and no I cannot use `__subclasscheck__` on my proxy type, because it is `datetime.__subclasscheck__` that is being called).

I still can't figure out how best to solve this. Researching...

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-12-02 17:47:22.000 UTC</summary>

For now, I have just marked this a known issue in the README in #219. If this becomes enough of an issue, we'll have to consider patching `datetime.__subclasscheck__`.

</details>

<details>
<summary><strong>iAnanich</strong> commented on 2024-01-27 03:32:01.000 UTC</summary>

Copilot brought me here...

My error looks slightly different:
`RuntimeError: Restriction state not present. Using subclasses of proxied objects is unsupported.`
is a cause of 
`RuntimeError: Failed validating workflow VerifyFaceWorkflowV1`

... and then discovered that python-samples contains data converter for Pydantic - that solved the issue for me.

Perhaps this issue could be closed?..

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-01-29 13:46:23.000 UTC</summary>

:+1: Will close.

For others arriving here, this is documented in the README and the supported way to use Pydantic is via a custom payload converter as shown in samples.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>jackklika</strong> commented on 2024-03-29 19:22:04.000 UTC</summary>

For future people investigating problems related to this: We ran into an error related to this and disabling sandboxing on the worker or passing through the datetime and pydantic (v1) imports here ultimately fixed it.

Disabling sandboxing on one workflow via the `sandboxed=False` did not work and we are unclear if this was because of  a sandboxed import occurring before importing into that non-sandboxed workflow, a top-level import on the file overriding something imported locally in the activity function, or something else. We need to look more into this ourselves.

Either way, the [guidance on the Readme added as a result of this issue](https://github.com/temporalio/sdk-python/pull/219/files#diff-b335630551682c19a781afebcf4d07bf978fb1f8ac04c6bf87428ed5106870f5R857) is accurate. 

Below is the error we got on importing a BaseModel in an activity. The error was specifically around a pydantic `Field` with a `default_factory` which instantiated a datetime.now(). I'm reposting it to aid anyone googling in the future and direct them to this issue.
```
  File "/usr/local/lib/python3.12/site-packages/temporalio/worker/_activity.py", line 438, in _run_activity
    result = await impl.execute_activity(input)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/site-packages/temporalio/worker/_activity.py", line 704, in execute_activity
    return await input.fn(*input.args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

   [...]

  # This call is when we import the pydantic BaseModel which is ultimately calling ModelMetaclass.__new__ here
  File "/usr/local/lib/python3.12/site-packages/pydantic/main.py", line 138, in __new__
    fields.update(smart_deepcopy(base.__fields__))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/site-packages/pydantic/utils.py", line 693, in smart_deepcopy
    return deepcopy(obj)  # slowest way when we actually might need a deepcopy
           ^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/copy.py", line 136, in deepcopy
    y = copier(x, memo)
        ^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/copy.py", line 221, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
                             ^^^^^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/copy.py", line 162, in deepcopy
    y = _reconstruct(x, memo, *rv)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/copy.py", line 259, in _reconstruct
    state = deepcopy(state, memo)
            ^^^^^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/copy.py", line 136, in deepcopy
    y = copier(x, memo)
        ^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/copy.py", line 201, in _deepcopy_tuple
    y = [deepcopy(a, memo) for a in x]
         ^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/copy.py", line 136, in deepcopy
    y = copier(x, memo)
        ^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/copy.py", line 221, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
                             ^^^^^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.12/copy.py", line 151, in deepcopy
    rv = reductor(4)
         ^^^^^^^^^^^
```

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>spacether</strong> commented on 2024-12-09 18:39:11.000 UTC</summary>

Could this be fixed by moving the issubclass for datetime check before the issubclass date check? What is the actual offending line that is causing this to happen?

</details>


---

<a id="792"></a>

### #792: [Bug] Wait_condition timeout makes tests impossible

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/792 |
| **State** | CLOSED |
| **Author** | Vaelatern (Toyam Cox) |
| **Created** | 2025-03-24 16:43:28.000 UTC (9 months ago) |
| **Updated** | 2025-03-26 12:53:56.000 UTC |
| **Closed** | 2025-03-26 12:53:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 12 |
| **Priority Score** | 12 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Have a workflow that times out when it has not received a signal in X hours. [works]

Test that workflow. [doesn't work]

### Describe the bug

```
$ uv run pytest
  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================================= test session starts ==============================================
platform linux -- Python 3.13.2, pytest-8.3.5, pluggy-1.5.0
rootdir: $(pwd)
configfile: pyproject.toml
plugins: asyncio-0.25.3
asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None
collected 2 items

test_main.py .                                                                                           [ 50%]
test_main_broken.py ^C

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! KeyboardInterrupt !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
/usr/lib/python3.13/selectors.py:452: KeyboardInterrupt
(to show a full traceback on KeyboardInterrupt use --full-trace)
======================================== 1 passed in 437.27s (0:07:17) =========================================
```

hangs forever until killed. See above where test_main_broken was killed after 7 minutes despite containing a sleep for `timedelta(minutes=1)` (and that shouldn't matter because: timeskip.)


### Minimal Reproduction

[Branch here ready to clone with minimal code](https://github.com/Vaelatern/-minimum-reproduction/tree/temporal-py-792)

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Linux, x86_64
- Temporal Version: Testing
- Straight python dependency as per the reproduction

### Additional context

Is there another way, using asyncio.sleep() instead, to have the same result?

#### Comments (12)

<details>
<summary><strong>cretz</strong> commented on 2025-03-25 12:37:49.000 UTC</summary>

Thanks for the replication! We are investigating...

Reactions: ðŸŽ‰ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-25 17:45:30.000 UTC</summary>

Ok, preliminary research has shown the issue here is that the `wait_condition` call is raising `TimeoutError` as expected. By default this exception does not fail the workflow but rather the workflow task (see https://github.com/temporalio/sdk-python?tab=readme-ov-file#exceptions) which effectively "suspends" the workflow until it is fixed. This type of workflow task failure is usually meant for code failures and not runtime ones like timeout. We chose to use `TimeoutError` here because this is just a wrapper around [asyncio.wait_for](https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for) which raises that error.

I will confer with the team to see if we want to consider allowing all cases of `TimeoutError` to actually be workflow failures instead of workflow task failures. In the meantime, you can change `@workflow.defn` to `@workflow.defn(failure_exception_types=[TimeoutError])` on the workflow itself, or pass in `workflow_failure_exception_types=[TimeoutError]` to the worker when creating it.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-25 18:22:06.000 UTC</summary>

I have opened #798

</details>

<details>
<summary><strong>Vaelatern</strong> commented on 2025-03-25 21:03:13.000 UTC</summary>

Thank you!

When I catch the exception in my minimum reproduction it works... unless I add the following discrete test:

```
import uuid
import pytest
from temporalio import workflow
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import Worker
from dataclasses import dataclass
from datetime import timedelta

@dataclass
class SayHelloArgs:
    name: str = ""

@workflow.defn
class SayHello:
    @workflow.run
    async def run(self, args: SayHelloArgs) -> str:
        self.signal_received = False
        try:
            await workflow.wait_condition(lambda: self.signal_received, timeout=timedelta(seconds=1))
        except asyncio.TimeoutError as e:
            pass
        return "Hello"


@pytest.mark.asyncio
async def test_empty_event_workflow():
    task_queue_name = str(uuid.uuid4())
    async with await WorkflowEnvironment.start_time_skipping() as env:

        async with Worker(
                env.client,
                task_queue=task_queue_name,
                workflows=[SayHello],
                ):
            handle = await env.client.start_workflow(
                    SayHello,
                    SayHelloArgs(),
                    id=str(uuid.uuid4()),
                    task_queue=task_queue_name,
                    )

            assert "Hello" == await handle.result()

```

</details>

<details>
<summary><strong>Vaelatern</strong> commented on 2025-03-25 21:35:47.000 UTC</summary>

Maybe what I'd really like is better introspection so I'd have a chance to look into why these tests are hanging myself

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-25 22:22:35.000 UTC</summary>

> unless I add the following discrete test

Not following what is meant here, are you saying this test does not pass?

> Maybe what I'd really like is better introspection so I'd have a chance to look into why these tests are hanging myself

Yes by default workflow task failures cause workflow suspension and often you need the UI or need to ask for the history to see that a workflow is suspended pending task failure. Setting the worker init option `workflow_failure_exception_types=[Exception]` will make any exception fail the workflow instead of the task which will raise an exception when trying to get workflow result.

</details>

<details>
<summary><strong>Vaelatern</strong> commented on 2025-03-25 22:23:15.000 UTC</summary>

Yes, the test I shared locks up as well.

</details>

<details>
<summary><strong>Vaelatern</strong> commented on 2025-03-25 22:27:51.000 UTC</summary>

I have updated the minimum reproduction branch with 2 commits. One fixes the first workflow, the second adds this other broken test.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-25 22:28:18.000 UTC</summary>

This does not happen for me. Taking your exact sample from https://github.com/Vaelatern/-minimum-reproduction/tree/temporal-py-792 and applying

```diff
diff --git a/main.py b/main.py
index 3e1619d..b4735c4 100644
--- a/main.py
+++ b/main.py
@@ -14,7 +14,10 @@ class WaitAMinuteWorkflow:
     @workflow.run
     async def run(self) -> str:
         self.is_poked = False
-        await workflow.wait_condition(lambda: self.is_poked, timeout=timedelta(minutes=1))
+        try:
+            await workflow.wait_condition(lambda: self.is_poked, timeout=timedelta(minutes=1))
+        except asyncio.TimeoutError as e:
+            pass
         return "all done"
 
     @workflow.signal
```

`uv run pytest` passes as expected

EDIT: I see your update, will test your latest commits

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-25 22:34:23.000 UTC</summary>

The error you are getting now is:

```
  File "path/to/minimum-reproduction/test_main_broken2.py", line 20, in run
    except asyncio.TimeoutError as e:
           ^^^^^^^
NameError: name 'asyncio' is not defined. Did you forget to import 'asyncio'?
```

I'd recommend passing `-k --log-cli-level=WARNING` so you can see log output, e.g. `uv run pytest -s --log-cli-level=WARNING -k test_empty_event_workflow`. This is the case where it is a legitimate task failure suspended waiting for a code fix, but yes task failures do not fail the workflow by default. As alluded to above, if you set `workflow_failure_exception_types=[Exception]` worker option on that test, the workflow will fail for all exceptions (including `NameError`).

</details>

<details>
<summary><strong>Vaelatern</strong> commented on 2025-03-26 05:23:06.000 UTC</summary>

That's the core issue! Finally worked out what was wrong in my original test (invoked activities that were not registered) by chasing down reasons like with this thread. 

Thank you! 

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-26 12:53:55.000 UTC</summary>

No problem! We have opened #798 to allow timeouts to cause workflow failures properly, but will otherwise close this issue (feel free to keep commenting or join us in `#python-sdk` on [Slack](https://t.mp/slack) or the forums).

Reactions: ðŸ‘ 1 ðŸŽ‰ 1

</details>


---

<a id="688"></a>

### #688: [Bug] Workflow sandbox issues with Protobuf

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/688 |
| **State** | OPEN |
| **Author** | Gr1N (Nikita Grishko) |
| **Created** | 2024-11-12 16:48:34.000 UTC (1y 1m ago) |
| **Updated** | 2025-03-11 17:02:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 12 |
| **Priority Score** | 12 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Hey!

I'm doing nothing special. I have one workflow and one activity. I use Protobuf messages as input parameters for both cases. I just expect to be able to execute the workflow and wait for it to be completed.

### Describe the bug

The issue happens only in the Kubernetes environment.

```
Traceback (most recent call last):
  File \"/usr/local/lib/python3.11/site-packages/temporalio/converter.py\", line 436, in from_payload
    return google.protobuf.json_format.Parse(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File \"/usr/local/lib/python3.11/site-packages/google/protobuf/json_format.py\", line 421, in Parse
    return ParseDict(js, message, ignore_unknown_fields, descriptor_pool,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File \"/usr/local/lib/python3.11/site-packages/google/protobuf/json_format.py\", line 446, in ParseDict
    parser.ConvertMessage(js_dict, message, '')
  File \"/usr/local/lib/python3.11/site-packages/google/protobuf/json_format.py\", line 487, in ConvertMessage
    self._ConvertFieldValuePair(value, message, path)
  File \"/usr/local/lib/python3.11/site-packages/google/protobuf/json_format.py\", line 563, in _ConvertFieldValuePair
    if _IsMapEntry(field):
       ^^^^^^^^^^^^^^^^^^
  File \"/usr/local/lib/python3.11/site-packages/google/protobuf/json_format.py\", line 151, in _IsMapEntry
    field.message_type.GetOptions().map_entry)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load
  File \"<frozen importlib._bootstrap>\", line 1130, in _find_and_load_unlocked
  File \"/usr/local/lib/python3.11/site-packages/temporalio/worker/workflow_sandbox/_importer.py\", line 393, in __getitem__
    return self.current[key]
           ~~~~~~~~~~~~^^^^^
KeyError: 'google.protobuf'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File \"/usr/local/lib/python3.11/site-packages/temporalio/converter.py\", line 306, in from_payloads
    values.append(converter.from_payload(payload, type_hint))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File \"/usr/local/lib/python3.11/site-packages/temporalio/converter.py\", line 442, in from_payload
    raise RuntimeError(f\"Unknown Protobuf type {message_type}\") from err
RuntimeError: Unknown Protobuf type internal.path.to.protobuf.TaskRequest

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File \"/usr/local/lib/python3.11/site-packages/temporalio/worker/_workflow_instance.py\", line 1625, in _convert_payloads
    return self._payload_converter.from_payloads(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File \"/usr/local/lib/python3.11/site-packages/temporalio/converter.py\", line 308, in from_payloads
    raise RuntimeError(
RuntimeError: Payload at index 0 with encoding json/protobuf could not be converted

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File \"/usr/local/lib/python3.11/site-packages/temporalio/worker/_workflow_instance.py\", line 364, in activate
    self._workflow_input = self._make_workflow_input(start_job)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File \"/usr/local/lib/python3.11/site-packages/temporalio/worker/_workflow_instance.py\", line 896, in _make_workflow_input
    args = self._convert_payloads(start_job.arguments, arg_types)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File \"/usr/local/lib/python3.11/site-packages/temporalio/worker/_workflow_instance.py\", line 1633, in _convert_payloads
    raise RuntimeError(\"Failed decoding arguments\") from err
```

### Minimal Reproduction

Unfortunately, I don't have steps for reproduction. Everything works perfectly in my and my colleague's local environments.

I would love to hear ideas or proposals on what to look into to understand the root cause. The only thing I can state is that if I add the following import, everything works:
```python
with workflow.unsafe.imports_passed_through():
    import google.protobuf
```

My other Protobuf imports (the result of code generation from Protobuf definitions) are already under unsafe context manager.

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Linux
- Temporal Version: 1.8.0
- Yes, Docker and Kubernetes

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (12)

<details>
<summary><strong>cretz</strong> commented on 2024-11-12 17:15:23.000 UTC</summary>

> Unfortunately, I don't have steps for reproduction. Everything works perfectly in my and my colleague's local environments.

Ideally we can replicate so we can help debug the problem. It is interesting that the exact same Python code does different things in different environments. It may be a protobuf version issue or an order of operations issue, but a standalone replication may be needed for us to confirm the issue.

> The only thing I can state is that if I add the following import, everything works:

Looking at the traceback, it's a bit of an interesting case where we catch `KeyError` because that's how we're checking a not-found protobuf, but this is actually having an import-level key error for importing `google.protobuf`. I wonder if it is lazily importing `google.protobuf` inside of `GetOptions` there somehow.

I can't really tell the problem without a replication. Can you confirm exact protobuf version (both library and protoc generator) and continually reduce the k8s-only form of replication until it's very small and standalone?

</details>

<details>
<summary><strong>Gr1N</strong> commented on 2024-11-12 17:23:00.000 UTC</summary>

Here are my versions:
```
protobuf 4.25.3
libprotoc 3.19.6
```

Here is an example of a workflow file:
```python
from temporalio import workflow

with workflow.unsafe.imports_passed_through():
    from temporalio.v1.lorem_pb2 import (
        LoremWorkflowRequest,
        LoremWorkflowResponse,
    )


@workflow.defn(name="lorem-workflow")
class LoremWorkflow:
    @workflow.run
    async def run(self, req: LoremWorkflowRequest) -> LoremWorkflowResponse:
        return LoremWorkflowResponse(text="i am response")
```

Could it be the issue with my Protobuf definitions and the fact that I use maps (but again, on local, all good)?
```protobuf
message LoremWorkflowRequest {
  map<string, bool> random = 1;
}
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-11-12 17:47:27.000 UTC</summary>

> Here is an example of a workflow file:

To confirm, the exact same code from Python start to end fails in one of your environments but not another? May need to see the full standalone code (i.e. how you are registering/running worker). There may be an operations order issue.

> Could it be the issue with my Protobuf definitions and the fact that I use maps (but again, on local, all good)?

I cannot know to be honest without replicating and debugging. The code you show above should work fine in every environment. I wonder if there are Python version differences or something else in the one environment where this happens that could help us figure out how to reliably replicate?

I think the main goal is to confirm the _exact_ same small code that fails in one environment but works in another. Even something as simple as the order something is imported may affect it which is why _exact_ code matters.

</details>

<details>
<summary><strong>Gr1N</strong> commented on 2024-11-12 19:27:05.000 UTC</summary>

Yep, I understand that providing insights without a reproducible example is almost impossible.

However, I figured out how to make my workflow workable. 

I had my Protobuf imports under unsafe context manager:
```python
with workflow.unsafe.imports_passed_through():
    from temporalio.v1.lorem_pb2 import (
        LoremWorkflowRequest,
        LoremWorkflowResponse,
    )
``` 

I changed that to the following, and now I see no exceptions:
```python
from temporalio.v1.lorem_pb2 import (
    LoremWorkflowRequest,
    LoremWorkflowResponse,
)
```

What is the right way? 

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-11-12 20:14:21.000 UTC</summary>

> What is the right way?

The first way with the imports passed through is usually better for models because with the second way you're re-importing the models on every single workflow run which costs memory and CPU. Having said that, we automatically mark any import beneath `temporalio` as pass through I believe, so a bit surprised there is a difference.

</details>

<details>
<summary><strong>trajano</strong> commented on 2025-03-09 16:03:25.000 UTC</summary>

Sounds like the same issue I have https://stackoverflow.com/questions/79495633/how-do-i-handle-json-protobuf-messages-in-temporal-python-sdk

But I think the problem is that type is not known by the time it gets to the converter for lookup as noted by `<unknown>`  on the message

```
site-packages\temporalio\converter.py", line 435, in from_payload
    value = _sym_db.GetSymbol(message_type)()
            ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
```

The message_type may not be known at that point because of 
```
message_type = payload.metadata.get("messageType", b"<unknown>").decode()
```

AFAIK python erases type information at runtime so I am not sure how you're supposed to determine that unless there's an undocumented annotation to say what the types are.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-10 13:36:01.000 UTC</summary>

> But I think the problem is that type is not known by the time it gets to the converter for lookup

The problem is that whatever is doing the protobuf conversion is not setting metadata on the payload properly. This would be a bit strange in all but the oldest Python SDK versions or in CLI. Can you show the raw payload in workflow history (can download still encoded history from UI)? How are you setting this value? Are you possibly using CLI?

> AFAIK python erases type information at runtime

No, type hints are available at runtime, but regardless, on deserialization we use a `messageType` metadata value that should be set

Also, instead of an existing GH issue or SO, there are also forums and `#python-sdk` channels on Slack if easier for you. I think your problem is unrelated to this issue which is about the sandbox.

</details>

<details>
<summary><strong>trajano</strong> commented on 2025-03-10 18:12:45.000 UTC</summary>

Are you talking about this?

```json
{
  "eventId": "1",
  "eventTime": "2025-03-10T18:11:26.549986494Z",
  "eventType": "WorkflowExecutionStarted",
  "taskId": "1049165",
  "userMetadata": {},
  "workflowExecutionStartedEventAttributes": {
    "workflowType": {
      "name": "GreetingWorkflowProtobuf"
    },
    "taskQueue": {
      "name": "foo",
      "kind": "TASK_QUEUE_KIND_NORMAL"
    },
    "input": {
      "payloads": [
        {
          "message": "Archie"
        }
      ]
    },
    "workflowTaskTimeout": "10s",
    "originalExecutionRunId": "cc2e7a38-8c5e-4ca7-806b-e441d0ad7514",
    "firstExecutionRunId": "cc2e7a38-8c5e-4ca7-806b-e441d0ad7514",
    "attempt": 1,
    "firstWorkflowTaskBackoff": "0s",
    "workflowId": "29e58714-ebe5-4f6b-b277-fe81215c5a58"
  },
  "links": []
}
```

> No, type hints are available at runtime, but regardless, on deserialization we use a messageType metadata value that should be set

I don't see that but it explicitly be set in `userMetadata` ? I don't recall having to do that when it's Java

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-10 19:06:40.000 UTC</summary>

> I don't see that but it explicitly be set in `userMetadata`? I don't recall having to do that when it's Java

Yes, Java was able to infer, but even modern Java SDK will set that metadata. But newer SDKs _require_ this field even when they can infer. How are you starting this workflow? Any recent SDK version you start it with using a protobuf message will set the proper metadata. Are you using CLI? If so, you'll need something like `--input-meta messageType=my.qualified.Proto`.

</details>

<details>
<summary><strong>trajano</strong> commented on 2025-03-11 01:31:18.000 UTC</summary>

@cretz I am starting through Temporal UI, the **Start workflow** button

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-11 15:32:19.000 UTC</summary>

When starting through the latest UI, you have to select `json/protobuf` as the encoding, which will then provide a "Message Type" field you must provide with the fully qualified proto message name. Most SDKs require this even if older SDKs/implementations don't for backwards compatibility reasons.

Note, this is unrelated to this GitHub issue which is specifically concerning the sandbox and protobuf. Can use Slack or forums for general help (or another GitHub issue if really needed), or you can open a cloud ticket if you're a cloud customer.

</details>

<details>
<summary><strong>trajano</strong> commented on 2025-03-11 17:02:12.000 UTC</summary>

@cretz  noted, it appears to be in the latest cloud UI, but not in the current dev-server.  I'll open up a related issue

</details>


---

<a id="190"></a>

### #190: [Feature Request] Upsert memo support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/190 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-04 19:43:03.000 UTC (3y 2m ago) |
| **Updated** | 2025-05-12 21:05:37.000 UTC |
| **Closed** | 2025-05-12 21:05:37.000 UTC |
| **Upvotes** | 5 |
| **Comments** | 2 |
| **Priority Score** | 12 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 5 |

#### Description

### Describe the solution you'd like

Think about how best to support upserting memo

#### Comments (2)

<details>
<summary><strong>aslanator</strong> commented on 2024-10-29 11:40:42.000 UTC</summary>

Any updates here?

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-29 12:27:28.000 UTC</summary>

It is on our relatively-near-term backlog, but no updates at this time

Reactions: ðŸ‘ 2

</details>


---

<a id="160"></a>

### #160: [Bug] OpenTelemetry interceptors report errors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/160 |
| **State** | OPEN |
| **Author** | anhdle14 (Le Anh Duc) |
| **Created** | 2022-10-19 18:14:32.000 UTC (3y 2m ago) |
| **Updated** | 2024-07-31 19:11:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 12 |
| **Priority Score** | 12 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

I am currently using OpenTelemetry and Temporal Interceptors to send traces to Grafana Tempo via Grafana Agent.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

```plaintext
Failed to detach context
Traceback (most recent call last):
  File "/app/.venv/lib/python3.10/site-packages/temporalio/contrib/opentelemetry.py", line 419, in _top_level_workflow_context
    yield None
  File "/app/.venv/lib/python3.10/site-packages/temporalio/contrib/opentelemetry.py", line 332, in execute_workflow
    return await super().execute_workflow(input)
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.10/site-packages/opentelemetry/context/__init__.py", line 157, in detach
    _RUNTIME_CONTEXT.detach(token)  # type: ignore
  File "/app/.venv/lib/python3.10/site-packages/opentelemetry/context/contextvars_context.py", line 50, in detach
    self._current_context.reset(token)  # type: ignore
ValueError: <Token var=<ContextVar name='current_context' default={} at 0x7f33b28ea750> at 0x7f33244d7e40> was created in a different Context
```

It just returns errors like this without any more detailed 

### Minimal Reproduction

```py
# worker.py

resource = Resource(attributes={SERVICE_NAME: "worker"})

provider = TracerProvider(resource=resource)
provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))
provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter(
    endpoint="tempo-us-central1.grafana.net:443",
    headers=[
        "authorization=Basic <REDACTED>"
    ]
)))

# Sets the global default tracer provider
trace.set_tracer_provider(provider)

# Creates a tracer from the global tracer provider
tracer = trace.get_tracer(__name__)


async def worker():
    client = await Client.connect(
        target_host="127.0.0.1:7233",
        namespace="default",
        interceptors=[TracingInterceptor(tracer=tracer)],
    )

    worker = Worker(
        client=client,
        task_queue="hello-query-task-queue",
        workflows=[GreetingWorkflow],
        activities=[compose_greeting],
    )
    
    await worker.run()


if __name__ == "__main__":
    asyncio.run(worker())
```

```py
# activity.py
resource = Resource(attributes={
    SERVICE_NAME: "python_temporal_opentelemetry_parallel"
})

provider = TracerProvider(resource=resource)
provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))
provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter()))

# Sets the global default tracer provider
trace.set_tracer_provider(provider)

# Creates a tracer from the global tracer provider
tracer = trace.get_tracer(__name__)


@activity.defn
async def say_hello_activity(name: str) -> str:
    return f"Hello, {name}!"


@workflow.defn
class SayHelloWorkflow:
    @workflow.run
    async def run(self) -> None:
        otel_workflow.completed_span("parallel_wf")
        # Run 5 activities at the same time

        # ...

async def main():
    # Start client
    client = await Client.connect(
        target_host="localhost:7233",
        interceptors=[TracingInterceptor(tracer)],
    )


    # While the worker is running, use the client to run the workflow and
    # print out its result. Note, in many production setups, the client
    # would be in a completely separate process from the worker.
    handle = await client.start_workflow(
        SayHelloWorkflow.run,
        id="hello-parallel-activity-workflow-id",
        task_queue="hello-activity-task-queue",
    )
        
    await handle
    # print(f"Result: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: AWS | EKS | Docker | Ubuntu
- Temporal version: `temporalio = "^0.1b2"`
- OTel version:
  ```
  opentelemetry-api = "=1.13.0"
  opentelemetry-sdk = "=1.13.0"
  opentelemetry-exporter-otlp = "=1.11.1"
  opentelemetry-instrumentation-pymongo = "=0.34b0"
  opentelemetry-instrumentation-logging = "=0.34b0"
  opentelemetry-instrumentation-redis = "=0.34b0"
  opentelemetry-instrumentation-fastapi = "=0.34b0"
  ```
- Are you using Docker or Kubernetes or building Temporal from source?

  Yes

### Additional context

N/A

<!-- Add any other context about the problem here. -->


#### Comments (12)

<details>
<summary><strong>cretz</strong> commented on 2022-10-19 18:41:17.000 UTC</summary>

Thanks for the reproduction! That really helps. I will hopefully get to this soon.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-03 14:07:02.000 UTC</summary>

@anhdle14 - While it takes me some time to get around to this, there is https://github.com/temporalio/samples-python/tree/main/open_telemetry that does work fine. Can you confirm? Also note that OTLP package that uses an older version of protobuf may not work with ours at this time.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2023-05-22 16:06:12.000 UTC</summary>

I'm seeing this as well in my tests with `temporalio==1.2.0, opentelemetry-api==1.17.0, `opentelemetry-sdk==`.17.0`

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-22 16:22:16.000 UTC</summary>

@nathanielobrown - Can you describe what you're trying to do exactly? Are you trying to use OpenTelemetry from a workflow or in your workflow file? Can you pass through the import?

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2023-05-22 16:33:18.000 UTC</summary>

I'm importing `from temporalio.contrib.opentelemetry import TracingInterceptor` and then using this interceptor for all my clients and workers. For the client I do have another interceptor I created for Sentry:
```python
def get_worker_interceptors() -> Sequence[Interceptor]:
    return [SentryInterceptor(), TracingInterceptor()]


def get_client_interceptors() -> Sequence[ClientInterceptor]:
    return [TracingInterceptor()]
```
I haven't got tracing working well yet, so I'm no expert/still a bit lost in the otel stuff, but I can say I see this log output in tests that run a workflow. Maybe for all tests that run workflows, not 100% sure:

```
ERROR:opentelemetry.context:Failed to detach context
Traceback (most recent call last):
  File "/Users/nob/Library/Caches/pypoetry/virtualenvs/dd-6mCrh732-py3.10/lib/python3.10/site-packages/temporalio/contrib/opentelemetry.py", line 427, in _top_level_workflow_context
    yield None
  File "/Users/nob/Library/Caches/pypoetry/virtualenvs/dd-6mCrh732-py3.10/lib/python3.10/site-packages/temporalio/contrib/opentelemetry.py", line 340, in execute_workflow
    return await super().execute_workflow(input)
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/nob/Library/Caches/pypoetry/virtualenvs/dd-6mCrh732-py3.10/lib/python3.10/site-packages/opentelemetry/context/__init__.py", line 163, in detach
    _RUNTIME_CONTEXT.detach(token)  # type: ignore
  File "/Users/nob/Library/Caches/pypoetry/virtualenvs/dd-6mCrh732-py3.10/lib/python3.10/site-packages/opentelemetry/context/contextvars_context.py", line 50, in detach
    self._current_context.reset(token)  # type: ignore
ValueError: <Token var=<ContextVar name='current_context' default={} at 0x1104908b0> at 0x168a7fc40> was created in a different Context
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-22 16:40:58.000 UTC</summary>

Is that in a workflow file? Have you made sure you pass through the import?

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2023-05-22 16:45:46.000 UTC</summary>

Not sure I totally follow, but I can say I've been using `UnsandboxedWorkflowRunner` so haven't been worrying about "pass through" imports. My definition of getting the interceptors and creating workers are in separate modules from my workflows. Does that answer the question?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-22 17:30:14.000 UTC</summary>

> Does that answer the question?

Yes, if you're not using the sandbox then it must be some other issue. We have unit tests for OTel at https://github.com/temporalio/sdk-python/blob/main/tests/contrib/test_opentelemetry.py, but the original users' sample recreates the tracer twice and yours using Sentry + OTel which shouldn't matter. I will try to create a small reproducible test case when I can get to this issue.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2023-06-20 11:22:01.000 UTC</summary>

Just wanted to jot down that I think this might just be an issue _on worker exit_. This happens all over the place in my tests, but I'm creating+shutting down workers all over the place as well. In production I see a lot of these errors when a worker is killed.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-06-20 12:41:28.000 UTC</summary>

Yeah, there's an issue with detach context on generator exit (i.e. when event loop is GC'd). May be similar to https://github.com/open-telemetry/opentelemetry-python/issues/2606. I have to probably just swallow detachment failure.

</details>

<details>
<summary><strong>anhdle14</strong> commented on 2023-06-20 23:34:48.000 UTC</summary>

> but the original users' sample recreates the tracer twice and yours using Sentry + OTel which shouldn't matter. I will try to create a small reproducible test case when I can get to this issue.

Sorry, I don't follow this, should I not create the tracer twice?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-07-05 13:23:26.000 UTC</summary>

> Sorry, I don't follow this, should I not create the tracer twice?

Correct. You are already setting it globally in `worker.py` that imports `activity.py`, you don't need to set it in `activity.py`. You are also including the workflow in with the activity and I would recommend that workflows be a separate file. You should not run global code in the workflow file (it is sandboxed and will run every workflow run). See README.

</details>


---

<a id="1048"></a>

### #1048: [Feature Request] Support raising cancellation in sync multiprocessed activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1048 |
| **State** | OPEN |
| **Author** | gregbrowndev (Greg Brown) |
| **Created** | 2025-08-24 12:17:29.000 UTC (4 months ago) |
| **Updated** | 2025-11-07 20:55:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 11 |
| **Priority Score** | 11 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

Currently, activities running on a multi-processed worker can not be cancelled, either voluntarily by the user/workflow or involuntarily e.g. by a missed heartbeat (or activity timeout, I think?).

This leads to wasted resources and potential pool starvation, as the activity will run to completion anyway just to get the error below and subsequently retried:

> Activity not found on completion. This may happen if the activity has already been cancelled but completed anyway. 

Additionally, in the case of missed heartbeats / timeout, the server will show the activity has failed and will retry the activity. Users might not expect or protect against this behaviour, leading to possible data corruption (e.g. if the activity was trying to write to a specific file) or duplicate side effects (e.g. if the activity did something at the end, like send an email). 

Reproduction: [hello_cancellation.py](https://github.com/gregbrowndev/samples-python/blob/4f61efdb875ca3d4f92c69ff6fa172ab858611cb/hello/hello_cancellation.py#L66-L81) shows a sync, multi-processed activity running forever in a while loop, preventing the worker from shutting down after the workflow was cancelled.

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

- Cancellation should be best effort in sync, multi-processed workers, like it is for async and multithreaded workers, with the known expectation that a cancellation cannot be received unless the activity is instrumented with heartbeats. 

- Provide clearer documentation in [Interrupt a Workflow Execution](https://docs.temporal.io/develop/python/cancellation#cancellation) and/or [Python SDK sync-vs-async](https://docs.temporal.io/develop/python/python-sdk-sync-vs-async) that sync, multiprocess workers do not currently support cancellation
- Provide guidance to avoid problematic side effects in such activities, e.g.
  - ensure output is written to a unique location or some kind of resource lock is acquired by the activity, 
  - trigger "only-once" side effects in subsequent activities in the workflow, e.g. sending an email (and that this requires a separate async or multithreaded worker / task queue)
  - etc.

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->

Other relevant work/issues:

- https://github.com/temporalio/sdk-python/pull/217
- https://github.com/temporalio/sdk-python/issues/1047

Root cause:

- Activity cancellation upon a missed heartbeat or timeout happens here in [_ActivityWorker](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/worker/_activity.py#L276)
- This sets various cancellation properties/events in [_RunningActivity](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/worker/_activity.py#L628-L629)
- For sync, multiprocessed activities, the `cancelled_event` is [initialised with a multiprocessing Manager event](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/worker/_activity.py#L451-L459) that can communicate across the process boundary.
- Multi-threaded activities initialise the `_ThreadExceptionRaiser` in [_execute_sync_activity](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/worker/_activity.py#L817-L820) with the thread ID, but this isn't done for the multi-processed case (I imagine it doesn't work the same way).
- So it looks like the `cancelled_event` is set and can be observed within the child process via [_Context](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/worker/_activity.py#L831-L833), but there is currently no built-in way for the user activity implementation to react to this event.

I also assume this affects the ["cancelled due to activity pause"](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/worker/_activity.py#L319-L336), at least after the activity has already started?

Out of interest, how does the the [_handle_start_activity_task](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/worker/_activity.py#L319-L336) work in this case? Does the `_ActivityWorker` continue to poll for tasks for an activity it has already picked up, as seems to be [suggested by the cancel task](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/worker/_activity.py#L166-L168)? Does the server somehow ensure these cancel tasks are sticky, so the correct worker instance that picked up the start task can process them? I'm assuming this mechanism only works for newly picked up activities, so `start` was never triggered. If not though, why isn't this mechanism also used for heartbeat failures?

Cheers,

I hope this assessment is accurate ðŸ™ðŸ» 



#### Comments (11)

<details>
<summary><strong>cretz</strong> commented on 2025-08-26 12:43:52.000 UTC</summary>

For cancel, we leverage interruption. It is easy for us to cancel/interrupt asyncio and we use an advanced Python C function to interrupt a thread, but for non-thread-pool executors, there is no clear way to interrupt processing. But utilities like `activity.is_cancelled()` and `activity.wait_for_cancelled_sync()` should work. If you use multiprocess activities, you'll have to manually check for cancellation due to there being no clear way to interrupt non-thread-pool executor (of which process-pool is one type).

</details>

<details>
<summary><strong>gregbrowndev</strong> commented on 2025-08-26 21:23:17.000 UTC</summary>

thanks @cretz for the explanation. 

Just so I understand, the reason the `_ThreadExceptionRaiser` / underlying C function approach cannot be used for the multi-processed worker is because:

1. We cannot serialise `_ThreadExceptionRaiser` across the process pool, or
2. The C function only works in the current Python process (i.e. in the parent process where it is called)? or
3. The exception raised by `_ThreadExceptionRaiser` requires a kind of wait point, similar to the asyncio `CancelledError`. Since CPU bound work may take a long time before hitting a OS call, e.g. `sleep`, etc. we wouldn't able to rely on this to interrupt the thread in a timely manner?

The problem with manually checking if the task is cancelled is because it might not be viable to do so, e.g. in a tight loop (check every iteration?) or calling into some long-running library function outside of your control. For the same reason, I do the heartbeats in a separate thread following the async autoheartbeater example, so I can send one periodically. But then you still have the same issue, how to interrupt the main cpu-bound thread from the io-bound heartbeat/cancellation-checking thread?

One possible solution could be to use [signals](https://docs.python.org/3/library/signal.html) to interrupt the child process? This seems to work fairly well:

https://gist.github.com/gregbrowndev/ea3e699224185591c10c4f68fb0e65b9



</details>

<details>
<summary><strong>cretz</strong> commented on 2025-08-27 12:48:50.000 UTC</summary>

> Just so I understand, the reason the _ThreadExceptionRaiser / underlying C function approach cannot be used for the multi-processed worker is because

I think the reason is more simply - we don't know what executor is being used so we can't guess at how to interrupt code.

> For the same reason, I do the heartbeats in a separate thread following the async autoheartbeater example, so I can send one periodically. But then you still have the same issue, how to interrupt the main cpu-bound thread from the io-bound heartbeat/cancellation-checking thread?

This is general purpose code at this point, and only you know how best to interrupt your Python code. How might you interrupt this Python code if it weren't a Temporal activity? _Technically_ you maybe can reuse `temporalio.bridge.runtime.Runtime._raise_in_thread`, though it is untested in child process and we can't guarantee compatibility. You might be able to signal the child.

> One possible solution could be to use [signals](https://docs.python.org/3/library/signal.html) to interrupt the child process?

We can't even be sure the user is using multiprocessing, this code path is just for the non-threadpool executor, not specifically process pool executor.

</details>

<details>
<summary><strong>gregbrowndev</strong> commented on 2025-08-30 15:18:23.000 UTC</summary>

Thanks again for explaining. I think the penny just dropped for me that you're talking about third-party, possibly distributed executors rather than just the standard library ones (I didn't know there were any others).

I do think it would be sensible and valuable for the SDK to support both standard library executors equally in time and add missing features such as this. Particularly now that I know its technically possible, its just the code path treats all the other executors as sync, non-threaded.

I can see this issue, like the other ProcessPool executor features I requested, will a bit further back on the priority list for the time being. So happy to leave it here. But let me know if contributions are welcome and I could pick it up.


</details>

<details>
<summary><strong>cretz</strong> commented on 2025-09-02 15:17:10.000 UTC</summary>

> I think the penny just dropped for me that you're talking about third-party, possibly distributed executors rather than just the standard library ones

It's not just that there are third party executors, even with process pool executor, there is no obvious, single interruption approach in Python I don't believe. How would you interrupt a process-pool executor based function/task in normal Python if Temporal weren't involved? Is the approach universally what people would want? It is also important that it can be traditionally caught, shielded from, etc.

If you need advanced logic beyond the default, what you may want is to use an `async def` activity and make your own `run_in_executor` call to your process pool executor, and wait for cancellation and apply it to your process-pool-based future how you prefer best. This is effectively what we do, with some added capabilities to send heartbeats and back to the parent process and cancel info to the child process.

</details>

<details>
<summary><strong>gregbrowndev</strong> commented on 2025-09-03 15:03:15.000 UTC</summary>

I understand the concern, but I think there's a reasonable solution simply flipping your argument around:

Why not handle `ProcessPoolExecutor` out of the box in a somewhat opinionated, Temporal-aware way, and then if the user requires more advanced executor logic beyond the default, they can run a custom executor within an async/threaded worker?

---

I think all of the issues you raised are solvable without too much change:

> How would you interrupt a process-pool executor based function/task in normal Python if Temporal weren't involved? 

I think they should be interrupted in an analogous way as the async and threaded workers, i.e. the interruption is hands off, driven by the cancellation message received by the ActivityWorker, and doesn't require user code to explicitly check which may be infeasible. 

Like I showed in the Gist above, using `os.pkill(pid, SIGINT)` looks as analogous to me as what the threaded worker's  `_ThreadExceptionRaiser` low-level C function is doing. All it does it raise a `KeyboardInterrupt` in process specified by the `pid`. It doesn't destroy the actual process, it can be reused for further work. Its no different from any other uncaught exception in the task. We could go further and register a `SIGINT` signal handler in the process raise a `temporalio.exceptions.CancelledError` to be consistent with the sync, threaded approach.

> Is the approach universally what people would want?

I think this would be a sensible default, consistent with the behaviour of the other workers. If its not the exact approach the user wants, they could run their own executor within a async/threaded worker as mentioned. I find it hard to imagine this behaviour wouldn't be the expectation of most users coming from the other two main worker types.

> It is also important that it can be traditionally caught, shielded from, etc.

The exception raised by `os.pkill(pid, SIGINT)` in the child process would be a `KeyboardInterrupt` by default, but can be handled to raise a different exception. Its raised in the main thread of the process, and can be caught, ignored, reraised, etc. by normal Python code.

In the gist, I didn't handle shielding but this could be handled by backing the shield depth with the `multiprocess.Manager`. 

Reactions: ðŸ‘ 2

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-09-03 15:41:09.000 UTC</summary>

> Why not handle ProcessPoolExecutor out of the box in a somewhat opinionated, Temporal-aware way, and then if the user requires more advanced executor logic beyond the default, they can run a custom executor within an async/threaded worker?

There hasn't really been enough desire for explicit process pool executor support. It's a fairly significant challenge to maintain these abstractions if they are rarely used.

There is actually some regret in supporting non-thread-pool based executors because they are much less commonly used and it's usually best for a user to control how they want to handle multiprocessing instead of Temporal choose. No other Temporal SDK supports multiprocessing. By having Temporal just execute a function and a user choose how to delegate their logic (multiprocess, sidecars, microservices, etc), we prevent issues such as our existing opinionated abstraction not sharing the same opinions as different users (e.g. not choosing a specific interruption approach, or not choosing to explicitly support process pools, or not making OTel work natively across a process boundary, etc).

There is concern about heaping on additional logic/expectations onto this lesser used non-thread-pool abstraction compared to just documenting its limitations and encouraging users to exercise more explicit control over the delegation of work/interruption/contexts from their own functions.

</details>

<details>
<summary><strong>ikseek</strong> commented on 2025-11-06 00:35:56.000 UTC</summary>

We use billiard.Pool adapted to ProcessPoolExecutor interface to run temporal sync activities that can segfault.
Is there a supported way I can hook into activity cancelation in such worker to send a SIGINT or in worst case SIGKILL to the subprocess executing the sync activity?
Thanks.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-11-07 14:43:53.000 UTC</summary>

Inside the activity itself, you can wait on cancellation via `activity.wait_for_cancellation_sync()` in a background thread. It will return when the activity has been cancelled (worker shutdown or server-sent cancel assuming you are properly heartbeating) at which point the process can exit itself if it'd like. There is no way outside the activity to get the cancel, but you can use a regular `async def` activity and do https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor to your process pool executor yourself and on asyncio cancellation do something to that started execution (or just use `def` and process pool executor or subprocessing in asyncio or threaded directly).

</details>

<details>
<summary><strong>ikseek</strong> commented on 2025-11-07 19:25:23.000 UTC</summary>

@cretz Thanks for the response!
Unfortunately wait_for_cancelled_sync raises `RuntimeError("Not in activity context")` when called not in the thread where activity executes.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-11-07 20:55:03.000 UTC</summary>

> Unfortunately wait_for_cancelled_sync raises RuntimeError("Not in activity context") when called not in the thread where activity executes.

This is touched on kinda at https://github.com/temporalio/sdk-python?tab=readme-ov-file#activity-context where it mentions the activity context uses https://docs.python.org/3/library/contextvars.html which do not span threads by default. You will have to use something like `copy_context` + `.run` to run code inside the same context.

</details>


---

<a id="686"></a>

### #686: [Feature Request] Timeout in Workflow definition not in execution

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/686 |
| **State** | CLOSED |
| **Author** | cosminpm (Cosmin) |
| **Created** | 2024-11-07 10:52:26.000 UTC (1y 2m ago) |
| **Updated** | 2024-12-30 10:27:56.000 UTC |
| **Closed** | 2024-11-21 07:28:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 11 |
| **Priority Score** | 11 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Iâ€™m trying to set a timeout for a Workflow, but Iâ€™m unsure if Iâ€™m missing documentation or if this might be a feature request.

I know itâ€™s possible to set a timeout when executing a workflow directly, like this:
```python
result = await client.execute_workflow(
    YourWorkflow.run,
    "your timeout argument",
    id="your-workflow-id",
    task_queue="your-task-queue",
    # Set Workflow Timeout duration
    execution_timeout=timedelta(seconds=2),
    # run_timeout=timedelta(seconds=2),
    # task_timeout=timedelta(seconds=2),
)
```

However, Iâ€™m running into an issue: I canâ€™t set `execution_timeout` when the workflow is triggered by a Schedule, as my workflows are executed by a worker:

```python
worker = Worker(
    self.client,
    task_queue=TASK_QUEUE,
    workflows=[
        Workflow1,
        Workflow2,
        Workflow3,
    ],
    activities=[
        activity_1,
        activity_2,
        activity_3,
    ],
)
```

Iâ€™ve reviewed the documentation and examples, but havenâ€™t found a way to configure a workflow timeout outside of the direct execution method.
### Describe the solution you'd like

Or the workflow has a timeout parameter in the definition or the worker, both of them would solve my issue.

### Additional context

N/A

#### Comments (11)

<details>
<summary><strong>cretz</strong> commented on 2024-11-12 15:11:27.000 UTC</summary>

This is a server side timeout set when the workflow is started, and that start is done by the internal scheduler on schedules. How are you creating the schedule? The workflow schedule action allows `execution_timeout`, so that's where you'd set this. Also, unless you're sure you need it, you should consider using timers _inside_ your workflow that you can react to for timing things out instead of the server-side timeout that terminates a workflow with no recourse.

</details>

<details>
<summary><strong>cosminpm</strong> commented on 2024-11-13 08:15:22.000 UTC</summary>

Hello @cretz!

Thanks for answering.

We are creating the schedule via de UI by passing the Workflow definition which does not provide an execution timeout as far as I can see from the available parameters.  

![image](https://github.com/user-attachments/assets/170ed29e-a0d7-4094-94de-1d5842b2c99d)

I'm implementing workflow timeouts due to an issue where, occasionally, our workflow remains in a _"Running"_ status indefinitely, with zero pending activities. This occurs despite having timeouts set at the activity level, and I havenâ€™t yet identified the root cause.

This is how I'm defining the workflows:

```python
with workflow.unsafe.imports_passed_through():
    from app.module import activity_1


@workflow.defn
class Workflow1:
    @workflow.run
    async def run(self) -> Any:
        return await workflow.execute_activity(
            activity_1_activity,
            start_to_close_timeout=timedelta(minutes=5),
        )


@activity.defn
async def activity_1_activity() -> Any:
    return activity_1()
```

Am I missing something?

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-11-13 14:29:35.000 UTC</summary>

> We are creating the schedule via de UI

This seems to be an issue with the UI not providing every workflow option and not related to Python SDK. I will confer with the UI team to see if we have plans here. In the meantime, you could consider creating the schedule via the CLI or programmatically in the Python SDK to get the extra options you need.

> I'm implementing workflow timeouts due to an issue where, occasionally, our workflow remains in a "Running" status indefinitely, with zero pending activities. This occurs despite having timeouts set at the activity level, and I havenâ€™t yet identified the root cause.

This is worth identifying instead of just setting workflow execution timeouts. Your activity timeout is only set for each attempt (as opposed to schedule to close timeout), so a failing activity will retry indefinitely there any look like it's hanging (see the UI to see if the pending activity is performing many attempts).

Also, for general assistance with activity timeouts and such, feel free to visit our [forums](https://community.temporal.io/) or `#python-sdk` on [Slack](https://t.mp/slack).

</details>

<details>
<summary><strong>cosminpm</strong> commented on 2024-11-14 07:51:54.000 UTC</summary>

Hello @cretz.

Thanks for answering! Regarding the UI team, do I have to open a request there _(if so can you provide a link)_ or do we keep this issue open?

> you could consider creating the schedule via the CLI or programmatically in the Python SDK to get the extra options you need.

Maybe I'm wrong but as far as I know creating a Workflow does not provide the option to set a timeout or at least in the [documentation](https://docs.temporal.io/develop/python/core-application#workflow-type) that's not specified, the only references are at Activity level.

The other option for Workflow timeouts are in the [Failure detection section](https://docs.temporal.io/develop/python/failure-detection), but I think this _(got from the official docs)_ would not work:
``` python
# ...
    result = await client.execute_workflow(
        YourWorkflow.run,
        "your timeout argument",
        id="your-workflow-id",
        task_queue="your-task-queue",
        # Set Workflow Timeout duration
        execution_timeout=timedelta(seconds=2),
        # run_timeout=timedelta(seconds=2),
        # task_timeout=timedelta(seconds=2),
    )
```

As it forces to execute the workflow in your code and my workflows are executed via a schedule. And in the schedule [documentation](https://docs.temporal.io/develop/python/schedules) there are no references about workflow timeouts.

> This is worth identifying instead of just setting workflow execution timeouts. Your activity timeout is only set for each attempt (as opposed to schedule to close timeout), so a failing activity will retry indefinitely there any look like it's hanging (see the UI to see if the pending activity is performing many attempts).

Indeed, but it's a very strange issue, as sometimes the workflow executes correctly but from now and then it just loops in Running without any pending activity, if we terminate the workflow and we wait for the next workflow execution it succeeds. So it's a non-deterministic behavior. For this reason as a fast solution would be nice to have a Workflow timeout with Schedules. 


For sure I may missed something and it's a mistake from my side, but it would be nice if you could help me! 

Thanks!

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-11-14 13:53:55.000 UTC</summary>

> And in the schedule [documentation](https://docs.temporal.io/develop/python/schedules) there are no references about workflow timeouts.

The schedule documentation doesn't mention every workflow option since many are the same as regular workflow start. The documentation at https://docs.temporal.io/develop/python/schedules#create shows an example of creating a schedule, you can add workflow options to `ScheduleActionStartWorkflow` as needed, including `execution_timeout` (see API documentation at https://python.temporal.io/temporalio.client.ScheduleActionStartWorkflow.html).

</details>

<details>
<summary><strong>cosminpm</strong> commented on 2024-11-20 13:57:59.000 UTC</summary>

Thanks @cretz.

Some final questions:
1. Do you have an ETA for when the timeout schedule parameter will be implemented in the UI? If not, can you provide a ticket to follow up on this?
2. I understand that timeouts can be specified via the CLI at the schedule level, but is there a way to define them at the Workflow level without using this option:
```python
    result = await client.execute_workflow(
        YourWorkflow.run,
        "your timeout argument",
        id="your-workflow-id",
        task_queue="your-task-queue",
        # Set Workflow Timeout duration
        execution_timeout=timedelta(seconds=2),
        # run_timeout=timedelta(seconds=2),
        # task_timeout=timedelta(seconds=2),
    )
```

Thanks!


</details>

<details>
<summary><strong>cretz</strong> commented on 2024-11-20 16:11:47.000 UTC</summary>

> Do you have an ETA for when the timeout schedule parameter will be implemented in the UI? If not, can you provide a ticket to follow up on this?

I do not have an ETA and there may be discussions on use cases the UI team would want to have with you and confirm whether it's something they want to expose. Instead of in the Python GitHub issue, I would recommend starting a UI discussion in a more general setting, e.g. at https://community.temporal.io/ or in Slack (https://t.mp/slack) or, if you're a cloud customer, open a ticket.

> I understand that timeouts can be specified via the CLI at the schedule level, but is there a way to define them at the Workflow level without using this option:

Can you clarify the question a bit? The `execution_timeout` option is available both for create schedule in Python (when you use the workflow action I mentioned above) and for just starting a workflow directly in Python. What is the issue with using the `execution_timeout` option?

</details>

<details>
<summary><strong>cosminpm</strong> commented on 2024-11-20 17:17:25.000 UTC</summary>

Yes for sure, I have requested it in the [#web-ui channel](https://temporalio.slack.com/archives/C011AAP3GFP/p1732122532648729). Thanks!

2. Well, my point if we could have something like this:
```python
from datetime import timedelta
from typing import Any
from temporalio import workflow, activity

with workflow.unsafe.imports_passed_through():
    from app.module import activity_1


@workflow.defn
class Workflow1:
    def __init__(self, timeout: timedelta = timedelta(minutes=5)): # <- HERE
        self.timeout = timeout

    @workflow.run
    async def run(self) -> Any:
        return await workflow.execute_activity(
            activity_1_activity,
            start_to_close_timeout=timedelta(minutes=5),
        )


@activity.defn
async def activity_1_activity() -> Any:
    return activity_1()
```

When the scheduler executes the workflow, it knows what is the timeout for each one, and this timeout it's defined at Workflow level, this would also simplify this code as you won't need to specify it in the client.

``` python
    result = await client.execute_workflow(
        YourWorkflow.run,
        "your timeout argument",
        id="your-workflow-id",
        task_queue="your-task-queue",
        # Set Workflow Timeout duration
        execution_timeout=timedelta(seconds=2), # <- HERE
        # run_timeout=timedelta(seconds=2),
        # task_timeout=timedelta(seconds=2),
    )
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-11-20 18:20:39.000 UTC</summary>

Still not following exactly. If you want timeout inside workflow code, you can have (untested):

```python
@dataclass
class Workflow1Args:
    timeout: int

@workflow.defn
class Workflow1:
    async def run(self, args: Workflow1Args) -> None:
        try:
            async with asyncio.timeout(args.timeout):
                # This will get canceled on timeout
                my_other_code
        except TimeoutError: # Note, may be something else depending on how cancel handled, make sure to test
            raise ApplicationError('timeout')
```

We recommend this approach over `execution_timeout` because `execution_timeout` is server side and you can't react to it. But if you must have server side timeout (e.g. you may not have a worker running), then no amount of in-workflow timeout will help because the workflow code may never be reached.

If you just want to create a schedule with an execution timeout, just:

```python
    await client.create_schedule(
        "workflow-schedule-id",
        Schedule(
            action=ScheduleActionStartWorkflow(
                YourSchedulesWorkflow.run,
                "my schedule arg",
                id="schedules-workflow-id",
                task_queue="schedules-task-queue",
                execution_timeout=timedelta(seconds=2)
            ),
            spec=ScheduleSpec(
                intervals=[ScheduleIntervalSpec(every=timedelta(minutes=2))]
            ),
            state=ScheduleState(note="Here's a note on my Schedule."),
        ),
    )
```

(though a 2-second timeout may be a bit aggressive)

</details>

<details>
<summary><strong>cosminpm</strong> commented on 2024-11-21 07:28:44.000 UTC</summary>

Thanks! I think this solves my question, now I will be following on the UI problem.

</details>

<details>
<summary><strong>cosminpm</strong> commented on 2024-12-30 10:27:33.000 UTC</summary>

Follow up issue: https://github.com/temporalio/ui/issues/2460

</details>


---

<a id="238"></a>

### #238: [Bug] Can't debugging workflow using vscode

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/238 |
| **State** | OPEN |
| **Author** | niros1 |
| **Created** | 2023-01-03 21:38:49.000 UTC (3 years ago) |
| **Updated** | 2024-11-16 16:40:38.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 5 |
| **Priority Score** | 11 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 3 |

#### Description


when run a workflow with vscode debugger attach I get the flowwing error as soon a workflow start.

```File "/usr/local/lib/python3.7/site-packages/debugpy/_vendored/pydevd/pydevd.py", line 1715, in process_internal_commands
    int_cmd.do_it(self)
  File "/usr/local/lib/python3.7/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_comm.py", line 542, in do_it
    self.method(dbg, *self.args, **self.kwargs)
  File "/usr/local/lib/python3.7/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_constants.py", line 511, in new_func
    with warnings.catch_warnings():
  File "/usr/local/lib/python3.7/warnings.py", line 458, in __init__
    self._module = sys.modules['warnings'] if module is None else module
  File "/usr/local/lib/python3.7/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 393, in __getitem__
    return self.current[key]
KeyError: 'warnings'
```



### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [Linux]
- Temporal Version: Python sdk 0.1b4
- Are you using Docker or Kubernetes or building Temporal from source? Nor


#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2023-01-03 21:48:23.000 UTC</summary>

I will investigate this as part of https://github.com/temporalio/sdk-python/issues/213. In the meantime you can disable the sandbox during debug (you may also want to set `debug=True` for the worker so you don't hit deadlock timeouts).

</details>

<details>
<summary><strong>niros1</strong> commented on 2023-01-04 08:10:18.000 UTC</summary>

10X a lot - I will try it and update

</details>

<details>
<summary><strong>niros1</strong> commented on 2023-01-04 11:54:39.000 UTC</summary>

workflow_runner=workflow_instance.UnsandboxedWorkflowRunner()
Workaround it.

Reactions: ðŸŽ‰ 3

</details>

<details>
<summary><strong>MauSant</strong> commented on 2024-01-05 02:37:03.000 UTC</summary>

> workflow_runner=workflow_instance.UnsandboxedWorkflowRunner() Workaround it.

Thank you!

It worked for me with the following code: 

```
from temporalio.client import Client  
from temporalio.worker import Worker, UnsandboxedWorkflowRunner  

Worker(
        client,
        task_queue=some_task_queue,
        activities=[],
        workflows=[SomeWorkflow],
        debug_mode=True,
        workflow_runner=UnsandboxedWorkflowRunner()
    ):






Reactions: ðŸ‘ 5

</details>

<details>
<summary><strong>challapradyumna</strong> commented on 2024-11-16 16:39:27.000 UTC</summary>

Created a new worker bypassing the `_pydevd_bundle` module. Works as of now. 

```
def new_sandbox_runner() -> SandboxedWorkflowRunner:
    # TODO(cretz): Use with_child_unrestricted when https://github.com/temporalio/sdk-python/issues/254
    # is fixed and released
    invalid_module_member_children = dict(
        SandboxRestrictions.invalid_module_members_default.children
    )
    del invalid_module_member_children["datetime"]
    return SandboxedWorkflowRunner(
        restrictions=dataclasses.replace(
            SandboxRestrictions.default.with_passthrough_modules("_pydevd_bundle"),
            invalid_module_members=dataclasses.replace(
                SandboxRestrictions.invalid_module_members_default,
                children=invalid_module_member_children,
            ),
        )
    )
```

used the above as a workflow runner as below

```
   worker = Worker(
        client,
        workflow_runner=new_sandbox_runner(),
        task_queue="queue",
        workflows=[Workflow],
        activities=[activity],
        max_concurrent_workflow_tasks = 1,
        max_concurrent_activities= 1
    )
```

Reactions: â¤ï¸ 1

</details>


---

<a id="893"></a>

### #893: [Bug] Pyright (strict mode) fails on `start`/`execute` activity/workflow calls

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/893 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2025-06-06 22:37:30.000 UTC (7 months ago) |
| **Updated** | 2025-10-02 20:40:56.000 UTC |
| **Closed** | 2025-10-02 20:40:56.000 UTC |
| **Upvotes** | 4 |
| **Comments** | 2 |
| **Priority Score** | 10 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 4 |

#### Description

<img width="1364" alt="Image" src="https://github.com/user-attachments/assets/b98a2c85-ad5d-4b41-b1b1-4e94a2b0b6ab" />

```
Type of "execute_activity" is partially unknown
  Type of "execute_activity" is "Overload[(activity: CallableAsyncNoParam[ReturnType@execute_activity], *, task_queue: str | None = None, schedule_to_close_timeout: timedelta | None = None, schedule_to_start_timeout: timedelta | None = None, start_to_close_timeout: timedelta | None = None, heartbeat_timeout: timedelta | None = None, retry_policy: RetryPolicy | None = None, cancellation_type: ActivityCancellationType = ActivityCancellationType.TRY_CANCEL, activity_id: str | None = None, versioning_intent: VersioningIntent | None = None, summary: str | None = None, priority: Priority = temporalio.common.Priority.default) -> Coroutine[Any, Any, ReturnType@execute_activity], (activity: CallableSyncNoParam[ReturnType@execute_activity], *, task_queue: str | None = None, schedule_to_close_timeout: timedelta | None = None, schedule_to_start_timeout: timedelta | None = None, start_to_close_timeout: timedelta | None = None, heartbeat_timeout: timedelta | None = None, retry_policy: RetryPolicy | None = None, cancellation_type: ActivityCancellationType = ActivityCancellationType.TRY_CANCEL, activity_id: str | None = None, versioning_intent: VersioningIntent | None = None, summary: str | None = None, priority: Priority = temporalio.common.Priority.default) -> Coroutine[Any, Any, ReturnType@execute_activity], (activity: CallableAsyncSingleParam[ParamType@execute_activity, ReturnType@execute_activity], arg: ParamType@execute_activity, *, task_queue: str | None = None, schedule_to_close_timeout: timedelta | None = None, schedule_to_start_timeout: timedelta | None = None, start_to_close_timeout: timedelta | None = None, heartbeat_timeout: timedelta | None = None, retry_policy: RetryPolicy | None = None, cancellation_type: ActivityCancellationType = ActivityCancellationType.TRY_CANCEL, activity_id: str | None = None, versioning_intent: VersioningIntent | None = None, summary: str | None = None, priority: Priority = temporalio.common.Priority.default) -> Coroutine[Any, Any, ReturnType@execute_activity], (activity: CallableSyncSingleParam[ParamType@execute_activity, ReturnType@execute_activity], arg: ParamType@execute_activity, *, task_queue: str | None = None, schedule_to_close_timeout: timedelta | None = None, schedule_to_start_timeout: timedelta | None = None, start_to_close_timeout: timedelta | None = None, heartbeat_timeout: timedelta | None = None, retry_policy: RetryPolicy | None = None, cancellation_type: ActivityCancellationType = ActivityCancellationType.TRY_CANCEL, activity_id: str | None = None, versioning_intent: VersioningIntent | None = None, summary: str | None = None, priority: Priority = temporalio.common.Priority.default) -> Coroutine[Any, Any, ReturnType@execute_activity], (activity: (...) -> Awaitable[ReturnType@execute_activity], *, args: Sequence[Any], task_queue: str | None = None, schedule_to_close_timeout: timedelta | None = None, schedule_to_start_timeout: timedelta | None = None, start_to_close_timeout: timedelta | None = None, heartbeat_timeout: timedelta | None = None, retry_policy: RetryPolicy | None = None, cancellation_type: ActivityCancellationType = ActivityCancellationType.TRY_CANCEL, activity_id: str | None = None, versioning_intent: VersioningIntent | None = None, summary: str | None = None, priority: Priority = temporalio.common.Priority.default) -> Coroutine[Any, Any, ReturnType@execute_activity], (activity: (...) -> ReturnType@execute_activity, *, args: Sequence[Any], task_queue: str | None = None, schedule_to_close_timeout: timedelta | None = None, schedule_to_start_timeout: timedelta | None = None, start_to_close_timeout: timedelta | None = None, heartbeat_timeout: timedelta | None = None, retry_policy: RetryPolicy | None = None, cancellation_type: ActivityCancellationType = ActivityCancellationType.TRY_CANCEL, activity_id: str | None = None, versioning_intent: VersioningIntent | None = None, summary: str | None = None, priority: Priority = temporalio.common.Priority.default) -> Coroutine[Any, Any, ReturnType@execute_activity], (activity: str, arg: Any = temporalio.common._arg_unset, *, args: Sequence[Any] = [], task_queue: str | None = None, result_type: Type[Unknown] | None = None, schedule_to_close_timeout: timedelta | None = None, schedule_to_start_timeout: timedelta | None = None, start_to_close_timeout: timedelta | None = None, heartbeat_timeout: timedelta | None = None, retry_policy: RetryPolicy | None = None, cancellation_type: ActivityCancellationType = ActivityCancellationType.TRY_CANCEL, activity_id: str | None = None, versioning_intent: VersioningIntent | None = None, summary: str | None = None, priority: Priority = temporalio.common.Priority.default) -> Coroutine[Any, Any, Any]]"
```


In VSCode, Pyright needs to be set to "strict" mode for this to be flagged.

```json
"python.analysis.typeCheckingMode": "strict"
```

#### Comments (2)

<details>
<summary><strong>zlalvani</strong> commented on 2025-06-26 23:56:08.000 UTC</summary>

Hi there @dandavison ! Do you know if this is being prioritized? 

</details>

<details>
<summary><strong>Sushisource</strong> commented on 2025-07-01 16:35:13.000 UTC</summary>

Let's do https://github.com/temporalio/sdk-python/issues/625 and https://github.com/temporalio/sdk-python/issues/795 at the same time

</details>


---

<a id="733"></a>

### #733: [Bug] Unable to run workflows with OpenTelemetry and ddtrace

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/733 |
| **State** | OPEN |
| **Author** | connected-bkiiskila (Benjamin Kiiskila) |
| **Created** | 2025-01-16 16:08:34.000 UTC (11 months ago) |
| **Updated** | 2025-08-04 15:32:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 10 |
| **Priority Score** | 10 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm attempting to enable tracing on my worker using the TracingInterceptor and the tracer provided by ddtrace.

### Describe the bug

A change upstream with the OpenTelemetry SDK seems to have caused an issue with Temporal's sandbox environment, causing an error stating os.environ.get isn't allowed in a workflow.

The relevant change on the OpenTelemetry side is [here](https://github.com/open-telemetry/opentelemetry-python/commit/52abb610dbdf1751cc745720055119ca1656ec16) - if I run with a version prior to 1.29 then I no longer get an error.

### Minimal Reproduction

A workflow implemented on a client with the TracingInterceptor attached and a valid OpenTelemetry SDK configured, in my case I'm using ddtrace with the DD_TRACE_OTEL_ENABLED environment variable set to true. From what I can tell though the call that isn't allowed in the sandbox is coming directly from the OpenTelemetry SDK and not the ddtrace patch.

### Environment/Versions

- OS and processor: macOS Apple Silicon
- Temporal Version: SDK 1.9.0
- Running in Docker arm64

### Additional context

You can see the stacktrace and error that's displayed on the Temporal UI here: https://gist.github.com/connected-bkiiskila/d11592cb86271b5f343a4d387097d418


#### Comments (10)

<details>
<summary><strong>cretz</strong> commented on 2025-01-16 17:40:59.000 UTC</summary>

Thanks! Hrmmm...

Looking at your stack trace, it appears at https://github.com/DataDog/dd-trace-py/blob/eddd51464e8c32db019e239c106317228b65667c/ddtrace/internal/opentelemetry/context.py#L24-L26 `opentelemetry.baggage` is and others are unfortunately imported at _runtime_ which means you may be inadvertently re-importing things for every workflow run. You should be able to add `import opentelemetry.baggage` somewhere outside of your workflow (e.g. where you start the worker) to run the module initialization code that `ddtrace` is running lazily. This will also improve your performance. `ddtrace` (reasonably) assumes modules are cached and that `import`s are not rerun in other environments, but that's not the case with workflows.

Though I am a bit surprised our `import opentelemetry.baggage.propagation` doesn't imply this. You may _also_ have to configure the sandbox restrictions to treat these OTel imports as pass through (https://github.com/temporalio/sdk-python?tab=readme-ov-file#passthrough-modules). It will require testing...

</details>

<details>
<summary><strong>connected-bkiiskila</strong> commented on 2025-01-16 17:42:43.000 UTC</summary>

I'll give that extra import a shot and see if that works and report back

</details>

<details>
<summary><strong>connected-bkiiskila</strong> commented on 2025-01-16 18:22:52.000 UTC</summary>

Hmmm, that doesn't seem to work either - only thing I'm having luck with is the downgrade to <1.29

For that passthrough documentation, would I just be flat out allowing os.environ.get or is there a way to allow specifically the call within the opentelemetry SDK? 

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-01-16 20:25:36.000 UTC</summary>

> For that passthrough documentation, would I just be flat out allowing os.environ.get or is there a way to allow specifically the call within the opentelemetry SDK?

The passthrough would be for passing through the imports, unrelated to the restrictions happening on `os.environ.get`. What's happening now, from your stack trace, is that `from opentelemetry.baggage import get_all` is being run _at runtime_ in the workflow. We want that to have already been cached. You can probably just add this to the top of your _workflow file_:

```
with workflow.unsafe.imports_passed_through():
    import opentelemetry.baggage
```

But this is untested. Even if we do allow `os.environ.get` through the sandbox, the bigger problem is that you are reimporting OTel libraries for _every workflow run_ which you don't want to do.

</details>

<details>
<summary><strong>connected-bkiiskila</strong> commented on 2025-01-16 21:45:18.000 UTC</summary>

Seems like that <1.29 fix was a fluke and works some of the time for whatever reason so I'm likely going to have to allow the import of os.environ.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-01-16 21:50:05.000 UTC</summary>

We may also be able to look into it, though it seems to be `ddtrace` specific in that they are importing at runtime. I do think it is worth doing your best to prevent this import from being treated as a new import for every workflow run (since that means it runs all OTel bootstrap code every run and takes up extra memory).

</details>

<details>
<summary><strong>connected-bkiiskila</strong> commented on 2025-01-16 21:51:07.000 UTC</summary>

Yeah I'd like to avoid it if possible, would you suggest creating a ticket on the ddtrace side and referencing this?

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-01-16 22:04:14.000 UTC</summary>

`ddtrace` is not really doing anything wrong as far as a normal Python library goes. https://github.com/DataDog/dd-trace-py/blob/eddd51464e8c32db019e239c106317228b65667c/ddtrace/internal/opentelemetry/context.py#L24-L26 is just a problem in Temporal workflows. But we need to get those imports preloaded and passed through. You may be able to do something like:

```python
import opentelemetry.baggage
import opentelemetry.trace

my_worker = Worker(
  ...,
  workflow_runner=SandboxedWorkflowRunner(
    restrictions=SandboxRestrictions.default.with_passthrough_modules("opentelemetry", "ddtrace")
  )
)
```

when you create your worker.

</details>

<details>
<summary><strong>TheCodeWrangler</strong> commented on 2025-07-30 17:48:31.000 UTC</summary>

I have been seeing similar errors in a deployment I have where datadog sidecars were enabled.

Is this something we would need to patch into all our application workers 

```
with workflow.unsafe.imports_passed_through():
    ...
```

OR is there something that could/should be done at the `python-sdk` level to solve this for all users?

>But we need to get those imports preloaded

@cretz is this something you guys plan on implementing?  If you give some clear guidance on what needs done i could possibly draft a PR for it



</details>

<details>
<summary><strong>cretz</strong> commented on 2025-08-04 14:59:42.000 UTC</summary>

I don't think we have plans to support `ddtrace` specifically at this time. I do think you need to pass through the import of `ddtrace` if it is used in workflow code, including in workflow interceptors which are just workflow code. There may be other sandbox avoidance things that need to happen depending on the details of what `ddtrace` tries to do.

</details>


---

<a id="700"></a>

### #700: [Bug] No clean way to cancel an activity and wait until it's cancelled

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/700 |
| **State** | OPEN |
| **Author** | andmis (Andrey Mishchenko) |
| **Created** | 2024-12-09 19:54:17.000 UTC (1y 1m ago) |
| **Updated** | 2025-05-14 09:27:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 10 |
| **Priority Score** | 10 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

Start an activity from a workflow, cancel it, wait for clean cancellation acknowledgement, exit the workflow.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

There appear to be several issues.
1. If you call `handle.cancel()` on an activity handle from the workflow, the `CancelledError` will only be raised in the activity _after_ an `activity.heartbeat()` call, _followed_ by an `await something()` call. Otherwise the activity itself doesn't seem to receive `CancelledError`. This kinda sucks -- both that you need to call `heartbeat()` for cancellation requests to get through, and the fact that long-running coros do not get interrupted correctly. I don't understand the underlying implementation, but if the reason we need to `heartbeat()` is that workers don't want to poll for cancellation, still, when you call `activity.heartbeat()`, the client library could check for cancellation and immediately call `task.cancel()` on the task running the activity, no? Currently the work-around seems to be to litter activity code with `asyncio.sleep(0.1)`.
2. From the workflow code, if you want to cleanly wait for the activity cancellation, you have to catch `ActivityError`, which is a broader exception type than I actually want to catch (presumably there are other `ActivityError`s which I don't want to catch).
3. If you use `WAIT_CANCELLATION_COMPLETED`, if you try to cancel an activity that's currently awaiting a long-running coro (like `asyncio.sleep(10)`), the activity won't receive a `CancelledError` until it sends a heartbeat, which it can't do until the coro it's waiting on finishes, so your workflow won't finish until the activity's long-running coro finishes.

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

https://github.com/andmis/snippets/tree/temporal-python-sdk-cannot-cleanly-cancel-activities

Using `python run_workflow.py` with no args, the workflow exits promptly, but the activity never receives `CancelledError`, completes, and we get warning log spam:
```
2024-12-09 19:44:06.116611+00:00 (Workflow) Starting, use_sleep=False use_wait=False
2024-12-09 11:44:06.124587 (Activity) Running sandbox_activity
2024-12-09 19:44:07.130422+00:00 (Workflow) Cancelling activity
2024-12-09 19:44:07.130422+00:00 (Workflow) Activity cancelled
2024-12-09 19:44:07.130422+00:00 (Workflow) Exiting
2024-12-09 11:44:12.868185 (Activity) Completing sandbox_activity
2024-12-09T19:44:12.881460Z  WARN temporal_sdk_core::worker::activities: Activity not found on completion. This may happen if the activity has already been cancelled but completed anyway. task_token=TaskToken(CiRhOTc2ZWZkMy1iM2NiLTQwZmMtOWYzZi1jNjk0MzU2NjMzN2ESEHNhbmRib3gtd29ya2Zsb3caJDVhMjAyNWI3LWRhY2MtNDg4OC04NjA5LWI3NGQ3MzI2MzA3YyAFKAEyATFCEHNhbmRib3hfYWN0aXZpdHlKCAgBEIeKQBgB) details=Status { code: NotFound, message: "workflow execution already completed", details: b"\x08\x05\x12$workflow execution already completed\x1aB\n@type.googleapis.com/temporal.api.errordetails.v1.NotFoundFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
```

Using `python run_workflow.py -w`, the workflow waits until the activity's long-running `sleep` finishes, despite the activity being cancelled (note timestamps), and the activity completes rather than being cancelled:
```
2024-12-09 19:45:15.119427+00:00 (Workflow) Starting, use_sleep=False use_wait=True
2024-12-09 11:45:15.127826 (Activity) Running sandbox_activity
2024-12-09 19:45:16.130733+00:00 (Workflow) Cancelling activity
2024-12-09 11:45:24.371849 (Activity) Completing sandbox_activity
2024-12-09 19:45:24.384356+00:00 (Workflow) Exiting
```

Using `python run_workflow.py -w -s`, the workflow waits for the long-running coro in the activity to finish, which is bad, and the activity does cancel rather than completing, but `-s` sucks:
```
2024-12-09 19:47:00.283375+00:00 (Workflow) Starting, use_sleep=True use_wait=True
2024-12-09 11:47:00.291593 (Activity) Running sandbox_activity
2024-12-09 19:47:01.301447+00:00 (Workflow) Cancelling activity
2024-12-09 11:47:06.547612 (Activity) Cancelling sandbox_activity
2024-12-09 19:47:06.551643+00:00 (Workflow) Activity cancelled
2024-12-09 19:47:06.551643+00:00 (Workflow) Exiting
```

Using `python run_workflow.py -s` results in the workflow exiting cleanly and promptly (since we aren't using `WAIT_CANCELLATION_COMPLETED`), and the activity cancels rather than completing, but still waits on the long-running coro:
```
2024-12-09 19:48:40.102363+00:00 (Workflow) Starting, use_sleep=True use_wait=False
2024-12-09 11:48:40.110083 (Activity) Running sandbox_activity
2024-12-09 19:48:41.121725+00:00 (Workflow) Cancelling activity
2024-12-09 19:48:41.121725+00:00 (Workflow) Activity cancelled
2024-12-09 19:48:41.121725+00:00 (Workflow) Exiting
2024-12-09 11:48:47.629316 (Activity) Cancelling sandbox_activity
```

## Environment/Versions

OS and processor: macOS, M1
Temporal version: 1.1.2
Python SDK version: 1.8.0
Are you using Docker or Kubernetes or building Temporal from source? No


#### Comments (10)

<details>
<summary><strong>andmis</strong> commented on 2024-12-09 20:04:34.000 UTC</summary>

Sorry if this report is a bit garbled -- I'd say the real issue is "there does not seem to be a clean, blessed way of doing this".

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-12-09 20:41:04.000 UTC</summary>

> If you call handle.cancel() on an activity handle from the workflow, the CancelledError will only be raised in the activity after an activity.heartbeat() call, followed by an await something() call. Otherwise the activity itself doesn't seem to receive CancelledError.

This is how Python asyncio cancellation works. It only can issue a cancel at the await point. The snippet you show completes the activity immediately after calling the non-blocking heartbeat, so I would not expect it to fail because it has completed.

> From the workflow code, if you want to cleanly wait for the activity cancellation, you have to catch ActivityError, which is a broader exception type than I actually want to catch (presumably there are other ActivityErrors which I don't want to catch).

Use the `__cause__` (aliased as `cause`) to differentiate

> the activity won't receive a CancelledError until it sends a heartbeat, which it can't do until the coro it's waiting on finishes, so your workflow won't finish until the activity's long-running coro finishes.

This is the nature of heartbeating. The server only relays cancellation on heartbeating to the worker, so if you're not heartbeating you're not receiving cancellation. You can run heartbeating in a background task or you can do something like sleep for a short period and heartbeat every so often. Also, heartbeats are throttled to within 0.8 times the heartbeat timeout anyways but you are not setting the heartbeat timeout so it uses a really high default.

See:

* https://docs.temporal.io/develop/python/failure-detection#activity-heartbeats
* https://docs.temporal.io/encyclopedia/detecting-activity-failures#activity-heartbeat
* https://github.com/temporalio/sdk-python?tab=readme-ov-file#heartbeating-and-cancellation

Etc to better understand how heartbeating and activity cancellation work together.

</details>

<details>
<summary><strong>andmis</strong> commented on 2024-12-09 20:44:24.000 UTC</summary>

**edit:** ignore this post, i misunderstood something above.

---

> This is how Python asyncio cancellation works. It only can issue a cancel at the await point. The snippet you show completes the activity immediately after calling the non-blocking heartbeat, so I would not expect it to fail because it has completed.

It's possible for `task.cancel()` to interrupt a coro such as `sleep`:

<img width="406" alt="image" src="https://github.com/user-attachments/assets/b6d1891e-76cc-4065-bbd1-703811c8e031">


</details>

<details>
<summary><strong>cretz</strong> commented on 2024-12-09 20:54:15.000 UTC</summary>

> It's possible for task.cancel() to interrupt a coro such as sleep:

Right, if `use_sleep` is set, that makes sense, it was just a bit strange to see an activity that could heartbeat and then complete immediately. But regardless, to the later point, in this case you are choosing to sleep without heartbeating which means the worker never even gets notified while sleeping that the activity is cancelling. You should heartbeat while sleeping if you want to cancel while sleeping.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>andmis</strong> commented on 2024-12-09 21:01:29.000 UTC</summary>

IIUC the point of the activity issuing a heartbeat is to let the server know it's still healthy. it sounds like your advice is that i could create a separate `asyncio.Task` and heartbeat from that task, and then i would get `CancelledError` interrupting my long-running coro.

so IIUC, `activity.heartbeat()` basically means `send_heartbeat_to_server_and_check_for_cancellation()`. is that right?

</details>

<details>
<summary><strong>andmis</strong> commented on 2024-12-09 21:06:19.000 UTC</summary>

i'd suggest noting in the docs the exact semantics here.

these docs make it sound like you can just `handle.cancel()` and that will work: https://python.temporal.io/temporalio.workflow.ActivityHandle.html

it sounds like the semantics are something like this, i'd appreciate clarification as well:
- call `handle.cancel()`
- from your activity code, call `activity.heartbeat()`
- after `handle.cancel()` followed by `acvitity.heartbeat()`, we will call `task.cancel()` on the root `asyncio.Task` running your activity

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-12-09 21:50:58.000 UTC</summary>

> these docs make it sound like you can just handle.cancel() and that will work

In Python asyncio, calling cancel on a task doesn't immediately cancel always (or at all), it's up to the implementation for how to handle. Some tasks may swallow cancel, some shield, some may do cleanup first, or in our case we communicate it with a server. But it does work in that the request is sent to the server and put on history (assuming activity is not complete), it's just not necessarily processed immediately. We will see about clarifying in the docs that Python asyncio Task cancellation (and explicit Temporal workflow activity cancellation in all Temporal languages) is actually a cancellation request.

> after handle.cancel() followed by acvitity.heartbeat(), we will call task.cancel() on the root asyncio.Task running your activity

Yes, though there are of course caveats concerning heartbeat throttling so it's not always immediately on heartbeating.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>spacether</strong> commented on 2025-03-31 23:35:42.000 UTC</summary>

It looks like [activity file handles extend asyncio.Task](https://python.temporal.io/temporalio.workflow.ActivityHandle.html).
So can one use workflow.wait_condition until activity_handle.canceled() is True?
hmm that did not work for us, but checking for activity_handle.done worked

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-01 13:13:25.000 UTC</summary>

>  So can one use workflow.wait_condition until activity_handle.canceled() is True?

The proper way is to wait on the activity task/handle itself, catching the `ActivityError` and checking that the `cause` is `CancelledError`, though yes `done()` may work. As for cancelled helpers on activity and child workflow tasks, I opened #810 on that.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>teije01</strong> commented on 2025-05-14 09:27:40.000 UTC</summary>

The way we went about this is the following:

```python
try:
    await workflow.execute_xxx(...)
except exceptions.ActivityError as e:
    if e.retry_state == exceptions.RetryState.CANCEL_REQUESTED:
        handle_explicit_cancellation_teardown()
```

or for a full activity example (heartbeat util from: https://github.com/temporalio/samples-python/blob/main/custom_decorator/activity_utils.py)
```python
import asyncio
import datetime as dt

from temporalio import activity, exceptions, workflow

class HelloGoodbyeActivities:
    def __init__(self):
        pass

    @activity.defn
    @heartbeat_utils.periodic_heartbeater
    async def say_hello(self):
        """Demo activity for saying hello."""
        activity.logger.info("activity=say_hello fired")
        for _ in range(5):
            activity.logger.info("Hello!")
            await asyncio.sleep(10)

    @activity.defn
    async def say_goodbye(self):
        """Demo activity for saying goodbye."""
        activity.logger.info("Goodbye!")

    @classmethod
    async def hello_goodbye(cls):
        try:
            await workflow.execute_activity_method(
                cls.say_hello,
                start_to_close_timeout=dt.timedelta(minutes=1),
                heartbeat_timeout=dt.timedelta(seconds=10),
            )
            await workflow.execute_activity_method(
                cls.say_goodbye, start_to_close_timeout=dt.timedelta(seconds=10)
            )
        except exceptions.ActivityError as e:
            if e.retry_state == exceptions.RetryState.CANCEL_REQUESTED:
                workflow.logger.exception(
                    f"Workflow canceled: {type(e.cause)}({e}), {e.retry_state=}"
                )
                await workflow.execute_activity_method(
                    cls.say_goodbye, start_to_close_timeout=dt.timedelta(seconds=10)
                )
            else:
                workflow.logger.exception(
                    f"Workflow failed: {e.cause} {type(e.cause)}({e}), {e.retry_state=}"
                )
            raise
        except Exception as e:
            workflow.logger.exception(f"Unknown exception: {e}, {e.__dict__}")



@workflow.defn
class SayHello:
    @workflow.run
    async def run(self) -> None:
        return await HelloGoodbyeActivities.hello_goodbye()
```

This example would always get to the say_goodbye activity, no matter when it was cancelled (even after a worker restart).

</details>


---

<a id="300"></a>

### #300: [Bug] Investigate PyGILState_Release issue for client

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/300 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-03-22 14:13:35.000 UTC (2y 9m ago) |
| **Updated** | 2024-11-20 23:03:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 10 |
| **Priority Score** | 10 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

On short-lived client-only process after `execute_workflow` client call completes, one user reported getting:

```
Fatal Python error: PyGILState_Release: thread state 0x***************** must be current when releasing
Python runtime state: finalizing (tstate=0x*****************)
Thread 0x****************** (most recent call first):
  <no Python frame>
```

Maybe this is caused by process death while waiting on client call complete? Try to replicate.

There is some discussion at https://github.com/PyO3/pyo3/issues/1274 that predates pyo3-asyncio. Maybe I am not implementing our custom Tokio pyo3 asyncio extension properly?

It looks like we shouldn't be calling `Python::with_gil` in callbacks (i.e. not in Python-owned thread) for any reason, so we need to work around that. But https://pyo3.rs/main/ecosystem/async-await.html#awaiting-a-rust-future-in-python shows it used in a callback.

First thing is a replication, then we can see whether an pyo3-asyncio upgrade can help.

#### Comments (10)

<details>
<summary><strong>TiunovNN</strong> commented on 2024-09-25 09:47:15.000 UTC</summary>

Hi @cretz . I've came across the similar problem. Here is a simple code to reproduce bug:

```python
import asyncio
import logging

from temporalio.client import Client
from temporalio.exceptions import WorkflowAlreadyStartedError
from temporalio.runtime import (LoggingConfig, Runtime, TelemetryConfig, TelemetryFilter)


async def connect_client(host):
    temporal_client: Client = await Client.connect(
        host,
        namespace='default',
        runtime=Runtime(
            telemetry=TelemetryConfig(
                logging=LoggingConfig(
                    filter=TelemetryFilter(
                        core_level='DEBUG',
                        other_level='DEBUG',
                    ),
                ),
            ),
        ),
    )

    try:
        await temporal_client.start_workflow('PackagerWorkflow', 'test', id='Pack',
                                             task_queue='default_queue')
    except WorkflowAlreadyStartedError:
        pass


def run(self):
    logging.basicConfig(level=logging.DEBUG, force=True)
    logging.info('start app')
    asyncio.run(connect_client('localhost:7233'))
    logging.info('finish app')

if __name__ == "__main__":
    run()
```

log from gdb:
```
2024-09-25T09:32:59.616938Z DEBUG hyper_util::client::legacy::connect::dns: resolving host="localhost"                                                                                                            
2024-09-25T09:32:59.657156Z DEBUG hyper_util::client::legacy::connect::http: connecting to [::1]:7233                                                                                                             
2024-09-25T09:32:59.657644Z DEBUG hyper_util::client::legacy::connect::http: connecting to 127.0.0.1:7233                                                                                                         
2024-09-25T09:32:59.657936Z DEBUG hyper_util::client::legacy::connect::http: connected to 127.0.0.1:7233                                                                                                          
2024-09-25T09:32:59.657996Z DEBUG h2::client: binding client connection                                                                                                                                           
2024-09-25T09:32:59.658088Z DEBUG h2::client: client connection bound                                                                                                                                             
2024-09-25T09:32:59.658129Z DEBUG h2::codec::framed_write: send frame=Settings { flags: (0x0), enable_push: 0, initial_window_size: 2097152, max_frame_size: 16384, max_header_list_size: 16384 }
2024-09-25T09:32:59.658413Z DEBUG Connection: h2::codec::framed_write: send frame=WindowUpdate { stream_id: StreamId(0), size_increment: 5177345 } peer=Client
2024-09-25T09:32:59.658430Z DEBUG tower::buffer::worker: service.ready=true processing request                                                                                                                    
2024-09-25T09:32:59.658655Z DEBUG Connection: h2::codec::framed_read: received frame=Settings { flags: (0x0), max_frame_size: 16384 } peer=Client      
2024-09-25T09:32:59.658704Z DEBUG Connection: h2::codec::framed_write: send frame=Settings { flags: (0x1: ACK) } peer=Client      
2024-09-25T09:32:59.658715Z DEBUG Connection: h2::codec::framed_write: send frame=Headers { stream_id: StreamId(1), flags: (0x4: END_HEADERS) } peer=Client                                                       
2024-09-25T09:32:59.658763Z DEBUG Connection: h2::codec::framed_write: send frame=Data { stream_id: StreamId(1) } peer=Client                                                                                     
2024-09-25T09:32:59.658773Z DEBUG Connection: h2::codec::framed_write: send frame=Data { stream_id: StreamId(1), flags: (0x1: END_STREAM) } peer=Client    
2024-09-25T09:32:59.658843Z DEBUG Connection: h2::codec::framed_read: received frame=Settings { flags: (0x1: ACK) } peer=Client                                                                                   
2024-09-25T09:32:59.658872Z DEBUG Connection: h2::proto::settings: received settings ACK; applying Settings { flags: (0x0), enable_push: 0, initial_window_size: 2097152, max_frame_size: 16384, max_header_list_s
ize: 16384 } peer=Client                                                                                                                                                                                          
2024-09-25T09:32:59.659016Z DEBUG Connection: h2::codec::framed_read: received frame=WindowUpdate { stream_id: StreamId(0), size_increment: 5 } peer=Client
2024-09-25T09:32:59.659059Z DEBUG Connection: h2::codec::framed_read: received frame=Ping { ack: false, payload: [2, 4, 16, 16, 9, 14, 7, 7] } peer=Client                 
2024-09-25T09:32:59.659073Z DEBUG Connection: h2::codec::framed_write: send frame=Ping { ack: true, payload: [2, 4, 16, 16, 9, 14, 7, 7] } peer=Client                                                            
2024-09-25T09:32:59.659470Z DEBUG Connection: h2::codec::framed_read: received frame=Headers { stream_id: StreamId(1), flags: (0x4: END_HEADERS) } peer=Client
2024-09-25T09:32:59.659492Z DEBUG Connection: h2::codec::framed_read: received frame=Data { stream_id: StreamId(1) } peer=Client  
2024-09-25T09:32:59.659506Z DEBUG Connection: h2::codec::framed_read: received frame=Headers { stream_id: StreamId(1), flags: (0x5: END_HEADERS | END_STREAM) } peer=Client
2024-09-25T09:32:59.660903Z DEBUG tower::buffer::worker: service.ready=true processing request                                                                                                                    
2024-09-25T09:32:59.661020Z DEBUG Connection: h2::codec::framed_write: send frame=Headers { stream_id: StreamId(3), flags: (0x4: END_HEADERS) } peer=Client                                                       
2024-09-25T09:32:59.661100Z DEBUG Connection: h2::codec::framed_write: send frame=Data { stream_id: StreamId(3) } peer=Client                                                                                     
2024-09-25T09:32:59.661111Z DEBUG Connection: h2::codec::framed_write: send frame=Data { stream_id: StreamId(3), flags: (0x1: END_STREAM) } peer=Client                                                           
2024-09-25T09:32:59.661419Z DEBUG Connection: h2::codec::framed_read: received frame=WindowUpdate { stream_id: StreamId(0), size_increment: 170 } peer=Client                                                     
2024-09-25T09:32:59.661438Z DEBUG Connection: h2::codec::framed_read: received frame=Ping { ack: false, payload: [2, 4, 16, 16, 9, 14, 7, 7] } peer=Client                                                        
2024-09-25T09:32:59.661444Z DEBUG Connection: h2::codec::framed_write: send frame=Ping { ack: true, payload: [2, 4, 16, 16, 9, 14, 7, 7] } peer=Client                                                            
2024-09-25T09:32:59.664355Z DEBUG Connection: h2::codec::framed_read: received frame=Headers { stream_id: StreamId(3), flags: (0x5: END_HEADERS | END_STREAM) } peer=Client                                       
INFO:root:finish app                                                                                                                                                                                              
Fatal Python error: This thread state must be current when releasing                                                                                                                                              
Python runtime state: finalizing (tstate=0x95d720)                                                                                                                                                                
                                                                                                                                                                                                                  
Current thread 0x00007ffedaff7700 (most recent call first):                                                                                                                                                       
  File "/usr/lib/python3.8/asyncio/selector_events.py", line 140 in _write_to_self                                                                                                                                
  File "/usr/lib/python3.8/asyncio/base_events.py", line 770 in call_soon_threadsafe                                                                                                                              
                                                                                                                                                                                                                  
Thread 0x00007ffff7bfd740 (most recent call first):                                                                                                                                                               
<no Python frame>                                                                                                                                                                                                 
                                                                                                                                                                                                                  
Thread 79 "tokio-runtime-w" received signal SIGABRT, Aborted.                                            
```

Full stack trace of failed thread: https://gist.github.com/TiunovNN/84c253d62f03f11270a49facee6c7b52

I suppose that root cause is the python does not call destructor immediately, and as written in docs `A Client does not have an explicit "close"`. When cpython is finalizing code, it acquires GIL and executes destructor of tokio::runtime::Runtime which is expecting while all tasks have finished. At the same time tokio::runtime got ping message and run python code to handle message, which is trying to acquire GIL as well.

It would be better if there was an explicit way to close client.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-25 12:57:25.000 UTC</summary>

> Here is a simple code to reproduce bug:

I am struggling to replicate. There is a slight bug where `run` accepts `self`, but fixing that, I didn't see it. Can you confirm your OS, `temporalio` SDK version, and Python version?

> It would be better if there was an explicit way to close client.

I don't think this would solve it if the issue is Tokio runtime cleanup. A better solution may be like what https://github.com/hydro-project/hydroflow/pull/699 did (linked from https://github.com/hydro-project/hydroflow/issues/619 who linked here as having a common issue). Specifically using something like `atexit.register` to register runtime drop.

But ideally we could get a reliable replication first so we can confirm it is fixed once we make the change. I am struggling to get this currently.

</details>

<details>
<summary><strong>TiunovNN</strong> commented on 2024-09-25 14:00:30.000 UTC</summary>

> I am struggling to replicate.

My mistake. I have forgot to mention that this bug appears not every run, but rarely. So you have to rerun script several times.
I used gdb in the following manner:
```shell
$ gdb python
(gdb) set pagination off
(gdb) break _exit
(gdb) command
> run
> end
(gdb) run script.py
```
And wait for several minutes.

> Can you confirm your OS, temporalio SDK version, and Python version?

The bug appears on the following sets:
* Ubuntu 20.04.6 LTS + python 3.12 + temporal 1.5.0
* Ubuntu 22.04.5 LTS + python 3.12 + temporal 1.5.0
* Ubuntu 20.04.6 LTS + python 3.12 + temporal 1.7.1
* Ubuntu 22.04.5 LTS + python 3.12 + temporal 1.7.1
* Ubuntu 20.04.6 LTS + python 3.8 + temporal 1.7.1




</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-25 14:31:49.000 UTC</summary>

Thanks! We will increase the priority on this.

</details>

<details>
<summary><strong>TiunovNN</strong> commented on 2024-10-02 08:52:46.000 UTC</summary>

@cretz 
> Specifically using something like atexit.register to register runtime drop

Please, take into account, that client destructor might be called even during execution process (not only at exit), when there is no references to the object. I would recommend to use context manager as appropriate solution for python.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-02 13:07:23.000 UTC</summary>

> that client destructor might be called even during execution process (not only at exit), when there is no references to the object.

This is not just about client destructor, this is about when it is dropped Rust side. If the client is in use in any way (Python or Rust), it is not closed/disconnected. We can't just use context manager because it is passed around (e.g. to a worker) and may only retain Rust references. I think this error is specific to the Python-Rust bridge and PyO3.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-15 16:58:33.000 UTC</summary>

Update here, I am still struggling to replicate. I suspect this is actually a Tokio runtime lifetime issue and not specific to the client, but we are still trying to replicate reliably (running the script under gdb for me, at least on 3.10, just continually works forever).

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-15 20:14:26.000 UTC</summary>

I can replicate a similar issue changing the code to just run a task in the background and then completing the app before waiting on complete, e.g.:

```python
import asyncio
import logging
import uuid

from temporalio.api.workflowservice.v1 import StartWorkflowExecutionRequest
from temporalio.client import Client
from temporalio.exceptions import WorkflowAlreadyStartedError
from temporalio.runtime import LoggingConfig, Runtime, TelemetryConfig, TelemetryFilter


async def connect_client(host):
    client = await Client.connect(
        host,
        namespace="default",
        runtime=Runtime(
            telemetry=TelemetryConfig(
                logging=LoggingConfig(
                    filter=TelemetryFilter(
                        core_level="DEBUG",
                        other_level="DEBUG",
                    ),
                ),
            ),
        ),
    )

    req = StartWorkflowExecutionRequest(
        namespace="default",
        workflow_id=f"my-workflow-id-{uuid.uuid4()}",
        identity="my-identity",
        request_id=str(uuid.uuid4()),
    )
    req.workflow_type.name = "my-workflow"
    req.task_queue.name = "my-task-queue"

    asyncio.create_task(client.workflow_service.start_workflow_execution(req))


def run():
    logging.basicConfig(level=logging.DEBUG, force=True)
    logging.info("start app")
    asyncio.run(connect_client("localhost:7233"))
    logging.info("finish app")


if __name__ == "__main__":
    run()
```

This will give:

```
FATAL: exception not rethrown
Aborted
```

My current guess is this is because pyo3-asyncio assumes `Python::with_gil` will succeed, but that delegates to `PyGILState_Ensure` which documents at https://docs.python.org/3/c-api/init.html#c.PyGILState_Ensure:

> Calling this function from a thread when the runtime is finalizing will terminate the thread, even if the thread was not created by Python. You can use [Py_IsFinalizing()](https://docs.python.org/3/c-api/init.html#c.Py_IsFinalizing) or [sys.is_finalizing()](https://docs.python.org/3/library/sys.html#sys.is_finalizing) to check if the interpreter is in process of being finalized before calling this function to avoid unwanted termination.

Unfortunately `Py_IsFinalizing` is 3.13+ and ideally we don't want the overhead of invoking `sys.is_finalizing`, but we may have to (and it's not that bad considering pyo3-asyncio already uses things like `call_soon_threadsafe`).

Unfortunately https://github.com/awestlake87/pyo3-asyncio is mostly abandoned, https://github.com/PyO3/pyo3-async-runtimes is just a simple fork, and built-in PyO3 asyncio with Rust futures is still being worked on. So we probably would just end up vendoring what we haven't already from pyo3-asyncio (we already copy and slightly alter some code to use the current Tokio runtime instead of one big static global).

</details>

<details>
<summary><strong>i3Cheese</strong> commented on 2024-11-20 18:24:54.000 UTC</summary>

@cretz
Hello! I encountered the same issue, and I would like to help fix it. Where do I start?


</details>

<details>
<summary><strong>cretz</strong> commented on 2024-11-20 23:03:32.000 UTC</summary>

Assuming our guess at the issue is correct, fixing this is daunting because it probably requires rewriting/vendoring a Rust library (pyo3-asyncio). Can you confirm this only happens on process/event-loop shutdown? The best way to avoid this is to not attempt to gracefully exit the program while the client is in use somewhere.

</details>


---

<a id="274"></a>

### #274: mutate workflow instance attributes from within activities that it executes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/274 |
| **State** | CLOSED |
| **Author** | AmirRezaSobhdel |
| **Created** | 2023-02-14 06:58:10.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-17 14:20:04.000 UTC |
| **Closed** | 2023-02-17 14:20:03.000 UTC |
| **Upvotes** | 4 |
| **Comments** | 2 |
| **Priority Score** | 10 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 4 |

#### Description

I have a **state_dict** instance attribute in my WorkFlow class and I want it to be passed as an argument to the activity to then be updated in the activity like this :

```
    @activity.defn
    async def compose_greeting(name, state) -> str:
        state['running_activity'] = "name_of_this_activity"
        return name
```
```
    @workflow.defn
    class GreetingWorkflow(BaseWorkflow):
        def __init__(self):
            self.state_dict = dict()
        @workflow.run
        async def run(self) -> str:
            res = await workflow.execute_activity(
                compose_greeting,
                args=['amirreza', self.state],
                start_to_close_timeout=timedelta(seconds=10),
            )
            print(self.state_dict)
            return res
```

but it appears that the argument pass of the state_dict is not reference-based and the changes I make to it in the activity, won't apply to the **self.state_dict** inside the WorkFlow class.

how can I get this to work?

My final purpose is to create a BaseActivity class which my callable activity classes inherit this `state['running_activity'] = "name_of_this_activity"` functionality from and update the **self.state_dict** when they are executed.


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-02-14 12:39:21.000 UTC</summary>

This cannot be done. Activities may not even run on the same machine. Activities and workflows are intentionally completely separate things that run in separate place that serialize the data sent between them.

If you want to relay information back to the workflow from the activity, you can do it via the return value or via signals.

If unsure how to model your use case onto workflows and activities, including how to pass state, feel free to ask in the forums or on public Slack.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-17 14:20:03.000 UTC</summary>

Closing issue as it was just a question

</details>


---

<a id="59"></a>

### #59: Investigate issues with gevent.monkey.patch_all()

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/59 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-06-29 14:29:00.000 UTC (3y 6m ago) |
| **Updated** | 2023-10-24 13:20:42.000 UTC |
| **Closed** | 2023-10-24 13:20:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 10 |
| **Priority Score** | 10 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Users on an older version of gevent (1.5.0) reported that running `gevent.monkey.patch_all()` breaks workflow asyncio.

### Describe the solution you'd like

* Replicate the error on that gevent version (a simple workflow is probably fine)
* Upgrade gevent to see if it still occurs
  * If not, essentially bisect to find offending version minimum and maybe document in README
  * If so, find workaround and document in README

#### Comments (10)

<details>
<summary><strong>alecgorge</strong> commented on 2022-06-29 16:13:59.000 UTC</summary>

Does this mean gevent with Temporal is meant to be a supported configuration? Do you need to use synchronous Activities to use gevent code?

Is there any documentation on the usage together? I tried looking for some earlier and couldnâ€™t find it.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-06-29 16:23:06.000 UTC</summary>

> Does this mean gevent with Temporal is meant to be a supported configuration?

No, the SDK does not use gevent in any way nor depend on it in any way. This is just a case where a user had an old gevent library in their existing code and it clashed with Temporal due to some bug in gevent's global side-effecting code that may have since been fixed. This issue is simply me acknowledging it so I can test and if necessary add a warning to the README for users of older gevent libraries.

> Do you need to use synchronous Activities to use gevent code? Is there any documentation on the usage together? I tried looking for some earlier and couldnâ€™t find it.

You may have to and there is no Temporal documentation here (nor is there with any Python async library integration beyond builtin `asyncio`). Synchronous activities are just normal functions that can do anything you'd do in a normal Python call, so yes you do have to use those for non-`async def` functions. I suppose you may technically be able to use something like https://github.com/gfmio/asyncio-gevent but I have not tested and in general would not recommend altering the built-in `asyncio` event loop implementation (or doing any of gevent's global "magic"/"patching" if you can avoid it).

</details>

<details>
<summary><strong>alecgorge</strong> commented on 2022-06-29 18:26:55.000 UTC</summary>

Thanks for the context. I am planning on testing our gevent setup with Temporal. I will report back the results w/gevent version once I have done so

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>alecgorge</strong> commented on 2022-07-28 23:34:41.000 UTC</summary>

As a follow-up, we use gevent 1.3.7 and ran into a number of issues. We haven't done a full investigation but here are some of the things we did:
* Only used synchronous activities
* Set `activity_executor` to an instance of a gevent monekypatched `concurrent.future.ThreadPoolExecutor`
* Avoided usage of any `WorkflowHandle.result()` calls (including `execute_workflow`) and prevented any gevent greenlets from executing

When things worked, it was awesome! Unfortunately, in general we experienced unpredictable behavior in our code where it would mysteriously hang indefinitely (usually around monkeypatched IO calls). We haven't had time to dig in and identify exactly where the incompatibility is. We have a general sense of uneasiness of using gevent monkeypatched code with asyncio code.

If I have time, I'm going to try an experiment where I run Temporal in a process forked off before gevent monkeypatching and using a cross-process executor. I don't expect it to work, but it is a fun exercise!

</details>

<details>
<summary><strong>reallistic</strong> commented on 2022-10-08 00:19:34.000 UTC</summary>

This is still failing on latest gevent (21.12.0) and temporalio (0.1b2).

## Minimal code to reproduce:
```
from gevent.monkey import patch_thread; patch_thread()

import asyncio
from datetime import timedelta

from temporalio import activity
from temporalio import workflow
from temporalio.client import Client
from temporalio.service import RPCError, RPCStatusCode
from temporalio.worker import Worker

@activity.defn
async def get_name(prefix: str) -> str:
    return f"{prefix} angela"


@workflow.defn
class SimpleWorkflow:
    @workflow.run
    async def run(self) -> str:
        return await workflow.execute_activity(
            get_name,
            "Dear",
            start_to_close_timeout=timedelta(seconds=10),
        )


async def start_temporal():
    temporal_client = await Client.connect("localhost:7233")
    try:
        await temporal_client.start_workflow(
            SimpleWorkflow.run,
            id="simple_workflow",
            task_queue="test-queue",
        )
    except RPCError as e:
        if e.status is not RPCStatusCode.ALREADY_EXISTS:
            raise

    worker = Worker(
        temporal_client,
        task_queue="test-queue",
        workflows=[SimpleWorkflow],
        activities=[get_name],
    )
    await worker.run()


if __name__ == "__main__":
    asyncio.run(start_temporal())

```

## Error
```
Failed handling activation on workflow with run ID f5df1f62-b1b3-4a84-8af8-0837c77e7208
Traceback (most recent call last):
  File "/Users/michase/workspace/ntflx/fastapi_temporal_poc/.venv/lib/python3.9/site-packages/temporalio/worker/workflow.py", line 186, in _handle_activation
    completion = await asyncio.wait_for(
  File "/Users/michase/.newt-cache/pyenv/versions/cpython-3.9.14/lib/python3.9/asyncio/tasks.py", line 435, in wait_for
    loop = events.get_running_loop()
RuntimeError: no running event loop
Failed completing activation on workflow with run ID f5df1f62-b1b3-4a84-8af8-0837c77e7208
Traceback (most recent call last):
  File "/Users/michase/workspace/ntflx/fastapi_temporal_poc/.venv/lib/python3.9/site-packages/temporalio/worker/workflow.py", line 234, in _handle_activation
    await self._bridge_worker().complete_workflow_activation(completion)
  File "/Users/michase/workspace/ntflx/fastapi_temporal_poc/.venv/lib/python3.9/site-packages/temporalio/bridge/worker.py", line 98, in complete_workflow_activation
    await self._ref.complete_workflow_activation(comp.SerializeToString())
RuntimeError: no running event loop
```

## Explanation
**NOTE** that only threading needs to be patched

I believe there is something funky with how gevent patches thread locals and how thread locals are used to store the current event loop.

The code [here](https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/workflow_instance.py#L1190) that calls `asyncio._set_running_loop(None)` is overwriting the loop on the main thread instead of in the thread in use from spawning the workflow in a thread pool executor [here](https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/workflow.py#L178). This happens on the main thread because the worker thread pool executor spawns greenlets instead of true threads. The greenlets run on the main thread. Under the hood, pure-python uses a [thread local](https://github.com/python/cpython/blob/3.9/Lib/asyncio/events.py#L673-L677) to manage the running loop and gevent should be patching that. However, cpython manages this using a [c specific implementation](https://github.com/python/cpython/blob/3.9/Modules/_asynciomodule.c#L292) that gevent cannot patch.


Similar to @alecgorge I tried to pass in a `gevent.threadpool.ThreadPoolExecutor` to the worker but the futures it returns are not asyncio compliant and raise `ValueError` for calls such as `asyncio.ensure_future` (which is invoked under [`wait_for`](https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/workflow.py#L186)). There may be a way to patch the gevent `ThreadPoolExecutor` in a simple subclass but I have not looked into that.


## Potential fix
Use `set_event_loop` and `get_event_loop` instead. Those implementations do still use the underlying c calls for `get_running_loop`. However, with this I can push the worker into a real thread. The base event loop policy does not create a new loop in the non-main thread so it should prevent `_get_running_loop` from returning a value and, instead the event loop policy will be used for managing the loop. That maintains a thread local that can be patched.

I will add an example with this soon.

</details>

<details>
<summary><strong>reallistic</strong> commented on 2022-10-10 17:42:33.000 UTC</summary>

After a deep dive into the asyncio code I found that a running loop always sets the c specific thread local and there is no (easy) way around that.

That said, I was able to get the gevent `ThreadPoolExecutor` working with the following:
```
    worker = Worker(
        temporal_client,
        task_queue="test-queue",
        workflows=[SimpleWorkflow],
        activities=[get_name],
        workflow_task_executor=CFGThreadPool(4)
    )
    await worker.run()

class CFGThreadPool(gevent.threadpool.ThreadPoolExecutor):
    def _set_result_and_wake(self, fut, loop, result, exc_info):
        if fut.cancelled():
            return

        # call_soon_threadsafe wakes up the loop under the hood
        if exc_info is not None:
            loop.call_soon_threadsafe(fut.set_exception, exc_info)
        else:
            loop.call_soon_threadsafe(fut.set_result, result)

    def submit(self, func, *args, **kwargs):
        loop = asyncio.get_running_loop()
        fut = concurrent.futures.Future()

        @functools.wraps(func)
        def wrapper(*w_args, **w_kwargs) -> None:

            result = None
            exc_info = None

            try:
                result = func(*w_args, **w_kwargs)
            except Exception as ex:
                exc_info = ex

            self._set_result_and_wake(fut, loop, result, exc_info)

            return result


        super().submit(wrapper, *args, **kwargs)
        return fut


```

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-10-10 17:53:39.000 UTC</summary>

@reallistic - Thanks for this deep dive. I have this on my schedule to dig into and even write an integration test for (if isolatable, i.e. can "unpatch" afterwards). 

> Use `set_event_loop` and `get_event_loop` instead.

I think this is the right approach and we should do this if it in fact fixes it. Who knows who else is setting default event loops on threads somehow. I just set to `None` because I thought I was in control of the thread.

</details>

<details>
<summary><strong>reallistic</strong> commented on 2022-10-10 18:23:50.000 UTC</summary>

> I just set to `None` because I thought I was in control of the thread. 

This is 100% sane and reasonable. The issue is that gevent monkey patches pure python threading but not the c implementation.

Also, changing `_(s|g)et_running_loop` to `(s|g)et_event_loop` is a good idea to prevent changes in the private API from breaking things.

However, using those apis does not help out with gevent. This is because the main loop used to connect the client and run the `Worker` (not the `_WorkflowInstanceImpl `) has to be running via either `asyncio.run` or `loop.run_until_complete`. Both implementations call `_set_running_loop` which sets the loop directly on the c thread local. The public `get_event_loop` call first calls `_get_running_loop` prior to consulting `get_event_loop_policy().get_event_loop()`. Thus, when gevent monkey patches threading, the main loop will always be returned effectively ignoring the public `set_event_loop`.

The only approach I can think of to allow gevent to work would be to use regular asyncio calls instead of a custom event loop. I recognize though that doing such an implementation would require a decent amount of work and you would lose the safeguards of preventing multi-threading etc.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-10-10 19:09:34.000 UTC</summary>

> Also, changing _(s|g)et_running_loop to (s|g)et_event_loop is a good idea to prevent changes in the private API from breaking things.

Completely agree, but there was something w/ the default event loop policy on some platform I think was causing me problems. But I forget what and I didn't document it, so will definitely try to move back to all public API.

> The only approach I can think of [...]

We really have to keep our custom event loop. It's the underpinning of what makes workflows work well.

Maybe we can have a similar monkey patch that overrides gevent for the life of the run_once and then puts it back after. I'll have to check gevent, but surely we're not the first that have wanted to do advanced asyncio loop management in a gevent-enabled process. I'll have to dig into what they really do. I only need a short synchronous window to own the loop on a thread run.


</details>

<details>
<summary><strong>cretz</strong> commented on 2023-10-11 17:47:32.000 UTC</summary>

I am happy to report I have succeeded combining gevent and Temporal: https://github.com/temporalio/samples-python/pull/84. @reallistic - I basically took your approach but I created a _separate_ executor for running the asyncio event loop in a native gevent thread (separate from the executor that is used to execute workflow/activity tasks).

I will have to update the README in this repo with the update and then I'll be able to close this out I think unless I'm missing something.

</details>


---

<a id="54"></a>

### #54: temporal-sdk-core-api-0.1.0: 1 vulnerabilities (highest severity is: 5.5) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/54 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2022-06-21 18:50:03.000 UTC (3y 6m ago) |
| **Updated** | 2023-11-06 17:41:44.000 UTC |
| **Closed** | 2023-11-06 17:41:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 10 |
| **Priority Score** | 10 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>temporal-sdk-core-api-0.1.0</b></p></summary>

<p></p>
<p>Path to dependency file: /temporalio/bridge/Cargo.toml</p>
<p>Path to vulnerable library: /temporalio/bridge/Cargo.toml</p>
<p>

</details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in (temporal-sdk-core-api version) | Remediation Possible** |
| ------------- | ------------- | ----- | ----- | ----- | ------------- | --- |
| [WS-2023-0366](https://github.com/bytecodealliance/rustix/commit/31fd98ca723b93cc6101a3e29843ea5cf094e159) | <img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png?' width=19 height=20> Medium | 5.5 | rustix-0.38.8.crate | Transitive | N/A* | &#10060; |
<p>*For some transitive vulnerabilities, there is no version of direct dependency with a fix. Check the "Details" section below to see if there is a version of transitive dependency where vulnerability is fixed.</p><p>**In some cases, Remediation PR cannot be created automatically for a vulnerability despite the availability of remediation</p>

## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png?' width=19 height=20> WS-2023-0366</summary>


###  Vulnerable Library - <b>rustix-0.38.8.crate</b></p>

<p>Safe Rust bindings to POSIX/Unix/Linux/Winsock2-like syscalls</p>
<p>Library home page: <a href="https://static.crates.io/crates/rustix/rustix-0.38.8.crate">https://static.crates.io/crates/rustix/rustix-0.38.8.crate</a></p>
<p>Path to dependency file: /temporalio/bridge/Cargo.toml</p>
<p>Path to vulnerable library: /temporalio/bridge/Cargo.toml</p>
<p>

Dependency Hierarchy:
  - temporal-sdk-core-api-0.1.0 (Root Library)
    - temporal-sdk-core-protos-0.1.0
      - prost-wkt-types-0.4.2.crate
        - prost-build-0.11.9.crate
          - tempfile-3.8.0.crate
            - :x: **rustix-0.38.8.crate** (Vulnerable Library)
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
rustix's `rustix::fs::Dir` iterator with the `linux_raw` backend can cause memory explosion

<p>Publish Date: 2023-10-18
<p>URL: <a href=https://github.com/bytecodealliance/rustix/commit/31fd98ca723b93cc6101a3e29843ea5cf094e159>WS-2023-0366</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>5.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Local
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: Required
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://github.com/advisories/GHSA-c827-hfw6-qwvm">https://github.com/advisories/GHSA-c827-hfw6-qwvm</a></p>
<p>Release Date: 2023-10-18</p>
<p>Fix Resolution: rustix - 0.35.15,0.36.16,0.37.25,0.38.19</p>

</p>

<p></p>

</details>

#### Comments (10)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-07-01 15:27:49.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-08-16 22:37:17.000 UTC</summary>

:information_source: This issue was automatically re-opened by Mend because the vulnerable library in the specific branch(es) has been detected in the Mend inventory.

</details>

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-10-19 20:08:55.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-01-16 01:55:40.000 UTC</summary>

:information_source: This issue was automatically re-opened by Mend because the vulnerable library in the specific branch(es) has been detected in the Mend inventory.

</details>

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-03-30 23:06:02.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-04-12 07:09:25.000 UTC</summary>

:information_source: This issue was automatically re-opened by Mend because the vulnerable library in the specific branch(es) has been detected in the Mend inventory.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-06-29 19:41:21.000 UTC</summary>

I think this can be fixed potentially with a cargo update. We just need to get past h2 0.3.16 which we are currently at.

</details>

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-07-05 21:00:07.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-10-25 06:19:22.000 UTC</summary>

:information_source: This issue was automatically re-opened by Mend because the vulnerable library in the specific branch(es) has been detected in the Mend inventory.

</details>

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-11-06 17:41:44.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="747"></a>

### #747: How to Get the Worker ID Executing the Current Activity?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/747 |
| **State** | CLOSED |
| **Author** | hebrd (hebrd) |
| **Created** | 2025-01-24 03:25:09.000 UTC (11 months ago) |
| **Updated** | 2025-02-15 08:49:22.000 UTC |
| **Closed** | 2025-02-04 18:16:12.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 9 |
| **Priority Score** | 9 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description



### Description:
Iâ€™m trying to retrieve the worker ID that is executing the current activity. However, I couldnâ€™t find a way to do this using `activity.info()`. Hereâ€™s what Iâ€™ve tried so far:

#### Approaches I Attempted:
1. **Using `get_workflow_execution_history`:**
   - I wasnâ€™t able to retrieve the activity started event while the activity is still executing. It seems this approach only works for completed activities.

2. **Setting the Worker ID in `Info`:**
   - I considered adding the worker ID into `Info` manually but couldnâ€™t figure out how to achieve this in practice.

---

### Questions:
- Is there a built-in way to get the worker ID for the worker executing the current activity?
- If not, is there a recommended approach or workaround to track which worker is running an activity at runtime?

---

### Context:
- **Use Case:** Tracking or debugging activities in distributed workflows by identifying the specific worker executing the activity.
- **Environment:** [Include Temporal version, SDK (e.g., Java, Go, Python), and any relevant configuration details]

Any guidance or suggestions would be greatly appreciated!

---

This version structures the issue clearly, explains what youâ€™ve already tried, and provides enough context for maintainers to understand your challenge. Adding environment details also helps them reproduce the issue if needed.

#### Comments (9)

<details>
<summary><strong>cretz</strong> commented on 2025-01-24 13:56:23.000 UTC</summary>

Can you clarify what you mean by "worker ID"?

Do you mean "identity" of the worker? The identity is set by you when creating the client and you can make it global (if it is a global, which it should be) or pass it in to the activity class for access. The same goes for any worker-level value you might want to access in the activity. In next release, assuming you have not overridden it when creating the worker, this will be available via `activity.client().service_client.config.identity`. But this identity just defaults to `f"{os.getpid()}@{socket.gethostname()}"` which is information you can also just use yourself in the activity.

Does this help? Also, feel free to join us on Slack or our forums for general questions.

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-02-04 18:16:12.000 UTC</summary>

Hi @hebrd, I'm going to close this since I think @cretz's answer should give you what you need. But feel free to keep discussing here (or in Slack).

</details>

<details>
<summary><strong>hebrd</strong> commented on 2025-02-08 08:06:41.000 UTC</summary>

> Can you clarify what you mean by "worker ID"?
> 
> Do you mean "identity" of the worker? The identity is set by you when creating the client and you can make it global (if it is a global, which it should be) or pass it in to the activity class for access. The same goes for any worker-level value you might want to access in the activity. In next release, assuming you have not overridden it when creating the worker, this will be available via `activity.client().service_client.config.identity`. But this identity just defaults to `f"{os.getpid()}@{socket.gethostname()}"` which is information you can also just use yourself in the activity.
> 
> Does this help? Also, feel free to join us on Slack or our forums for general questions.

Apologies for the delayed response during my vacation.
Regarding my specific situation, one temporal client has multiple workers. When an activity is created, I cannot determine which worker will be responsible for executing it until runtime. Therefore, I am unable to pass the worker's identity to the activity class beforehand.
I believe the `activity.client().service_client.config.identity` feature in the upcoming release will address our requirements. Looking forward to its release!

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-02-10 16:17:26.000 UTC</summary>

> one temporal client has multiple workers [...] I believe the activity.client().service_client.config.identity feature in the upcoming release will address our requirements

But this is the one client across multiple workers, so this is the client identity (which is the worker identity by default), so it is the same for all activities on all workers of that client. If you need activities on different workers to have different state, consider using activity methods and instantiating different instances, e.g. one worker has `activities=[MyActivity(myState1).my_activity]` while the other has `activities=[MyActivity(myState2).my_activity]`.

</details>

<details>
<summary><strong>hebrd</strong> commented on 2025-02-13 05:56:22.000 UTC</summary>

> But this is the one client across multiple workers, so this is the client identity (which is the worker identity by default), so it is the same for all activities on all workers of that client. If you need activities on different workers to have different state, consider using activity methods and instantiating different instances, e.g. one worker has `activities=[MyActivity(myState1).my_activity]` while the other has `activities=[MyActivity(myState2).my_activity]`.

I have several workers with different identities for one client , for example:
```python
workers = []
    futures = []
    for i in range(worker_num):
        worker_name = f"{worker_prefix}-{i + 1}@{socket.gethostname()}"
        worker = Worker(
            client,
            task_queue=q_name,
            workflows=[BotActionWorkflow],
            activities=[execute_action],
            identity=worker_name,
            interceptors=[ActivityStatusInterceptor()],
            max_concurrent_activities=max_activity
        )
        workers.append(worker)
        futures.append(worker.run())
        logger.info("worker added: {}", worker_name)
```

What i want is to get worker identity executing current activity in `ActivityStatusInterceptor`, would you please advise?

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-02-13 14:40:25.000 UTC</summary>

If you want worker-specific state in your interceptors, you should pass the state to the interceptor on instantiation, e.g `ActivityStatusInterceptor(worker_name)`. You can then store/use this in the interceptor and it is different per worker.

</details>

<details>
<summary><strong>hebrd</strong> commented on 2025-02-14 09:13:44.000 UTC</summary>

Thank you, Cretz. My concern is that I cannot determine the worker's identity before an activity is dynamically dispatched to a random worker.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-02-14 16:03:16.000 UTC</summary>

But you know that each worker has its own instance of the interceptor so you know if the interceptor for an activity is called, its state (e.g. worker identity) is only for the worker you're running on.

</details>

<details>
<summary><strong>hebrd</strong> commented on 2025-02-15 08:49:21.000 UTC</summary>

Got it finally. Thank you for your patience and assistance.

Reactions: ðŸ‘ 1

</details>


---

<a id="639"></a>

### #639: [Bug] Operation was canceled when start_workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/639 |
| **State** | OPEN |
| **Author** | duy-nguyen-ts (duy-nguyen-ts) |
| **Created** | 2024-09-10 03:47:36.000 UTC (1y 4m ago) |
| **Updated** | 2024-09-13 02:49:54.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 7 |
| **Priority Score** | 9 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### What are you really trying to do?
- Hi team, I am having an issue when trying to `start_workflow` and `signal_workflow`
<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

### Describe the bug

- It happens when I called method `start_workflow`. Maybe it cant connect to create workflow on temporal and return `temporal_sdk_bridge.RPCError: (1, 'operation was canceled', b'')`
- I started 10 workflows but received 6 success and 4 error cancelled
- I want to know why it happens, does it due to network or anything else ? How can I fix that ? E.x: Add retry policy when start_workflow,...

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Mac M2
- Temporal Version: ^1.6.0
- Are you using Docker or Kubernetes or building Temporal from source: Using Docker

### Additional context

<!-- Add any other context about the problem here. -->

#### Comments (7)

<details>
<summary><strong>duy-nguyen-ts</strong> commented on 2024-09-10 03:57:25.000 UTC</summary>

I had check my logs again, this error also happens when I call signal to workflow.

</details>

<details>
<summary><strong>duy-nguyen-ts</strong> commented on 2024-09-10 04:27:33.000 UTC</summary>

After tracing this issue, I saw it happened at this line, maybe error when it made a rpc call to temporal
![Screenshot 2024-09-10 at 11 25 47](https://github.com/user-attachments/assets/459b144e-ff48-4409-9335-eeb792a656af)


</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-10 12:11:19.000 UTC</summary>

Can you replicate this reliably? If so, can you alter a sample to show how to replicate? And is it against Temporal cloud or self-hosted server? We are releasing a fix in the next couple of days for a similar error at https://github.com/temporalio/sdk-core/issues/807, but we believe that only affected 1.7.0.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>duy-nguyen-ts</strong> commented on 2024-09-11 02:43:03.000 UTC</summary>

Hi @cretz , thanks for your reply,  I am using Temporal as self-hosted server and I can't always replicate it, sometime it happened and not. I investigated and assumed that it caused at point in above image. Currently, I added retry when call start_workflow and this error still happen but less than before. About my code, it just sample like this: 
- Create a client with connect
`temporal_client = await Client.connect(target_host=...,namespace=...)`
- Call start_workflow (maybe many calls at the same time)
`handler = await temporal_client.start_workflow(workflow, args=[arg], id="workflow_id", task_queue="task_queue")`

</details>

<details>
<summary><strong>duy-nguyen-ts</strong> commented on 2024-09-11 02:45:11.000 UTC</summary>

I am using version 1.6.0 so maybe it not similar to https://github.com/temporalio/sdk-core/issues/807

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-11 13:10:05.000 UTC</summary>

> I am using Temporal as self-hosted server and I can't always replicate it, sometime it happened and not

Even if it takes a minute to replicate, any replication would help us debug.

I am afraid there's not much to go on here. We have many samples/users starting hundreds/thousands of workflows without any issues on self-hosted servers. Can you make sure you're not doing something like accidentally blocking the thread in an `async def` call thereby causing asyncio to stop working properly?

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>duy-nguyen-ts</strong> commented on 2024-09-13 02:49:53.000 UTC</summary>

Okk @cretz , thank you for your response. I will continue monitor it ðŸ˜„ 

</details>


---

<a id="306"></a>

### #306: No module named 'temporalio.bridge.temporal_sdk_bridge'

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/306 |
| **State** | CLOSED |
| **Author** | arthosdevelops |
| **Created** | 2023-03-30 02:08:05.000 UTC (2y 9m ago) |
| **Updated** | 2025-02-04 02:06:44.000 UTC |
| **Closed** | 2023-05-22 19:11:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 9 |
| **Priority Score** | 9 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
We're simply using the temporalio library to interact with the temporal server. 
-->

### Describe the bug

When importing the temporalio module, out code constantly fails with the following error

```    from test.temporal.consts import TEST_TEMPORAL_NAMESPACE
test/temporal/consts.py:3: in <module>
    from temporalio.common import RetryPolicy
../../../../Library/Caches/pypoetry/virtualenvs/test-FDULRtEl-py3.11/lib/python3.11/site-packages/temporalio/__init__.py:11: in <module>
    from .service import __version__ as __sdk_version
../../../../Library/Caches/pypoetry/virtualenvs/test-FDULRtEl-py3.11/lib/python3.11/site-packages/temporalio/service.py:22: in <module>
    import temporalio.bridge.client
../../../../Library/Caches/pypoetry/virtualenvs/test-FDULRtEl-py3.11/lib/python3.11/site-packages/temporalio/bridge/client.py:14: in <module>
    import temporalio.bridge.temporal_sdk_bridge
E   ModuleNotFoundError: No module named 'temporalio.bridge.temporal_sdk_bridge'
```

This module does not seem to exist in the site packages too. Do we need to run something in order for this to exist?

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Intel Mac
- Temporalio SDK Version: (0.1b2)
- Python version: 3.11.0



#### Comments (9)

<details>
<summary><strong>cretz</strong> commented on 2023-03-30 12:08:55.000 UTC</summary>

> Temporalio SDK Version: (0.1b2)

Please use the latest version (that version is beta). If the problem still exists, please show exact steps from the beginning that you are running to replicate this error.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>ksbrar</strong> commented on 2023-05-05 13:10:44.000 UTC</summary>

I also ran into this issue with `0.1a1`, which I am not able to upgrade from just yet for my purposes.

Investigating it further, for me it seems to be a rust build error - the `temporal_sdk_bridge` package is never compiled, and trying to do so manually results in a linker failure:
```
â¯ cargo build
   Compiling temporal-sdk-bridge v0.1.0 (/Users/ksbrar/.pcli/venv/lib/python3.9/site-packages/temporalio/bridge)
error: linking with `cc` failed: exit status: 1
             ...
            "_PyUnicode_FromStringAndSize", referenced from:
                pyo3::types::string::PyString::new::hac218b81a1b9faed in libpyo3-4d7deb7ea960b60a.rlib(pyo3-4d7deb7ea960b60a.pyo3.c5bd815a-cgu.12.rcgu.o)
            "_Py_IsInitialized", referenced from:
                pyo3::gil::GILGuard::acquire::_$u7b$$u7b$closure$u7d$$u7d$::h691710afe03fc4c2 in libpyo3-4d7deb7ea960b60a.rlib(pyo3-4d7deb7ea960b60a.pyo3.c5bd815a-cgu.4.rcgu.o)
            "__Py_Dealloc", referenced from:
                pyo3::ffi::object::Py_DECREF::h14f28702b66fbc35 in temporal_sdk_bridge.2p9swp66al6j7n8f.rcgu.o
                pyo3::ffi::object::Py_DECREF::ha8bd5992141a11c5 in libpyo3_asyncio-361bd96192777d66.rlib(pyo3_asyncio-361bd96192777d66.pyo3_asyncio.e183b368-cgu.9.rcgu.o)
                pyo3::ffi::object::Py_DECREF::h544ebe01724e2b12 in libpyo3-4d7deb7ea960b60a.rlib(pyo3-4d7deb7ea960b60a.pyo3.c5bd815a-cgu.14.rcgu.o)
            "__Py_NoneStruct", referenced from:
                pyo3::ffi::object::Py_None::h958a038b7e388cfa in temporal_sdk_bridge.2p9swp66al6j7n8f.rcgu.o
                pyo3::ffi::object::Py_None::ha7faed160e6e3af4 in libpyo3_asyncio-361bd96192777d66.rlib(pyo3_asyncio-361bd96192777d66.pyo3_asyncio.e183b368-cgu.9.rcgu.o)
                pyo3::ffi::object::Py_None::h9fc6a76cbf9a9b3b in libpyo3-4d7deb7ea960b60a.rlib(pyo3-4d7deb7ea960b60a.pyo3.c5bd815a-cgu.14.rcgu.o)
            "__Py_TrueStruct", referenced from:
                pyo3::ffi::boolobject::Py_True::h65dc75eb72cb98a5 in libpyo3-4d7deb7ea960b60a.rlib(pyo3-4d7deb7ea960b60a.pyo3.c5bd815a-cgu.7.rcgu.o)
          ld: symbol(s) not found for architecture arm64
          clang: error: linker command failed with exit code 1 (use -v to see invocation)


error: could not compile `temporal-sdk-bridge` due to previous error
```

Strange, because I had been using `0.1a1` with no issue and suddenly this began occurring. The only thing I can think of atm is perhaps upgrading to macOS 13.3


</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-05 13:22:20.000 UTC</summary>

We have many changes since alpha/beta and only support official releases. Does this happen on latest release?

Others reporting similar issues said some of the things at https://pyo3.rs/v0.18.3/building_and_distribution#macos helped them solve it. Can try that.

(may close issue here soon if this is not occurring on latest release, but comments can still come in)

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-22 19:11:39.000 UTC</summary>

(closing)

</details>

<details>
<summary><strong>spacether</strong> commented on 2025-02-02 21:38:21.000 UTC</summary>

FYI I just ran into this issue on master branch when I tried to follow the build instructions to write a PR.
I am also on MacOS
`poetry install --no-root --all-extras -vv` succeeds with no error, but when temporal is imported python says:
```
    import temporalio.bridge.temporal_sdk_bridge
ModuleNotFoundError: No module named 'temporalio.bridge.temporal_sdk_bridge'
```
OS and processor: MacOS 15.1, Apple M4
Temporalio SDK Version: Main branch, commit 51f4b66f4d01302426958b62940ee26ae413e181
Python version: 3.11.11

If items are not linked or build correctly, why did install succeed?

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-02-03 15:25:32.000 UTC</summary>

> I tried to follow the build instructions to write a PR

Did you run `poe build-develop` per https://github.com/temporalio/sdk-python?tab=readme-ov-file#local-sdk-development-environment?

</details>

<details>
<summary><strong>spacether</strong> commented on 2025-02-03 18:28:43.000 UTC</summary>

Nope I did not, my mistake. I assume that going through the install preparation steps was enough because it had poetry install. Sorry about that, it was my user error. Why I try building that fails:
Build failed:
```
justin@Justins-MacBook-Pro temporal-sdk-python % poetry install --no-root --all-extras @ succeeds
justin@Justins-MacBook-Pro temporal-sdk-python % poetry build
The "poetry.dev-dependencies" section is deprecated and will be removed in a future version. Use "poetry.group.dev.dependencies" instead.
Preparing build environment with build-system requirements poetry-core>=1.0.0, setuptools, wheel, setuptools-rust
Building temporalio (1.9.0)
  - Building sdist
  - Built temporalio-1.9.0.tar.gz
  - Building wheel
running build
running build_ext
running build_rust
    Updating crates.io index
error: failed to get `temporal-client` as a dependency of package `temporal-sdk-bridge v0.1.0 (/Users/justin/code/temporal-sdk-python/temporalio/bridge)`

Caused by:
  failed to load source for dependency `temporal-client`

Caused by:
  Unable to update /Users/justin/code/temporal-sdk-python/temporalio/bridge/sdk-core/client

Caused by:
  failed to read `/Users/justin/code/temporal-sdk-python/temporalio/bridge/sdk-core/client/Cargo.toml`

Caused by:
  No such file or directory (os error 2)
error: `cargo metadata --manifest-path temporalio/bridge/Cargo.toml --format-version 1` failed with code 101
-- Output captured from stdout:


Command '['/var/folders/_k/f3jx_pxs6n54136pgm7706rw0000gn/T/tmp9odzfxwq/.venv/bin/python', '/Users/justin/code/temporal-sdk-python/setup.py', 'build', '-b', '/Users/justin/code/temporal-sdk-python/build', '--build-purelib', '/Users/justin/code/temporal-sdk-python/build/lib', '--build-platlib', '/Users/justin/code/temporal-sdk-python/build/lib.macosx-15.1-arm64-cpython-311']' returned non-zero exit status 1.
```
poe build develop fails too:
```
(.venv) justin@Justins-MacBook-Pro temporal-sdk-python % poe build-develop        
Poe => python scripts/setup_bridge.py develop
The "poetry.dev-dependencies" section is deprecated and will be removed in a future version. Use "poetry.group.dev.dependencies" instead.
running develop
/Users/justin/code/temporal-sdk-python/.venv/lib/python3.11/site-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` and ``easy_install``.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://github.com/pypa/setuptools/issues/917 for details.
        ********************************************************************************

!!
  easy_install.initialize_options(self)
/Users/justin/code/temporal-sdk-python/.venv/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` directly.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
        ********************************************************************************

!!
  self.initialize_options()
running egg_info
creating temporalio.egg-info
writing temporalio.egg-info/PKG-INFO
writing dependency_links to temporalio.egg-info/dependency_links.txt
writing top-level names to temporalio.egg-info/top_level.txt
writing manifest file 'temporalio.egg-info/SOURCES.txt'
reading manifest file 'temporalio.egg-info/SOURCES.txt'
adding license file 'LICENSE'
writing manifest file 'temporalio.egg-info/SOURCES.txt'
running build_ext
running build_rust
cargo rustc --lib --message-format=json-render-diagnostics --manifest-path temporalio/bridge/Cargo.toml -v --features pyo3/abi3-py39 pyo3/extension-module -- --crate-type cdylib -C link-args=-undefined dynamic_lookup -Wl,-install_name,@rpath/temporal_sdk_bridge.abi3.so
    Updating crates.io index
error: failed to get `temporal-client` as a dependency of package `temporal-sdk-bridge v0.1.0 (/Users/justin/code/temporal-sdk-python/temporalio/bridge)`

Caused by:
  failed to load source for dependency `temporal-client`

Caused by:
  Unable to update /Users/justin/code/temporal-sdk-python/temporalio/bridge/sdk-core/client

Caused by:
  failed to read `/Users/justin/code/temporal-sdk-python/temporalio/bridge/sdk-core/client/Cargo.toml`

Caused by:
  No such file or directory (os error 2)
error: `cargo rustc --lib --message-format=json-render-diagnostics --manifest-path temporalio/bridge/Cargo.toml -v --features 'pyo3/abi3-py39 pyo3/extension-module' -- --crate-type cdylib -C 'link-args=-undefined dynamic_lookup -Wl,-install_name,@rpath/temporal_sdk_bridge.abi3.so'` failed with code 101
```


</details>

<details>
<summary><strong>cretz</strong> commented on 2025-02-03 18:46:10.000 UTC</summary>

Is the repo cloned recursively?

</details>

<details>
<summary><strong>spacether</strong> commented on 2025-02-03 19:00:04.000 UTC</summary>

No I was not; sorry. I just cloned it recursively and was able to `poe build-develop`, thank you for your help!

</details>


---

<a id="276"></a>

### #276: [Bug] logging adapter does not include extra attributes?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/276 |
| **State** | CLOSED |
| **Author** | jmdacruz (Marcelo Da Cruz Pinto) |
| **Created** | 2023-02-14 19:22:21.000 UTC (2y 10m ago) |
| **Updated** | 2025-06-10 14:02:05.000 UTC |
| **Closed** | 2023-02-17 01:02:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 9 |
| **Priority Score** | 9 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm trying to use structured logging with the Python SDK, using the `extra` attributes to provide more context to log entries. The Python SDK seems to honor existing attributes when adding the activity/workflow contextual info: https://github.com/temporalio/sdk-python/blob/1fa36c2437eaa35c0d81a0339caf4960b37f1bb6/temporalio/activity.py#L374-L376

### Describe the bug

When enabling basic logging for one of the Python samples, using a format string that references an `extra` attribute that is provided in all log entries (on workflow, activity, and main entry point for the app), the logging calls inside the workflow and activities raises a `KeyError` exception because it can't find the attribute. The same logging call on the `main()` entry point works just fine

### Minimal Reproduction

On the https://github.com/temporalio/samples-python repo, apply the following diff (to file `hello/hello_activity.py`):

```patch
diff --git a/hello/hello_activity.py b/hello/hello_activity.py
index 6615b55..e7716ef 100644
--- a/hello/hello_activity.py
+++ b/hello/hello_activity.py
@@ -20,7 +20,8 @@ class ComposeGreetingInput:
 # Basic activity that logs and does string concatenation
 @activity.defn
 async def compose_greeting(input: ComposeGreetingInput) -> str:
-    activity.logger.info("Running activity with parameter %s" % input)
+    # Logging a record with extra attribute "foo" here causes a "KeyError" exception
+    activity.logger.info("Running activity with parameter %s" % input, extra={"foo": "in activity"})
     return f"{input.greeting}, {input.name}!"
 
 
@@ -29,7 +30,8 @@ async def compose_greeting(input: ComposeGreetingInput) -> str:
 class GreetingWorkflow:
     @workflow.run
     async def run(self, name: str) -> str:
-        workflow.logger.info("Running workflow with parameter %s" % name)
+        # Logging a record with extra attribute "foo" here causes a "KeyError" exception
+        workflow.logger.info("Running workflow with parameter %s" % name, extra={"foo": "in workflow"})
         return await workflow.execute_activity(
             compose_greeting,
             ComposeGreetingInput("Hello", name),
@@ -39,7 +41,12 @@ class GreetingWorkflow:
 
 async def main():
     # Uncomment the line below to see logging
-    # logging.basicConfig(level=logging.INFO)
+    # Including attribute "foo" on the log format
+    logging.basicConfig(level=logging.INFO, format="--- %(message)s --- %(foo)s")
+
+    # Logging a record with extra attribute "foo" here works fine
+    logger = logging.getLogger("root")
+    logger.info("logging in main", extra={"foo": "in main"})
 
     # Start client
     client = await Client.connect("localhost:7233")
```

Run the example against temporalite, started like:
```shell
temporalite start --ephemeral --namespace=default
```

The expected result is that all 3 log entries are printed, with the `foo` attribute. The actual result is that only the entry generated by the `main()` function is printed, and the two entries on the workflow and activity fail with a `KeyError` exception when trying to index the `foo` attribute.

### Environment/Versions

- OS and processor: Intel Mac, Monterey
- Temporal Version: temporalite version (devel) (server 1.19.1), Python SDK with latest commit https://github.com/temporalio/samples-python/commit/a4f0a05b64c2171332c88f3b595bed03b1623ab6
- Are you using Docker or Kubernetes or building Temporal from source?: No, running directly with Python 3.10 and 3.11, using temporalite



#### Comments (9)

<details>
<summary><strong>cretz</strong> commented on 2023-02-14 21:29:00.000 UTC</summary>

Confirmed, this is caused by us calling calling `super.process()` in our logging adapter which does override `extra`: https://github.com/python/cpython/blob/aad5f6a89125ad4529ab6ed5add693064351df1a/Lib/logging/__init__.py#L1825. Will fix.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>jmdacruz</strong> commented on 2023-02-15 15:30:58.000 UTC</summary>

@cretz I can create a PR removing those calls on all the LoggerAdapters.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-15 16:04:38.000 UTC</summary>

@jmdacruz - Sure, would be much appreciated! Otherwise I was gonna get to it probably early next week

</details>

<details>
<summary><strong>jmdacruz</strong> commented on 2023-02-15 18:37:40.000 UTC</summary>

@cretz I'm taking a stab at this by adding a common `LoggerAdapter` that changes the default behavior of the `process(...)` method by adding the `self.extra` attributes to the `kwargs["extra"]` (if this is there, else reverts to the current behavior). This is overkill now, because the singleton instance of the adapter is created with `extra=None`, but it could prove useful in the future if you decide to add extra attributes on the singleton.

Do you think this makes sense, or should I just remove the calls to `super.process(...)`?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-15 19:54:18.000 UTC</summary>

Just remove the super call

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>jmdacruz</strong> commented on 2023-02-15 21:23:53.000 UTC</summary>

Done, creating PR now

Reactions: â¤ï¸ 1

</details>

<details>
<summary><strong>danmarinescu</strong> commented on 2025-06-06 17:11:02.000 UTC</summary>

While https://github.com/temporalio/sdk-python/pull/280 did fix the issue above it causes another problem. Say I create a logger like this
```
logger = workflow.LoggerAdapter(logging.getLogger(__name__), extra={'some_key':'some_val'})
```
then when the current implementation of `process` is called the extra dict set on the LoggerAdapter will not be included. See for reference the [implementation](https://github.com/python/cpython/blob/3.13/Lib/logging/__init__.py#L1890-L1904) in the `LoggerAdapter` superclass:
```
if self.merge_extra and "extra" in kwargs:
    kwargs["extra"] = {**self.extra, **kwargs["extra"]}
else:
    kwargs["extra"] = self.extra
return msg, kwargs
```

You can see that since we're no longer calling `super.process()` we are also using `self.extra`. 

Imo the right fix here would be to:
- change the `LoggerAdapter` initializer implementation in Temporal to accept a `merge_extra` param like the superclass
- revert the above fix, i.e. call `super.process()` at the start of the `process` method

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-06-06 20:26:39.000 UTC</summary>

Thanks! When this issue was solved there was no `merge_extra`.

> Imo the right fix here would be to:
> * change the LoggerAdapter initializer implementation in Temporal to accept a merge_extra param like the superclass
> * revert the above fix, i.e. call super.process() at the start of the process method

We can't really call super and have this problem solved because `merge_extra` was only added in Python 3.13 and we support more versions than that. Arguably we should have never accepted `extra` in the constructor and instead told users that if they wanted extra data, they needed to subclass and override `process` instead of instantiating.

It sounds like we may need to support our own form of `merge_extra` and default it to `False` which will continue to do the same behavior as today. Or maybe we should only support merge_extra in newer Python versions. I have opened #892.

In the meantime, I would recommend subclassing and overriding `process` to get the extra data needed. 


</details>

<details>
<summary><strong>danmarinescu</strong> commented on 2025-06-10 14:02:05.000 UTC</summary>

Thanks @cretz, that makes sense. Indeed subclassing and overriding `process` is what I did to work around the issue.

</details>


---

<a id="894"></a>

### #894: [Feature Request] Create anaconda package

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/894 |
| **State** | CLOSED |
| **Author** | mikekuzak |
| **Created** | 2025-06-09 21:13:39.000 UTC (7 months ago) |
| **Updated** | 2025-08-26 15:15:51.000 UTC |
| **Closed** | 2025-08-26 15:15:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 8 |
| **Priority Score** | 8 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi,

Would be nice to be able to install this in a plain conda environment without pip

Thanks

#### Comments (8)

<details>
<summary><strong>cretz</strong> commented on 2025-06-09 21:24:38.000 UTC</summary>

We will look and see if this is something we're willing to invest in near term. I see it likely involves maintaining a feedstock repo on the https://github.com/conda-forge org. We may also be able to encourage/support a community effort here if it's something you're interested in.

Reactions: ðŸ‘ 2

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-06-24 13:42:44.000 UTC</summary>

We will leave this issue open, but at this time we are not prioritizing efforts to support anaconda in this repo or publish it ourselves during release. We'd be ok with a community-driven effort at https://github.com/conda-forge though, feel free to pursue that if wanted.

</details>

<details>
<summary><strong>dgasmith</strong> commented on 2025-07-14 15:45:03.000 UTC</summary>

If we publish the package, are there others who are willing to help maintain it?

</details>

<details>
<summary><strong>dgasmith</strong> commented on 2025-07-17 20:12:32.000 UTC</summary>

We (Achira) is going to commit to supporting this on c-f, let me know if others wish to maintain: https://github.com/conda-forge/staged-recipes/pull/30561

</details>

<details>
<summary><strong>dgasmith</strong> commented on 2025-08-25 13:36:57.000 UTC</summary>

Temporalio is now on conda-forge: https://github.com/conda-forge/temporalio-feedstock

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-08-25 13:40:58.000 UTC</summary>

@dgasmith How will the version be kept up-to-date? It looks like it's currently 2 versions behind, and hence is missing some important changes including bug fixes.

</details>

<details>
<summary><strong>dgasmith</strong> commented on 2025-08-26 00:12:03.000 UTC</summary>

@dandavison It's a community effort, I can add you as a maintainer if you wish to help out!

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-08-26 15:15:51.000 UTC</summary>

@dgasmith Thanks, but we'll leave that to the community.

</details>


---

<a id="815"></a>

### #815: [Feature Request] Fail workflow upon pydantic.ValidationError when using the pydantic_data_converter

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/815 |
| **State** | CLOSED |
| **Author** | teije01 |
| **Created** | 2025-04-03 08:40:47.000 UTC (9 months ago) |
| **Updated** | 2025-06-20 22:22:15.000 UTC |
| **Closed** | 2025-06-20 22:22:15.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 6 |
| **Priority Score** | 8 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 â¤ï¸ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

When using the `pydantic_data_converter` from `temporalio.contrib.pydantic` (great feature by the way), it may happen that the payload cannot be decoded due to an incorrect payload. This results in a `pydantic.ValidationError` being raised upon decoding the payload.

The current behavior is for these failures to be [re-raised as a RuntimeError](https://github.com/temporalio/sdk-python/blob/92b77589204a0733f49b5d081942fc9303e436c4/temporalio/worker/_workflow_instance.py#L1703). Then the default behavior for these errors is to result in a `Workflow Task Failed (Failed decoding arguments)` while keeping the workflow running. However, there is no way that such a workflow will ever complete with an incorrect payload.

In this issue https://community.temporal.io/t/force-workflow-failure-when-workflow-cannot-decode-arguments/10680/4 I found that you can specify which exceptions are treated as workflow failure, meaning there is a way to make the workflow fail directly upon workflow failures by using the experimental `workflow_failure_exception_types`.

Specifying the pydantic.ValidationError does not work as it is re-raised by temporalio as a RuntimeError
```python
    workflow_failure_exception_types = [
        pydantic.ValidationError,  # does not work, is re-raised as RuntimeError upon decoding
    ]
    async with Worker(
        client,
        task_queue="task-queue",
        workflow_failure_exception_types=workflow_failure_exception_types,
    ):
```

I'm unsure about the side-effects of treating a RuntimeError as a workflow_failure_exception_types, because it is such a general exception.

### Describe the solution you'd like

I would like to see a workflow failing directly upon 'Failed decoding arguments' while not having to specify a `RuntimeError` as an error in `workflow_failure_exception_types`.

I believe if the pydantic payload converter would raise a [temporalio.exceptions.FailureError](https://github.com/temporalio/sdk-python/blob/92b77589204a0733f49b5d081942fc9303e436c4/temporalio/worker/_workflow_instance.py#L1699), would this then also fail the workflow? because that would seem most preferable.

Or otherwise, if the ValidationError would be re-raised instead of a RuntimeError, this would allow us to fail a workflow more specifically:
```python
    workflow_failure_exception_types = [
        pydantic.ValidationError,
    ]
```


#### Comments (6)

<details>
<summary><strong>dandavison</strong> commented on 2025-04-03 12:26:35.000 UTC</summary>

Thanks for this excellent write-up @teije01.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-03 13:00:53.000 UTC</summary>

ðŸ‘ I think the best solution may be to stop wrapping the exception that comes back from argument conversion, or maybe at least only wrap if `self._is_workflow_failure_exception` is false for it. We already made #329 (as part #321), but that was for `ApplicationError` only because it predates "workflow failure exception types" concept.

</details>

<details>
<summary><strong>teije01</strong> commented on 2025-04-03 13:31:20.000 UTC</summary>

Thanks! #321 does indeed seem like it would solve this more specific converter feature as well. This approach would make most sense to me for decoding failures that are not transient.

> maybe at least only wrap if self._is_workflow_failure_exception is false for it.

That would make sense to me

</details>

<details>
<summary><strong>CtrlAltGiri</strong> commented on 2025-04-20 12:39:57.000 UTC</summary>

@cretz I want to start contributing to this project. 

Would love to pick this up since it's a simple change. 

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-21 12:22:48.000 UTC</summary>

Yes, the change is fairly easy, feel free to submit a PR. Simply do not wrap the exception (i.e. re-raise the original) if the exception "is workflow failure exception".

</details>

<details>
<summary><strong>teije01</strong> commented on 2025-05-13 10:59:20.000 UTC</summary>

> [@cretz](https://github.com/cretz) I want to start contributing to this project.
> 
> Would love to pick this up since it's a simple change.

Let me know if you have time for it, I don't mind picking it up otherwise

</details>


---

<a id="775"></a>

### #775: [Bug] sdk metrics do not include tags added with "telemetry.global_tags"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/775 |
| **State** | CLOSED |
| **Author** | antmendoza (Antonio Mendoza PÃ©rez) |
| **Created** | 2025-02-25 19:43:48.000 UTC (10 months ago) |
| **Updated** | 2025-11-25 17:39:38.000 UTC |
| **Closed** | 2025-11-25 17:39:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 8 |
| **Priority Score** | 8 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?
Add custom tags to SDK metrics 


<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

### Describe the bug

sdk metrics no longer include tags added with telemetry.global_tags , which appear to be the expected behavior https://python.temporal.io/temporalio.runtime.TelemetryConfig.html#global_tags


<img width="788" alt="Image" src="https://github.com/user-attachments/assets/1a05b6f8-7bda-4640-8738-28612cafd8e1" />

running the same with version 1.3 works as expected 

<img width="1456" alt="Image" src="https://github.com/user-attachments/assets/c3c37aa1-671a-491b-ae42-3f38238c5c69" />


### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (8)

<details>
<summary><strong>cretz</strong> commented on 2025-02-25 19:46:01.000 UTC</summary>

To clarify, issue #380 inadvertently didn't set `global_tags` on Rust side. We need to make sure it is set and add a test or alter `test_different_runtimes` to confirm this setting (and maybe any others we want coverage for).

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-02-27 21:27:13.000 UTC</summary>

The issue here is that we use OTel's Prometheus exporting which takes the approach at https://prometheus.io/docs/guides/opentelemetry/#including-resource-attributes-at-query-time. Namely the `global_tags` are on `target_info` which you can combine with the rest at query time. I have opened https://github.com/temporalio/sdk-core/issues/882 to have us put the labels on every metric by default (with an option to restore to today's behavior).

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-25 13:34:40.000 UTC</summary>

Note, just for linking, this is caused by an upstream OTel Rust issue (our Rust core relies on this): https://github.com/open-telemetry/opentelemetry-rust/issues/2858

</details>

<details>
<summary><strong>gregbrowndev</strong> commented on 2025-07-22 11:03:05.000 UTC</summary>

> I have opened https://github.com/temporalio/sdk-core/issues/882 to have us put the labels on every metric by default (with an option to restore to today's behaviour).

@cretz, what was the original intended behaviour of `global_tags` supposed to be? It could be a bit dangerous to change the default behaviour to updating metric labels now, as it could catch people off guard if they now rely on the current behaviour.

For a long time on my project, I've been using `global_tags` to set labels like `service_name`, `service_namespace`, etc. But I had a downstream Alloy collector that added these labels to all metrics (not just Temporal SDKs) from the OTEL resource by default, if they weren't already set. So I never noticed the regression in behaviour, since `global_tags` changed to update the OTEL resource / Prometheus `target_info` in the SDK, they were set as metric labels by the collector instead...

Since then, I've added an ECS resource detector to the app, so I can set OTEL resource attributes like `aws.ecs.task.id`, `aws.ecs.cluster.arn`, etc. to all telemetry, and I've been using `global_tags` to do this for the SDK metrics, assuming this was always the way it was intended to work (since there isn't any other option to set an OTEL resource).

So I'd suggest, if possible, rather than have an option that toggles the behaviour of `global_tags`. It would be better to have two options, one to set resource attributes (maybe directly in `OpenTelemetryConfig`) and one to set global metric labels. There may be cases where people want to have both options simultaneously.

Cheers ðŸ™ðŸ» 




</details>

<details>
<summary><strong>Sushisource</strong> commented on 2025-07-24 21:18:29.000 UTC</summary>

The original intended behavior is that the tags would apply to all emitted metrics. I agree with you that the two separate options for OTel makes good sense, we can add an item to track adding those options.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-08-18 17:20:32.000 UTC</summary>

@gregbrowndev - with `global_tags` now applying to all metrics (something we didn't intend to undo), is that acceptable? Or are you asking for a new setting for, say, `target_info_tags`?

</details>

<details>
<summary><strong>gregbrowndev</strong> commented on 2025-08-26 23:15:01.000 UTC</summary>

@cretz sorry if I was unclear. For me, if we only had one option, it would make more sense to be able to configure the OTEL resource attributes / Prometheus `target_info` via the `TelemetryConfig`, rather than adding common attributes to every metric. If we had both options, then that would be better ofc.

(This is probably quite bias from having an OTEL/Alloy collector set up, but...)

The latter is a convenience. Its quite easy to add metric labels (datapoint attributes in OTEL) anywhere outside of the application by reading them from the resource attributes ([example](https://grafana.com/docs/alloy/latest/reference/components/otelcol/otelcol.exporter.prometheus/#create-prometheus-labels-from-otlp-resource-attributes)). However, it'd be pretty horrible to configure the right Resource attributes if there wasn't a direct way to do it in the SDK. You'd basically have to intercept the metrics after they've been emitted by the Rust SDK with another collector sidecar, so you could use a [resource detector](https://grafana.com/docs/alloy/latest/reference/components/otelcol/otelcol.processor.resourcedetection/) to fix them before sending them along.

However, maybe the opposite is true for Prometheus scraping job users. The difference is OTEL is primarily push-based, so its better to set the resource attributes at the source (I'm using the same `Resource` object from an ECS detector library within the Python app to configure metrics, logs, traces, and profiles, for example. The metrics are sent to Prometheus as well...). But Prometheus scraping is pull-based, and you'd typically need service discovery anyway to do it at scale. In k8s, you get the resource metadata built-in with that (I think).

I might be wrong, but I think you can relabel metrics in Prometheus scrape config using `target_info` in the same way that Alloy example does it. And for me it would be nicer to have a single `Resource` object in Python that can configure all the telemetry the same way, not just Prometheus metrics.

Hope that helps 

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-08-27 12:43:03.000 UTC</summary>

> For me, if we only had one option, it would make more sense to be able to configure the OTEL resource attributes / Prometheus target_info via the TelemetryConfig, rather than adding common attributes to every metric.

Unfortunately many others saw the one option, `global_tags`, as meaning global and applying on every metric, and that is how we had it at one point and regressed, but we fixed the regression.

Can you be a bit more concrete on the specific ask here? Knowing that we cannot alter the `global_tags` behavior that people currently rely on, what other option might you need and what should its behavior be?

</details>


---

<a id="758"></a>

### #758: [Feature Request] Suggest Providing activities extraction utilities to get them from a class and a module

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/758 |
| **State** | OPEN |
| **Author** | spacether (Justin Black) |
| **Created** | 2025-01-31 19:52:04.000 UTC (11 months ago) |
| **Updated** | 2025-06-05 02:42:36.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 6 |
| **Priority Score** | 8 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

Creating activities and decorating them is easy using activy.def in python
Making sure that they are all included in worker launch is more difficult.

### Describe the solution you'd like

It would be helpful if a utility was provided that allowed
- extracting activities from a class
- extracting activities from a class instance
- extracting activities from a module

Below is a sample implementation for extraction from class and class instances, assuming async method implementations that uses ast and inspection.

It looks like one could find which methods are decorate by checking if `fn.__temporal_activity_definition` but that is a private variable name and is not exposed in the temporalio activity.py module. Code that uses __temporal_activity_definition would be simpler and not invoking `ast.parse(inspect.getsource(cls))` is preferrable.
```
import ast
import inspect
import typing


class _MyNodeVisitor(ast.NodeVisitor):
    def __init__(self):
        self.fn_name_to_decorators: dict[str, set[str]] = {}

    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):
        self.fn_name_to_decorators[node.name] = set()
        for decorator in node.decorator_list:
            print(decorator)
            if isinstance(decorator, ast.Call):
                # noinspection PyUnresolvedReferences
                name = (
                    decorator.func.attr
                    if isinstance(decorator.func, ast.Attribute)
                    else decorator.func.id
                )
            else:
                # noinspection PyUnresolvedReferences
                name = (
                    decorator.value.id + "." + decorator.attr
                    if isinstance(decorator, ast.Attribute)
                    else decorator.id
                )

            self.fn_name_to_decorators[node.name].add(name)

    def get_fn_name_to_decorators(self) -> dict[str, set[str]]:
        return self.fn_name_to_decorators


class ActivitiesListProvider:
    @classmethod
    def __get_activities(
        cls,
        instance: typing.Union[
            type["ActivitiesListProvider"], "ActivitiesListProvider"
        ],
    ) -> list[typing.Callable]:
        visitor = _MyNodeVisitor()
        visitor.visit(ast.parse(inspect.getsource(cls)))
        fn_name_to_decorators: dict[str, set[str]] = visitor.get_fn_name_to_decorators()

        activities = []
        for fn_name, decorators in fn_name_to_decorators.items():
            if "activity.defn" in decorators:
                method = getattr(instance, fn_name)
                activities.append(method)
        return activities

    def get_activities_from_instance(self) -> list[typing.Callable]:
        return self.__get_activities(self)

    @classmethod
    def get_activities_from_cls(cls) -> list[typing.Callable]:
        return cls.__get_activities(cls)
```
And some Tests:
```
from workflow_metrics.temporal_tools import activities_class

from temporalio import activity


class SomeActivities(activities_class.ActivitiesListProvider):
    @activity.defn
    async def instance_method_activity(self):
        pass

    @activity.defn
    async def class_method_activity(self):
        pass

    @staticmethod
    @activity.defn
    async def static_method_activity():
        pass


def test_get_activities_from_cls():
    assert SomeActivities.get_activities_from_cls() == [
        SomeActivities.instance_method_activity,
        SomeActivities.class_method_activity,
        SomeActivities.static_method_activity,
    ]


class ActivitiesClassThatNeedsInstance(activities_class.ActivitiesListProvider):
    @activity.defn
    async def instance_method_activity(self):
        pass

    @activity.defn
    async def class_method_activity(self):
        pass

    @staticmethod
    @activity.defn
    async def static_method_activity():
        pass


def test_get_activities_from_instance():
    inst = ActivitiesClassThatNeedsInstance()
    assert inst.get_activities_from_instance() == [
        inst.instance_method_activity,
        inst.class_method_activity,
        inst.static_method_activity,
    ]
```

### Additional context

In code that I am working on activities are mainly defined in one module when they are fns and in class methods.


#### Comments (6)

<details>
<summary><strong>cretz</strong> commented on 2025-02-03 16:58:12.000 UTC</summary>

Sorry, just noticed this feature issue. Having discussion if viability on PR at https://github.com/temporalio/sdk-python/pull/759#discussion_r1939572982. Also circulating with team.

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-02-03 17:32:22.000 UTC</summary>

Hi @spacether, thanks for the input here. Before getting into implementation, can you expand on the problem that you want to see solved and what you see as the requirements for possible solutions? I.e. expand on this

> Making sure that they are all included in worker launch is more difficult.

E.g. what is it that you don't like currently, and, without speculating about specific implementations, what sorts of behaviors / semantics regarding activities would you like users to be able to express when starting a worker? It might help if you sketch some Worker launch code featuring imaginary APIs / function calls that would give you the semantics that you want.

</details>

<details>
<summary><strong>spacether</strong> commented on 2025-02-03 18:09:04.000 UTC</summary>

So right now it is entirely up to a develop to manually build an explicit list of activities in python. Our activities are already segregated by the kind fo work that they do into a python modules and activity classes. So then when we need to use them in a worker, one has to to list every activity decorated funcction in that module and in one or two classes when we already know that the activities I need come from these 3 sources: 1 module, and two classes. Adding these utilities lets me vend activities from those sources easily.

I would like:
```
worker = Worker(
  activities=[*ActivitiesClass.get_acitivities(), *get_activities(activities_module)]
)
```
or
```
worker = Worker(
  activities_classes=(ActivitiesClass,)
  activities_modules=(activities_module),
)
```
or
```
worker = Worker(
  activities=[ActivitiesClass, activities_module, etc...]
)
```
One could make activities accept a list of (callables or a module or a class that has activities methods in it), where all of the activities from the class or module would be loaded into the worker.

</details>

<details>
<summary><strong>noxasaxon</strong> commented on 2025-02-04 09:43:31.000 UTC</summary>

I recently came to the same conclusion as @spacether and made a similar function for collecting activities as part of a utility library for enforcing best practices at test time to avoid runtime failures. 

My developers use a collection function in a worker.py file so that they don't forget to add an activity method to the worker every time they write a new one.

I also have a function that is intended to run in a test, you just import your project's temporal directory module and it gets every @activity.defn method in the module and submodules and so that we can validate everything automatically:

Repo is very much a work in progress still, not much documentation for the function-based validation as opposed to inheriting from a special validator class but the collection methods are here:

https://github.com/noxasaxon/temporal_utils_python/blob/main/src/temporal_utils/collectors.py

</details>

<details>
<summary><strong>charlesmelby</strong> commented on 2025-06-05 02:11:34.000 UTC</summary>

I've resorted more than once to wrapping temporal's activity and workflow decorators in my own to make activity and workflow collection easy.

Typically speaking, I have two needs:
1. be able to import activities easily from a file or folder
2. be able to put some filters in place -- e.g. allow certain test-oriented activities to be co-located with standard activities to avoid a fragmented codebase.

Manual import is messy because
1. it is error-prone, especially in the case of things like feature flags
2. it makes for poor separation of concerns, since the worker needs to be *explicitly* aware of every piece of functionality it serves.

Personally I'd be just as happy with a method that provides for easy collection, e.g.
```py
from temporalio import worker
import my_activities
from other_activities import ActivityClass

w = worker.Worker(
  ...,
  activities=worker.collect_activities(
    "path/to/activity/folder",
    my_activities,
    ActivityClass
  ), ...
)
```
together with some simple filters.
One that would be useful is a skip directive in the activity decorator, which causes collection to be skipped (but doesn't prevent it from being registered explicitly with a worker):
```py
@activity.defn(skip=True)
async def my_activity():
    ...
```
This would facilitate e.g. test cases, dev/prod feature flags, and the like.

More flexible and comprehensive would be the ability to configure tags (this could easily be used in lieu of `skip`):
```py
@activity.defn(tags=["dev", "e2e"])
async def prototyped_task():
    ...
```
together with filtering directives in the collect_activities function:
```py
activities = worker.collect_activities(module, tags=["prod"])
activities = worker.collect_activities(module, exclude_tags=["test"])
```
Finally, some kind of name pattern filter might be nice (though once again this could generally be handled by tags):
```py
activities = worker.collect_activities(module, regex=r".*_v[45]|database_.*")
```

My activities are usually bare functions so I don't have a real opinion on the best approach for activities that are owned by a class or class instance.

</details>

<details>
<summary><strong>charlesmelby</strong> commented on 2025-06-05 02:41:48.000 UTC</summary>

As a side note, one important issue to address is being able to deal effectively with mocks for tests.
With my current wrappers I actually just collect the activities at definition time (optionally grouped by worker name), but also provide facilities to override them inside tests: 
```py
@flows.activity
async def real_activity():
  ...

@flows.activity(worker="other_worker")
async def other_activity():
  ...

async def real_activity_mock():
  ...

async def other_activity_mock():
  ...

@flows.testing.case
@flows.testing.provide(activities={"real_activity": other_activity_mock})
@pytest.mark.asyncio
async def test_with_mock_other_activity(flow_runner):
  await flow_runner.execute_workflow(my_workflow, ...)

# actually mainly used to mock activities handled by remote workers,
# but can be used with other workers in the same process
@flows.testing.case
@flows.testing.provide("other_worker", activities={"other_activity": other_activity_mock})
@pytest.mark.asyncio
async def test_with_mock_other_activity(flow_runner):
  await flow_runner.execute_workflow(multi_worker_workflow, ...)
```

With collection by a function as above configuring concise declarative tests might be more challenging.
Including an override directive could be helpful for this, e.g.
```py
worker.collect_activities(module, overrides={"my_activity": my_activity_mock})
```

</details>


---

<a id="679"></a>

### #679: [Bug/Question] non_retryable_error_types seems not working

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/679 |
| **State** | CLOSED |
| **Author** | yitian-reevo (Yitian Huang) |
| **Created** | 2024-10-29 20:42:48.000 UTC (1y 2m ago) |
| **Updated** | 2024-12-26 13:57:26.000 UTC |
| **Closed** | 2024-10-30 18:04:12.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 8 |
| **Priority Score** | 8 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm testing the behavior of RetryPolicy for an activity, esp the `non retryable` case

### Describe the bug
The code I used for testing as following: 

Workflow:
```
from datetime import timedelta

from temporalio import workflow

with workflow.unsafe.imports_passed_through():
    from salestech_be.temporal.activities import (
        say_hello,
    )
    from temporalio.common import RetryPolicy


@workflow.defn
class SayHello:
    @workflow.run
    async def run(self, name: str) -> str:

        return await workflow.execute_activity(
            say_hello, name, start_to_close_timeout=timedelta(seconds=5),
            retry_policy=RetryPolicy(
                maximum_interval=timedelta(seconds=1),
                non_retryable_error_types=["MY_CUSTOM_ERROR_TYPE"],
                maximum_attempts=2,
            )
        )
```
Activity
```
from temporalio import activity
from temporalio.exceptions import ApplicationError


@activity.defn
async def say_hello(name: str) -> str:
    raise ApplicationError(
            message="test",
            details={"reason": "test"},
            type="MY_CUSTOM_ERROR_TYPE",
            non_retryable=True,
        )
    return f"Hello, {name}!"
```
I also tried different combinations of params when raise the error, none of them works.

Test Script:
```
await client.start_workflow(
    SayHello.run,
    "test_say_hello",
    id=f"test_say_hello_{uuid4()}",
    task_queue=DEFAULT_TASK_QUEUE,
    id_reuse_policy=WorkflowIDReusePolicy.ALLOW_DUPLICATE_FAILED_ONLY,
)
```
Based on the docs I read, what I expected is this activity should not be retried, and there will be a Activity Task Failed record in the Event history with something like non-retryable error content.

but in fact, the activity is still being retried and fail on a start_to_close timeout eventually, can I ask some help for understanding this? is it a bug, or I misunderstood something? much appreciate!

### Minimal Reproduction

see above

### Environment/Versions

- OS and processor: m2 macbook pro 15.1
- Temporal Version:  1.25.0 and SDK version 1.8.0
- Are you using Docker or Kubernetes or building Temporal from source? using Docker

### Additional context


#### Comments (8)

<details>
<summary><strong>cretz</strong> commented on 2024-10-30 12:50:39.000 UTC</summary>

When I run your code I get:

```
TypeError: ApplicationError.__init__() got an unexpected keyword argument 'details'
```

This is because `details` is a vararg. Changing your code to remove `message=` and `details=`, the workflow properly fails bubbling out the activity failure.

> the activity is still being retried and fail on a start_to_close timeout eventually

Hrmm, this is a bit confusing. A start to close timeout only fails if the activity body exceeds the timeout, but your code raises immediately. Schedule to close or schedule to start timeouts may make sense if you forgot to register your activity or if you don't have a worker running.

</details>

<details>
<summary><strong>yitian-reevo</strong> commented on 2024-10-30 12:55:57.000 UTC</summary>

thanks, @cretz, yep I find the 'details' errors as well, but things don't change if i removed the `details` field. And I'm pretty sure I did register the activity and had a working running.

Are you able to reproduce this your side? it should not be two complicated to reproduce, I've confirmed with one of my team and he claims he faces the same issue.

We both use MacOs with M2 chips, not sure if this matters. just fyi.

</details>

<details>
<summary><strong>yitian-reevo</strong> commented on 2024-10-30 13:03:25.000 UTC</summary>

oh sorry, i missed this line:
> Changing your code to remove message= and details=, the workflow properly fails bubbling out the activity failure.

let me try again.

</details>

<details>
<summary><strong>yitian-reevo</strong> commented on 2024-10-30 18:04:13.000 UTC</summary>

@cretz  After hours of debugging, I can confirm this retry not working issue is due to somehow a conflict of our custom logger built on loguru. But haven't have a clue on how to solve it. At least find the cause now. thanks!

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>barbatus</strong> commented on 2024-12-23 14:46:53.000 UTC</summary>

@yitian-reevo I am wondering did you solve the issue? we are having same issues and also using loguru and it's not clear at all form the first sight at least where does exactly the issue lie, so I'd really appreciate your insight ðŸ™

</details>

<details>
<summary><strong>yitian-reevo</strong> commented on 2024-12-23 23:20:04.000 UTC</summary>

@barbatus yup, the root cause for us is we had a custom patcher when init the loguru logger, and inside the patch there is deserialization bug. So this might not a common issue.
 



</details>

<details>
<summary><strong>barbatus</strong> commented on 2024-12-26 10:28:42.000 UTC</summary>

@yitian-reevo I assume an activity now cancels right away when you raise `ApplicationError(type="MY_CUSTOM_ERROR_TYPE")` and it does not wait start_to_close_timeout?

</details>

<details>
<summary><strong>yitian-reevo</strong> commented on 2024-12-26 13:57:25.000 UTC</summary>

@barbatus that's correct, exceptions listed in non_retryable_error param will terminate the acitivity immediately.

I would suggest you setup a tempory pure Temporl env (e.g in a dokcer) to test how it behaves, then you can have a clear view on this feature.

</details>


---

<a id="672"></a>

### #672: [Feature Request] Drop 3.8 support, confirm 3.13 support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/672 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-10-15 20:23:28.000 UTC (1y 2m ago) |
| **Updated** | 2025-02-03 15:05:14.000 UTC |
| **Closed** | 2025-01-10 16:18:01.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 6 |
| **Priority Score** | 8 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 ðŸŽ‰ 1 |

#### Description

### Describe the solution you'd like

See #398 (PR #422) where we did this for 3.7/3.12. Can see the several things that were done there and similar should be done here.

#### Comments (6)

<details>
<summary><strong>josh-berry</strong> commented on 2024-11-21 20:27:22.000 UTC</summary>

I happened to discover by accident that grpcio does not yet support Python 3.13 because it apparently depends on some C APIs that have been removed. So it seems unlikely we'd be able to support it without updating grpcio (if a fix is available; it may already be)

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-11-21 22:31:24.000 UTC</summary>

`grpcio` is an optional dependency. We only added it because we do generate the gRPC stubs in the `temporalio.api.workflowservice.v1.service_pb2_grpc` and _technically_ allow users to use that module if they want pure Python gRPC clients or servers even though we don't use it in the SDK. We do use it in tests to implement a custom gRPC server and figured "why not let users use the generated gRPC stuff too" in addition to protos. We may be able to remove this stuff if we have to (otherwise we may be able to upgrade it, it all depends on how old of a protobuf library we need to support).

</details>

<details>
<summary><strong>sciyoshi</strong> commented on 2024-12-04 19:16:51.000 UTC</summary>

Tracebacks are currently broken in Python 3.13. The original traceback for an exception happening in a workflow is clobbered and instead the following error is shown:

```
  File "/code/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow.py", line 260, in _handle_activation
    completion = await asyncio.wait_for(
                 ^^^^^^^^^^^^^^^^^^^^^^^
        activate_task, self._deadlock_timeout_seconds
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^

  File "/usr/local/lib/python3.13/asyncio/tasks.py", line 507, in wait_for
    return await fut
           ^^^^^^^^^

  File "/usr/local/lib/python3.13/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)

  File "/code/.venv/lib/python3.13/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 160, in activate
    self._run_code(
    ~~~~~~~~~~~~~~^
        "with __temporal_importer.applied():\n"
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        __temporal_activation=act,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^

  File "/code/.venv/lib/python3.13/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 176, in _run_code
    exec(code, self.globals_and_locals, self.globals_and_locals)
    ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "<string>", line 2, in <module>

  File "/code/.venv/lib/python3.13/site-packages/temporalio/worker/workflow_sandbox/_in_sandbox.py", line 81, in activate
    return self.instance.activate(act)
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^

  File "/code/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py", line 411, in activate
    logger.warning(
    ~~~~~~~~~~~~~~^
        f"Failed activation on workflow {self._info.workflow_type} with ID {self._info.workflow_id} and run ID {self._info.run_id}",
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        exc_info=activation_err,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        extra={"temporal_workflow": self._info._logger_details()},
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^

  File "/usr/local/lib/python3.13/logging/__init__.py", line 1531, in warning
    self._log(WARNING, msg, args, **kwargs)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.13/logging/__init__.py", line 1664, in _log
    self.handle(record)
    ~~~~~~~~~~~^^^^^^^^

  File "/usr/local/lib/python3.13/logging/__init__.py", line 1680, in handle
    self.callHandlers(record)
    ~~~~~~~~~~~~~~~~~^^^^^^^^

  File "/usr/local/lib/python3.13/logging/__init__.py", line 1744, in callHandlers
    lastResort.handle(record)
    ~~~~~~~~~~~~~~~~~^^^^^^^^

  File "/usr/local/lib/python3.13/logging/__init__.py", line 1026, in handle
    self.emit(record)
    ~~~~~~~~~^^^^^^^^

  File "/usr/local/lib/python3.13/logging/__init__.py", line 1158, in emit
    self.handleError(record)
    ~~~~~~~~~~~~~~~~^^^^^^^^

  File "/usr/local/lib/python3.13/logging/__init__.py", line 1075, in handleError
    traceback.print_exception(exc, limit=None, file=sys.stderr)
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.13/traceback.py", line 129, in print_exception
    te = TracebackException(type(value), value, tb, limit=limit, compact=True)

  File "/usr/local/lib/python3.13/traceback.py", line 1044, in __init__
    self.stack = StackSummary._extract_from_extended_frame_gen(
                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        _walk_tb_with_full_positions(exc_traceback),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        limit=limit, lookup_lines=lookup_lines,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        capture_locals=capture_locals)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.13/traceback.py", line 492, in _extract_from_extended_frame_gen
    f.line

  File "/usr/local/lib/python3.13/traceback.py", line 369, in line
    self._set_lines()
    ~~~~~~~~~~~~~~~^^

  File "/usr/local/lib/python3.13/traceback.py", line 350, in _set_lines
    lines.append(linecache.getline(self.filename, lineno).rstrip())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^

  File "/usr/local/lib/python3.13/linecache.py", line 25, in getline
    lines = getlines(filename, module_globals)

  File "/usr/local/lib/python3.13/linecache.py", line 41, in getlines
    return updatecache(filename, module_globals)

  File "/usr/local/lib/python3.13/linecache.py", line 100, in updatecache
    stat = os.stat(fullname)

  File "/code/.venv/lib/python3.13/site-packages/temporalio/worker/workflow_sandbox/_restrictions.py", line 964, in __call__
    state.assert_child_not_restricted("__call__")
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^

  File "/code/.venv/lib/python3.13/site-packages/temporalio/worker/workflow_sandbox/_restrictions.py", line 792, in assert_child_not_restricted
    raise RestrictedWorkflowAccessError(
        f"{self.name}.{name}", override_message=matcher.leaf_message
    )
```

The error message occurs from an attempt to access linecache inside the original exception, leading to this:

```
Cannot access os.stat.__call__ from inside a workflow. If this is code from a module not used in a workflow or known to only be used deterministically from a workflow, mark the import as pass through.
```

Reactions: ðŸ‘ 2 ðŸ‘€ 2

</details>

<details>
<summary><strong>cprafullchandra</strong> commented on 2025-01-28 23:47:53.000 UTC</summary>

Hi, I am still seeing this issue.

```
temporalio.worker.workflow_sandbox._restrictions.RestrictedWorkflowAccessError: Cannot access os.stat.__call__ from inside a workflow. If this is code from a module not used in a workflow or known to only be used deterministically from a workflow, mark the import as pass through.
```

Python = 3.13.0
temporalio = 1.9.0
temporal (server) = 1.2.0

Reverting to Python 3.12 fixed the tracebacks so I could see the actual error.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>FatihKartal</strong> commented on 2025-02-03 14:26:46.000 UTC</summary>

Also facing what @cprafullchandra mentioned when hosting our workers for Temporal cloud. I hope this can be resolved soon as we are locked into Python 3.13. currently.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-02-03 15:05:13.000 UTC</summary>

3.13 support is in the next release (should not be too long now, though no exact timelines)

Reactions: ðŸŽ‰ 3

</details>


---

<a id="232"></a>

### #232: [Bug] Python 3.11 TypeError: Passing coroutines is forbidden, use tasks explicitly.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/232 |
| **State** | CLOSED |
| **Author** | AndrewIngram (Andy Ingram) |
| **Created** | 2022-12-23 15:46:10.000 UTC (3 years ago) |
| **Updated** | 2023-01-04 17:47:06.000 UTC |
| **Closed** | 2023-01-04 17:47:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 8 |
| **Priority Score** | 8 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

The following lines of code run without issue in Python 3.10, but throw in 3.11:

```python
await asyncio.wait(
    [asyncio.sleep((snooze_until - now).total_seconds()), self._is_reopened.wait()],
    return_when=asyncio.FIRST_COMPLETED,
)
```

(this is similar to an example in the readme, so it seems idiomatic)

The error I get in 3.11 is `TypeError: Passing coroutines is forbidden, use tasks explicitly.`

Encountered on the `python:3.11-slim` Docker image, initially using `temporalio=0.1b3`, reproduced on `0.1b4` 

I think the relevant Python docs are https://docs.python.org/3/library/asyncio-task.html#waiting-primitives

> Passing coroutine objects to wait() directly is forbidden

Not sure if I'd class it as a bug, as the change in Python appears to be deliberate, but it means the canonical way to solve this in Temporal might now be different?

#### Comments (8)

<details>
<summary><strong>cretz</strong> commented on 2022-12-23 15:53:30.000 UTC</summary>

This is a known Python change and unrelated to Temporal. Why use wait with a sleep instead of just sleep? Also, when using wait, you need a task. So use create_task or similar.

I will update the README to use tasks.

</details>

<details>
<summary><strong>AndrewIngram</strong> commented on 2022-12-23 15:59:14.000 UTC</summary>

I'm not sure I understand your question, I want to wait for a specific time, or an event, whichever happens first.

But I hit the same issue even without the sleep and just using wait on the event (as per the readme example), so i'm keen to see what the canonical way of doing this is -- i'd been assuming it was what's in the readme.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-12-23 16:02:12.000 UTC</summary>

Sorry, my mobile phone cut off the right part, I see the event wait now.

The proper way since Python 3.11 is to use tasks (so wrap your pieces in create_task). We just haven't updated the README yet. This is not Temporal specific, you'd do the same thing using normal asyncio in Python 3.11.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-12-23 16:03:20.000 UTC</summary>

Though to wait for something until timeout, wait_for is the better approach

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>AndrewIngram</strong> commented on 2022-12-23 16:17:48.000 UTC</summary>

Do you mean like this (which seems to work in 3.11)?

```python
try:
    await asyncio.wait_for(
        self._is_reopened.wait(), timeout=(snooze_until - now).total_seconds()
    )
except TimeoutError:
    pass
```



</details>

<details>
<summary><strong>AndrewIngram</strong> commented on 2022-12-23 16:18:28.000 UTC</summary>

Do you want me to leave this open so you can track the readme change, or close it?

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-12-23 16:23:05.000 UTC</summary>

That wait_for example looks fine. Yes you can leave this open for me to update the README.

The best practice is to use asyncio in Temporal like you would outside of Temporal.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>AndrewIngram</strong> commented on 2022-12-23 16:26:20.000 UTC</summary>

Easier said then done when this is our first time using async Python in anger after a 15+ years of non-async Python :)

</details>


---

<a id="78"></a>

### #78: Improve dataclass type hinting

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/78 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-07-19 13:58:17.000 UTC (3y 5m ago) |
| **Updated** | 2022-08-12 12:18:06.000 UTC |
| **Closed** | 2022-08-12 12:18:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 8 |
| **Priority Score** | 8 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | beta-2 |

#### Description

### Is your feature request related to a problem? Please describe.

Our use of https://github.com/konradhalas/dacite is too primitive. It only supports to/from the object itself, not collections, optional, etc.

### Describe the solution you'd like

All of the following should be supported via type hints:

* `MyDataClass`
* `List[MyDataClass]`, `Set[MyDataClass]`, other collections, etc
* `Mapping[str, MyDataClass]`, `Dict[str, MyDataClass]`, other dicts, etc
* `Union[MyDataClass1, MyDataClass2]`, `Union[List[int], Dict[str, MyDataClass]]`, etc
* `Optional[MyDataClass1]`, `Optional[List[Dict[str, MyDataClass]]]`, etc

Here are the libraries found so far:

* https://github.com/konradhalas/dacite
  * What we currently use
  * No top-level collection, mapping, etc support
  * Could be good base to build off of
* https://github.com/lidatong/dataclasses-json
  * No support for non-decorator/non-mixin use (see issue on their repo numbered 243)
  * Requires marshmallow
* https://github.com/rnag/dataclass-wizard
  * Not the most popular (yet?)
  * Does not require inheritance/mixin use
  * Still requires us to figure out whether it's a dict or list or what not
* https://github.com/lovasoa/marshmallow_dataclass
  * Requires marshmallow (which is a popular library and therefore may clash with other users' marshmallow needs)
  * Seems to be the layer backing dataclasses-json
  * Will still have to determine data types expected

We should probably support customization via `instance.to_json()`/`instance.to_dict()` and `Class.to_json()`/`Class.to_dict()`. This means those using some of those other libraries will work automatically.

Needs investigation...

#### Comments (8)

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-19 16:36:55.000 UTC</summary>

> Our use of https://github.com/konradhalas/dacite is too primitive. It only supports to/from the object itself, not collections, optional, etc.

@cretz, just in case you haven't heard of them, I'd like to point out [pydantic](https://github.com/samuelcolvin/pydantic) and [fastapi](https://github.com/tiangolo/fastapi) as potentially useful references for this issue.

Pydantic is basically a better dataclasses that's defining feature is really good runtime data validation using types. I use it for serialization/de-serialization with Temporal via a data converter. I find having this runtime checking very useful for catching issues in task processing.

FastAPI is a web framework that uses Pydantic to for serialization/de-serialization. They handle the cases you are referencing here, like taking a list or other generic as an argument. It might be worth a look.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-19 19:28:03.000 UTC</summary>

:+1: One of my fears w/ pydantic or any of the other high profile modelling dependencies is that I would be carrying that dependency with the SDK. But I will look into using it if adding it isn't too much of a burden on SDK users. It seems the major version is stable enough.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-20 20:51:27.000 UTC</summary>

Yeah, I feel like Pydantic should definitely be optional dependency if included at all. I mostly wanted to point out FastAPI as an example of something that's done exactly what this issue describing.

I could imagine libraries building on top of Temporal (or any task library, really) to provide a FastAPI-like developer experience that's more opinionated and makes more choices for you. But having an extensions/contrib/whatever optional module would be nice too.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-20 22:05:02.000 UTC</summary>

> Yeah, I feel like Pydantic should definitely be optional dependency if included at all. 

I think I have to support dataclass type-hint-based payload conversion out of the box though. But maybe not. Will think on it.

</details>

<details>
<summary><strong>NullOsama</strong> commented on 2022-07-28 15:35:05.000 UTC</summary>

> Yeah, I feel like Pydantic should definitely be optional dependency if included at all. I mostly wanted to point out FastAPI as an example of something that's done exactly what this issue describing.
> 
> I could imagine libraries building on top of Temporal (or any task library, really) to provide a FastAPI-like developer experience that's more opinionated and makes more choices for you. But having an extensions/contrib/whatever optional module would be nice too.

+1 on that.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-08-02 16:03:59.000 UTC</summary>

Unfortunately general-purpose dataclass use with Pydantic appears limited. I have opened discussion at https://github.com/samuelcolvin/pydantic/discussions/4318. I will try other options, but I cannot yet find a single Python library that I can give a JSON list of `[{"foo": "bar"}]` and a type hint of `List[MyDataClass]` and expect it to do the right thing. 

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-08-02 16:23:35.000 UTC</summary>

Alternatively, if I don't want to write a full JSON parser using type hints, we could only support [TypedDict](https://docs.python.org/3/library/typing.html#typing.TypedDict) for now and leave dataclasses alone.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-08-02 18:50:17.000 UTC</summary>

Ok, after internal discussion, we are going to write our own JSON deserializer. I am probably going to borrow from https://github.com/yukinarit/pyserde but remove the requirement to decorate the classes.

</details>


---

<a id="1224"></a>

### #1224: Cannot reliably catch workflow cancellation with try/except pattern

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1224 |
| **State** | CLOSED |
| **Author** | pinkynrg (Francesco Meli) |
| **Created** | 2025-11-25 09:14:00.000 UTC (1 months ago) |
| **Updated** | 2025-12-05 19:21:25.000 UTC |
| **Closed** | 2025-12-05 19:21:25.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 3 |
| **Priority Score** | 7 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 2 |

#### Description

### What are you really trying to do?

I'm building an AI agent workflow using Temporal and the OpenAI Agents SDK. Users can start long-running agent conversations and need the ability to cancel them mid-execution using Temporal's built-in `.cancel()` method. When a user requests cancellation, I want to:

1. Stop the agent execution immediately (or as soon as possible)
2. Execute my `handle_cancellation()` method to perform cleanup (save interrupt tag, notify frontend, clear session)
3. Have the cancellation handler run reliably every time

The agent execution happens via `await Runner.run()` from the OpenAI Agents SDK, which can take several minutes to complete.

### Describe the bug

When a user requests workflow cancellation via Temporal's `.cancel()` method, there's a race condition where the workflow gets marked as "Cancelled" by Temporal before my exception handler can execute `handle_cancellation()`. 

More or less 10% of the time, **no `CancelledError` exception was raised** during the workflow task execution, so my `except` block never ran to call `handle_cancellation()`. The workflow was cancelled, after the task completed successfully.

For the workflow result I get:

```
{
  "type": "workflowExecutionCanceledEventAttributes",
  "workflowTaskCompletedEventId": "41"
}
```

**Expected behavior:**

The `except` block should catch the cancellation and execute `handle_cancellation()`:

```python
except Exception as e:
    if is_cancelled_exception(e) or (e.__cause__ and is_cancelled_exception(e.__cause__)):
        workflow.logger.info("AgentWorkflow cancelled by user")
        return await self.handle_cancellation(topic, session, payload)
```

**Actual behavior:**

The workflow is cancelled by Temporal before the exception handler runs, so `handle_cancellation()` never executes and:
- Event log shows "Workflow cancelled" instead of showing cleanup activities
- No interrupt tag is saved to OpenSearch
- Frontend never receives cancellation notification
- Session is not cleared
- Workflow status shows "Cancelled" instead of "Completed"

### Minimal Reproduction

```python
from temporalio import workflow
from temporalio.exceptions import is_cancelled_exception
from agents import Runner

@workflow.defn
class AgentWorkflow:
    async def handle_cancellation(self, topic: str, session, payload):
        """
        Critical cleanup:
        - Save interrupt tag to message history
        - Notify frontend
        - Clear session
        """
        workflow.logger.info("Cleanup executed")
        interrupt_tag = "<interrupt />"
        
        # Save interrupt tag to OpenSearch
        await workflow.execute_activity(
            activity=OpenSearchActivities.save_agent_message_history,
            args=(SaveAgentMessageHistory(
                workflow_id=workflow.info().workflow_id,
                session_id=session.session_id,
                messages=[{"content": interrupt_tag, "role": "assistant", "type": "message"}]
            ),),
            start_to_close_timeout=DEFAULT_START_TO_CLOSE_TIMEOUT,
            retry_policy=DEFAULT_RETRY_POLICY,
        )
        
        # Notify frontend
        await notify_frontend(
            topic=topic,
            message=MessageAgentFinalResponseToFrontend(value=interrupt_tag, session_id=session.session_id),
        )
        
        return interrupt_tag
    
    @workflow.run
    async def run(self, payload) -> str | None:
        try:
            topic = f"agent_{payload.uid}_{payload.mcp_session_id}"
            info = workflow.info()
            session = AgentSession(info.workflow_id, session_id=str(workflow.uuid4()))
            
            # Long-running agent execution with internal activities
            result = await Runner.run(
                starting_agent=agent,
                input=user_input,
                session=session,
                max_turns=30,
            )
            
            return result.final_output
            
        except Exception as e:
            if is_cancelled_exception(e) or (e.__cause__ and is_cancelled_exception(e.__cause__)):
                # This never executes when cancellation arrives
                # between activity completion and workflow task start
                return await self.handle_cancellation(topic, session, payload)
            raise e
```

**Steps to reproduce:**

1. Start the workflow with a long-running `Runner.run()` call
2. Wait for `Runner.run()` to complete an internal activity
3. Call `workflow_handle.cancel()` immediately after the activity completes (within ~13ms window)
4. Observe that the workflow gets cancelled by Temporal before `handle_cancellation()` runs

### Environment/Versions

- OS and processor: M3 Mac (Apple Silicon)
- Temporal SDK: Python SDK version ~1.8.0~1.19.0
- Python: 3.12
- OpenAI Agents SDK: 0.0.11

### Additional context

The `handle_cancellation()` method performs critical cleanup:
- Saves interrupt tag to OpenSearch (so conversation history shows the cancellation)
- Sends notifications to frontend via Lightstreamer
- Clears the agent session
- Updates Langfuse trace with cancellation info

When it doesn't run, the system is left in an inconsistent state and users see incomplete or confusing behavior.

**Questions:**

1. Is there a way to ensure `CancelledError` is raised during the workflow task when cancellation is pending?
2. Should I be using a different pattern (e.g., `CancellationScope`, signals, or polling) for handling cancellation in workflows with long-running operations like `Runner.run()`?
3. Is there a way to guarantee cleanup code runs even when the workflow is cancelled at a task boundary?


#### Comments (3)

<details>
<summary><strong>tconley1428</strong> commented on 2025-11-25 17:05:18.000 UTC</summary>

Ensuring a user facing exception appears is more difficult in Python than other languages due to the use of `asyncio`, but we try to do that as much as possible. In the case of an activity with the default cancellation method, it should be guaranteed. I would recommend a few things.

1. Due to the general difficulty of ensuring the exceptions propagate appropriately in Python (child workflows or changing the cancellation type can cause different scenarios), it is often advisable to use signals to move the workflow into a cleanup state rather than rely on the exceptions.

2. We would be interested in determining why you aren't seeing an exception in this particular case, but we would need a replication, as I don't think the code provided would actually run as is. `agent` doesn't exist for instance. Additionally, we would be fixing any issues on the latest SDK version, and you are using one that is over a year old. It would be good to see if the issue still exists on the latest version `1.19.0`.

3. Not exactly to your question, but I would look at https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents if you haven't. We built an integration to do some of the things it sounds like you are doing manually.

</details>

<details>
<summary><strong>pinkynrg</strong> commented on 2025-11-25 23:00:56.000 UTC</summary>

Thanks for the detailed response @tconley1428

A quick correction: the SDK version is actually `1.19.0`. I used an agent to summarize my original post and it hallucinated an older version number. My bad, Iâ€™ll update the issue so itâ€™s accurate.

I can share the real workflow code if needed, but Iâ€™d prefer to do that through a more private channel since it includes business logic I donâ€™t want to post directly in a GitHub issue.

And yes, weâ€™re already using `temporalio.contrib.openai_agents`. Iâ€™m curious which parts you think we might be reinventing, if thereâ€™s something in the contrib package that can simplify our cancellation/cleanup logic, Iâ€™d love to know.

In the meantime, I might dig a bit deeper into signals. I originally skipped them because handling the cancellation exception seemed more straightforward, but given this behavior, it might be worth revisiting.

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-11-26 18:15:20.000 UTC</summary>

Gotcha, the old versions would have ruled out `temporalio.contrib.openai_agents`, so I assumed you would be reinventing the activity separation to make it work. If you'd like me to look into this particular repro, you can send me details in the community slack. There are other scenarios that may mean signal is more straighfoward though.

</details>


---

<a id="1107"></a>

### #1107: [Feature Request] Update dependency constraint for protobuf to protobuf>=3.20

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1107 |
| **State** | CLOSED |
| **Author** | cristiangreco94 (cristiangreco94) |
| **Created** | 2025-09-16 09:35:15.000 UTC (3 months ago) |
| **Updated** | 2025-09-16 14:24:40.000 UTC |
| **Closed** | 2025-09-16 14:24:40.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 1 |
| **Priority Score** | 7 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 3 |

#### Description

### Is your feature request related to a problem? Please describe.

Currently, the dependency specification for protobuf is pinned as protobuf>=3.20,<6. This upper bound prevents usage of the latest protobuf versions (6.x and above), which may block compatibility with other libraries or tools that already depend on newer releases.
Protobuf 5x will reach End of Support in Q1 2026 so it's better to upgrade asap https://protobuf.dev/support/version-support/#python

### Describe the solution you'd like

Update the dependency constraint to allow any version of protobuf starting from 3.20 without restricting the upper bound, e.g.:

```text
protobuf>=3.20
```

This will provide flexibility to use the latest protobuf releases while maintaining compatibility with versions from 3.20 onwards.

### Additional context
Removing the <6 upper bound will help ensure this project stays compatible with evolving protobuf releases and reduces dependency conflicts in downstream projects.
https://protobuf.dev/support/version-support/#python Protobuf 5.x will reach End of Support in Q1 2026. 

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-09-16 14:04:09.000 UTC</summary>

There is a PR in the works to support 6.x at #1082

</details>


---

<a id="1104"></a>

### #1104: Setting debug_mode in a Worker still doesn't allow the user of breakpoints

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1104 |
| **State** | OPEN |
| **Author** | togakangaroo (George Mauer) |
| **Created** | 2025-09-13 14:20:02.000 UTC (3 months ago) |
| **Updated** | 2025-09-22 14:46:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 7 |
| **Priority Score** | 7 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I am trying to use the pdb debugger from inside unit tests literally with the `breakpoint()` function.

### Describe the bug

It is pretty straightforward really, change the workflow to use sandboxing and set a `breakpoint()`. When you do you get an endless scroll with
```
temporalio.worker.workflow_sandbox._restrictions.RestrictedWorkflowAccessError: Cannot access __builtins__.breakpoint from inside a workflow. If this is code from a module not used in a workflow or known to only be used deterministically from a workflow, mark the import as pass through.
08:48:32 [ WARNING] Failed activation on workflow GreetingWorkflow with ID 68cd5a5e-2ada-40ff-aa67-63eae9ab3055 and run ID 01994355-86ca-7dd3-9335-69a9e5be86ff (_workflow_instance.py:452)
Traceback (most recent call last):
  File "/Users/georgemauer/code/temporal-samples/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py", line 412, in activate
    self._run_once(check_conditions=index == 1 or index == 2)
    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/georgemauer/code/temporal-samples/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py", line 2130, in _run_once
    raise self._current_activation_error
  File "/Users/georgemauer/code/temporal-samples/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py", line 2148, in _run_top_level_workflow_function
    await coro
  File "/Users/georgemauer/code/temporal-samples/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py", line 974, in run_workflow
    result = await self._inbound.execute_workflow(input)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/georgemauer/code/temporal-samples/.venv/lib/python3.13/site-packages/temporalio/testing/_workflow.py", line 518, in execute_workflow
    return await super().execute_workflow(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/georgemauer/code/temporal-samples/.venv/lib/python3.13/site-packages/temporalio/worker/_interceptor.py", line 385, in execute_workflow
    return await self.next.execute_workflow(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/georgemauer/code/temporal-samples/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py", line 2529, in execute_workflow
    return await input.run_fn(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/georgemauer/code/temporal-samples/hello/hello_activity.py", line 32, in run
    breakpoint()
    ~~~~~~~~~~^^
  File "/Users/georgemauer/code/temporal-samples/.venv/lib/python3.13/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 497, in __call__
    return self.current(*args, **kwargs)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/Users/georgemauer/code/temporal-samples/.venv/lib/python3.13/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 94, in restrict_built_in
    raise RestrictedWorkflowAccessError(f"__builtins__.{name}")
```
### Minimal Reproduction

I have [recreated this in temporal samples here](https://github.com/temporalio/samples-python/compare/main...togakangaroo:temporal-samples-python:demonstrate-debug-mode-not-working?expand=1)

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- Apple M4 Pro
- Temporal Version: 1.15.0
- Literally just running the samples
 
### Additional context

 I am debugging as part of trying to figure out why passing `RetryPolicy(max_attempts=1)` on `client.execute_workflow` in my unit tests doesn't seem to turn off retries of activities. 

What I ultimately want is to be able to tell my unit tests to only retry once or twice - essentially to set a retry policy specifically for my tests. The documentation implies that doing this by passing `retry_policy=RetryPolicy(max_attempts=2)` to `client.execute_workflow()` should work yet it doesn't for me. The only way to get it working is by passing a retry policy to `workflow.execute_activity()` but that would mean changing my code specifically for simple automation tests which I want to avoid.

So I started trying to inspect the code. Because there are base classes involved I want to inspect things at run time, however setting a breakpoint in my workflow fails. According to the documentation, `debug_mode=True` should make it possible to use breakpoints but that seems to *only* be the case when you don't have `sandbox=True` on your workflows which means I'd have to modify my code specifically for testing.


#### Comments (7)

<details>
<summary><strong>togakangaroo</strong> commented on 2025-09-14 20:49:23.000 UTC</summary>

This seems to be happening with `@workflow.defn(sandboxed=False)` and `workflow_runner=UnsandboxedWorkflowRunner()` as well.

I have to emphasize how utterly painful it is to have to develop things without the ability to place a breakpoint. Even when you can test activties in isolation, what we care about is the workflow as whole and that testability is really key. In my real-world case, the activities are basically just doing db loading and projection, not much to test - its the logic in the workflows and ensuring how it works across retries thats important.

</details>

<details>
<summary><strong>togakangaroo</strong> commented on 2025-09-15 17:28:19.000 UTC</summary>

Another example of where I'm struggling without `breakpoint` - I'm troubleshooting a situation in my tests where a mock is not set the way I intend it to. I want to inspect what is going on and I simply cannot.

For what its worth, I have tried
```
    with workflow.unsafe.sandbox_unrestricted():
        breakpoint()
```
which *does* hit the breakpoint, but the breakpoint is not in my code. I have to navigate up the call stack and then *do* see my code there and can inspect it, but boy is it a big workaround.

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-16 15:55:16.000 UTC</summary>

> The documentation implies that doing this by passing retry_policy=RetryPolicy(max_attempts=2) to client.execute_workflow() should work yet it doesn't for me

Can you point to documentation that says this? The retry policy in `execute_workflow` is the policy for workflow retries, not activities. If the documentation says otherwise or is unclear, it should be fixed. 

As to the larger question of breakpoints, they should work with the unsandboxed runner, so that is something we would have to look into. The recommended way to debug would be to debug a replay of the workflow run, as that will avoid any interaction with server timeouts, but that is more of a problem when you have hit and are staying at the breakpoint, not in hitting it in the first place.

</details>

<details>
<summary><strong>togakangaroo</strong> commented on 2025-09-17 18:19:55.000 UTC</summary>

@tconley1428 I guess I don't really understand the distinction between workflow and activity retries. If I have a workflow with activities A then B, if activity B fails, then it will retry again later and when it does it will rerun the entire workflow while skipping activity A because it's already completed successfully in the event history. It does not, sticky cache aside, save the entire workflow memory state in between the two runs, the workflow runs from the beginning.

So if the entire workflow reruns again...then what's the distinction? Is it that failed workflows themselves would be retried?

If so, I think that bit is very confusing since the entire workflow *does* rerun on activity retries. Additionally, [the documentation does mention a default RetryPolicy](https://docs.temporal.io/develop/python/failure-detection#activity-timeouts:~:text=Activity%20Executions%20are%20automatically%20associated%20with%20a%20default%20Retry%20Policy) but clicking that link simply takes you to the encyclopedia page on retries in general.

Given the above and, because both use the same `RetryPolicy` class, it seems a reasonable inference that setting it at the workflow level is what controls the default. I think the docs need to make a very clear distinction on it as well as explaining how to set the default (which is needed since you would want a different policy to be used in unit tests).

I'll write another reply about breakpoints...

</details>

<details>
<summary><strong>togakangaroo</strong> commented on 2025-09-17 18:37:06.000 UTC</summary>

Regarding breakpoints. If I take exactly what is in that branch I have in the demo and run the unit test, it actually *is* hitting the breakpoint now (which is odd, I could have sworn it threw an error earlier, but maybe that was when I was trying in different configurations).

```
uv run pytest --pdb '/Users/georgemauer/code/temporal-samples/tests/hello/hello_activity_test.py::test_mock_activity'
```

<img width="1617" height="291" alt="showing test run in the terminal with a pause at the breakpoint but no ability to interact with the pdb debugger" src="https://github.com/user-attachments/assets/113e4f65-f349-4c93-b3a0-1c92f767a528" />

However, notice in that screenshot that while it *does* break, it is not in the pdb repl and I cannot interact with the debugger in any way. I cannot inspect a variable, I cannot step, I cannot continue, I can't even seem to quit out of it without having to kil the underlying process in another terminal. This is despite me having `debug-mode=True` *and* the workflow being marked as not sandboxed.

Interestingly, in the samples this is happening even *if* I do `workflow_runner=UnsandboxedWorkflowRunner()` *and* the `        with workflow.unsafe.sandbox_unrestricted()` stuff I figured out above. That worked in my real project, but in this samples branch it just hangs.

</details>

<details>
<summary><strong>togakangaroo</strong> commented on 2025-09-17 18:40:34.000 UTC</summary>

I want to comment on this as well @tconley1428 

> The recommended way to debug would be to debug a replay of the workflow run

That precludes test-driven-development as workflow does it not? Since you'd need something deployed somewhere so that you could get something to replay?

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-22 14:46:05.000 UTC</summary>

> That precludes test-driven-development as workflow does it not? Since you'd need something deployed somewhere so that you could get something to replay?

Take a look at the `Replayer`: https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/_replayer.py#L33. You can run a workflow in tests, get its history, then either store it (we do this to test replays across sdk version changes) or run it through the replayer to replay immediately.

> If so, I think that bit is very confusing since the entire workflow does rerun on activity retries

Only kinda. The workflow _might_ replay to retry an activity, but doesn't necessarily. And that's different from rerunning the workflow, which would redo any completed activities, being a fresh start of the workflow. That said, it isn't very common and not recommended to rely on workflow retries. 

I take your point about the documentation of a default retry policy though. The "default" retry policy is not a mutable one. If you'd like, you can make an additional issue requesting a means of overriding the default activity retry policy at a higher level, and we'll consider it. 

On breakpoints, we might need to find some time to reproduce your behavior. With sandboxing off, it shouldn't really be different from a standard python program, but I haven't debugged in that way before.

</details>


---

<a id="1032"></a>

### #1032: [Bug] value_to_type fails with custom string types as dictionary keys

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1032 |
| **State** | CLOSED |
| **Author** | avalatea (Ava Li) |
| **Created** | 2025-08-18 17:14:50.000 UTC (4 months ago) |
| **Updated** | 2025-08-29 16:23:26.000 UTC |
| **Closed** | 2025-08-29 16:23:26.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 7 |
| **Priority Score** | 7 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm trying to use Temporal's `value_to_type` function to convert JSON payloads to dataclasses that contain dictionary fields with custom string types as keys (either string subclasses or NewType string aliases). This is a common pattern when working with typed identifiers like MongoDB ObjectIds or UUIDs.

Note: I am not calling the function directly; the bug is caused when triggering a workflow, which, under the hood will use `value_to_type` to convert types.

### Describe the bug

The `value_to_type` function has two bugs when handling dictionary keys with custom string types:

1. **String subclasses are converted to character lists**: When the key type is a string subclass (like `class CustomStr(str)`), the converter treats the string as an iterable and converts it to a list of characters, making it unhashable and unusable as a dictionary key.

2. **NewType fails isinstance check**: When the key type is a NewType alias (like `MyNewTypeStr = NewType("MyNewTypeStr", str)`), the converter fails with "isinstance() arg 2 must be a type" because NewType creates a callable, not a proper type for isinstance checks.

### Minimal Reproduction

```python
import traceback
from dataclasses import dataclass
from typing import NewType

from temporalio.converter import value_to_type


class CustomStr(str):
    pass


MyNewTypeStr = NewType("MyNewTypeStr", str)


@dataclass
class SimpleMessage:
    data: dict[CustomStr, str]


@dataclass 
class NewTypeMessage:
    data: dict[MyNewTypeStr, str]


def test_key_conversion():
    print("=" * 50)
    print("Testing direct key conversion with CustomStr...")
    print("=" * 50)
    key = "key1"

    converted_key = value_to_type(CustomStr, key)
    print(f"Original key: {repr(key)} (type: {type(key)})")
    print(f"Converted key: {repr(converted_key)} (type: {type(converted_key)})")


def test_newtype_key_conversion():
    print("=" * 50)
    print("Testing direct key conversion with NewType...")
    print("=" * 50)
    key = "key1"

    converted_key = value_to_type(MyNewTypeStr, key)
    print(f"Original key: {repr(key)} (type: {type(key)})")
    print(f"Converted key: {repr(converted_key)} (type: {type(converted_key)})")


def test_full_conversion():
    print("=" * 50)
    print("Testing full message conversion with CustomStr...")
    print("=" * 50)
    payload = {"data": {"key1": "value1"}}

    try:
        result = value_to_type(SimpleMessage, payload)
        print(f"Success: {result}")
    except Exception as e:
        print(f"Full conversion failed: {e}")
        traceback.print_exc()


def test_newtype_full_conversion():
    print("=" * 50)
    print("Testing full message conversion with NewType...")
    print("=" * 50)
    payload = {"data": {"key1": "value1"}}

    try:
        result = value_to_type(NewTypeMessage, payload)
        print(f"Success: {result}")
    except Exception as e:
        print(f"Full conversion failed: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    test_key_conversion()
    test_newtype_key_conversion()
    test_full_conversion()
    test_newtype_full_conversion()
```

#### Comments (7)

<details>
<summary><strong>tconley1428</strong> commented on 2025-08-19 16:23:54.000 UTC</summary>

Thanks @avalatea. The default data converter is known not to be the most fully featured serialization library there is. We'll look into this particular issue, but I can't promise it will be very high priority. As an alternative, we recommend the pydantic data converter for a more fully featured serialization story.

</details>

<details>
<summary><strong>avalatea</strong> commented on 2025-08-19 16:35:18.000 UTC</summary>

@tconley1428 Thanks for the response. We're exploring several options at the moment.

We did find that this behavior did not happen with temporal 1.10.0 vs 1.15.0. Do you happen to know at which version major changes happened with converter behavior? If not, that's fine. I am just curious what change made NewType suddenly no longer supported.

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-08-19 22:34:44.000 UTC</summary>

My guess based on the error message, though a stack would be make it more certain:
https://github.com/temporalio/sdk-python/commit/4933dc5f72e38412b85fe453ee935352294dd06f

Supporting non-string key types added this line:
```
                    if not isinstance(key, key_type):
```
Potentially that is failing on the newtype. The fact that it used to work will make it more likely that we will address it. I'll bring it up in triage.

Reactions: ðŸ‘ 1 ðŸŽ‰ 1

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-08-27 14:41:33.000 UTC</summary>

Created a PR here: https://github.com/temporalio/sdk-python/pull/1059. Am I right in understanding that the `CustomStr` method is not known to have worked in the past?

</details>

<details>
<summary><strong>avalatea</strong> commented on 2025-08-27 15:03:07.000 UTC</summary>

I am unsure if the `CustomStr` ever worked in the past, but I was attempting a fix by using a `CustomStr` class, which did not work as well.

I figured I would throw it in. It would make sense that a custom class be serializable, but perhaps I am mistaken? In any case, our systems currently only care about the NewType case as far as I can tell.

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-08-27 17:58:51.000 UTC</summary>

It does make sense, but we aren't prioritizing additional coverage in the default data converter. I don't think the implementation would be entirely straightforward either since we'd have to ensure that the custom class doesn't have additional data and is _just_ a string.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-08-29 16:23:26.000 UTC</summary>

Completed the PR to fix newtype. I'm going to close the issue, if you think custom str subtypes is a feature you would like, please open a feature request issue.

Reactions: ðŸŽ‰ 1

</details>


---

<a id="807"></a>

### #807: [Feature Request] Loss of stack trace information

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/807 |
| **State** | OPEN |
| **Author** | mrknmc (Mark Nemec) |
| **Created** | 2025-03-29 13:08:51.000 UTC (9 months ago) |
| **Updated** | 2025-04-04 14:55:02.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 3 |
| **Priority Score** | 7 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 2 |

#### Description

### Is your feature request related to a problem? Please describe.

* I would like to include full stack trace information in Sentry - see screenshot below where only `ChildWorkflowError` has a stack trace and both `ActivityError` and `ApplicationError` are missing it.
* I understand the exceptions are passed through the distributed system using the `Failure` Protobuf message which has a `stack_trace` field which could be used to populate the Sentry stack trace. This works for getting the stack trace of the `ApplicationError` in the activity. However, it's impossible to get the stack trace for the `ActivityError` in the `TestChildWorkflow` because the failure is just copied from the original failure here: https://github.com/temporalio/sdk-python/blob/bf747f1170e603b1e1adbd77c673fa83f0c99e7d/temporalio/converter.py#L826-L829

<img width="1131" alt="Image" src="https://github.com/user-attachments/assets/fb08e736-8f58-4aa0-bf45-c17523ce0292" />

This is the workflow and activity code I'm using to test this:

```
@activity.defn(...)
async def test_activity() -> None:
    raise TestException("Test error")


@workflow.defn
class TestChildWorkflow:
    @workflow.run
    async def run(self) -> None:
        # execute_default_activity is our wrapper around workflow.execute_activity
        await execute_default_activity(
            test_activity,
            retry_policy=RetryPolicy(maximum_attempts=1),
        ),


@workflow.defn
class TestParentWorkflow:
    @workflow.run
    async def run(self) -> None:
        # child workflow fails parent workflow
        await workflow.execute_child_workflow(
            TestChildWorkflow.run,
            id=f"test-child-workflow-{int(workflow.time())}",
            task_queue=get_temporal_tasks_queue_name(),
        )
```

### Describe the solution you'd like

* Ability to get the stack trace for every error in the chain. I am happy to contribute this code but I am not sure if there is a strong reason for why it is done this way and whether it can be changed.


#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2025-03-31 14:12:33.000 UTC</summary>

Python exceptions don't support easily deserializing tracebacks that come from remote/arbitrary strings (such as the ones we store which can come from any programming language or Python version). Therefore, we make the raw failure available to access this if needed. So for instance, this would get a collection of messages and tracebacks (untested, just typed here in code):

```python
@dataclass
class ExcDetails:
    message: str
    stack_trace: Optional[str]

def populate_chain(exc: Exception, chain: list[ExcDetails]) -> None:
    stack_trace = None
    if isinstance(exc, FailureError) and exc.failure:
        stack_trace = exc.failure.stack_trace
    elif exc.__traceback__:
        stack_trace = "\n".join(traceback.format_tb(exc.__traceback__))
    chain.append(ExcDetails(message=exc.message, stack_trace=stack_trace))
    if exc.__cause__:
        populate_chain(exc.__cause__, chain)
```

Something like that should work. Or do you have alternative suggestions on how to make arbitrary string stack trace available via error?

</details>

<details>
<summary><strong>tpetkov-red</strong> commented on 2025-04-04 08:34:41.000 UTC</summary>

> Something like that should work. Or do you have alternative suggestions on how to make arbitrary string stack trace available via error?

Found this issue while trying to solve a similar problem.  
Honestly, I'd be happy with the above (or similar) solution plopped into the sdk.
@cretz  do you think this can be included into the project?

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-04 14:54:15.000 UTC</summary>

The SDK already includes stack trace on the errors, it's just via `failure.stack_trace` instead of `__traceback__` because Python has limitations on what can be on `__traceback__`. This little sample code was just showing how you can call `failure.stack_trace` in your code, but I don't think we'll include this sample in the SDK proper. You can use a whole new object to collect stack traces if you want like this sample does, though many may just prefer to use the existing Python exceptions and provide a formatter of sorts to whatever is displaying/recording the exception.

</details>


---

<a id="691"></a>

### #691: [Feature Request] Make option for "all passthrough" (i.e. disable import isolation part of sandbox)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/691 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-11-20 20:18:02.000 UTC (1y 1m ago) |
| **Updated** | 2025-01-23 15:31:12.000 UTC |
| **Closed** | 2025-01-23 15:31:12.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 1 |
| **Priority Score** | 7 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 3 |

#### Description

### Describe the solution you'd like

Today we allow you to specify a certain set of passthrough modules when configuring the sandbox runner, but it only works if you know the module name (but it is hierarchical). Some users may want to pass through everything regardless of root module, so we should probably add a `SandboxRestrictions` field for `passthrough_all_modules: bool`.


#### Comments (1)

<details>
<summary><strong>josh-berry</strong> commented on 2025-01-14 18:26:39.000 UTC</summary>

This is helpful because it still allows the sandbox to provide state isolation for each workflow, but without requiring users to worry about allow-listing individual modules as being deterministic.

</details>


---

<a id="681"></a>

### #681: [Feature Request] Support for test server without internet access

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/681 |
| **State** | CLOSED |
| **Author** | wilsoniya (Michael Wilson) |
| **Created** | 2024-10-31 18:22:38.000 UTC (1y 2m ago) |
| **Updated** | 2024-11-25 16:39:43.000 UTC |
| **Closed** | 2024-10-31 20:14:09.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 5 |
| **Priority Score** | 7 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

I'm trying to run temporal workflow tests in an environment without access to the public internet. This means that `WorkflowEnvironment.start_time_skipping(...)` et. al. can't fetch the time skipping test server from temporal.download. 

There's a possible workaround I found while stepping through temporal-core-sdk in which I could e.g. make a [test server tarball](https://temporal.download/temporal-test-server/default?platform=linux&arch=amd64&sdk-version=default&sdk-name=temporal-test-server)  available on the air-gapped infrastructure, unpack it, then thread the path to the binary into my test harness and pass it as the `test_server_existing_path` to `WorkflowEnvironment.start_time_skipping`. But I wish a more ergonomic option existed, especially for a use case that might not be uncommon.

### Describe the solution you'd like

In order of descending convenience for my use case:

* be able to provide a URL of the tarball itself via environment variable (without the json info indirection); I'm able to cache the tarball at a different location
* be able to provide a URL of the test server binary via environment variable
* be able to provide an absolute path to the test server binary on disk via environment variable

Basically, configuration via environment variable saves having to thread information through to every WorkflowEnvironment instantiation.

It would also be helpful to document the location where test server binaries can be acquired. 

An alternative solution could be packaging and publishing platform/arch-specific pypi packages containing the test server. This could either be in the sdk-python package itself, or a separate package which is conditionally depended on by sdk-python via dependency group.

I'm def open to other approaches. Thanks!

### Additional context
* PR introducing on-demand test server fetching: https://github.com/temporalio/sdk-core/pull/389
* Call site where test server is downloaded: https://github.com/cretz/temporal-sdk-core/blob/3105ef349fe60265157ae7fa96674c964ae5c979/core/src/ephemeral_server/mod.rs#L298


#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2024-10-31 19:26:55.000 UTC</summary>

> In order of descending convenience for my use case:

As mentioned there, we provide the option for you to give the executable path. This gives you full freedom without us being too specific. I would suggest if you want a URL-based download, you can use your preferred downloading tool/library to download it and extract the tarball. Same if it were on S3 or provided via some other method. Similarly if you want to have this option set via environment variable, just `test_server_existing_path=os.getenv('MY_ENV_VAR')`.

I am not sure we want to bake in so many ways to get a file beyond the default/no-option and the advanced option.

</details>

<details>
<summary><strong>wilsoniya</strong> commented on 2024-10-31 20:14:09.000 UTC</summary>

> I am not sure we want to bake in so many ways to get a file beyond the default/no-option and the advanced option.

Thanks for the response, and I can understand this :+1: 

I appreciate the forethought to provide this escape hatch. Is there documentation describing where to find the test server binary tarballs? I ended up reading the sdk-core implementation to figure out where they are. 

Thanks again and I consider this closed.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-31 20:45:38.000 UTC</summary>

> Is there documentation describing where to find the test server binary tarballs?

No, we can add this. Yeah they are part of https://github.com/temporalio/sdk-java releases

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>Kilo59</strong> commented on 2024-11-25 15:29:22.000 UTC</summary>

@cretz @wilsoniya 

Where can I find this documentation?

Also can the error message here could be improved. At least logging or including the HTTP status code returned.
```
RuntimeError: Failed starting Temporal dev server: error sending request for url (https://temporal.download/cli/default?arch=arm64&platform=darwin&format=tar.gz&sdk-name=sdk-python&sdk-version=1.8.0)
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-11-25 16:39:41.000 UTC</summary>

This documentation is in the API docs at https://python.temporal.io/temporalio.testing.WorkflowEnvironment.html#start_time_skipping:

> test_server_existing_path [...] Existing path to the test server binary. If present, no download will be attempted to fetch the binary.

Though your error looks like it's referring to `start_local`, so those are at https://python.temporal.io/temporalio.testing.WorkflowEnvironment.html#start_local. But I am afraid it, and many of the other advanced options there, are not (yet) documented outside of the API docs.

> Also can the error message here could be improved. At least logging or including the HTTP status code returned.

Yes, this is because the Rust HTTP request library does not include more detail from the inner message (https://github.com/seanmonstar/reqwest/blob/e2d4b14c695acaebe531af7ce4712e182278f38a/src/error.rs#L185). We can look into improving it.

</details>


---

<a id="676"></a>

### #676: [Feature Request] Support `(str, Enum)` like `StrEnum`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/676 |
| **State** | OPEN |
| **Author** | sayevsky (Slava) |
| **Created** | 2024-10-24 14:36:28.000 UTC (1y 2m ago) |
| **Updated** | 2025-03-01 20:21:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 7 |
| **Priority Score** | 7 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->
I'm trying to use enum class to pass it to activity as an input.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

Activity deserializes bytes to list of chars instead of given Enum

### Minimal Reproduction
```
from enum import Enum

from temporalio.api.common.v1.message_pb2 import Payload
from temporalio.converter import JSONPlainPayloadConverter

class SomeEnum(str, Enum):
    ONE = "one"
    TWO = "two"



converter = JSONPlainPayloadConverter()
result = converter.from_payload(Payload(metadata={
    "key": b"encoding",
    "value": b"json/plain"},
    data=b"\"one\""),
    SomeEnum)
print(result)
```
Output:
```
['o', 'n', 'e']
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- M1 Mac, though any architecture impacted
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
SDK version is 1.6.0
- Are you using Docker or Kubernetes or building Temporal from source?
No

### Additional context

JSONPlainPayloadConverter deserializes IntEnum, StrEnum correctly, but it is more reliable to convert payload to any Enum

<!-- Add any other context about the problem here. -->


#### Comments (7)

<details>
<summary><strong>cretz</strong> commented on 2024-10-24 14:38:51.000 UTC</summary>

Per https://github.com/temporalio/sdk-python?tab=readme-ov-file#data-conversion and https://docs.temporal.io/develop/python/converters-and-encryption#payload-conversion, only `StrEnum` and `IntEnum` are supported directly. Can you extend from one of those?

</details>

<details>
<summary><strong>sayevsky</strong> commented on 2024-10-25 08:33:01.000 UTC</summary>

Hello, @cretz, thank you for the quick follow up.
Is the decision by not supporting basic Enum intentional? Are there any known corner-cases that stop adding additional functionality?

I can extend, but it is seems, that I need to copy-paste the whole `value_to_type` into my project. That is because types could be generic and `value_to_type` traverse the type recursively. 

</details>

<details>
<summary><strong>sayevsky</strong> commented on 2024-10-25 09:10:28.000 UTC</summary>

As proposal:

```
...
# StrEnum, available in 3.11+
    if sys.version_info >= (3, 11):
        if inspect.isclass(hint) and issubclass(hint, StrEnum):
            if not isinstance(value, str):
                raise TypeError(
                    f"Cannot convert to enum {hint}, value not a string, value is {type(value)}"
                )
            return hint(value)
# proposal to add the code:
    if inspect.isclass(hint) and issubclass(hint, Enum):
        if not isinstance(value, str):
            raise TypeError(
                f"Cannot convert to enum {hint}, value not a string, value is {type(value)}"
            )
        return hint(value)
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-25 13:00:41.000 UTC</summary>

> Is the decision by not supporting basic Enum intentional?

It is because there is no clear serialization of plain `Enum`. But there is for `(str, Enum)` together so we can probably support that combo (though we, and Python, suggest using `StrEnum` now if you can). Your proposal is close, but we probably also need to also check that it's a subclass of _both_ `str` _and_ `Enum` and not just `Enum`. We'll look into adding this, thanks!

</details>

<details>
<summary><strong>sayevsky</strong> commented on 2024-10-25 13:52:56.000 UTC</summary>

Similar enhancement for `(int, Enum)` would be nice. And if both checks do not match, then throw Exception. Because current behaviour is not obvious.

Thanks.

</details>

<details>
<summary><strong>spacether</strong> commented on 2025-03-01 07:55:45.000 UTC</summary>

Why not pickle enums into bytes for serialization? Or leverage what pydantic does? 

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-03-01 20:21:50.000 UTC</summary>

@spacether tangential to this ticket, but we recently added an official Pydantic data converter to the SDK: https://github.com/temporalio/sdk-python?tab=readme-ov-file#pydantic-support If you use that data converter you'll get Pydantic's enum support (which does support `(str, Enum)`).

Reactions: ðŸ‘ 1

</details>


---

<a id="603"></a>

### #603: [Feature Request] Investigate/document how to use PyCharm debugger with workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/603 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-08-06 17:50:17.000 UTC (1y 5m ago) |
| **Updated** | 2025-01-17 20:28:59.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 1 |
| **Priority Score** | 7 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 3 ðŸ‘€ 3 |

#### Description

### Describe the solution you'd like

May be similar to #238 but maybe not (that's more about the sandbox).

It seems when trying to debug via replayer in PyCharm, some things break that work just fine when not using PyCharm's debugger. Investigate what breaks and why. This may be due to some patching/expectations PyCharm has around asyncio that don't hold true for custom asyncio implementations. There's another issue at https://youtrack.jetbrains.com/issue/PY-64542/Asyncio-support-for-the-debugger-doesnt-work-with-pytest-asyncio.

We need to document how exactly to use breakpoints in workflows from this IDE.

#### Comments (1)

<details>
<summary><strong>mnichols</strong> commented on 2025-01-17 20:28:58.000 UTC</summary>

@cretz here is my first crack at such a doc:

Python developers must take into consideration both the [sandbox](https://docs.temporal.io/develop/python/python-sdk-sandbox) and the Workflow Task Timeout for any Debugger interactions. Here are some tips to help you "step through" Workflow Code with a Debugger.

### Debugging Workflow Code

Finding problems within complex Workflow code might be aided by stepping through
the code as you normally would to inspect variable state, step through execution paths, etc.

Workflow code executed by a Worker does so within the bounds of a Workflow Task which is
itself governed by a Timeout. 
* The maximum amount of time a Workflow Task can be configured to take before `Failed` is *120 seconds*. 
* The default Timeout is _10 seconds_.
* That means you have a maximum of _two minutes_ to diagnose your code before the current Task will be failed and rescheduled.
You'll see a flood of error messages in your console related to these Task issues. This may not be enough time to perform your diagnosis.

#### _So how can I step through my Workflow Code??_

You can get around this Task boundary by executing your code within a `Replayer`.
This is done by passing Workflow Execution history into the Replayer while the debugger is attached to step through the code.
Executed this way, there is no Task to interfere with your diagnosis.

> **PyCharm** users might experience issues with `asyncio` debugging.
> If you are having difficulty debugging with **PyCharm**, you might consider the [steps in this ticket](https://youtrack.jetbrains.com/issue/PY-57667/Asyncio-support-for-the-debugger-EXPERIMENTAL-FEATURE)
> to solve it. The `python.debug.asyncio.repl` can be toggled within PyCharm by:
> 1. Use Shift + Shift (Search Everywhere)
> 2. In the popup type: Registry and press Enter
> 3. Find "Registry" in the list of results and click on it.
> 4. In the new popup find `python.debug.asyncio.repl` line and check (enable) or uncheck (disable) the respective checkbox
> 5. Press Close.
> 6. Restart the IDE.

Here are a couple of approaches to debug Temporal Workflows with a Debugger:

### The Debug Two-Step

This approach involves running first _without_ the Debugger attached to generate Workflow Execution histories.
Subsequently, these histories can be used in the second run _with_ the Debugger attached.
This approach can be handy if you already have Execution histories you want to verify that failed, for example, in production.


_First run_ (without Debugger attached)
1. Execute the Workflow(s) and keep the Temporal Server running (if testing locally)

_Second run_ (with Debugger attached)
1. Set appropriate breakpoints in Workflow code.
2. Fetch the related Workflow Execution History(ies) and store in a variable
3. Pass the history into a `Replayer` with the following properties set:
```python
replayer = Replayer(
    workflows=[MyWorkflow], 
    debug_mode=True,
    namespace='debug',
    workflow_runner=temporalio.worker.UnsandboxedWorkflowRunner())
```
4. Profit.

### The Debug One-Shot

1. Set appropriate breakpoints in Workflow code, but this time add a `Condition` of `workflow.info().namespace == 'debug'.
2. Run the test *with Debugger attached* that:
   1. Executes the Workflow(s) 
   2. Fetches the related Workflow Execution History(ies) and store in a variable
   3. Passes the history into a `Replayer` with the following properties set:
```python
replayer = Replayer(
    workflows=[MyWorkflow], 
    debug_mode=True,
    namespace='debug',
    workflow_runner=temporalio.worker.UnsandboxedWorkflowRunner())
```
3. Profit.

Reactions: â¤ï¸ 3

</details>


---

<a id="572"></a>

### #572: [Bug] Upserting datetime search attribute fails

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/572 |
| **State** | CLOSED |
| **Author** | irdbl (Kevin Pham) |
| **Created** | 2024-07-05 20:58:11.000 UTC (1y 6m ago) |
| **Updated** | 2024-08-16 03:36:19.000 UTC |
| **Closed** | 2024-08-16 03:18:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 7 |
| **Priority Score** | 7 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm trying to update a workflow with a search attribute `checkout_time` of type datetime using the python SDK. The `checkout_time` datetime search attribute exists on that particular namespace. 

### Describe the bug

```py
from datetime import datetime
from temporalio import workflow
from temporalio.common import SearchAttributeKey, SearchAttributeValue

t = datetime.fromisoformat("2024-07-05T15:43:07.875302" ) # <- yes, workflow is deterministic
k = SearchAttributeKey.for_datetime("checkout_time")
u = k.value_set(t)
workflow.upsert_search_attributes([u])


```

Results in this worker error:

2024-07-05T20:46:30.554682Z  WARN temporal_sdk_core::worker::workflow: Error while completing workflow activation error=status: InvalidArgument, message: "BadSearchAttributes: invalid value for search attribute checkout of type Datetime: 2024-07-05T15:43:07.875302", details: [], metadata: MetadataMap { headers: {"content-type": "application/grpc", "server": "temporal", "date": "Fri, 05 Jul 2024 15:43:07 GMT"} }

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [Intel Mac]



#### Comments (7)

<details>
<summary><strong>cretz</strong> commented on 2024-07-08 13:32:32.000 UTC</summary>

:+1: Unfortunately not all ISO-8601 timestamps are accepted. The Temporal server uses the Go `RFC3339Nano` format which has some incompatibilities. In this case, the time is invalid, see https://go.dev/play/p/3SzyCA_UlUK. May be able to add a `Z` at the end of the string for it to parse, we will test. https://docs.temporal.io/visibility kinda touches on the format expected for date time for the built-in attributes (we will update the docs to more clearly state the date time format in general).

Thanks for the report. We will look into fixing the bug for this timestamp and format string.


Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>irdbl</strong> commented on 2024-07-08 14:08:42.000 UTC</summary>

Thank you for taking a look at it.

I also noticed the python examples in the docs for SearchAttribute upserting are using the old style without types. Might be a good idea to update those to use the new style too once changes are stable.

https://docs.temporal.io/develop/python/observability#search-attributes

- https://github.com/temporalio/documentation/blob/main/sample-apps/python/your_visibility/starter_dacx.py#L16-L37
- https://github.com/temporalio/documentation/blob/main/sample-apps/python/your_visibility/workflow_dacx.py#L22

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-07-08 14:47:27.000 UTC</summary>

Yes, we have issues opened on the documentation side to update to typed search attributes

</details>

<details>
<summary><strong>irdbl</strong> commented on 2024-08-05 22:57:19.000 UTC</summary>

Any updates on this?

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-08-06 13:03:42.000 UTC</summary>

This is considered a high priority and we are actively working on it, but no updates as of yet. It very likely may just be adding a `Z` to the end of the string internally, we are investigating.

</details>

<details>
<summary><strong>dandavison</strong> commented on 2024-08-16 03:29:17.000 UTC</summary>

Hi @accupham, the issue here is that the SDK requires you to specify a timezone -- our server needs to store user timestamps with a timezone and the SDK can't assume that your timestamp refers to time in the location of the machine running the worker process.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>dandavison</strong> commented on 2024-08-16 03:36:18.000 UTC</summary>

I've opened https://github.com/temporalio/sdk-python/issues/611 to have the SDK raise an exception with a more helpful error message -- thanks for the report!

</details>


---

<a id="512"></a>

### #512: [Feature Request] Reload client certificate 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/512 |
| **State** | CLOSED |
| **Author** | pvcnt (Vincent Primault) |
| **Created** | 2024-04-24 11:23:27.000 UTC (1y 8m ago) |
| **Updated** | 2025-06-26 23:36:55.000 UTC |
| **Closed** | 2024-05-03 20:36:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 7 |
| **Priority Score** | 7 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

I am connecting to a Temporal server with a client certificate, which is "short"-lived (a few days). When it expires, my client becomes unusable.

### Describe the solution you'd like

I need a way to refresh the certificate without restarting my entire client. For example, I would need to be able to specify a path to a certificate, or a function supplying a certificate in TLSConfig, instead of giving directly the certificate.

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->


#### Comments (7)

<details>
<summary><strong>cretz</strong> commented on 2024-04-24 13:27:47.000 UTC</summary>

> I need a way to refresh the certificate without restarting my entire client

If you mean just change in the client, TLS establishment is done on connect, so you need to connect a new client. But if you mean replace the client in the worker without restarting the worker, yes, this is something we are adding soon (I just opened #513 since I didn't see an original public issue on this).

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-05-03 20:36:51.000 UTC</summary>

Closing this issue in favor of #513 where we allow client replacement on long-running workers (which means it can be reconnected with a new cert). To use a new TLS certificate on a client, you have to reconnect a client.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>pvcnt</strong> commented on 2024-05-04 13:02:11.000 UTC</summary>

Excellent, thank you!

</details>

<details>
<summary><strong>ankit-sultania</strong> commented on 2025-06-26 20:50:13.000 UTC</summary>

Lets say my worker is running indefinitely with a client and after an hour the cert is rotated and I replace the worker client, but seems like that is not enough and I need to reconnect. Does it mean I need to kind of stop and start the worker? Like how do you reconnect the client without having to stop the worker that is already running? 

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-06-26 21:45:46.000 UTC</summary>

You connect the client before replacing it on the worker. You set the `client` property on the worker (which has a [setter for `client` property](https://python.temporal.io/temporalio.worker.Worker.html#client)) to replace the client with a newly connected client. You do not have to stop the worker, the next time the worker issues requests it will start using the new client (but outstanding poll requests will still complete using the existing client).

</details>

<details>
<summary><strong>ankit-sultania</strong> commented on 2025-06-26 22:04:14.000 UTC</summary>

Thanks for the explanation. So, you are saying that workers would be still be able to pull from the task queue after 1) certs are rotated and 2) the client is replaced with the new client. Correct?

</details>

<details>
<summary><strong>ankit-sultania</strong> commented on 2025-06-26 23:36:55.000 UTC</summary>

Ok, now i see what's going on here. 1) bridge service client is extracted [here](https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/_worker.py#L320) which is used for polling tasks 2) upon replacing the worker.client we also replace the bridge client under the hood [here](https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/_worker.py#L566) which means this new bridge client is used for polling

</details>


---

<a id="309"></a>

### #309: [Bug] Unable to install temporalio within [image: sonarsource/sonar-scanner-cli:4.6] container

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/309 |
| **State** | CLOSED |
| **Author** | kaustav1996 (Kaustav Banerjee) |
| **Created** | 2023-04-03 10:19:29.000 UTC (2y 9m ago) |
| **Updated** | 2023-05-08 18:06:37.000 UTC |
| **Closed** | 2023-04-05 06:42:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 7 |
| **Priority Score** | 7 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

While running CI in our GitLab CI Pipeline we are unable to install temporalio because it is unable to find protoc-wheel-0

### Describe the bug

Here is our Job :
```yaml
merge-request-static-analyzer:
  image: sonarsource/sonar-scanner-cli:4.6
  stage: sonar_analysis
  before_script:
    - echo "Source -> ${CI_MERGE_REQUEST_SOURCE_BRANCH_NAME} Target -> ${CI_MERGE_REQUEST_TARGET_BRANCH_NAME} MR -> ${CI_MERGE_REQUEST_IID}"
  script:
    - ls -lah
    - apk add --update --no-cache python3 postgresql-libs postgresql-dev gcc python3-dev musl-dev groff alpine-sdk bash jq curl libffi-dev openssl-dev util-linux go postgresql-client curl-dev python3-dev libressl-dev
    - python3 -m ensurepip
    - pip3 install --ignore-installed --no-cache --upgrade pip setuptools virtualenv bump2version==1.0.1
    - rm -rf venv && virtualenv venv
    - source venv/bin/activate
    - pip3 install --find-links=protoc_wheel_0-21.10-py2.py3-none-any.whl protoc-wheel-0
    - pip3 freeze
    - pip3 install -r requirements.txt
    - source ./test/environments/.prod.env
    - make test.coverage
    - coverage xml --fail-under 95
    - sonar-scanner
      -Dsonar.projectKey=zion
      -Dsonar.sources=.
      -Dsonar.qualitygate.wait=true
      -Dsonar.exclusions="./deployment/**, ./test/**, ./venv/**"
      -Dsonar.login=${SONAR_LOGIN}
      -Dsonar.host.url=${SONAR_HOST}
      -Dsonar.pullrequest.key=${CI_MERGE_REQUEST_IID}
      -Dsonar.pullrequest.branch=${CI_MERGE_REQUEST_SOURCE_BRANCH_NAME}
      -Dsonar.pullrequest.base=${CI_MERGE_REQUEST_TARGET_BRANCH_NAME}
    - echo "Done"
  artifacts:
    when: always
    reports:
      junit: coverage.xml
  only:
    - merge_requests
```

Since we were first unable to install protoc-wheel-0 on this architecture we built the whl file and exported it using a separate docker :
```Dockerfile
FROM alpine:3.17
RUN apk update
RUN apk add --no-cache --virtual build-dependencies wget unzip gnupg libffi-dev gcc libc-dev linux-headers
RUN apk add --no-cache git python3-dev py-pip bash shellcheck 'nodejs>12' openjdk11-jre curl musl-locales musl-locales-lang;
RUN git clone https://gitlab.com/dfritzsche/create-protoc-wheel.git  protoc
WORKDIR /protoc
RUN pip install -r requirements.txt
RUN python create_protoc_wheel.py 21.10

```

I am then going inside the container building the wheel and exporting it. Hence the line is there in the CI job to install it within the sonar container :
```yaml
- pip3 install --find-links=protoc_wheel_0-21.10-py2.py3-none-any.whl protoc-wheel-0
```

Output when the Job runs :
```bash
$ source venv/bin/activate
$ pip3 install --find-links=protoc_wheel_0-21.10-py2.py3-none-any.whl protoc-wheel-0
Looking in links: protoc_wheel_0-21.10-py2.py3-none-any.whl
Processing ./protoc_wheel_0-21.10-py2.py3-none-any.whl
Installing collected packages: protoc-wheel-0
Successfully installed protoc-wheel-0-21.10
$ pip3 freeze
protoc-wheel-0==21.10
$ pip3 install -r requirements.txt
.
.
.
Collecting temporalio==1.1.0
  Downloading temporalio-1.1.0.tar.gz (851 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 851.4/851.4 kB 41.5 MB/s eta 0:00:00
  Installing build dependencies: started
  Installing build dependencies: finished with status 'error'
  error: subprocess-exited-with-error
  
  Ã— pip subprocess to install build dependencies did not run successfully.
  â”‚ exit code: 1
  â•°â”€> [11 lines of output]
      Collecting poetry-core>=1.0.0
        Downloading poetry_core-1.5.2-py3-none-any.whl (465 kB)
           â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 465.2/465.2 kB 8.1 MB/s eta 0:00:00
      Collecting setuptools
        Using cached setuptools-67.6.1-py3-none-any.whl (1.1 MB)
      Collecting wheel
        Using cached wheel-0.40.0-py3-none-any.whl (64 kB)
      Collecting setuptools-rust
        Downloading setuptools_rust-1.5.2-py3-none-any.whl (23 kB)
      ERROR: Could not find a version that satisfies the requirement protoc-wheel-0 (from versions: none)
      ERROR: No matching distribution found for protoc-wheel-0
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error
Ã— pip subprocess to install build dependencies did not run successfully.
â”‚ exit code: 1
â•°â”€> See above for output.
note: This error originates from a subprocess, and is likely not a problem with pip.
```

What I cannot understand is why it is not able to detect the already installed `protoc-wheel-0` even though it is mentioned in `pyproject.toml` that `^21.1` is required. :

```toml
[tool.poetry.dev-dependencies]
protoc-wheel-0 = "^21.1"
```

### Environment/Versions

- OS and processor: `Linux runner-j2nyww-s-project-26698668-concurrent-0 5.4.109+ #1 SMP Wed Jun 16 20:00:10 PDT 2021 x86_64 Linux`
- Temporal Version: `1.1.0`
- Are you using Docker or Kubernetes or building Temporal from source? : `No`

#### Comments (7)

<details>
<summary><strong>kaustav1996</strong> commented on 2023-04-03 10:50:24.000 UTC</summary>

I just saw in the last line of `pyproject.toml` it says :
```toml
requires = ["poetry-core>=1.0.0", "setuptools", "wheel", "setuptools-rust", "protoc-wheel-0"]
```
does it mean it will always try to download it and not use the one that is installed in the env?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-04-03 12:16:02.000 UTC</summary>

What is happening is `pip` is trying to install `temporalio` from source instead of using a prebuilt one because you are using Alpine. This uses musl instead of glibc which we don't provide pre-built package for. Then, when it tries to build it is installing dev dependencies which includes `protoc-wheel-0` which _also_ does not seem to provide a musl-based package for Alpine.

We have a couple of options here. You can stop using Alpine. If this is not an option, maybe we can make `protoc-wheel-0` an optional dev dependency. But you'll have to make `protoc` available on the `PATH` somehow because this will need to build via Rust and one of our crates requires `protoc`. Also the build will take a while. It is suggested to just use a different distro (just about any non-alpine should work) so that you don't have to build the SDK.

</details>

<details>
<summary><strong>kaustav1996</strong> commented on 2023-04-05 06:41:53.000 UTC</summary>

@cretz Thanks for the response. We found a way to use an alternate Distro and were able to install temporalio there. 

</details>

<details>
<summary><strong>tynianovddi</strong> commented on 2023-05-08 15:36:53.000 UTC</summary>

What should I do if i can not use any other distro apart from alpine?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-08 15:45:03.000 UTC</summary>

You can attempt to build the SDK yourself with MUSL for the Rust part and it will likely work fine. We just don't distribute one. You'll have to use an alternative `protoc` approach, but that should also not be hard (we only depend on it as a dev dependency so devs don't have to manually download when building).

</details>

<details>
<summary><strong>tynianovddi</strong> commented on 2023-05-08 17:08:11.000 UTC</summary>

@cretz thanks, are there any guides how to do that or some mentions how to address this issue?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-08 18:06:37.000 UTC</summary>

At the bottom of the README is the build guide. You'll have to research and adapt it for MUSL and to disable `protoc` Python dependency (to use your own on the `PATH`) as this is a setup we don't support by default.

</details>


---

<a id="194"></a>

### #194: [Feature Request] Support patch search attribute

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/194 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-09 22:22:25.000 UTC (3y 2m ago) |
| **Updated** | 2023-02-17 14:42:56.000 UTC |
| **Closed** | 2023-02-17 14:42:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 7 |
| **Priority Score** | 7 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

TBD. Go puts `TemporalChangeVersion` key with a list of strings like `"<changeid>-<version>"` for every version seen. What do we want to do here? cc @bergundy 

#### Comments (7)

<details>
<summary><strong>bergundy</strong> commented on 2022-11-10 00:12:36.000 UTC</summary>

Reuse the same search attribute and use patch id for the value

</details>

<details>
<summary><strong>bergundy</strong> commented on 2022-11-10 05:37:39.000 UTC</summary>

I'd want us to buffer all of the search attribute updates and emit a single upsert command at the end of the activation like we discussed in the TS search attributes proposal: https://github.com/temporalio/proposals/pull/67/files#diff-7d87e4d9af7e84955486e56bc4d577483f5fa34479dbe84b2ceeb50f4d64a81cR31
We should try and get this in for beta 3 to avoid another history incompatible change.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-10 12:23:10.000 UTC</summary>

@bergundy - This also kinda overlaps with #153. I see we even added a new type in https://github.com/temporalio/api/pull/248. Should I go ahead and, from a user POV, make it easy to update/get single search attributes?

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-10 17:00:57.000 UTC</summary>

> I'd want us to buffer all of the search attribute updates and emit a single upsert command at the end of the activation like we discussed in the TS search attributes proposal:

Also, can you clarify in that proposal when we shouldn't send the update? If an exception is caused that causes workflow task failure, does that mean my pending search attributes are not persisted yet? But an exception that would just cause workflow failure I do send the pending update. Do I have to order the command before the failure?

Also, what if I have an activity that uses the client to check search attributes of the calling workflow? Can I guarantee that an upsert search attribute command after a schedule activity command will always persist the values before an activity started? Can I even guarantee that today if upsert search attribute is in order? (I will ask server team)

</details>

<details>
<summary><strong>bergundy</strong> commented on 2022-11-10 17:08:24.000 UTC</summary>

No commands are sent on WFT failure so I don't think there's an issue here.
All commands should be handled in atomically on the server, the ordering shouldn't matter for correctness here.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-10 17:21:23.000 UTC</summary>

> the ordering shouldn't matter for correctness here.

Except workflow completion/failed/cancelled which I think we have to flush before that

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-17 14:42:55.000 UTC</summary>

This is being done in core not lang.

</details>


---

<a id="143"></a>

### #143: Pydantic models with nested non-JSON-friendly field types cannot be serialized

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/143 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-27 16:37:23.000 UTC (3y 3m ago) |
| **Updated** | 2023-02-07 22:03:44.000 UTC |
| **Closed** | 2023-02-07 22:03:44.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 5 |
| **Priority Score** | 7 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 â¤ï¸ 1 |

#### Description

### What are you really trying to do?

Use this as a param:

```python
class MyClass(BaseModel):
   field_a: list[UUID]
```

### Describe the bug

Today we basically just call `dict()` on this which then tries to `json.dumps` that which it can't because that is _not_ a JSON-friendly dict. Unfortunately Pydantic does not provide a way to turn a model to a JSON-friendly dict. We cannot directly return the string result of `json()` in our encoder because we need it for nested purposes (i.e. `JSONEncoder.default`, _not_ `JSONEncoder.encode` which is only called at the top level, not nested).

The best solution right now I think is to accept anything that has a callable `json` attribute and invoke it, then turn it _back_ to Python object form via `json.loads`. This is the only easy way I am aware but has an obvious performance penalty. Was also mentioned at https://stackoverflow.com/questions/65622045/pydantic-convert-to-jsonable-dict-not-full-json-string.

#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2022-09-27 16:39:05.000 UTC</summary>

Also see https://github.com/pydantic/pydantic/discussions/4456

</details>

<details>
<summary><strong>KMilhan</strong> commented on 2022-09-28 00:33:19.000 UTC</summary>

Maybe we can add rather optimized json implementation such as`orjson` as an additional dependency and let the users utilize it if the performance penalty is the concern?
At least in my situation, json performance penalty has way lower priority than the developer's productivity. Ofc this can vastly vary case by case.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-09-28 12:26:04.000 UTC</summary>

We are working hard to make the SDK flexible enough to use your own dependencies but not force too many dependencies on people. If you want to use `orjson`, you can create your own payload converter. https://github.com/temporalio/samples-python/tree/main/custom_converter is an example of adding a new converter, but you can do similar except replace the default converter set when creating a composite converter to change the last converter (which is `JSONPlainPayloadConverter`) with your own.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-03 16:23:14.000 UTC</summary>

I am still gathering feedback on whether we accept the perf penalty of using `json()` instead of `dict()` (only to parse it back again). If Pydantic had `json_dict()` this would be no problem.

Until we either choose to support stringified JSON in models or Pydantic adds support for JSON-able dicts, a custom converter can be used.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-07 19:30:07.000 UTC</summary>

After team discussion what we'll do is:

* Add support for `UUID` and maybe a couple of other types at https://pydantic-docs.helpmanual.io/usage/types/, but not ambiguous ones like `datetime` (yet, even though it can be argued ISO 8601 is _close_ to universal)
* Do not support `json()` from Pydantic models so we don't have to parse right back out to dict for nested use. Rather we will wait until Pydantic supports JSON-able dicts.
* Add a sample with easy to take code to allow _all_ Pydantic models to work via the slower JSON string approach

Reactions: ðŸ‘ 1

</details>


---

<a id="68"></a>

### #68: [Feature Request] Inject dependencies into activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/68 |
| **State** | CLOSED |
| **Author** | gregbrowndev (Greg Brown) |
| **Created** | 2022-07-10 12:13:31.000 UTC (3y 6m ago) |
| **Updated** | 2023-11-09 20:34:02.000 UTC |
| **Closed** | 2022-07-14 14:56:35.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 5 |
| **Priority Score** | 7 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

Hi, 

This may admittedly be more of a question than a feature request as there might already be a simple solution I missed. But nonetheless, this may be a topic worth documenting for users new to async/temporal.

I'm looking for a way to pass request/job-specific dependencies into the activity functions, such as the DB session. One idea described below uses a factory to create the worker, like you'd see in some other frameworks. The problem is creating a closure around the activities makes it impossible to import those activities into the workflows throwing away type-safety.

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

Here's an example of how I initially tried to solve the issue:

```py
# main.py
def create_worker() -> Worker:
    my_app: MyApp = bootstrap()

    @activity.defn
    async def task1(user_id: uuid.UUID) -> str:
        return my_app.task1(
            user_id=user_id
        )

    @activity.defn
    async def task2(user_id: uuid.UUID) -> str:
        return my_app.task2(
            user_id=user_id
        )

    @activity.defn
    async def task3(user_id: uuid.UUID) -> str:
        return my_app.task3(
            user_id=user_id
        )
    
    client = await Client.connect("http://localhost:7233")
    return Worker(
        client,
        task_queue="my-task-queue",
        workflows=[MyWorkflow],
        activities=[
            task1,
            task2,
            task3,
        ],
    )


async def main():
    worker = create_worker()
    await worker.run()


if __name__ == "__main__":
    asyncio.run(main())
``` 

This is similar to how you might write a Flask API's `create_app` factory. Another framework, FastAPI, supports async and has full DI around its route handlers.

I don't have a great deal of experience building async apps in Python, so it isn't completely clear to me whether the above solution would even be safe. The `MyApp` object returned by `bootstrap` handles transaction management and all the business logic (follows a Hexagonal approach), so it definitely shouldn't be used for more than one activity at a time. 

I'm going to feel silly now after writing all of this if the simplest solution is just to call `bootstrap` within the activity like below:

```py
@activity.defn
async def task1(user_id: uuid.UUID) -> str:
    my_app = bootstrap()
    return my_app.task1(
        user_id=user_id
    )
```

I've always thought this would be bad to repeatedly throw away the internal DB session and various HTTP/SDK clients after each "request". But I guess that only works in sync frameworks because it processes one request at a time.

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->

I think I may have just rubber-ducked the problem to myself, but it would be good to see some guidance on the best practice here.

Thanks! 


#### Comments (5)

<details>
<summary><strong>gregbrowndev</strong> commented on 2022-07-10 13:06:22.000 UTC</summary>

Actually, some form of DI probably would be quite useful, as I now have a circular dependency:

```mermaid
  graph TD;
      activities.py-->bootstrap.py;
      bootstrap.py-->adapter.py;
      adapter.py-->workflow.py;
      workflow.py-->activities.py;
```

Hopefully, it's clear enough knowing I'm building a hexagonal-style app without seeing all the source code, but I'll try to explain:

* `activities.py` contains the Temporal activity definitions
* `bootstrap.py` is a composition root, it builds all the dependencies of `MyApp`, i.e. each outbound adapter and their dependencies and returns an instance of `MyApp`
* `adapter.py` is an outbound adapter that exposes workflow functions to `MyApp`, e.g. APIs to start the workflow, run signals and queries without the app depending directly on the Temporal workflow
* `workflow.py` contains the Temporal workflow definition

For now, I think the only way to break the circular dependency is to use the string form of `execute_activity`.

</details>

<details>
<summary><strong>gregbrowndev</strong> commented on 2022-07-10 19:58:10.000 UTC</summary>

Also, it is interesting to find and read the discussion about [workflow/activity contracts](https://github.com/temporalio/proposals/blob/master/python/phase-2.md#workflow-contracts). For me at least it wouldn't be a problem if the contract didn't include the signal/query decorators, since that is how I plan to expose those functions on the adapter interface to my app anyway:

```py
class IPriceChangeWorkflowAdapter(t.Protocol):
    async def run(self, price_change: PriceChange) -> None:
        ...

    async def query_some_state(self, price_change: PriceChange) -> Foo:
        ...
    
    async def send_some_signal(self, price_change: PriceChange) -> None:
        ...
```

and the concrete impl would look like:

```py
class PriceChangeWorkflowAdapter(IPriceChangeWorkflowAdapter):
    client: Client

    def __init__(self, client: Client):
        self.client = client

    async def run(self, price_change: PriceChange) -> None:
        price_change_id = str(price_change.price_change_id)
        await self.client.start_workflow(
            PriceChangeWorkflow.run,
            price_change_id,
            id=price_change_id,
            task_queue="my-task-queue",
        )
    
    async def query_some_state(self, price_change: PriceChange) -> Foo:
        # get workflow object
        # send query 

    async def send_some_signal(self, price_change: PriceChange) -> None:
        # get workflow object
        # send signal 
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-11 12:21:57.000 UTC</summary>

> it isn't completely clear to me whether the above solution would even be safe

It is safe.

We are actively working on #52 which will solve your problem here. You'll be able to more-easily register methods on a class instance or a class with `__call__` as an activity. Then, you can just store your externally needed state on the class (you can do this today, but it's a bit confusing from the workflow caller side, see discussion at #66).

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>vaidik</strong> commented on 2023-11-09 16:33:32.000 UTC</summary>

I think I am in kind of a similar situation but not able to wrap my head around how to make it work on the basis of @cretz 's response.

My situation is this:

I have a FastAPI app. It is structured like so:
* Controllers (as in MVC) - get executed on an incoming HTTP request. Sanitize the incoming payload and perform the business logic.
* Services - actually wrap the business logic as well as the calls to the database (models). So controllers handle the HTTP request and then call services. Services can do one or more of these things:
  * Wrap all the business logic
  * Perform one or more database operations (CRUD)
  * Execute part of the business logic asynchronously using Temporal by executing a workflow

So for example, there is an HTTP API to create a user and then send a message on slack to someone. Ideally, sending a message to someone/some channel on Slack happens asynchronously using a Temporal workflow, which executes a Temporal activity. Now what I want to achieve is that I want to keep the logic of sending message on Slack also on the service class to keep everything related to creating a user colocated. Here is the pseudo code:


user_service.py:
```py
from create_user_workflow import CreateUserWorkflow

class UserService:

   def __init__(self):
      self.db = get_db()
      self.temporal_client = Client(...)

   async def create_user(self, username):
      user = User(username=username)
      self.db.add(user)
      self.db.commit()
      await self.temporal_client.start_workflow(
         CreateUserWorkflow,
         ...
      )

    def notify_on_slack(self, username):
      ....
      # Do something to send message on slack about the new user
```


create_user_workflow.py:
```py
from user_service import UserService

@activity.defn(name="notify_on_slack")
async def notify_on_slack(username):
  user_service = UserService()
  user_service.notify_on_slack(username)

@workflow.defn(name="CreateUserWorkflow")
class CreateUserWorkflow:

  @workflow.run
  def run(self, params):
    await workflow.start_activity(
      notify_on_slack
    )
```

But this leads to Cyclic Import in Python.

My current workaround is that I have just moved the logic of sending a message to Slack from the `UserService` to the workflow module, but I am not too happy about that.

What's the right way to do this?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-11-09 20:32:01.000 UTC</summary>

@vaidik - feel free to join us on Slack or the forums. It can be a bit hard to get good answers on closed issues. In your case, it doesn't make much sense to put an activity in a workflow file. There are general ways to avoid this kind of circular import in Python, it's not really specific to Temporal (imagine you had FastAPI service A depending on B which depended on C which depended on A, how would you solve?)

</details>


---

<a id="62"></a>

### #62: [Bug] retry_policy not availably in activity info

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/62 |
| **State** | CLOSED |
| **Author** | nathanielobrown (Nathaniel Brown) |
| **Created** | 2022-07-05 19:21:23.000 UTC (3y 6m ago) |
| **Updated** | 2022-07-12 20:28:53.000 UTC |
| **Closed** | 2022-07-12 20:28:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 7 |
| **Priority Score** | 7 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

From withing an activity I want to get the attempt number

### Describe the bug

I pass in a `retry_policy` to `workflow.execute_activity` and expect to be able to get that policy from within the execute activity with `activity.iinfo().retry_policy` but I get `None` instead.

### Minimal Reproduction
Not entirely self contained example, but I hope you get the idea.

I tried to make a failing test in this repo, but I'm still unable to run the development environment. I guess this is a separate issue, but running the tests fails with `ImportError: dlopen(/Users/nob/repos/sdk-python/temporalio/bridge/temporal_sdk_bridge.abi3.so, 0x0002): tried: '/Users/nob/repos/sdk-python/temporalio/bridge/temporal_sdk_bridge.abi3.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e'))`. My guess is that rust is cross compiling for `x86_64` rather than the `arm64e` I need for my M1 Mac for some reason ðŸ¤·â€â™‚ï¸.

```python
from datetime import timedelta
import uuid
from temporalio import activity, workflow
from temporalio.client import Client
from temporalio.common import RetryPolicy
from bots_interface.temporal.server import temporal_worker_for_test


@activity.defn
async def return_retry_policy() -> RetryPolicy:
    activity_info = activity.info()
    # This is None!!!
    return activity_info.retry_policy


@workflow.defn
class Workflow:
    @workflow.run
    async def run(self) -> RetryPolicy:
        policy = await workflow.execute_activity(
            return_retry_policy,
            retry_policy=RetryPolicy(maximum_attempts=3),
            schedule_to_close_timeout=timedelta(seconds=5),
        )
        return policy


async def test_activity_info_retry_policy(temporal_client: Client):
    async with temporal_worker_for_test(
        temporal_client, Workflow, activities=[return_retry_policy]
    ) as worker:
        result = await temporal_client.execute_workflow(
            Workflow.run,
            id=str(uuid.uuid4()),
            task_queue=worker.task_queue,
            execution_timeout=timedelta(seconds=10),
            retry_policy=RetryPolicy(maximum_attempts=1),
        )
    assert result.maximum_attempts == 3  # This fails
```

### Environment/Versions


- OS and processor: M1 Mac, MacOS 12.4
- Temporal Version: `temporalio==0.1a2`
- Are you using Docker or Kubernetes or building Temporal from source?: Using embedded golang temporal server as used in the repo. So compiling myself I guess


#### Comments (7)

<details>
<summary><strong>cretz</strong> commented on 2022-07-05 19:26:26.000 UTC</summary>

Thanks! Will investigate why retry policy is not there. Looks like I didn't properly assert for it at https://github.com/temporalio/sdk-python/blob/0392468f21d9604c828d50d34daf8c9be03b31cd/tests/worker/test_activity.py#L78. There may be a bug or two with what the server is returning on activity execution.

As for the cross-compile issue, you'll need to check `rustup` and Rust in general is defaulted to the arm target and not the x64 one. Once confirmed your default compilation target is arm, you can `poe build-develop` to get the right shared lib.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-05 19:56:00.000 UTC</summary>

Ah, thank you. This explains it:
```
rustup show
Default host: x86_64-apple-darwin
rustup home:  /Users/nob/.rustup

stable-x86_64-apple-darwin (default)
```
I switched my default toolchain with `rustup default stable-aarch64-apple-darwin`, re-ran `poe build-develop` and now I can run the tests. Nice to learn a little something about rust.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-05 21:14:07.000 UTC</summary>

I have confirmed via https://github.com/temporalio/sdk-core/blob/15ea5d3ee580e2a226eefdcf0d8444b2ae30a997/protos/local/temporal/sdk/core/activity_task/activity_task.proto#L55-L58 that we should not even expose this (that is a different kind of retry policy). None of our SDKs provide the original retry policy via the activity info (ref https://pkg.go.dev/go.temporal.io/sdk/internal#ActivityInfo, https://typescript.temporal.io/api/interfaces/activity.Info, https://www.javadoc.io/static/io.temporal/temporal-sdk/1.14.0/io/temporal/activity/ActivityInfo.html, etc) and Python shouldn't either.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-05 22:52:38.000 UTC</summary>

Hmm, OK is there a different way for an activity to understand what retry policy it's operating under?

My use case is that my existing tasks (run using Dramatiq) log a Sentry errors and HTTP request logs on the _last retry_, which reduces logged errors to those that were not solved by retry. At some point I would also like to ask your opinion on the best way to do a Sentry integration (my guess is an interceptor?) but I don't want to ask too many questions in one day ðŸ™‚.

If there's a different approach you think could solve this problem, I would love to hear that as well.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-06 13:42:39.000 UTC</summary>

> Hmm, OK is there a different way for an activity to understand what retry policy it's operating under? [...] My use case is that my existing tasks (run using Dramatiq) log a Sentry errors and HTTP request logs on the last retry, which reduces logged errors to those that were not solved by retry.

The problem is that an activity may not even get called for its last attempt (what if it times out before execution?). However, if you still want to do something from the activity side on last attempt, you probably need to send that as a parameter to the activity, e.g.:

```python
@dataclass
class MyActivityParams:
    some_other_param: str
    send_error_after_attempt: int

@activity.defn
async def my_activity(params: MyActivityParams):
    try:
        ...
    except Exception as err:
        if activity.info().attempt >= params.send_error_after_attempt:
            send_error(err)
        raise
```

This is untested of course, I just typed it here in issues.

But it can often be clearer and more accurate to react to activity failures when you get them back from the workflow instead of in the activity (which yes may mean starting another activity to send off the failure).

> At some point I would also like to ask your opinion on the best way to do a Sentry integration (my guess is an interceptor?) but I don't want to ask too many questions in one day ðŸ™‚.

Ask away. Though I would recommend the forums at https://community.temporal.io/ or the `#python-sdk` Slack channel at https://temporal.io/slack for questions instead of GH issues.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-06 17:17:15.000 UTC</summary>

> The problem is that an activity may not even get called for its last attempt (what if it times out before execution?). However, if you still want to do something from the activity side on last attempt, you probably need to send that as a parameter to the activity, e.g.:

In practice, I do not find this to be an issue. For timeouts, all I really want to track is what percentage of the time a given task times out, I don't want Sentry errors because the details are not usually important and as a timeout can happen at any time you get a whole bunch of different error reports for one reason: the task is timing out.

The issue with logging activity failures from the workflow is I would lose all of the great context that Sentry collects, like local variables and breadcrumbs. For actually control flow due to the activity failures, I agree doing that in the workflow is best but I'm thinking more of observability of errors here.

It would be nice to have the retry policy because then you can have error handling contexts that can react to to the retry policy in a generic way without passing this for every activity. However, if it's not there it's not there and thank you for the example. At least I have the attempt number and I can make it work.

I'll do some experiments with Sentry setups and ask away when I'm a bit more oriented.


Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-12 20:28:53.000 UTC</summary>

Closing. Feel free to reopen as necessary.

</details>


---

<a id="1154"></a>

### #1154: [Feature Request] Support InterpreterPoolExecutor from py3.14

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1154 |
| **State** | OPEN |
| **Author** | blueraft (Ahmed Ilyas) |
| **Created** | 2025-10-13 09:41:39.000 UTC (2 months ago) |
| **Updated** | 2025-10-15 15:51:48.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 2 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 2 |

#### Description

Python 3.14 adds [InterpreterPoolExecutor](https://docs.python.org/3.14/library/concurrent.futures.html#concurrent.futures.InterpreterPoolExecutor), a subclass of ThreadPoolExecutor that runs tasks in isolated subinterpreters.

The Temporal Python SDK appears to assume ThreadPoolExecutor-like behavior in [worker/_activity.py](https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/_activity.py), which doesnâ€™t hold for InterpreterPoolExecutor. Using it as an activity executor leads to the following errors:

```python
concurrent.interpreters.NotShareableError: (Queue(2), <built-in method run of _contextvars.Context object at 0x1015fec00>, (<function _execute_sync_activity at 0x1014171c0>, Info(activity_id='61', activity_type='task', attempt=3, current_attempt_scheduled_time=datetime.datetime(2025, 10, 13, 9, 32, 51, 324811, tzinfo=datetime.timezone.utc), heartbeat_details=[], heartbeat_timeout=None, is_local=False, schedule_to_close_timeout=None, scheduled_time=datetime.datetime(2025, 10, 13, 9, 32, 47, 122424, tzinfo=datetime.timezone.utc), start_to_close_timeout=datetime.timedelta(seconds=36000), started_time=datetime.datetime(2025, 10, 13, 9, 32, 51, 692224, tzinfo=datetime.timezone.utc), task_queue='hello-task-queue', task_token=b'\n$0199c8e3-3716-7223-b18b-3a720150440f\x1231906eac1-8e34-4119-b7d5-0da5e8ffefd4-hello-workflow\x1a$0199dcea-4e2f-7cb2-bf9b-ef9859498ab6 A(\x032\x0261B\x04taskJ\x08\x08\x01\x10\x93\xb4A\x18\x01', workflow_id='1906eac1-8e34-4119-b7d5-0da5e8ffefd4-hello-workflow', workflow_namespace='default', workflow_run_id='0199dcea-4e2f-7cb2-bf9b-ef9859498ab6', workflow_type='GreetingWorkflow', priority=Priority(priority_key=None, fairness_key=None, fairness_weight=None), retry_policy=RetryPolicy(initial_interval=datetime.timedelta(seconds=1), backoff_coefficient=2.0, maximum_interval=datetime.timedelta(seconds=100), maximum_attempts=0, non_retryable_error_types=None)), <function _ActivityInboundImpl.execute_activity.<locals>.<lambda> at 0x1016381a0>, <temporalio.worker._activity._ThreadExceptionRaiser object at 0x10161df60>, <threading.Event at 0x101620fb0: unset>, <threading.Event at 0x10152d810: unset>, <temporalio.converter.DefaultPayloadConverter object at 0x1010e4c20>, <temporalio.common._NoopMetricMeter object at 0x100d9be00>, _ActivityCancellationDetailsHolder(details=None), <function task at 0x101529430>), {}) does not support cross-interpreter data
```




#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-15 15:33:59.000 UTC</summary>

We'll take a look when we can. We still have to do some due diligence on compatibility with python 3.14 as a whole. Some of our dev dependencies don't (or didn't recently) support 3.14 yet, so officially supporting and validating it will take some work.

</details>

<details>
<summary><strong>blueraft</strong> commented on 2025-10-15 15:51:48.000 UTC</summary>

Thanks, appreciate it!

</details>


---

<a id="837"></a>

### #837: [Feature Request] Make Temporal logger adapter accomodate to OpenTelemetry

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/837 |
| **State** | OPEN |
| **Author** | RoggeOhta (RoggeOhta) |
| **Created** | 2025-04-22 11:48:20.000 UTC (8 months ago) |
| **Updated** | 2025-04-24 14:45:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘€ 1 |

#### Description

### Problem
Temporal's activity/workflow logger adapter uses a Dict for its "extra" field, violating the OpenTelemetry format that requires basic data types (bool, str, bytes, int, float).

### Solution
#838 


#### Comments (6)

<details>
<summary><strong>RoggeOhta</strong> commented on 2025-04-22 12:01:15.000 UTC</summary>

#838 I've write a draft solution for this problem

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-22 13:42:22.000 UTC</summary>

Thanks for the issue!

We cannot incompatibly change default logging for all users because of OpenTelemetry-only limitations. Also per https://opentelemetry.io/docs/languages/python/, OTel logging is still in development. Once it does stabilize, we can definitely look into opt-in options for either splatting the details as done in the PR, or adding something to `temporalio.contrib.opentelemetry` to help handle this nested "extra" data on log records.

Digging through the OTel source, it seems attributes can have nested dicts. It links to https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md#field-attributes which says the attribute values can be `any` which supports nested dicts I think: https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md#type-any. I wonder what type of error you are seeing here?

</details>

<details>
<summary><strong>RoggeOhta</strong> commented on 2025-04-22 15:57:09.000 UTC</summary>

> Thanks for the issue!
> 
> We cannot incompatibly change default logging for all users because of OpenTelemetry-only limitations. Also per https://opentelemetry.io/docs/languages/python/, OTel logging is still in development. Once it does stabilize, we can definitely look into opt-in options for either splatting the details as done in the PR, or adding something to `temporalio.contrib.opentelemetry` to help handle this nested "extra" data on log records.
> 
> Digging through the OTel source, it seems attributes can have nested dicts. It links to https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md#field-attributes which says the attribute values can be `any` which supports nested dicts I think: https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md#type-any. I wonder what type of error you are seeing here?

Thank you for the response. I wasn't aware of the opentelemetry-specification. The error is actually a warning generated by the `opentelemetry-sdk` Python library.
https://github.com/open-telemetry/opentelemetry-python/blob/742171e50c70f4a4540a01d2c6c3cbcf4882d810/opentelemetry-api/src/opentelemetry/attributes/__init__.py#L111

The library verifies if an attribute type is among the `_VALID_ATTR_VALUE_TYPES`, but it has a broader category called `_VALID_ANY_VALUE_TYPES` that includes Mapping types, which was not utilized. When it finds attributes that are not of basic types, it removes them, complicating the ability to trace specific activities or workflows.
https://github.com/open-telemetry/opentelemetry-python/blob/742171e50c70f4a4540a01d2c6c3cbcf4882d810/opentelemetry-api/src/opentelemetry/attributes/__init__.py#L25

Could we potentially implement a workaround to address this, like adding an option to flatten the logger's extra field?

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-22 22:46:09.000 UTC</summary>

> Could we potentially implement a workaround to address this, like adding an option to flatten the logger's extra field?

I am concerned about flattening Temporal-specific values just for OTel purposes. To do this right, we'd really need to prefix `temporal_activity.` and `temporal_workflow.` to all the extra value names when flattening. But it is technically something we can do, though we may want to wait until logging stabilizes in case there is a better handler-level approach we can devise instead of doing this at the logger level.

I think a better approach may be to encourage custom adapters. You could set `temporalio.activity.logger` and `temporalio.workflow.logger` to your own class extensions of `temporalio.activity.LoggerAdapter` and `temporalio.workflow.LoggerAdapter` setting the `activity_info_on_extra` and `workflow_info_on_extra` to `False` on super init or post super init. Then you can override `process` to set whatever "extra" values from the activity/workflow context you'd like instead of using the preset default ones. This is the primary reason we exposed and documented those adapter classes is for this kind of advanced customization.

</details>

<details>
<summary><strong>RoggeOhta</strong> commented on 2025-04-24 10:04:11.000 UTC</summary>

Okay, I found a workaround. Posting in case it helps others who encounter this issue.

First, set global `activity` and `workflow` logger at your program init. Turn off the extra info.
```python
activity.logger.activity_info_on_extra = False
workflow.logger.workflow_info_on_extra = False
```

Then, in every `activity/workflow`, add this info manually to your own logger.
```
# start of some activity
my_logger.extra = activity.info()._logger_details()

# start of some workflow
my_logger.extra = workflow.info()._logger_details()
```

This should bypass the python oTel library warning while retaining activity trace logs.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-24 14:38:32.000 UTC</summary>

:+1:

> Then, in every activity/workflow, add this info manually to your own logger

An alternative is to replace those loggers with your own extension of `activity.LoggerAdapter` and `workflow.LoggerAdapter` that overrides `process` to set these values.

The concern here for us adding this option as-is is that we need to qualify all keys with `temporal_activity.` and `temporal_workflow.` prefixes probably if we're not allowed nested maps. We may be able to add that option today or wait until OTel logging stabilizes.

Reactions: ðŸ‘ 2

</details>


---

<a id="646"></a>

### #646: [Bug] _convert_payloads fails with One or more mappers failed to initialize

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/646 |
| **State** | CLOSED |
| **Author** | rakesh-163 |
| **Created** | 2024-09-18 16:36:33.000 UTC (1y 3m ago) |
| **Updated** | 2024-12-17 18:27:19.000 UTC |
| **Closed** | 2024-12-16 22:23:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I have an activity that calls a function that performs database operations. The first operation in that activity is a read. The SQLModel that I am trying to read is called a "Story".

### Describe the bug

The activity runs fine... until it starts to fail and cause failures for the workflow that calls it... It says something about the workflow accessing os.environ.get (See stack trace below) but the function that calls does not have that. So, it is either that the failure should not occur or it may be that it is telling the wrong reason why it occurs. In either case, it is a bug. 

Also, I have scoured the library (i.e. SQLAlchemy) module for signs of an os.environ.get, I did not find any. 

I have also passed through all external library imports at this point...

Any help would be appreciated!


### Minimal Reproduction

It is hard to reproduce because a lot of the times the activity just succeeds. It is usually after an hour or so, this particular activity starts to fail.

### Environment/Versions

- OS and processor: Linux
- Temporal Version: Python SDK: temporalio==1.6.0
- Are you using Docker or Kubernetes or building Temporal from source? I am using Docker for the workers.

### Additional context

Here's the full stack trace that I see when the failure occurs:


{"message":"Failed decoding arguments","stackTrace":"  File \"/usr/local/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py\", line 326, in activate\n    self._apply(job)\n\n  File \"/usr/local/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py\", line 422, in _apply\n    self._apply_resolve_activity(job.resolve_activity)\n\n  File \"/usr/local/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py\", line 654, in _apply_resolve_activity\n    ret_vals = self._convert_payloads(\n               ^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py\", line 1563, in _convert_payloads\n    raise RuntimeError(\"Failed decoding arguments\") from err\n","cause":{"message":"One or more mappers failed to initialize - can't proceed with initialization of other mappers. Triggering mapper: 'Mapper[Story(story)]'. Original exception was: Cannot access os.environ.get from inside a workflow. If this is code from a module not used in a workflow or known to only be used deterministically from a workflow, mark the import as pass through.","stackTrace":"  File \"/usr/local/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py\", line 1555, in _convert_payloads\n    return self._payload_converter.from_payloads(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/site-packages/temporalio/converter.py\", line 307, in from_payloads\n    values.append(converter.from_payload(payload, type_hint))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/site-packages/temporalio/converter.py\", line 583, in from_payload\n    obj = value_to_type(type_hint, obj, self._custom_type_converters)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/site-packages/temporalio/converter.py\", line 1533, in value_to_type\n    return getattr(hint, \"parse_obj\")(value)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/site-packages/typing_extensions.py\", line 2853, in wrapper\n    return arg(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/site-packages/sqlmodel/main.py\", line 951, in parse_obj\n    return cls.model_validate(obj, update=update)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/site-packages/sqlmodel/main.py\", line 848, in model_validate\n    return sqlmodel_validate(\n           ^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/site-packages/sqlmodel/_compat.py\", line 311, in sqlmodel_validate\n    new_obj = cls()\n              ^^^^^\n\n  File \"<string>\", line 4, in __init__\n\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/orm/state.py\", line 566, in _initialize_instance\n    manager.dispatch.init(self, args, kwargs)\n\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/event/attr.py\", line 497, in __call__\n    fn(*args, **kw)\n\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/orm/mapper.py\", line 4396, in _event_on_init\n    instrumenting_mapper._check_configure()\n\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/orm/mapper.py\", line 2388, in _check_configure\n    _configure_registries({self.registry}, cascade=True)\n\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/orm/mapper.py\", line 4204, in _configure_registries\n    _do_configure_registries(registries, cascade)\n\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/orm/mapper.py\", line 4241, in _do_configure_registries\n    raise e\n","applicationFailureInfo":{"type":"InvalidRequestError"}},"applicationFailureInfo":{"type":"RuntimeError"}}

---

Also, I am using this sandboxed runner on the workers to help deal with the datetime issue that the Pydantic models pose. I am not sure if this interacts with the converter bits in the stack trace above.

def new_sandbox_runner() -> SandboxedWorkflowRunner:
    # TODO(cretz): Use with_child_unrestricted when https://github.com/temporalio/sdk-python/issues/254
    # is fixed and released
    invalid_module_member_children = dict(
        SandboxRestrictions.invalid_module_members_default.children
    )
    del invalid_module_member_children["datetime"]
    return SandboxedWorkflowRunner(
        restrictions=dataclasses.replace(
            SandboxRestrictions.default,
            invalid_module_members=dataclasses.replace(
                SandboxRestrictions.invalid_module_members_default,
                children=invalid_module_member_children,
            ),
        )
    )

#### Comments (6)

<details>
<summary><strong>cretz</strong> commented on 2024-09-23 13:20:27.000 UTC</summary>

Something in `sqlalchemy` is calling `os.environ` but they choose to swallow that stack trace and wrap with "One or more mappers failed to initialize" so you can't see where. I would recommend not using `sqlalchemy` ORM objects inside a workflow, but instead have simpler `dataclass` objects you translate to `sqlalchemy` equivalents in activities as needed.

</details>

<details>
<summary><strong>rakesh-163</strong> commented on 2024-09-24 20:54:01.000 UTC</summary>

Hey Chad, Thanks for the reply. Appreciate that you looked inside the sqlalchemy codebase. Could you point me to the line of code that is doing the os.environ call? I could not find it when I grepped for it.


</details>

<details>
<summary><strong>cretz</strong> commented on 2024-09-25 13:09:53.000 UTC</summary>

> Could you point me to the line of code that is doing the os.environ call? I could not find it when I grepped for it.

It may be nested in something else and not directly called. To debug, first you'd need to patch sqlalchemy to not swallow the true stack trace of why a mapper fails to initialize. Probably around https://github.com/sqlalchemy/sqlalchemy/blob/rel_2_0_35/lib/sqlalchemy/orm/mapper.py#L4232-L4252. You need to true stack trace of that exception. Regardless, I would not recommend using sqlalchemy models in workflows because they are likely non-deterministic.

</details>

<details>
<summary><strong>nvachhar</strong> commented on 2024-12-16 21:59:24.000 UTC</summary>

I've hit lots of issues with SQLModel and Temporal. One thing I learned is that using any `SQLModel` where `table=True` as a type to pass between Workflows and Activities is fraught. These objects have tight relationships to sessions and it requires far too much care to get things right. Instead, having a base class where `table=False` is far safer. This class behaves like a Pydantic `BaseModel` and is a lot safer/easier to use.

</details>

<details>
<summary><strong>rakesh-163</strong> commented on 2024-12-16 22:23:41.000 UTC</summary>

Yes! Indeed, I ended up doing exactly what you are suggesting. It is not ideal though. 

There is something in the SQLAlchemy (the ORM) that irritates the Python SDK. 

I am not sure if there is any "real" non-determinism though. I am just trying to write to a database using an ORM, really. It is either that the ORM is doing something fancy (and I never had the time to dig into why), or it is because the Python SDK is being very aggressive about the use of os.environ.get -- when practically, the environment doesn't really change.

</details>

<details>
<summary><strong>nvachhar</strong> commented on 2024-12-17 18:27:18.000 UTC</summary>

I think assuming the environment doesn't change is dangerous. Workflows and activities can be running in different processes on different machines. If you scale out your worker, there could be multiple instances with different environments. Since workflows have to be deterministic, blocking the environment seems reasonable.

IMHO, SQLModel is very appealing but differences between how table models and non-table models work is a bit jarring. I don't have the example handy, but even model validation is different in the 2 cases in a way I found shocking. I've never used SQLAlchemy without SQLModel, but there's quite a bit of subtlety in how to use ORM objects. Combine that with the strict rules of Temporal, and it makes for some complex work.

But let us all know if you ever do figure out what was accessing the environment. These errors are the most annoying to track down.

Reactions: ðŸ‘ 1

</details>


---

<a id="627"></a>

### #627: [Feature Request] Support serialization of `datetime` values in default JSON converter

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/627 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-08-28 13:09:49.000 UTC (1y 4m ago) |
| **Updated** | 2025-06-05 22:14:33.000 UTC |
| **Closed** | 2025-06-05 22:14:33.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 0 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 3 |

#### Description

### Describe the solution you'd like

`datetime` needs to be supported to/from string in default JSON converter (use iso8601).


---

<a id="429"></a>

### #429: [Bug] asyncio.wait is non-deterministic when used with coroutines instead of tasks

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/429 |
| **State** | CLOSED |
| **Author** | ntessman-capsule (Nate Tessman) |
| **Created** | 2023-11-15 06:19:10.000 UTC (2y 1m ago) |
| **Updated** | 2026-01-05 07:10:20.000 UTC |
| **Closed** | 2024-06-04 14:07:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸš€ 1 |

#### Description

### What are you really trying to do?

Split a list of incoming events into concurrently executed activity streams. The repro below mimics a stripped down version of the original code structure.

### Describe the bug

When using `asyncio.wait()` with a list of async method calls, which contain a series of activity executions, an error can occur which results in later activities receiving the return values of different concurrent executions.

Here is a table to illustrate what I mean. Given an activity that executes `f(x) -> x`, if we pass the numbers 1-3 concurrently, we might see this:

|Activity Input|Expected Output|Actual Output|
|-------------|---------|-------|
|1                    |1              |1         |
|2                   |2              |3         |
|3                    |3              |2         |

This issue is random, and rare. The reproduction below is designed to maximize the chance of running into it.

### Minimal Reproduction

Below is a self-contained reproduction of the issue. Using this input should encounter the error fairly consistently, a little less than once per run:

```json
{"execution_iterations": 20,"activity_iterations": 5,"concurrency": 100,"wait_time": 0.1}
```

The temporal setup I used to reproduce this issue is a vanilla Temporal server install running via `temporal server start-dev`, with two worker instances running the below python file using `python -m main`. I primarily tested using Python 3.8.

When there is a mismatch, it is printed to the console.

```python
import asyncio
from datetime import timedelta
from typing import List
from temporalio import workflow, activity
from temporalio.worker import Worker
from temporalio.client import Client

from dataclasses import dataclass

@dataclass
class EchoRequest:
    execution_iteration: int
    activity_iteration: int
    input: str
    wait_time: float

@dataclass
class EchoResponse:
    output: str

@activity.defn(name="echo")
async def echo(
    echo: EchoRequest
) -> EchoResponse:
    await asyncio.sleep(echo.wait_time)
    return EchoResponse(output=echo.input)


@dataclass
class RaceConditionIteration:
    execution_id: int
    iterations: int
    wait_time: float


@dataclass
class RaceConditionTestInput:
    execution_iterations: int
    activity_iterations: int
    concurrency: int
    wait_time: float


@activity.defn(name="race_condition_get_input_events")
async def race_condition_get_input_events(
    input: RaceConditionTestInput
) -> List[RaceConditionIteration]:
    return [
        RaceConditionIteration(
            execution_id=id,
            iterations=input.activity_iterations,
            wait_time=input.wait_time,
        )
        for id in range(input.concurrency)
    ]


@workflow.defn(name="RaceConditionTestWorkflow")
class RaceConditionTestWorkflow:
    @workflow.run
    async def run(self, event: RaceConditionTestInput) -> str:
        print('Starting workflow...')
        
        # Simulate a list of input events
        generated_input_data = await workflow.execute_activity(
            race_condition_get_input_events,
            event,
            start_to_close_timeout=timedelta(seconds=10),
            schedule_to_close_timeout=timedelta(seconds=60)
        )

        for iteration in range(event.execution_iterations):
            _, _ = await asyncio.wait(
                [
                    self.execute_activities(
                        iteration=iteration,
                        input=input
                    )
                    for input in generated_input_data
                ],
                return_when=asyncio.ALL_COMPLETED
            )
        
        return "Done."

    async def execute_activities(
        self,
        iteration: int,
        input: RaceConditionIteration,
    ) -> None:
        for iter in range(input.iterations):
            iter_id = f"{iteration}.{input.execution_id}.{iter}"
            result = await workflow.execute_activity(
                echo,
                EchoRequest(
                    execution_iteration=iteration,
                    activity_iteration=iter,
                    input=iter_id,
                    wait_time=input.wait_time
                ),
                start_to_close_timeout=timedelta(seconds=10),
                schedule_to_close_timeout=timedelta(seconds=60)
            )

            if iter_id != result.output:
                print(f"Expected: {iter_id}, Actual: {result.output}")
            

async def main():
    client = await Client.connect("localhost:7233")

    worker = Worker(
        client,
        task_queue="race-condition",
        workflows=[RaceConditionTestWorkflow],
        activities=[echo, race_condition_get_input_events],
    )

    print('Starting worker...')

    await worker.run()

if __name__ == '__main__':
    asyncio.run(main())
```

### Environment/Versions

- OS and processor: Tested on M2 Mac, Linux
- Temporal SDK Versions: Tested on 1.13, 1.14
- Python: <3.11 (mitigated in 3.11 due to disallowing coroutines, see below)
- Occurs within Kubernetes deployment as well as local install

### Additional context

It appears that this bug occurs after this warning is printed to the worker console:

```
2023-11-15T05:38:09.387580Z  WARN temporal_sdk_core::worker::workflow: Task not found when completing error=status: NotFound, message: "Workflow task not found.", details: [8, 5, 18, 24, 87, 111, 114, 107, 102, 108, 111, 119, 32, 116, 97, 115, 107, 32, 110, 111, 116, 32, 102, 111, 117, 110, 100, 46, 26, 66, 10, 64, 116, 121, 112, 101, 46, 103, 111, 111, 103, 108, 101, 97, 112, 105, 115, 46, 99, 111, 109, 47, 116, 101, 109, 112, 111, 114, 97, 108, 46, 97, 112, 105, 46, 101, 114, 114, 111, 114, 100, 101, 116, 97, 105, 108, 115, 46, 118, 49, 46, 78, 111, 116, 70, 111, 117, 110, 100, 70, 97, 105, 108, 117, 114, 101], metadata: MetadataMap { headers: {"content-type": "application/grpc"} } run_id="eb9b8aed-c730-4fe6-a7e5-a772f6757be2"
```

I can't confirm whether this warning was also appearing when this issue was happening in the real code. It also appears to result in the workflow task restarting, which seems correlated with the determinism breakdown.

Additionally, it appears that this issue has indirectly been mitigated in 3.11, as `asyncio.wait()` [no longer allows passing coroutines directly](https://github.com/python/cpython/issues/78971). When the method is wrapped in `asyncio.create_task()`, this issue disappears. The above warning is still printed, but doesn't result in disordered activity results.

This code change in the workflow removes the issue:

```diff
_, _ = await asyncio.wait(
    [
-      self.execute_activities(
+      asyncio.create_task(self.execute_activities(
            iteration=iteration,
            input=input
-       )
+       ))
        for input in generated_input_data
    ],
    return_when=asyncio.ALL_COMPLETED
)

#### Comments (6)

<details>
<summary><strong>HillaShx</strong> commented on 2023-11-27 09:06:22.000 UTC</summary>

I'm facing this same issue, it seems, by the logs. I, too, use asyncio for concurrent activity execution (by multiple workers separated by threads).

Environment/Versions
OS and processor: M2 Mac, MacOS Ventura 13.5.1
Temporal SDK Versions: 1.3.0
Python: 3.9.6
Occurs within Temporal Cluster service deployment, with the workers deployed to an Azure Web App.
LMK what other context might help in resolving this.


Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-11-27 18:12:12.000 UTC</summary>

Hrmm, I was only able to get the mismatch after replay (replay was being forced due to overgrowing history size I think). I am still investigating here...

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-11-27 19:11:40.000 UTC</summary>

Turns out `asyncio.wait` is non-deterministic. Here is the problem: https://github.com/python/cpython/blob/967f2a3052c2d22e31564b428a9aa8cc63dc2a9f/Lib/asyncio/tasks.py#L443

This is not a Temporal-specific issue. What is the output of the following?

```python
    async def do_something(name: str) -> None:
        print(f"Run {name}")
    await asyncio.wait([
        do_something("something1"),
        do_something("something2"),
        do_something("something3"),
    ])
    await asyncio.wait([
        do_something("something4"),
        do_something("something5"),
        do_something("something6"),
    ])
```

In 3.10, ignoring deprecation warnings, it may be something like (but it can change with only slight code alterations):

```
Run something1
Run something2
Run something3
Run something6
Run something4
Run something5
```

Why was that output not predictable?

Unfortunately, Python in `asyncio.wait` non-deterministically converts the input to a `set` (which unlike even dict, does not have any reliable ordering). This means the coroutines are not even started in the same order. This problem does not exist with `start_activity` because that is a sync function that returns an awaitable (so it is called when you invoke it not when awaited), but `execute_activity` is an async function (so it is called when it's awaited not when it's invoked). This means that `foo = workflow.execute_activity(...)` does nothing until `await foo` is run.

The reason this works with `create_task` is because `create_task` starts the coroutine immediately when `create_task` is run, so the coroutines run in a deterministic order.

So, basically we need to educate people that `asyncio.wait` is non-deterministic when used with coroutines instead of tasks (which isn't allowed in 3.11+). I was originally going to open a CPython issue to stop converting to a set (digging through git blame, this was done way back in at least 3.4), but that would only apply to new versions (doubt they'd cherry pick this), so you'd be forced to use tasks anyways.

Does this make sense? I will add a README note/warning. We could offer our own deterministic form of this function, but we'd probably require tasks anyways like Python does now. Any alternative suggestions or ways we can approach this?

Reactions: ðŸš€ 1

</details>

<details>
<summary><strong>ntessman-capsule</strong> commented on 2024-01-24 17:46:36.000 UTC</summary>

Thanks for your investigation @cretz. I'll leave this open for comment in case anyone has a better idea, but for my purposes this issue is resolved through the use of tasks. Personally, I think the only thing that can be done is provide a warning. It's a gotcha that is on Python's shoulders, not the SDK, and as you said it's unlikely that a fix will be applied retroactively.

Reactions: ðŸ‘ 2

</details>

<details>
<summary><strong>kshitij-g</strong> commented on 2025-12-29 15:40:20.000 UTC</summary>

Does this mean that asyncio.gather for multiple execute_activity is deterministic or not?

</details>

<details>
<summary><strong>ntessman-capsule</strong> commented on 2026-01-05 07:10:20.000 UTC</summary>

It appears that `asyncio.gather` uses a dictionary internally for its futures so it's probably safe to use. If you're unsure however, it's easy enough to wrap your tasks in a list and pass it to `workflow.wait` instead.

</details>


---

<a id="393"></a>

### #393: [Feature Request] Support ExceptionGroup traceback

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/393 |
| **State** | CLOSED |
| **Author** | jashandeep-sohi (Jashandeep Sohi) |
| **Created** | 2023-10-06 00:32:31.000 UTC (2y 3m ago) |
| **Updated** | 2023-10-20 12:40:06.000 UTC |
| **Closed** | 2023-10-20 12:40:06.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 4 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

I'm using a `asyncio.TaskGroup` in a workflow to launch and wait for a few activities in parallel. Happy path (seems) to work great, but when there's an exception in the `TaskGroup` async context manager, that's raised as a `ExceptionGroup` ([PEP 654](https://peps.python.org/pep-0654/)). Unfortunately, the sub-exception tracebacks are not included in the stack_trace, so it makes debugging a little harder.

### Describe the solution you'd like

I think this might stem from the fact you're `traceback.format_tb` ing the `__traceback__` https://github.com/temporalio/sdk-python/blob/d03f356fb65d1044e1182fdc1a4c60a432d420d8/temporalio/converter.py#L832C59-L832C59

I'm wondering if it's possible to use `traceback.format_exception` to get a output similar to what the Python spits out. That seems to include the sub-exceptions as-well. What's the advantage of using one over the other?

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2023-10-06 14:45:37.000 UTC</summary>

> I'm wondering if it's possible to use traceback.format_exception to get a output similar to what the Python spits out. That seems to include the sub-exceptions as-well. What's the advantage of using one over the other?

I believe I used `format_tb` over `format_exception` because I want just the traceback in our failure `stack_trace` field, not other exception information or multiple tracebacks.

We don't really have a concept of multiple exceptions in [our failure message](https://github.com/temporalio/api/blob/135691242e9b4ed6214a7b5e1231c1c9930ff6c8/temporal/api/failure/v1/message.proto#L81). I will discuss with the team on how best to support multiple exception stack traces, though it won't help immediately. You'll want to catch/log. But maybe we can just for `ExceptionGroup` we can use `format_exception`.

There's another problem with `ExceptionGroup` - we only fail the workflow on `FailureError` based exceptions, all others fail the task (which means suspend the workflow until a code fix is deployed to fix the unexpected error). `ActivityError` is a `FailureError` so if raised out of the workflow that will fail the workflow, but `ExceptionGroup` is not that type so it will be treated differently. So really, you'll probably need to catch the exception and re-raise as `ApplicationError` anyways, and it's at this time you can log if needed.

(feel free to also come discuss details on our `#python-sdk` channel on our public Slack at https://t.mp/slack if that's easier)

</details>

<details>
<summary><strong>jashandeep-sohi</strong> commented on 2023-10-06 20:59:22.000 UTC</summary>

> I believe I used `format_tb` over `format_exception` because I want just the traceback in our failure `stack_trace` field, not other exception information or multiple tracebacks.

Oh, I see. The Failure proto has a cause Failure, so you're keeping track of every level individually, that `format_exception` would also render if used.

My interaction with this field is mostly via the UI where it renders the traceback. As a Python developer I was expecting it to look similar to when an exception is printed by the Python interpreter to the console. And I think for the most part it does.

> We don't really have a concept of multiple exceptions in [our failure message](https://github.com/temporalio/api/blob/135691242e9b4ed6214a7b5e1231c1c9930ff6c8/temporal/api/failure/v1/message.proto#L81). I will discuss with the team on how best to support multiple exception stack traces, though it won't help immediately. You'll want to catch/log. But maybe we can just for `ExceptionGroup` we can use `format_exception`.

Thanks!

> There's another problem with `ExceptionGroup` - we only fail the workflow on `FailureError` based exceptions, all others fail the task (which means suspend the workflow until a code fix is deployed to fix the unexpected error). `ActivityError` is a `FailureError` so if raised out of the workflow that will fail the workflow, but `ExceptionGroup` is not that type so it will be treated differently. So really, you'll probably need to catch the exception and re-raise as `ApplicationError` anyways, and it's at this time you can log if needed.

Yup, that's what I was expecting. I don't want the `ExceptionGroup` to fail the workflow, I'd just like to be able to see the sub-exceptions' traceback in the UI  and  try to possibly fix the workflow.


</details>

<details>
<summary><strong>cretz</strong> commented on 2023-10-06 22:17:22.000 UTC</summary>

> Yup, that's what I was expecting. I don't want the ExceptionGroup to fail the workflow, I'd just like to be able to see the sub-exceptions' traceback in the UI and try to possibly fix the workflow.

We should be logging it on the worker as a task failure (with log line starting with "Failed handling activation on workflow with run ID"). I cannot currently think of a good way to display multiple aggregate exceptions in our model today that would work well. Also the serialized failure has some size limits that could be more easily reached on aggregate exceptions with stack traces. I think for this case, the UI is not going to be better than logs for retrieving this level of detail.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-10-20 12:40:06.000 UTC</summary>

Closing as there is nothing actionable to do here. Exception group is not serializable to a Temporal failure which is a single message w/ a single stack trace and a single cause.

</details>


---

<a id="235"></a>

### #235: [Feature Request] Raise exception for payloads that violate gRPC message max size

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/235 |
| **State** | OPEN |
| **Author** | nathanielobrown (Nathaniel Brown) |
| **Created** | 2022-12-30 17:41:06.000 UTC (3 years ago) |
| **Updated** | 2025-07-07 19:37:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.
I ran into an issue where my code tried to send a payload that was larger than the 4MB supported by gRPC. The issue is that it wasn't so obvious that this was the issue. I only figured it out after reviewing my logs and seeing the following line:

```
WARNÂ temporal_client::retry:Â gRPCÂ callÂ respond_workflow_task_completedÂ retriedÂ 5Â timesÂ error=StatusÂ {Â code:Â ResourceExhausted,Â message:Â "grpc:Â receivedÂ messageÂ largerÂ thanÂ maxÂ (10248653Â vs.Â 4194304)",Â metadata:Â MetadataMapÂ {Â headers:Â {"content-type":Â "application/grpc"}Â },Â source:Â NoneÂ }
```

For some reason this error did not raise an exception (which I would have seen in Sentry) and in the Temporal UI all I would see were timeouts. This made it take longer to understand the root issue.

### Describe the solution you'd like

I think the default data converter could check for this case and raise a clear exception to help developers quickly understand the issue. Alternatively, maybe some error from gRPC could be propagated better, IDK about this but I'm sure @cretz will know best!

Here's what I added to my custom JSON encoder to add this behaviour for myself:
```python
from typing import Any, Optional, Type

import orjson
import temporalio.api.common.v1
from pydantic.json import pydantic_encoder
from temporalio.converter import (
    JSONPlainPayloadConverter as BaseJSONPlainPayloadConverter,
)


MAX_PAYLOAD_SIZE = 4_194_304


class JSONPlainPayloadConverter(BaseJSONPlainPayloadConverter):
    def to_payload(self, value: Any) -> Optional[temporalio.api.common.v1.Payload]:
        serialized_valued = orjson.dumps(value, default=pydantic_encoder)
        if len(serialized_valued) > MAX_PAYLOAD_SIZE:
            raise RuntimeError(
                f"Payload size {len(serialized_valued):,d} "
                + f"exceeds max size {MAX_PAYLOAD_SIZE}"
            )
        return temporalio.api.common.v1.Payload(
            metadata={"encoding": self._encoding.encode()},
            data=serialized_valued,
        )
```


#### Comments (6)

<details>
<summary><strong>cretz</strong> commented on 2022-12-30 17:57:41.000 UTC</summary>

But it's not just single payload that can cause this. What if you have two payloads that are each 2.5MB in the same gRPC call (very possible just calling two activities)? Or other things like string names or lots of little memo values that make it over 4MB? Also, I am guessing this is a server side error and therefore the server may be able to configure this (though we may not expose such a knob by default). Also, your solution doesn't help people using proto payloads or byte array payloads.

If the only purpose is to change English words in your specific case of single-payload-too-large from "grpc:Â receivedÂ messageÂ largerÂ thanÂ max" to "payload size exceeds max size" what you have is acceptable. But I am not sure we need to put this in the SDK if this is a server side error and can come about via many ways the JSON-payload-converter check you have won't catch.

> Alternatively, maybe some error from gRPC could be propagated better,

How could the propagation improve? IIUC you're just changing the string message and stack trace, you're still gonna fail the workflow task and have to look at logs to see what happened.

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-12-30 18:46:17.000 UTC</summary>

@cretz, I definitely don't know the best way to do this and I agree that the solution I used is not a "final" solution that would work for the SDK. I do feel like something could be improved here, but maybe I am wrong. I'll just try to detail what I'm experiencing.

The log message comes from the worker who is executing the workflow, although I would gather that it is just logging an error returned by the server.

What's important to me is not that we change the message in some way, but that we clearly fail whatever task we were trying to perform rather than retrying and timing out. The timeout means we have to embark on an investigation and cross-reference with logs, while an exception would make things more clear.

Maybe when a client talking to gRPC gets this specific error it can raise an exception rather than retrying and eventually timing out silently? If we receive an error response like this I would think a retry will never help fix the issue. I realize there are probably similar gRPC errors that should be retried and it might be a little in-the-weeds to try to isolate this (and potentially other unfixable errors) as a case for special handling, but I also think that I will not be the last one who runs across this issue and others may experience the same confusion.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-12-30 19:20:30.000 UTC</summary>

> but that we clearly fail whatever task we were trying to perform rather than retrying and timing out

Ah, I see now. So when we make gRPC calls, we take some failure statuses to mean that there is a temporary issue and we should retry the call. Other failure statuses we treat as a problem with the workflow and fail the task (which is just like throwing from a workflow like you do here). `ResourceExhausted` is seen as a transient error even though it's not in this case. I will discuss with team to see if there may be a better way here. I think this is an issue in all SDKs today.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-01-13 14:35:07.000 UTC</summary>

This is pending https://github.com/temporalio/sdk-core/issues/462

</details>

<details>
<summary><strong>GSmithApps</strong> commented on 2025-07-07 17:37:48.000 UTC</summary>

@cretz should this be closed?

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-07-07 19:37:39.000 UTC</summary>

Yes, though I wonder if we should first add a test or at least have some manual confirmation that too-large gRPC message does indeed bubble out and is not implicitly retried.

Reactions: ðŸ‘ 1

</details>


---

<a id="181"></a>

### #181: [Feature Request] Review options for allowing protobuf 3.x to work with this library

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/181 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-01 22:43:55.000 UTC (3y 2m ago) |
| **Updated** | 2022-12-01 20:39:29.000 UTC |
| **Closed** | 2022-11-29 13:06:49.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 4 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

Many libraries have not upgraded to 4.x while many have (including us). Depending on libraries that haven't is causing a problem for users. Most libraries seem to know they need to move to protobuf 4.x but just haven't done it.

### Describe the solution you'd like

Consider options for supporting both. This could likely take the form of:

* Generating proto 3.x form in addition to proto 4.x form
* Changing generation script to have the `__all__` from the `__init__` of the proto packages switch to importing the 3.x generated code if determined to be on 3.x
* Adding a CI step that confirms proto 3 works with tests and is generated properly
* Experiment with ways to have 4.x be the default depended on version but that 3.x works. This may prove to be a bit difficult (can we have an "extra" that downgrades? can we ask users to force downgrade/shade?). IIRC Poetry doesn't support transitive overrides for example.
  * I don't think we want 4.x to be an optional extra, I think we want it default because of a bug in 3.x w/ C static state sharing across imports which harms our sandbox.

#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2022-11-15 19:20:39.000 UTC</summary>

https://github.com/open-telemetry/opentelemetry-python/issues/2880#issuecomment-1314293057 mentions that we may be able to just generate with proto 3.20+. Need to try this.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-28 13:57:01.000 UTC</summary>

This is complicated by the fact that we use https://pypi.org/project/grpcio-tools/ to generate our protobufs and the last version that supports proto 3.x (https://pypi.org/project/grpcio-tools/1.48.2/) doesn't have a Python 3.11 build. So code can't even be generated on latest Python when older proto library is in use.

</details>

<details>
<summary><strong>schibrikov</strong> commented on 2022-12-01 20:15:26.000 UTC</summary>

Thanks a lot for working on this @cretz !

Do you have any idea on when this might be released?

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-12-01 20:36:45.000 UTC</summary>

@schibrikov - I do not, but hopefully soon. Beta 4 should be released soon (but no exact date)

Reactions: ðŸš€ 1

</details>


---

<a id="134"></a>

### #134: [Feature Request] Extend readme contents about exception from activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/134 |
| **State** | CLOSED |
| **Author** | KMilhan (Milhan) |
| **Created** | 2022-09-16 04:57:20.000 UTC (3y 3m ago) |
| **Updated** | 2024-10-30 06:19:36.000 UTC |
| **Closed** | 2022-09-21 12:43:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

In [here](https://github.com/temporalio/sdk-python#exceptions), the project describes how the exception from workflows are handled.

Recently, I discovered many exceptions (such as `httpx` timeout) from activity create start_to_close timeout. I wish we have better documentation about the exception from activity side

### Describe the solution you'd like

Additional description about exceptions from the activity (worker) side. Which exception can we raise to mark it failed activity run and retry? what are not allowed exceptions?


#### Comments (6)

<details>
<summary><strong>cretz</strong> commented on 2022-09-16 12:49:11.000 UTC</summary>

Can you clarify the suggestion a bit?

> I discovered many exceptions (such as httpx timeout) from activity create start_to_close timeout

All exceptions from activities are treated equally (except for some special exceptions concerning cancellation and complete async). Not sure what you mean by "create start to close timeout" but the relation of timeouts/retries to errors are explained at https://docs.temporal.io/activities.

> Which exception can we raise to mark it failed activity run and retry?

All

> What are not allowed exceptions?

All exceptions are allowed. Can you clarify the question?

(also feel free to contact us on Slack at https://temporal.io/slack or the forum at https://community.temporal.io to get quicker answers)

</details>

<details>
<summary><strong>KMilhan</strong> commented on 2022-09-19 23:53:21.000 UTC</summary>

Let me work on my side if I can reproduce the behavior I saw the last couple of weeks.

> All exceptions are allowed. Can you clarify the question?

In the current README,

it says

> Exceptions
> Workflows can raise exceptions to fail the workflow
> Using temporalio.exceptions.ApplicationError, exceptions can be marked as non-retryable or include details

I wanted to know what exceptions we can raise to mark the activity `failed` because I've seen activities left "stale" with the exception cases and Workflow has `ApplicationError` to officially mark it as failed workflow. Can we use this exception in activity too? or any exceptions are treated as failures? For activities, what's the exception that marks it "retryable" and what's the one that marks it "non-retryable"?

So I thought maybe an exception explanation for the activities is "missing" by mistake. So I created this ticket. I know the readme I quote above is under the workflow section, but activities section does not have exception subsection.

Also, thank you for giving me the reference to the slack and community channel. I'll write questions on those channels.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-09-20 13:07:36.000 UTC</summary>

> because I've seen activities left "stale" with the exception cases

There is no such thing as a "stale" activity. Are you sure it's not continually retrying? What does the UI show?

> Can we use this exception in activity too?

Yes, you can use the exception in activities too (it's good for passing details, setting type, and marking exceptions as not retryable)

> or any exceptions are treated as failures?

For activities, they are

> For activities, what's the exception that marks it "retryable" and what's the one that marks it "non-retryable"?

All are retryable by default except for 1) if you create `ApplicationError` with `non_retryable=True`, 2) the `retry_policy` set when you invoke the activity has a `non_retryable_error_types` that includes the unqualified name of the exception class (or application error `type`), or 3) the retry policy permits no more retries (i.e. max attempts has been reached; set it to 1 to disable retries).

> So I thought maybe an exception explanation for the activities is "missing" by mistake

:+1: I can make this section. Note, the README is not the canonical source of documentation, it's just there while the https://docs.temporal.io/application-development/ gets up to speed. The concept of activity failures/errors is not Python specific (though there are of course Python-specific parts). I will also submit a request to improve https://docs.temporal.io/application-development/foundations?lang=python#activity-execution and https://docs.temporal.io/activities/#activity-execution with regards to activity exceptions.

</details>

<details>
<summary><strong>KMilhan</strong> commented on 2022-09-21 02:32:24.000 UTC</summary>

Thank you very much @cretz. In the UI, we had numerous activities reaching the max timeouts when the code raised an error (based on log), which looked like it didn't catch the exception as a failure but lost the activity worker.

I tried to reproduce the error but it seems like I can't with the minimal configuration. Seems like the issue was actually on our codebase side not on temporalio sides.

Also appreciate your consideration of adding readme section and guiding me to the resources.

Please feel free to close this ticket. Many thanks!

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-09-21 12:43:22.000 UTC</summary>

:+1: I have communicated with documentation to document errors properly for workflows and activities.

</details>

<details>
<summary><strong>yitian-reevo</strong> commented on 2024-10-30 06:19:35.000 UTC</summary>

lol, a follow up question from year 2024, i think for latest version 1.8.0 (i may have an example), can you please take a look at this issue (https://github.com/temporalio/sdk-python/issues/679)? @cretz thanks!

I found no matter how I write, the activity are still being retried, no sure if it's our codebase problem.

</details>


---

<a id="1215"></a>

### #1215: [Bug] must only be set once: TaskLocals { event_loop: Py(0x1165b6120), context: Py(0x116ad1d40) }

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1215 |
| **State** | CLOSED |
| **Author** | jkk-playon (Joe Koberg) |
| **Created** | 2025-11-14 20:31:23.000 UTC (1 months ago) |
| **Updated** | 2025-11-14 23:58:33.000 UTC |
| **Closed** | 2025-11-14 23:45:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

First time quickstart, running python worker. Just trying to start it up with a trivial workflow.


### Describe the bug

Receive this message from the worker process, which does not exit.  Client hangs on connect:

```
thread '<unnamed>' panicked at src/worker.rs:527:14:
must only be set once: TaskLocals { event_loop: Py(0x1165b6120), context: Py(0x116ad1d40) }
stack backtrace:
   0:        0x123b2d6ac - <std::sys::backtrace::BacktraceLock::print::DisplayBacktrace as core::fmt::Display>::fmt::hf35f9734f9a29483
   1:        0x123865f54 - core::fmt::write::h60ec6633daab7b35
   2:        0x123b2d044 - std::io::Write::write_fmt::hc29709fdab2e34e2
   3:        0x123b2d56c - std::sys::backtrace::BacktraceLock::print::hca95bffd78053951
   4:        0x123b2cc4c - std::panicking::rust_panic_with_hook::h557a23724a5de839
   5:        0x123b537f4 - std::panicking::begin_panic_handler::{{closure}}::h269cace6208fef05
   6:        0x123b53764 - std::sys::backtrace::__rust_end_short_backtrace::h5be0da278f3aaec7
   7:        0x123b544c8 - __rustc[de2ca18b4c54d5b8]::rust_begin_unwind
   8:        0x1247f5d90 - core::panicking::panic_fmt::h477ff48eff31ffa4
   9:        0x1247f6180 - core::result::unwrap_failed::h61c3c2f1df5908ff
  10:        0x123850d54 - temporal_sdk_bridge::worker::WorkerRef::__pymethod_validate__::h0d5c2056c6b1c4b7
  11:        0x12381e470 - pyo3::impl_::trampoline::trampoline::hbce46ba09996ee98
  12:        0x123850008 - temporal_sdk_bridge::worker::<impl pyo3::impl_::pyclass::PyMethods<temporal_sdk_bridge::worker::WorkerRef> for pyo3::impl_::pyclass::PyClassImplCollector<temporal_sdk_bridge::worker::WorkerRef>>::py_methods::ITEMS::trampoline::h5af5b64b656159ef
  13:        0x1051b6f0c - __PyEval_EvalFrameDefault
  14:        0x1050a2a5c - _gen_send_ex2
  15:        0x10563adc0 - _task_step_impl
  16:        0x10563ab98 - _task_step
  17:        0x105080be8 - __PyObject_MakeTpCall
  18:        0x1051e42d4 - _context_run
  19:        0x1051ba144 - __PyEval_EvalFrameDefault
  20:        0x1051b3754 - _PyEval_EvalCode
  21:        0x10522f958 - _run_mod
  22:        0x10522e034 - _pyrun_file
  23:        0x10522d5b8 - __PyRun_SimpleFileObject
  24:        0x10522d21c - __PyRun_AnyFileObject
  25:        0x10525caf8 - _pymain_run_file_obj
  26:        0x10525c840 - _pymain_run_file
  27:        0x10525bd80 - _Py_RunMain
  28:        0x10525c218 - _pymain_main
  29:        0x10525c2b4 - _Py_BytesMain
```


### Minimal Reproduction


### Environment/Versions

Using uv, python3.14, non-sandboxed with snowflake and kafka connectors spun up.
uname: `Darwin ip-192-168-1-131.ec2.internal 24.6.0 Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:30 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6020 arm64`

"temporalio>=1.18.2"

server running with `temporal server start-dev`

temporal installed via brew, latest version.



#### Comments (5)

<details>
<summary><strong>jkk-playon</strong> commented on 2025-11-14 20:45:19.000 UTC</summary>

Minimal reproduction:

```python
import asyncio
from dataclasses import dataclass

from temporalio.client import Client as TemporalClient
from temporalio import activity, workflow

from temporalio.worker import Worker

@dataclass
class FileArrivalArgs:
    msg_key: str
    msg_body: str


@dataclass
class Activities:
    
    @activity.defn
    async def run_process(self, event: FileArrivalArgs) -> None:
        await asyncio.sleep(5)
    
@workflow.defn
class FileArrival:    
    @workflow.run
    async def run(self, arg: FileArrivalArgs):
        await asyncio.sleep(30)


async def run_temporal_worker() -> None:
    activities = Activities()
    async with Worker(
        await TemporalClient.connect("localhost:7233"),
        task_queue="default",
        activities=[activities.run_process],
        workflows=[FileArrival],
    ) as  temporal_worker:
        await temporal_worker.run()
                    


if __name__ == "__main__":
    asyncio.run(run_temporal_worker())

```

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-11-14 20:59:39.000 UTC</summary>

That could use a better error condition, but I think the problem is that you are running the worker twice. `async with Worker` already runs the worker for the duration of that context. Using `run` again tries to redo things and results in the failure.

</details>

<details>
<summary><strong>jkk-playon</strong> commented on 2025-11-14 23:45:44.000 UTC</summary>

Yes, that seemed to fix it.  But, I wonder what I'm expected to be doing once the worker has started its background task and yielded - sleep? `await temporal_worker._shutdown_complete_event.wait()` ?  I do want to use the context manager pattern to ensure resource safety.

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-11-14 23:53:00.000 UTC</summary>

The context manager is really just for use when you have things you want to do with that worker. So you would just use it, execute workflows, etc. You could theoretically wait on some sort of application level cancellation depending on your scenario, but if you just want the process to be a perpetual worker, there's no reason to use the context manager.

</details>

<details>
<summary><strong>jkk-playon</strong> commented on 2025-11-14 23:56:50.000 UTC</summary>

Right - it never exits.  The thing is, when a service offers the context manager protocol, I assume there is some cleanup on exception unwind.  I use that pattern almost exclusively for any kind of long-lived stateful facility.

</details>


---

<a id="1136"></a>

### #1136: [Bug] Langfuse Tracing Not Working with Temporal OpenAI Agents Plugin

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1136 |
| **State** | OPEN |
| **Author** | databill86 |
| **Created** | 2025-09-30 15:25:56.000 UTC (3 months ago) |
| **Updated** | 2025-12-02 17:34:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Context:** This issue was separated from a [previous bug report](https://github.com/temporalio/sdk-python/issues/1134). @tconley1428 confirmed that the plugin approach is the only way to run agents [edit: Tim - Agents SDK agents] in Temporal workflows, but the tracing/observability issue remains unresolved and I think it deserves a separate issue.

### What are you really trying to do?

I'm building a multi-tenant application using Temporal workflows with OpenAI agents and need observability/tracing to work properly with Langfuse (though logfire, as explained [here](https://langfuse.com/integrations/frameworks/openai-agents)). I want to trace agent execution, tool calls, and model interactions through Langfuse and logfire.

### Describe the bug

When using Temporal workflows with OpenAI agents and the required `OpenAIAgentsPlugin`, the Langfuse tracing instrumentation fails to work properly. The tracing setup doesn't capture agent execution traces in Langfuse.

**Expected Behavior:**
- Agent execution should be traced and visible in Langfuse.

<img width="1638" height="288" alt="Image" src="https://github.com/user-attachments/assets/2fdfdaca-4d31-4ad3-8d58-b64f26245f9b" />

**Current Behavior:**
- No traces appear in Langfuse when using the plugin approach
- Agent execution happens but is not observable through Langfuse
- Tracing works fine when running agents outside of Temporal workflows

### Minimal Reproduction

**1. Tracing Setup (that works outside workflows):**
```python
from agents import set_trace_processors
import logfire
import nest_asyncio
import os

def init_tracing():
    """Initialize tracing and observability."""
    # Set Langfuse env vars from settings
    os.environ.setdefault("LANGFUSE_PUBLIC_KEY", LANGFUSE_PUBLIC_KEY)
    os.environ.setdefault("LANGFUSE_SECRET_KEY", LANGFUSE_SECRET_KEY)
    os.environ.setdefault("LANGFUSE_HOST", LANGFUSE_HOST)

    set_trace_processors([])  # only disable OpenAI tracing

    # Instrument OpenAI Agents SDK via pydantic-ai logfire
    try:
        nest_asyncio.apply()
        logfire.configure(service_name="temporal-demo", send_to_logfire=False)
        # This method automatically patches the OpenAI Agents SDK to send logs via OTLP to Langfuse.
        logfire.instrument_openai_agents()
    except Exception as exc:
        logger.error(f"Logfire instrumentation not available: {exc}")
```

**2. Worker Setup with Plugin (required for Temporal workflows):**
```python
from temporalio.worker import Worker
from temporalio.contrib.openai_agents import OpenAIAgentsPlugin, ModelActivityParameters
from datetime import timedelta

async def main():
    # Initialize tracing (conflicts with plugins)
    init_tracing()  # Sets up logfire â†’ OTLP â†’ Langfuse
    
    # Create Temporal client with plugins (required for agents)
    plugins = [
        OpenAIAgentsPlugin(
            model_params=ModelActivityParameters(
                start_to_close_timeout=timedelta(seconds=30)
            ),
            model_provider=CustomLitellmProvider(
                base_url=PROXY_BASE_URL,
                api_key=PROXY_API_KEY,
            ),
        ),
    ]
    
    client = await create_temporal_client(include_plugins=True)
    
    # Run the worker
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as activity_executor:
        worker = Worker(
            client,
            task_queue="demo-task-queue",
            workflows=[MyWorkflow],
            activities=[simple_tool_activity],
            activity_executor=activity_executor,
        )
        await worker.run()
```

**3. Workflow with Agent:**
```python
from temporalio import workflow
from agents import Agent

@workflow.defn
class MyWorkflow:
    @workflow.run
    async def run(self) -> str:
        # Create agent with string model (required with plugins)
        agent = Agent(
            name="Triage Agent",
            instructions="Your instructions here",
            model="gpt-4o-mini",  # String model name required with plugins
            tools=tools,
        )
        
        # This executes but doesn't appear in Langfuse traces
        result = await agent.run("Your message here")
        return result
```

**4. Custom Model Provider (for proxy configuration):**

```python
from agents.extensions.models.litellm_model import LitellmModel
from agents.models.interface import Model, ModelProvider

class CustomLitellmProvider(ModelProvider):
    """Custom ModelProvider that uses LiteLLM with configurable base_url and api_key."""

    def __init__(self, base_url: str | None = None, api_key: str | None = None):
        self.base_url = base_url
        self.api_key = api_key

    @property
    def model_class(self) -> type[Model]:
        return LitellmModel

    @property
    def provider_name(self) -> str:
        return "CustomLitellmProvider"

    def get_model(self, model_name: str) -> Model:
        return LitellmModel(
            model=model_name,
            base_url=self.base_url,
            api_key=self.api_key,
        )
```

### Environment/Versions

- OS and processor: Linux
- Temporal Version: temporalio==1.18.0
- OpenAI SDK: openai==1.109.0
- OpenAI Agents: openai-agents==0.3.2
- Python: 3.11
- Langfuse: Latest version
- logfire: Latest version
- Are you using Docker or Kubernetes or building Temporal from source? Using Docker


**Current Behavior:**
No traces appear in Langfuse at all when using the plugin approach.


Thanks ðŸ™


#### Comments (5)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-30 15:39:35.000 UTC</summary>

Can you clarify, the link you provided doesn't mention logfire at all.

</details>

<details>
<summary><strong>databill86</strong> commented on 2025-09-30 16:04:19.000 UTC</summary>

Sorry about that, I actually had the link saved in a readme file and copy pasted without checking. The tutorial has been updated, and it no longer uses [logfire instrument](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents). 

Instead of `logfire.instrument_openai_agents()`, they now recommend:
```python
from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor
OpenAIAgentsInstrumentor().instrument()
```

I tested this, and it worked! I can see the traces in Langfuse (screenshot attached).

<img width="1896" height="831" alt="Image" src="https://github.com/user-attachments/assets/85ec8346-9af8-463d-9c5c-efa1956af564" />

However, I have the following errors in the logs:

```
ERROR:opentelemetry.context:Failed to detach context
Traceback (most recent call last):
  File "/lib/python3.11/site-packages/opentelemetry/context/__init__.py", line 155, in detach
    _RUNTIME_CONTEXT.detach(token)
  File "/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py", line 53, in detach
    self._current_context.reset(token)
ValueError: <Token var=<ContextVar name='current_context' default={} at 0x79c8f92d40e0> at 0x79c8c81a70c0> was created in a different Context

2025-09-30 17:53:03.498 | INFO     | activities.simple_tool_activity:tool_func:27 - âœ… Tool activity completed: Tool processed: 'Hello, this is a test message for the demo!' - This is a simple response from the tool activity!

ERROR:opentelemetry.context:Failed to detach context
Traceback (most recent call last):
  File "/lib/python3.11/site-packages/opentelemetry/context/__init__.py", line 155, in detach
    _RUNTIME_CONTEXT.detach(token)
  File "/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py", line 53, in detach
    self._current_context.reset(token)
ValueError: <Token var=<ContextVar name='current_context' default={} at 0x79c8f92d40e0> at 0x79c8b81d9b40> was created in a different Context
ERROR:opentelemetry.context:Failed to detach context
Traceback (most recent call last):
  File "/lib/python3.11/site-packages/opentelemetry/context/__init__.py", line 155, in detach
    _RUNTIME_CONTEXT.detach(token)
  File "/lib/python3.11/site-packages/opentelemetry/context/contextvars_context.py", line 53, in detach
    self._current_context.reset(token)
ValueError: <Token var=<ContextVar name='current_context' default={} at 0x79c8f92d40e0> at 0x79c8c81a47c0> was created in a different Context
```
Should I be concerned about these context detachment errors?



</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-30 16:17:27.000 UTC</summary>

We have an open report of that which has been around for a while. https://github.com/temporalio/sdk-python/issues/441
If you don't see other issues with the run, it is probably okay to carry on, but it is something that is bubbling up for us to prioritize investigating. 

</details>

<details>
<summary><strong>databill86</strong> commented on 2025-10-01 08:37:28.000 UTC</summary>


Thanks for the quick response.

**Tracing is absolutely critical for us**, we can't avoid it. The specific tracing tool doesn't matter as long as it works reliably with Temporal. Langfuse is probably the most widely used, so if there's something that works well with that, it would be cool.

To summarize, I've done a lot of testing with different tracing approaches and here's what I found:

1. Using [logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents) with **`logfire.instrument_openai_agents()`** 
   - âŒ **Doesn't work with Temporal (when using plugins in the client definition)** - nothing gets traced, no errors
   - âœ… **Works without Temporal plugins** - traces perfectly when agent runs in a regular activity outside workflow

2. Using [logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents) with **`logfire.instrument_openai()`** (global OpenAI client instrumentation)
   - âš ï¸ **Partially works with Temporal (when using plugins in the client definition)** - traces metadata but input/output are null, also we have multiple observations in different langfuse traces for the same agent run, without any errors.

3. Using [openinference](https://langfuse.com/integrations/frameworks/openai-agents) **`OpenAIAgentsInstrumentor().instrument()`** 
`from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor
OpenAIAgentsInstrumentor().instrument()`
   - âš ï¸ **Works but with context detachment errors**
   - Traces content but throws context-related errors

Thanks again for looking into this! ðŸ™


</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-01 17:04:00.000 UTC</summary>

1. I looked into why logfire doesn't work. Their instrumentation overrides the TracingProvider, which is also what we do to enable tracing and these conflict. I found a way to combine the two, but that doesn't help, as logfire's implementation is not durable safe. 

2. Wouldn't be expected to work well really, since the agents instrumentor exists to better instrument the scenario, versus doing so directly through the openai client library.

3. We have the context detachment errors on the docket to look at.

Reactions: ðŸ‘ 1

</details>


---

<a id="1009"></a>

### #1009: [Feature Request] OpenAI Agents Streaming support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1009 |
| **State** | OPEN |
| **Author** | surki (Suresh Kumar Ponnusamy) |
| **Created** | 2025-07-29 11:50:24.000 UTC (5 months ago) |
| **Updated** | 2025-08-04 18:28:12.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Feature Request
1. Looks like run_streamed() is not supported. In a long running reasoning agent, we need to keep the chat updated with the progress. Any plan to support it?

### Answered Questions
2. What's the recommended way to integrate? the `OpenAIAgentsPlugin` or `set_open_ai_agent_temporal_overrides`? If plugin, any plan to cut a release with latest changes?
3. Is it safe to use single worker for both regular workflows/activities and agent workflows? concerned about plugin/override negatively interacting with other workflows.

#### Comments (5)

<details>
<summary><strong>tconley1428</strong> commented on 2025-07-29 14:00:07.000 UTC</summary>

1. We do plan to support streaming. It's on the roadmap.
2. `OpenAIAgentsPlugin` is intended to be the only usage method. It released last night: https://github.com/temporalio/sdk-python/releases/tag/1.15.0, with samples and readme updates in the works to align to that new model.
3. It is safe to use both. You can look at the plugin implementation which is quite small to identify the set of changes. The only one of particular concern is that other workflows will have to use the `pydantic_data_converter`. This is generally not a bad idea anyway since it is more robust than our built in one, but for some scenarios it could be problematic. For instance, adding it into an existing worker with histories may not work as the conversion process would differ.

I'll repurpose this issue specifically to track the streaming support request. In general for free form questions rather than a feature request, I would recommend using our community slack: https://temporalio.slack.com/archives/CTT84RS0P

</details>

<details>
<summary><strong>surki</strong> commented on 2025-07-31 05:25:15.000 UTC</summary>


Thanks @tconley1428 , I will keep an eye on this for streaming support, happy to test when it lands.

I upgraded and moved to use plugin, seems good so far, didn't notice any issues, will test some more. 
I didn't see any examples for doing pytest, seems `temporalio.testing.WorkflowEnvironment`  doesn't support plugins?

I seem to be using non-pydantic version for other workflows, so would run two workers for now.

Thanks for sharing community slack, will use that going forward for the questions.


</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-07-31 17:35:15.000 UTC</summary>

For pytest, you can see https://github.com/temporalio/sdk-python/blob/main/tests/contrib/openai_agents/test_openai.py. In our particular case the `WorkflowEnvironment` was already generated, so adding it there didn't come up, but that's something we can do.

</details>

<details>
<summary><strong>mav913ai</strong> commented on 2025-08-03 17:26:46.000 UTC</summary>

I am new to Temporal - just was trying to test it with OpenAI agents sdk, but couldn't since Runner.run_streamed is not supported. Has anyone figured out a workaround for this? 

We can't run the agent without streaming. 


</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-08-04 18:28:12.000 UTC</summary>

Streaming is not currently supported, as it is non-trivial to bring those updates back to the workflow from the activity in which the models are running. It is on the roadmap, but not currently supported. 

</details>


---

<a id="875"></a>

### #875: [Feature Request] Expose SDK metric for `worker._count_not_evict_count`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/875 |
| **State** | OPEN |
| **Author** | millerick (Michael Millerick) |
| **Created** | 2025-05-20 15:46:54.000 UTC (7 months ago) |
| **Updated** | 2025-05-27 19:32:29.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 3 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

We have found that periodically (for reasons that we still need to root cause) our workers run into a series of `Failed running eviction job for run ID 0196d798-a08b-7a00-9082-353865f449b4, continually retrying eviction. Since eviction could not be processed, this worker may not complete and the slot may remain forever used unless it eventually completes.` errors.  Then hours later when the pod containing the worker is terminated, we see this log: `Shutting down workflow worker, but 46 workflow(s) could not be evicted previously, so the shutdown may hang`.  For this particular worker, we run 50 concurrent workflows, which if I interpret things correct means that for several hours the worker was in an infinite loop trying to allow 46 workflows to evict and only able to process 4 workflow tasks at a time.

We would like to be able to detect and alert on these situations more proactively.  Usually we end up finding out about them because the worker set scales up to the maximum number of replicas for an extended period of time.

### Describe the solution you'd like

Since the code already keeps track of when it is in its own infinite loop trying to process the eviction, I think it would be useful to expose that information as a metric so that alerting tools can be used to alert when pods have been in that state for whatever the team monitoring the metric determines to be "too long".

### Additional context

If the team is bold enough, it could also be nice to do one or more of the following:
1. Provide a setting that forces the worker to shutdown if it has been in an eviction loop for too long.
2. Provide more threads than `max_concurrent_workflow_tasks` so that the ability to process workflows isn't as likely to be impeded by the infinite eviction loop.


#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2025-05-27 15:07:47.000 UTC</summary>

> for reasons that we still need to root cause

Hrmm, this is a fairly advanced/rare situation and should only occur if something is written improperly or something else unexpected is happening. We do prefer to expose these rare situations as logs instead of metrics. All SDK metrics are Core/Rust-based and we have no pure Python or Python-SDK-specific metrics.

Is it possible to use logs as the exposure mechanism? There are lots of other rare places where problems can occur that we log and do not treat as metrics or worker-shutdown-able situations. I will confer with team on their thoughts here for this specific situation.

I think you may be able to use `temporal_worker_task_slots_available` and `temporal_worker_task_slots_used` to determine if slots are never returned. Only kinda though, because metrics do not give the information that logs do.

</details>

<details>
<summary><strong>millerick</strong> commented on 2025-05-27 19:00:11.000 UTC</summary>

> I think you may be able to use `temporal_worker_task_slots_available` and `temporal_worker_task_slots_used` to determine if slots are never returned. Only kinda though, because metrics do not give the information that logs do.

Yes, using the `temporal_worker_task_slots_used` metric is less direct, but it may be usable for this purpose.  We have yet to run into a situation where all of the worker task slots are reported as used for legitimate reasons.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-05-27 19:32:01.000 UTC</summary>

:+1: After conferring, this may be a useful metric, but it may be a bit of a heavy lift for language-specific metrics in our Core-based SDKs for this type of metric. We will leave the issue open, but adding this metric may not be a priority for us at this time.

Reactions: ðŸ‘ 1 ðŸ˜• 1

</details>


---

<a id="855"></a>

### #855: [Bug] Helper execute activity doesn't work as expected

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/855 |
| **State** | CLOSED |
| **Author** | gerryfp (Gerry FP) |
| **Created** | 2025-05-07 04:11:33.000 UTC (8 months ago) |
| **Updated** | 2025-09-23 20:02:11.000 UTC |
| **Closed** | 2025-09-23 20:02:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm having issue with creating helper function to execute activity. My main goal is to have execute activities with the same configuration. Without helper function it works just fine, but after creating a helper function and called it inside workflow, it asks to provide task queue. I would expect the code to behave exactly the same way.

### Describe the bug
Code
```
async def execute_activity_base(
    temporal_activity,
    job_id,
    start_to_close_timeout=timedelta(hours=1),
):
    return await workflow.execute_activity(
        temporal_activity,
        job_id,
        start_to_close_timeout=start_to_close_timeout,
        retry_policy=RetryPolicy(
            initial_interval=timedelta(seconds=30),
            backoff_coefficient=2.0,
            maximum_attempts=3,
            maximum_interval=timedelta(seconds=10),
        ),
    )
```

Error message

`2025-05-06T03:01:36.718693Z  WARN temporal_sdk_core::worker::workflow: Error while completing workflow activation error=status: InvalidArgument, message: "invalid TaskQueue on ScheduleActivityTaskCommand: missing task queue name. ActivityId=3 ActivityType=estimated_generation", details: [], metadata: MetadataMap { headers: {"content-type": "application/grpc", "server": "temporal", "date": "Tue, 06 May 2025 03:01:36 GMT"} }`

### Minimal Reproduction

1. Instead of calling workflow.execute_activity in workflow call the helper method defined above. And the method is define in a different file.

### Environment/Versions
- Temporal Version: 1.9.0


#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2025-05-07 13:00:20.000 UTC</summary>

I am unable to replicate. For instance, the following code works as expected:

```python
@activity.defn
async def say_hello(name: str) -> str:
    return f"Hello, {name}!"

@workflow.defn
class ActivityHelperWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        return await self.my_execute_activity_helper(name)

    async def my_execute_activity_helper(self, name: str) -> str:
        return await workflow.execute_activity(
            say_hello,
            name,
            start_to_close_timeout=timedelta(seconds=30)
        )
```

Can you provide a full standalone replication with workflow and everything? Can just alter https://github.com/temporalio/samples-python/blob/main/hello/hello_activity.py to replicate your error if that helps.

</details>

<details>
<summary><strong>gerryfp</strong> commented on 2025-05-07 18:48:36.000 UTC</summary>

Here you go:

```py
from .helper import execute_activity_base

@activity.defn
async def say_hello(name: str) -> str:
    return f"Hello, {name}!"

@workflow.defn
class ActivityHelperWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        return await execute_activity_base(say_hello, name)
```

```py
# THIS IS helper.py
async def execute_activity_base(
    temporal_activity,
    name,
    start_to_close_timeout=timedelta(hours=1),
):
    return await workflow.execute_activity(
        say_hello,
        name,
        start_to_close_timeout=start_to_close_timeout,
        retry_policy=RetryPolicy(
            initial_interval=timedelta(seconds=30),
            backoff_coefficient=2.0,
            maximum_attempts=3,
            maximum_interval=timedelta(seconds=10),
        ),
    )
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-05-08 13:34:13.000 UTC</summary>

The code you pasted has two bugs. First, in `helper.py`, you have to replace `say_hello` with `temporal_activity`. Second, if you run this you get "Maximum interval cannot be less than initial interval" because the retry policy is invalid (can set to 30 like initial interval). 

I assume those two bugs aren't related to your problem though. When fixing those two bugs, I am unable to replicate anything concerning task queue, the code works just fine. If it still happens for you, we can open a draft PR on the samples repo with this as a test case to try to get it to fail CI tests to confirm it is not something unique to your environment.

</details>

<details>
<summary><strong>gerryfp</strong> commented on 2025-05-26 22:53:17.000 UTC</summary>

This is so weird, when I set initial_interval=timedelta(seconds=1), it works perfectly. But if I change it to initial_interval=timedelta(seconds=30), it throws an â€œinvalid TaskQueueâ€ error. There must be a bug somewhere internally.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-05-27 14:31:24.000 UTC</summary>

I am still not able to replicate even with that retry policy. Can you provide a full, standalone replication? If easiest, can open a draft PR to the samples-python repo, e.g. changing https://github.com/temporalio/samples-python/blob/main/hello/hello_activity.py to replicate what you are seeing.

</details>


---

<a id="823"></a>

### #823: [Feature Request] Disabling Deadlock detection in data converters

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/823 |
| **State** | CLOSED |
| **Author** | CtrlAltGiri (Giridhar Balachandran) |
| **Created** | 2025-04-10 05:24:36.000 UTC (9 months ago) |
| **Updated** | 2025-09-23 20:03:23.000 UTC |
| **Closed** | 2025-09-23 20:03:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Looking to get a workflow.disabledeadlockdetection context manager similar to other languages for data converters: 

https://github.com/temporalio/rules/blob/main/rules/TMPRL1101.md



#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2025-04-10 12:46:51.000 UTC</summary>

That feature is in Go and Java SDK because they don't support a clear separation of payload codecs and payload converters, but other SDKs do. Deadlock detection is already disabled for the payload codec part of the data converter (because it runs outside the sandbox), just not the payload converter part. This is by intention. You should not do any heavy/IO/async work inside a payload converter. Rather, do it in a payload codec. Sometimes a custom payload codec can be combined with a custom payload converter so the converter can set some info in payload metadata that the codec needs.

Does this help? If not, can you share your use case for why deadlock detection needs to be disabled for your payload converter instead?

</details>

<details>
<summary><strong>CtrlAltGiri</strong> commented on 2025-04-20 12:31:23.000 UTC</summary>

Got it. 

Thanks @cretz 

I've written a custom data converter which serializers generics and pydantic objects myself. This doesn't take more than 100-200ms at best, but for large payloads, I get this 2s deadlock error.

I've given my worker pod 1 CPU core, so it's definitely not a scheduling issue. 

Trying to understand if something else is the issue here.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-21 12:39:13.000 UTC</summary>

> This doesn't take more than 100-200ms at best, but for large payloads, I get this 2s deadlock error.

Yes, there may be something else causing deadlock detection. Deadlock detection just means a workflow task is not processing within 2s. This could mean Python is not able to process code quick enough, or something like a spinning loop or accidental thread blocking in the workflow. It could just be a simple case of too many things on one core to process workflow tasks fast enough.

</details>

<details>
<summary><strong>CtrlAltGiri</strong> commented on 2025-04-28 18:04:09.000 UTC</summary>

Thanks @cretz I'm moving my data converter code into the codec to ensure I have enough time to serialize.

One quick question before I close the issue: Can you point me to the logic that whitelists the codec from the sandbox? I tried digging in the sdk, but couldn't find anything specific.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-28 19:43:57.000 UTC</summary>

> Can you point me to the logic that whitelists the codec from the sandbox?

It's not whitelisted per se, it just runs before/after the sandbox. So we decode at https://github.com/temporalio/sdk-python/blob/e3603989f081bb812d883189be5f5ac9aa127089/temporalio/worker/_workflow.py#L251-L254, then we let the workflow task be processed (which is subject to deadlock detection and does payload converter stuff within it), then we encode it on the way out at https://github.com/temporalio/sdk-python/blob/e3603989f081bb812d883189be5f5ac9aa127089/temporalio/worker/_workflow.py#L337-L341

So the logic for codecs is on the boundaries, not within the task processing. It is of course still subject to task timeouts (default 10s) so we recommend trying to make it as fast as possible (e.g. if using a KMS for encryption keys, try to cache keys locally where it makes sense instead of downloading every time).

</details>


---

<a id="796"></a>

### #796: [Feature Request] Serialization context for codecs and converters

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/796 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-03-25 15:03:18.000 UTC (9 months ago) |
| **Updated** | 2025-09-30 20:14:31.000 UTC |
| **Closed** | 2025-09-30 20:14:31.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 1 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 2 |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/434

#### Comments (1)

<details>
<summary><strong>GSmithApps</strong> commented on 2025-06-23 14:47:49.000 UTC</summary>

~~ignore this -- just posting to follow this issue~~

</details>


---

<a id="777"></a>

### #777: [Feature Request] Allow custom metric buckets

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/777 |
| **State** | CLOSED |
| **Author** | TheCodeWrangler (Nathan Price) |
| **Created** | 2025-02-26 18:02:04.000 UTC (10 months ago) |
| **Updated** | 2025-04-21 15:43:44.000 UTC |
| **Closed** | 2025-04-21 15:43:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently the default binning for activity metrics are top out at 60 seconds.  This limits my observability into activities which might take a long time or even take more than 60 seconds on P99 metrics.

I would like to have a method of specifying a desired binning strategy on activity metrics.


#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2025-02-26 19:42:06.000 UTC</summary>

Yes, now that https://github.com/temporalio/sdk-core/pull/844 is available, we can do this here too. I have changed the title.

Reactions: ðŸš€ 1

</details>

<details>
<summary><strong>TheCodeWrangler</strong> commented on 2025-02-27 13:44:24.000 UTC</summary>

Any estimate on a timeline?  Is this something I could help speed along with a PR?

As best I can tell (though I am not very familiar with rust) the sdk-core binding accepts `histogram_bucket_overrides`

So possibly just python updates in `runtime.py` 

```python
@dataclass(frozen=True)
class PrometheusConfig:
    """Configuration for Prometheus metrics endpoint."""

    bind_address: str
    counters_total_suffix: bool = False
    unit_suffix: bool = False
    durations_as_seconds: bool = False
    histogram_bucket_overrides: dict[str, list[float]] | None = None # New property

    def _to_bridge_config(self) -> temporalio.bridge.runtime.PrometheusConfig:
        return temporalio.bridge.runtime.PrometheusConfig(
            bind_address=self.bind_address,
            counters_total_suffix=self.counters_total_suffix,
            unit_suffix=self.unit_suffix,
            durations_as_seconds=self.durations_as_seconds,
            histogram_bucket_overrides=self.histogram_bucket_overrides or {}, # New arg
        )
```

I am pretty naive on what might be involved here though.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-02-27 14:40:29.000 UTC</summary>

There is no estimate on timeline, but it is on the near-term backlog since it was recently implemented on our Core side. Yes what you have there is mostly correct and we definitely welcome PRs if you want to tackle it (will want a test to confirm it works for built-in and user-defined histograms too, which is usually just as simple as making an HTTP call to the Prometheus endpoint and checking contents).

</details>

<details>
<summary><strong>TheCodeWrangler</strong> commented on 2025-03-03 16:59:22.000 UTC</summary>

I took a stab in a PR.
https://github.com/temporalio/sdk-python/pull/781

This was branched off of 1.10.0 tag because I was not able to get `uv build` to succeed on the main branch, but was able to get the poetry build working on the tag.  This limited me from being able to compile and test cahnges on the main.

Was able to get a test to work and confirm that custom histogram binning worked for a custom metric BUT was not seeing it have an effect on the workflow end-to-end latency.  This has me a bit nervoous that the update to workflow or activity binning (which is all i actually care about at the moment) will require a change to the core-sdk.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-21 15:43:43.000 UTC</summary>

Done in #781, thanks @TheCodeWrangler!

</details>


---

<a id="641"></a>

### #641: [Bug] Sending a query that is not registered in the workflow results in an "Encoded failure" when encode_common_attributes=True

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/641 |
| **State** | OPEN |
| **Author** | antmendoza (Antonio Mendoza PÃ©rez) |
| **Created** | 2024-09-11 09:14:14.000 UTC (1y 4m ago) |
| **Updated** | 2024-12-05 17:54:26.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 3 |
| **Priority Score** | 5 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### What are you really trying to do?

List workflow queries using the UI (Queries tab) 

<img width="1132" alt="image" src="https://github.com/user-attachments/assets/5f644d8a-2ab1-4492-8e78-892e002ef0c8">

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

### Describe the bug
The UI shows "Encoded failure" but this is an error thrown by the worker when the UI tries to get the list of queries the workflow exposes with `queryType: "@@temporal-internal__list`

I get the same error when I use the client to send a query that is not registered in the workflow, and the client configuration has the `data_converter` which `failure_converter_class` has  `encode_common_attributes=True`


### Minimal Reproduction

> Full reproduction is here https://github.com/antmendoza/my-temporal-pocs/tree/main/python/_8156


clone https://github.com/temporalio/samples-python/tree/main/encryption 

Add the following modifications:
- Create a new DefaultFailureConverter with encode_common_attributes=True

```
class FailureConverterWithDecodedAttributes(temporalio.DefaultFailureConverter):
    def __init__(self) -> None:
        super().__init__(encode_common_attributes=True)
```


- Add `failure_converter_class=FailureConverterWithDecodedAttributes` to the client.
```
    client = await Client.connect(
        "localhost:7233",
        # Use the default converter, but change the codec
        data_converter=dataclasses.replace(
            temporalio.converter.default(),
            payload_codec=EncryptionCodec(),
            failure_converter_class=FailureConverterWithDecodedAttributes,
        ),
    )
```


After running a workflow, try to send a query that does not exist for the workflow.
```
        await client.get_workflow_handle("encryption-workflow-id").query("non-existing-query")
```

The query will fail with "Encoded failure"



#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2024-09-11 13:01:45.000 UTC</summary>

Hrmm, so this is a general Temporal Core SDK issue where query failures use the `Failure` structure but the server doesn't support that for query failures. I will confer with the team on this.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-10 21:22:29.000 UTC</summary>

I believe we have confirmed that this is due to the UI using query failures to populate the query list but encrypting these failures is incompatible with that. We have filed an issue with the UI team to support failure decryption.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-12-05 17:54:25.000 UTC</summary>

Note, there is a server side issue to fix query failure encoding: https://github.com/temporalio/temporal/issues/6845. If this becomes high enough priority, we can add SDK-side workarounds to keep query failures in plain text temporarily.

</details>


---

<a id="588"></a>

### #588: [Feature Request] Default Workflow\Activity Params In Declaration

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/588 |
| **State** | CLOSED |
| **Author** | ytaben (Yaroslav Taben) |
| **Created** | 2024-07-18 15:45:47.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-25 18:30:41.000 UTC |
| **Closed** | 2024-07-18 20:05:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.
Greetings, this is more of a 'hear your thoughts' kind of feature. If you believe the idea is viable for the SDK - we would also be happy to help with a contribution for it.

As of now - various activity and workflow parameters such as timeouts, task queue, retry policy and so on have to be specified at call sites. What we find in practice from experience - these parameters are _almost_ always tied to the definition\declaration of the workflow\activity itself.

For example - if an author makes a non-idempotent activity that should never be retried - it makes sense to specify maximum attempt to 1 in activity declaration. Same thing with heartbeating: if the author implemented heartbeating - the users will either forget to specify it at call site or copy paste the same heartbeat timeout everywhere. With regular timeouts - we just find people copy-pasting the same values or making helper functions like 'start_my_activity' that encapsulates those params.

### Describe the solution you'd like
The proposed solution is to allow these parameters to be specified in declarations.
For example:
```
@activity.defn(
  task_queue="urgent_important_task_queue",
  start_to_close_timeout=timedelta(minutes=10),
  heartbeat_timeout=timedelta(seconds=30),
  retry_policy=RetryPolicy(maximum_attempts=1)
)
def my_activity(...) -> ...:
  ...
```

In each relevant SDK method - such as `start_activity` or `execute_activity`, etc we would pick call-site value first and fall back to declaration if not given.

### Additional context

The intention would be to keep any existing behavior exactly the same.
There are some edge cases to think about such as having to use a sentinel value as default to allow explicitly setting `None`.

We are also happy to do this in our wrapper internally but wanted to share in case this makes the overall SDK experience better.


#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2024-07-18 16:49:28.000 UTC</summary>

Many users in all SDKs choose to set their activities at the definition site and often refer to them at call site (or use a helper method for activity invocation that retrieves them). This is very common and we encourage it. It is well known that for many/most, it makes more sense for the activity implementer to define their options than the caller.

However, the SDK is unable to make these general assumptions about which definition site is the actual one desired because the call site may be a completely different set of code (and options) from the definition site. Also, it may give the implementer a false sense of what the options are since the caller is the true definer of the options not the implementer (i.e. can override). The Temporal platform would have to be altered to support worker-side options which we may have in the future but not at this time.

So while we definitely encourage users to put their options where they fit with their code best, I fear at this time without Temporal platform support for declaration-side options, it is too confusing to pretend that it exists (especially since it may exist one day). The lack of this is common across all SDKs and we regularly encourage users to put their reusable options in code where they prefer.

</details>

<details>
<summary><strong>ytaben</strong> commented on 2024-07-18 20:05:31.000 UTC</summary>

That is fair reasoning.
I, as an end user, would be ok with understanding 'these are defaults but end users override them' but I can see how this extends to situations where it's not even the default. Someone can just call an activity as string from anywhere and the decorator is not at play at all.


I also do think that platform support for worker side options has value (i.e. non-idempotent activities for example) so we'll likely add these default options to our wrapper and if worker side options become a thing - adapt our wrapper then.

Thank you!

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-07-18 20:13:04.000 UTC</summary>

> I also do think that platform support for worker side options has value

We agree. There is a potential effort coming that will let you update the options via a client (i.e. externally from the workflow) for already started activities which technically allows activities to update their own options. The more explicit ahead-of-time options definition from a worker will be a bit harder and take longer because Temporal has no mechanism to even make the server aware an activity exists ahead of time much less some options it may have.

</details>

<details>
<summary><strong>ytaben</strong> commented on 2024-07-23 14:11:38.000 UTC</summary>

> Temporal has no mechanism to even make the server aware an activity exists ahead of time

Yeah, that's definitely a new thing with its own quirks (probably similar to task queue rate limit where last update from any worker wins).
We'll keep an eye for 'update options via a client' functionality cause that in theory allows _some_ degree of control from definition site (though not sure how it works for something like a timeout cause it doesn't make sense to change it once you're in flight)

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-07-25 18:30:39.000 UTC</summary>

https://github.com/temporalio/temporal/issues/6339 is a tracking issue for the server-side activity update concept

</details>


---

<a id="586"></a>

### #586: [Bug] workflow.start_child_workflow() behaving differently than workflow.execute_child_workflow() in tests

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/586 |
| **State** | OPEN |
| **Author** | cpg1111 (Christian Grabowski) |
| **Created** | 2024-07-18 00:23:24.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-18 13:47:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Hi there, I am writing tests for a workflow in which it spawns N child workflows using `workflow.start_child_workflow()`, places them in a list, and resolves the result later, like the following example:
```python
class Child:
    async def run(self) -> bool:
        return True

class Parent:
    async def run(self, n: int):
        children = []

        for _ in range(n):
            child = await workflow.start_child_workflow(Child, ...)
            children.append(child)

        ...
        for child in children:
            result = await child.result()
            # do something with result 
```

and test code like the following:
```python
async def test_parent():
    # setup test env worker with both workflows
    wf = await env.client.start_workflow("parent", ...)
    await env.sleep(duration=timedelta(seconds=5))
    await wf.result()
```

### Describe the bug

Even when I have that exact child workflow, I get an `InvalidStateError` saying `Result is not set`. If I do not put the handler in a list and immediately call `await child.result()` it blocks indefinitely. However if I use `workflow.execute_child_workflow()` everything works fine. It's also worth noting these workflows work fine in an actual execution, this issue only exists using the `testing.WorkflowEnvironment`.


### Minimal Reproduction

Write a workflow that calls multiple child workflows using `workflow.start_child_workflow()`, write a test with a single sleep before awaiting the parent's result.

### Environment/Versions

- Ubuntu 22.04 AMD EPYC 7352
- Temporal 1.22.5 and Python SDK 1.3.0
- from source

### Additional context

N/A


#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2024-07-18 13:33:11.000 UTC</summary>

> await child.result()

This is not valid API and a type checker should catch this. Starting a child workflow returns a `ChildWorkflowHandle` which is an `asyncio.Task` that is already waiting on result and should be awaited itself to get the result (e.g. just `await child`). This is unlike starting a workflow from a client which uses a workflow handle that can be fetched in other ways and therefore is not always a task and therefore does have a separate `.result()` to start the awaiting process.

</details>

<details>
<summary><strong>cpg1111</strong> commented on 2024-07-18 13:40:46.000 UTC</summary>

Ah ok, yes that fixed it. I didn't see anything around this in the docs regarding `ChildWorkflowHandle`, just that it inherits from `asycnio.Task`, I'd be happy to submit a PR including this, assuming the docs are autogenerated?

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-07-18 13:42:49.000 UTC</summary>

Unsure which docs you are referring to (there is https://github.com/temporalio/sdk-python/blob/main/README.md, https://python.temporal.io, and https://docs.temporal.io/develop/python/) but yes, we happily accept contributions on them all.

</details>

<details>
<summary><strong>cpg1111</strong> commented on 2024-07-18 13:45:24.000 UTC</summary>

I was referring to the second link, but I can submit PRs to the others where it makes sense.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-07-18 13:46:59.000 UTC</summary>

API docs make sense. That is auto generated from docstrings yes. PRs to this repo welcome to update those API docs.

Reactions: ðŸ‘ 1

</details>


---

<a id="568"></a>

### #568: [Bug] Fix flaky tests

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/568 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-07-01 14:56:20.000 UTC (1y 6m ago) |
| **Updated** | 2025-04-09 13:32:24.000 UTC |
| **Closed** | 2025-04-09 13:32:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

The test suite currently has multiple tests that sometimes fail. Let's collect examples in this ticket, and fix them.

#### Comments (5)

<details>
<summary><strong>dandavison</strong> commented on 2024-08-19 11:12:39.000 UTC</summary>

https://github.com/temporalio/sdk-python/actions/runs/10403635533/job/28817496590?pr=609

```
FAILED tests/worker/test_activity.py::test_sync_activity_process_non_picklable_heartbeat_details - assert "Can't pickle" in "AttributeError: Can't get local object 'picklable_activity_non_pickable_heartbeat_details.<locals>.<lambda>'"
```

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-01-23 14:32:44.000 UTC</summary>

https://github.com/temporalio/sdk-python/actions/runs/12930726515/job/36062930291?pr=739

```
poe test -s --workflow-environment time-skipping --junit-xml=junit-xml/3.13--macos-intel--time-skipping.xml
```

```
FAILED tests/worker/test_workflow.py::test_unfinished_handler_on_workflow_termination[-cancellation---no-wait-all-handlers-finish---not-dynamic---late-registered---signal-] - Failed: Timeout >600.0s
```

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-01-24 08:29:53.000 UTC</summary>

https://github.com/temporalio/sdk-python/actions/runs/12939859391/job/36092964653?pr=739

```
=========== 342 passed, 10 skipped, 9 warnings in 316.04s (0:05:16) ============
FATAL: exception not rethrown
Error: Process completed with exit code 250.
```

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-01-28 13:21:46.000 UTC</summary>

```
FAILED tests/worker/test_workflow.py::test_workflow_patch_memoized - Failed: Timeout >600.0s
```

https://github.com/temporalio/sdk-python/actions/runs/9745494374/job/26893565606?pr=566
https://github.com/temporalio/sdk-python/actions/runs/9745832979/attempts/1?pr=567

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-09 13:32:22.000 UTC</summary>

(accidentally reopened)

</details>


---

<a id="554"></a>

### #554: Document workflow-friendly concurrency control

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/554 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-06-18 17:14:16.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-30 16:21:47.000 UTC |
| **Closed** | 2024-07-30 16:21:47.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 3 |
| **Priority Score** | 5 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

Document and add tests confirming that `asyncio.Lock` and `asyncio.Semaphore` can be used to control interleaving of signal and update handlers and the main workflow coroutine.

#### Comments (3)

<details>
<summary><strong>sreeprasannar</strong> commented on 2024-07-29 17:23:58.000 UTC</summary>

does this mean currently using `asyncio.Lock` could lead to a deadlock? 

</details>

<details>
<summary><strong>dandavison</strong> commented on 2024-07-29 17:33:35.000 UTC</summary>

Hi @sreeprasannar

> does this mean currently using asyncio.Lock could lead to a deadlock?

No -- using `asyncio.Lock` in `sdk-python` workflow code is appropriate and recommended.

Thanks! I've changed the title & wording of this ticket.

</details>

<details>
<summary><strong>drewhoskins-temporal</strong> commented on 2024-07-30 16:21:47.000 UTC</summary>

https://github.com/temporalio/samples-python/pull/123
https://github.com/temporalio/documentation/pull/2959


</details>


---

<a id="540"></a>

### #540: [Bug] Exception in converter encoding doesn't propagate as expected

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/540 |
| **State** | CLOSED |
| **Author** | assafz-q |
| **Created** | 2024-06-04 13:01:37.000 UTC (1y 7m ago) |
| **Updated** | 2024-08-09 18:19:45.000 UTC |
| **Closed** | 2024-08-09 18:19:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I am writing a temporal workflow and I want to handle the exceptions that happen during the execution, I want the workflow to catch any unexpected error and fail the entire workflow in a non-retriable way.

### Describe the bug

I am catching all the exceptions during workflow run and re-raise them as non-retriable ApplicationErrors and everything works as expected except for the case where the exception is thrown during the encoding of data to an activity.

Running a workflow that sends unserializable obj to an activity results in the following history (which doesn't contain any failure in it except for the timeout [2s]):
![image](https://github.com/temporalio/sdk-python/assets/75679153/0e8bfaf6-d458-4ad5-bd93-f61e16f177c4)

and the following warning in the logs:
```python
WARNÂ temporal_sdk_core::worker::workflow:Â ErrorÂ whileÂ completingÂ workflowÂ activationÂ error=status:Â InvalidArgument,Â message:Â "invalidÂ TaskQueueÂ onÂ ScheduleActivityTaskCommand:Â missingÂ taskÂ queueÂ name.Â ActivityId=1Â ActivityType=some_activity",Â details:Â [],Â metadata:Â MetadataMapÂ {Â headers:Â {"content-type":Â "application/grpc"}Â }
```

I would expect the workflow to fail because of the exception (and log it in the history) before the timeout occurs, in the same way it happens if exception is thrown in the middle of the workflow execution.


### Minimal Reproduction
```python
â€‹importÂ asyncio
importÂ datetime
importÂ uuid

importÂ temporalio.activity
importÂ temporalio.client
importÂ temporalio.common
importÂ temporalio.exceptions
importÂ temporalio.worker
importÂ temporalio.workflow


####Â activities
classÂ SomeObj:Â  #Â nonÂ serializableÂ byÂ defaultÂ soÂ senfdingÂ itÂ toÂ activitiesÂ shouldÂ fail
 Â Â  defÂ __init__(self,Â value:Â str):
 Â Â Â Â Â Â Â self.valueÂ =Â value


@temporalio.activity.defn
asyncÂ defÂ some_activity(obj:Â SomeObj)Â ->Â str:
 Â Â  returnÂ obj.value


####Â workflow
@temporalio.workflow.defn
classÂ FailingWorkflowExceptionDuringActivitySend:
 Â Â Â @temporalio.workflow.run
 Â Â  asyncÂ defÂ run(self)Â ->Â str:
 Â Â Â Â Â Â  try:
 Â Â Â Â Â Â Â Â Â Â Â objÂ =Â SomeObj("value")
 Â Â Â Â Â Â Â Â Â Â Â valueÂ =Â awaitÂ temporalio.workflow.execute_activity(
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â some_activity,
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â obj,
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â start_to_close_timeout=datetime.timedelta(seconds=10),
 Â Â Â Â Â Â Â Â Â Â Â )
 Â Â Â Â Â Â Â Â Â Â  returnÂ value
 Â Â Â Â Â Â  exceptÂ ExceptionÂ asÂ e:
 Â Â Â Â Â Â Â Â Â Â  raiseÂ temporalio.exceptions.ApplicationError(
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â  "FailedÂ duringÂ activityÂ sendÂ asÂ expected",Â non_retryable=True
 Â Â Â Â Â Â Â Â Â Â Â )Â fromÂ e


####


asyncÂ defÂ _main():
 Â Â Â clientÂ =Â awaitÂ temporalio.client.Client.connect(
 Â Â Â Â Â Â Â target_host="localhost:7233",
 Â Â Â )

 Â Â Â task_queueÂ =Â "task-queue-name"
 Â Â Â workflowÂ =Â FailingWorkflowExceptionDuringActivitySend

 Â Â  asyncÂ withÂ temporalio.worker.Worker(
 Â Â Â Â Â Â Â client,
 Â Â Â Â Â Â Â task_queue=task_queue,
 Â Â Â Â Â Â Â workflows=[workflow],
 Â Â Â Â Â Â Â activities=[some_activity],
 Â Â Â ):
 Â Â Â Â Â Â  try:
 Â Â Â Â Â Â Â Â Â Â Â handleÂ =Â awaitÂ client.start_workflow(
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â workflow.run,
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â  id=f"{workflow.__name__}-{uuid.uuid4().hex}",
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â task_queue=task_queue,
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â run_timeout=datetime.timedelta(seconds=2),
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â execution_timeout=datetime.timedelta(seconds=2),
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â retry_policy=temporalio.common.RetryPolicy(
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â maximum_attempts=1,
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ),
 Â Â Â Â Â Â Â Â Â Â Â )

 Â Â Â Â Â Â Â Â Â Â Â resultÂ =Â awaitÂ handle.result()
 Â Â Â Â Â Â Â Â Â Â  print(f"Result:Â {result}")
 Â Â Â Â Â Â  exceptÂ ExceptionÂ asÂ e:
 Â Â Â Â Â Â Â Â Â Â  print(f"Workflow:Â {workflow.__name__}Â failed:{e.__cause__}")


ifÂ __name__Â ==Â "__main__":
 Â Â Â asyncio.run(_main())
```

### Environment/Versions

- OS and processor: Linux
- Temporal Version: sdk version: temporalio==1.6.0, temporal cli (`temporal --version`) `temporal version 0.12.0 (server 1.23.0) (ui 2.26.2)`
- Are you using Docker or Kubernetes or building Temporal from source? No


#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2024-06-04 13:04:53.000 UTC</summary>

> I would expect the workflow to fail because of the exception (and log it in the history) before the timeout occurs, in the same way it happens if exception is thrown in the middle of the workflow execution.

Common exceptions do not fail the workflow by default, they "suspend" it. This is the same way as if an exception is thrown in the middle of workflow execution. See https://github.com/temporalio/sdk-python?tab=readme-ov-file#exceptions. You can customize this behavior and/or customize the converter to throw certain exceptions.

</details>

<details>
<summary><strong>assafz-q</strong> commented on 2024-06-04 13:21:08.000 UTC</summary>

> > I would expect the workflow to fail because of the exception (and log it in the history) before the timeout occurs, in the same way it happens if exception is thrown in the middle of the workflow execution.
> 
> Common exceptions do not fail the workflow by default, they "suspend" it. This is the same way as if an exception is thrown in the middle of workflow execution. See https://github.com/temporalio/sdk-python?tab=readme-ov-file#exceptions. You can customize this behavior and/or customize the converter to throw certain exceptions.

I know, that is why I catch the exception and raise it again as non-retriable `ApplicationError`.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-06-04 13:30:27.000 UTC</summary>

Right, so the exception raised by the data converter is similar to raising the exception in the workflow, but it occurs on a different path (it occurs in the payload converter not the workflow code). It's a bit advanced, but you can customize the converter the same way you customize the workflow to catch and re-raise in a certain way. Alternatively to both, you can just put `failure_exception_types=[Exception]` or similar in the `@workflow.defn` and it should fail the workflow with any exception instead of failing the task (i.e. suspending pending code fix).

</details>

<details>
<summary><strong>assafz-q</strong> commented on 2024-06-04 14:46:47.000 UTC</summary>

> Right, so the exception raised by the data converter is similar to raising the exception in the workflow, but it occurs on a different path (it occurs in the payload converter not the workflow code). It's a bit advanced, but you can customize the converter the same way you customize the workflow to catch and re-raise in a certain way. Alternatively to both, you can just put `failure_exception_types=[Exception]` or similar in the `@workflow.defn` and it should fail the workflow with any exception instead of failing the task (i.e. suspending pending code fix).

Thanks!
But I am able to catch the exception in the try except of the workflow so I think everything is as I expected.
I think the problem is related to the warning log I attached to the issue:
```python
WARNÂ temporal_sdk_core::worker::workflow:Â ErrorÂ whileÂ completingÂ workflowÂ activationÂ error=status:Â InvalidArgument,Â message:Â "invalidÂ TaskQueueÂ onÂ ScheduleActivityTaskCommand:Â missingÂ taskÂ queueÂ name.Â ActivityId=1Â ActivityType=some_activity",Â details:Â [],Â metadata:Â MetadataMapÂ {Â headers:Â {"content-type":Â "application/grpc"}Â }
```

I was able to (maybe) fix that locally if in (`temporalio.worker._workflow_instance._ActivityHandle._apply_schedule_command`)
https://github.com/temporalio/sdk-python/blob/365ceadb0732f36b66bf1ae2970131fc7944b565/temporalio/worker/_workflow_instance.py#L2145-L2215
I moved the part that handles the payloads:
https://github.com/temporalio/sdk-python/blob/365ceadb0732f36b66bf1ae2970131fc7944b565/temporalio/worker/_workflow_instance.py#L2167-L2170
to the end of the function (_apply_schedule_command)

I am not too familiar with the code, but it seems like in that context the queue name is only initialized after the payload conversion, so if I had an exception during the payload conversion it could not be sent to the temporal server.

Reactions: ðŸ‘ 2

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-06-27 13:47:07.000 UTC</summary>

> I was able to (maybe) fix that locally if in (`temporalio.worker._workflow_instance._ActivityHandle._apply_schedule_command`) to the end of the function (`_apply_schedule_command`)

I see the issue. The issue is that we create the command before we try to serialize the contents. So if this does not fail the workflow task, the command is sent off incomplete (because it threw an exception during building). I have opened https://github.com/temporalio/sdk-python/issues/564 to track (it is easier as a separate issue to state the problem clearly), but will keep this issue open.

Reactions: ðŸ‘ 2

</details>


---

<a id="476"></a>

### #476: [Bug] Failed during multiprocess queue poll for heartbeat

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/476 |
| **State** | OPEN |
| **Author** | wilsoniya (Michael Wilson) |
| **Created** | 2024-02-22 20:33:09.000 UTC (1y 10m ago) |
| **Updated** | 2024-03-28 12:42:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Operate a heavily-loaded multiprocess worker. 


### Describe the bug

In a heavily-loaded multiprocess worker with activities that may possibly be issuing heartbeats too frequently, I observe logging indicating the parent process times out reading from the  `_MultiprocessingSharedStateManager._heartbeat_queue` thus the worker tree effectively hangs, i.e., exhibits no further liveness in its ability to process tasks.  If this situation is unrecoverable, I would expect the worker process tree to terminate, thus allowing a restart (i.e., via systemd, k8s policy, etc.).

It's not clear to me why this timeout is happening, but it also results in the heartbeat queue filling up and blocking further progress on activities which heartbeat.

I see the following logging. First, presumably logs emitted by the parent of the process tree (i.e., the reader of the heartbeat queue): 

```json
{
  "git": "0bd697f642d775670c808da1e216c44764ddb854",
  "version": "1.2.0",
  "event": "Failed during multiprocess queue poll for heartbeat",
  "timestamp": "2024-02-13T19:33:27.414756Z",
  "logger": "temporalio.worker._activity",
  "level": "error",
  "exception": [
    {
      "exc_type": "TimeoutError",
      "exc_value": "",
      "syntax_error": null,
      "is_cause": false,
      "frames": [
        {
          "filename": "/opt/DPS/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py",
          "lineno": 905,
          "name": "_heartbeat_processor",
          "line": "",
          "locals": {
            "self": "'<temporalio.worker._activity._MultiprocessingSharedStateManager object at 0x7f23'+9",
            "item": "\"(b'\\\\n$bc8c0612-11a9-4a58-ab6b-43d37f064f56\\\\x12\\\\x18replicate-treenode-20060\\\\x1a$8\"+115",
            "completion": "'<function _MultiprocessingSharedStateManager.unregister_heartbeater.<locals>.<la'+24",
            "fn": "'<function _ActivityInboundImpl.execute_activity.<locals>.<lambda> at 0x7f23412cb'+4"
          }
        },
        {
          "filename": "/opt/DPS/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py",
          "lineno": 644,
          "name": "<lambda>",
          "line": "",
          "locals": {
            "details": "()",
            "heartbeat_with_context": "'<function _ActivityInboundImpl.execute_activity.<locals>.heartbeat_with_context '+18",
            "loop": "<_UnixSelectorEventLoop running=True closed=False debug=False>"
          }
        },
        {
          "filename": "/usr/lib/python3.8/concurrent/futures/_base.py",
          "lineno": 446,
          "name": "result",
          "line": "",
          "locals": {
            "self": "None",
            "timeout": "10"
          }
        }
      ]
    }
  ]
}
```
And second, from the a child in the process tree, as it attempts to push heartbeats to the queue:

```json
{
  "git": "0bd697f642d775670c808da1e216c44764ddb854",
  "version": "1.2.0",
  "event": "ProgressPipe callback raised exception",
  "timestamp": "2024-02-08T21:26:47.729792Z",
  "logger": "vault.storage_manager.stream",
  "level": "error",
  "exception": [
    {
      "exc_type": "Full",
      "exc_value": "",
      "syntax_error": null,
      "is_cause": false,
      "frames": [
        {
          "filename": "/opt/DPS/vault-site/vault/storage_manager/stream.py",
          "lineno": 250,
          "name": "_invoke_callback",
          "line": "",
          "locals": {
            "self": "<vault.storage_manager.stream.ProgressPipe object at 0x7f78d9ab2eb0>",
            "e": "Full()"
          }
        },
        {
          "filename": "/opt/DPS/vault-site/vault/temporal/replication/activities.py",
          "lineno": 225,
          "name": "<lambda>",
          "line": "",
          "locals": {
            "n": "1960837120",
            "tree_node": "<TreeNode: TreeNode object (278873)>"
          }
        },
        {
          "filename": "/opt/DPS/venv/lib/python3.8/site-packages/temporalio/activity.py",
          "lineno": 274,
          "name": "heartbeat",
          "line": "",
          "locals": {
            "details": "(1960837120, 2077125573)",
            "heartbeat_fn": "<function _execute_sync_activity.<locals>.<lambda> at 0x7f78dab99790>"
          }
        },
        {
          "filename": "/opt/DPS/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py",
          "lineno": 743,
          "name": "<lambda>",
          "line": "",
          "locals": {
            "details": "(1960837120, 2077125573)",
            "heartbeat_sender": "'<temporalio.worker._activity._MultiprocessingSharedHeartbeatSender object at 0x7'+12",
            "info": "\"Info(activity_id='3', activity_type='adjust_replica', attempt=1, current_attempt\"+875"
          }
        },
        {
          "filename": "/opt/DPS/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py",
          "lineno": 924,
          "name": "send_heartbeat",
          "line": "",
          "locals": {
            "self": "'<temporalio.worker._activity._MultiprocessingSharedHeartbeatSender object at 0x7'+12",
            "task_token": "\"b'\\\\n$bc8c0612-11a9-4a58-ab6b-43d37f064f56\\\\x12\\\\x19replicate-treenode-278873\\\\x1a$5\"+110",
            "details": "(1960837120, 2077125573)"
          }
        },
        {
          "filename": "<string>",
          "lineno": 2,
          "name": "put",
          "line": "",
          "locals": {
            "self": "<AutoProxy[Queue] object, typeid 'Queue' at 0x7f78dab2b070>",
            "args": "\"((b'\\\\n$bc8c0612-11a9-4a58-ab6b-43d37f064f56\\\\x12\\\\x19replicate-treenode-278873\\\\x1a\"+150",
            "kwds": "{}"
          }
        },
        {
          "filename": "/usr/lib/python3.8/multiprocessing/managers.py",
          "lineno": 850,
          "name": "_callmethod",
          "line": "",
          "locals": {
            "self": "<AutoProxy[Queue] object, typeid 'Queue' at 0x7f78dab2b070>",
            "methodname": "put",
            "args": "\"((b'\\\\n$bc8c0612-11a9-4a58-ab6b-43d37f064f56\\\\x12\\\\x19replicate-treenode-278873\\\\x1a\"+150",
            "kwds": "{}",
            "conn": "<multiprocessing.connection.Connection object at 0x7f78e1c62460>",
            "kind": "#ERROR",
            "result": "Full()"
          }
        }
      ]
    }
  ]
}
```

### Minimal Reproduction

Unfortunately, I have no reproduction, but I do have some analysis to share:

I think the log message `Failed during multiprocess queue poll for heartbeat` is potentially misleading, as [raised here](https://github.com/temporalio/sdk-python/blob/1.5.0/temporalio/worker/_activity.py#L908-L909) on the top `Exception` type. Digging into the first stack trace above, the exception is a `TimeoutError` and the point of origin appears to be here: https://github.com/temporalio/sdk-python/blob/1.5.0/temporalio/worker/_activity.py#L644 - the body of that expression ends with `Future.result(10)`, which can raise `TimeoutError`.  So the `TimeoutError` arises not from queue polling, but from executing a coroutine on an event loop. As to why the `Future.result(10)` call times out, I can't understand why.

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: `5.4.0-42-generic #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux`
- Temporal Version: 1.22.1, sdk-python 1.5.0
- Using the binary distribution, operated natively, i.e., non-containerized.



#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2024-02-26 15:39:57.000 UTC</summary>

Hrmm, so in a well-running multiprocess setup, a heartbeat should be immediate and barely queued. Because the primary process should be processing the cross-process heartbeat almost immediately.

> So the TimeoutError arises not from queue polling, but from executing a coroutine on an event loop.

Well, it's more like from the lack of queue polling. I believe if the heartbeat queue is not being polled/processed fast enough, this is where this call gets hung.

> As to why the Future.result(10) call times out, I can't understand why.

The only reason why I can think this would fail is if your primary process is not processing asyncio quickly enough. This often happens if you're doing a blocking call in some `async def` somewhere but I suppose could be caused by sending too many heartbeats too fast (or a combination of both things).

I am not sure there is much that can be done here. I believe the worker is not processing heartbeats faster than you are sending them and at some point it's a timeout. We can't queue heartbeats indefinitely. Open to suggestions though. For whatever suggestion, we likely need a replication to even confirm it solves your issue.

Alternatively, you may consider standard threaded (or async) activities that you, inside the activity, invoke multiprocessing.

</details>

<details>
<summary><strong>wilsoniya</strong> commented on 2024-02-29 22:27:42.000 UTC</summary>

Thanks for replying and generally being so responsive!

> The only reason why I can think this would fail is if your primary process is not processing asyncio quickly enough. This often happens if you're doing a blocking call in some async def somewhere but I suppose could be caused by sending too many heartbeats too fast (or a combination of both things).

I suspect you're right. I'm using Pydantic 2.x, and [using the custom data converter recipe you provide](https://github.com/temporalio/samples-python/tree/main/pydantic_converter). The activity with frequent heartbeats is sending the details parameter, [thus invoking the data converter](https://github.com/temporalio/sdk-python/blob/1.5.0/temporalio/worker/_activity.py#L250-L252) (if I understand correctly). Pydantic 2.x appears to have some serious unresolved perf issues, so I'm wondering if the primary process is bogged down doing Pydantic serde of heartbeat details. I'm going to experiment with omitting heartbeat details and see if that resolves the issue.

> I am not sure there is much that can be done here. I believe the worker is not processing heartbeats faster than you are sending them and at some point it's a timeout. We can't queue heartbeats indefinitely. Open to suggestions though. For whatever suggestion, we likely need a replication to even confirm it solves your issue.

Is this `return` call potentially problematic? 

https://github.com/temporalio/sdk-python/blob/4037dd4ce57461124612a9536f534abc05a7eb79/temporalio/worker/_activity.py#L908-L910

It seems like returning from `_MultiprocessingSharedStateManager._heartbeat_processor()` when we catch a `TimeoutError` when evaluating the heartbeat future here 

https://github.com/temporalio/sdk-python/blob/4037dd4ce57461124612a9536f534abc05a7eb79/temporalio/worker/_activity.py#L644-L646

will result in processing no further heartbeats, and thus the heartbeat queue filling up and blocking further activity liveness. I have an incomplete understanding of this code, but I suspect that the error I'm describing is unrecoverable. 

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-03-01 13:31:40.000 UTC</summary>

> Is this return call potentially problematic?

I fear it will encourage lossy heartbeats as opposed to just stopping the processor. If we had an easy way to surface the exception we would, but in the meantime I am worried about just accepting some heartbeats and not others

> will result in processing no further heartbeats, and thus the heartbeat queue filling up and blocking further activity liveness

Yes, this is a dangerous state to get in. I am not exactly sure what we should do when the shared heartbeat processor fails. It should never fail. I do want to avoid adding some kind of advanced code (e.g. draining just this specific activity's heartbeat queue somehow) to this never-should-happen failure state. Really it shouldn't even be a queue, it should be almost immediate where the queue never has a size more than one.

Open to suggestions about how to better surface this. Should I try to find a way to throw an exception and shut the whole worker down?

</details>

<details>
<summary><strong>wilsoniya</strong> commented on 2024-03-27 23:17:14.000 UTC</summary>

Sorry for the delayed response.

> Should I try to find a way to throw an exception and shut the whole worker down?

That would be ideal for my use case. But more generally, I think encountering situations one has consciously conceived of as "impossible" merits a hard crash. And given a choice between a process crashing or technically persisting in a dysfunctional state, I think the former is more conducive to detecting and recovering from a problem. 

Thanks!

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-03-28 12:41:02.000 UTC</summary>

With Temporal, we often favor partially disfunctional workers that still work for other things (e.g. things that aren't heartbeating and workflows) over fail-fast. But maybe we can make it an option.

But I am now wondering if we should deprecate multiprocess activities and instead tell users they can use normal Python subprocessing in their activities if they _must_ have separate processes. It doesn't have much value in the SDK anymore than any other form of remote activity would. The details of dealing with out-of-process activities is probably best left to the (advanced) user.

</details>


---

<a id="459"></a>

### #459: [Bug] Worker hangs after polling error

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/459 |
| **State** | CLOSED |
| **Author** | ndtretyak (Nikolay Tretyak) |
| **Created** | 2024-01-10 14:34:53.000 UTC (2 years ago) |
| **Updated** | 2024-08-06 13:28:08.000 UTC |
| **Closed** | 2024-08-06 13:28:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Worker hangs after an error during polling.

### Minimal Reproduction

I just start a worker with an incorrect token, so that my server responds with `Request unauthorized.`.

```python
import asyncio

from temporalio import activity, client, workflow, worker


@activity.defn
async def a() -> None:
    pass


@workflow.defn
class Workflow:
    @workflow.run
    async def run(self) -> None:
        pass


async def main():
    c = await client.Client.connect(
        "my_temporal_host:7233",
        rpc_metadata={"authorization": "wrong_token"},
        tls=True,
    )
    w = worker.Worker(
        c,
        task_queue="default",
        activities=[a],
        workflows=[Workflow],
    )
    await w.run()


if __name__ == "__main__":
    asyncio.run(main())
```

Output:
```
2024-01-10T14:27:15.726021Z  WARN temporal_sdk_core::worker::activities::activity_task_poller_stream: Error while polling for activity tasks error=Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
2024-01-10T14:27:15.726020Z  WARN temporal_sdk_core::worker::workflow::wft_poller: Error while polling for workflow tasks error=Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
2024-01-10T14:27:15.726316Z  WARN temporal_sdk_core::worker::workflow::wft_poller: Error while polling for workflow tasks error=Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
2024-01-10T14:27:15.726543Z  WARN temporal_sdk_core::worker::workflow::wft_poller: Error while polling for workflow tasks error=Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
2024-01-10T14:27:15.726584Z  WARN temporal_sdk_core::worker::workflow::wft_poller: Error while polling for workflow tasks error=Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
2024-01-10T14:27:15.727412Z  WARN temporal_sdk_core::worker::workflow::wft_poller: Error while polling for workflow tasks error=Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
2024-01-10T14:27:15.727472Z  WARN temporal_sdk_core::worker::workflow::workflow_stream: WFT poller errored, shutting down
2024-01-10T14:27:15.727490Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None })
2024-01-10T14:27:15.728052Z  WARN temporal_sdk_core::worker::workflow::workflow_stream: WFT poller errored, shutting down
2024-01-10T14:27:15.728070Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None })
2024-01-10T14:27:15.728168Z  WARN temporal_sdk_core::worker::workflow::workflow_stream: WFT poller errored, shutting down
2024-01-10T14:27:15.728475Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None })
2024-01-10T14:27:15.728506Z  WARN temporal_sdk_core::worker::workflow::workflow_stream: WFT poller errored, shutting down
2024-01-10T14:27:15.728512Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None })
2024-01-10T14:27:15.728945Z  WARN temporal_sdk_core::worker::workflow::workflow_stream: WFT poller errored, shutting down
2024-01-10T14:27:15.728958Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None })
Worker failed, shutting down
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/temporalio/worker/_activity.py", line 160, in run
    task = await poll_task
           ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/temporalio/bridge/worker.py", line 98, in poll_activity_task
    await self._ref.poll_activity_task()
RuntimeError: Poll failure: Unhandled grpc error when activity polling: Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/temporalio/worker/_activity.py", line 180, in run
    raise RuntimeError("Activity worker failed") from err
RuntimeError: Activity worker failed
2024-01-10T14:27:15.729754Z  WARN temporal_sdk_core::worker::activities::activity_task_poller_stream: Error while polling for activity tasks error=Status { code: PermissionDenied, message: "Request unauthorized.", details: b"\x08\x07\x12\x15Request unauthorized.\x1aJ\nHtype.googleapis.com/temporal.api.errordetails.v1.PermissionDeniedFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
```

Then, the process just hangs forever.

### Environment/Versions

- SDK Version: 1.5.0



#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2024-01-10 14:57:01.000 UTC</summary>

Thanks for the report! The worker shutdown should succeed and the exception be thrown out of `run()` if those are the actual workflow and activities you're running. But if it's a more advanced activity that is not respecting cancellation, it's possible worker shutdown is not completing because it is cancelling activities but they are not respecting cancellation in some way. I will test with your above scenario and confirm.

</details>

<details>
<summary><strong>ndtretyak</strong> commented on 2024-01-10 15:10:19.000 UTC</summary>

I don't actually run the activities. The authorization token is incorrect, so the worker cannot retrieve any tasks from the server.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-01-10 20:46:39.000 UTC</summary>

I have replicated this with your code. I think this is actually in our Rust Core layer, so I have opened https://github.com/temporalio/sdk-core/issues/667.

</details>

<details>
<summary><strong>milani</strong> commented on 2024-07-29 20:15:42.000 UTC</summary>

@cretz the ticket in rust sdk is resolved. does it mean this ticket is resolved and can be closed?

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-08-06 13:28:08.000 UTC</summary>

Yes, closing. This has not yet been made available in a released Python SDK version, but we hope to do another release soon.

</details>


---

<a id="426"></a>

### #426: [Feature Request] Implement `is_running` for `_WorkflowInstanceImpl`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/426 |
| **State** | CLOSED |
| **Author** | tomasfarias (TomÃ¡s FarÃ­as Santana) |
| **Created** | 2023-11-13 19:32:33.000 UTC (2y 1m ago) |
| **Updated** | 2024-11-06 15:38:12.000 UTC |
| **Closed** | 2023-11-13 23:46:14.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 3 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

Trying to produce messages to Kafka using `aiokafka` from a Temporal Workflow fails [when trying to check if the current event loop is running](https://github.com/aio-libs/aiokafka/blob/master/aiokafka/util.py#L84). This is caused by the the Temporal event loop not fully implement the `AbstractEventLoop` interface, which includes `is_running`. Although more understandable when it comes to thread methods (`to_thread`, etc...), simpler methods like `is_running` should be easy to implement.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

### Describe the solution you'd like

Add an `is_running` method to `_WorkflowInstanceImpl` to better conform to the `AbstractEventLoop` interface, perhaps by checking `_deleting`:

```python
def is_running(self) -> bool:
    return not self._deleting
```

Although perhaps it should always be `True` as long as there is a Runtime defined?

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

### Additional context

The Python doc [states](https://docs.python.org/3.12/library/asyncio-eventloop.html?highlight=abstracteventloop#asyncio.AbstractEventLoop) that custom implementations of `AbstractEventLoop` should have all methods defined [here](https://docs.python.org/3.12/library/asyncio-eventloop.html?highlight=abstracteventloop#asyncio-event-loop-methods). Although they are not being strict about it ("should have defined"), the closer Temporal's event loop gets to the interface, the more it can interoperate with other Python packages, like `aiokafka`.

<!-- Add any other context or screenshots about the feature request here. -->


#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2023-11-13 21:58:41.000 UTC</summary>

We can't implement all methods of the event loop, only deterministic ones. You should never call Kafka from inside a workflow, only from inside an activity. A workflow is meant to be deterministic code only. There is never a case where you should need `is_running` and we can't really give an accurate value for that or `is_closed` like a non-distributed event loop can. I guess _technically_ we could just always return `True` from `is_running` and always return `False` from `is_closed`, but it's not really accurate as it's never really running/closed in the background like a normal event loop.

I'm actually kind of glad this raised a not-implemented so it catches illegal things happening to the workflow's event loop, such as code based on whether it's "running" or not as if it was a local event loop.

</details>

<details>
<summary><strong>tomasfarias</strong> commented on 2023-11-13 23:46:14.000 UTC</summary>

> You should never call Kafka from inside a workflow, only from inside an activity. A workflow is meant to be deterministic code only.

A call to Kafka (or any other external service) can be made deterministic: It's a matter of handling possible errors in a deterministic fashion. In my case, I ignore them and move on.

> I'm actually kind of glad this raised a not-implemented so it catches illegal things happening to the workflow's event loop, such as code based on whether it's "running" or not as if it was a local event loop.

Not much else to say from my end as this is the direction you are glad to go with. We will deal with the issue from our side. Thank you for taking time to answer this request. I'll go ahead and close it.

</details>

<details>
<summary><strong>EdAyers</strong> commented on 2024-11-06 15:26:10.000 UTC</summary>

Parts of python standard library assume that is_running and is_closed are implemented:

Eg consider this line in aiohttp https://github.com/aio-libs/aiohttp/blob/3f2f4a705f588d79a8649bfb17018cb30431df89/aiohttp/connector.py#L960
```py
        if sys.version_info >= (3, 12):
            # Optimization for Python 3.12, try to send immediately
            resolved_host_task = asyncio.Task(coro, loop=loop, eager_start=True)
```
And if we look in the stdlib asyncio code we can see this calls:

https://github.com/python/cpython/blob/83827ad81972e8e6438a29274dbdcb923d7b1cc4/Lib/asyncio/tasks.py#L124
```py
        if eager_start and self._loop.is_running():
            self.__eager_start()
```
But is_running() raises a NotImplementedError because the method is abstract.

https://github.com/temporalio/sdk-python/blob/0b327b04a95fc7954ed2c9e4e1a6c5de47c51ebd/temporalio/worker/_workflow_instance.py#L195

If you do not support parts of the standard AbstractEventLoop interface that the standard core python library assumes exists, you need to reimplement them and give an error message that makes it clear what is being done wrong. It's particularly hard to debug since the event loop is only this WorkflowInstance in certain circumstances.

From the discussion above I think it's a mistake to try to make an aiohttp ClientSession outside of an activity. So that's how we fixed it, but more helpful errors explaining why the usage was wrong would be really appreciated here.

</details>


---

<a id="321"></a>

### #321: [Feature Request] Temporal exceptions from converters and codecs should fail the workflow not task

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/321 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-05-24 20:17:13.000 UTC (2y 7m ago) |
| **Updated** | 2023-07-05 15:34:46.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 1 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 2 |

#### Description

### Describe the solution you'd like

Need to let conversion and encoding/decoding fail workflow instead of just task.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-07-05 15:34:46.000 UTC</summary>

We have merged #329 which does let conversion fail workflow, but we haven't done codec because it is more complicated and we need more consensus here.

</details>


---

<a id="315"></a>

### #315: [Feature Request] Support constructing JSONProtoPayloadConverter with google.protobuf.json_format.Parse options

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/315 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-04-28 20:34:17.000 UTC (2y 8m ago) |
| **Updated** | 2023-08-16 15:43:16.000 UTC |
| **Closed** | 2023-08-16 15:43:16.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 1 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 2 |

#### Description

### Describe the solution you'd like

People may want to ignore unknown fields and right now they have to remake the class themselves. Make it easier to set the parse options if they wanted to construct this converter w/ different options. Can accept all options or at least ignore unknown fields.

#### Comments (1)

<details>
<summary><strong>elliot-graebert-skydio</strong> commented on 2023-07-26 17:31:41.000 UTC</summary>

This would be hugely helpful. Right now we are heading down the path of doing a custom payload converter because we can't just directly set the option for ignore unknown fields.

Really, what we want is compatibility with the https://pkg.go.dev/go.temporal.io/sdk/converter#ProtoJSONPayloadConverterOptions options.

</details>


---

<a id="311"></a>

### #311: [Feature Request] Log Forwarding Support for python-SDK

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/311 |
| **State** | CLOSED |
| **Author** | samuelmakarovskiy |
| **Created** | 2023-04-13 16:53:14.000 UTC (2y 9m ago) |
| **Updated** | 2023-11-06 17:39:39.000 UTC |
| **Closed** | 2023-11-06 17:39:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Have noticed an interesting behavior that arose when all instances of a temporal server are brought down after a python worker has connected. The worker infinite retries polling to reconnect which is fine, but the real issue is that the console logs for the underlying rust core are not formatted like the python worker logic ones. See below:

```
^[[2m2023-04-13T14:17:45.292663Z^[[0m ^[[33m WARN^[[0m ^[[1mtemporal_client::retry^[[0m^[[2m:^[[0m gRPC call poll_workflow_task_queue retried 119 times ^[[3merror^[[0m^[[2m=^[[0mStatus { code: Unavailable, message: "error trying to connect: dns error: failed to lookup address information: Name or service not known", source: Some(tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("dns error", Custom { kind: Uncategorized, error: "failed to lookup address information: Name or service not known" })))) }
```
Fixing this would allow me to also guarantee any core issues can be detected by monitoring unified logs and not just stdout
### Describe the solution you'd like

Couple of things would work

1. Could turn off pretty printing for the rust core logs so that at least I can make my application stdout logs match more reliably
2. The sdk could support passing back core logs to python like it does for typescript (and maybe other sdks). I see it's not supported in the [code](https://github.com/temporalio/sdk-python/blob/dbf96fdbd5edb3dcb0da0590de6e3dd5b4bc270b/temporalio/runtime.py#L125-L126), but maybe worth clarifying why?

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->


#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2023-04-13 18:47:24.000 UTC</summary>

> Could turn off pretty printing for the rust core logs so that at least I can make my application stdout logs match more reliably

This can be disabled by setting `RUST_LOG_COLORS=never` env var I believe. Ref https://docs.rs/env_logger/latest/env_logger/#disabling-colors. (can probably also make it clear somehow on the tty that it does not support colors, e.g. `TERM=dumb`)

> I see it's not supported in the [code](https://github.com/temporalio/sdk-python/blob/dbf96fdbd5edb3dcb0da0590de6e3dd5b4bc270b/temporalio/runtime.py#L125-L126), but maybe worth clarifying why?

The only real reason is you're the first to need it :-) This is something we can add I think. I will leave this issue open on the backlog and we will look into implementing it.

Reactions: â¤ï¸ 1

</details>

<details>
<summary><strong>samuelmakarovskiy</strong> commented on 2023-04-13 19:03:11.000 UTC</summary>

> This can be disabled by setting RUST_LOG_COLORS=never env var I believe. Ref https://docs.rs/env_logger/latest/env_logger/#disabling-colors. (can probably also make it clear somehow on the tty that it does not support colors, e.g. TERM=dumb)

Didn't seem to work, I think the [rust core](https://github.com/temporalio/sdk-core/blob/6146ed5709d6ed2cf24488e5df4930c7f0cb6ae1/core/src/telemetry/mod.rs#L188-L208) uses `tokio-rs/tracing`

And from [this thread](https://github.com/foundry-rs/foundry/issues/1295#issuecomment-1098460733) appears the config requires the filter to have `set_ansi` on it, [defaults to true](https://docs.rs/tracing-subscriber/latest/tracing_subscriber/fmt/struct.Layer.html#method.set_ansi)


</details>

<details>
<summary><strong>cretz</strong> commented on 2023-04-13 19:13:05.000 UTC</summary>

I see! I opened https://github.com/temporalio/sdk-core/issues/529 to let that be disabled at runtime.

Reactions: â¤ï¸ 1

</details>

<details>
<summary><strong>jmdacruz</strong> commented on 2023-08-25 16:10:54.000 UTC</summary>

Somewhat related to this topic, it would be nice to have more control on the structure of the Rust logging. One scenario is using structured (JSON) logging on the Python side, which currently gets mixed with the Rust log lines (as @samuelmakarovskiy shared on the description). Structured logging is desired in certain scenarios, like Kubernetes deployments where the infrastructure takes containers stdout/stderr and forwards that to some logging infrastructure.

Edit: @cretz clarified on Slack that "log forwarding" in this context means forwarding to the Python side, which would effectively solve the problem

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-10-31 15:44:51.000 UTC</summary>

After further discussion, we are going to wait on https://github.com/temporalio/sdk-core/issues/618 and have buffered, pushed logs be sent to Python logging.

</details>


---

<a id="113"></a>

### #113: [Feature Request] UUID support as an arg

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/113 |
| **State** | CLOSED |
| **Author** | KMilhan (Milhan) |
| **Created** | 2022-08-24 08:14:43.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-06 14:43:43.000 UTC |
| **Closed** | 2022-09-06 14:43:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

I tried to use UUID as an argument of activity. Received an error that UUID is not JSON serializable.

### Describe the solution you'd like

UUID is considered JSON serializable in many popular Python libraries, for example, Pydantic and FastAPI, hence Python devs tend to consider UUID as JSON serializable objects. i.e., string form of UUID.

Is there any limitation from the Temporal core side? If not, it would be great to have UUID support in Python SDK.

#### Comments (5)

<details>
<summary><strong>cretz</strong> commented on 2022-08-25 05:21:05.000 UTC</summary>

I think FastAPI just uses Pydantic, so not sure it's "many" popular libs.

Yes, Pydantic probably considers several things JSON serializable that the Python JSON lib does not and we do not. I don't think any of our SDKs support serialization of UUIDs by default (except maybe Go because it's a byte array) and I'm not sure we want to make Python special here.

Pydantic also supports JSON serialization of dates/times, IP addresses, enums besides just `IntEnum`, `Path`s, etc. But we do not. We have to concern ourselves with cross-language compatibility. I will consult with the team, but i am not sure we should support these types by default in Python unless we support them by default in all of our SDKs.

</details>

<details>
<summary><strong>KMilhan</strong> commented on 2022-08-25 06:14:53.000 UTC</summary>

Thank you very much for your thoughtful message.

Totally understand supporting various data types such as DateTime/path in a cross-platform/language environment is a challenge and appreciate all the consideration.

If you allow me to add a little bit more about UUID - it has quite a straightforward conversion (str to native ds) in popular languages, and it is a greatly common choice for unique ID design. For example, we use UUID for workflow ID where we want every workflow to be unique and don't want to limit the execution by it. I carefully believe that many other users have chosen a similar approach.

Thanks again for your consideration.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-08-29 13:42:31.000 UTC</summary>

Definitely a straightforward conversion. As it is for many of those other types I listed. I am not sure UUID is special here nor am I sure that we need every SDK to support UUID as a type natively. I will consult with the team, but we can probably support this.

Of course, since the conversion is so straightforward you can do this conversion yourself and just store as a string. Just like you would a file path, IP address, or any other type with a string representation that Python doesn't consider JSON serializable.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-08-30 12:53:17.000 UTC</summary>

I think we can reopen this until I at least get team confirmation that we do or don't want to support this.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-09-06 14:43:42.000 UTC</summary>

After some research, yeah since UUIDs don't have nested abilities to auto-encode to JSON, it causes trouble with https://docs.python.org/3/library/dataclasses.html#dataclasses.asdict and similar use cases. So while we could support encoding single UUIDs to JSON and we could support decoding single UUIDs _and_ nested UUIDs from JSON, we cannot (easily) support encoding nested UUIDs to JSON.

Gonna close this for now as not supported, but if it's a large enough burden to rework our nested encoding, we can look into it.

</details>


---

<a id="85"></a>

### #85: [Feature Request] Ability to install without having Rust installed 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/85 |
| **State** | CLOSED |
| **Author** | crubier (Vincent Lecrubier) |
| **Created** | 2022-07-22 14:44:27.000 UTC (3y 5m ago) |
| **Updated** | 2022-08-08 12:55:49.000 UTC |
| **Closed** | 2022-08-08 12:54:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? 

Requiring rust to be installed to pip install this package complicates deployment 


### Describe the solution you'd like

Publish a version that does not require Rust to install with Pip

### Additional context

Trying to use temporal sdk at my company, but we donâ€™t have Rust on our base images. Requiring it complicates things. 


#### Comments (5)

<details>
<summary><strong>Geethree</strong> commented on 2022-07-22 17:41:45.000 UTC</summary>

Unfortunately, it looks like the core sdk is rust..

https://github.com/temporalio/sdk-python/tree/main/temporalio/bridge

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-22 17:49:30.000 UTC</summary>

@crubier and @Geethree - Our goal is to _not_ require Rust in any way to install. Our last alpha release only had a limited number of wheels pre-built (because it is alpha):

* Windows x64
* macOS x64
* macOS arm64
* Linux x64 for libc >= 2.31

These can be seen at https://pypi.org/project/temporalio/0.1a2/#files. You only need Rust if you aren't on one of those platforms. The next release, coming soon, will support more including Linux x64 for libc >= 2.17 and Linux arm64. If you want to use these wheels now, you can get them as artifacts on the latest successful master build (so https://github.com/temporalio/sdk-python/actions/runs/2712855146).

So after next release, only in rare environments would you ever need Rust.

Reactions: ðŸ‘ 1 ðŸŽ‰ 1

</details>

<details>
<summary><strong>vincent-lecrubier-skydio</strong> commented on 2022-07-22 19:09:51.000 UTC</summary>

Cool, indeed, being on ubuntu bionic with libc 2.27, I had to build, couldn't use the wheel.

Thank you @cretz !

</details>

<details>
<summary><strong>vincent-lecrubier-skydio</strong> commented on 2022-08-07 17:42:17.000 UTC</summary>

I was excited to see version 0.1b1 was released, but it does not have a linux aarch64 wheel. 

And for some reason, Trying to install a wheel from github gives me the following error:

```
#0 1.250 ERROR: temporalio-0.1b1-cp37-abi3-manylinux2014_aarch64.manylinux_2_17_aarch64.whl is not a supported wheel on this platform.
```

For reference I am trying to build this:

```dockerfile
# This is a docker image based on Alpine, containing Temporalio only
FROM python:3.10.4-alpine
WORKDIR /app

COPY temporalio-0.1b1-cp37-abi3-manylinux2014_aarch64.manylinux_2_17_aarch64.whl temporalio-0.1b1-cp37-abi3-manylinux2014_aarch64.manylinux_2_17_aarch64.whl

# Install Python pakages, including temporalio
RUN python -m pip install \
  asyncio==3.4.3 \
  /app/temporalio-0.1b1-cp37-abi3-manylinux2014_aarch64.manylinux_2_17_aarch64.whl

# Run everything
COPY . .
CMD ["sh", "-c", "python -m worker.worker"]
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-08-08 12:54:23.000 UTC</summary>

@vincent-lecrubier-skydio - Thanks for testing this for us. I didn't put the wheel in PyPI since we haven't been able to test ourselves.

I have opened #100 specifically about investigating Linux aarch64. We will try to get an environment to test in there. Any help welcome. Also, you might want to confirm with `pip debug --verbose` that the `cp37-abi3-manylinux2014_aarch64` or `manylinux_2_17_aarch64` tags are supported.

In the meantime I am gonna close this general purpose issues since beta has solved this for other environments.

</details>


---

<a id="61"></a>

### #61: temporal-sdk-core-0.1.0: 1 vulnerabilities (highest severity is: 7.5) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/61 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2022-07-01 15:27:46.000 UTC (3y 6m ago) |
| **Updated** | 2023-03-30 23:06:05.000 UTC |
| **Closed** | 2023-03-30 23:06:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>temporal-sdk-core-0.1.0</b></p></summary>

<p></p>
<p>


<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/0392468f21d9604c828d50d34daf8c9be03b31cd">0392468f21d9604c828d50d34daf8c9be03b31cd</a></p></details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in (temporal-sdk-core version) | Remediation Available |
| ------------- | ------------- | ----- | ----- | ----- | ------------- | --- |
| [CVE-2023-22895](https://www.mend.io/vulnerability-database/CVE-2023-22895) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | bzip2-0.4.3.crate | Transitive | N/A* | &#10060; |
<p>*For some transitive vulnerabilities, there is no version of direct dependency with a fix. Check the section "Details" below to see if there is a version of transitive dependency where vulnerability is fixed.</p>

## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2023-22895</summary>


###  Vulnerable Library - <b>bzip2-0.4.3.crate</b></p>

<p>Bindings to libbzip2 for bzip2 compression and decompression exposed as
Reader/Writer streams.
</p>
<p>Library home page: <a href="https://crates.io/api/v1/crates/bzip2/0.4.3/download">https://crates.io/api/v1/crates/bzip2/0.4.3/download</a></p>
<p>

Dependency Hierarchy:
  - temporal-sdk-core-0.1.0 (Root Library)
    - zip-0.6.3.crate
      - :x: **bzip2-0.4.3.crate** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/0392468f21d9604c828d50d34daf8c9be03b31cd">0392468f21d9604c828d50d34daf8c9be03b31cd</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
The bzip2 crate before 0.4.4 for Rust allow attackers to cause a denial of service via a large file that triggers an integer overflow in mem.rs. NOTE: this is unrelated to the https://crates.io/crates/bzip2-rs product.

<p>Publish Date: 2023-01-10
<p>URL: <a href=https://www.mend.io/vulnerability-database/CVE-2023-22895>CVE-2023-22895</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-22895">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-22895</a></p>
<p>Release Date: 2023-01-10</p>
<p>Fix Resolution: bzip2 - 0.4.4</p>

</p>

<p></p>

</details>

#### Comments (5)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-08-16 22:37:20.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-01-10 22:15:40.000 UTC</summary>

:information_source: This issue was automatically re-opened by Mend because the vulnerable library in the specific branch(es) has been detected in the Mend inventory.

</details>

<details>
<summary><strong>raravena80</strong> commented on 2023-02-16 23:52:45.000 UTC</summary>

Hello, is there a timeline to get this fix in?  A lot of enterprise customers are starting to use temporal and they don't allow software that has high/critical vulnerabilities to get installed. Thx.

</details>

<details>
<summary><strong>raravena80</strong> commented on 2023-02-28 18:08:17.000 UTC</summary>

@cretz following up on this.  The sdk cannot be used by our enterprise customers because of this vulnerability.  I believe this is probably the case for temporal customers too.   Thanks!

</details>

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-03-30 23:06:05.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="1267"></a>

### #1267: [Bug] Logs not emitted during workflow queries

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1267 |
| **State** | OPEN |
| **Author** | marcoriopel |
| **Created** | 2026-01-08 15:16:55.000 UTC (1 days ago) |
| **Updated** | 2026-01-08 15:18:22.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 2 |

#### Description

### What are you really trying to do?
Add some logs to query functions

### Describe the bug
When adding logs to a query function using the provided replay safe logger, it doesn't log anything.

The log below will always be dropped:
```
from temporalio import workflow

@workflow.defn
class TestWorkflow:
    @workflow.run
    async def run(self) -> bool:
       return True
    
    @workflow.query
    def get_test(self) -> str:
        workflow.logger.info("Query received")
        return "Test"

```

It seems like `workflow.unsafe.is_replaying()` always returns True when in the context of a query. This is used by the replay safe logger to determine if to drop logs or not so I assume this is why we have this behaviour.



---

<a id="1250"></a>

### #1250: RuntimeError when starting Temporal test server in Python SDK

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1250 |
| **State** | OPEN |
| **Author** | grinyael86 |
| **Created** | 2025-11-17 14:18:31.000 UTC (1 months ago) |
| **Updated** | 2025-12-11 23:25:41.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 2 |
| **Priority Score** | 4 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

## Expected Behavior

The temporal test server is downloaded and running

## Actual Behavior

When trying to start the Temporal test server using the Python SDK, I encounter the following error:

`RuntimeError: Failed starting test server: error sending request for url (https://temporal.download/temporal-test-server/default?arch=amd64&platform=linux&sdk-name=sdk-python&sdk-version=1.17.0)
`

The issue does not always happen, and sometimes it works fine - no blocking firewall or any network restriction from my side

## Steps to Reproduce the Problem

Create a test that has a worker who uses that as a client:

`   
async with await WorkflowEnvironment.start_time_skipping() as env:
   yield env.client
`

like:

`
async with Worker(
            client=temporal_env,
            task_queue="my_job_queue",
            workflows=[MyWorkflow],
            activities=[my_activity],
            activity_executor=activity_executor,
        ):
            result = await temporal_env.execute_workflow(
                WorkflowsNames.MY_WORKFLOW,
                workflow_input,
                id=f"test-workflow-{workflow_input.data.trans_id}",
                task_queue="MY_job_queue",
                execution_timeout=timedelta(minutes=10),
            )
`

a second way to reproduce:

run for linux:

`
curl -v "https://temporal.download/temporal-test-server/default?arch=amd64&platform=linux&sdk-name=sdk-python&sdk-version=1.17.0"
`

run for windows:

`
curl -v "https://temporal.download/temporal-test-server/default?arch=amd64&platform=windows&sdk-name=sdk-python&sdk-version=1.17.0"
`

and check if it's blocked or not

#### Comments (2)

<details>
<summary><strong>vladislavkoz</strong> commented on 2025-11-18 14:31:51.000 UTC</summary>

Have similar problem with 

 Failed starting Temporal dev server: HTTP status server error (500 Internal Server Error) for url (https://temporal.download/cli/default?arch=arm64&platform=darwin&format=tar.gz&sdk-name=sdk-python&sdk-version=1.19.0

</details>

<details>
<summary><strong>grinyael86</strong> commented on 2025-11-19 07:34:47.000 UTC</summary>

For now, I downloaded the server to my project and configured the temporal test environment to point to it.

</details>


---

<a id="1194"></a>

### #1194: [Feature Request] Set Temporal-Namespace header on every namespace-specific gRPC request

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1194 |
| **State** | CLOSED |
| **Author** | matlegit (matlegit) |
| **Created** | 2025-10-27 17:01:44.000 UTC (2 months ago) |
| **Updated** | 2025-10-27 17:22:41.000 UTC |
| **Closed** | 2025-10-27 17:03:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/475

### Additional context

Hi there, does this feature exist yet in the Python SDK? Thank you!


#### Comments (4)

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-27 17:03:09.000 UTC</summary>

Yes, it does.

</details>

<details>
<summary><strong>matlegit</strong> commented on 2025-10-27 17:14:03.000 UTC</summary>

@tconley1428 Awesome thank you! Do you happen to know which release or version started having this feature?

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-27 17:18:56.000 UTC</summary>

I think `1.7.0`, it has been present for over a year.

</details>

<details>
<summary><strong>matlegit</strong> commented on 2025-10-27 17:22:41.000 UTC</summary>

> I think `1.7.0`, it has been present for over a year.

Thank you!

</details>


---

<a id="1134"></a>

### #1134: [Bug] Temporal Workflows Require Model Name to be String - LitellmModel Not Supported

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1134 |
| **State** | CLOSED |
| **Author** | databill86 |
| **Created** | 2025-09-30 13:27:01.000 UTC (3 months ago) |
| **Updated** | 2025-09-30 15:15:10.000 UTC |
| **Closed** | 2025-09-30 15:08:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm building a multi-tenant application using Temporal workflows with OpenAI agents where different agents need to use different proxy configurations (different API keys and base URLs). I want each agent to have its own model configuration rather than being forced to use a global OpenAI client configuration.

### Describe the bug

Temporal workflows with OpenAI agents require the model name to be a string, but this prevents using custom model objects like `LitellmModel` with specific API keys and base URLs for individual agents. This limitation forces all agents to use the same global OpenAI client configuration, making it impossible to have different proxy configurations per agent.

When using Temporal workflows with OpenAI agents, the `convert_agent` function in `temporalio/contrib/openai_agents/_openai_runner.py` enforces that the model must be a string:

```python
def _model_name(agent):
    # ...
    raise ValueError(
        "Temporal workflows require a model name to be a string in the agent."
    )
```

This prevents using custom model objects like `LitellmModel` that allow per-agent configuration of API keys and base URLs.

**Expected Behavior:**
Agents should be able to use custom model objects that implement the `Model` interface, allowing for:
- Per-agent API key configuration
- Per-agent base URL configuration (for proxy setups)
- Flexible model provider selection per agent

**Current Behavior:**
Using a custom `LitellmModel` object in an agent results in:
```
ValueError: Temporal workflows require a model name to be a string in the agent.
```

### Minimal Reproduction

**1. Activity Definition (Tool â†’ Activity conversion):**
```python
from temporalio import activity

@activity.defn
async def simple_tool_activity(message: str) -> str:
    """Simple tool activity that processes a message."""
    logger.info(f"ðŸ”§ Simple tool activity processing: {message}")
    response = f"Tool processed: '{message}' - Response from activity!"
    return response

# The activity is automatically converted to a tool when passed to Agent
tools = [simple_tool_activity]
```

**2. Workflow with Agent:**
```python
from agents import Agent
from agents.extensions.models.litellm_model import LitellmModel
from temporalio import workflow

@workflow.defn
class MyWorkflow:
    @workflow.run
    async def run(self) -> str:
        # Inside the workflow
        
        agent = Agent(
            name="Triage Agent",
            instructions="Your instructions here",
            model=LitellmModel(model="model_name", api_key="proxy_api_key", base_url="proxy_base_url"),
            # or this: model=OpenAIChatCompletionsModel(model="model_name", openai_client=AsyncOpenAI(api_key=key, base_url=base_url)),
            # model="model_name", # This works if the client is created with plugins
            tools=tools,  # Activities are automatically converted to tools
            handoffs=handoffs,
        )
        
        # This is where the error occurs - agent.run() internally calls convert_agent()
        result = await agent.run("Your message here")
        return result
```

**3. Worker Setup:**
```python
from temporalio.worker import Worker

async def main():
    # Create Temporal client with plugins (required for agents)
    client = await create_temporal_client(include_plugins=True)
    
    # Run the worker
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as activity_executor:
        worker = Worker(
            client,
            task_queue="demo-task-queue",
            workflows=[MyWorkflow],
            activities=[simple_tool_activity],  # Register the activity
            activity_executor=activity_executor,
        )
        await worker.run()

# This fails when client is created without plugins
Client.connect(
    "localhost:7233",
    namespace="default",
    tls=False,
    plugins=[],  # Empty plugins causes the error
)

# Start the workflow
workflow_handle = await client.start_workflow(
    MyWorkflow.run,
    id="my-workflow-id",
    task_queue="demo-task-queue",
)
```

**Workaround that works:**

```python
plugins = [
    OpenAIAgentsPlugin(
        model_params=ModelActivityParameters(
            start_to_close_timeout=timedelta(seconds=30)
        ),
        model_provider=CustomLitellmProvider(
            base_url=PROXY_BASE_URL,
            api_key=PROXY_API_KEY,
        ),
    ),
]

Client.connect(
    "localhost:7233",
    namespace="default",
    tls=False,
    plugins=plugins,  # With plugins it works
)

# In the workflow: use model="model_name" instead of LitellmModel
# The agent.run() call succeeds because convert_agent() can handle string model names
```


This is the CustomLitellmProvider implementation:

```python
from agents.extensions.models.litellm_model import LitellmModel
from agents.models.interface import Model, ModelProvider


class CustomLitellmProvider(ModelProvider):
    """
    A custom ModelProvider that uses LiteLLM with configurable base_url and api_key.
    """

    def __init__(self, base_url: str | None = None, api_key: str | None = None):
        """
        Initialize the custom Litellm provider.

        Args:
            base_url: The base URL for the API (e.g., proxy endpoint)
            api_key: The API key for authentication
        """
        self.base_url = base_url
        self.api_key = api_key

    @property
    def model_class(self) -> type[Model]:
        """Get the model class used by this provider."""
        return LitellmModel

    @property
    def provider_name(self) -> str:
        """Get the name of this provider."""
        return "CustomLitellmProvider"

    @property
    def is_proxy_configured(self) -> bool:
        """Check if proxy configuration is set."""
        return self.base_url is not None and self.api_key is not None

    def get_model(self, model_name: str) -> Model:
        """
        Get a LitellmModel instance with the configured base_url and api_key.

        Args:
            model_name: The name of the model to use

        Returns:
            A LitellmModel instance configured with the provider's settings
        """
        if not model_name:
            raise ValueError("Model name is required")
        return LitellmModel(
            model=model_name,
            base_url=self.base_url,
            api_key=self.api_key,
        )

    def __repr__(self) -> str:
        """Return a string representation of the provider."""
        proxy_status = "configured" if self.is_proxy_configured else "not configured"
        return (
            f"CustomLitellmProvider("
            f"base_url={self.base_url!r}, "
            f"api_key={'***' if self.api_key else None!r}, "
            f"proxy={proxy_status})"
        )

    def __str__(self) -> str:
        """Return a user-friendly string representation."""
        return (
            f"CustomLitellmProvider with "
            f"base_url={self.base_url or 'default'}, "
            f"proxy={'enabled' if self.is_proxy_configured else 'disabled'}"
        )

```

### Environment/Versions

- OS and processor: Linux
- Temporal Version: temporalio==1.18.0
- OpenAI SDK: openai==1.109.0
- OpenAI Agents: openai-agents==0.3.2
- Python: 3.11
- Are you using Docker or Kubernetes or building Temporal from source? Using Docker

### Additional context

**Use Case:**
In a multi-tenant application with different proxy configurations per agent:
- Agent A needs to use Proxy 1 with specific API key
- Agent B needs to use Proxy 2 with different API key  
- Agent C needs to use direct OpenAI API

Currently, this is impossible because all agents must use the same global OpenAI client configuration.

**Important Note:**
One of the most important reasons I don't want to use the global OpenAI client configuration is to use logfire to trace the agents. When I used the plugins, the logfire tracing was not working at all.

## Tracing Conflict Issue

The plugin approach conflicts with observability/tracing setup. Here's the tracing configuration that doesn't work with plugins:

```python
from agents import set_trace_processors
import logfire
import nest_asyncio

def init_tracing():
    """Initialize tracing and observability."""
    # Set Langfuse env vars from settings (user can override via real env)
    os.environ.setdefault("LANGFUSE_PUBLIC_KEY", LANGFUSE_PUBLIC_KEY)
    os.environ.setdefault("LANGFUSE_SECRET_KEY", LANGFUSE_SECRET_KEY)
    os.environ.setdefault("LANGFUSE_HOST", LANGFUSE_HOST)

    set_trace_processors([])  # only disable OpenAI tracing

    # Instrument OpenAI Agents SDK via pydantic-ai logfire
    try:
        nest_asyncio.apply()
        logfire.configure(service_name="proxy", send_to_logfire=False)
        # This method automatically patches the OpenAI Agents SDK to send logs via OTLP to Langfuse.
        logfire.instrument_openai_agents()
    except Exception as exc:  # noqa: BLE001
        logger.error(f"Logfire instrumentation not available: {exc}")

# Worker setup with tracing
async def main():
    # Initialize tracing (conflicts with plugins)
    init_tracing()  # Sets up logfire â†’ OTLP â†’ Langfuse
    
    # Create Temporal client with plugins (required for agents)
    client = await create_temporal_client(include_plugins=True)
    
    # Run the worker
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as activity_executor:
        worker = Worker(
            client,
            task_queue="demo-task-queue",
            workflows=[MyWorkflow],
            activities=[simple_tool_activity],
            activity_executor=activity_executor,
        )
        await worker.run()
```

**The Problem:** When using plugins, the logfire tracing instrumentation doesn't work properly, making it impossible to trace agent execution in Langfuse. This forces a choice between:
1. Using plugins (works with string models) but losing observability
2. Using custom model objects (maybe better observability, not tested because of the string model requirement).

If you've run into this same issue or have any insights on how to work around it, I'd really appreciate hearing from you. 

Thanks ðŸ™



#### Comments (4)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-30 14:06:20.000 UTC</summary>

Using a string to represent the model is a hard requirement. Because the models need to be invoked inside an activity, we need to be able to acquire the model there. Because `Model` is generally not serializable, this means that we need an alternate way of acquiring the model rather than being provided an instantiated model. The answer to this is a model name, combined with a `ModelProvider` which can be instantiated on the worker side, where the activities are executed.

This does _not_ however mean that this is true:
> This prevents using custom model objects like LitellmModel that allow per-agent configuration of API keys and base URLs.

You can either use an existing `ModelProvider` if it gives you the semantics you need, like `MultiProvider` or `LitellmProvider`, or you can always create your own to provide an arbitrary mapping from model name to the model you desire, however complex. Consider this sample: https://github.com/temporalio/samples-python/blob/main/openai_agents/model_providers/run_gpt_oss_worker.py
It has a simple implementation of a custom model provider, but in your case you can define the name to model mapping however you want. The tenant can be included in the name in some fashion and you can construct an appropriate LiteLLM model. (You can look at the definition of `LitellmProvider` as a reference). 

If you need additional assistance in making that work, I would recommend the community slack. https://t.mp/slack allows better back and forth than github issues.

The tracing concern appears to be an entirely separate issue. Unless you see reason there is a substantial intersection between the two, please open a new issue instead on that concern. That way any conversation or changes to address it will not be interleaved with this.


Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>databill86</strong> commented on 2025-09-30 14:41:36.000 UTC</summary>

Thank you for you response.


After further investigation, I conducted additional testing to explore alternative approaches and understand the behavior better. Here are my findings: 

1. Interestingly, when I don't use plugins, but specify the model name as a string, I have the traces in langfuse, but the workflow fails with the error:


```log
[EDITED]
ðŸ¤– Running agent without plugins + string model...
16:10:26.073 OpenAI Agents trace: Agent workflow
16:55:53.393   Agent run: 'Demo Agent Name'
Getting response for model: gpt-4.1-mini-2025-04-14
Model settings: ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), base_url: https://api.openai.com/v1/, api_key: sk-proj-******
16:55:53.435     Responses API with 'gpt-4.1-mini-2025-04-14'
Traceback (most recent call last):
  File "/lib/python3.11/site-packages/agents/models/openai_responses.py", line 92, in get_response
    response = await self._fetch_response(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lib/python3.11/site-packages/agents/models/openai_responses.py", line 309, in _fetch_response
    return await self._client.responses.create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lib/python3.11/site-packages/openai/resources/responses/responses.py", line 2259, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/lib/python3.11/site-packages/openai/_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lib/python3.11/site-packages/openai/_base_client.py", line 1494, in request
    self._platform = await asyncify(get_platform)()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lib/python3.11/site-packages/openai/_utils/_sync.py", line 84, in wrapper
    return await to_thread(function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lib/python3.11/site-packages/openai/_utils/_sync.py", line 45, in to_thread
    return await _asyncio_to_thread(func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lib/python3.11/asyncio/threads.py", line 25, in to_thread
    return await loop.run_in_executor(None, func_call)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lib/python3.11/asyncio/events.py", line 290, in run_in_executor
    raise NotImplementedError
NotImplementedError
ERROR:openai.agents:Error getting response: . (request_id: None)
```

The code used in the workflow is:

```python
tool = temporal_agents.workflow.activity_as_tool(
    simple_tool_activity,
    start_to_close_timeout=timedelta(minutes=2),
)
os.environ.setdefault("OPENAI_API_KEY", OPENAI_API_KEY)
agent = Agent(
    name="Demo Agent",
    instructions="instructions here",
    model=model_name,  # String model name
    tools=[tool],

# Run the agent
input_items = cast(
    List[TResponseInputItem], [{"role": "user", "content": "test message"}]
)

logger.info("Running agent without plugins + string model...")
result = await Runner.run(agent, input_items, context={})

)
```

2. I also tried to use the `LitellmModel` object in the Agent, but without plugins in the client, and surprisingly I got the traces in langfuse, but the workflow failed with the error:

```log
- ðŸ¤– Running agent without plugins + LitellmModel...
16:20:32.620 OpenAI Agents trace: Agent workflow
16:20:32.622   Agent run: 'Demo Agent'
16:20:32.625     Chat completion with 'gpt-4.1-mini-2025-04-14' [LLM]
16:20:32 - LiteLLM:DEBUG: utils.py:359 - 

DEBUG:LiteLLM:



16:20:32 - LiteLLM:DEBUG: utils.py:359 - Request to litellm:
DEBUG:LiteLLM:Request to litellm:
Request to litellm:
16:20:32 - LiteLLM:DEBUG: utils.py:359 - litellm.acompletion(model='gpt-4.1-mini-2025-04-14', messages=[{'content': '# System context\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_<agent_name>`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\n\nYou are a demo agent. Use the simple_tool_activity to process user messages.', 'role': 'system'}, {'role': 'user', 'content': 'Hello, this is a test message for the demo!'}], tools=[{'type': 'function', 'function': {'name': 'simple_tool_activity', 'description': '', 'parameters': {'properties': {'message': {'title': 'Message', 'type': 'string'}}, 'required': ['message'], 'title': 'simple_tool_activity_args', 'type': 'object', 'additionalProperties': False}}}], temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.3.2'}, api_key='sk-6******', base_url='http://0.0.0.0:4004')
DEBUG:LiteLLM:litellm.acompletion(model='gpt-4.1-mini-2025-04-14', messages=[{'content': '# System context\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_<agent_name>`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\n\nYou are a demo agent. Use the simple_tool_activity to process user messages.', 'role': 'system'}, {'role': 'user', 'content': 'Hello, this is a test message for the demo!'}], tools=[{'type': 'function', 'function': {'name': 'simple_tool_activity', 'description': '', 'parameters': {'properties': {'message': {'title': 'Message', 'type': 'string'}}, 'required': ['message'], 'title': 'simple_tool_activity_args', 'type': 'object', 'additionalProperties': False}}}], temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.3.2'}, api_key='sk-*****', base_url='http://0.0.0.0:4004')
litellm.acompletion(model='gpt-4.1-mini-2025-04-14', messages=[{'content': '# System context\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_<agent_name>`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\n\nYou are a demo agent. Use the simple_tool_activity to process user messages.', 'role': 'system'}, {'role': 'user', 'content': 'Hello, this is a test message for the demo!'}], tools=[{'type': 'function', 'function': {'name': 'simple_tool_activity', 'description': '', 'parameters': {'properties': {'message': {'title': 'Message', 'type': 'string'}}, 'required': ['message'], 'title': 'simple_tool_activity_args', 'type': 'object', 'additionalProperties': False}}}], temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, max_tokens=None, tool_choice=None, response_format=None, parallel_tool_calls=None, stream=False, stream_options=None, reasoning_effort=None, top_logprobs=None, extra_headers={'User-Agent': 'Agents/Python 0.3.2'}, api_key='sk-*****', base_url='http://0.0.0.0:4004')
16:20:32 - LiteLLM:DEBUG: utils.py:359 - 

DEBUG:LiteLLM:



16:20:32 - LiteLLM:WARNING: utils.py:538 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
WARNING:LiteLLM:`litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
16:20:32 - LiteLLM:DEBUG: litellm_logging.py:476 - self.optional_params: {}
DEBUG:LiteLLM:self.optional_params: {}
16:20:32 - LiteLLM:DEBUG: utils.py:359 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
DEBUG:LiteLLM:ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
16:20:32 - LiteLLM:DEBUG: caching_handler.py:233 - CACHE RESULT: None
DEBUG:LiteLLM:CACHE RESULT: None

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.

16:20:32 - LiteLLM:DEBUG: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False
DEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False
16:20:32 - LiteLLM:DEBUG: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []
DEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []
2025-09-30 16:20:32.668 | ERROR    | temporal_demo.workflows.demo_workflow:run:78 - âŒ Demo workflow failed: litellm.APIConnectionError: APIConnectionError: OpenAIException - 
```


The code used in the workflow is:   

```python
# Create a simple tool
tool = temporal_agents.workflow.activity_as_tool(
    simple_tool_activity,
    start_to_close_timeout=timedelta(minutes=2),
)

# Create agent with LitellmModel but no plugins
agent = Agent(
    name="Demo Agent",
    instructions="instructions here",
    model=LitellmModel(
        model=model_name,
        api_key="sk-*****",
        base_url="http://0.0.0.0:4004",
    ),
    tools=[tool],
)

# Run the agent
input_items = cast(
    List[TResponseInputItem], [{"role": "user", "content": "test message"}]
)

logger.info("Running agent without plugins + LitellmModel...")
result = await Runner.run(agent, input_items, context={})
```

Note that the same litellm config is working when used outside of the workflow.

So, I think that specifying the plugins in the worker have something to do with the traces not being sent to langfuse here.

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-30 15:08:01.000 UTC</summary>

You can't use the agents SDK in a workflow without the plugin. It simply will not work. The plugin does a bunch of things to make it work, and without it the agents SDK has no provision for running the models in activities. It simply isn't an option without essentially doing everything the plugin does anyway. Langfuse tracing not working should be a separate issue which we can investigate.

</details>

<details>
<summary><strong>databill86</strong> commented on 2025-09-30 15:11:28.000 UTC</summary>

Thanks, I will create a separate issue for the tracing. 

</details>


---

<a id="1113"></a>

### #1113: [Bug] Client connect with TLS does not work anymore

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1113 |
| **State** | CLOSED |
| **Author** | BehnazMoradabadi |
| **Created** | 2025-09-22 12:37:57.000 UTC (3 months ago) |
| **Updated** | 2025-09-23 10:10:13.000 UTC |
| **Closed** | 2025-09-23 10:10:13.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 2 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### What are you really trying to do?

Client connect with TLS does not work anymore after the new release.

### Describe the bug
```
import asyncio
from temporalio.client import Client, TLSConfig

client = await Client.connect(
    "xxx",
    namespace="xxx",
    tls=TLSConfig(
        client_cert=open("tls.crt", "rb").read(),
        client_private_key=open("tls.key", "rb").read(),
        server_root_ca_cert=open("ca.crt", "rb").read(),
    ),
)
print(client)
```
Used to work but now it raises error:
```
Failed client connect: Server connection error: tonic::transport::Error(Transport, ConnectError(Custom { kind: InvalidData, error: InvalidCertificate(UnknownIssuer) }))

```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-22 13:44:09.000 UTC</summary>

There was a change in the release (I just added a call out to the release notes), to only enable native roots if the root ca cert is not provided. https://github.com/temporalio/sdk-core/pull/1007. Is it possible you were relying on native roots before? Try removing `server_root_ca_cert` and see if the issues resolves.

</details>

<details>
<summary><strong>BehnazMoradabadi</strong> commented on 2025-09-23 10:10:13.000 UTC</summary>

Yes it worked. Thanks

</details>


---

<a id="1050"></a>

### #1050: [Feature Request] Configurable Deadlock Detection Timeout in Python SDK

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1050 |
| **State** | CLOSED |
| **Author** | SanilK2108 |
| **Created** | 2025-08-25 05:57:58.000 UTC (4 months ago) |
| **Updated** | 2025-08-26 20:40:28.000 UTC |
| **Closed** | 2025-08-26 20:40:28.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 2 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

Currently, the Temporal Python SDK enforces a fixed 2-second deadlock detection timeout for workflows. This behavior cannot be configured in production: the only way to disable it is to run in debug_mode or set TEMPORAL_DEBUG.

This creates friction for production workflows that legitimately need longer CPU-bound or blocking sections. Unlike the Go SDK, which allows configuring WorkerOptions.DeadlockDetectionTimeout, the Python SDK forces developers to restructure code even in cases where a slightly longer threshold would be sufficient.

### Describe the solution you'd like

Introduce a configurable deadlock detection timeout for the Python SDK, similar to Goâ€™s DeadlockDetectionTimeout option.

Example proposal:

```
worker = Worker(
    client,
    task_queue="my-task-queue",
    workflows=[MyWorkflow],
    activities=[...],
    deadlock_detection_timeout=timedelta(seconds=5)  # new option
)
```

Default remains 2 seconds (to preserve current behavior).

Users can override the timeout per worker in production.

Setting None (or 0) could fully disable the check, mirroring debug mode.

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->


#### Comments (2)

<details>
<summary><strong>SanilK2108</strong> commented on 2025-08-25 05:58:54.000 UTC</summary>

Iâ€™d be happy to contribute this feature and open a PR to add configurable deadlock detection timeout support to the Python SDK.

</details>

<details>
<summary><strong>Sushisource</strong> commented on 2025-08-26 17:59:07.000 UTC</summary>

@SanilK2108 ~We would welcome a PR!~ Actually, after some discussion with the team we're pretty sure we want to keep this limited purposefully. If you're doing things that extend the timeout beyond those values, there's a nearly 100% chance that you shouldn't be doing those things in a workflow context.

IE
> workflows that legitimately need longer CPU-bound or blocking sections

There is pretty much never a "legitimate" need for this. Pushing that code down into a local (or nonlocal) activity is the right thing to do.


</details>


---

<a id="1020"></a>

### #1020: [Bug] Using TaskGroup for child workflow execution does not correctly detect failure

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1020 |
| **State** | CLOSED |
| **Author** | cpnielsen (Christian PalmhÃ¸j Nielsen) |
| **Created** | 2025-08-06 08:52:26.000 UTC (5 months ago) |
| **Updated** | 2025-08-06 09:22:44.000 UTC |
| **Closed** | 2025-08-06 09:21:45.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 2 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### What are you really trying to do?

I want to make use of [asyncio.TaskGroup](https://docs.python.org/3/library/asyncio-task.html#task-groups) to execute a number of child workflows in parallel. However, if one of the child workflows fail, it does **not** cause the parent workflow to fail, but instead continues to run forever.

The expected behavior would be for child workflow execution failure to cause the parent workflow to fail as well, using default configuration.

### Describe the bug

When a child workflow fails it raises a [ChildWorkflowError](https://python.temporal.io/temporalio.exceptions.ChildWorkflowError.html). However, when using `TaskGroup` it is encapsulated in an `ExceptionGroup` and raised, which does _not_ fail the calling workflow, causing it to run forever as per the documented error logic (https://docs.temporal.io/references/failures#errors-in-workflows).

At a glance it looks like the Temporal code never considers `ExceptionGroup` or other inner exceptions that are of the [FailureError](https://python.temporal.io/temporalio.exceptions.FailureError.html) type that triggers an upstream failure.

Note, that if I instead use [asyncio.gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) it works as expected. This is expected, as `asyncio.gather` raises the first exception raised directly, passing the raw failure up which correctly triggers an overall failure.

### Minimal Reproduction

```python
# workflows.py
import asyncio
from datetime import timedelta
from typing import List

from temporalio import workflow
from temporalio.common import RetryPolicy

# Import activity, passing it through the sandbox without reloading the module
with workflow.unsafe.imports_passed_through():
    from activities import say_hello


@workflow.defn
class SayHello:
    @workflow.run
    async def run(self, name: str) -> str:
        return await workflow.execute_activity(
            say_hello, name, start_to_close_timeout=timedelta(seconds=5), retry_policy=RetryPolicy(maximum_attempts=3)
        )


@workflow.defn
class MasterWorkflow:
    @workflow.run
    async def run(self, names: List[str]) -> List[str]:
        tasks: List[asyncio.Task[str]] = []
        async with asyncio.TaskGroup() as tg:
            for name in names:
                tasks.append(
                    tg.create_task(
                        workflow.execute_child_workflow(
                            SayHello.run, name, id=f"say-hello-{name}", retry_policy=RetryPolicy(maximum_attempts=1)
                        )
                    )
                )

        greetings = [task.result() for task in tasks]

        return greetings
```

```python
# activities.py
from temporalio import activity


@activity.defn
async def say_hello(name: str) -> str:
    if name == "fail":
        raise Exception("Uh-oh")
    return f"Hello, {name}!"
```

```python
# run_workflow.py
import asyncio

from temporalio.client import Client

from run_worker import MasterWorkflow


async def main():
    # Create client connected to server at the given address
    client = await Client.connect("localhost:7233")

    # Execute a workflow
    result = await client.execute_workflow(
        MasterWorkflow.run, ["ok", "fail", "okay", "cool"], id="master-workflow", task_queue="hello-task-queue"
    )

    print(f"Result: {result}")


if __name__ == "__main__":
    asyncio.run(main())
```

```python
# run_worker.py
import asyncio

from temporalio.client import Client
from temporalio.worker import Worker

from activities import say_hello
from workflows import MasterWorkflow, SayHello


async def main():
    client = await Client.connect("localhost:7233", namespace="default")
    # Run the worker
    worker = Worker(
        client, task_queue="hello-task-queue", workflows=[MasterWorkflow, SayHello], activities=[say_hello]
    )
    await worker.run()


if __name__ == "__main__":
    asyncio.run(main())
```

The exception raised from the workflow is of type `ExceptionGroup` with the message `unhandled errors in a TaskGroup (1 sub-exception)`.

Changing the code to user `asyncio.gather` instead correctly registers as a "Child Workflow Execution Failure".

### Environment/Versions

- OS and processor: M2 Macbook Pro, x64 Linux
- Temporal Version: 1.12.0 (client+worker)
- Tested consistently locally (Macbook), but observed running in kubernetes as well.

As the nature of the bug is "working as intended", I expect it to behave identically across environments.

### Additional context

I am happy to provide a PR (with tests) if you can point me to some good starting point. I checked for places where the code handles/raises `FailureError`, but I don't feel sure enough to just dive in.


#### Comments (2)

<details>
<summary><strong>cpnielsen</strong> commented on 2025-08-06 09:21:44.000 UTC</summary>

After looking at the code again, I think the following might solve it:

```python
# temporalio/worker/_workflow_instance.py

# <-- IMPORTS -->

LOG_IGNORE_DURING_DELETE = False

if sys.version_info < (3, 11):
    from exceptiongroup import ExceptionGroup

# <-- MORE CODE -->

    def _is_workflow_failure_exception(self, err: BaseException) -> bool:
        # An exception is a failure instead of a task fail if it's already a
        # failure error or if it is a timeout error or if it is an instance of
        # any of the failure types in the worker or workflow-level setting
        wf_failure_exception_types = self._defn.failure_exception_types
        if self._dynamic_failure_exception_types is not None:
            wf_failure_exception_types = self._dynamic_failure_exception_types
        return (
            isinstance(err, temporalio.exceptions.FailureError)
            or self._is_exception_group_failure(err)
            or isinstance(err, asyncio.TimeoutError)
            or any(isinstance(err, typ) for typ in wf_failure_exception_types)
            or any(
                isinstance(err, typ)
                for typ in self._worker_level_failure_exception_types
            )
        )

    def _is_exception_group_failure(self, err: BaseException) -> bool:
        if isinstance(err, ExceptionGroup):
            return any(self._is_workflow_failure_exception(e) for e in err.exceptions)

        return False

# <-- MORE CODE -->
```


</details>

<details>
<summary><strong>cpnielsen</strong> commented on 2025-08-06 09:22:44.000 UTC</summary>

@cretz Could you re-open this please, I accidentally hit a hotkey to close the issue ðŸ¤¦ 

</details>


---

<a id="928"></a>

### #928: [Feature Request] Research and make SDK free-threading capable/compatible

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/928 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-06-27 15:57:23.000 UTC (6 months ago) |
| **Updated** | 2025-06-27 15:57:23.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 2 |

#### Description

### Describe the solution you'd like

Kind of related to #863. Per https://pyo3.rs/v0.25.1/free-threading.html this should be a simple opt-in, but we may have struggles with Python Protobuf (have to confirm they are free-threading compat too). Need to research first and confirm findings and what we'd have to do if it's anything significant. We will also need to have CI-based confirmation of our compatibility with a free-threading runtime.


---

<a id="927"></a>

### #927: [Bug]  Batch operation feature in the Temporal Python SDK doesn't work

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/927 |
| **State** | CLOSED |
| **Author** | DmitryZap (Dmitry Zaporozhchenko) |
| **Created** | 2025-06-27 10:07:53.000 UTC (6 months ago) |
| **Updated** | 2025-10-06 19:45:16.000 UTC |
| **Closed** | 2025-10-06 19:45:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm trying to use the batch operation feature in the Temporal Python SDK to reset multiple workflows at scale. This is particularly useful for maintenance and debugging operations on large numbers of workflows.

### Describe the bug

When attempting to start a batch operation (specifically a workflow reset operation), the Temporal Python SDK throws an error: `Unknown RPC call start_batch_operation`. This indicates that the SDK is making an RPC call that is not recognized by the Temporal server, suggesting missing or unimplemented functionality for batch operations in the Python SDK.

Error log:
```
2025-06-17 20:10:50,930 - INFO - Processing 1 workflows
2025-06-17 20:10:50,930 - INFO - Resetting workflow 'demo-workflow-1750180163' with reset_type 'first_task'
2025-06-17 20:10:50,930 - ERROR - Failed to reset workflow 'demo-workflow-1750180163': Unknown RPC call start_batch_operation
2025-06-17 20:10:50,930 - ERROR - Error resetting workflow demo-workflow-1750180163: Unknown RPC call start_batch_operation
2025-06-17 20:10:50,930 - INFO - Reset 0 workflows successfully, 1 errors
2025-06-17 20:10:50,930 - INFO - Reset operation completed successfully
```

### Minimal Reproduction

```python
import grpc
from temporalio.api.workflowservice.v1 import workflow_service_pb2 as workflow_service
from temporalio.api.workflowservice.v1 import workflow_service_pb2_grpc as workflow_service_grpc
from temporalio.api.common.v1 import payload_pb2
from temporalio.api.batch.v1 import message_pb2 as batch_message

def run_batch_operation():
    # Create gRPC channel to Temporal server
    channel = grpc.insecure_channel('localhost:7233')
    stub = workflow_service_grpc.WorkflowServiceStub(channel)
    
    # Create batch operation request
    request = workflow_service.StartBatchOperationRequest(
        namespace="default",
        job_id="batch-reset-job-1",
        reason="Resetting workflows via gRPC",
        reset_operation=batch_message.BatchOperationReset(
            reset_type=workflow_service.RESET_TYPE_FIRST_WORKFLOW_TASK,
            reset_reapply_type=workflow_service.RESET_REAPPLY_TYPE_SIGNAL
        ),
    )
    
    try:
        # Execute batch operation via gRPC
        response = stub.StartBatchOperation(request)
        print(f"Batch operation started successfully. Job ID: {response.job_id}")
        print(f"Progress: {response.total_operation_count} workflows to process")
    except grpc.RpcError as e:
        print(f"gRPC call failed: {e.details()}")
        print(f"Error code: {e.code()}")

if __name__ == "__main__":
    run_batch_operation()
```

### Environment/Versions

- OS and processor: macOS Sonoma (Apple M1 Pro)
- Temporal Version: Python SDK 1.3.0, Temporal Server 1.25.0
- Are you using Docker or Kubernetes or building Temporal from source? Without docker and builds

### Additional context

The error "Unknown RPC call start_batch_operation" suggests that the batch operation functionality might not be implemented in the Rust Core SDK used by the Temporal Python SDK.


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2025-06-27 14:12:50.000 UTC</summary>

> The error "Unknown RPC call start_batch_operation" suggests that the batch operation functionality might not be implemented in the Rust Core SDK used by the Temporal Python SDK.

Your code is bypassing the Rust Core and using gRPC directly. Haven't tested direct use, but can you try to use traditional `Client.connect` and then using `my_client.workflow_service.start_batch_operation(workflow_service_pb2.StartBatchOperationRequest(...` instead?

Also, where are the logs you have above showing from? For example, is "2025-06-17 20:10:50,930 - INFO - Processing 1 workflows" a server log or your log? Are you using the dev server? There may be something wrong server side. Can you confirm this exists with latest dev server version?

</details>

<details>
<summary><strong>DmitryZap</strong> commented on 2025-07-02 08:39:45.000 UTC</summary>

I tried to do the same thing through the client. Got the same problem.

Another example (with default client):

```python3
import argparse
import asyncio
import logging
from datetime import datetime
from typing import Optional

from temporalio.client import Client
from temporalio.service import RPCError
from temporalio.api.workflowservice.v1 import workflow_service_pb2
from temporalio.api.batch.v1 import message_pb2
from temporalio.api.enums.v1 import reset_pb2

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

async def get_client(address: str = "localhost:7233") -> Client:
    """Create and return a Temporal client."""
    return await Client.connect(
        address,
        namespace="default",
        tls=False,
    )

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Batch reset workflows using Temporal Python SDK")
    parser.add_argument(
        "--query",
        required=True,
        help="Query to select workflows for reset (e.g. WorkflowType='MyWorkflow')"
    )
    parser.add_argument(
        "--reason",
        required=True,
        help="Reason for reset"
    )
    parser.add_argument(
        "--namespace",
        default="default",
        help="Temporal namespace"
    )
    parser.add_argument(
        "--address",
        default="localhost:7233",
        help="Temporal server address"
    )
    parser.add_argument(
        "--reset-type",
        choices=["LastWorkflowTask", "FirstWorkflowTask"],
        default="LastWorkflowTask",
        help="Reset point type"
    )
    return parser.parse_args()

async def run_batch_reset(
    client: Client,
    namespace: str,
    query: str,
    reason: str,
    reset_type: str,
) -> Optional[str]:
    """
    Run a batch reset operation using Temporal Python SDK.
    
    Args:
        client: Client instance
        namespace: Temporal namespace
        query: Query to select workflows for reset
        reason: Reason for reset operation
        reset_type: Reset point type
        
    Returns:
        Job ID if successful, None otherwise
    """
    try:
        # Create unique job ID
        job_id = f"batch-reset-{int(datetime.now().timestamp())}"
        
        # Map reset type to enum
        reset_type_enum = {
            "LastWorkflowTask": reset_pb2.RESET_TYPE_LAST_WORKFLOW_TASK,
            "FirstWorkflowTask": reset_pb2.RESET_TYPE_FIRST_WORKFLOW_TASK,
        }[reset_type]
        
        # Create batch operation request
        request = workflow_service_pb2.StartBatchOperationRequest(
            namespace=namespace,
            visibility_query=query,
            job_id=job_id,
            reason=reason,
            reset_operation=message_pb2.BatchOperationReset(
                reset_type=reset_type_enum
            )
        )
        
        # Start batch operation using workflow service
        response = await client.workflow_service.start_batch_operation(request)
        logger.info(f"Started batch reset operation with job ID: {job_id}")
        return job_id
        
    except Exception as e:
        logger.error(f"Failed to start batch reset: {e}")
        return None

async def main() -> None:
    """Parse arguments and run batch reset operation."""
    args = parse_args()
    
    # Create Temporal client
    client = await get_client(args.address)
    
    # Run batch reset
    logger.info(f"Starting batch reset with query: {args.query}")
    job_id = await run_batch_reset(
        client=client,
        namespace=args.namespace,
        query=args.query,
        reason=args.reason,
        reset_type=args.reset_type,
    )
    
    if job_id:
        logger.info(f"Batch reset operation started successfully!")
        logger.info(f"Job ID: {job_id}")
        logger.info(f"Query: {args.query}")
        logger.info(f"Reset type: {args.reset_type}")
    else:
        logger.error("Failed to start batch reset operation")

if __name__ == "__main__":
    asyncio.run(main()) 
```



</details>

<details>
<summary><strong>cretz</strong> commented on 2025-07-02 13:18:52.000 UTC</summary>

Thanks, we have replicated here. The issue is our Rust core does not have that API call. We will add it and make sure that our test that is supposed to check that all are present is in fact doing so.

Reactions: ðŸš€ 1

</details>

<details>
<summary><strong>Sushisource</strong> commented on 2025-07-16 23:17:46.000 UTC</summary>

Core has it, rather, the Python bridge client does not. Tests for that will need the fix.

</details>


---

<a id="795"></a>

### #795: [Bug] Pyright typing issue

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/795 |
| **State** | CLOSED |
| **Author** | mattbrandman (Matthew Brandman) |
| **Created** | 2025-03-25 14:37:42.000 UTC (9 months ago) |
| **Updated** | 2025-10-02 20:40:56.000 UTC |
| **Closed** | 2025-10-02 20:40:56.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 2 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->
Ensure that my type checking returns minimal errors when using temporal execute_activity
### Describe the bug
Type checking with Pyright strict returns partiallyUnknown error when using Temporal execute_activity I believe because the line here https://github.com/temporalio/sdk-python/blob/49ca10e413ba67e1adfeed3b5577cb4f5b007e54/temporalio/workflow.py#L2185 has an implicit Any on the optional Type
<!-- A clear and concise description of what the bug is. -->
Adding an explicit Any allows Pyright to properly parse the block
<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction
Look at any execute_activity call in the samples repo with Pyright strict enabled in VSCode
<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (2)

<details>
<summary><strong>mattbrandman</strong> commented on 2025-03-25 14:38:17.000 UTC</summary>

Happy to open the PR for this if it's safe to add the explicit Any there

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-03-25 15:13:58.000 UTC</summary>

Thanks @mattbrandman. Yes, PR welcome!

</details>


---

<a id="783"></a>

### #783: [Bug] Sticky execution after Worker shutdown causes "Workflow Task Timed Out"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/783 |
| **State** | CLOSED |
| **Author** | gonced8 (GonÃ§alo Raposo) |
| **Created** | 2025-03-05 15:46:10.000 UTC (10 months ago) |
| **Updated** | 2025-04-04 14:49:52.000 UTC |
| **Closed** | 2025-04-04 14:40:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

Restarting workers, even with graceful shutdown, causes Workflow Tasks to timeout due to sticky execution.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

When a Workflow is executing and the Workflow Worker restarts, the next Workflow Task still gets scheduled to that worker, due to execution stickiness, and causes a "Workflow Task Timed Out". It would be expected for the Worker to inform Temporal server of its shutdown and disable the stickiness, scheduling the next Workflow Task to the original task queue.

Moreover, as for informing the server, the Workflow Worker should inform as soon as the shutdown process starts, so that no new Workflow Tasks are assigned to this worker during its graceful shutdown, because the worker wouldn't be polling anymore and those Workflow Tasks would timeout because of that.

Below I attach a minimal example to reproduce this bug. The Workflow has a 10s sleep and then returns. The Workflow Worker starts, stays running for 5 seconds and shutdowns. The Workflow Task after the sleep should be assigned to the original task queue, since the Workflow Worker is no longer running.

Here you can see a screenshot of the current behavior. There is a "Workflow Task Timed Out" because of sticky execution.
<img width="1035" alt="Image" src="https://github.com/user-attachments/assets/e3e5c171-4528-40ac-9188-4a94d7d11a8f" />

Disabling the sticky execution (`max_cached_workflows=0`), the Workflow Task doesn't timeout, as expected.
<img width="930" alt="Image" src="https://github.com/user-attachments/assets/9bbc813d-33f2-4afa-b297-b561502935bc" />

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

```python
import asyncio
import multiprocessing
from datetime import timedelta

from temporalio import workflow
from temporalio.client import Client
from temporalio.worker import Worker


@workflow.defn
class SimpleWorkflow:
    @workflow.run
    async def run(self) -> None:
        workflow.logger.info("Running SimpleWorkflow")
        workflow.logger.info("Sleeping for 10 seconds")
        await workflow.sleep(timedelta(seconds=10))
        workflow.logger.info("Done sleeping")
        return


async def worker():
    # Create client connected to server at the given address
    client = await Client.connect("localhost:7233")

    # Run the worker
    async with Worker(
        client,
        task_queue="my-task-queue",
        workflows=[SimpleWorkflow],
        graceful_shutdown_timeout=timedelta(seconds=3),
        # max_cached_workflows=0,
    ) as worker:
        print("Starting worker")
        # Run for 5 seconds
        await asyncio.sleep(5)
        print("Stopping worker")

    print("Stopping Temporal client")


async def client():
    # Create client connected to server at the given address
    client = await Client.connect("localhost:7233")

    # Execute a workflow
    print("Executing SimpleWorkflow")
    await client.execute_workflow(
        SimpleWorkflow.run,
        id="test-id",
        task_queue="my-task-queue",
        run_timeout=timedelta(seconds=25),
    )
    print(f"Finished SimpleWorkflow")


def start_client():
    asyncio.run(client())


if __name__ == "__main__":
    # Start client in different process
    multiprocessing.Process(target=start_client).start()

    # Start worker
    asyncio.run(worker())
    print("Exited worker")
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: M2 Mac, but same error happening with AMD based image
- Temporal Version: CLI 1.3.0 (Server 1.27.1, UI 2.36.0) and sdk-python 1.10.0
- Are you using Docker or Kubernetes or building Temporal from source? Tried with Temporal CLI and also with Docker and Kubernetes.

### Additional context

<!-- Add any other context about the problem here. -->
Discussed this issue with @cretz during Replay conference (thank you!).


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2025-03-07 11:37:55.000 UTC</summary>

Thanks for opening this! We will investigate. This penalty always existed in the past but we have a new-ish `ShutdownWorker` call and something may be amiss here.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-03-26 13:06:46.000 UTC</summary>

Just a heads up that we are looking into this. One issue we have found is that `ShutdownWorker` may not be working properly on cloud and is in the process of being fixed. However, this issue seems to state that you can replicate with a local dev server. We have replicated this and believe it may be a server-side bug. We will update when we have more details.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-03 18:35:02.000 UTC</summary>

Ok, we have done investigation on this and this is expected behavior, but the cosmetics of the task timeout could be improved and we have opened an issue on that (https://github.com/temporalio/temporal/issues/7566).

So what's happening in your code snippet is you are shutting down the worker but not start a new one. If you had a new worker you would see no penalty or task timeout. The task is showing as scheduled on the sticky queue but it's actually not on that queue necessarily since it has been shutdown, and if a worker were available listening on the normal queue, that would be the one that picked it up. The task timeout is only to re-schedule the task back on the normal queue explicitly instead of implicitly, and that only happens because there is no worker around.

So to recap, there is no runtime sticky penalty and the task scheduled on the sticky queue is actually effectively scheduled on both, and the only reason you are getting a task timeout is because there is no worker running for the on-both-queues task so it reschedules on. But there is no runtime behavior difference here.

Does this make sense? Can you confirm that this issue is only cosmetic in that you see a task timeout in history and does not have any bearing on the runtime behavior? Can you also confirm if you have another worker running instead of no worker running even the task timeout in history doesn't occur?

</details>

<details>
<summary><strong>gonced8</strong> commented on 2025-04-04 14:40:43.000 UTC</summary>

Yes, it makes sense and thank you for the explanation!
I tested locally by adding a second worker and it worked exactly as expected.
Thank you for your time!

Reactions: ðŸ‘ 1

</details>


---

<a id="716"></a>

### #716: [Bug] Test server for ARM64 not available (404)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/716 |
| **State** | CLOSED |
| **Author** | st3fan (Stefan Arentz) |
| **Created** | 2024-12-24 03:16:57.000 UTC (1 years ago) |
| **Updated** | 2025-02-06 22:48:06.000 UTC |
| **Closed** | 2025-02-06 22:48:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

On a Ubuntu 24.04 ARM64 host I am trying to run the unit test from https://github.com/temporalio/edu-102-python-code/blob/main/exercises/testing-code/solution/tests/test_workflow.py

It fails with `RuntimeError: Failed starting test server: HTTP status client error (404 Not Found) for url (https://temporal.download/temporal-test-server/default?arch=arm64&platform=linux&sdk-name=sdk-python&sdk-version=1.8.0)`  which looks like it is trying to download a version of the test server that does not exist.

The full output of the failing test is as follows:

```
runtime = <temporalio.bridge.runtime.Runtime object at 0xfc783c4bccb0>
config = TestServerConfig(existing_path=None, sdk_name='sdk-python', sdk_version='1.8.0', download_version='default', download_dest_dir=None, port=None, extra_args=[])

    @staticmethod
    async def start_test_server(
        runtime: temporalio.bridge.runtime.Runtime, config: TestServerConfig
    ) -> EphemeralServer:
        """Start a test server instance."""
        return EphemeralServer(
>           await temporalio.bridge.temporal_sdk_bridge.start_test_server(
                runtime._ref, config
            )
        )
E       RuntimeError: Failed starting test server: HTTP status client error (404 Not Found) for url (https://temporal.download/temporal-test-server/default?arch=arm64&platform=linux&sdk-name=sdk-python&sdk-version=1.8.0)

.venv/lib/python3.12/site-packages/temporalio/bridge/testing.py:67: RuntimeError
=================================================== short test summary info ===================================================
FAILED workflows_test.py::test_greet_workflow - RuntimeError: Failed starting test server: HTTP status client error (404 Not Found) for url (https://temporal.download/tem...
```


#### Comments (4)

<details>
<summary><strong>st3fan</strong> commented on 2024-12-24 03:20:56.000 UTC</summary>

I noticed that I am one SDK release behind, but changing the version number in the URL from `1.8.0` to `1.9.0` still results in a 404.

</details>

<details>
<summary><strong>st3fan</strong> commented on 2024-12-24 03:26:31.000 UTC</summary>

I tried `WorkflowEnvironment.start_time_skipping(test_server_existing_path="/usr/local/bin/temporal")`  but I think it expects a different kind of server as that will fail with `unknown command` errors from temporal.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-01-06 16:11:04.000 UTC</summary>

This is a known limitation with the time-skipping test server. Currently the time-skipping test server does not work on ARM. From the README at https://github.com/temporalio/sdk-python?tab=readme-ov-file#testing (added at #379):

> NOTE: The time-skipping test environment does not work on ARM. The SDK will try to download the x64 binary on macOS for use with the Intel emulator, but for Linux or Windows ARM there is no proper time-skipping test server at this time.

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-02-06 22:48:05.000 UTC</summary>

@st3fan it looks like this was solved/answered, so I'm going to close this. Let us know if there's more we can do.

</details>


---

<a id="697"></a>

### #697: [Feature Request] Handle edge case of recursive exceptions in failure converter

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/697 |
| **State** | OPEN |
| **Author** | yunmanger1 (German Ilin) |
| **Created** | 2024-12-03 21:56:44.000 UTC (1y 1m ago) |
| **Updated** | 2024-12-25 01:01:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

It is kind of an easy mistake to do in python:

```
try:
    raise ValueError("test")
except Exception as e:
    raise e from e
```

The re-raised exception will have `e.__cause__` referencing itself resulting in failure to serialize error and python sdk won't report anything resulting in retries and startToClose timeouts.

### Describe the solution you'd like

Handle a case  when `e.__cause__ == e` in DefaultFailureConverter. Created a dummy pull request #696

### Additional context

https://temporalio.slack.com/archives/CTT84RS0P/p1719632201314119

#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2024-12-03 22:01:04.000 UTC</summary>

> python sdk won't report anything

I think this is the primary bug. There are situations where recursion and stack overflow can occur in serialization, and the linked PR naively only helps the case of the cause being the same as itself (but if it was two causes deep that recursed, the issue would still appear). We just need to make it clear when it happens (or any other failure serialization happens).

If we are indeed not reporting anything, we need to fix that. #685 is related when failing to build a failure.

</details>

<details>
<summary><strong>yunmanger1</strong> commented on 2024-12-04 18:07:54.000 UTC</summary>

You are right that it doesn't solve all cycles. Not sure if your case happens in practice. What needs to happen to have an exception cycled after 2 or 3 steps?

Maybe the solution could be just have a max depth of 10 let's say. During serialization you truncate too  deep stacks anyway. 

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-12-04 19:25:20.000 UTC</summary>

I think we just need to log the failure of serializing this (assume there's a stack depth/overflow exception but is not visible due to #685)

</details>

<details>
<summary><strong>Ye123459</strong> commented on 2024-12-25 01:00:57.000 UTC</summary>

When handling exceptions in DefaultFailureConverter, to avoid e.__cause__ == e problems caused by reraising exceptions, you can take the following solutions:

Avoid using the from statement when catching and rethrowing exceptions:

Reraising the exception directly without specifying from avoids the problem of __cause__ pointing to itself.
Check and reset the __cause__ attribute:

After catching the exception, check the __cause__ attribute and reset it to None if it points to itself.
Custom exception handling logic:

Add logic to DefaultFailureConverter to ensure that __cause__ does not point to itself when handling exceptions.
Here is a specific code example:

class DefaultFailureConverter:
    def convert(self, exception):
        try:
            raise exception
        except Exception as e:
            if e.__cause__ is e:
                e.__cause__ = None
            raise e

</details>


---

<a id="666"></a>

### #666: [Feature Request] Worker instance lifespan

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/666 |
| **State** | CLOSED |
| **Author** | funkindy (Alexey Trofimov) |
| **Created** | 2024-10-10 17:51:16.000 UTC (1y 3m ago) |
| **Updated** | 2025-04-29 17:50:28.000 UTC |
| **Closed** | 2025-04-29 17:50:26.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

It would be nice to have some kind of lifespan for worker instances.

In my case i need shared http client (httpx.AsyncClient) for my activities, and i now don't see a proper place to call `await client.aclose()`. 

#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2024-10-10 18:24:34.000 UTC</summary>

Can you clarify the request a bit? A worker is running and using what it has until `run` returns (or if you're using `async with`, until that block is done). A worker's `run` does not return until it is shutdown and all activities are done (see https://github.com/temporalio/sdk-python?tab=readme-ov-file#worker-shutdown). You should add your client close after the worker `run` is complete.

</details>

<details>
<summary><strong>funkindy</strong> commented on 2024-10-14 08:18:10.000 UTC</summary>

`async with` has its own complex cleanup logic, which i have to reproduce in my scenario without context manager.

Why not to include the implementation of `__aexit__`  to some kind of `asynd def cleanup(...)`? Or maybe `worker.shutdown` is sufficient?

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-15 14:02:31.000 UTC</summary>

> Why not to include the implementation of __aexit__ to some kind of asynd def cleanup(...)? Or maybe worker.shutdown is sufficient?

Worker shutdown should be sufficient. If you add your cleanup code after worker is shutdown (or after context manager is complete), that is enough to guarantee the worker isn't using things anymore. A worker lifetime is the life of its run or context manager, both will not complete while any activities are still running (neither will the shutdown call), add any cleanup after that as needed.

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-04-29 17:50:27.000 UTC</summary>

Closing, but feel free to keep discussing or reopen @funkindy.

</details>


---

<a id="647"></a>

### #647: [Feature Request] OpenTelemetry Metrics OLTP/HTTP support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/647 |
| **State** | CLOSED |
| **Author** | slingshotsys (Geoff) |
| **Created** | 2024-09-23 17:24:47.000 UTC (1y 3m ago) |
| **Updated** | 2025-01-23 22:48:18.000 UTC |
| **Closed** | 2025-01-23 22:48:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Right now the Python SDK (and possibly other Core SDKs?) only support OLTP over gRPC, but some services (like Pydantic [Logfire](https://logfire.pydantic.dev/)) require [OLTP over HTTPS](https://opentelemetry.io/docs/specs/otlp/#otlphttp)

When trying to export metrics to these services today, I receive the following error:

`OpenTelemetry metrics error occurred. Metrics exporter otlp failed with the grpc server returns error (Internal error): , detailed error message: protocol error: received message with invalid compression flag: 60 (valid flags are 0 and 1) while receiving response with status: 403 Forbidden`

### Describe the solution you'd like

Support for exporting OTel Metrics to services via OLTP/HTTP.

### Additional context

This would bring the Python SDK up to parity with the Go SDK and possibly others which do support HTTP.


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2024-09-23 18:21:44.000 UTC</summary>

That is unfortunate that such a platform doesn't support such a common OTLP approach. This is implemented in our Rust Core layer, so I have opened an issue over there to support HTTP: https://github.com/temporalio/sdk-core/issues/820. Once updated there we will apply here (may just be an `http=True` type of option on the OTel config).

Reactions: ðŸŽ‰ 2

</details>

<details>
<summary><strong>samuelcolvin</strong> commented on 2024-09-23 20:46:00.000 UTC</summary>

Sorry temporal, we'll try to get grpc added as soon as possible.

Reactions: â¤ï¸ 2

</details>

<details>
<summary><strong>gregbrowndev</strong> commented on 2024-10-11 16:04:31.000 UTC</summary>

@slingshotvfx as a workaround, I would recommend deploying an OTEL collector which can receive grpc/http telemetry and export to Logfire via HTTP. This would be a better solution in the long run, as you ideally don't want to send telemetry directly to the backend via OTLP from your app. A collector allows you to add batching, retry/fault tolerance, and transformations to customise the metric labels. 

Here's a quick example in Docker Compose (not fully tested) using Grafana Alloy (I prefer it over the standard OTel collector for its UI and live debugging):

<details><summary>Details</summary>
<p>

_docker-compose.yaml_:

```yaml
services:
  alloy:
    image: grafana/alloy:v1.3.1
    environment:
      - OTLP_EXPORTER_ENDPOINT="https://log-fire.com/whatever"
    ports:
      - "4317:4317" # OTLP grpc receiver
      - "4318:4318" # OTLP http receiver
      - "12345:12345" # Debug UI
    volumes:
      - alloy_gateway:/var/lib/alloy/data
      - type: bind
        source: ./config.alloy
        target: /etc/alloy/config.alloy
    command:
      - "run"
      - "--server.http.listen-addr=0.0.0.0:12345"
      - "--storage.path=/var/lib/alloy/data"
      - "--stability.level=experimental"  # enables live debug mode
      - "/etc/alloy/config.alloy"
 ```

_config.alloy_:

```alloy
livedebugging {
	// Enable live debugging for the collector
	// See http://localhost:12345
	enabled = true
}

otelcol.receiver.otlp "default" {
	// Setup OTLP receiver to accept metrics, traces, and logs from instrumented services
	// https://grafana.com/docs/alloy/latest/reference/components/otelcol/otelcol.receiver.otlp/
	grpc {
		endpoint = "0.0.0.0:4317"
	}

	http {
		endpoint = "0.0.0.0:4318"
	}

	output {
		metrics = [otelcol.processor.batch.input]
		# traces  = []
		# logs    = []
	}
}

otelcol.processor.batch "metrics" {
	// Adds batching to metrics
	// https://grafana.com/docs/alloy/latest/reference/components/otelcol/otelcol.processor.batch/
	timeout = "200ms"

	output {
		metrics = [otelcol.exporter.otlphttp.logfire.input]
	}
}

// Can add additional components as needed

otelcol.exporter.otlphttp "logfire" {
	// Exports telemetry to another OTLP receiver via http
	// https://grafana.com/docs/alloy/latest/reference/components/otelcol/otelcol.exporter.otlphttp/
	client {
		endpoint = env("OTLP_EXPORTER_ENDPOINT")
		// You'll likely need to configure auth / tls to push to your backend
	}
}
```

Then you just need to configure your Python app to send telemetry to `http://alloy:4317` over grpc
</p>
</details> 



</details>

<details>
<summary><strong>cretz</strong> commented on 2025-01-15 13:39:05.000 UTC</summary>

Would recommend this be just a simple default-false `http` boolean on `OpenTelemetryConfig`

</details>


---

<a id="622"></a>

### #622: [Bug] unable to install temporal on python 3.11-alpine3.16

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/622 |
| **State** | OPEN |
| **Author** | hansraj1999 (Hansraj Deghun) |
| **Created** | 2024-08-26 12:11:27.000 UTC (1y 4m ago) |
| **Updated** | 2025-11-14 17:39:48.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 2 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 ðŸ‘€ 1 |

#### Description


Using cached temporalio-1.7.0.tar.gz (1.3 MB)
Installing build dependencies ... error
error: subprocess-exited-with-error
Ã— pip subprocess to install build dependencies did not run successfully.
â”‚ exit code: 1
â•°â”€> [10 lines of output]
Collecting poetry-core>=1.0.0
Using cached poetry_core-1.9.0-py3-none-any.whl.metadata (3.5 kB)
Collecting setuptools
Using cached setuptools-73.0.1-py3-none-any.whl.metadata (6.6 kB)
Collecting wheel
Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)
Collecting setuptools-rust
Using cached setuptools_rust-1.10.1-py3-none-any.whl.metadata (9.2 kB)
ERROR: Could not find a version that satisfies the requirement protoc-wheel-0 (from versions: none)
ERROR: No matching distribution found for protoc-wheel-0
end of output]

note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

base image python 3.11-alpine3.16
getting this error while running python -m pip install temporalio command



#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-08-26 12:25:10.000 UTC</summary>

We do not provide Alpine/musl builds in PyPI. You will have to build the SDK yourself. See https://github.com/temporalio/sdk-python?tab=readme-ov-file#building. I am a bit surprised that it is requiring protoc wheel still however.

Reactions: ðŸ‘ 1 ðŸ‘Ž 2 ðŸ˜• 1

</details>

<details>
<summary><strong>nebenwelt</strong> commented on 2025-11-14 17:39:48.000 UTC</summary>

> We do not provide Alpine

Very bad. Alpine is a very popular lightweight Python image.

</details>


---

<a id="578"></a>

### #578: activity returning dict object forcefully

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/578 |
| **State** | CLOSED |
| **Author** | MaazBinMusa (MaazBinMusa) |
| **Created** | 2024-07-12 00:11:45.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-12 18:16:57.000 UTC |
| **Closed** | 2024-07-12 17:17:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Im trying to return a dataclass object from an activity
-->

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

I have a workflow where I run 3 activities and I have defined dataclass types for each activity input output. The 3rd activity is acting uniquely strange, where eventhough I outline the activity to return a specific type of dataclass obj, it keeps returing the dict version of it.

I am forced to check after that activity if the obj is a dataclass instance or not, and if not, I have to make a new obj and initialize it with all the items in the dict that was returned.

### Additional context

I'm running the python version of temporal, on the cloud. We also have a codec server in the middle. I don't think the env has much to do with it since the first two activities work just fine but the last one is acting strange.

<!-- Add any other context about the problem here. -->


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2024-07-12 13:01:56.000 UTC</summary>

This is dependent upon how it is called in the workflow. The workflow `execute_activity` uses the activity's return type hint to know what to deserialize into, or if it doesn't have one, it just does default for JSON which is a `dict`. What does the signature of the activity look like and how are you calling it from a workflow?

(also, feel free to come join us on [Slack](https://t.mp/slack) in the `#python-sdk` channel for questions/help)

</details>

<details>
<summary><strong>MaazBinMusa</strong> commented on 2024-07-12 16:51:00.000 UTC</summary>

@cretz thank you for the prompt attention.

This is how I call the activity in the workflow
```
post_process_dto = await workflow.execute_activity(
      activity=activity_name,
      arg=post_process_dto,
      start_to_close_timeout=timedelta(seconds=10),
  )
```

And this is the activity itself

```
async def postprocess_find_confidence(self, dto: PostAbstractionDTO) -> PostAbstractionDTO:
        return dto
```




</details>

<details>
<summary><strong>cretz</strong> commented on 2024-07-12 17:01:15.000 UTC</summary>

If you call the activity via a string name, the code does not know the type to deserialize to, so it defaults to whatever JSON does (which is dict in this case). You should add `result_type=PostAbstractionDTO` or, even better, call it via `execute_activity_method(MyActivityClass.postprocess_find_confidence, ...` where the return type can be extracted from the signature.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>MaazBinMusa</strong> commented on 2024-07-12 17:17:53.000 UTC</summary>

specifying the return type fixed the issue ðŸ‘ Thankyou!

</details>


---

<a id="448"></a>

### #448: [Bug] Cut a new release please

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/448 |
| **State** | CLOSED |
| **Author** | arlyon (Alexander Lyon) |
| **Created** | 2023-12-14 10:02:19.000 UTC (2 years ago) |
| **Updated** | 2024-01-26 19:21:33.000 UTC |
| **Closed** | 2024-01-26 19:21:33.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 2 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### What are you really trying to do?

Let me just prefix this as this is a raw stream of consciousness to help illustrate the perspective of an outside user. Thanks for all the great work you guys do (especially how open it is) :heart: 

Hello, this was my experience trying to update temporal to 1.4.0 and noticing all my pytest tests failing (since it fails tests with deprecation warnings by default) due to typed search attributes

https://github.com/temporalio/features/issues/336#issuecomment-1855475624

### Describe the bug

After running around trying to find an example to update my code, I ended up hitting a bug that was fixed in https://github.com/temporalio/sdk-python/commit/6f966c7ca59d01c02243561d186867e199f252a6

### Minimal Reproduction

N/A

### Environment/Versions
N/A

### Additional context

Updating the python samples and a longer transition period before marking old apis as deprecated would be nice. Even just one release to massage any bugs. Now I have a half migrated codebase which is just annoying.


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-01-03 17:34:27.000 UTC</summary>

:+1: I agree. We will cut a release soon.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-01-26 19:21:33.000 UTC</summary>

1.5.0 was released

Reactions: ðŸŽ‰ 1

</details>


---

<a id="445"></a>

### #445: [Bug] delete_workflow_execution not properly available on workflow_service

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/445 |
| **State** | CLOSED |
| **Author** | RonaldGalea (Raul Ronald Galea) |
| **Created** | 2023-12-11 08:50:01.000 UTC (2y 1m ago) |
| **Updated** | 2024-02-27 15:12:48.000 UTC |
| **Closed** | 2024-02-27 15:12:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently getting: `ValueError: Unknown RPC call delete_workflow_execution`


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2023-12-11 13:39:03.000 UTC</summary>

Thanks! It looks like mistakenly placed this on `operator_service` on our Rust side, we will fix.

Note, this is a low-level call and we don't recommend deleting workflow executions. We recommend just leaving them as complete. Feel free to join us at `#python-sdk` on [Slack](https://t.mp/slack) or the forums to discuss the use case for deletion and if it's really needed.



</details>

<details>
<summary><strong>RonaldGalea</strong> commented on 2023-12-12 13:45:04.000 UTC</summary>

Thanks for the heads-up! For our setup we actually plan to let workflows fail after a reasonable retry chance fails - as we assume this requires manual intervention and we need to be aware/get notified of these failures.

Later, once the fix is deployed, the workflow is reset. If the reset run succeeds, the old failed run is deleted

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-12-12 17:48:09.000 UTC</summary>

There is no need to delete the old run. Non-running workflows can just live until retention is up.

</details>

<details>
<summary><strong>RonaldGalea</strong> commented on 2023-12-12 18:00:23.000 UTC</summary>

No need, but it's for organization. If we have failed workflows (which is very rare), it means someone needs to look at them. 

Deletion is just for maintaining these semantics and avoiding the situation where there are multiple failed workflows, some were handled and some were not, and it's not immediately clear which need attention.


</details>


---

<a id="383"></a>

### #383: [Feature Request] Provide last result and last failure to workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/383 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-09-05 19:26:21.000 UTC (2y 4m ago) |
| **Updated** | 2025-09-04 16:38:12.000 UTC |
| **Closed** | 2025-09-04 16:38:12.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 2 |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/635


---

<a id="333"></a>

### #333: [Bug] BaseException exceptions are not caught when raised in an activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/333 |
| **State** | CLOSED |
| **Author** | joshua-auchincloss (Joshua Auchincloss) |
| **Created** | 2023-06-23 13:43:42.000 UTC (2y 6m ago) |
| **Updated** | 2023-07-18 11:07:18.000 UTC |
| **Closed** | 2023-07-18 11:07:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?
- raise an exception subclassed from BaseException in an activity


### Describe the bug

The bug occurs when raising custom exceptions during an activity. the system raises ```TypeError -> exception causes must derive from BaseException```, which overshadows the actual error


### Minimal Reproduction

```py
....

# also with normal Exception
class MyException(BaseException):
    def __init__(self, *args):
        super().__init__(*args)

@activity.defn
async def my_act():
    raise MyException("abc") from None


@workflow.defn
class Workflow:
    @workflow.run
    async def run(self):
          workflow.start_activity(
            my_act,
            ....
          )

```

### Environment/Versions
- OS and processor: m1 mac
- Temporal Version: temporal @ 1.20.2, python sdk @ 1.2.0
- Are you using Docker or Kubernetes or building Temporal from source? docker + build from wheel

#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2023-06-23 15:26:30.000 UTC</summary>

I think you've written invalid Python code. I haven't tested yet, but I think `super().__init__(self, *args)` should be `super().__init__(*args)`.

</details>

<details>
<summary><strong>joshua-auchincloss</strong> commented on 2023-06-23 15:28:47.000 UTC</summary>

> I think you've written invalid Python code. I haven't tested yet, but I think `super().__init__(self, *args)` should be `super().__init__(*args)`.

Ahh this one's a typo - on mobile for the next few days ðŸ˜…. Apologies, amending - the source is the later `super().__init__(*args)`, which is the condition for the issue presenting 

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-06-23 20:01:00.000 UTC</summary>

Ok, I see the issue. We intentionally only catch `Exception` and its derivatives, not `BaseException`. From https://docs.python.org/3/library/exceptions.html:

> programmers are encouraged to derive new exceptions from the [Exception](https://docs.python.org/3/library/exceptions.html#Exception) class or one of its subclasses, and not from [BaseException](https://docs.python.org/3/library/exceptions.html#BaseException).

And from https://docs.python.org/3/tutorial/errors.html#handling-exceptions:

> [Exception](https://docs.python.org/3/library/exceptions.html#Exception)s which are not subclasses of Exception are not typically handled, because they are used to indicate that the program should terminate.

I'm not sure we want to trap `BaseException`, but maybe we do. Having said that, we don't want to silently swallow it either (which in effect is what is happening today), so should we crash the entire worker for a base exception?

In the meantime, please derive from `Exception` and not `BaseException` in Python. And if another library unfortunately is using `BaseException` as their base, you will have to catch and rethrow as a normal exception in your activity. But I'll leave this issue open because we should not silently swallow `BaseException`.

</details>

<details>
<summary><strong>joshua-auchincloss</strong> commented on 2023-06-23 22:10:43.000 UTC</summary>

> Ok, I see the issue. We intentionally only catch `Exception` and its derivatives, not `BaseException`. From https://docs.python.org/3/library/exceptions.html:
> 
> 
> 
> > programmers are encouraged to derive new exceptions from the [Exception](https://docs.python.org/3/library/exceptions.html#Exception) class or one of its subclasses, and not from [BaseException](https://docs.python.org/3/library/exceptions.html#BaseException).
> 
> 
> 
> And from https://docs.python.org/3/tutorial/errors.html#handling-exceptions:
> 
> 
> 
> > [Exception](https://docs.python.org/3/library/exceptions.html#Exception)s which are not subclasses of Exception are not typically handled, because they are used to indicate that the program should terminate.
> 
> 
> 
> I'm not sure we want to trap `BaseException`, but maybe we do. Having said that, we don't want to silently swallow it either (which in effect is what is happening today), so should we crash the entire worker for a base exception?
> 
> 
> 
> In the meantime, please derive from `Exception` and not `BaseException` in Python. And if another library unfortunately is using `BaseException` as their base, you will have to catch and rethrow as a normal exception in your activity. But I'll leave this issue open because we should not silently swallow `BaseException`.

Gotcha - yep it's an upstream `BaseException`, no issues using try-except so thanks for the quick workaround :)

IMO, crashing the worker seems overkill - maybe a `logging.warn`? Either way, you guys do awesome work so thanks for the help

Reactions: ðŸ‘ 1

</details>


---

<a id="322"></a>

### #322: [Bug] Python generics for function type hints

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/322 |
| **State** | CLOSED |
| **Author** | cloud-sauce |
| **Created** | 2023-05-24 20:43:26.000 UTC (2y 7m ago) |
| **Updated** | 2023-06-15 19:05:45.000 UTC |
| **Closed** | 2023-06-15 19:05:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

I would like to have a generic container that is used as the type hint for workload and activity functions, for example,

```python
from dataclasses import dataclass
from typing import Generic, TypeVar

VariableType = TypeVar("VariableType")


@dataclass
class MyContainer(Generic[VariableType]):
    common: int
    variable: VariableType
```

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

When a generic container is used the following issue occurs,

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

```
Traceback (most recent call last):
  File "/home/.../.cache/pypoetry/virtualenvs/temporalio-samples-XXX-py3.10/lib/python3.10/site-packages/temporalio/worker/_activity.py", line 350, in _run_activity
    else await self._data_converter.decode(
  File "/home/.../.cache/pypoetry/virtualenvs/temporalio-samples-XXX-py3.10/lib/python3.10/site-packages/temporalio/converter.py", line 1000, in decode
    return self.payload_converter.from_payloads(payloads, type_hints)
  File "/home/.../.cache/pypoetry/virtualenvs/temporalio-samples-XXX-py3.10/lib/python3.10/site-packages/temporalio/converter.py", line 246, in from_payloads
    values.append(converter.from_payload(payload, type_hint))
  File "/home/.../.cache/pypoetry/virtualenvs/temporalio-samples-XXX-py3.10/lib/python3.10/site-packages/temporalio/converter.py", line 508, in from_payload
    obj = value_to_type(type_hint, obj, self._custom_type_converters)
  File "/home/.../.cache/pypoetry/virtualenvs/temporalio-samples-XXX-py3.10/lib/python3.10/site-packages/temporalio/converter.py", line 1394, in value_to_type
    raise TypeError(f"Unserializable type during conversion: {hint}")
TypeError: Unserializable type during conversion: __main__.ComposeGreetingInput[int]
```

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

The error can be reproduced using the [`hello_activity`](https://github.com/temporalio/samples-python/blob/main/hello/hello_activity.py) example with the modifications,

```python
from typing import Generic, TypeVar

BonusType = TypeVar("BonusType")


@dataclass
class ComposeGreetingInput(Generic[BonusType]):
    greeting: str
    name: str
    bonus: BonusType


# Basic activity that logs and does string concatenation
@activity.defn
async def compose_greeting(input: ComposeGreetingInput[int]) -> str:
    activity.logger.info("Running activity with parameter %s" % input)
    return f"{input.greeting}, {input.name}! Bonus info: {input.bonus}"


# Basic workflow that logs and invokes an activity
@workflow.defn
class GreetingWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        workflow.logger.info("Running workflow with parameter %s" % name)
        return await workflow.execute_activity(
            compose_greeting,
            ComposeGreetingInput("Hello", name, 123),
            start_to_close_timeout=timedelta(seconds=10),
        )
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: WSL (Ubuntu 20.04 LTS)
- Temporal Version: Python SDK version `1.2.0` and services [from template](https://github.com/temporalio/docker-compose/blob/main/docker-compose-cass-es.yml) (`1.20.2`)
- Are you using **Docker** or Kubernetes or building Temporal from source? (Docker compose)

### Additional context

<!-- Add any other context about the problem here. -->

I realize the problem could be solved by child class definitions with static type hints, ex.

```python
@dataclass
class IntegerGreeting(ComposeGreetingInput):
    bonus: int


# Basic activity that logs and does string concatenation
@activity.defn
async def compose_greeting(input: IntegerGreeting) -> str:
```

but it would be very nice to be able to use generic typing




#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2023-05-24 22:12:19.000 UTC</summary>

From the README:

> Classes with generics may not have the generics properly resolved. The current implementation, similar to Pydantic, does not have generic type resolution. Users should use concrete types.

It is much too complicated to solve this with any kind of consistency.  We could resolve simple generic types, but having a bunch of rules for the user on what is and isn't allowed with generics is more difficult than disallowing. We would request you make specific non-generic concrete dataclasses for specific use cases instead of reusing/creating generic ones.

</details>

<details>
<summary><strong>cloud-sauce</strong> commented on 2023-05-25 15:39:49.000 UTC</summary>

What about adding a flag that would *allow* generics, but only support JSON-serializable types for them? For example, `BonusType` (from above) would be restricted to `float`, `int`, `string`, or `bool`. 

Or is there a comment or documentation around what the underlying issue with allowing them is? Is it a problem with accessing the type hinted class in the consumer? I'm interested in better understanding the limitation, if you don't mind

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-25 15:48:26.000 UTC</summary>

It's not that we _couldn't_ do it, it's that there is limited benefit.

The main limitation is that such limited support adds limited value and more confusion (the next issue will be opened asking why we don't support other generic ways and so on ad infinitum). You can just as easily create specialized data classes for the few primitives needed rather than altering the SDK to support such a limited subset of generics. We also encourage people to build dataclasses specifically for Temporal use instead of trying to reuse other ones that may not have the same constraints (e.g. JSON serializable, prefer smaller in size, etc). We don't support generic activity functions or workflow classes either.

If it were hard to create non-generic forms of these models, it'd be worth implementing. Similarly if Python had a built in way to serialize dataclasses to/from JSON with generic support we'd be happy to reuse it as we do in SDK languages where that exists.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-06-15 19:05:39.000 UTC</summary>

Closing as intentionally unsupported (and documented as such)

</details>


---

<a id="317"></a>

### #317: [Bug] await call in async query function would cause the function to return immediately and cause an error on client side

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/317 |
| **State** | CLOSED |
| **Author** | orby-will (Will Lu) |
| **Created** | 2023-05-15 22:43:22.000 UTC (2y 7m ago) |
| **Updated** | 2023-05-30 12:14:53.000 UTC |
| **Closed** | 2023-05-22 19:13:01.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Use "await" in workflow.query decorated async function. In the SDK documentation, it clearly says that the async function is expected. If async functions are expected, I assume that I should be able to expect await to work properly

### Describe the bug

Await call in async query function would cause the function to return immediately and cause an error on the client side.

Code:

>     @workflow.query
>     async def get_suggestion(self) -> str:
>         await workflow.wait_condition(
>             lambda: self.orbflow.suggested_action != "" or self._exit.is_set())
>         return self.orbflow.suggested_action

Error Msg and Stack Trace

> Traceback (most recent call last):
>   File "/Users/ldxstc/orby/automl/env/lib/python3.10/site-packages/temporalio/service.py", line 723, in _rpc_call
>     resp = await client.call(
>   File "/Users/ldxstc/orby/automl/env/lib/python3.10/site-packages/temporalio/bridge/client.py", line 126, in call
>     result = await resp_fut
> temporal_sdk_bridge.RPCError: (3, 'Query response was empty', b'\x08\x03\x12\x18Query response was empty\x1aE\nCtype.googleapis.com/temporal.api.errordetails.v1.QueryFailedFailure')
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File "/Users/ldxstc/orby/automl/env/lib/python3.10/site-packages/temporalio/client.py", line 2465, in query_workflow
>     resp = await self._client.workflow_service.query_workflow(
>   File "/Users/ldxstc/orby/automl/env/lib/python3.10/site-packages/temporalio/service.py", line 658, in __call__
>     return await self.service_client._rpc_call(
>   File "/Users/ldxstc/orby/automl/env/lib/python3.10/site-packages/temporalio/service.py", line 738, in _rpc_call
>     raise RPCError(message, RPCStatusCode(status), details)
> temporalio.service.RPCError: Query response was empty
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 44, in run
>     return loop.run_until_complete(main)
>   File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py", line 646, in run_until_complete
>     return future.result()
>   File "/Users/ldxstc/orby/automl/scripts/run_workflow_orbot.py", line 166, in main
>     await browser_loop(crawler, workflow_handle)
>   File "/Users/ldxstc/orby/automl/scripts/run_workflow_orbot.py", line 94, in browser_loop
>     gpt_action = await workflow_handle.query(
>   File "/Users/ldxstc/orby/automl/env/lib/python3.10/site-packages/temporalio/client.py", line 1240, in query
>     return await self._client._impl.query_workflow(
>   File "/Users/ldxstc/orby/automl/env/lib/python3.10/site-packages/temporalio/client.py", line 2472, in query_workflow
>     raise WorkflowQueryFailedError(err.message)
> temporalio.client.WorkflowQueryFailedError: Query response was empty

### Minimal Reproduction
>     @workflow.query
>     async def get_suggestion(self) -> str:
>         await asyncio.sleep(1)
>         return self.orbflow.suggested_action


### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: M1 Mac,
- Temporal Version: temporalio==1.0.0
- Are you using Docker or Kubernetes or building Temporal from source? -> docker

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2023-05-15 22:48:27.000 UTC</summary>

Queries need to be read-only with no side effects, so they should never await on anything Temporal. Ideally you would not use an async query function, but we made it available because there are plenty of Python async def functions that are still side-effect free. In other SDKs we disallow async queries to prevent users from doing what you're doing.

You started a timer in a query which is disallowed. If/when we get a static analyzer, it would catch things like this. In the meantime, refer to https://docs.temporal.io/workflows#query:

> A Query must never mutate the state of the Workflow Executionâ€”that is, Queries are read-only and cannot contain any blocking code. This means, for example, that Query handling logic cannot schedule Activity Executions.

Or in this case, timers. We mention in README and docstring of query decorator that queries cannot mutate workflow state (which starting a timer does). We can probably add more docs in the README to make this clearer that's what we mean by "mutate workflow state".

We might also consider a warning for any `async def` queries.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-22 19:13:01.000 UTC</summary>

Closing as question. Just because `async def` allowed for query doesn't mean query can do async _workflow_ calls, just that if there is a Python library being used in a query function with a fast async call it will work.

</details>

<details>
<summary><strong>orby-will</strong> commented on 2023-05-28 01:24:18.000 UTC</summary>

Thanks for replying. But can you please explain more about why "starting a timer" mutates a workflow state?

How about waiting for a state to become true, such as "await workflow.wait_condition(xxx)"? It does not work either. How is it mutating the state of a workflow?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-30 12:14:33.000 UTC</summary>

> But can you please explain more about why "starting a timer" mutates a workflow state?

Temporal workflows are backed by a history, and starting a timer writes an event to that history. It's how timers can be durable.

> How is it mutating the state of a workflow?

While that isn't mutating the state, that is asynchronous operation that hangs a query open which we also do not allow. Queries have to be immediate and read-only. They cannot call asynchronous `workflow` module methods. See https://docs.temporal.io/workflows#query. We allow `async def` on the query because some Python calls that are immediate are still marked with that so it's still effectively synchronous.

</details>


---

<a id="272"></a>

### #272: Temporal CLI based ephemeral server

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/272 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2023-02-12 23:25:09.000 UTC (2y 11m ago) |
| **Updated** | 2023-07-07 13:41:13.000 UTC |
| **Closed** | 2023-07-07 13:41:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

*No description provided.*

#### Comments (4)

<details>
<summary><strong>bergundy</strong> commented on 2023-02-12 23:27:42.000 UTC</summary>

Depends on https://github.com/temporalio/sdk-core/issues/485

Consider maintaining support for Temporalite for a while.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-04-28 14:42:20.000 UTC</summary>

This is also needed for some patch fixes that use the SDK metadata field not present in older Temporalite

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-09 19:11:29.000 UTC</summary>

Make sure to accept additional namespaces here beyond the primary one.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-06-14 18:18:11.000 UTC</summary>

This cannot be done until https://github.com/temporalio/temporal/pull/4317 is in the Temporal CLI

</details>


---

<a id="262"></a>

### #262: [Feature Request] Reset Workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/262 |
| **State** | CLOSED |
| **Author** | bbkgh (Babak Ghadiri) |
| **Created** | 2023-01-28 18:30:27.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-09 15:42:16.000 UTC |
| **Closed** | 2023-02-07 13:50:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi. Is there any way/code snippet to reset workflows in python sdk?

like this question
[https://community.temporal.io/t/how-do-i-restart-an-opened-workflow/1044/2](https://community.temporal.io/t/how-do-i-restart-an-opened-workflow/1044/2)

https://docs.temporal.io/tctl-v1/workflow#reset

#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2023-01-30 13:59:14.000 UTC</summary>

There are not any samples of this in any of our SDKs (that I know of) because this is an advanced concept. If you don't want to use the CLI, _technically_ you can call `await my_client.workflow_service.reset_workflow_execution(temporalio.api.workflowservice.v1.ResetWorkflowExecutionRequest(...` which would be calling the [raw API](https://github.com/temporalio/api/blob/386b7cf9dccba864bdb38f28699f216a0f3fb2ca/temporal/api/workflowservice/v1/service.proto#L246-L251) with a [raw API request](https://github.com/temporalio/api/blob/386b7cf9dccba864bdb38f28699f216a0f3fb2ca/temporal/api/workflowservice/v1/request_response.proto#L621-L632). But that is a bit of an advanced API.

Resets are meant to be rarely used, not necessarily part of normal code, so we may not provide a common code sample for this.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-07 13:50:24.000 UTC</summary>

Closing as answered

</details>

<details>
<summary><strong>bbkgh</strong> commented on 2023-02-09 15:11:40.000 UTC</summary>

Thanks for your help. I found that there is a "reset_type" in tctl (that can be LastContinuedAsNew) but it's not supported in ResetWorkflowExecutionRequest structure. what is reason of this mismatch? (background conversion in tctl or a missing field in proto or ...)

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-09 15:42:15.000 UTC</summary>

That is used to crawl the history to find a finishing task ID to reset to. See https://github.com/temporalio/tctl/blob/d124bd56ba6bce7d6b6b90decb5093d4c007c653/cli_curr/workflowCommands.go#L1438-L1490 and https://github.com/temporalio/tctl/blob/d124bd56ba6bce7d6b6b90decb5093d4c007c653/cli_curr/workflowCommands.go#L1796-L1824 and associated code.

Reactions: ðŸ‘ 1

</details>


---

<a id="220"></a>

### #220: [Bug] Conflict using temporal-sdk with loguru

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/220 |
| **State** | CLOSED |
| **Author** | northpowered (Oleg Romanov) |
| **Created** | 2022-12-04 19:57:11.000 UTC (3y 1m ago) |
| **Updated** | 2022-12-05 13:18:59.000 UTC |
| **Closed** | 2022-12-05 13:18:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?
I`m trying to use [Loguru](https://github.com/Delgan/loguru) module with temporal-sdk code

OS: Ubuntu 22.04 LTS 5.15.0-33-generic

- python = 3.10.4
- temporalio = "0.1b3"
- loguru = "0.6.0"

```python
from temporalio.client import Client
from temporalio.worker import Worker
from loguru import logger

#  Some code
```
or 

```python3
from temporalio import workflow, activity
from loguru import logger

# Some code
```
### Describe the bug

Imports like these raise this:


<details>
<summary> Long traceback, CLICK to expand</summary>
Traceback (most recent call last):

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/_workflow.py", line 113, in __init__
    workflow_runner.prepare_workflow(defn)

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 53, in prepare_workflow
    self.create_instance(

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 87, in create_instance
    return _Instance(det, self._runner_class, self._restrictions)

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 107, in __init__
    self._create_instance()

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 118, in _create_instance
    self._run_code(

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 160, in _run_code
    exec(code, self.globals_and_locals, self.globals_and_locals)

  File "<string>", line 2, in <module>

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 201, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)

  File "<frozen importlib._bootstrap>", line 1129, in __import__

  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import

  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load

  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked

  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed

  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import

  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load

  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked

  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked

  File "<frozen importlib._bootstrap_external>", line 883, in exec_module

  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed

  File "/home/northstar/projects/alif-interactor/source/temporal_runner/__init__.py", line 1, in <module>
    from .factory import Temporal

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 201, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)

  File "<frozen importlib._bootstrap>", line 1133, in __import__

  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import

  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load

  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked

  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked

  File "<frozen importlib._bootstrap_external>", line 883, in exec_module

  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed

  File "/home/northstar/projects/alif-interactor/source/temporal_runner/factory.py", line 3, in <module>
    from loguru import logger

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 201, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)

  File "<frozen importlib._bootstrap>", line 1129, in __import__

  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import

  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load

  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked

  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked

  File "<frozen importlib._bootstrap_external>", line 883, in exec_module

  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/loguru/__init__.py", line 10, in <module>
    from ._logger import Core as _Core

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 201, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)

  File "<frozen importlib._bootstrap>", line 1133, in __import__

  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import

  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load

  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked

  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked

  File "<frozen importlib._bootstrap_external>", line 883, in exec_module

  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/loguru/_logger.py", line 92, in <module>
    from ._datetime import aware_now

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 398, in __call__
    return self.current(*args, **kwargs)

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_importer.py", line 201, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)

  File "<frozen importlib._bootstrap>", line 1133, in __import__

  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import

  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load

  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked

  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked

  File "<frozen importlib._bootstrap_external>", line 883, in exec_module

  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/loguru/_datetime.py", line 12, in <module>
    class datetime(datetime_):

TypeError: _RestrictedProxy.__init__() missing 1 required positional argument: 'matcher'


The above exception was the direct cause of the following exception:


Traceback (most recent call last):

  File "/home/northstar/projects/alif-interactor/source/main.py", line 13, in <module>
    app()  # pragma: no cover

  File "/home/northstar/projects/alif-interactor/source/cli/run.py", line 14, in workflows
    temporal.start_worker_sync(

  File "/home/northstar/projects/alif-interactor/source/temporal_runner/factory.py", line 39, in start_worker_sync
    w = asyncio.run(generator)

  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)

  File "/usr/lib/python3.10/asyncio/base_events.py", line 646, in run_until_complete
    return future.result()

  File "/home/northstar/projects/alif-interactor/source/temporal_runner/factory.py", line 66, in create_workflow_worker
    return Worker(

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/_worker.py", line 261, in __init__
    self._workflow_worker = _WorkflowWorker(

  File "/home/northstar/.cache/pypoetry/virtualenvs/alif-interactor-tmPkMBix-py3.10/lib/python3.10/site-packages/temporalio/worker/_workflow.py", line 117, in __init__
    raise RuntimeError(f"Failed validating workflow {defn.name}") from err

RuntimeError: Failed validating workflow FooFlow
</details>

Some notes:

- I`d never seen conflicts like this
- Loguru fails only in code file with temporal-sdk imports
- This exception rises [THIS](https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/_workflow.py#L116) validation exception
- Using Loguru in other modules WITHOUT temporal code, but in THIS virtualenv is ok
 
For example (this is working fine)
```python
import typer
from cli import run_app  # Importing module with temporal code
from utils.logger import logger  # Importing loguru


app = typer.Typer(no_args_is_help=True, help="add help")
app.add_typer(run_app, name='run')



if __name__ == "__main__":
    logger.info('start')  
    app() 
```


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2022-12-05 13:03:46.000 UTC</summary>

This is due to a known issue with the sandbox when imports extend restricted classes like `datetime`. See https://github.com/temporalio/sdk-python/tree/0.1b3#extending-restricted-classes. Hopefully this will be fixed in a release coming soon, but in the meantime, you need to mark `loguru` as a "passthrough module" (https://github.com/temporalio/sdk-python/tree/0.1b3#passthrough-modules), e.g.:

```python
my_restrictions = dataclasses.replace(
    SandboxRestrictions.default,
    passthrough_modules=SandboxRestrictions.passthrough_modules_default | SandboxMatcher(access={"loguru"}),
)
my_worker = Worker(..., workflow_runner=SandboxedWorkflowRunner(restrictions=my_restrictions))
```

We have made this easier in `main` which should be released shortly.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>northpowered</strong> commented on 2022-12-05 13:15:07.000 UTC</summary>

@cretz This works great, tnx a lot!
Adding some modules to custom restrictions should look like this?
```python
access={"loguru","pydantic"}
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-12-05 13:17:14.000 UTC</summary>

Yes, but that is going to change in a couple of days :-) See the equivalent section on `main` at https://github.com/temporalio/sdk-python#passthrough-modules (though hopefully this bug won't surface unless you actually try to _use_ the logger in the workflow file/class which you probably shouldn't).

</details>

<details>
<summary><strong>northpowered</strong> commented on 2022-12-05 13:18:59.000 UTC</summary>

Thanx, we`re not using logging right in the workflows :-)

Closed

</details>


---

<a id="213"></a>

### #213: [Feature Request] Confirm vscode debuggability within sandbox

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/213 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-28 12:52:15.000 UTC (3y 1m ago) |
| **Updated** | 2022-11-28 12:52:15.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 2 |

#### Description

### Is your feature request related to a problem? Please describe.

User reported issue debugging while in sandbox


---

<a id="153"></a>

### #153: [Feature Request] Rework search attributes and add list endpoint to client

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/153 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-13 12:25:51.000 UTC (3y 2m ago) |
| **Updated** | 2023-10-20 12:46:25.000 UTC |
| **Closed** | 2023-10-20 12:46:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

* Need to support individual, per-type search attributes and only allow collections for keywords
* Expose workflow listing API on client which should be a lazy iterator

See https://github.com/temporalio/proposals/pull/67

#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2022-11-03 14:35:58.000 UTC</summary>

(accidentally closed, reopened)

</details>

<details>
<summary><strong>bergundy</strong> commented on 2022-11-03 15:27:54.000 UTC</summary>

@cretz this should still be open, we need to rework search attributes too.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-03 15:51:24.000 UTC</summary>

Sorry, I said I reopened but closed again by accident

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-10-20 12:46:25.000 UTC</summary>

This is now a duplicate of #381 (and #366)

</details>


---

<a id="128"></a>

### #128: Add support for Pydantic's secret types

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/128 |
| **State** | CLOSED |
| **Author** | averri (Alexandre Verri) |
| **Created** | 2022-09-07 17:33:03.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-07 20:31:47.000 UTC |
| **Closed** | 2022-09-07 20:31:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

I would like to suggest adding support for serializing/deserializing Pydantic's secret types, like `SecretStr` and `SecretField`.
More information at: https://pydantic-docs.helpmanual.io/usage/types/#secret-types

### Describe the solution you'd like

Considering that Temporal logs all input and output arguments of activities and workflows, this is considered a security issue when sensitive information is present. Sensitive information must be protected from viewing in the Temporal UI and in the log files. The Pydantic's secret types already work fine with standard logging, but they fail to work with Temporal Python SDK because some SDK components do not know how to serialize/deserialize the secret types.

#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2022-09-07 17:52:12.000 UTC</summary>

Temporal is a general purpose system and it does not currently have a way to exclude certain pieces of a payload from visualizing in the UI. This is not Python specific.

If you want data to be hidden from Temporal, we suggest end-to-end encryption. I am actively working on https://github.com/temporalio/samples-python/issues/5 to show how to do this in Python. But there is no way to say you only want some nested part of some payload to be hidden in some cases across languages but still exist as visible in the DB.

</details>

<details>
<summary><strong>averri</strong> commented on 2022-09-07 18:36:34.000 UTC</summary>

That is the purpose of the `SecretStr` from Pydantic. If we use a class derived from `BaseModel` using fields of type `SecretStr`, those fields will appear in the Temporal UI with asterisks. See example:

```python
from pydantic import SecretStr, BaseModel
from hashlib import sha256

if __name__ == '__main__':

    class Signup(BaseModel):
        username: str
        clear_password: SecretStr

        @property
        def encrypted_password(self):
            return sha256(self.clear_password.get_secret_value().encode("utf-8")).hexdigest()


    class User(BaseModel):
        username: str
        encrypted_password: str

    # This data comes from a REST call, for example.
    signup = Signup(username="Alex", clear_password="12345678")
    
    # Printing sensitive data
    print(f"signup: {signup}")

    # Using sensitive data, creating a user to store in the database.
    user = User(**signup.dict(), encrypted_password=signup.encrypted_password)

    print(f"user: {user}")
```

Running the above code produces:

```
signup: username='Alex' clear_password=SecretStr('**********')
user: username='Alex' encrypted_password='ef797c8118f02dfb649607dd5d3f8c7623048c9c063d532cc95c5ed7a898a64f'
```

We can see that no sensitive information appears in the logs, nor in the entities used to store information in the database (the `User` class can be used as a DB entity). This is just an example, in real apps, I also use the encrypted password as a `SecretStr`.

I have tested your other PR that fixes the support for Pydantic classes, it's working fine, it's just missing the Pydantic's secret types. 

As a workaround, I believe I can use a custom data converter. In the method `connect` of the `Client` class, there is a named argument called `data_converter`. Do you have an example of how to add the support for custom converters using this `data_converter`?

That example (https://github.com/temporalio/samples-python/issues/5) involving encryption is a great thing, thank you for that.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-09-07 20:24:31.000 UTC</summary>

It only produces that in Python using Python constructs. The Temporal server is not written in Python and does not know anything about Pydantic or other Python-specific constructs. The way Temporal works is we serialize all input/output as JSON (usually). It is not specific to a language and many times users build things across languages. We do not have Temporal support for excluding specific parts of a payload currently.

You can't take the Pydantic/Python-specific things and apply them generically to the Temporal server or other languages. The input/output will be JSON that can be deserialized by all languages, so if you want a piece encrypted/hidden, you'll have to manually do so.

> Do you have an example of how to add the support for custom converters using this data_converter?

I do not have an example yet, I have created https://github.com/temporalio/samples-python/issues/15. Basically you'd create a custom payload converter that you'd either create a data converter with or add it to a composite payload converter on top of the existing ones.

</details>

<details>
<summary><strong>averri</strong> commented on 2022-09-07 20:31:47.000 UTC</summary>

I am going to close this issue, as the solution for handling sensitive information in Temporal is to use the converters. Thanks for the https://github.com/temporalio/samples-python/issues/15.

</details>


---

<a id="126"></a>

### #126: The SDK is failing to serialize objects from Pydantic classes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/126 |
| **State** | CLOSED |
| **Author** | averri (Alexandre Verri) |
| **Created** | 2022-09-07 12:07:25.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-21 01:58:52.000 UTC |
| **Closed** | 2022-09-20 15:37:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?
I'm trying to run a workflow passing an object of a class that is derived from Pydantic's `BaseModel`.

### Describe the bug
When running the workflow, the following exception happens:

```
Object of type SignupCreate is not JSON serializable
```

More details:
```
"  File \"C:\\Users\\alexa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\master-control-fastapi-xNDFbqbb-py3.10\\lib\\site-packages\\temporalio\\converter.py\", line 177, in to_payloads\n    payload = converter.to_payload(value)\n", "  File \"C:\\Users\\alexa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\master-control-fastapi-xNDFbqbb-py3.10\\lib\\site-packages\\temporalio\\converter.py\", line 425, in to_payload\n    data=json.dumps(\n", "  File \"C:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n", "  File \"C:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\encoder.py\", line 199, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n", "  File \"C:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\encoder.py\", line 257, in iterencode\n    return _iterencode(o, 0)\n", "  File \"C:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\n"
```

### Minimal Reproduction
```python
import asyncio
from datetime import timedelta

from pydantic import constr, BaseModel, EmailStr, Extra
from temporalio import activity, workflow
from temporalio.client import Client
from temporalio.worker import Worker

UsernameField = constr(min_length=3, max_length=20, strip_whitespace=True)


# While we could use multiple parameters in the activity, Temporal strongly
# encourages using a single dataclass instead which can have fields added to it
# in a backwards-compatible way.

class SignupCreate(BaseModel, extra=Extra.forbid):
    username: UsernameField
    email: EmailStr


# Basic activity that logs and does string concatenation
@activity.defn
async def create_signup(input: SignupCreate) -> str:
    activity.logger.info("Running activity with parameter %s" % input)
    return f"signup: {input}"


# Basic workflow that logs and invokes an activity
@workflow.defn
class SignupWorkflow:
    @workflow.run
    async def run(self, signup: SignupCreate) -> str:
        workflow.logger.info("Running workflow with parameter %s" % signup)
        return await workflow.execute_activity(
            create_signup,
            signup,
            start_to_close_timeout=timedelta(seconds=10),
        )


async def main():
    # Uncomment the line below to see logging
    # logging.basicConfig(level=logging.INFO)

    # Start client
    client = await Client.connect("localhost:7233")

    # Run a worker for the workflow
    async with Worker(
            client,
            task_queue="hello-activity-task-queue",
            workflows=[SignupWorkflow],
            activities=[create_signup],
    ):
        # While the worker is running, use the client to run the workflow and
        # print out its result. Note, in many production setups, the client
        # would be in a completely separate process from the worker.
        result = await client.execute_workflow(
            SignupWorkflow.run,
            SignupCreate(username="alex", email="me@gmail.com"),
            id="hello-activity-workflow-id",
            task_queue="hello-activity-task-queue",
        )
        print(f"Result: {result}")


if __name__ == "__main__":
    asyncio.run(main())

```


### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Windows 11, Intel I9
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
  - Python SDK version 0.1b1
- Are you using Docker or Kubernetes or building Temporal from source? Answer: I'm using Docker compose, images:
  - `temporalio/ui:2.5.1`
  - `temporalio/auto-setup:1.17.5`
  


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2022-09-07 12:18:37.000 UTC</summary>

This was implemented in #102 which is not yet released.

Reactions: â¤ï¸ 1

</details>

<details>
<summary><strong>averri</strong> commented on 2022-09-07 12:21:21.000 UTC</summary>

Thank you, @cretz. This Python SDK is very well done, the API is super clean. 

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-09-20 15:37:07.000 UTC</summary>

This has now been released. Let me know if you run into issues.

</details>

<details>
<summary><strong>averri</strong> commented on 2022-09-21 01:58:52.000 UTC</summary>

Very nice! Thanks, @cretz 

</details>


---

<a id="64"></a>

### #64: [Feature Request] Get workflow handle/send signals from non-async code

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/64 |
| **State** | CLOSED |
| **Author** | nathanielobrown (Nathaniel Brown) |
| **Created** | 2022-07-05 20:03:17.000 UTC (3y 6m ago) |
| **Updated** | 2024-09-19 21:03:12.000 UTC |
| **Closed** | 2022-07-05 20:15:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.
I have a non-async activity and I want to send signals to the parent workflow from this activity. I want to do this because I have existing code that scrapes websites and yields those results in a generator and I want to notify the parent workflow of those results as they are fetched. I would prefer not to convert the activity to async because:

1. The code the activity calls is not async and I don't want to convert it completely to async (it would ruin my tests requests mocking for one thing)
2. I'm nervous about executing my non-async code in a thread pool. I know there is already a threadpool, but spawning threadpools all over the place _feels_ like it could get out of control. Better to have one thread pool and be able to control it's size is my thought
3. If I convert to async code and run my sync code in a thread pool, I'd have to communicate generator results through a queue and it would be a complicated solution

### Describe the solution you'd like
Would it be possible to add synchronous versions of `workflow.get_external_workflow_handle_for` (not defined as async, but fails if called outside of async function) and `workflow_handle.signal`? In general I guess the question is, are the functions for interacting with workflows going to support use from non-async code?

I can imagine lots of future cases where I'll want to interact form non-async code as my database interactions are not async so lots of my code is not async.

### Additional context
Maybe there are some async patterns I can use to mitigate these issues (for example [asyncer](https://asyncer.tiangolo.com/)) but I have not had much luck so far. I'm not an async expert, so any advice would be appreciated.


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2022-07-05 20:08:24.000 UTC</summary>

> Would it be possible to add synchronous versions of workflow.get_external_workflow_handle_for (not defined as async, but fails if called outside of async function) and workflow_handle.signal?

I think there may be some confusion here. Everything in the `workflow` package is only for calling from inside the workflow. You would not use them for an activity. If you want to signal a workflow from an activity, you'd use a client. Activity code is just any regular Python code. 

(granted I need to solve #52 soon so you can have a callable class with the client as a class attribute, but you can create the client inside the activity as needed)

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-05 20:15:59.000 UTC</summary>

> I think there may be some confusion here. Everything in the `workflow` package is only for calling from inside the workflow. You would not use them for an activity. If you want to signal a workflow from an activity, you'd use a client. Activity code is just any regular Python code.

Ooh, OK. That makes sense and is an easy enough rule to remember. I guess something to highlight in the docs as that gets built out.

I have some thoughts on the callable class issue, I'll comment on that issue.



</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-05 20:17:23.000 UTC</summary>

:+1: I will try to make it clearer at https://python.temporal.io/temporalio.workflow.html and once we get general purpose documentation (actively being worked on) I'll make sure it is clear there too.

</details>

<details>
<summary><strong>ofirsa</strong> commented on 2024-09-19 21:02:22.000 UTC</summary>

Hi,
Iâ€™m facing the same situation as you described, meaning, I want to send a signal from a non async activity to the parent workflow.
Can you please explain how you solved the issue?
Thanks

</details>


---

<a id="52"></a>

### #52: [Feature Request] Support non-function callables as activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/52 |
| **State** | CLOSED |
| **Author** | llchan (Lawrence Chan) |
| **Created** | 2022-06-17 06:21:36.000 UTC (3y 6m ago) |
| **Updated** | 2022-07-14 14:56:35.000 UTC |
| **Closed** | 2022-07-14 14:56:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Currently the activity definition implementation [here](https://github.com/temporalio/sdk-python/blob/431ca1967d365556a9cf5aa9aac00243b71059f8/temporalio/activity.py#L375) requires `__code__` on a callable.  From what I can tell it seems like it's only using that to verify that there are no kwonly arguments.  

### Describe the solution you'd like

We can instead use the `inspect` module to analyze the callable's signature, allowing a broader set of non-function callables to be used as activities.


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2022-06-17 11:21:51.000 UTC</summary>

Definitely can fix this. Can you give an example of which callable you are using and how so I can include it in the integration tests?

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-05 20:37:25.000 UTC</summary>

So this isn't 100% related but I'm just dumping my thoughts here in case they're useful when changing the `@activity.defn` decorator.

I think it is nice to have a thin wrapper around activities that allows the developer to more easily customize their activity functions. Dramatiq uses a [wrapper class](https://github.com/Bogdanp/dramatiq/blob/master/dramatiq/actor.py#L29) which is applied using a [decorator](https://github.com/Bogdanp/dramatiq/blob/master/dramatiq/actor.py#L157). I like this setup because:

1. It makes it easy to mock out the underlying function without messing up the task related attributes and without creating an otherwise useless wrapper function. For Dramatiq you just mock `task.fn`
   - As it stands in this SDK, if you directly mock the function things fail because it's missing `__temporal_activity_definition`. This isn't the end of the world because you can just pass a stubbed actor to the worker when you test, but it still might be nice to have easy mocking
3. It makes the path to customization fairly clear and straightforward. Just subclass the Actor class
4. It's easier not to interfere with the internals of the framework when you customize how the callable is called, because the callable is an attribute of the wrapper, rather than the wrapper attributes stuff on the callable.

I'm still customizing how the current decorator is applied, it's just a little more complicated. Here's what I'm using in my project. It logs exceptions, injects a database access object and does some extra parsing of arguments that are Pydantic classes:

```python
import functools
import inspect
from time import time
from typing import Any, Callable, no_type_check, Optional, TypeVar

from loguru import logger
from temporalio import activity

from bots_interface.actors import _parse_type
from dd.repositories import repo as repo_module

ActivityFunc = TypeVar("ActivityFunc", bound=Callable[..., Any])


@no_type_check
def define_activity(
    fn: ActivityFunc = None, name: Optional[str] = None
) -> ActivityFunc:
    if name is not None and fn is None:
        return functools.partial(define_activity, name=name)
    assert fn is not None
    defn = activity.defn(name=name or fn.__name__)(fn)
    fn_annotations = fn.__annotations__

    @functools.wraps(fn)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        if "repo" in fn_annotations:
            assert fn_annotations["repo"] is repo_module.Repository
            assert "repo" not in kwargs
            repo = repo_module.repo_factory()
            kwargs["repo"] = repo
        parsed_kwargs = _parse_args(fn, args, kwargs)
        if "repo" in fn_annotations:
            with repo.commit_and_close():
                return _run_func_log_exception(fn, parsed_kwargs)
        else:
            return _run_func_log_exception(fn, parsed_kwargs)

    wrapper.__temporal_activity_definition = defn.__temporal_activity_definition
    return wrapper


@no_type_check
def _run_func_log_exception(fn: ActivityFunc, kwargs: dict) -> Any:
    t_start = time()
    try:
        result = fn(**kwargs)
    except Exception:
        logger.exception(f"Activity function {fn.__name__!r} failed")
        raise
    duration = time() - t_start
    logger.debug(f"{fn.__name__} ran in {duration:.3f} seconds")
    return result


def _parse_args(fn: Callable, args: Any, kwargs: Any) -> dict:
    """Returns kwargs with args converted to kwargs and pydantic types parsed.
    Pendulum Datetimes are parsed as well"""
    call_args = inspect.getcallargs(fn, *args, **kwargs)
    for arg_name, value in call_args.items():
        annotation = fn.__annotations__.get(arg_name)
        call_args[arg_name] = _parse_type(annotation, value)
    return call_args
```


</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-05 20:54:35.000 UTC</summary>

There is no customization beyond name. I just need a way to mark a callable as an activity and, at the time, an attribute seemed more reasonable than wrapping the function. This is what the `@no_type_check` decorator you're showing in your example does, see: https://github.com/python/cpython/blob/40d81fd63b46cf998880ce3bf3e5cb42bc3199c1/Lib/typing.py#L2476-L2509. This allows _other_ decorators that actually do need to proxy the invocation to do so without affecting our simple need to just set a small piece of metadata.

I may introduce an alternative activity concept that can be used instead of the decorator, but again the only customization is name. And the only thing I need is simple metadata, I do not want to proxy the function. Is there a more Pythonic way to carry around metadata without proxying the invocation? (forgive my naivete on Python here)

</details>

<details>
<summary><strong>nathanielobrown</strong> commented on 2022-07-05 23:12:42.000 UTC</summary>

> I may introduce an alternative activity concept that can be used instead of the decorator, but again the only customization is name. And the only thing I need is simple metadata, I do not want to proxy the function. Is there a more Pythonic way to carry around metadata without proxying the invocation? (forgive my naivete on Python here)

Nothing more pythonic comes to mind. In fact, stuffing attributes on whatever object feels _very_ Python...

I like what you're saying about keeping it simple, and I think if I want to have proxied/mockable/customizeable tasks I can still do that under this model with my own decorator. I realize now that my current decorator could be cleaned up so it doesn't care about what `activity.defn` does if I apply it at the end (`activity.defn(name=name or fn.__name__)(wrapper)`).

</details>


---

<a id="1207"></a>

### #1207: [Feature Request] Add a clean way to extract failure cause from client side

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1207 |
| **State** | CLOSED |
| **Author** | funkindy (Alexey Trofimov) |
| **Created** | 2025-11-06 07:24:58.000 UTC (2 months ago) |
| **Updated** | 2025-12-04 17:20:51.000 UTC |
| **Closed** | 2025-12-04 17:20:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I need a straightforward way to show a failure reason on the client side.

AFAIK now there are a couple of ways to do it:
1. Dig into the history 
2. try/except over `await handle.result` and then dig into the exception.

I wonder why there is no method on the handle to get current (not static) details and current summary?

I don want to do all the digging on the client side, i just want to put readable reason to `current_*` attributes and read them from the client.

Maybe some shortcut method to get latest event can work too.

What do you think?

Thank you!


#### Comments (3)

<details>
<summary><strong>tconley1428</strong> commented on 2025-11-06 17:57:50.000 UTC</summary>

Can you elaborate some? What type of failure are you talking about? Actual failure of the entire workflow? Something having failed but the workflow is still running?

> i just want to put readable reason to current_* attributes and read them from the client.
I'm not sure I understand what this means. You want to set some state of your own the client can read? Or you want some notion of the current 'state' to be available for querying on the workflow handle?

</details>

<details>
<summary><strong>funkindy</strong> commented on 2025-11-07 08:38:38.000 UTC</summary>

Hi! Yes, sorry. Id like to have a way to share current "whats happening" or final "whats failed" with the client (For UI or some decision-making on the client)

Now the client can get status and history of events. Status is too general, events are too technial and can be implementation specific.

I think it can be handy to let client obtain `current_summary` and `current_details` from the workflow.

I now use "upsert_memo" inside TWF and handle.describe().memo() on the client side as a workaround.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-11-07 20:51:05.000 UTC</summary>

> I need a straightforward way to show a failure reason on the client side.

Today success/failure result is only available by awaiting result or walking history (which is what result effectively does). We have opened https://github.com/temporalio/temporal/issues/8608 to provide this type of information on `describe` of a workflow.

> I wonder why there is no method on the handle to get current (not static) details and current summary?

There is no such thing as "current summary". For "current details", that's actually powered by a Temporal query named `__temporal_workflow_metadata` which returns a proto JSON'd result of https://github.com/temporalio/api/blob/master/temporal/api/sdk/v1/workflow_metadata.proto's `WorkflowMetadata` which includes current detail as obtained from the workflow code, it is not in history or on the server at any time.

You can write your own query that can return anything you want too and it can be called even after a workflow is closed.

Reactions: ðŸ‘ 1

</details>


---

<a id="1186"></a>

### #1186: [Bug] Incomplete fix for #1091 - conversation_id still required in _TemporalModelStub.get_response()

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1186 |
| **State** | CLOSED |
| **Author** | danielmillerp (Daniel Miller) |
| **Created** | 2025-10-24 20:54:57.000 UTC (2 months ago) |
| **Updated** | 2026-01-05 14:54:46.000 UTC |
| **Closed** | 2026-01-05 14:54:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

  I'm trying to use the Temporal Python SDK's OpenAI agents integration plugin to build and run agents with Temporal workflows.

### Describe the bug

When using the Temporal OpenAI agents plugin, the _TemporalModelStub.get_response() method throws a TypeError because the optional parameters lack default
  values:

  TypeError: _TemporalModelStub.get_response() missing 1 required keyword-only argument: 'conversation_id'

  This issue was previously reported in #1091 (https://github.com/temporalio/sdk-python/issues/1091) and supposedly fixed by PR #1092, but the fix didn't fully
  resolve the problem. The PR added the parameters to the method signature but didn't give them default values, so they're still required at runtime despite
  being marked as Optional.

### Minimal Reproduction

  1. Install temporalio 1.18.1 and openai-agents 0.2.7
  2. Set up any basic Temporal workflow using the OpenAI agents plugin
  3. Run the workflow
  4. Observe TypeError when the plugin tries to call get_response()
 
### Environment/Versions

<!-- Please complete the following information where relevant. -->

  - OS and processor: macOS Darwin 24.6.0 (M1)
  - Temporal SDK Version: 1.18.1
  - OpenAI Agents Version: 0.2.7
  - Python Version: 3.13

### Additional context

 Issue #1091 identified this problem and PR #1092 attempted to fix it by adding the conversation_id, previous_response_id, and prompt parameters. However, the
  fix was incomplete - the parameters were added without default values.

  Current implementation in temporalio/contrib/openai_agents/_temporal_model_stub.py:60-72:
  async def get_response(
      ...
      *,
      previous_response_id: Optional[str],
      conversation_id: Optional[str],
      prompt: Optional[ResponsePromptParam],
  ) -> ModelResponse:

  Required fix:
  async def get_response(
      ...
      *,
      previous_response_id: Optional[str] = None,
      conversation_id: Optional[str] = None,
      prompt: Optional[ResponsePromptParam] = None,
  ) -> ModelResponse:

  Adding = None makes these parameters truly optional, allowing the plugin to work without requiring callers to explicitly pass these arguments. Without this,
  the TypeError persists despite the attempted fix.


#### Comments (3)

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-27 14:18:05.000 UTC</summary>

We aren't intending to maintain backwards compatibility with any arbitrary version of `openai-agents` given that it is developing rapidly. Our current (in main) requirements for it are `"openai-agents>=0.3,<0.5",`. Do you have a compelling reason not to upgrade? Agents is currently on `0.4.2`

</details>

<details>
<summary><strong>danielmillerp</strong> commented on 2025-11-02 20:57:46.000 UTC</summary>

> We aren't intending to maintain backwards compatibility with any arbitrary version of `openai-agents` given that it is developing rapidly. Our current (in main) requirements for it are `"openai-agents>=0.3,<0.5",`. Do you have a compelling reason not to upgrade? Agents is currently on `0.4.2`

I see the same error in version 0.4.2

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-11-03 18:06:51.000 UTC</summary>

OpenAI Agents should be passing the conversation_id. https://github.com/openai/openai-agents-python/blob/a30c32e722a4cdb761fe8b250606757ec8581296/src/agents/run.py#L1672 Do you have a stacktrace for the failure, and can you confirm that the update to `0.4.2` took place?

</details>


---

<a id="1184"></a>

### #1184: Feature request: Call activities as decorated functions inside workflows (sugar over execute_activity)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1184 |
| **State** | CLOSED |
| **Author** | maver1ck (Maciek BryÅ„ski) |
| **Created** | 2025-10-24 12:42:07.000 UTC (2 months ago) |
| **Updated** | 2025-12-05 19:24:57.000 UTC |
| **Closed** | 2025-12-05 19:24:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Problem
In the Python SDK, workflows must invoke activities explicitly via `workflow.execute_activity()` / `start_activity()`. This adds boilerplate at call sites (repeating options like timeouts/retries) and surprises users who expect decoratorâ€‘marked callables to be directly invocable inside workflows.

Why it matters
- Readability/teachability: code is less declarative; `@activity.defn` looks like a function you can call, but you canâ€™t.
- Ergonomics: the same options are repeated in every call site even when theyâ€™re identical.
- Consistency: other task/flow frameworks allow direct invocation of decorated tasks within flows, reducing ceremony.

Comparison with Prefect
Below are minimal examples showing how a similar pattern looks today.

Prefect (2.x):
```python
from prefect import flow, task

@task
def fetch_article(url: str) -> str:
    ...

@flow
def pipeline(url: str):
    # Direct call (sync semantics) or .submit() for concurrency
    html = fetch_article(url)
    # html = fetch_article.submit(url)
    return html
```

Temporal Python today:
```python
from datetime import timedelta
from temporalio import workflow
from temporalio import activity

@activity.defn
async def fetch_article(url: str) -> str:
    ...

@workflow.defn
class Pipeline:
    @workflow.run
    async def run(self, url: str) -> str:
        html = await workflow.execute_activity(
            fetch_article,
            url,
            start_to_close_timeout=timedelta(seconds=30),
            # plus any retry/heartbeat options at each call site
        )
        return html
```

This issue is to track interest in more ergonomic, decoratorâ€‘style activity invocation in Python workflowsâ€”while preserving Temporalâ€™s determinism and activity semantics. No specific API is proposed here; the goal is to document the pain point and contrast with common expectations from Prefectâ€‘style flows.


#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2025-10-24 13:31:43.000 UTC</summary>

This was a _very_ intentional decision during the design of the APIs. We did the looks-like-normal-function approach in Java and (kinda) TypeScript and have since backed off that approach.

Firstly, we don't want activity invocation to look like regular function invocation. You need to understand that you are invoking an activity and not just calling a Python function.

Secondly, you need to be able to provide options to an activity. Languages like Java and TypeScript that did the looks-like-normal-function-invocation have confusing away-from-call semantics for providing activity options.

Thirdly, there is `start_activity`, `execute_activity`, `start_local_activity`, and `execute_local_activity` (and sometime in the future maybe more for standalone activities). The activity implementation/decorator does not decide which of those to use, the caller does. And we don't want inconsistency between starting and executing or inconsistency between local and non-local.

Fourthly, we support activities as methods with calls like `start_activity_method` and `execute_activity_method`. It is very common. How would that work in this scenario?

Fifthly, we want consistency with how you start/execute workflows, which are classes not functions.

Finally, for those that do want to give up this explicitness/clarity and know exactly how they may call an activity (local or not, what options, etc), they can make this decorator/utility themselves.

Feel free to join us on `#python-sdk` on [Slack](https://t.mp/slack) for general discussion topics.

</details>

<details>
<summary><strong>maver1ck</strong> commented on 2025-10-24 14:27:53.000 UTC</summary>

Hi @cretz,
Thank you for the great explanation.

## start_activity vs execute_activity
Let's start with `start_activity` vs `execute_activity`
From my understanding this is simpler problem to solve.

Let's assume that we have custom decorator:
```
@callable_activity.defn(start_to_close_timeout=timedelta(seconds=30))
async def fetch_article(url: str) -> str:
    ...

```
Parameters of start/execute may be defined in decorator

Then we have two options how to use it:
1. without await which equals to start_activity
```
article_activity1 = fetch_article(url1)
article_activity2 = fetch_article(url2)
article1 = await article_activity1
article2 = await article_activity2
```

2. with await which equals to execute_activity
```
article = await fetch_article(url)
```

## local vs non-local
This may be just solved using parameters in decorator
```
@callable_activity.defn(local=True)
async def fetch_article(url: str) -> str:
    ...

```

> Finally, for those that do want to give up this explicitness/clarity and know exactly how they may call an activity (local or not, what options, etc), they can make this decorator/utility themselves.

Of course this is something that may be easy implemented outside of SDK (probably also for workflows).
The question is - is it valuable addition to Python SDK.




</details>

<details>
<summary><strong>cretz</strong> commented on 2025-10-24 17:09:16.000 UTC</summary>

> Then we have two options how to use it:

There are more calls than just start/execute, and there are many other reasons listed. And we may want to add more things to workflow-specific https://python.temporal.io/temporalio.workflow.ActivityHandle.html (returned from start only if you call start inside of workflows) which we would never alter the decorator's type hint to return because we want standalone activities too (which will be these same activities called from non-worfklow contexts).

> This may be just solved using parameters in decorator

This is a caller choice not an implementer choice

And there are many other reasons listed for why we want activity invocation to be explicit and not hidden

</details>


---

<a id="1177"></a>

### #1177: [Bug] OpenAI agent sdk non retryable error on liteLLM 503

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1177 |
| **State** | CLOSED |
| **Author** | morgan-cromell (Morgan Cromell) |
| **Created** | 2025-10-22 08:04:47.000 UTC (2 months ago) |
| **Updated** | 2025-10-27 14:19:04.000 UTC |
| **Closed** | 2025-10-27 14:19:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

Using the openai agent sdk with temporal and LiteLLM with aws bedrock. For example the anthropic models.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

When you get rate limited by aws bedrock it throws an  `litellm.exceptions.ServiceUnavailableError`

For example: 
`litellm.ServiceUnavailableError: BedrockException - {"message":"Model is getting throttled. Try your request again."}`

This is a retryable error and should produce a retry of the activity. Instead it results in this error
`temporalio.exceptions.ApplicationError: Non retryable OpenAI status code`

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

Setup one of the openAI agent SDK temporal samples and use the LiteLLM provider. Use one of the models that has a lot of throttling and wait until you get throttled/rate limited.

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?
Temporal python sdk 1.18.1
### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (3)

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-22 16:58:27.000 UTC</summary>

We tried to mimic the retry policies implemented by OpenAI. https://github.com/openai/openai-python/blob/dff16b5e9964bf85157eb41181255ed5dde8dda4/src/openai/_base_client.py#L750. In this particular case, given that the error did not provide retry headers, we base it on the status code reported by the underlying provider. LiteLLM is turning it into https://github.com/BerriAI/litellm/blob/57a2ec3beb7fcfb7ec7ccd379a19d2db951da9ef/litellm/exceptions.py#L485 which has a status code of 503. We could theoretically change our retry behavior on that, but I'm wary of creating that discrepancy, since the same retry logic is used for litellm and openai. 

It does seem like LiteLLM's throttling error detection is either incorrect or insufficient for bedrock: https://github.com/BerriAI/litellm/blob/57a2ec3beb7fcfb7ec7ccd379a19d2db951da9ef/litellm/litellm_core_utils/exception_mapping_utils.py#L942, which is something you could report to them. If it was converted to a `429`, retry would happen.

I think the more likely thing for us to do would be to add an additional configuration point to let you do your own exception handling for your particular needs. 

</details>

<details>
<summary><strong>morgan-cromell</strong> commented on 2025-10-23 09:54:56.000 UTC</summary>

@tconley1428 your implementation does not correctly mimic the openAI retry policies. You only retry on 500 errors while the openAI code retries on >= 500
https://github.com/openai/openai-python/blob/dff16b5e9964bf85157eb41181255ed5dde8dda4/src/openai/_base_client.py#L778
```
        # Retry internal errors.
        if response.status_code >= 500:
            log.debug("Retrying due to status code %i", response.status_code)
            return True
```

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-23 14:43:08.000 UTC</summary>

Ah, thank you. I definitely missed the inequality comparison there. That's a straightforward fix then.

Reactions: ðŸ‘ 1

</details>


---

<a id="1144"></a>

### #1144: [Bug] Revisit activity/nexus worker error handling on shutdown

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1144 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2025-10-07 15:46:19.000 UTC (3 months ago) |
| **Updated** | 2025-11-26 16:04:24.000 UTC |
| **Closed** | 2025-10-20 23:29:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See https://github.com/temporalio/sdk-python/pull/813/files#r2183804451 and https://github.com/temporalio/sdk-python/pull/813/files#r2183805267.

<img width="817" height="1008" alt="Image" src="https://github.com/user-attachments/assets/1bcd6703-1727-4c94-880e-3a92b1e00811" />

Also https://github.com/temporalio/sdk-python/pull/813/files#r2183909640 re potential memory leaks in activity worker

#### Comments (3)

<details>
<summary><strong>sonianuj287</strong> commented on 2025-10-15 17:25:52.000 UTC</summary>

Hi @dandavison , I've started working on this issue. Please let me know if you have anything more to share regarding it. 
Thanks :)

</details>

<details>
<summary><strong>VegetarianOrc</strong> commented on 2025-10-20 23:29:04.000 UTC</summary>

Closing this as won't do. Core handles graceful shutdown and the behavior of the Nexus worker is consistent with the activities worker.

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-11-26 16:04:24.000 UTC</summary>

> the behavior of the Nexus worker is consistent with the activities worker.

It's the behavior of the activity worker that this issue is calling into question though.

</details>


---

<a id="1120"></a>

### #1120: [Question] LIFO logic in task queue or workflow Type

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1120 |
| **State** | CLOSED |
| **Author** | bbkgh (Babak Ghadiri) |
| **Created** | 2025-09-25 10:00:31.000 UTC (3 months ago) |
| **Updated** | 2025-10-01 18:25:46.000 UTC |
| **Closed** | 2025-10-01 18:25:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Can we have /simulate LIFO login in taskqueue or workflow types? new activities that come to a queue  execute first

#### Comments (3)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-29 16:40:59.000 UTC</summary>

We don't have explicit LIFO scheduling, we do provide priorities which you can use to give some activities preference:
https://python.temporal.io/temporalio.common.Priority.html 

</details>

<details>
<summary><strong>bbkgh</strong> commented on 2025-09-29 17:46:23.000 UTC</summary>

I want to make newer workflows of same type take precedence over old ones. What is "Task Queue Fairness" ? is that related?

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-29 20:11:39.000 UTC</summary>

Not really. Here's a video on fairness: https://www.youtube.com/watch?v=Cf6_PBoyxbk. We don't currently provide a way to prioritize newer runs. There is feature proposal which would help here, I'm trying to see if there is a public issue I can link. 

</details>


---

<a id="1049"></a>

### #1049: [Feature Request] Support access to metric meter in multi-processed workers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1049 |
| **State** | OPEN |
| **Author** | gregbrowndev (Greg Brown) |
| **Created** | 2025-08-24 16:52:34.000 UTC (4 months ago) |
| **Updated** | 2025-09-03 23:46:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

Currently, the SDK doesn't extend custom metric support via the Runtime's MetricMeter to activities running within a multi-processed worker. This means users have to figure out how to support this themselves, which adds complexity to the project and may be infeasible for some.

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

Ideally, I'd hope to see metrics wired up across the process pool out of the box. So following a similar approach to the existing heartbeat manager, metrics emitted by the child process would be sent via a multiprocessing queue to the parent worker process.

Possible high-level approach:

- Provide `temporalio.common.MetricMeter` implementation alternate to [_MetricMeter](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/runtime.py#L480) that is set in [`_Context`](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/activity.py#L190) automatically in process pool workers.
- Instead of being backed by `temporalio.bridge.metric.MetricMeter`, they would push messages onto a queue consumed by the parent process, which would then rely those actions against the real `temporalio.bridge.metric.MetricMeter`.

This might be easier Python side, than trying to wire up the Rust code under the hood.

Worth pointing out there are on-going proposals to do this natively in OTEL: https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/4968.

I'd be happy to support this work, if you think the plan sounds reasonable. 

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->

#### **Background**

The previous work to add custom metric support, https://github.com/temporalio/sdk-python/pull/384, at the time didn't include support for multi-processed worker:

> NOTE: This does not return a metric meter for sync activities that are non-threaded (i.e. multiprocess) because a meter cannot be shared across processes and it's a large effort to support metric shipping across processes for this rarely used activity type

#### **Case to support this feature**

The multi-processed worker provides a lot of value, at least on my project which uses Temporal for its typical system integration use cases but also for managing some reasonably heavy CPU bound workloads (e.g. processing PDFs, NLP, small batch jobs, etc.) all within one tool. 

It might be the least frequently used worker type, but it provides several key benefits of its own:

- It highlights a clear design principle to split up CPU bound workloads at the workflow level, i.e. fanning out, and then keep each activity single threaded/single processed to optimise performance, avoid under/over utilisation of the underlying CPU cores, and support better auto-scaling decisions. Its easier to understand when size and scheduling of tasks on the process pool is managed by the worker itself, rather than a free-for-all that could occur if async/threaded activities were to manage their own/shared processes or pools.
- It provides out of the box support for heartbeating in activities running in the process pool, which users would need to wire up themselves if managing processes in the asyncio/threaded workers.
- Python is a common data science language, so a process pool based worker would be the go to over async/threaded

#### **Existing solution** 

Besides having to set up and configure a second OTEL metrics toolchain (`MeterProvider`, processors, exporters, and using different `meter` and `metric` classes), users would need to:

- ensure the toolchain is initialised correctly to properly configure the process pool
- be concerned with different default `mp_context`  behaviours on different OSs, e.g. maybe they develop on MacOS which defaults to `spawn`, but deploy in a linux based image, which defaults to `fork` ([the default will change to `forkserver` Python 3.14](https://docs.python.org/3.14/whatsnew/3.14.html#incompatible-changes)) 
- ensure with the global metric provider isn't accidentally inherited by the child process, which can cause deadlocks, or understand fork pre init hooks. 

Once the process pool is configured, we'd then need to get metrics out of the process correctly, e.g.:

- Tagging metrics with the process ID `pid` and using `OTLPMetricExporter` provides a simple solution, since each process would just push its own metrics to a collector. However, this would lead to high-cardinality, increased costs, and further complexity down the line.

  > Note: the metric payloads scraped/pushed to the backend need to be uniquely labeled, otherwise you get out-of-order writes, conflicts, and are otherwise left with meaningless metric values interleaved from multiple ambiguous sources. (It can also cause you to exceed the platform's data points per minute (DPM), which in Grafana Clouds case DPM=1, i.e. scrape/emit every 60s. 

- The initial simplicity would then come at the cost of having to set up some external metric aggregation. E.g. within an OTEL collector's [transformprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor#supported-functions) using [aggregate_on_attributes](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/v0.128.0/processor/transformprocessor#aggregate_on_attributes) to aggregate out the PID label. This also requires sensitive handling using `interval`, `batch`, `metricstarttime`, etc. to ensure you capture all the metrics within one interval.

#### Comments (3)

<details>
<summary><strong>tconley1428</strong> commented on 2025-08-25 17:24:29.000 UTC</summary>

This does seem like a valuable feature for multi process workers, but that isn't a very large scenario at the moment and we have limited bandwidth. We'll put it up for triage, but it isn't very likely to be prioritized soon. 

</details>

<details>
<summary><strong>gregbrowndev</strong> commented on 2025-08-26 22:46:42.000 UTC</summary>

Thanks for considering the request @tconley1428 ðŸ™ðŸ» 

I understand the bandwidth constraints (I hope the Nexus work is going well!). Would it be helpful to receive contributions in this area? I'd quite like to drive forward observability related features for my own project's sake and to give back to the project. 

---

I did spend quite a bit of time over the weekend digging into the Rust SDK code to better understand whats happening under the hood. 

For me, most of the SDK's latency histogram metric are basically a bit useless and expensive, since we can't specify bucket overrides in `OtelConfig` like you can in the Prometheus config and all of my tasks are long running and even `scheduled_to_start` latencies go higher than the [default settings](https://github.com/temporalio/sdk-core/blob/d34c1d6d393462a816baf2469c256a21ffbaf196/core/src/telemetry/metrics.rs#L675-L725) (very much specific to my own use case, I know). So even if I solved this issue, I'd still be constrained by the bucket settings in the Rust code. Though, ideally, we'd be able to export exponential histogram formats.

Additionally, the currently metric meter doesn't support other advanced features like [capturing trace exemplars](https://github.com/open-telemetry/opentelemetry-python/blob/1aaa2a25872c36aee208442ff654a67f5daa5736/opentelemetry-sdk/src/opentelemetry/sdk/metrics/_internal/__init__.py#L427-L437) (trace-to-metrics), as it would mean adding that to the abstraction / dynamically loading OTEL's tracer within the metric meter some how...

So I'm just wondering now whether the SDK would be better off with a Python-first OTEL integration built on top of the `MetricBuffer` to access the raw SDK datapoints and then have access to all the features, like `views`, `resource`, tracing-integration, etc. all configurable within the standard Python OTEL SDK. 

Is that something that is being considered? Otherwise, I'd be happy to be able to contribute towards these missing features if the [language] -> Rust approach is the intended solution going forward.

Cheers

</details>

<details>
<summary><strong>Sushisource</strong> commented on 2025-09-03 23:46:02.000 UTC</summary>

@gregbrowndev Thanks for all this great feedback.

So, I agree that probably the best option here is to use the `MetricBuffer` as you suggest, so that you can directly do the more advanced things you'd like to do in Python using the exporter directly. It is, indeed, exactly what `MetricBuffer` is intended for.

Something using it to export via OTel could I think be added to contrib here: https://github.com/temporalio/sdk-python/blob/main/temporalio/contrib/opentelemetry.py and would be a welcome contribution

</details>


---

<a id="1047"></a>

### #1047: [Feature Request] Set OpenTelemetry span status to failed for heartbeat timeout cancellations

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1047 |
| **State** | CLOSED |
| **Author** | gregbrowndev (Greg Brown) |
| **Created** | 2025-08-23 23:34:50.000 UTC (4 months ago) |
| **Updated** | 2025-10-21 19:59:06.000 UTC |
| **Closed** | 2025-10-21 19:59:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

Heartbeat timeout failures can be a major pain point when working with async Temporal activities. It would be great if the OpenTelemetry [`TracingInterceptor`](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/contrib/opentelemetry.py#L272-L295) contrib were able to mark spans as failed for cancellations due to heartbeat timeouts, so we can identify workers with heartbeat issues more easily and assess the scale of the issue.

The SDK metrics also don't help much in this case, since [activity_execution_failed](https://docs.temporal.io/references/sdk-metrics#activity_execution_failed) doesn't provide a reason/code label. 

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

Something like this works for the async/threaded cases, though it doesn't distinguish different kinds of cancellation reasons. It doesn't work for sync, multi-processed activities since they don't raise cancellation errors.

```py
class _TracingActivityInboundInterceptor(
    temporalio.worker.ActivityInboundInterceptor
):
    def __init__(
        self,
        next: temporalio.worker.ActivityInboundInterceptor,
        root: TracingInterceptor,
    ) -> None:
        super().__init__(next)
        self.root = root

    async def execute_activity(
        self, input: temporalio.worker.ExecuteActivityInput
    ) -> Any:
        info = temporalio.activity.info()
        with self.root.tracer.start_as_current_span(
            f"RunActivity:{info.activity_type}",
            context=self.root._context_from_headers(input.headers),
            attributes={
                "temporalWorkflowID": info.workflow_id,
                "temporalRunID": info.workflow_run_id,
                "temporalActivityID": info.activity_id,
            },
            kind=opentelemetry.trace.SpanKind.SERVER,
        ) as span:
            try:
                return await super().execute_activity(input)
            except BaseException as e:
                # Catch heartbeat cancellation errors to mark span as failed
                # Note: asyncio.CancelledError extends BaseException which would be ingored by span
                if isinstance(e, asyncio.CancelledError):
                    span.record_exception(e)
                    span.set_status(
                        opentelemetry.trace.Status(
                            status_code=opentelemetry.trace.StatusCode.ERROR,
                            description=f"{type(e).__name__}: {e}",
                        )
                    )
                raise e
```

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->

This only really applies in the case where the worker/activities are blocked for some reason and fail to heartbeat. Since the activity is still actually running, they receive a cancellation request from the server on their next, eventual heartbeat.  If the worker completely died, I wouldn't expect to see any spans at all. 

**The problem** is `asyncio.CancelledError` extends `BaseException`, but the OTEL Tracer's [`start_as_current_span`](https://github.com/open-telemetry/opentelemetry-python/blob/1aaa2a25872c36aee208442ff654a67f5daa5736/opentelemetry-sdk/src/opentelemetry/sdk/trace/__init__.py#L1077) only catches `Exception` type errors ([reference](https://github.com/open-telemetry/opentelemetry-python/blob/1aaa2a25872c36aee208442ff654a67f5daa5736/opentelemetry-api/src/opentelemetry/trace/__init__.py#L593-L596)), since exceptions directly subclassing `BaseException` are usually not considered errors, e.g. `GeneratorExit`, `KeyboardInterrupt`, `CancelledError` (according to that code). 

However, in this context, a `CancelledError` (both async and the sync Temporal kind) can **represent a major failure of the system to behave as intended**.

**The second problem** is that it doesn't look like there is enough context in the `TracingInterceptor` to distinguish different kinds of cancellation reasons, including:

- cancelled by user (e.g. in Temporal UI, SDK, or CLI) - shouldn't be marked as failed
- cancelled by workflow due to a failure in some other parallel branch, e.g. `await asyncio.gather(activity1, activity2, ...)` - probably not a failure for this activity?
- cancelled by heartbeat timeout - IMO is a failure
- cancelled by activity timeout - could be context specific, leaning towards a failure though
- cancelled due to activity pause - not a failure
- cancelled due to worker shutdown event - not a failure

I'm not sure what the correct semantics should be for each of these cases. It would probably help to align them with the existing SDK metrics, e.g. [`activity_execution_cancelled`](https://docs.temporal.io/references/sdk-metrics#activity_execution_cancelled) and [`activity_execution_failed`](https://docs.temporal.io/references/sdk-metrics#activity_execution_failed) (Note: I've not actually seen any `activity_execution_cancelled` in my app as its Java only...) so not sure if all these cancellation errors are currently being emitted as `activity_execution_failed` or not? If that is the case, then it might make sense to simply make all cancellations fail the span (though I do think it should be more refined than that, e.g. involuntary cancellations due to something bad happening, vs voluntary cancellations initiated by the user/system). 

The `_Context` object's `cancellation_details` doesn't seem to contain anything useful, as its not set during the `_ActivityWorker`'s [`_heartbeat_async`](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/worker/_activity.py#L242) method, and the interceptor doesn't have access to the info in `_RunningActivity`, from what I could tell. Updating the `cancellation_details` when a heartbeat error is received could solve this part of the issue since we can access it in the interceptor via `_Context` / [`cancellation_details`](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/activity.py#L324-L326) accessor.

**Lastly**, it doesn't look like cancellation happens at all in multi-process workers. After debugging, I saw that even after the whole workflow failed, the sync activity continued until completion. The `thread_event=manager.new_event()` event set in `_ActivityWorker` doesn't seem to be actively used yet.  The result returned through the `TracingInterceptor` was the expected return value, not an exception, so these aren't even caught as exception events. However, I rarely have issues with the multi-processor worker, since I do its auto-heartbeating in a background thread. Though, I have seen these fail occasionally, maybe due to GIL blocking or CPU throttling. 

It would be nice if cancellations could be applied properly to sync, multi-processed activities to save time and resources. I've raised that as a separate issue: https://github.com/temporalio/sdk-python/issues/1048.


Since this fix isn't likely that straight-forward, I'll likely patch this in my own interceptor for now. This has been suggested in these other tracing related issues:

- https://github.com/temporalio/sdk-python/pull/1017
- https://github.com/temporalio/sdk-python/issues/1041


Cheers,

I'm happy to contribute towards all the issues mentioned in this area, if the solution/requirements are clarified. I also want to extend the SDK's runtime metrics support to sync, mp activities too. So I'm not shy to pick up fairly challenging bits of work to help out ðŸ™ðŸ» 








#### Comments (3)

<details>
<summary><strong>gregbrowndev</strong> commented on 2025-08-25 11:39:58.000 UTC</summary>

Further thoughts on implementation:

- I noticed there is some inconsistency in how spans handle cancellation between async and sync-threaded workers. 

  - Since the threaded worker raises a `temporalio.exceptions.CancelledError`, which extends `Exception`, the root span created by this interceptor and any downstream childspans created by the user will be marked automatically as failed. 
  - However, for the async worker, the `asyncio.CancelledError` isn't treated as a failure at all. This PR fixes the behaviour in the root span, but childspans will be inconsistent, unless they apply the same `try/exception` same logic.
  - This also suggests that the interceptor should treat all `asyncio.CancelledError` errors as failures (regardless of the reason), to be consistent with the sync case, as is. Alternatively, the handling of `temporalio.exceptions.CancelledError` could be updated to make it consistent with `asyncio.CancelledError`. 

- Since https://github.com/temporalio/sdk-python/issues/1041 aims to treat `ApplicationError(category=BENIGN)` encoded errors (which derives from `Exception`) as none-failures, this too should probably be handled consistently in the interceptor and user activity implement (if that behaviour is what we want).

Both of these cases suggest it might be a good idea to provide a modified `Tracer` in `activity.py`, similar to the [`metric_meter`](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/activity.py#L469-L484) helper, that applies Temporal specific behaviours when making a span. This would make the interceptor and downstream childspans consistent. However, this would be a much bigger change to create a similar ABC in [common.py](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/common.py#L566) to avoid bundling the OTEL SDK as a core dependency.

What do you think?

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-03 23:03:08.000 UTC</summary>

@gregbrowndev appreciate your thoughts. I'm still coming to terms with the details of the OTel ecosystem here. If I follow correctly, the idea of providing a modified `Tracer` would be to allow the activity author to easily handle span generation in the same way that we would be doing after some of these changes, correct? In that case, I'm not sure I totally follow why we would provide a new abstraction over it inside of the core SDK package. Wouldn't it be possible for the otel integration to provide a modified tracer, and allow the author to set that in otel itself?

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-09-04 20:22:35.000 UTC</summary>

If I understand correctly, the concern is that cancellation exceptions for threaded vs asyncio are different? This is a Python issue where they chose to make an asyncio cancellation concept that was not portable. Then they went and changed the base class of their asyncio cancellation exception in 3.8.

We recognize there are a great many differences in how cancellation behavior/exceptions are treated in threaded vs async and the code author may have to account for those. One is that they have different base classes, making libraries like OTel treat them differently (and maybe Sentry and maybe loggers and maybe any number of Python libraries that react to `BaseException` vs not).

I am not sure trying to make async and threaded cancellation exceptions consistent is worth the confusion to users of each individually. The former expects to do things the Python way. The latter has no Python way, so Temporal does as libraries do and extends `Exception`. But we do not blindly assume every asyncio cancellation exception raised from an activity is an activity cancellation, we check whether there is a server exception first.

Libraries like OTel choose to treat built-in `BaseException`s differently than user/library defined `Exception`s. Same as if you wrote `catch Exception` and didn't catch asyncio exceptions in any code. I would rather let OTel's behavior be the predictable user behavior that OTel users understand rather than us treat this special asyncio cancelled exception differently than OTel chose to. This comes at the unfortunate cost of different activity types handling exceptions differently, but that happens anyways with loggers, Sentry, homemade metrics doing `catch Exception`, and, yes OTel context managers etc. But one can have their own interceptor if you want asyncio exceptions to be treated differently than OTel's default.

</details>


---

<a id="1041"></a>

### #1041: [Feature Request] Reclassify Benign Application errors in OpenTelemetry

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1041 |
| **State** | CLOSED |
| **Author** | gbxl (Lucas) |
| **Created** | 2025-08-20 21:03:57.000 UTC (4 months ago) |
| **Updated** | 2025-09-10 15:11:48.000 UTC |
| **Closed** | 2025-09-10 15:11:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | â¤ï¸ 2 |

#### Description

### Is your feature request related to a problem? Please describe.

When using [the suggested polling style](https://github.com/temporalio/samples-python/blob/main/polling/test_service.py#L29) leveraging `raise ApplicationError(category=BENIGN)` and activity retries, Temporal seem to properly catch that it's not really an error and log it at a debug level. Unfortunately, OpenTelemetry still reports the exception as an error just as dire as any other one. This is an issue because then libraries like `logfire` that plug into OpenTelemetry will trigger alerts for "expected behavior"

### Describe the solution you'd like

It'd be great to have a possible configuration or have an update to the OpenTelemetry contrib so that benign errors aren't treated as a real error in OpenTelemetry spans

### Additional context

I ended up adding a custom interceptor like this:
```
class BenignAppErrorInterceptor(Interceptor):
    """Interceptor to handle benign application errors in activities."""

    def intercept_activity(
        self,
        next: ActivityInboundInterceptor,  # noqa: A002
    ) -> ActivityInboundInterceptor:
        """Intercept activity execution to handle benign application errors."""
        return _BenignAppErrorActivityInboundInterceptor(next)


class _BenignAppErrorActivityInboundInterceptor(ActivityInboundInterceptor):
    def __init__(self, next: ActivityInboundInterceptor) -> None:  # noqa: A002
        super().__init__(next)

    async def execute_activity(self, input, *args, **kwargs):  # noqa: A002, ANN002, ANN003, ANN001, ANN202
        try:
            return await super().execute_activity(input, *args, **kwargs)
        except ApplicationError as e:
            msg = str(e)
            category = getattr(e, "category", None) or ""
            if category == "BENIGN":
                span = trace.get_current_span()
                if span and span.is_recording():
                    span.set_attribute("temporal.error.benign", value=True)
                    span.add_event(
                        "benign_application_error_filtered",
                        {
                            "exception.type": "ApplicationError",
                            "exception.message": msg,
                            "exception.category": str(category),
                        },
                    )
                    span.set_status(Status(StatusCode.OK))
                raise
```
But that seems really unideal to have to do that on the user side.

#### Comments (3)

<details>
<summary><strong>gregbrowndev</strong> commented on 2025-08-23 23:42:11.000 UTC</summary>

Btw, this only suppresses the error status in the span because calling:

```py
span = trace.get_current_span()
...
span.set_status(Status(StatusCode.OK))
``` 

prevents the status being set again on the same span. But you'll still get the exception event attached by the upstream `TracingInterceptor`. Would it be necessary to suppress that too, to align with the logging/metric behaviour?

I'm sure you know, suppressing the fail with an OK status probably isn't ideal either, but it works as a hack since `set_status` explicitly ignores being called with `UNSET` ([reference](https://github.com/open-telemetry/opentelemetry-python/blob/1aaa2a25872c36aee208442ff654a67f5daa5736/opentelemetry-sdk/src/opentelemetry/sdk/trace/__init__.py#L964)). 

For reference, from the [OTEL spec](https://opentelemetry.io/docs/specs/otel/trace/api/#set-status):

> Generally, Instrumentation Libraries SHOULD NOT set the status code to Ok, unless explicitly configured to do so. Instrumentation Libraries SHOULD leave the status code as Unset unless there is an error, as described above.
>
> Application developers and Operators may set the status code to Ok.

The fix here probably does require changes to the `TracingInterceptor` to avoid these issues, setting `set_status_on_exception=False` in the TracingInterceptor and handling the exceptions explicitly like I suggested in https://github.com/temporalio/sdk-python/issues/1047.


Also to point out, @gbxl, even if you modify the interceptor to fix the parent span status for these "BENIGN" errors, any childspans you create further downstream will still be marked as failed unless you add the same logic to those spans too. Not sure if that causes a problem with the logfire integration, or if it only cares about the parent span?

One possible solution to solve that would be to make your own `BenignException` class that extends `BaseException`, which would be ignored by any childspans ([reference](https://github.com/open-telemetry/opentelemetry-python/blob/1aaa2a25872c36aee208442ff654a67f5daa5736/opentelemetry-api/src/opentelemetry/trace/__init__.py#L593-L596)). But then you may need to catch and convert that back into the Temporal kind at the start of your activity function or maybe in an interceptor, otherwise Temporal would convert it back into a default `ApplicationError`, I think.

Reactions: ðŸ‘€ 1

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-08-25 17:16:59.000 UTC</summary>

We agree that this is a valuable change. We'll evaluate it in triage and see if we can prioritize it.

Reactions: â¤ï¸ 1

</details>

<details>
<summary><strong>gbxl</strong> commented on 2025-08-25 18:08:48.000 UTC</summary>

> I'm sure you know, suppressing the fail with an OK status probably isn't ideal either, but it works as a hack since `set_status` explicitly ignores being called with `UNSET` ([reference](https://github.com/open-telemetry/opentelemetry-python/blob/1aaa2a25872c36aee208442ff654a67f5daa5736/opentelemetry-sdk/src/opentelemetry/sdk/trace/__init__.py#L964)).

@gregbrowndev Yeah this is 100% a hack I was playing with and probably won't go to production (I'd rather have a rule in logfire to ignore ApplicationErrors with text match). 
Love the insights you added to your comment though <3 

> even if you modify the interceptor to fix the parent span status for these "BENIGN" errors, any childspans you create further downstream will still be marked as failed unless you add the same logic to those spans too. Not sure if that causes a problem with the logfire integration, or if it only cares about the parent span?

In this case the span ends with the error since the span wraps the activity execution, but yeah it would likely be an issue with adding more children to it.



Reactions: â¤ï¸ 1

</details>


---

<a id="891"></a>

### #891: [Feature Request] Support executing Workflow directly

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/891 |
| **State** | OPEN |
| **Author** | Kilo59 (Gabriel) |
| **Created** | 2025-06-06 17:54:34.000 UTC (7 months ago) |
| **Updated** | 2025-06-09 20:47:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

I'm often frustrated when I have some piece business logic as part of a Temporal Workflow that I also want to execute outside of a Temporal Worker. I would like to be able to avoid duplicating this code as much as possible.

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

Being able to call a `Workflow.run` method directly rather than only being able to run it via a temporal worker is one way to get around this issue.

However the below example will throw a `temporalio.workflow._NotInWorkflowEventLoopError: Not in workflow event loop` error.


```python
import asyncio
import dataclasses

from temporalio import activity, workflow


@dataclasses.dataclass
class MyArgs:
    to_greet: str


@workflow.defn
class GreetingWorkflow:
    @workflow.run
    async def run(self, arg: MyArgs) -> list[str]:
        log: list[str] = []

        hello = await workflow.execute_activity(
            GreetingActivities.say_hello,
            arg.to_greet,
        )
        print(hello)
        log.append(hello)

        goodbye = await workflow.execute_activity(
            GreetingActivities.say_goodbye,
            arg.to_greet,
        )
        print(goodbye)
        log.append(goodbye)
        return log


class GreetingActivities:
    @activity.defn
    async def say_hello(self, to_greet: str) -> str:
        return f"Hello {to_greet}!"

    @activity.defn
    async def say_goodbye(self, to_greet: str) -> str:
        return f"Goodbye {to_greet}!"


if __name__ == "__main__":
    wf = GreetingWorkflow()
    asyncio.run(wf.run(MyArgs(to_greet="world")))
```


### Additional context

<!-- Add any other context or screenshots about the feature request here. -->

I don't particular care about running the workflow per se, I just would like to avoid duplicating the "activity" orchestration logic in the workflow layer.

Also when run this way I don't expect to retain the standard temporal "durable execution" guarantees. 

#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2025-06-06 19:25:28.000 UTC</summary>

> Being able to call a Workflow.run method directly rather than only being able to run it via a temporal worker is one way to get around this issue.

Those calls like `workflow.execute_activity(...` are durable calls that only make sense when run in a worker-based environment. They make no sense outside of a durable context. It seems the request is to run Temporal without a server. It would not be reasonable to effectively embed all server logic like retries and such into the SDK, nor do we expect the primitives to be "durable optional".

> Also when run this way I don't expect to retain the standard temporal "durable execution" guarantees.

The APIs are only designed for use in a durable environment. If you must have a durable and non-durable way to run the same code, you may consider your own abstraction that checks `workflow.in_workflow()` to fall back to whatever non-durable form you prefer (e.g. your durable-optional form of execute activity could fall back to just invoking with manual retry logic). But Temporal workflow primitives require the server to be present and require durability.

</details>

<details>
<summary><strong>Kilo59</strong> commented on 2025-06-09 19:03:46.000 UTC</summary>

@cretz 

Yeah (as I said) I don't care about the durable execution when calling the workflow this way.

I assumed I was going to have to create my own abstraction, but I was hoping to avoid that.
For example is there a way to register a standard async event loop as "Workflow EventLoop" so that it doesn't throw this `temporalio.workflow._NotInWorkflowEventLoopError: Not in workflow event loop` error?

But assuming no, back to creating my own abstraction...
Is there some kind of Workflow activity method registry that I can pull from? That way I could programatically know what activity methods the workflow would call?
I would think something like this might also be useful when testing workflow business logic.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-06-09 20:41:18.000 UTC</summary>

> For example is there a way to register a standard async event loop as "Workflow EventLoop" so that it doesn't throw this temporalio.workflow._NotInWorkflowEventLoopError: Not in workflow event loop error?

I am afraid not. The workflow event loop is a specially-crafted event loop to be deterministic and has certain things the rest of the workflow calls expect to be present. You can check `workflow.in_workflow()` and only call `workflow.` module calls if that is true.

> Is there some kind of Workflow activity method registry that I can pull from? That way I could programatically know what activity methods the workflow would call?

To clarify, are you asking which activities are available to a workflow? Technically you can call any activity. While there are overloads for type-safe activity invocation, technically you can pass any string as the activity name. And you can invoke activities that could be on any other task queue on any other worker (even in a different programming language). So there is no centralized place to determine all activities a workflow could call. You can keep track of the activities you pass to your worker if you want to know which activities are available on that task queue (assuming you're not using dynamic activities).

</details>


---

<a id="878"></a>

### #878: [Bug] workflow.in_workflow() expects to be called in a thread with a running event loop

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/878 |
| **State** | CLOSED |
| **Author** | dmarcato (Dario Marcato) |
| **Created** | 2025-05-28 18:46:26.000 UTC (7 months ago) |
| **Updated** | 2025-06-05 22:47:16.000 UTC |
| **Closed** | 2025-06-05 22:47:16.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### What are you really trying to do?

I have a `sync` activity that triggers a utility method that needs to make sure it's not being called inside a workflow.
At a high level, the code is structured like this:
```python
# activities.py
@activity.defn
def my_activity(arg: MyArg):
    utility.do_stuff()

# utility.py
def do_stuff():
    if workflow.in_workflow():
        raise RuntimeError("Can not call from inside a workflow")
```

### Describe the bug

The activity triggers the following error:
```
RuntimeError
no running event loop
```
which can be traced to the `maybe_current()` method:

![Image](https://github.com/user-attachments/assets/d7e13842-fe27-4315-8de2-de20d8b6647a)

### Minimal Reproduction

The example above should be enough to reproduce.
The worker is configured with a `ThreadPoolExecutor` to execute the `sync` activities.

### Environment/Versions

- Temporal Version: Python 1.11.0
- Running inside Docker


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-05-29 14:00:25.000 UTC</summary>

Thanks, good find. We expect this to be able to be called from all contexts and we need to return `False` (via `None` on maybe_current) if there is no running loop. We will fix.

Reactions: ðŸŽ‰ 1

</details>


---

<a id="863"></a>

### #863: [Feature Request] Confirm support for Python 3.14

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/863 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-05-12 12:40:42.000 UTC (8 months ago) |
| **Updated** | 2025-10-21 14:37:00.000 UTC |
| **Closed** | 2025-10-21 14:37:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Python 3.14 is now beta: https://www.python.org/downloads/release/python-3140b1/. Please confirm support. If this doesn't include explicitly upgrading CI for fears of it not being stable yet, please open a separate issue to do that in October. Also, consider opening an issue to drop 3.9 support when 3.14 is stable in October.

#### Comments (3)

<details>
<summary><strong>tconley1428</strong> commented on 2025-06-04 22:58:56.000 UTC</summary>

Python 3.14 support in `pyo3` is in version `0.25`, which will require a migration to `pyo3-async-runtimes`. Therefore this is blocked on #816 

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-06-17 16:18:37.000 UTC</summary>

`pyo3` has been upgraded. Now blocked by `pydantic`. https://github.com/pydantic/pydantic/issues/11613

Reactions: â¤ï¸ 1

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-30 17:42:25.000 UTC</summary>

Now blocked by `fastuuid` by way of litellm: https://github.com/fastuuid/fastuuid/issues/52

</details>


---

<a id="782"></a>

### #782: [Bug] cancelled timer callback causes asyncio.exceptions.InvalidStateError

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/782 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2025-03-05 02:07:47.000 UTC (10 months ago) |
| **Updated** | 2025-03-07 13:11:34.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

The workflow below succeeds, but an exception occurs and is logged by the worker. In the example below this exception does not cause a workflow or workflow task failure, but we should nevertheless prevent it. What happens is:

1. An `asyncio.Task` is created, which blocks on a timer.
2. The task is cancelled, which throws `asyncio.CancelledError` into the coroutine, and cancels the `sleep` future.
3. But SDK internals have set a callback that attempts to resolve the future. This callback still fires at the set timer time, despite the future having been cancelled in the interim, causing `asyncio.exceptions.InvalidStateError`. See

https://github.com/temporalio/sdk-python/blob/49ca10e413ba67e1adfeed3b5577cb4f5b007e54/temporalio/worker/_workflow_instance.py#L1453-L1457


```
ðŸ”´ caught asyncio.CancelledError when sleeping in task
Exception in callback _WorkflowInstanceImpl.workflow_sleep.<locals>.<lambda>() at /Users/dan/src/temporalio/sdk-python/temporalio/worker/_workflow_instance.py:1456
handle: <_TimerHandle when=1741139892.760706 _WorkflowInstanceImpl.workflow_sleep.<locals>.<lambda>() at /Users/dan/src/temporalio/sdk-python/temporalio/worker/_workflow_instance.py:1456>
Traceback (most recent call last):
  File "/Users/dan/.local/share/uv/python/cpython-3.13.1-macos-aarch64-none/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/dan/src/temporalio/sdk-python/temporalio/worker/_workflow_instance.py", line 1456, in <lambda>
    lambda: fut.set_result(None),
            ~~~~~~~~~~~~~~^^^^^^
asyncio.exceptions.InvalidStateError: invalid state
Result: Hello, World!
```


```python
import asyncio
import uuid

from temporalio import workflow
from temporalio.client import Client
from temporalio.worker import Worker


@workflow.defn
class SayHello:
    async def _my_task(self) -> None:
        try:
            await workflow.sleep(6, summary="my task")
        except asyncio.CancelledError:
            print("ðŸ”´ caught asyncio.CancelledError when sleeping in task")

    @workflow.run
    async def run(self) -> str:
        task = asyncio.create_task(self._my_task())
        await workflow.sleep(2, summary="let the loop start")

        try:
            task.cancel()
            await task
        except BaseException:
            assert False, "unreachable"

        await workflow.sleep(15)
        return "Hello, World!"


async def main():
    client = await Client.connect("localhost:7233")
    task_queue = "timer-callback-exception-task-queue"
    async with Worker(
        client,
        task_queue=task_queue,
        workflows=[SayHello],
    ):
        result = await client.execute_workflow(
            SayHello.run,
            id=str(uuid.uuid4()),
            task_queue=task_queue,
        )
        print(f"Result: {result}")


if __name__ == "__main__":
    asyncio.run(main())
```

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-03-07 13:11:33.000 UTC</summary>

:+1: Should just not call `set_result` on an already done future (and find where else we might be doing that). This is probably just us (me) not properly thinking about the fact that in Python's `asyncio.Future` set-result-if-already-done is not a no-op (as it is in other langs and as `concurrent.futures.Future` result setter was until 3.8).

</details>


---

<a id="663"></a>

### #663: [Feature Request] Activity specific worker tuning

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/663 |
| **State** | CLOSED |
| **Author** | gregbrowndev (Greg Brown) |
| **Created** | 2024-10-09 13:57:04.000 UTC (1y 3m ago) |
| **Updated** | 2024-10-09 14:46:42.000 UTC |
| **Closed** | 2024-10-09 14:45:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

My application is a batch-oriented ETL pipeline. There are different async activities that interact with a different third-party API throughout the workflow. For example, the `geocoding` activity uses Google Places API, another activity might use the OpenAI API, etc.

Each activity is subject to external rate limits. 

While I can control rate limiting within my code, I can't easily control the concurrency of activities executed by the worker. This means when multiple activities of the same type are running, they are in contention for the rate limit, and simply take longer to finish or worse the activity can end up timing out due to the global throttling. 

My solution so far has been to deploy a separate worker for each activity and set the `max_concurrent_activities` worker option to control the concurrency and allow one or two activities to complete as quick as possible. However, this has created a lot of DevOps overhead and it can be hard for people on the team to set the worker up correctly when they add a new activity that has a rate limit.


### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

I saw the experimental `WorkerTuner` option added in this PR: https://github.com/temporalio/sdk-python/pull/559.

It would be great for this to support tuning for specific activities, e.g. defining the number of slots for a given activity name. This would allow a single async worker to handle all of my activities but only let a controllable number of each type run at any given time.


### Additional context

<!-- Add any other context or screenshots about the feature request here. -->

I saw there is a Go- (and Java I think) based example for creating a [Mutex Workflow](https://github.com/temporalio/samples-go/blob/main/mutex/mutex_workflow.go) that could control concurrency globally. This might be a better overall solution, but I struggled to apply this pattern in Python. I wouldn't expect the worker tuning to prevent contention if you scaled out the number of workers. However, in my solution, I have specifically disabled autoscaling on this worker for this reason, so controlling this at the worker level would be a lot more simple. 

#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2024-10-09 14:03:14.000 UTC</summary>

This will not work with how slots are used. You should use separate task queues if you need separate tuning options.

A slot is a reserved spot to request work from a _task queue_ from the server (and if no slot available, we don't ask for work). The activity name is not known at slot reservation time. Granted when we expand the slot supplier interface in Python to allow custom implementations, you can use the existing scheduled activities to help you derive the next slot reservation, but it will still always be per _task queue_ because Temporal distributes work to workers at the task queue level not the activity name level.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>gregbrowndev</strong> commented on 2024-10-09 14:15:03.000 UTC</summary>

Thanks, @cretz that does make sense. 

I think then the only other option is the mutex workflow (at least if I understand it). Basically, I just need to avoid scheduling an activity altogether if there's already a given number of them executing already.

If I have a workflow and the next activity to schedule is `geocoding`, I assume the mutex workflow would allow me to simply make the calling workflow wait until the resource can be acquired before it can schedule it. 

I can't allow the workflow to schedule the activity and make the activity wait on a semaphore because it would eventually timeout if 100s of these activities were scheduled during a short period. 

Happy for you to close the issue ðŸ‘ðŸ» 

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-09 14:45:42.000 UTC</summary>

> I think then the only other option is the mutex workflow

I think you should use separate task queues (i.e. separate workers) for activities that need to have their concurrency limited separately. This is a very common approach.

Granted the only global setting is max task queue concurrent activities there, so yes a mutex workflow may work best for global, but note it would not support high-throughput rate limiting.

Reactions: ðŸ‘ 1

</details>


---

<a id="661"></a>

### #661: [Bug] Child workflow execution stuck when running in testing.WorkflowEnvironment and workflow update is involved.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/661 |
| **State** | CLOSED |
| **Author** | Chengdyc |
| **Created** | 2024-10-08 03:46:55.000 UTC (1y 3m ago) |
| **Updated** | 2024-10-21 16:35:19.000 UTC |
| **Closed** | 2024-10-21 16:35:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

We're using Temporal to build a chat application and we have a parent-child workflow set up that models the conversation (session) and each conversation turn (request+response). The conversation is a long running entity workflow, and each input is submitted via a 'workflow update' which starts a child workflow to generate the response.

we'd want to write a unit test for the conversation workflow to verify it's behavior. In the unit test, we mock the child workflow so we don't need to mock the logic and activities within the child workflow.

### Describe the bug

When we run the unit test, the test get stuck starting the child workflow. We added print statements before calling execute_child_workflow and at the start of the child workflow. It appears that Temporal test environment is unable to start the child workflow.

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction

unit test to reproduce the bug. there are 3 test cases here.
1. send 'workflow update' to parent workflow that starts a child workflow. this test will get stuck, remove the pytest.mark.skip annotation to run it
2. send 'signal' to parent workflow that starts a child workflow. this test succeeds
3. send 'workflow update' to workflow that starts an activity (no child workflow involved). this test succeeds.

I'm using Temporal Python SDK version 1.7.1

```
import asyncio
import logging
import uuid
from datetime import timedelta

import pytest
from temporalio import activity, workflow
from temporalio.common import RetryPolicy
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import Worker

logger = logging.getLogger(__name__)

@workflow.defn
class ParentWorkflow:
    def __init__(self) -> None:
        self.input = None
        self.output = None

    @workflow.run
    async def run(self):
        while True:
            try:
                await workflow.wait_condition(
                    lambda: self.input is not None,
                    timeout=timedelta(minutes=60)
                )
            except TimeoutError:
                workflow.logger.info("we're done")
                return

            workflow.logger.info("exited wait, starting child workflow")
            self.output = await workflow.execute_child_workflow(
                ChildWorkflow.run,
                args=[self.input],
                task_queue=workflow.info().task_queue,
            )
            workflow.logger.info("child workflow complete, got output")
            self.input = None
    
    @workflow.update
    async def process_update(self, input: str):
        self.input = input
        self.output = None
        logger.info("waiting for output")
        await workflow.wait_condition(
            lambda: self.output is not None
        )
        return self.output
    
    @workflow.signal
    async def signal(self, input: str):
        self.input = input
        self.output = None
    

@workflow.defn
class ChildlessWorkflow:
    def __init__(self) -> None:
        self.input = None
        self.output = None

    @workflow.run
    async def run(self):
        while True:
            try:
                await workflow.wait_condition(
                    lambda: self.input is not None,
                    timeout=timedelta(minutes=60)
                )
            except TimeoutError:
                workflow.logger.info("we're done")
                return

            workflow.logger.info("exited wait, starting activity")
            self.output = await workflow.execute_activity_method(
                Activities.child_activity,
                args=[self.input],
                task_queue=workflow.info().task_queue,
                schedule_to_close_timeout=timedelta(
                    seconds=60
                ),
                retry_policy=RetryPolicy(maximum_attempts=1)
            )
            workflow.logger.info("child workflow complete, got output")
            self.input = None
    
    @workflow.update
    async def process_update(self, input: str):
        self.input = input
        self.output = None
        logger.info("waiting for output")
        await workflow.wait_condition(
            lambda: self.output is not None
        )
        return self.output


class Activities:
    @activity.defn
    async def child_activity(
        self,
        name: str
    ) -> str:
        return f"child activity {name}"


@workflow.defn
class ChildWorkflow:
    @workflow.run
    async def run(self, name: str):
        return await workflow.execute_activity_method(
            Activities.child_activity,
            args=[name],
            schedule_to_close_timeout=timedelta(
                seconds=60
            ),
            task_queue=workflow.info().task_queue,
            retry_policy=RetryPolicy(maximum_attempts=1)
       )


@workflow.defn(name="ChildWorkflow")
class MockChildWorkflow:
    @workflow.run
    async def run(self, name: str):
        await asyncio.sleep(60)
        return f"mock {name} done"
    
pytestmark = pytest.mark.asyncio


@pytest.mark.skip(
    reason="test will get stuck"
)
async def test_workflow_update():
    """ Test to show that if we use workflow update together with child workflow execution we'll trigger the bug. This test will get stuck """
    tmp_task_queue = f"task_queue-{uuid.uuid4()}"
    activities = Activities()
    async with await WorkflowEnvironment.start_time_skipping() as env:
        async with Worker(
            env.client,
            task_queue=tmp_task_queue,
            workflows=[ParentWorkflow, MockChildWorkflow],
            activities=[activities.child_activity],
        ):
            # env is WorkflowEnvironment with time-skipping
            handle = await env.client.start_workflow(
                ParentWorkflow.run,
                args=[],
                task_queue=tmp_task_queue,
                id=str(uuid.uuid4())
            )
            logger.info("executing update")
            update_result = await handle.execute_update(
                ParentWorkflow.process_update,
                args=["my name"]
            )
            logger.info(f"got result: {update_result}, waiting for termination")
            # do some other verification with workflow handle
            # wait for workflow to finish
            result = await handle.result()
            print(result)


async def test_workflow_signal():
    """ Test to show that if we are NOT sending workflow update, the bug is not triggered. This test will succeed """
    tmp_task_queue = f"task_queue-{uuid.uuid4()}"
    activities = Activities()
    async with await WorkflowEnvironment.start_time_skipping() as env:
        async with Worker(
            env.client,
            task_queue=tmp_task_queue,
            workflows=[ParentWorkflow, MockChildWorkflow],
            activities=[activities.child_activity],
        ):
            # env is WorkflowEnvironment with time-skipping
            handle = await env.client.start_workflow(
                ParentWorkflow.run,
                args=[],
                task_queue=tmp_task_queue,
                id=str(uuid.uuid4())
            )
            logger.info("executing update")
            update_result = await handle.signal(
                ParentWorkflow.signal,
                args=["my name"]
            )
            logger.info(f"got result: {update_result}, waiting for termination")
            # do some other verification with workflow handle
            # wait for workflow to finish
            result = await handle.result()
            print(result)


async def test_childless_workflow():
    """ Test to show that if we use workflow update and there is no child workflow, the bug is not triggered. this test will succeed """
    tmp_task_queue = f"task_queue-{uuid.uuid4()}"
    activities = Activities()
    async with await WorkflowEnvironment.start_time_skipping() as env:
        async with Worker(
            env.client,
            task_queue=tmp_task_queue,
            workflows=[ChildlessWorkflow],
            activities=[activities.child_activity],
        ):
            # env is WorkflowEnvironment with time-skipping
            handle = await env.client.start_workflow(
                ChildlessWorkflow.run,
                args=[],
                task_queue=tmp_task_queue,
                id=str(uuid.uuid4())
            )
            logger.info("executing update")
            update_result = await handle.execute_update(
                ChildlessWorkflow.process_update,
                args=["my name"]
            )
            logger.info(f"got result: {update_result}, waiting for termination")
            # do some other verification with workflow handle
            # wait for workflow to finish
            result = await handle.result()
            print(result)
```

Command to run this:
```
poetry run pytest -rA -s test_mock_child.py
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Linux (WSL2)
- Temporal Version: 1.7.1 
- Are you using Docker or Kubernetes or building Temporal from source? No

### Additional context




#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2024-10-08 12:25:00.000 UTC</summary>

Thanks for the report! We will investigate.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-09 14:36:25.000 UTC</summary>

Automatic time skipping only works when you're waiting on the workflow result. If you start the workflow and do not wait on the result, the automatic time skipping will not occur. From https://github.com/temporalio/sdk-python?tab=readme-ov-file#automatic-time-skipping:

> Anytime a workflow result is waited on, the time-skipping server automatically advances to the next event it can

If, before `logger.info("executing update")` you had `result_task = asyncio.create_task(handle.result())` that would wait for the result in the background and ensure automatic time skipping. Granted that would immediately trigger the 60s timeout. You can also enable auto-time-skipping with `with env.time_skipping_unlocked():`. I will confer with the team if we want to also enable automatic time skipping when waiting on update result.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-21 16:35:19.000 UTC</summary>

We have opened #675 to enable auto time skipping when waiting on an update result. Closing this issue in favor of that one (you can still do the workaround of waiting on the workflow in the background to enable auto-skipping).

</details>


---

<a id="659"></a>

### #659: [Feature Request] Some kind of registry for workflows and activities to get classes/function with their names

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/659 |
| **State** | CLOSED |
| **Author** | funkindy (Alexey Trofimov) |
| **Created** | 2024-10-07 12:29:15.000 UTC (1y 3m ago) |
| **Updated** | 2024-10-08 07:57:09.000 UTC |
| **Closed** | 2024-10-08 07:57:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I'd like to instantiate a worker instance with CLI wrapper. In this scenario i can only provide workflow names and activity names as strings.

It would be nice to have an SDK option to get python objects with this names or even to instantiate `Worker` with lists of strings directly. Maybe register types somewhere in `*.defn`.

Whats your opinion?



#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2024-10-07 13:32:50.000 UTC</summary>

We try not to assume global use of workflows/activities in the SDK. Many users create many workflows/activities and only start some of them in some workers and others in other workers (some may even have the same name). I would recommend maintaining a `dict` of workflows/activities if you need a form of global lookup. This can be done with a custom decorator if needed.

</details>

<details>
<summary><strong>funkindy</strong> commented on 2024-10-07 16:31:34.000 UTC</summary>

I see, thank you. Maybe you have a recipe for production deployment of workers?

We are looking for deploying pods with workers with some text based configuration (e.g. environment variables)

Thank you

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-07 19:23:15.000 UTC</summary>

We do not have a specific recipe. You can have a simple Python script that reads environment variables and builds the sets of workflows/activities based on what those values are (e.g. a `dict` lookup), but most people hardcode the sets of workflows/activities to a task queue because every worker on a task queue needs to have the same set of workflows/activities, so it often doesn't make much sense to have that be configurable at runtime.

(also feel free to join us on the forums or Slack if that's an easier way for you to discuss)

</details>


---

<a id="587"></a>

### #587: [Feature Request] add "id_conflict_policy" arg for "execute_child_workflow"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/587 |
| **State** | CLOSED |
| **Author** | johnny-smitherson (Johnny) |
| **Created** | 2024-07-18 09:54:10.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-18 22:35:24.000 UTC |
| **Closed** | 2024-07-18 21:55:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

The new `id_conflict_policy` arg is very useful to reduce the amount of try/except one has to use in workflows. But it doesn't work for `start_child_workflow(...)` / `execute_child_workflow`.

### Describe the solution you'd like

```
await workflow.execute_child_workflow(
    ...,
    id_conflict_policy=WorkflowIDConflictPolicy.USE_EXISTING,  # master branch only
)

---> TypeError: execute_child_workflow() got an unexpected keyword argument 'id_conflict_policy'
```

### Additional context

Is this a limitation of the API (and requires additions there too), or just the python library?

API PR was here https://github.com/temporalio/api/pull/359

If filed in wrong place, please move issue

Thank you!

#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2024-07-18 13:17:16.000 UTC</summary>

> Is this a limitation of the API (and requires additions there too)

This is a limitation of the API

> If filed in wrong place, please move issue

We can move to server repo, but there is a question of whether this is needed.

As a workflow author, you are the starter of the child workflows in your workflow and the listener of their completion states. Therefore you can perform whatever conflict resolution you'd like instead of the limited set offered by this value. We recommend using `WorkflowAlreadyStartedError` and consider it a healthy use of try/except. You can wrap it in a helper if needed, but I am not sure we want to do this helper (this is shown as an error in history, and we think it's important for code and history to be clear on these things).

</details>

<details>
<summary><strong>johnny-smitherson</strong> commented on 2024-07-18 21:55:31.000 UTC</summary>

Ah yes I see - for child tasks i can do

```
try:
        return await workflow.execute_child_workflow(
            ...
            id=the_id
        )
    except WorkflowAlreadyStartedError:
        handle = await workflow.get_external_workflow_handle(the_id)
        return await handle.result()
```

And that's more or less `USE_EXISTING` of the new param

or instead of `handle.result()`, I would terminate and restart to obtain the other behaviors of the param

Thanks!

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-07-18 22:34:57.000 UTC</summary>

`get_external_workflow_handle` does not return a handle with `result`. Getting a non-child result of a workflow from another workflow directly is not supported. Rather, if that already exists as a child, it means you started it some other time in your workflow. You can store that child handle on the class and access its result. If you need the result of a workflow that was not started as a child, you may have to break out into an activity and use the Temporal client.

Reactions: ðŸ‘ 1

</details>


---

<a id="520"></a>

### #520: [Feature Request] Update rustls and mio in Rust dependencies

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/520 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-05-08 16:58:42.000 UTC (1y 8m ago) |
| **Updated** | 2024-08-12 19:58:11.000 UTC |
| **Closed** | 2024-06-04 22:22:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Even though we are unaffected by the vulnerabilities, security scans show our current rustls and mio versions as of this writing have vulnerabilities. Update them (and any other things as needed).

#### Comments (3)

<details>
<summary><strong>bbemis017</strong> commented on 2024-06-03 18:24:47.000 UTC</summary>

Can we also add H2 to this list of dependencies that need to be upgraded since GHSA-q6cp-qfwq-4gcv is flagging the h2 version the sdk currently uses

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>bbemis017</strong> commented on 2024-08-12 19:46:34.000 UTC</summary>

Are there any updates on when the next python sdk could be release to include the dependency updates?/

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-08-12 19:58:10.000 UTC</summary>

No updates here on specific release timelines, but hopefully soon.

</details>


---

<a id="454"></a>

### #454: [Feature Request] Schedule creation should create tracing span by default

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/454 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-01-03 21:13:37.000 UTC (2 years ago) |
| **Updated** | 2025-07-24 13:56:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

When the tracing interceptor is in use, by default creating a schedule should create a span for the schedule and set it on the workflow header if the action is starting a workflow. Users need to be able to opt-out in their tracing interceptor options.

#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2024-03-05 18:30:13.000 UTC</summary>

(do not blindly implement as is, see discussion starting with comment https://github.com/temporalio/features/issues/394#issuecomment-1966551344 before proceeding)

</details>

<details>
<summary><strong>jherbert</strong> commented on 2025-07-23 19:03:36.000 UTC</summary>

@cretz  Know of any workarounds for this in the Python SDK? With the `always_create_workflow_spans` #794  parameter do get spans created when kicked off from a schedule, but without a common parent span so they are all not associated with each other.

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-07-24 13:55:05.000 UTC</summary>

There is not currently a workaround since scheduled workflows are created server side and spans are created client side. Even if you create your own span (like we do) when workflow starts client-side, that span will not remain the parent if the workflow runs on another worker.

</details>


---

<a id="449"></a>

### #449: [Bug] Scheduled workflow did not receive OpenTelemetry tracing

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/449 |
| **State** | CLOSED |
| **Author** | Holi0317 (Hollis Wu) |
| **Created** | 2023-12-15 03:47:22.000 UTC (2 years ago) |
| **Updated** | 2024-01-03 21:17:38.000 UTC |
| **Closed** | 2024-01-03 21:17:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

Implement otel tracing for our workflows, where most are triggered by temporal schedule.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

Trace information is not generated and propagated for scheduled workflow executions.

### Minimal Reproduction

https://github.com/Holi0317/temporal-bugreport/tree/py-schedule-tracing

1. Run `poetry install`
2. Start jaeger with following snippet:
```bash
docker run -d --name jaeger \
  -e COLLECTOR_OTLP_ENABLED=true \
  -p 16686:16686 \
  -p 4317:4317 \
  -p 4318:4318 \
  jaegertracing/all-in-one:latest
```
3. Run `poetry run python temporal_tracing/worker.py` to start the worker
4. Run `poetry run python temporal_tracing/starter.py` to run the workflow once by manual invoke
5. Run `poetry run python temporal_tracing/scheduler.py` to schedule workflow execution. Wait til a minute passed and there should be execution result of scheduled execution

If the workflow is scheduled, jaeger shows 2 distinct traces with different trace ID shown in the console, each only with `RunActivity:compose_greeting` in it

```
Worker started, ctrl+c to exit
Hello in activity. Let's check the traces b69e29b63117d84f98d1095948520fd3 55c467efaf77c85d
Hello in activity. Let's check the traces daea5868fe257f559621af68d9bd869d 66f6fc4a8d938155
```

![SCR-20231214-tpaa](https://github.com/temporalio/sdk-python/assets/6838982/d5d77ce5-4eb2-4791-9576-22d4c7fef514)

If the workflow is invoked manually, jaeger shows a single trace that looks not broken. And the trace ID are the same across activities:

```
Worker started, ctrl+c to exit
Hello in activity. Let's check the traces 3fd71755262b64b3bc6097d93f419822 cb8aca4ed13cfc11
Hello in activity. Let's check the traces 3fd71755262b64b3bc6097d93f419822 598ecfcfb4157a7e
```

![SCR-20231214-tqpo](https://github.com/temporalio/sdk-python/assets/6838982/4376c7e0-514f-4973-825b-9316b94998e0)
![SCR-20231214-tqqa](https://github.com/temporalio/sdk-python/assets/6838982/94aefb43-2a1b-4706-ae47-0e29a742df2a)

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: M1 Mac
- Temporal Version: [e.g. 1.14.0?] and/or SDK version: Temporal version 0.10.7 (server 1.22.2) (ui 2.21.3). SDK 1.4.0
- Are you using Docker or Kubernetes or building Temporal from source? Temporal dev server from CLI

### Additional context

<!-- Add any other context about the problem here. -->

Kinda relates to https://github.com/temporalio/sdk-python/issues/362.

Related slack thread: https://temporalio.slack.com/archives/CTT84RS0P/p1702589499220879 (Issue is described above and the message did not provide more context. Not going to copy-paste the message here)

#### Comments (3)

<details>
<summary><strong>mjameswh</strong> commented on 2023-12-15 15:02:52.000 UTC</summary>

Seems like we did in Go. Missing in all other SDKs.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-12-15 15:12:06.000 UTC</summary>

Mentioned in Slack, but basically only user-controlled clients can create the initial spans for workflows when starting workflows. We can't know what tracer or tracing framework a user is using to create the span for them when we start the workflow server side. We don't create parentless spans on the worker side if there is no parent span coming from the client side because it would be a bunch of orphans, but maybe we can support orphaned worker-side spans or maybe we can support some new mechanism to attach start-workflow headers on first task. The SDK team will discuss options here.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-01-03 21:16:36.000 UTC</summary>

Separate issue opened. This will be fixed in #454.

Reactions: ðŸš€ 1

</details>


---

<a id="444"></a>

### #444: [Bug] Fix lint warnings for Client.execute_workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/444 |
| **State** | CLOSED |
| **Author** | lethe3000 (YiLi) |
| **Created** | 2023-12-11 03:30:13.000 UTC (2y 1m ago) |
| **Updated** | 2023-12-12 13:31:03.000 UTC |
| **Closed** | 2023-12-12 13:31:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Execute a workflow using python sdk: client.execute_workflow

### Describe the bug

I followed tutorial: Build a data pipeline Workflow with Temporal and Python with code
```
stories = await client.execute_workflow(
        TemporalCommunityWorkflow.run,
        id="temporal-community-workflow",
        task_queue=TASK_QUEUE_NAME,
    )
```
 but got lint warnings as below:

```
Unexpected type(s):(() -> Coroutine[Any, Any, list[TemporalCommunityPost]], str, Any)Possible type(s):(MethodAsyncNoParam[SelfType, ReturnType], str, str)(str, str, str)
```

### Minimal Reproduction


### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Mac intel
- Temporal python sdk: 1.4.0
- IDE: Pycharm CE


#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2023-12-11 13:44:42.000 UTC</summary>

Assuming the workflow properly doesn't have any parameters, I wonder if Pycharm is the problem here and not Temporal. Can you confirm this same issue happens with other type checkers like MyPy and Pyright/vscode?

</details>

<details>
<summary><strong>lethe3000</strong> commented on 2023-12-12 02:07:34.000 UTC</summary>

> Assuming the workflow properly doesn't have any parameters, I wonder if Pycharm is the problem here and not Temporal. Can you confirm this same issue happens with other type checkers like MyPy and Pyright/vscode?

I tested my code with pyright, no issues reported, it should be a Pycharm issue.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-12-12 13:31:03.000 UTC</summary>

:+1: Closing issue. May be worth opening an issue there. We test with MyPy in our CI, but we don't have one for a specific IDE.

</details>


---

<a id="432"></a>

### #432: [Bug] Exception raise from workflow causes indefinite retry

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/432 |
| **State** | CLOSED |
| **Author** | Holi0317 (Hollis Wu) |
| **Created** | 2023-11-25 03:52:50.000 UTC (2y 1m ago) |
| **Updated** | 2025-07-02 23:04:32.000 UTC |
| **Closed** | 2023-11-27 14:23:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I am trying to raise an exception inside workflow which should cause the workflow to fail.

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

### Describe the bug

In the official concept documentation, the expected default behavior of retry on workflow is:

> Workflow Execution: When a Workflow Execution is spawned, it is not associated with a default Retry Policy and thus does not retry by default.
>
> https://docs.temporal.io/retry-policies#default-behavior

I try to raise an `RuntimeError`, or actually just a `ValueError` will cause this problem where python sdk raise "Failed activation on workflow" and retries the workflow indefinitely.

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction

<details>
  <summary>Python</summary>
  
```python
import asyncio
import logging

from temporalio import workflow
from temporalio.client import Client, WorkflowFailureError
from temporalio.worker import Worker


@workflow.defn
class GreetingWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        raise RuntimeError("Goodbye world")

        return ""


async def main():
    # Start client
    client = await Client.connect("localhost:7233")

    # Run a worker for the workflow
    async with Worker(
        client,
        task_queue="hello-exception-task-queue",
        workflows=[GreetingWorkflow],
    ):
        # While the worker is running, use the client to run the workflow and
        # print out its result. Note, in many production setups, the client
        # would be in a completely separate process from the worker.
        #
        # This will raise a WorkflowFailureError with cause ActivityError with
        # cause ApplicationError with the error message and stack trace.
        try:
            await client.execute_workflow(
                GreetingWorkflow.run,
                "World",
                id="hello-exception-workflow-id",
                task_queue="hello-exception-task-queue",
            )
        except WorkflowFailureError:
            # Log the exception
            logging.exception("Got workflow failure")


if __name__ == "__main__":
    asyncio.run(main())
```

</details>

It fails with following stacktrace:

```
Failed activation on workflow GreetingWorkflow with ID hello-exception-workflow-id and run ID 0d6c8b96-0db1-4652-a19a-d11055526325
Traceback (most recent call last):
  File "/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py", line 315, in activate
    self._run_once(check_conditions=index == 1 or index == 2)
  File "/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py", line 1617, in _run_once
    raise self._current_activation_error
  File "/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py", line 1635, in _run_top_level_workflow_function
    await coro
  File "/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py", line 766, in run_workflow
    result = await self._inbound.execute_workflow(input)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py", line 1888, in execute_workflow
    return await input.run_fn(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Volumes/Projects/example/main.py", line 13, in run
    raise RuntimeError("Goodbye world")
RuntimeError: Goodbye world
2023-11-25T03:38:06.936582Z  WARN temporal_sdk_core::worker::workflow: Failing workflow task run_id=0d6c8b96-0db1-4652-a19a-d11055526325 failure=Failure { failure: Some(Failure { message: "Goodbye world", source: "", stack_trace: "  File \"/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py\", line 315, in activate\n    self._run_once(check_conditions=index == 1 or index == 2)\n\n  File \"/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py\", line 1617, in _run_once\n    raise self._current_activation_error\n\n  File \"/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py\", line 1635, in _run_top_level_workflow_function\n    await coro\n\n  File \"/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py\", line 766, in run_workflow\n    result = await self._inbound.execute_workflow(input)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py\", line 1888, in execute_workflow\n    return await input.run_fn(*args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Volumes/Projects/example/main.py\", line 13, in run\n    raise RuntimeError(\"Goodbye world\")\n", encoded_attributes: None, cause: None, failure_info: Some(ApplicationFailureInfo(ApplicationFailureInfo { r#type: "RuntimeError", non_retryable: false, details: None })) }), force_cause: Unspecified }
Failed activation on workflow GreetingWorkflow with ID hello-exception-workflow-id and run ID 0d6c8b96-0db1-4652-a19a-d11055526325
Traceback (most recent call last):
  File "/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py", line 315, in activate
    self._run_once(check_conditions=index == 1 or index == 2)
  File "/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py", line 1617, in _run_once
    raise self._current_activation_error
  File "/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py", line 1635, in _run_top_level_workflow_function
    await coro
  File "/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py", line 766, in run_workflow
    result = await self._inbound.execute_workflow(input)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/holliswu/Library/Caches/pypoetry/virtualenvs/example-5pZqKdJX-py3.12/lib/python3.12/site-packages/temporalio/worker/_workflow_instance.py", line 1888, in execute_workflow
    return await input.run_fn(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Volumes/Projects/example/main.py", line 13, in run
    raise RuntimeError("Goodbye world")
RuntimeError: Goodbye world
```

And temporal UI showing the workflow is "running", not "Failed"

> Ignore the "No Workers Running" warning. I took this screenshot in last minute, after killing the python worker.

![image](https://github.com/temporalio/sdk-python/assets/6838982/71869551-473e-403b-b271-d3526373f276)


I did try to replicate this issue with go and the temporal workflow stopped correctly there:

<details>
  <summary>Go implementation</summary>

```go
package main

import (
	"context"
	"fmt"
	"log"
	"time"

	"go.temporal.io/sdk/client"

	"go.temporal.io/sdk/worker"
	"go.temporal.io/sdk/workflow"
)

// Workflow is a Hello World workflow definition.
func Workflow(ctx workflow.Context, name string) (string, error) {
	ao := workflow.ActivityOptions{
		StartToCloseTimeout: 10 * time.Second,
	}
	ctx = workflow.WithActivityOptions(ctx, ao)

	logger := workflow.GetLogger(ctx)
	logger.Info("HelloWorld workflow started", "name", name)

	return "", fmt.Errorf("Goodbye world")
}

func workerMain() {
	// The client and worker are heavyweight objects that should be created once per process.
	c, err := client.Dial(client.Options{})
	if err != nil {
		log.Fatalln("Unable to create client", err)
	}
	defer c.Close()

	w := worker.New(c, "hello-world", worker.Options{})

	w.RegisterWorkflow(Workflow)

	err = w.Run(worker.InterruptCh())
	if err != nil {
		log.Fatalln("Unable to start worker", err)
	}
}

func main() {
	go workerMain()

	// The client is a heavyweight object that should be created once per process.
	c, err := client.Dial(client.Options{})
	if err != nil {
		log.Fatalln("Unable to create client", err)
	}
	defer c.Close()

	workflowOptions := client.StartWorkflowOptions{
		ID:        "hello_world_workflowID",
		TaskQueue: "hello-world",
	}

	we, err := c.ExecuteWorkflow(context.Background(), workflowOptions, Workflow, "Temporal")
	if err != nil {
		log.Fatalln("Unable to execute workflow", err)
	}

	log.Println("Started workflow", "WorkflowID", we.GetID(), "RunID", we.GetRunID())

	// Synchronously wait for the workflow completion.
	var result string
	err = we.Get(context.Background(), &result)
	if err != nil {
		log.Fatalln("Unable get workflow result", err)
	}
	log.Println("Workflow result:", result)
}
```

</details>

Output of the go version is:

```
2023/11/24 22:45:19 INFO  No logger configured for temporal client. Created default one.
2023/11/24 22:45:19 INFO  No logger configured for temporal client. Created default one.
2023/11/24 22:45:19 Started workflow WorkflowID hello_world_workflowID RunID ae2b5d3e-ff8a-4183-a717-575efbb240e0
2023/11/24 22:45:19 INFO  Started Worker Namespace default TaskQueue hello-world WorkerID 15651@Holliss-MacBook-Pro.local@
2023/11/24 22:45:19 INFO  HelloWorld workflow started Namespace default TaskQueue hello-world WorkerID 15651@Holliss-MacBook-Pro.local@ WorkflowType Workflow WorkflowID hello_world_workflowID RunID ae2b5d3e-ff8a-4183-a717-575efbb240e0 Attempt 1 name Temporal
2023/11/24 22:45:19 Unable get workflow result workflow execution error (type: Workflow, workflowID: hello_world_workflowID, runID: ae2b5d3e-ff8a-4183-a717-575efbb240e0): Goodbye world
exit status 1
```

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: M1 Mac
- Temporal Version: temporal version 0.10.7 (server 1.22.2) (ui 2.21.3)
  - Python SDK version: 1.4.0
  - Go SDK version: 1.25.1
- Are you using Docker or Kubernetes or building Temporal from source? No. It's `temporal` cli from brew

### Additional context

I did found https://github.com/temporalio/sdk-python/pull/329 which seems related to this issue.

<!-- Add any other context about the problem here. -->


#### Comments (3)

<details>
<summary><strong>Quinn-With-Two-Ns</strong> commented on 2023-11-27 00:50:12.000 UTC</summary>

This is expected, in all exception based SDKs  (like python) only Temporal Errors fail a Workflow Execution by default, other exceptions will retry the Workflow Task.  You can use [ApplicationError ](https://python.temporal.io/temporalio.exceptions.ApplicationError.html) in  the Python SDK to fail the workflow.

This is also in our documentation
https://docs.temporal.io/references/failures#throw-from-workflows

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-11-27 14:23:58.000 UTC</summary>

:+1: To what @Quinn-With-Two-Ns said. Closing, but feel free to continue conversation here or on `#python-sdk` Slack or in the forums.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>fabianoengler</strong> commented on 2025-07-02 23:04:32.000 UTC</summary>

I find this behavior really weird an unexpected, I ended up here after a lot of googling and had a similar journey than the OP, but my motivation is a bit different: it's for testing.

I understand all the idea behind the concept of "workflows should never fail".

But in case of writing tests for the workflows you'd expect that if the workflow fails the test would fail as well. But that's not what happens, if the workflow fails the workflow keeps being retried indefinitely and just hangs the test suit. An that's case is a really PITA to debug.

So I spent a lot of hours googling and fiddling with the SDK source code to try to disable this automatic workflow retry.

I finally found how to disable this and that's the main reason of my comment here, if others find themselves on the same situation and end landing here, hopefully they will find this comment.

First thing I tried was to set a retry policy on the workflow with `RetryPolicy(maximum_attempts=1)`, but that doesn't work. I also tried `RetryPolicy(non_retryable_error_types=['Exception'])` but that doesn't work either, which is very surprising to me.

I finally made it work by adding `workflow_failure_exception_types=[Exception]` to the test worker:

```py
async def test_MyWorkflow(temporal_client: 'Client'):
    task_queue_name = str(uuid.uuid4())
    async with Worker(
        temporal_client,
        task_queue=task_queue_name,
        workflows=[
            MyWorkflow,
        ],
        activities=[
            my_activity1_mocked,
            my_activity2_mocked,
            my_activity3_mocked,
        ],
        workflow_failure_exception_types=[Exception],
    ):
        result = await temporal_client.execute_workflow(
            MyWorkflow.run,
            {},
            id=str(uuid.uuid4()),
            task_queue=task_queue_name,
        )
```

Reactions: ðŸ‘ 1

</details>


---

<a id="420"></a>

### #420: [Feature Request] Make start_update/execute_update work with vscode/pyright and add pyright to CI

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/420 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-09 14:31:29.000 UTC (2y 2m ago) |
| **Updated** | 2024-08-27 17:02:53.000 UTC |
| **Closed** | 2024-08-27 17:02:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Right now vscode/pylance/pyright fails with:
![image](https://github.com/temporalio/sdk-python/assets/206396/f53fe5da-843c-4f40-a3f1-dbf842129d8b)

I think we probably just need to remove the typing for multi-param update (use `Sequence[Any]` like signal/query). Then we need to add https://github.com/microsoft/pyright to CI alongside MyPy if it's not too difficult.

#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2023-11-09 15:15:09.000 UTC</summary>

Also note that the return type for `execute_update` and similar is not properly inferred from the method in Pyright like it is in MyPy

</details>

<details>
<summary><strong>dandavison</strong> commented on 2024-02-12 20:46:20.000 UTC</summary>

Another example:

```
samples-python(main) .venv/bin/pyright --pythonpath .venv/bin/python hello/hello_activity.py
/Users/dan/src/temporalio/samples-python/hello/hello_activity.py
  /Users/dan/src/temporalio/samples-python/hello/hello_activity.py:58:24 - error: No overloads for "execute_workflow" match the provided arguments (reportCallIssue)
  /Users/dan/src/temporalio/samples-python/hello/hello_activity.py:59:13 - error: Argument of type "(self: GreetingWorkflow, name: str) -> Coroutine[Any, Any, str]" cannot be assigned to parameter "workflow" of type "str" in function "execute_workflow"
    "function" is incompatible with "str" (reportArgumentType)
2 errors, 0 warnings, 0 informations
```

```
samples-python(main) .venv/bin/pyright --version
pyright 1.1.350
samples-python(main) .venv/bin/python --version
Python 3.10.13
```

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-02-13 22:07:41.000 UTC</summary>

So #471 probably fixed this error, but leaving this open because we still want pyright in CI

</details>


---

<a id="415"></a>

### #415: [Feature Request] Add `temporalio.workflow.in_workflow()` call

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/415 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-06 16:20:07.000 UTC (2y 2m ago) |
| **Updated** | 2025-04-07 15:50:54.000 UTC |
| **Closed** | 2025-04-07 15:50:54.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Describe the solution you'd like

Similar to https://www.javadoc.io/static/io.temporal/temporal-sdk/1.22.2/io/temporal/workflow/unsafe/WorkflowUnsafe.html#isWorkflowThread() or https://typescript.temporal.io/api/namespaces/workflow#inworkflowcontext, but discourage its use of course.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-03-19 13:02:07.000 UTC</summary>

Arguably this doesn't have to be in "unsafe" because it is not unsafe _technically_, up to implementer

</details>


---

<a id="399"></a>

### #399: [Bug] Older Python versions cannot auto-import activity return class in sandbox

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/399 |
| **State** | OPEN |
| **Author** | devery59 (Kris) |
| **Created** | 2023-10-12 01:56:27.000 UTC (2y 3m ago) |
| **Updated** | 2023-10-20 12:38:56.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸš€ 1 |

#### Description

### What are you really trying to do?
I'm Using Temporal on EKS Cluster and I try to connect temporal in other application and run workflow. When I use custom @dataclass as return value of activity, "Failed decoding arguments" error has occured. 
<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

```
{
  "message": "Failed decoding arguments",
  "source": "",
  "stackTrace": "  File \"/usr/local/lib/python3.9/site-packages/temporalio/worker/_workflow_instance.py\", line 301, in activate\n    self._apply(job)\n\n  File \"/usr/local/lib/python3.9/site-packages/temporalio/worker/_workflow_instance.py\", line 373, in _apply\n    self._apply_resolve_activity(job.resolve_activity)\n\n  File \"/usr/local/lib/python3.9/site-packages/temporalio/worker/_workflow_instance.py\", line 490, in _apply_resolve_activity\n    ret_vals = self._convert_payloads(\n\n  File \"/usr/local/lib/python3.9/site-packages/temporalio/worker/_workflow_instance.py\", line 1232, in _convert_payloads\n    raise RuntimeError(\"Failed decoding arguments\") from err\n",
  "encodedAttributes": null,
  "cause": {
    "message": "<Custom dataclass path>",
    "source": "",
    "stackTrace": "  File \"/usr/local/lib/python3.9/site-packages/temporalio/worker/_workflow_instance.py\", line 1224, in _convert_payloads\n    return self._payload_converter.from_payloads(\n\n  File \"/usr/local/lib/python3.9/site-packages/temporalio/converter.py\", line 307, in from_payloads\n    values.append(converter.from_payload(payload, type_hint))\n\n  File \"/usr/local/lib/python3.9/site-packages/temporalio/converter.py\", line 569, in from_payload\n    obj = value_to_type(type_hint, obj, self._custom_type_converters)\n\n  File \"/usr/local/lib/python3.9/site-packages/temporalio/converter.py\", line 1361, in value_to_type\n    field_hints = get_type_hints(hint)\n\n  File \"/usr/local/lib/python3.9/typing.py\", line 1450, in get_type_hints\n    base_globals = sys.modules[base.__module__].__dict__\n\n  File \"/usr/local/lib/python3.9/site-packages/temporalio/worker/workflow_sandbox/_importer.py\", line 393, in __getitem__\n    return self.current[key]\n",
    "encodedAttributes": null,
    "cause": null,
    "applicationFailureInfo": {
      "type": "KeyError",
      "nonRetryable": false,
      "details": null
    }
  },
  "applicationFailureInfo": {
    "type": "RuntimeError",
    "nonRetryable": false,
    "details": null
  }
}
``` 

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

This is my execution code.
```
async with Worker(
        client,
        task_queue=TASK_QUEUE_NAME,
        workflows=[test_workflow],
        activities=[get_request_s3_dir],
    ):

        await client.execute_workflow(
            test_workflow.run,
            request,
            id=REQUEST_ID,
            task_queue=TASK_QUEUE_NAME,
            retry_policy=RetryPolicy(maximum_interval=timedelta(seconds=2), maximum_attempts=3),
        )
``` 

This is my workflow code.
```
@workflow.defn
class test_workflow:
 
    @workflow.run
    async def run(self, request: requestDataClass):
        request_s3_dir = await workflow.execute_activity(
            get_request_s3_dir,
            args = [BUCKET_NAME, request.request_id],
            start_to_close_timeout=timedelta(seconds=15),
        )
``` 
This is my activity code.
```
@activity.defn
async def get_request_s3_dir(bucket_name: str, request_id: str) -> customDataClass:
    uri = f"s3://{bucket_name}/{request_id}"
    return customDataClass.from_uri(uri) 
``` 
This is my custom dataclass code.
```
@dataclass
class customDataClass:
    uri: str
    bucket_name: str
    object_key: str

    @classmethod
    def from_uri(cls, uri: str):
        ...
        return cls(uri, bucket_name, object_key)
``` 
### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Make Docker image and run pod in EKS
- Temporal Version: 2.16.2(on my web) & temporalio == 1.3.0
- base Image of Docker : python:3.9-slim-buster 

### Additional context

<!-- Add any other context about the problem here. -->

1. I'm using python 3.9 version to test on EKS. But when i use python 3.11 version as my Base Image or python setting there is no issue. Is there any difference between python 3.9 version's temporal env and 3.11 version's temporal env? ( I checked pip list versions. requirements version is same)
2. I know if i use in my workflow like below...
```
in python 3.9 verison ( above problem is solved)
with workflow.unsafe.imports_passed_through():
    from file import customDataClass
``` 
```
in python 3.11 verison
 < no need to import >
``` 

3. Let me know what is stable python version when i use temporal cluster and is there any best way to use pattern like this?


Thanks for your all effort.

#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2023-10-12 12:24:58.000 UTC</summary>

> \<Custom dataclass path>

Is the full error message just a path to the data class? Can I get the _exact_ error message?

> Is there any difference between python 3.9 version's temporal env and 3.11 version's temporal env?

None in our code, but Python may have changed how imports work between the two

> I know if i use in my workflow like below...

So it appears in newer Python versions the sandbox importer can import your dataclass but in older Python versions you have to import it yourself

> Minimal Reproduction

It may be easier with a full replication instead of just snippets, but I may have the idea here.

Regardless, I will see if I can investigate this. In the meantime, import the custom data class yourself as you have shown. Or upgrade to your Python version.




Reactions: â¤ï¸ 1

</details>

<details>
<summary><strong>devery59</strong> commented on 2023-10-13 03:32:08.000 UTC</summary>

@cretz 
Thanks for reply! 

> Is the full error message just a path to the data class? Can I get the exact error message?

Yes error message just a path to the data class

> Regardless, I will see if I can investigate this. In the meantime, import the custom data class yourself as you have shown. Or upgrade to your Python version.

Thanks for your comment! I expect specific reason of above issue. Let me know if there is another thought.


</details>

<details>
<summary><strong>cretz</strong> commented on 2023-10-20 12:38:56.000 UTC</summary>

Is it possible to provide a standalone replication? Maybe alter https://github.com/temporalio/samples-python/blob/main/hello/hello_activity.py until you can replicate the failure? (feel free to separate out to multiple files)

</details>


---

<a id="394"></a>

### #394: [Bug] Compilation fails for ARCH `s390x` and `ppc64el`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/394 |
| **State** | CLOSED |
| **Author** | troyanov (Anton Troyanov) |
| **Created** | 2023-10-06 09:55:37.000 UTC (2y 3m ago) |
| **Updated** | 2023-10-29 20:50:43.000 UTC |
| **Closed** | 2023-10-29 20:50:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I am trying to cross compile `core-sdk` in order to use [python-sdk ](https://github.com/temporalio/sdk-python)on `s390x` and `ppc64el` architectures

### Describe the bug

```
error: failed to run custom build command for `ring v0.16.20`

Caused by:
  process didn't exit successfully: `/Users/troyanov/Documents/work/canonical/temporalio/sdk-core/target/debug/build/ring-f698636ef115f979/build-script-build` (exit status: 101)
  --- stderr
  thread 'main' panicked at 'called `Option::unwrap()` on a `None` value', /Users/troyanov/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ring-0.16.20/build.rs:358:10
  note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
warning: build failed, waiting for other jobs to finish...
```

### Minimal Reproduction

`cargo build --target=s390x-unknown-linux-gnu` 
`cargo build --target=powerpc64le-unknown-linux-gnu` 

(make sure required targets are installed, if not add them with `rustup target add`)

### Environment/Versions

```
â¯ git rev-parse @
617612aa419d687aebabcf0258ac86f5c36df189

â¯ rustc --version
rustc 1.71.0 (8ede3aae2 2023-07-12)

â¯ rustup --version
rustup 1.26.0 (5af9b9484 2023-04-05)
info: This is the version for the rustup toolchain manager, not the rustc compiler.
info: The currently active `rustc` version is `rustc 1.71.0 (8ede3aae2 2023-07-12)`

â¯ sysctl -n machdep.cpu.brand_string
Apple M1 Pro
```

### Additional context

Compilation issue should be fixed in the recent `ring` [v0.17.0](https://crates.io/crates/ring/0.17.0) version.
Judging based on the following issues were marked as fixed:

- briansmith/ring#986
- briansmith/ring#1182
- briansmith/ring#1630

```
ring v0.16.20
â”œâ”€â”€ rustls v0.21.7
â”‚   â”œâ”€â”€ hyper-rustls v0.24.1
â”‚   â”‚   â””â”€â”€ reqwest v0.11.21
â”‚   â”‚       â””â”€â”€ temporal-sdk-core v0.1.0 (/cyberfarm/temporalio/sdk-core/core)
â”‚   â”‚           â”œâ”€â”€ temporal-sdk v0.1.0-alpha.1 (/cyberfarm/temporalio/sdk-core/sdk)
â”‚   â”‚           â”‚   â””â”€â”€ temporal-sdk-core-test-utils v0.1.0 (/cyberfarm/temporalio/sdk-core/test-utils)
â”‚   â”‚           â”‚       [dev-dependencies]
â”‚   â”‚           â”‚       â””â”€â”€ temporal-sdk-core v0.1.0 (/cyberfarm/temporalio/sdk-core/core) (*)
â”‚   â”‚           â”‚   [dev-dependencies]
â”‚   â”‚           â”‚   â””â”€â”€ temporal-sdk-core v0.1.0 (/cyberfarm/temporalio/sdk-core/core) (*)
â”‚   â”‚           â””â”€â”€ temporal-sdk-core-test-utils v0.1.0 (/cyberfarm/temporalio/sdk-core/test-utils) (*)
â”‚   â”œâ”€â”€ reqwest v0.11.21 (*)
â”‚   â””â”€â”€ tokio-rustls v0.24.1
â”‚       â”œâ”€â”€ hyper-rustls v0.24.1 (*)
â”‚       â”œâ”€â”€ reqwest v0.11.21 (*)
â”‚       â””â”€â”€ tonic v0.9.2
â”‚           â”œâ”€â”€ opentelemetry-otlp v0.13.0
â”‚           â”‚   â””â”€â”€ temporal-sdk-core v0.1.0 (/cyberfarm/temporalio/sdk-core/core) (*)
â”‚           â”œâ”€â”€ opentelemetry-proto v0.3.0
â”‚           â”‚   â””â”€â”€ opentelemetry-otlp v0.13.0 (*)
â”‚           â”œâ”€â”€ temporal-client v0.1.0 (/cyberfarm/temporalio/sdk-core/client)
â”‚           â”‚   â”œâ”€â”€ temporal-sdk v0.1.0-alpha.1 (/cyberfarm/temporalio/sdk-core/sdk) (*)
â”‚           â”‚   â”œâ”€â”€ temporal-sdk-core v0.1.0 (/cyberfarm/temporalio/sdk-core/core) (*)
â”‚           â”‚   â””â”€â”€ temporal-sdk-core-test-utils v0.1.0 (/cyberfarm/temporalio/sdk-core/test-utils) (*)
â”‚           â”œâ”€â”€ temporal-sdk v0.1.0-alpha.1 (/cyberfarm/temporalio/sdk-core/sdk) (*)
â”‚           â”œâ”€â”€ temporal-sdk-core v0.1.0 (/cyberfarm/temporalio/sdk-core/core) (*)
â”‚           â”œâ”€â”€ temporal-sdk-core-api v0.1.0 (/cyberfarm/temporalio/sdk-core/core-api)
â”‚           â”‚   â”œâ”€â”€ temporal-client v0.1.0 (/cyberfarm/temporalio/sdk-core/client) (*)
â”‚           â”‚   â”œâ”€â”€ temporal-sdk v0.1.0-alpha.1 (/cyberfarm/temporalio/sdk-core/sdk) (*)
â”‚           â”‚   â”œâ”€â”€ temporal-sdk-core v0.1.0 (/cyberfarm/temporalio/sdk-core/core) (*)
â”‚           â”‚   â””â”€â”€ temporal-sdk-core-test-utils v0.1.0 (/cyberfarm/temporalio/sdk-core/test-utils) (*)
â”‚           â””â”€â”€ temporal-sdk-core-protos v0.1.0 (/cyberfarm/temporalio/sdk-core/sdk-core-protos)
â”‚               â”œâ”€â”€ temporal-client v0.1.0 (/cyberfarm/temporalio/sdk-core/client) (*)
â”‚               â”œâ”€â”€ temporal-sdk v0.1.0-alpha.1 (/cyberfarm/temporalio/sdk-core/sdk) (*)
â”‚               â”œâ”€â”€ temporal-sdk-core v0.1.0 (/cyberfarm/temporalio/sdk-core/core) (*)
â”‚               â”œâ”€â”€ temporal-sdk-core-api v0.1.0 (/cyberfarm/temporalio/sdk-core/core-api) (*)
â”‚               â””â”€â”€ temporal-sdk-core-test-utils v0.1.0 (/cyberfarm/temporalio/sdk-core/test-utils) (*)
â”œâ”€â”€ rustls-webpki v0.101.6
â”‚   â””â”€â”€ rustls v0.21.7 (*)
â””â”€â”€ sct v0.7.0
    â””â”€â”€ rustls v0.21.7 (*)
```

Since `core-sdk` doesn't depend directly on the `ring` crate, dependant dependencies should be tracked.
Most of them depend on `ring` through `rustls`
- corresponding PR to bump `ring` version in `rustls` rustls/rustls#1508



#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2023-10-06 14:24:39.000 UTC</summary>

Thanks for the report! Since this is actually a dependency of a dependency of a dependency and core is a library, there is nothing to do in this repo since there is no `Cargo.lock`. I am moving this issue to the Python repo.

I don't see why, once rustls merges and tags a release w/ ring updated, we can't then `cargo update` in `temporalio/bridge` dir to get updates. I will watch the PR (let us know if the tag happens and we didn't notice though). Reading the comments on that ring PR, it's looking hopeful they will backport this to a `0.21` patch release. If they don't, we'd need https://github.com/rustls/tokio-rustls to accept the update first.

</details>

<details>
<summary><strong>troyanov</strong> commented on 2023-10-06 16:16:34.000 UTC</summary>

Thanks for a quick reply. Indeed `sdk-python` might be a better place for this bug.

Just for the reference, here is a corresponding `0.21` backport PR to track
- https://github.com/rustls/rustls/pull/1525

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>troyanov</strong> commented on 2023-10-29 20:50:43.000 UTC</summary>

@cretz it seems the issue is solved now as `rustls` was bumped to `0.21.8` in https://github.com/temporalio/sdk-python/pull/400

I was able to cross-compile temporalio/bridge without any problems.

Reactions: ðŸ‘ 1

</details>


---

<a id="385"></a>

### #385: [Bug] Client stuck connecting or starting a workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/385 |
| **State** | CLOSED |
| **Author** | KundaPanda (VojtÄ›ch Dohnal) |
| **Created** | 2023-09-14 10:58:00.000 UTC (2y 3m ago) |
| **Updated** | 2023-10-06 16:35:49.000 UTC |
| **Closed** | 2023-10-06 16:35:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

To create a second client in a different process (i.e. in gunicorn).

### Describe the bug

The client randomly hangs during `connect` of a client in a new process, after another client was created in the current process.

### Minimal Reproduction

<details><summary>Code from a modified sample</summary>

```python
import asyncio
import logging
from dataclasses import dataclass
from datetime import timedelta

from temporalio import activity, workflow
from temporalio.client import Client
from temporalio.runtime import Runtime, TelemetryConfig
from temporalio.worker import Worker


# While we could use multiple parameters in the activity, Temporal strongly
# encourages using a single dataclass instead which can have fields added to it
# in a backwards-compatible way.
@dataclass
class ComposeGreetingInput:
    greeting: str
    name: str


# Basic activity that logs and does string concatenation
@activity.defn
async def compose_greeting(input: ComposeGreetingInput) -> str:
    activity.logger.info("Running activity with parameter %s" % input)
    return f"{input.greeting}, {input.name}!"


# Basic workflow that logs and invokes an activity
@workflow.defn
class GreetingWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        workflow.logger.info("Running workflow with parameter %s" % name)
        return await workflow.execute_activity(
            compose_greeting,
            ComposeGreetingInput("Hello", name),
            start_to_close_timeout=timedelta(seconds=10),
        )


async def run_workflow():
    logging.debug("Creating a temporal client")
    client = await Client.connect("localhost:7233", runtime=Runtime(telemetry=TelemetryConfig()))
    res = await client.execute_workflow(
        GreetingWorkflow.run,
        "World",
        id="hello-activity-workflow-id",
        task_queue="hello-activity-task-queue",
    )
    logging.debug(f"Result: {res}")


async def worker():
    client = await Client.connect("localhost:7233")
    worker = Worker(
        client,
        task_queue="hello-activity-task-queue",
        workflows=[GreetingWorkflow],
        activities=[compose_greeting],
        debug_mode=True,
    )
    await worker.run()


async def main():
    from multiprocessing import Process

    # Uncomment the line below to see logging
    logging.basicConfig(level=logging.DEBUG)

    worker_process = Process(target=lambda: asyncio.run(worker()))
    worker_process.start()

    # While the worker is running, use the client to run the workflow and
    # print out its result. Note, in many production setups, the client
    # would be in a completely separate process from the worker.
    await run_workflow()
    p = Process(target=lambda: asyncio.run(run_workflow()))
    p.start()
    p.join()
    worker_process.terminate()


if __name__ == "__main__":
    asyncio.run(main())

````

</details>

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Fedora 38, Ryzen 7 PRO 6850U
- Temporal Version: Python SDK 1.3.0, Temporal 1.22.0
- Are you using Docker or Kubernetes or building Temporal from source? Yes, the *auto-setup* docker image

### Additional context

The client usually hangs in `temporalio.bridge.client.py:78` at `temporalio.bridge.temporal_sdk_bridge.connect_client`.


#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2023-09-18 13:41:19.000 UTC</summary>

> To create a second client in a different process (i.e. in gunicorn).

Does this mean in a post-forked process? The SDK does not work with forking at this time (#11). It would be a challenge to share the Rust runtime across forks properly/safely.

</details>

<details>
<summary><strong>KundaPanda</strong> commented on 2023-09-20 07:03:02.000 UTC</summary>

Yes it does, I haven't found this limitation anywhere in the Temporal.io documentation and thought that it could be solved by passing a new Runtime instance to the Client.connect method. Is this not the case? What exactly does a Runtime object represent?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-09-22 13:05:04.000 UTC</summary>

> I haven't found this limitation anywhere in the Temporal.io documentation

:+1: We will add documentation for this

> thought that it could be solved by passing a new Runtime instance to the Client.connect method. Is this not the case? What exactly does a Runtime object represent?

No it cannot be solved that way. A runtime represents a Rust Tokio thread pool and runtime (see https://github.com/tokio-rs/tokio/issues/1541 and similar).

Reactions: ðŸ‘ 1

</details>


---

<a id="367"></a>

### #367: [Bug] Temporal graceful shutdown does not prevent activity failure

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/367 |
| **State** | CLOSED |
| **Author** | jayfjin (Jay Jin) |
| **Created** | 2023-08-10 21:57:49.000 UTC (2y 5m ago) |
| **Updated** | 2023-08-11 13:52:20.000 UTC |
| **Closed** | 2023-08-11 13:52:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?
We have a long running activity and would like for it not to be retried upon a rolling restart of our worker fleet.


### Describe the bug

Despite setting graceful_shutdown_timeout=timedelta(minutes=20), we still see our activity fail once SIGTERM is issued.


### Minimal Reproduction
1. Set up a long running activity and a worker processing it
2. Send SIGTERM to that worker


### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Linux
- Temporal Version: 1.1.0
- Building the sdk from source, then packaging our own docker container and running on k8s

### Additional context

Based upon my reading of the code, I suspect the graceful_shutdown_timeout option isn't actually doing anything: https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/_worker.py#L436

However, I didn't do a super deep dive on this so could definitely be wrong. 

If there was a way to suspend polling in python at the worker level, that would be great too - and we could implement our own signal handler. worker.suspendpolling appears to be available for go/java temporal sdk.



#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2023-08-11 13:02:16.000 UTC</summary>

> we still see our activity fail once SIGTERM is issued.

Are you calling `shutdown()` on the worker? That's what stops polling and waits graceful time before cancelling activities.

> If there was a way to suspend polling in python at the worker level, that would be great too

This is what `shutdown()` does. It stops polling, notifies all activities of worker shutdown (there are things on the activity module that can be checked), waits the graceful timeout, then forcefully cancels activities.

See https://github.com/temporalio/sdk-python#worker-shutdown.

</details>

<details>
<summary><strong>jayfjin</strong> commented on 2023-08-11 13:40:11.000 UTC</summary>

Interesting, yes that was also tried with a SIGTERM handler, but after reading the docs now I suspect our implementation of that was flawed - thanks.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-08-11 13:52:20.000 UTC</summary>

(feel free to reopen or join us on Slack if still not working for you)

</details>


---

<a id="362"></a>

### #362: [Feature Request] Make sure OTel baggage propagates properly throughout activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/362 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-08-04 22:07:04.000 UTC (2y 5m ago) |
| **Updated** | 2025-10-27 21:32:40.000 UTC |
| **Closed** | 2025-10-27 21:32:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Write a test confirming that baggage is properly serialized/rehydrated to/within activities and if it is not, fix that.

#### Comments (3)

<details>
<summary><strong>recht</strong> commented on 2023-08-04 22:11:28.000 UTC</summary>

I did a quick change to `_TracingActivityInboundInterceptor` to add attach/detach calls, and then it works:
```
        ctx = self.root._context_from_headers(input.headers)
        with self.root.tracer.start_as_current_span(
            f"RunActivity:{info.activity_type}",
            context=ctx,
            attributes={
                "temporalWorkflowID": info.workflow_id,
                "temporalRunID": info.workflow_run_id,
                "temporalActivityID": info.activity_id,
            },
        ):
            token = opentelemetry.context.attach(ctx)
            try:
                return await super().execute_activity(input)
            finally:
                opentelemetry.context.detach(token)

```

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-08-04 22:14:23.000 UTC</summary>

Thanks! I guess it's intentional that https://opentelemetry-python.readthedocs.io/en/latest/api/trace.html#opentelemetry.trace.Tracer.start_as_current_span does not do that.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-08-04 22:18:50.000 UTC</summary>

At first glance, this appears caused by a known OTel Python bug https://github.com/open-telemetry/opentelemetry-python/issues/3350 (as linked from https://github.com/open-telemetry/opentelemetry-python/issues/2432). We will workaround by re-attaching the context (so long as we don't lose the span we created). I will write tests to confirm.

</details>


---

<a id="338"></a>

### #338: [Bug] Wrong types when using "workflow.unsafe.imports_passed_through"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/338 |
| **State** | CLOSED |
| **Author** | masylum (Pau Ramon Revilla) |
| **Created** | 2023-07-06 08:38:09.000 UTC (2y 6m ago) |
| **Updated** | 2023-07-11 17:33:28.000 UTC |
| **Closed** | 2023-07-11 16:46:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

When I need to import activities for a given workflow, the type system fails to recognize the activity type.

### Describe the bug

I'm currently using pyright and it tells me that the `activity` parameter when calling `workflow.execute_activity` should be a string. It's doing so because it does not match any of the overloads. When I bring the activity to the same file (so I bypass the `workflow.unsafe.imports_passed_through`) then the type system is happy

### Minimal Reproduction

I will use a simplified example from the README:

```py
# activities.py
from temporalio import activity

@activity.defn
async def say_hello(name: str) -> str:
    return f"Hello, {name}!"

# workflow.py
from temporalio import workflow

# Import our activity, passing it through the sandbox
with workflow.unsafe.imports_passed_through():
    from .activities import say_hello

@workflow.defn
class SayHello:
    @workflow.run
    async def run(self, name: str) -> str:
        return await workflow.execute_activity(
            say_hello, name
```

This makes pyright complain:

```
Argument of type "Module("say_hello") | ((params: str) -> Coroutine[Any, Any, str])" cannot be assigned to parameter "activity" of type "str" in function "execute_activity"
Type "Module("say_hello") | ((params: str) -> Coroutine[Any, Any, str])" cannot be assigned to type "str"
Type "Module("say_hello")" cannot be assigned to type "str" [reportGeneralTypeIssues]
```

### Environment/Versions

Python 3.11.4

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2023-07-06 13:15:04.000 UTC</summary>

Does this occur with MyPy? I wonder why Pyright is considering `say_hello` a module or a function here instead of just a function? I think this may be a Pyright bug, because surrounding an import with something should not change its types from a static analysis perspective.

</details>

<details>
<summary><strong>masylum</strong> commented on 2023-07-11 16:46:06.000 UTC</summary>

hmm... Honestly, I've treid to setup nvim with mypy and I failed miserably. Feel free to close it if that's just a pyright bug

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-07-11 17:33:27.000 UTC</summary>

I am not sure it is a PyRight bug specifically, but yes, if you cannot replicate except with PyRight it might be a PyRight bug worth submitting to them.

</details>


---

<a id="313"></a>

### #313: [Bug] Retry Policy on start_workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/313 |
| **State** | CLOSED |
| **Author** | mbernier (Matt Bernier) |
| **Created** | 2023-04-25 16:56:37.000 UTC (2y 8m ago) |
| **Updated** | 2023-05-22 19:13:40.000 UTC |
| **Closed** | 2023-05-22 19:13:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I initially was trying to restrict a retry policy to have a maximum interval of 5 seconds with a coefficient of 1x.

### Describe the bug

When retry policy is passed on start_workflow, the behavior of the retry is that it keeps doubling and the max_interval is not respected.

### Minimal Reproduction

```
retryPolicy = RetryPolicy(
            initial_interval=timedelta(minutes=2),
            backoff_coefficient=float(1.0),
            maximum_interval=timedelta(minutes=50),
            maximum_attempts=4
        )

workflow_handle = await temporal_client.start_workflow(
    MyWorkflow.run,
    id="my-workflow-2",
    task_queue="default",
    retry_policy=retryPolicy
)
await workflow_handle.result()
```

### Environment/Versions

Name: temporalio
Version: 1.1.0

Python: 3.10

OS: Mac OSX Ventura 13.2.1

#### Comments (3)

<details>
<summary><strong>Quinn-With-Two-Ns</strong> commented on 2023-04-25 17:01:01.000 UTC</summary>

ah misread the expected vs buggy behavior

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-04-25 19:54:02.000 UTC</summary>

I am not able to replicate this. For example, here's a standalone runnable Python script:

```python
import asyncio
import logging
from datetime import timedelta
from uuid import uuid4

from temporalio import workflow
from temporalio.client import Client
from temporalio.worker import Worker
from temporalio.exceptions import ApplicationError
from temporalio.common import RetryPolicy

@workflow.defn
class AlwaysErrorWorkflow:
    @workflow.run
    async def run(self) -> None:
        workflow.logger.info("Workflow started at: %s" % workflow.now())
        raise ApplicationError("Intentional failure!")


async def main():
    logging.basicConfig(level=logging.INFO)

    client = await Client.connect("localhost:7233")

    logging.info("Starting worker")
    task_queue = f"tq-{uuid4()}"
    async with Worker(
        client,
        task_queue=task_queue,
        workflows=[AlwaysErrorWorkflow],
    ):
        logging.info("Running workflow")
        await client.execute_workflow(
            AlwaysErrorWorkflow.run,
            id=f"wf-{uuid4()}",
            task_queue=task_queue,
            retry_policy=RetryPolicy(
                initial_interval=timedelta(seconds=1),
                backoff_coefficient=1.0,
                maximum_interval=timedelta(seconds=30),
            ),
        )


if __name__ == "__main__":
    asyncio.run(main())
```

This retries every second as expected, it does not double it. And it never reaches max interval because initial interval is just mulitplied by 1 every time. Make sure you are not observing an older workflow still running (i.e. change your task queue and workflow ID each time if needed). Otherwise, can you alter the above sample to demonstrate what you are seeing (preferably w/ seconds instead of minutes just so we don't have to wait)?

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-22 19:13:39.000 UTC</summary>

Closing as unable to replicate and weeks without original opener response

</details>


---

<a id="286"></a>

### #286: [Bug] Worker started state is not reset after shutdown

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/286 |
| **State** | CLOSED |
| **Author** | schibrikov (Eugeny Schibrikov) |
| **Created** | 2023-02-23 10:30:20.000 UTC (2y 10m ago) |
| **Updated** | 2023-02-23 17:19:27.000 UTC |
| **Closed** | 2023-02-23 17:19:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | question |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I want to shutdown worker and then spin it up again later. So I'm calling `worker.shutdown()`, and then after it is off for some time calling `worker.run()` again.

### Describe the bug

`run()` throws `Already started` error, despite worker has shut down already. This happens because `self._started` is set to `True` when starting, but not reset when shutting down.

I believe this is a bug and we should reset `self._started` flag to False, when worker has shut down.
Is it the case or I'm doing something very wrong?

### Extra context

My worker is processing long-running, resource-consuming tasks, so that only 1 can be processed at a time. Though the whole workflow consists of 3 different tasks (file-system dependent) that should run on a single node. So effectively I'm running 3 workers:

1. One is for the workflow itself, no activities
2. Second contains an activity that reports unique worker id and that shut down itself
3. Third processes all 3 long-running activities on the unique task queue and after that, in a separate activity - restarts the second worker.

Is this scheme meaningful or I can get it running simply other way?

#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2023-02-23 12:45:55.000 UTC</summary>

>  Is it the case or I'm doing something very wrong?

Workers are single use by intention to prevent confusion and misuse. You can create another worker with the same config and call `run` on it.

> Is this scheme meaningful or I can get it running simply other way?

I would need to know more about _why_ you need to restart a worker. Most workers run forever and you use workflows to orchestrate or limit activities. If easier this can be discussed in our forum or Slack.

</details>

<details>
<summary><strong>schibrikov</strong> commented on 2023-02-23 16:12:01.000 UTC</summary>

Yeah, right, I had no luck manually setting `worker._started = False` after shut down, so there are other concerns with restarting it. And yes, I got around this issue by simply running a new worker, instead of reusing, so they kind of "one-off" now and it works exactly as I expected, thanks.

I would happily describe my use-case in details later, since it appears common to me and may worth documenting.
In short: I'm running ML workflow (download-inference-upload/cleanup). So I need to stick my workflow to a single node and onyly run a single one on this node, also inference is long-running, so I don't want to assign the next workflow to a node currently computing. And I cannot simply follow sticky sessions tutorial, since my set of nodes is dynamic.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-23 17:19:27.000 UTC</summary>

> I would happily describe my use-case in details later

:+1: Will be on the look out and close this issue as not a bug.

> And I cannot simply follow sticky sessions tutorial, since my set of nodes is dynamic.

The sticky sessions example at https://github.com/temporalio/samples-python/tree/main/activity_sticky_queues is for a dynamic number of nodes too. You don't have to use UUID to identify each host if you need more specific service discovery. You can also use workflows identified by host-specific identifiers to orchestrate activities onto host-specific task queues if needed.

Can discuss in another place if needed, closing as not a bug.



</details>


---

<a id="211"></a>

### #211: [Feature Request] Integration in django application

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/211 |
| **State** | CLOSED |
| **Author** | AdrianPardo99 (Adrian GonzÃ¡lez Pardo) |
| **Created** | 2022-11-23 18:45:47.000 UTC (3y 1m ago) |
| **Updated** | 2022-12-02 16:44:20.000 UTC |
| **Closed** | 2022-12-02 16:44:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

I want to implement a worker in my django app but I don't found how to implemented

### Additional context

I use zappa for async task, but my manager assign to me change all zappa async task to temporal io workflows and activities, so I deploy and test a temporal server on my docker instances, but when i tried to export my envars and my settings there is no much friendly for integrate my worker or my temporal client. 


#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2022-11-23 18:54:39.000 UTC</summary>

There is no one way to integrate Temporal into your existing Python project. Temporal uses asyncio. Integrate it like you would any asyncio software. You can start the worker on app startup in an `async def` with `asyncio.run()`. Or maybe you want your worker to be in a separate process from your web app which is common.

As for using the client in Django app, please refer to Django documentation on how to make async calls.

For general help, feel free to join us on Slack at https://temporal.io/slack or our forums at https://community.temporal.io.

</details>

<details>
<summary><strong>AdrianPardo99</strong> commented on 2022-11-24 21:11:06.000 UTC</summary>

@cretz ok tnx, I'll check if this sdk can integrate in future in another project.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-12-02 16:44:20.000 UTC</summary>

Let us know in forum or Slack if you have other questions regarding this integration

</details>


---

<a id="144"></a>

### #144: Search attribute Datetime need to support full ISO 8601 (and maybe more)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/144 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-27 22:47:14.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-07 13:37:24.000 UTC |
| **Closed** | 2022-11-07 13:37:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

`2022-09-27T22:28:42Z` cannot be parsed by `datetime.fromisoformat` which we are using for search attributes. Make sure the full [RFC3339Nano](https://pkg.go.dev/time#pkg-constants) is supported.

#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2022-09-27 22:51:09.000 UTC</summary>

From https://docs.python.org/3/library/datetime.html#datetime.datetime.fromisoformat:
> Caution
>This does not support parsing arbitrary ISO 8601 strings - it is only intended as the inverse operation of datetime.isoformat(). A more full-featured ISO 8601 parser, dateutil.parser.isoparse is available in the third-party package dateutil.

</details>

<details>
<summary><strong>Oracen</strong> commented on 2022-10-30 07:47:07.000 UTC</summary>

I've been experimenting with the new Scheduler system released in Temporal `1.18.0`. Workers using the Python API break on the same problem described here. This can be replicated with the following code snippet, then setting up a schedule either in the web UI or `tctl`.

```
import asyncio
from datetime import datetime

from dateutil import parser
from temporalio import workflow
from temporalio.client import Client
from temporalio.worker import Worker


@workflow.defn
class Replication:
    @workflow.run
    async def run(self):
        pass


async def main():
    client = await Client.connect("localhost:7233")
    worker = Worker(client, workflows=[Replication], task_queue="replication")

    print("Starting worker")
    await worker.run()


if __name__ == "__main__":
    print("Before:", datetime.fromisoformat("2022-10-30T05:13:00"))
    try:
        print("Won't run:", datetime.fromisoformat("2022-10-30T05:13:00Z"))
    except ValueError:
        print("Exception thrown")
    print("After:", parser.isoparse("2022-10-30T05:13:00Z"))

    asyncio.run(main())
```

This produces the following stack trace:
```
Failed handling activation on workflow with run ID 435abaf1-36bc-49ff-b003-4d6a8d833315
Traceback (most recent call last):
  File "./.venv/lib/site-packages/temporalio/worker/workflow.py", line 174, in _handle_activation
    workflow = await self._create_workflow_instance(act)
  File "./.venv/lib/site-packages/temporalio/worker/workflow.py", line 321, in _create_workflow_instance
    search_attributes=temporalio.converter.decode_search_attributes(
  File "./.venv/lib/site-packages/temporalio/converter.py", line 704, in decode_search_attributes
    val = [datetime.fromisoformat(v) for v in val]
  File "./.venv/lib/site-packages/temporalio/converter.py", line 704, in <listcomp>
    val = [datetime.fromisoformat(v) for v in val]
ValueError: Invalid isoformat string: '2022-10-30T06:28:30Z'
```

I did some work on a fork that resolved down to a one-line fix + some unit tests for regression testing around the `converter.decode_search_attributes` function, linked here: https://github.com/Oracen/sdk-python

I'll open a PR with the fix now, hopefully the problem is as simple as it appears

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-10-31 12:40:50.000 UTC</summary>

:+1: Saw the PR but wondering if we can avoid the additional dependency. Are there any snippets/approaches where we could do this using just the standard library?

(EDIT: Starting to look unreasonable to vendor this or avoid the dependency. I'll update the PR)

</details>


---

<a id="130"></a>

### #130: [Feature Request] client.OutboundInterceptor should support get_workflow_execution_history()

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/130 |
| **State** | CLOSED |
| **Author** | lambyqq (Siyang Xie) |
| **Created** | 2022-09-13 00:13:30.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-01 21:40:21.000 UTC |
| **Closed** | 2022-11-01 21:40:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Client can install custom interceptors that work on most of outbound requests, except `get_workflow_execution_history()`, i.e., no interceptors could run on:
```
wf_handle = client.get_workflow_handle(wf_id)
await wf_handle.result()
```
Note that `wf_handle.describe()` does invoke the interceptors, but not wf_handle.result().

### Describe the solution you'd like

For completeness of the interceptor as plugin, there should be at least a way for OutboundInterceptor to run before get_workflow_execution_history().

### Additional context

I'm building a cross-cutting layer on top of Temporal via the interceptor, which needs to make runtime changes to workflow id and task queue name (e.g., append <hostname-pid> suffix to workflow id for a shared testing namespace)


#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2022-09-13 13:38:06.000 UTC</summary>

Currently none of our SDKs offer an interceptor for `result`. I will speak to the team about intercepting this, though alternatively we may offer a way to intercept getting a handle so that you can subclass it.

For now, you can extend the client and return your own subclass of `WorkflowHandle` in an override of `get_workflow_handle` for your uses of a client.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-09-13 14:52:38.000 UTC</summary>

I have opened https://github.com/temporalio/sdk-features/issues/130 to track this request across all SDKs.

</details>

<details>
<summary><strong>lambyqq</strong> commented on 2022-09-13 15:32:48.000 UTC</summary>

Thanks @cretz !  I'll take the workaround for now while waiting for an official interceptor support

</details>


---

<a id="116"></a>

### #116: [Bug] Search attributes require specific non-documented format

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/116 |
| **State** | CLOSED |
| **Author** | adrianbeloqui (Adrian Beloqui) |
| **Created** | 2022-08-29 21:42:10.000 UTC (3y 4m ago) |
| **Updated** | 2022-08-30 15:00:18.000 UTC |
| **Closed** | 2022-08-30 15:00:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I am writing some example workflows and activities to test the capabilities of this library. As I was doing so, while trying to execute a child flow with the `search_attributes` parameter set and after trying many times and getting an exception every time I decided to try the exact example on https://github.com/temporalio/samples-python/blob/main/hello/hello_search_attributes.py and that worked. So, trying a bit more I realized that it seems like the format of the key of the search attribute must be title cased or it won't work.

### Describe the bug

Setting `search_attributes` with keys that are not title case fails with the below trace stack.

```bash
2022-08-29 21:33:02,745 - ERROR - Failed completing activation on workflow with run ID 79534a74-5e75-4f51-9ff4-a77bdfac8196
Traceback (most recent call last):
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow.py", line 215, in _handle_activation
    await self._bridge_worker().complete_workflow_activation(completion)
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/bridge/worker.py", line 76, in complete_workflow_activation
    await self._ref.complete_workflow_activation(comp.SerializeToString())
RuntimeError: Completion failure: Unhandled grpc error when completing workflow task: Status { code: InvalidArgument, message: "BadSearchAttributes: search attribute customKeywordField is not defined", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
```

### Minimal Reproduction

```python
await workflow.execute_child_workflow(
            "ChildWorkflow",
            {"arg1": 1, "arg2": 2},
            id="child-workflow-1",
            task_timeout=timedelta(seconds=120),
            retry_policy=RetryPolicy(
                backoff_coefficient=2.0,
                maximum_interval=timedelta(seconds=120),
                maximum_attempts=5,
                non_retryable_error_types=["MyCustomNonRetriableException"],
            ),
            search_attributes={"customKeywordField": ["old-value"], "another_format":[""]},
)
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Mac running Colima
- Temporal Version: `temporalio = "^0.1b1"`
- Are you using Docker or Kubernetes or building Temporal from source?  Using Temporal's docker-compose provided environment. Executing workflow from a VSCode Dev Container using Python 3.10.6

### Additional context

Nothing to add at this point


#### Comments (3)

<details>
<summary><strong>adrianbeloqui</strong> commented on 2022-08-29 21:52:12.000 UTC</summary>

This might not be a bug. This might be my own misinterpretation of the search attributes and how they work.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-08-29 22:30:56.000 UTC</summary>

> So, trying a bit more I realized that it seems like the format of the key of the search attribute must be title cased or it won't work.

I think this depends on what the registered search attribute is. We do have default search attributes pre-registered in our docker-compose server like `CustomKeywordField` which appears pascal-cased, but that doesn't mean you can't register your own snake-cased name I believe. You can only set search attributes which have been registered/created on the server. See https://docs.temporal.io/tctl/admin/cluster/add-search-attributes for how to do this with the `tctl` CLI tool. If you run into any trouble with this, feel free to contact us on Slack or the forum.

</details>

<details>
<summary><strong>adrianbeloqui</strong> commented on 2022-08-30 15:00:17.000 UTC</summary>

Hi @cretz, thank you for point me into the right direction, I was able to figure out. I can safely say that this feature to the extent I have tested it works well so I am going to close this issue.

Reactions: ðŸ‘ 1

</details>


---

<a id="92"></a>

### #92: [Feature Request] Lazy connectivity for client and healthCheck

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/92 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2022-08-03 17:07:10.000 UTC (3y 5m ago) |
| **Updated** | 2022-09-07 22:29:18.000 UTC |
| **Closed** | 2022-09-07 22:29:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

*No description provided.*

#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2022-08-03 17:35:44.000 UTC</summary>

Opened https://github.com/temporalio/sdk-core/issues/367 for health check in Core which this will depend on

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-09-06 19:40:06.000 UTC</summary>

This depends on https://github.com/temporalio/sdk-core/issues/385

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-09-06 22:32:48.000 UTC</summary>

There's an open question about how workers are created. In Rust and Python, worker creation is a synchronous task w/ an existing already-connected client. If we make that lazy on the Python side, we have to make worker creation async on the Python side which I think is a bit confusing. Also we can't defer client connection to worker start because we want to eagerly create the worker in Rust so it can do its validation on worker construction. For now I just disallow lazy clients to be used for workers.

IMO we need to support lazy connectivity in core. It's not quite right that Rust worker creation gets to be sync but language-level does not.

</details>


---

<a id="82"></a>

### #82: Replayer

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/82 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-07-21 13:14:47.000 UTC (3y 5m ago) |
| **Updated** | 2022-08-19 18:23:23.000 UTC |
| **Closed** | 2022-08-19 18:23:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | beta-2 |

#### Description

### Describe the solution you'd like

Need workflow replayer (also includes history JSON parser/fixer)

#### Comments (3)

<details>
<summary><strong>cretz</strong> commented on 2022-07-25 21:46:34.000 UTC</summary>

Don't forget to add something to README about disabling deadlock detection.

</details>

<details>
<summary><strong>ccl0326</strong> commented on 2022-08-16 07:52:59.000 UTC</summary>

Hi @cretz , what's the progress for replayer now? We want to use sdk-python in production, but  replayer is a blocking feature for production.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-08-16 11:51:21.000 UTC</summary>

It is under active development in a branch on my fork. Hopefully a PR will be open soon.

</details>


---

<a id="1252"></a>

### #1252: [Feature Request] add client support for sync activities run by `ThreadPoolExecutor`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1252 |
| **State** | CLOSED |
| **Author** | ClemDoum (ClÃ©ment Doumouro) |
| **Created** | 2025-12-15 17:20:43.000 UTC (25 days ago) |
| **Updated** | 2025-12-16 08:51:06.000 UTC |
| **Closed** | 2025-12-16 08:51:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

I'm trying to implement a workflow with activities which publish their own progress:

```python
@activity.defn
async def process_batch(batch: list[int]) -> int:
    info = activity.info()
    run_id = info.workflow_run_id
    workflow_id = info.workflow_id
    client = activity.client()
    activity_id = info.activity_id
    workflow_handle = client.get_workflow_handle(workflow_id, run_id=run_id)
    progress = partial(
        progress_handler,
        handle=workflow_handle,
        run_id=run_id,
        activity_id=activity_id,
    )
    total = 0
    for n, batch_item in enumerate(batch):
        total += batch_item
        activity.logger.info("going to sleep...")
        await asyncio.sleep(1)
        await progress(n / len(batch))
    activity.logger.info("done sleeping !")
    return total

async def progress_handler(
    progress: float,
    handle: WorkflowHandle,
    *,
    activity_id: str,
    run_id: str,
    weight: float = 1.0,
) -> None:
    signal = ProgressSignal(
        activity_id=activity_id, run_id=run_id, progress=progress, weight=weight
    )
    await handle.signal("update_progress", signal)
```
this works great for async activities but for sync / CPU-bound activities I can't access the activity client.

I'm getting: `RuntimeError: No client available. The client is only available in async def activities; not in def activities. In tests you can pass a client when creating ActivityEnvironment.`

### Describe the solution you'd like

This error make sense for sync activities run using a process pool executor as this will generate picking errors, however there's no reason why this should occur when we use a thread pool executor.


#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-12-15 21:29:39.000 UTC</summary>

There is a good reason not to allow reuse of the client from within a sync activity. Because the client was created on a different event loop than the one you would be producing inside your sync activity. This potentially leads to concurrency errors since asyncio synchronization is not guaranteed across multiple threads. In this case you would either need to ensure you provide the _same_ event loop to the activity and execute the client operation on that loop, or create a new client on the new event loop. Simply providing the same client could fail in unexpected ways.

</details>

<details>
<summary><strong>ClemDoum</strong> commented on 2025-12-16 08:51:06.000 UTC</summary>

Very clear thank you for your feedback, I made the change to see if that was breaking any unit test, as I assume the only problem was with the pickling error. Understood, thanks a lot !

</details>


---

<a id="1203"></a>

### #1203: [Bug] `retry_policy` not set in `ativity.info()` despite being defined in the `workflow.execute_activity()` call

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1203 |
| **State** | OPEN |
| **Author** | brent-cybrid (Brent Carrara) |
| **Created** | 2025-10-31 21:11:02.000 UTC (2 months ago) |
| **Updated** | 2025-11-04 13:49:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

We're trying to access the `retry_policy` from the `activity.info` from within an activity.

### Describe the bug

`retry_policy` isn't set even though it's set on the `workflow.execute_activity` call.

### Minimal Reproduction

```python
#!/usr/bin/env python3
"""
Minimal reproducible example demonstrating Temporal SDK bug:
activity.info().retry_policy is None even when workflow specifies retry_policy

This script is standalone and only requires: pip install temporalio
"""
import asyncio
from datetime import timedelta
from temporalio import activity, workflow
from temporalio.common import RetryPolicy
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import Worker


# Simple activity that captures what retry_policy information it receives
@activity.defn
async def check_retry_policy_activity() -> dict:
    """Activity that returns information about the retry_policy it receives."""
    info = activity.info()

    return {
        'has_retry_policy_attr': hasattr(info, 'retry_policy'),
        'rety_policy_is_set': info.retry_policy is not None if hasattr(info, 'retry_policy') else False,
        'non_retryable_error_types': (
            info.retry_policy.non_retryable_error_types
            if (
                hasattr(info, 'retry_policy')
                and hasattr(info.retry_policy, 'non_retryable_error_types')
            )
            else None
        )
    }


# Simple workflow that specifies retry_policy with non_retryable_error_types
@workflow.defn
class RetryPolicyTestWorkflow:
    """Workflow that calls activity with specific retry_policy."""

    @workflow.run
    async def run(self) -> dict:
        # Execute activity with retry_policy that includes non_retryable_error_types
        return await workflow.execute_activity(
            check_retry_policy_activity,
            start_to_close_timeout=timedelta(seconds=10),
            retry_policy=RetryPolicy(
                non_retryable_error_types=["MyNonRetryableError", "AnotherNonRetryableError"]
            ),
        )


async def main():
    """Run the test workflow and verify retry_policy is available in activity."""
    
    async with await WorkflowEnvironment.start_time_skipping() as env:
        # Create worker with our workflow and activity
        async with Worker(
            env.client,
            task_queue="test-queue",
            workflows=[RetryPolicyTestWorkflow],
            activities=[check_retry_policy_activity],
        ):
            # Execute the workflow
            result = await env.client.execute_workflow(
                RetryPolicyTestWorkflow.run,
                id="test-workflow-retry-policy",
                task_queue="test-queue",
            )

            # Verify the bug
            if result['rety_policy_is_set']:
                return True
            else:
                print("BUG PRESENT: retry_policy is not set, even though workflow specifies it")
                return False


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
```

### Environment/Versions

Temporal Cloud

### Additional context




#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-11-03 19:07:52.000 UTC</summary>

Can you validate on a dev or real server as well? There are some things the time skipping server does differently from the standard servers. 

</details>

<details>
<summary><strong>michael-cybrid</strong> commented on 2025-11-04 13:49:24.000 UTC</summary>

@brent-cybrid is a colleague of mine. We see this in Temporal Cloud as well.

Reactions: ðŸ‘ 1

</details>


---

<a id="1196"></a>

### #1196: [Feature Request] Enable Worker Heartbeating

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1196 |
| **State** | CLOSED |
| **Author** | yuandrew (Andrew Yuan) |
| **Created** | 2025-10-28 02:00:03.000 UTC (2 months ago) |
| **Updated** | 2025-11-25 18:17:37.000 UTC |
| **Closed** | 2025-11-25 18:17:37.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Describe the solution you'd like

Worker heartbeating is implemented in Core, need to enable worker heartbeating by allowing users to specify the worker heartbeat interval when initializing Core Runtime

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="1190"></a>

### #1190: OpenTelemetry spans missing workflow input/output attributes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1190 |
| **State** | CLOSED |
| **Author** | maver1ck (Maciek BryÅ„ski) |
| **Created** | 2025-10-27 05:39:42.000 UTC (2 months ago) |
| **Updated** | 2025-12-05 19:24:05.000 UTC |
| **Closed** | 2025-12-05 19:24:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Summary
`temporalio.contrib.opentelemetry.TracingInterceptor` does not put workflow input/output values on the spans it exports. Only the workflow/run IDs show up, so downstream OTEL backends (Langfuse in my case) cannot see what was invoked.

## Reproduction
1. Save the script below as `temporal_minimal_otel_bug.py` (needs a Temporal dev server on `localhost:7233`.
```python
"""Minimal Temporal workflow runner with optional OpenTelemetry tracing."""

from __future__ import annotations

import asyncio
import logging
import os
import uuid

from opentelemetry import trace
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor
from temporalio import workflow
from temporalio.client import Client
from temporalio.worker import Worker
from temporalio.contrib.opentelemetry import TracingInterceptor


log = logging.getLogger(__name__)


def _otel_interceptors() -> list[object]:
    """Return Temporal interceptors that emit OpenTelemetry spans when available."""
    if TracingInterceptor is None:
        log.debug(
            "Skipping OpenTelemetry tracing; install temporalio[opentelemetry] extras to enable."
        )
        return []
    log.info("OpenTelemetry tracing enabled for minimal PoC (console exporter).")
    return [TracingInterceptor()]


def _configure_console_exporter() -> None:
    """Set up a console exporter so spans are visible locally."""
    existing = trace.get_tracer_provider()
    if isinstance(existing, TracerProvider):
        # Already configured (e.g., tests) -> skip
        return
    resource = Resource.create({"service.name": os.getenv("OTEL_SERVICE_NAME", "temporal-minimal")})
    provider = TracerProvider(resource=resource)
    provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))
    trace.set_tracer_provider(provider)


@workflow.defn(sandboxed=False)
class HelloWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        log.info("Running HelloWorkflow for %s", name)
        return f"Hello, {name}!"


async def main() -> None:
    _configure_console_exporter()
    interceptors = _otel_interceptors()
    client = await Client.connect(
        "localhost:7233",
        interceptors=interceptors,
    )

    async with Worker(
        client,
        task_queue="minimal",
        workflows=[HelloWorkflow],
    ):
        result = await client.execute_workflow(
            HelloWorkflow.run,
            "Temporal",
            id=f"hello-{uuid.uuid4()}",
            task_queue="minimal",
        )
        print(result)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())

```

2. Run `uv run python temporal_minimal_otel_bug.py`.
3. Check stdout for Temporal spans. 

## Expected
Workflow spans include attributes for the serialized input and output (even truncated strings would work) so downstream tools can show what the workflow did.

## Actual
Only identifiers are exported; input/output payloads are absent even though the Temporal history contains them.

<img width="1298" height="780" alt="Image" src="https://github.com/user-attachments/assets/231dee49-232a-4cdc-91b6-6d912c183701" />

## Environment
- temporalio==1.18.0
- opentelemetry-sdk==1.38.0
- Python 3.12
- Temporal Server 2.41.0
- OTLP exporter: HTTP/protobuf to Langfuse Cloud

## Notes
Temporal history for the same run shows one input payload and one output payload  The data is available; it just never makes it onto the spans.

<img width="1308" height="675" alt="Image" src="https://github.com/user-attachments/assets/aebee8da-d95d-4ecd-8b71-e287d93f278c" />


#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-28 17:36:08.000 UTC</summary>

We'll take a look at the request. I think we would be unlikely to add that information by default, both because of compatibility guarantees and the potential for users who don't want to expose that information to downstream systems. An option to enrich with that information is potentially viable however.

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-12-05 19:24:04.000 UTC</summary>

We don't think it makes sense to add this behavior as a built in. It will often be surprisingly dangerous to output that much information to the tracing system. If you want to do this, we would currently recommend your own interceptor to add this information, or emitting your own span.

</details>


---

<a id="1168"></a>

### #1168: [Bug] When the task volume is greater than the number of concurrent activities, the subsequent activity execution time does not reach the start_to_close_timeout and is directly canceled

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1168 |
| **State** | CLOSED |
| **Author** | xih1919 |
| **Created** | 2025-10-17 03:46:10.000 UTC (2 months ago) |
| **Updated** | 2025-10-18 03:15:14.000 UTC |
| **Closed** | 2025-10-17 18:36:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

My activity tasks are concurrent with 10. If the number of ExecuteWorkflow declarations is greater than 10 at the same time, for example 20, my first 10 activities will be executed normally, and the next 10 activities will be Cancelled before the start_to_close_timeout is reached.

It looks like self._bridge_worker().poll_activity_task() in temporalio/worker/_activity.py gets the activity task, and after getting it, the start timer starts. Since the thread pool is full, the subsequent tasks will be blocked, but they will continue to time, causing the subsequent tasks to be canceled before reaching the start_to_close_time.


The following is my approximate configurationï¼š
```
    // golang ExecuteWorkflow
	workflowOptions := client.StartWorkflowOptions{
		ID:        req.TaskId,
		TaskQueue: Queue, // é˜Ÿåˆ—å
	}
	Task := models.Task{
		TaskID:      req.TaskId,
	}

	_, err := TemporalClient.ExecuteWorkflow(
		context.Background(), workflowOptions, task.Workflow, Task)
	
	
	// python Worker and execute_activity
	with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        worker = Worker(
            temporal_client.client,
            task_queue=TASK_QUEUE,
            workflows=[TaskClass],
            activity_executor=executor,
            activities=[task_activity],
            workflow_runner=custom_runner,
        )
        await worker.run()
		
		
	@workflow.defn(name="workflow-test")
    class TaskClass:
		@workflow.run
		async def run(self, task: Task):
			task_retry_policy = RetryPolicy(
				initial_interval=timedelta(
					seconds=TEMPORAL_CONFIG.task_initial_interval // 15s
				),
				maximum_attempts=TEMPORAL_CONFIG.task_maximum_attempts, // 2
			)
			await workflow.execute_activity(
				activity=task_activity,
				arg=task,
				start_to_close_timeout=timedelta(
					seconds=TEMPORAL_CONFIG.task_execute_time # 120s 
				),
				retry_policy=task_retry_policy,
			)
```


```
2025-10-09 12:41:14,340.340 12112 INFO ../task_activity.py:19:task_activity() start task_activity: 9135dc0b-fb8d-42a2-a81b-9007361170aa
2025-10-09 12:41:58,395.395 12112 ERROR ../my_task_activity.py:61:handle_cancelled_error() task 9135dc0b-fb8d-42a2-a81b-9007361170aa execute exceeds 120.0 s
[trace-0a46e1632c9f41ef]Traceback (most recent call last):
[trace-0a46e1632c9f41ef][trace-0a46e1632c9f41ef]  File "../task_activity.py", line 20, in task_activity
[trace-0a46e1632c9f41ef]    task.strategy.execute_task()
[trace-0a46e1632c9f41ef][trace-0a46e1632c9f41ef]  File "../strategy.py", line 46, in execute_task
[trace-0a46e1632c9f41ef]    self.execute(task_id=self.task_id)
[trace-0a46e1632c9f41ef][trace-0a46e1632c9f41ef]  File "../strategy.py", line 145, in execute
[trace-0a46e1632c9f41ef]    self.flow_result = alarm_run(task_id)
[trace-0a46e1632c9f41ef][trace-0a46e1632c9f41ef]  File "../flow.py", line 396, in alarm_run
[trace-0a46e1632c9f41ef]    time.sleep(1)
[trace-0a46e1632c9f41ef][trace-0a46e1632c9f41ef]temporalio.exceptions.CancelledError: Cancelled



{
  "events": [
    {
      "eventId": "1",
      "eventTime": "2025-10-09T12:39:50.253818124Z",
      "eventType": "EVENT_TYPE_WORKFLOW_EXECUTION_STARTED",
      "taskId": "2928672812",
      "workflowExecutionStartedEventAttributes": {
        "workflowType": {
          "name": "workflow-test"
        },
        "taskQueue": {
          "name": "queue-test",
          "kind": "TASK_QUEUE_KIND_NORMAL"
        },
        "input": {
          "payloads": [
            {
              "metadata": {
                "encoding": "anNvbi9wbGFpbg=="
              },
              "data": {
                "task_id": "9135dc0b-fb8d-42a2-a81b-9007361170aa",
                "trigger_type": "alarm"
              }
            }
          ]
        },
        "workflowExecutionTimeout": "0s",
        "workflowRunTimeout": "0s",
        "workflowTaskTimeout": "10s",
        "originalExecutionRunId": "0c8517cd-d3f7-4b02-a607-2d613dd8269b",
        "identity": "122992@api-kt@",
        "firstExecutionRunId": "0c8517cd-d3f7-4b02-a607-2d613dd8269b",
        "attempt": 1,
        "firstWorkflowTaskBackoff": "0s",
        "header": {},
        "workflowId": "9135dc0b-fb8d-42a2-a81b-9007361170aa"
      }
    },
    {
      "eventId": "2",
      "eventTime": "2025-10-09T12:39:50.253854783Z",
      "eventType": "EVENT_TYPE_WORKFLOW_TASK_SCHEDULED",
      "taskId": "2928672813",
      "workflowTaskScheduledEventAttributes": {
        "taskQueue": {
          "name": "queue-test",
          "kind": "TASK_QUEUE_KIND_NORMAL"
        },
        "startToCloseTimeout": "10s",
        "attempt": 1
      }
    },
    {
      "eventId": "3",
      "eventTime": "2025-10-09T12:39:52.742335867Z",
      "eventType": "EVENT_TYPE_WORKFLOW_TASK_STARTED",
      "taskId": "2928672854",
      "workflowTaskStartedEventAttributes": {
        "scheduledEventId": "2",
        "identity": "12112@tmp-kt",
        "requestId": "d153ab16-84a5-4a27-9636-b9bb7db5ebe4",
        "historySizeBytes": "403"
      }
    },
    {
      "eventId": "4",
      "eventTime": "2025-10-09T12:39:52.777696270Z",
      "eventType": "EVENT_TYPE_WORKFLOW_TASK_COMPLETED",
      "taskId": "2928672860",
      "workflowTaskCompletedEventAttributes": {
        "scheduledEventId": "2",
        "startedEventId": "3",
        "identity": "12112@tmp-kt",
        "workerVersion": {
          "buildId": "14de874560c3a2202fb30bb2b7483138"
        },
        "sdkMetadata": {
          "coreUsedFlags": [
            1,
            3,
            2
          ],
          "sdkName": "temporal-python",
          "sdkVersion": "1.14.1"
        },
        "meteringMetadata": {}
      }
    },    
    {
      "eventId": "5",
      "eventTime": "2025-10-09T12:39:52.777784956Z",
      "eventType": "EVENT_TYPE_ACTIVITY_TASK_SCHEDULED",
      "taskId": "2928672861",
      "activityTaskScheduledEventAttributes": {
        "activityId": "1",
        "activityType": {
          "name": "task_activity"
        },
        "taskQueue": {
          "name": "queue-test",
          "kind": "TASK_QUEUE_KIND_NORMAL"
        },
        "header": {},
        "input": {
          "payloads": [
            {
              "metadata": {
                "encoding": "anNvbi9wbGFpbg=="
              },
              "data": {
                "task_id": "9135dc0b-fb8d-42a2-a81b-9007361170aa",
                "trigger_type": "alarm"
              }
            }
          ]
        },
        "scheduleToCloseTimeout": "0s",
        "scheduleToStartTimeout": "0s",
        "startToCloseTimeout": "120s",
        "heartbeatTimeout": "0s",
        "workflowTaskCompletedEventId": "4",
        "retryPolicy": {
          "initialInterval": "15s",
          "backoffCoefficient": 2,
          "maximumInterval": "1500s",
          "maximumAttempts": 2
        },
        "useWorkflowBuildId": true
      }
    },
    {
      "eventId": "6",
      "eventTime": "2025-10-09T12:42:07.837531112Z",
      "eventType": "EVENT_TYPE_ACTIVITY_TASK_STARTED",
      "taskId": "2928672919",
      "activityTaskStartedEventAttributes": {
        "scheduledEventId": "5",
        "identity": "12112@tmp-kt",
        "requestId": "685edd23-7c8f-4d8c-9c22-a3d9af4a086e",
        "attempt": 2,
        "lastFailure": {
          "message": "activity StartToClose timeout",
          "source": "Server",
          "timeoutFailureInfo": {
            "timeoutType": "TIMEOUT_TYPE_START_TO_CLOSE"
          }
        }
      }
    },
    {
      "eventId": "7",
      "eventTime": "2025-10-09T12:42:07.917188550Z",
      "eventType": "EVENT_TYPE_ACTIVITY_TASK_COMPLETED",
      "taskId": "2928672920",
      "activityTaskCompletedEventAttributes": {
        "result": {
          "payloads": [
            {
              "metadata": {
                "encoding": "YmluYXJ5L251bGw="
              },
              "data": null
            }
          ]
        },
        "scheduledEventId": "5",
        "startedEventId": "6",
        "identity": "12112@tmp-kt"
      }
    },
    {
      "eventId": "8",
      "eventTime": "2025-10-09T12:42:07.917199061Z",
      "eventType": "EVENT_TYPE_WORKFLOW_TASK_SCHEDULED",
      "taskId": "2928672921",
      "workflowTaskScheduledEventAttributes": {
        "taskQueue": {
          "name": "12112@tmp-kt-12e959e874744652ae08109ffb2fb9bc",
          "kind": "TASK_QUEUE_KIND_STICKY",
          "normalName": "queue-test"
        },
        "startToCloseTimeout": "10s",
        "attempt": 1
      }
    },
        {
      "eventId": "9",
      "eventTime": "2025-10-09T12:42:08.039527045Z",
      "eventType": "EVENT_TYPE_WORKFLOW_TASK_STARTED",
      "taskId": "2928672929",
      "workflowTaskStartedEventAttributes": {
        "scheduledEventId": "8",
        "identity": "12112@tmp-kt",
        "requestId": "6f02fe0a-cc3a-4208-bb93-0a11eb104e35",
        "historySizeBytes": "1196"
      }
    },
    {
      "eventId": "10",
      "eventTime": "2025-10-09T12:42:08.120574717Z",
      "eventType": "EVENT_TYPE_WORKFLOW_TASK_COMPLETED",
      "taskId": "2928672939",
      "workflowTaskCompletedEventAttributes": {
        "scheduledEventId": "8",
        "startedEventId": "9",
        "identity": "12112@tmp-kt",
        "workerVersion": {
          "buildId": "14de874560c3a2202fb30bb2b7483138"
        },
        "sdkMetadata": {},
        "meteringMetadata": {}
      }
    },
    {
      "eventId": "11",
      "eventTime": "2025-10-09T12:42:08.120635997Z",
      "eventType": "EVENT_TYPE_WORKFLOW_EXECUTION_COMPLETED",
      "taskId": "2928672940",
      "workflowExecutionCompletedEventAttributes": {
        "result": {
          "payloads": [
            {
              "metadata": {
                "encoding": "YmluYXJ5L251bGw="
              },
              "data": null
            }
          ]
        },
        "workflowTaskCompletedEventId": "10"
      }
    }
  ]
}

```

#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-17 18:36:07.000 UTC</summary>

This is an intended behavior. `start_to_close_timeout` refers to the time allowed after a worker has accepted a task. This doesn't necessarily guarantee that the user code has reached an particular point of execution, which would not really be achievable in the general case. 

In order to restrict the number of activities a worker can process at once, you should use Temporal provided tuning mechanisms, many of which are detailed in https://docs.temporal.io/develop/worker-performance. In Python, you can use the `max_concurrent_activities` argument on a worker. For more complex situations, you can look at slot suppliers. A fixed size slot supplier for activities would likely be what you are trying to accomplish.

</details>

<details>
<summary><strong>xih1919</strong> commented on 2025-10-18 03:15:14.000 UTC</summary>

> This is an intended behavior. `start_to_close_timeout` refers to the time allowed after a worker has accepted a task. This doesn't necessarily guarantee that the user code has reached an particular point of execution, which would not really be achievable in the general case.è¿™æ˜¯é¢„æœŸçš„è¡Œä¸ºã€‚`start_to_close_timeout` æ˜¯æŒ‡å·¥ä½œäººå‘˜æŽ¥å—ä»»åŠ¡åŽå…è®¸çš„æ—¶é—´ã€‚è¿™å¹¶ä¸ä¸€å®šä¿è¯ç”¨æˆ·ä»£ç å·²è¾¾åˆ°ç‰¹å®šçš„æ‰§è¡Œç‚¹ï¼Œè¿™åœ¨ä¸€èˆ¬æƒ…å†µä¸‹æ˜¯æ— æ³•å®žçŽ°çš„ã€‚
> 
> In order to restrict the number of activities a worker can process at once, you should use Temporal provided tuning mechanisms, many of which are detailed in https://docs.temporal.io/develop/worker-performance. In Python, you can use the `max_concurrent_activities` argument on a worker. For more complex situations, you can look at slot suppliers. A fixed size slot supplier for activities would likely be what you are trying to accomplish.ä¸ºäº†é™åˆ¶å·¥ä½œç¨‹åºä¸€æ¬¡å¯ä»¥å¤„ç†çš„æ´»åŠ¨æ•°é‡ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨ Temporal æä¾›çš„è°ƒä¼˜æœºåˆ¶ï¼Œå…¶ä¸­è®¸å¤šæœºåˆ¶åœ¨ https://docs.temporal.io/develop/worker-performance ä¸­è¿›è¡Œäº†è¯¦ç»†è¯´æ˜Žã€‚åœ¨ Python ä¸­ï¼Œæ‚¨å¯ä»¥åœ¨å·¥ä½œç¨‹åºä¸Šä½¿ç”¨ `max_concurrent_activities` å‚æ•°ã€‚å¯¹äºŽæ›´å¤æ‚çš„æƒ…å†µï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹æ’æ§½ä¾›åº”å•†ã€‚ç”¨äºŽæ´»åŠ¨çš„å›ºå®šå°ºå¯¸æ’æ§½ä¾›åº”å•†å¯èƒ½æ˜¯æ‚¨æƒ³è¦å®žçŽ°çš„ç›®æ ‡ã€‚


This is very useful for me, thanks

</details>


---

<a id="1166"></a>

### #1166: [Feature Request] Suggest improving type hint for activity in execute_activity to work with acitvity methods

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1166 |
| **State** | CLOSED |
| **Author** | spacether (Justin Black) |
| **Created** | 2025-10-16 08:16:52.000 UTC (2 months ago) |
| **Updated** | 2025-10-22 13:27:01.000 UTC |
| **Closed** | 2025-10-21 19:57:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Python allows a class to have methods decorated as activities
Those methods may accept only self or self and one arg. They are callable.
The type hint for execute_activity does not work for that though because it expects no args or one arg.

### Describe the solution you'd like

Please update the type hint to work for this use case OR provide a new decorator activity_method which will conform to this type hint. Your provided samples show that this is an acceptable use case:
- https://github.com/temporalio/samples-python/blob/main/hello/hello_activity_method.py#L19

### Additional context

Your provided samples show that this is an acceptable use case:
- https://github.com/temporalio/samples-python/blob/main/hello/hello_activity_method.py#L19

#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-16 16:01:17.000 UTC</summary>

Can you elaborate on the problem you face a little bit? Is the request just that you want to be able to use `execute_activity` instead of `execute_activity_method` which is provided for this purpose?

</details>

<details>
<summary><strong>spacether</strong> commented on 2025-10-22 13:27:01.000 UTC</summary>

Ah I missed that we gave execute_activity_method

</details>


---

<a id="1146"></a>

### #1146: [Feature Request] Allow providing additional data to MCP server construction

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1146 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-10-08 17:04:09.000 UTC (3 months ago) |
| **Updated** | 2025-10-14 19:53:50.000 UTC |
| **Closed** | 2025-10-14 19:53:50.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

Currently, a workflow cannot provide context specific information to the construction of an MCP server, it had to have been defined at the worker level through a factory callable with no arguments. This doesn't allow the user to create the server in different ways depending on the workflow.

### Describe the solution you'd like

Ability to provide some additional data to the mcp server reference which is then provided to the factory callable.

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="1129"></a>

### #1129: [Bug] Child workflow search attributes aren't decrypted in 1.18.0

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1129 |
| **State** | CLOSED |
| **Author** | slingshotsys (Geoff) |
| **Created** | 2025-09-29 20:22:16.000 UTC (3 months ago) |
| **Updated** | 2025-10-06 18:33:30.000 UTC |
| **Closed** | 2025-09-30 01:09:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

As of `1.18.0`, launching a child workflow with search_attributes fails when using a custom payload_codec.

On `1.17.0` this completes succesfully, but on `1.18.0` we get the following error:

<img width="1129" height="235" alt="Image" src="https://github.com/user-attachments/assets/f4f593a1-f649-4619-b3d9-5ce99df0f926" />

```
2025-09-29T20:18:32.022484Z  WARN temporal_sdk_core::worker::workflow: Error while completing workflow activation error=status: InvalidArgument, message: "invalid SearchAttributes on StartChildWorkflowCommand: invalid value for search attribute show_name of type Keyword: value from <metadata:{key:\"encoding\"  value:\"binary/encrypted\"}  metadata:{key:\"encryption-key-id\"  value:\"test-key-id\"}  data:\"\\xf9H\\x99+\\xb6\\xee\\xffo\\xd4}\\xde\\xe9\\x7fg\\xb4\\x89\\xa5\\xb4\\x10\\x0e\\xefT\\xd2a\\xf2\\xb9\\t\\xb2t\\xbd\\xa0\\xae\\x8d<\\xf7\\xd2?\\xe4\\xca6X\\xfb\\x83\\xf2\\xb0\\x1aÕ”\\x0e\\xbbZ\\xe4\\x94ZU\\x81d\\xf3e\\xc4Zx\\x98\\xf0h\\xc2\\x10Ó\\x84\\xfdm\\x9b\\x15SÂ¡8\\x8fI\\xf2R\">. WorkflowId=child-Temporal WorkflowType=ChildWorkflow Namespace=default", details: [], metadata: MetadataMap { headers: {"content-type": "application/grpc"} }
```

### Minimal Reproduction
Based off the sample https://github.com/temporalio/samples-python/tree/main/encryption

```python
import asyncio
import dataclasses

import temporalio.converter
from temporalio import workflow
from temporalio.client import Client
from temporalio.worker import Worker

import os
from typing import Iterable, List

from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from temporalio.api.common.v1 import Payload
from temporalio.converter import PayloadCodec
from temporalio.common import SearchAttributeKey, SearchAttributePair, TypedSearchAttributes

default_key = b"test-key-test-key-test-key-test!"
default_key_id = "test-key-id"


class EncryptionCodec(PayloadCodec):
    def __init__(self, key_id: str = default_key_id, key: bytes = default_key) -> None:
        super().__init__()
        self.key_id = key_id
        # We are using direct AESGCM to be compatible with samples from
        # TypeScript and Go. Pure Python samples may prefer the higher-level,
        # safer APIs.
        self.encryptor = AESGCM(key)

    async def encode(self, payloads: Iterable[Payload]) -> List[Payload]:
        # We blindly encode all payloads with the key and set the metadata
        # saying which key we used
        return [
            Payload(
                metadata={
                    "encoding": b"binary/encrypted",
                    "encryption-key-id": self.key_id.encode(),
                },
                data=self.encrypt(p.SerializeToString()),
            )
            for p in payloads
        ]

    async def decode(self, payloads: Iterable[Payload]) -> List[Payload]:
        ret: List[Payload] = []
        for p in payloads:
            # Ignore ones w/out our expected encoding
            if p.metadata.get("encoding", b"").decode() != "binary/encrypted":
                ret.append(p)
                continue
            # Confirm our key ID is the same
            key_id = p.metadata.get("encryption-key-id", b"").decode()
            if key_id != self.key_id:
                raise ValueError(f"Unrecognized key ID {key_id}. Current key ID is {self.key_id}.")
            # Decrypt and append
            ret.append(Payload.FromString(self.decrypt(p.data)))
        return ret

    def encrypt(self, data: bytes) -> bytes:
        nonce = os.urandom(12)
        return nonce + self.encryptor.encrypt(nonce, data, None)

    def decrypt(self, data: bytes) -> bytes:
        return self.encryptor.decrypt(data[:12], data[12:], None)


@workflow.defn(name="Workflow")
class GreetingWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        print(
            await workflow.execute_child_workflow(
                workflow=ChildWorkflow.run,
                arg=name,
                id=f"child-{name}",
                search_attributes=workflow.info().typed_search_attributes,
            )
        )
        return f"Hello, {name}"


@workflow.defn(name="ChildWorkflow")
class ChildWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        return f"Hello from child, {name}"


async def main():
    # Connect client
    client = await Client.connect(
        "localhost:7233",
        # Use the default converter, but change the codec
        data_converter=dataclasses.replace(temporalio.converter.default(), payload_codec=EncryptionCodec()),
    )

    # Run a worker for the workflow
    async with Worker(
        client,
        task_queue="encryption-task-queue",
        workflows=[GreetingWorkflow, ChildWorkflow],
    ):
        # Run workflow
        result = await client.execute_workflow(
            GreetingWorkflow.run,
            "Temporal",
            id=f"encryption-workflow-id",
            task_queue="encryption-task-queue",
            search_attributes=TypedSearchAttributes(
                [SearchAttributePair(SearchAttributeKey.for_keyword("show_name"), "test_show")]
            ),
        )
    print(f"Workflow result: {result}")


if __name__ == "__main__":
    asyncio.run(main())
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Windows 11
- Temporal Version: Python SDK 1.18.0
- Occurs on both the local temporal CLI and temporal cloud.



#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-29 21:19:19.000 UTC</summary>

Definitely a regression. Thanks for the report! I will be releasing a fix as `1.18.1` today or tomorrow.

Reactions: ðŸŽ‰ 1

</details>

<details>
<summary><strong>slingshotsys</strong> commented on 2025-10-06 18:33:30.000 UTC</summary>

Confirmed working in `1.18.1`, thanks for fixing so quickly!

</details>


---

<a id="1108"></a>

### #1108: [Feature Request] Update dependency constraint openai-agents to  v0.3.0

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1108 |
| **State** | CLOSED |
| **Author** | gstrauss-zscaler (Guy Strauss) |
| **Created** | 2025-09-16 14:08:08.000 UTC (3 months ago) |
| **Updated** | 2025-09-16 14:23:01.000 UTC |
| **Closed** | 2025-09-16 14:23:01.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Openai new version fixes various bugs and add new improvements to the hooks mechanism that we would like to incorporate 

### Describe the solution you'd like

Update the library

### Additional context

#### Current failures
```
WARN temporal_sdk_core::worker::workflow: Failing workflow task run_id=019952cc-2ab8-7f07-82d1-c0b6abb01f37 failure=Failure { failure: Some(Failure { message: "_TemporalModelStub.get_response() got an unexpected keyword argument 'conversation_id'", source: ""
```

#### Comments (2)

<details>
<summary><strong>gstrauss-zscaler</strong> commented on 2025-09-16 14:08:38.000 UTC</summary>

I would like to help solve this if it's a good ticket for first time contributer

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-16 14:22:50.000 UTC</summary>

It wouldn't be a bad one, but it is already fixed, we just haven't released a new version yet. You can build the dependency from `main` if you would like in the meantime, and a release should go soon, either this week or next.

</details>


---

<a id="1106"></a>

### #1106: [Bug] TLS error from `poll_workflow_task_queue`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1106 |
| **State** | CLOSED |
| **Author** | spanktar (Tom Kapanka) |
| **Created** | 2025-09-15 17:44:34.000 UTC (3 months ago) |
| **Updated** | 2025-09-17 15:42:05.000 UTC |
| **Closed** | 2025-09-17 15:42:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm installing Temporal with the Helm chart. Our Python workers start fine and connect to the Temporal server, but I'm getting a TLS error from `poll_workflow_task_queue`. I'm trying to determine what this code is trying to resolve in order to try to eliminate this error.

### Describe the bug

Despite having TLS configured properly (I hope! â€” the clients connect fine), I'm getting a TLS error from the `poll_workflow_task_queue`. I have the long form of the K8S service name `temporal-frontend.main.svc.cluster.local` and the short name `temporal-frontend.main.svc` in the certificate SAN, and I am setting `TEMPORAL_HOSTNAME=temporal-frontend.main.svc`.  Despite this, I'm getting the error message below about the TLS cert not being valid for IP `10.244.3.67`. This is obviously the case because we're not gonna put ephemeral IPs into the SANs. This IP resolves to the Matching pod `temporal-matching-6c8ff5f74c-gcwgc`. So I'm trying to understand what the `poll_workflow_task_queue` is trying to resolve, and getting that IP address back, or how it's coming up with that IP address. It seems to me that it should be talking to `temporal-frontend.main.svc`, which would have the correct SAN, but it isn't?

### Environment/Versions

- OS and processor: Ubuntu host
- Temporal Version: Python SDK
- Are you using Docker or Kubernetes or building Temporal from source? Kubernetes Helm chart v0.65.0

### Additional context

Error message:

```
2025-09-15T17:17:43.248391Z  WARN temporal_client::retry: gRPC call poll_workflow_task_queue retried 8 times error=Status { code: Unavailable, message: "connection error: desc = \"transport: authentication handshake failed: tls: failed to verify certificate: x509: certificate is valid for 127.0.0.1, ::1, not 10.244.3.67\"", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
```

#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-16 15:59:57.000 UTC</summary>

`TEMPORAL_HOSTNAME` is not a thing as far as we are aware. If that is being used by your code in some way, it doesn't give us full detail. I would recommend inquiring at the community slack https://t.mp/slack where customer success folks can help if you provide more detail on the scenario.

</details>

<details>
<summary><strong>spanktar</strong> commented on 2025-09-17 15:42:05.000 UTC</summary>

https://temporalio.slack.com/archives/CTRCR8RBP/p1758123380477309?thread_ts=1758121148.745729&cid=CTRCR8RBP

Summary:
I was missing `server_name` under `server.config.tls.internode.client` in my Helm config.
https://docs.temporal.io/references/configuration#tls


</details>


---

<a id="1098"></a>

### #1098: [Bug] Error running hello world workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1098 |
| **State** | CLOSED |
| **Author** | cecilpang |
| **Created** | 2025-09-12 13:13:09.000 UTC (3 months ago) |
| **Updated** | 2025-09-12 14:25:29.000 UTC |
| **Closed** | 2025-09-12 14:25:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?


Run the hello world workflow in Python


### Describe the bug

<!-- A clear and concise description of what the bug is. -->

Getting this error in run_worker.py:

TypeError: _TemporalModelStub.get_response() got an unexpected keyword argument 'conversation_id'
Failed activation on workflow HelloWorldAgent with ID my-workflow-id and run ID 01993e0a-60d4-729b-986c-40c623c0c9e7

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: M4 Pro Mac
- Temporal Version: 1.17.0
- OpenAI Agents SDK Version: 0.3.0
- Are you using Docker or Kubernetes or building Temporal from source? No

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-12 14:13:45.000 UTC</summary>

Breaking changes were made in `0.2.11` and subsequently exist in `0.3.0`. It has been fixed in main, but an upper bound of `<0.3.0` has been added, as that version didn't exist at the time. For now you have to pin openai agents to `0.2.9` or lower. 

</details>

<details>
<summary><strong>cecilpang</strong> commented on 2025-09-12 14:25:29.000 UTC</summary>

Pinning openai agents to 0.2.9 works. Thank you for the quick response.

</details>


---

<a id="1091"></a>

### #1091: [Bug] `_TemporalModelStub.get_response` incompatible with latest `openai-agents-python` (`conversation_id` kwarg)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1091 |
| **State** | CLOSED |
| **Author** | nikhil-pandey (Nikhil Pandey) |
| **Created** | 2025-09-09 22:37:50.000 UTC (4 months ago) |
| **Updated** | 2025-09-10 17:44:14.000 UTC |
| **Closed** | 2025-09-10 17:44:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Use the Temporal Python SDKâ€™s openai agents integration with the latest `openai-agents` python library to run agents.

### Describe the bug

When running a workflow `_TemporalModelStub.get_response`, raises a `TypeError` because the method signature does not accept the new `conversation_id` argument recently added upstream in `openai-agents-python`.

Error trace:

```
TypeError: _TemporalModelStub.get_response() got an unexpected keyword argument 'conversation_id'

/.venv/lib/python3.13/site-packages/agents/run.py", line 1398, in _get_new_response
    new_response = await model.get_response(
        system_instructions=filtered.instructions,
        prompt=prompt_config,
        conversation_id=conversation_id,
    )
    ^
```

Upstream change that introduced this parameter:

* [https://github.com/openai/openai-agents-python/commit/164acb59d9512c1dc0db739b88bda1b75264ecfe](https://github.com/openai/openai-agents-python/commit/164acb59d9512c1dc0db739b88bda1b75264ecfe)
* PR: [[Fix #1564 Add conversations API support (#1587)](https://github.com/openai/openai-agents-python/pull/1587)](https://github.com/openai/openai-agents-python/pull/1587)

### Minimal Reproduction

1. Install the latest `openai-agents` python (post conversations API support).
2. Run a sample agents workflow.
3. Observe `TypeError`.

Relevant code in SDK:
[`[_temporal_model_stub.py#L60](https://github.com/temporalio/sdk-python/blob/44f0c42fa8b0d48c2d13bb9b238023eb983dc07f/temporalio/contrib/openai_agents/_temporal_model_stub.py#L60)`](https://github.com/temporalio/sdk-python/blob/44f0c42fa8b0d48c2d13bb9b238023eb983dc07f/temporalio/contrib/openai_agents/_temporal_model_stub.py#L60)

### Environment/Versions

* **OS and processor:** e.g. macOS 14.5 (M1)
* **Python version:** 3.13
* **Temporal SDK version:** 1.4.1
* **OpenAI Agents Python version: >0.2.10. (conversation_id was added in 0.2.10)

### Additional context

This bug was introduced due to drift between the Temporal stub and the upstream `openai-agents` python model interface.

Possible fixes:

* Update `_TemporalModelStub.get_response` to accept `conversation_id: Optional[str] = None`.
* Consider adding `**kwargs` passthrough to reduce future breakage.

#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-09 23:17:37.000 UTC</summary>

Internally we had to pin to versions before 0.2.10 because of https://github.com/openai/openai-agents-python/issues/1640, but that pin hasn't gone in release version, and that is now resolved. It may be worthwhile to pin our integration to a known compatible version of the agents SDK, though that has other costs. I'll update main for these changes, so the next release will work with the new agents version. In the meantime, you can use an agents version from before `0.2.10`.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-09 23:27:29.000 UTC</summary>

We can't convert to `**kwargs` in the primary case, because we need to ferry those arguments through serialization. Blindly allowing additional parameters without confirming they are serializable would cause a different kind of instability which would be more difficult to diagnose.

</details>


---

<a id="1089"></a>

### #1089: [Feature Request] Provide supported Session implementations for OpenAI Agents

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1089 |
| **State** | OPEN |
| **Author** | tconley1428 |
| **Created** | 2025-09-09 19:56:51.000 UTC (4 months ago) |
| **Updated** | 2025-09-09 19:56:51.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

We don't provide any functional `Session` implementations, as we don't support the builtin Sqlite one. 

### Describe the solution you'd like

Provide at least one in the box `Session` which works in a workflow context.

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="1024"></a>

### #1024: [Feature Request] Display Agent name in the timeline

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1024 |
| **State** | CLOSED |
| **Author** | dimasespi |
| **Created** | 2025-08-08 12:12:25.000 UTC (5 months ago) |
| **Updated** | 2025-08-20 19:29:37.000 UTC |
| **Closed** | 2025-08-20 19:29:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I would like to only display the agent name in the timeline.

Currently, it is displaying the Activity Type + Summary:

<img width="719" height="182" alt="Image" src="https://github.com/user-attachments/assets/1f613a35-8c56-47a9-b818-d127f2fa7a5a" />


Maybe by updating the runner here:
https://github.com/temporalio/sdk-python/blob/main/temporalio/contrib/openai_agents/_openai_runner.py#L87-L90

```
        updated_run_config = replace(
            run_config,
            model=_TemporalModelStub(
                model_name=model_name,
                model_params=self.model_params,
                agent_name=starting_agent.name,
            ),
        )
```
And then add the agent_name in the the class of _TemporalModelStub. 
 https://github.com/temporalio/sdk-python/blob/main/temporalio/contrib/openai_agents/_temporal_model_stub.py#L137-L150

#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-08-12 16:01:31.000 UTC</summary>

Unfortunately I realized this won't be reliably possible without changes to the agents SDK. The `Agent` which is present in the runner is only the starting agent, while the model is used for all agent invocations across the run. This could include handoffs which would change the agent name without informing the model.

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-08-15 16:00:02.000 UTC</summary>

In further investigation, I discovered that this isn't the only issue with the scenario above. The model names themselves are not specified at the agent level, so it is not currently possible to use different models in different portions of the same agentic loop. I'll be fixing that, which will also enable this scenario.

</details>


---

<a id="1012"></a>

### #1012: [Bug]  The second time the process exits, the activity will not be re-executed immediately

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1012 |
| **State** | CLOSED |
| **Author** | xih1919 |
| **Created** | 2025-07-30 16:16:02.000 UTC (5 months ago) |
| **Updated** | 2025-07-30 16:46:50.000 UTC |
| **Closed** | 2025-07-30 16:39:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

`
        retry_policy_options = RetryPolicy(
            maximum_attempts=5,
        )
	await workflow.execute_activity(
		test,
		data,
		start_to_close_timeout=timedelta(
			seconds= 30 * 60,
		),
		retry_policy=retry_policy_options,
	)
`


During the test function, the first time the process exits, you can see that another virtual machine restarts the activity, but the process exits quickly after the restart. At this time, no worker quickly restarts the activity, and it takes 30 * 60 seconds to restart the activity. 

My expectation is: to be able to retry the activity quickly, rather than waiting 30 * 60 seconds

#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-07-30 16:39:55.000 UTC</summary>

We don't currently support automatic detection of the worker's process exit. The recommended solution is to add heartbeating to your activity. https://docs.temporal.io/encyclopedia/detecting-activity-failures#activity-heartbeat. If the heartbeat doesn't arrive from your activity, we will know to retry. 

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-07-30 16:46:50.000 UTC</summary>

Feature tracking a request for auto heartbeating: https://github.com/temporalio/features/issues/229

</details>


---

<a id="980"></a>

### #980: [Bug] Inaccessible type in NamespaceSpec

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/980 |
| **State** | CLOSED |
| **Author** | kislyuk-openai (Andrey Kislyuk) |
| **Created** | 2025-07-19 14:53:43.000 UTC (5 months ago) |
| **Updated** | 2025-07-22 17:25:36.000 UTC |
| **Closed** | 2025-07-22 17:25:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Minor annoyance: I can't seem to pass type checking when instantiating `temporalio.api.cloud.namespace.v1.NamespaceSpec` with `search_attributes` without accessing the private type `temporalio.api.cloud.namespace.v1.NamespaceSpec._SearchAttributeType.ValueType`. Can SearchAttributeType please be exposed as a non-private name? Thanks.

#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-07-21 15:57:07.000 UTC</summary>

Thanks for reporting. Looks like something that potentially needs to be fixed in the proto generation. We will review.

</details>

<details>
<summary><strong>tconley1428</strong> commented on 2025-07-22 15:19:14.000 UTC</summary>

You can use the public subclass to accomplish this:
```python
    temporalio.api.cloud.namespace.v1.NamespaceSpec(
        search_attributes={ 
            "": temporalio.api.cloud.namespace.v1.NamespaceSpec.SearchAttributeType.SEARCH_ATTRIBUTE_TYPE_UNSPECIFIED
        }
    )
```

</details>


---

<a id="979"></a>

### #979: [Bug] [temporal python sdk release] invoke_model_activity is missing in latest release

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/979 |
| **State** | CLOSED |
| **Author** | surki (Suresh Kumar Ponnusamy) |
| **Created** | 2025-07-18 09:44:20.000 UTC (5 months ago) |
| **Updated** | 2025-07-21 04:07:44.000 UTC |
| **Closed** | 2025-07-21 04:07:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

The openai examples from https://github.com/temporalio/samples-python/blob/main/openai_agents/run_worker.py don't seem to work with latest release sdk version

Can you please cut a release with latest changes?

### Describe the bug


<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (2)

<details>
<summary><strong>tconley1428</strong> commented on 2025-07-18 23:03:20.000 UTC</summary>

Can you clarify? I just tested the sample against sdk version `1.14.1` which is the latest, and it ran without issues. You mention `1.14.0` which is not the latest release, so maybe what you are asking for has already occurred, `1.14.1` released July 10th.

</details>

<details>
<summary><strong>surki</strong> commented on 2025-07-21 04:07:44.000 UTC</summary>

Thanks, it works, sorry for the noise. (using devcontainers, the version inside was wrong!)


</details>


---

<a id="971"></a>

### #971: [Feature Request] Configure logging without info on message

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/971 |
| **State** | CLOSED |
| **Author** | teije01 |
| **Created** | 2025-07-15 13:08:11.000 UTC (5 months ago) |
| **Updated** | 2025-07-16 13:04:31.000 UTC |
| **Closed** | 2025-07-15 17:46:54.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem?

Our current logging set-up uses structural logging with all messages being json payloads. Part of configuring logging for our application involves patching the `workflow.logger` and `activity.logger` to drop contextual information out of the log message while keeping that in the logging extras.

An example of how our application configures logging is approximately the following:

```python
from temporalio import activity, workflow

def my_app_configure_logging():
    ....
    # configure workflow logger
    workflow.logger.workflow_info_on_message = False
    workflow.logger.workflow_info_on_extra = True        # default
    workflow.logger.full_workflow_info_on_extra = False  # default
    workflow.logger.log_during_replay = False            # default

    # configure activity logger
    activity.logger.activity_info_on_message = False
    activity.logger.activity_info_on_extra = True        # default
    activity.logger.full_activity_info_on_extra = True   # default
```

This works as expected, we get the logging exactly like we want to see it. The one thing I wonder about is if this is the right way to configure this.

### Describe the solution you'd like

A user-facing interface to configure logging for activities and workflow loggers. e.g. exposing a function that holds the user arguments explicitly with their defaults. e.g. `temporal.workflow.configure_logging`

```python
def configure_logging(
    *,
    info_on_message: bool = True,
    info_on_extra: bool = True,
    full_info_on_extra: bool = False,
    log_during_replay: bool = False,
) -> None:
    pass
```


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2025-07-15 14:29:07.000 UTC</summary>

> The one thing I wonder about is if this is the right way to configure this.

It is, and is why we made these attributes visible on the logger adapter (and the logger adapter itself visible). We even clearly document them, e.g. https://python.temporal.io/temporalio.workflow.LoggerAdapter.html. Though of course there's no need to set attributes that are already set with the default, so it sounds like you just need to set an attribute on each adapter.

</details>

<details>
<summary><strong>teije01</strong> commented on 2025-07-15 17:46:54.000 UTC</summary>

Thanks! The attributes were very clear, though I did not know whether it was alright to patch them in-place, but thanks for confirming that this is how it's supposed to work. I'll close this issue as resolved

Reactions: ðŸ‘ 1

</details>


---

<a id="848"></a>

### #848: [Bug] Updates conflict with replay

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/848 |
| **State** | CLOSED |
| **Author** | ndtretyak (Nikolay Tretyak) |
| **Created** | 2025-04-24 08:32:11.000 UTC (8 months ago) |
| **Updated** | 2025-06-02 20:56:00.000 UTC |
| **Closed** | 2025-06-02 20:56:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm using updates to allow the workflow to proceed to the next step.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

If an update occurs when the workflow execution is not cached on the worker, the update is effectively "lost", and I need to issue another update for it to be applied.

### Minimal Reproduction

1. Start the worker
2. Start  the workflow

    ```
    temporal workflow start --task-queue my-task-queue --type Workflow --workflow-id test --id-reuse-policy TerminateIfRunning
    ```
3. Restart the worker
4. Execute the update
    ```
    temporal workflow update execute --workflow-id test --name go
    ```
5. Observe that the activity has not been scheduled during the update handling. Additionally, there are some debug prints in the workflow code that may help.

```python
import asyncio
import concurrent.futures
import datetime

from temporalio import activity
from temporalio import workflow
from temporalio.client import Client
from temporalio.worker import Worker


@activity.defn
async def job() -> None:
    activity.logger.info("Activity started")


@workflow.defn(sandboxed=False)
class Workflow:
    def _set_can_run(self, value: bool, comment: str) -> None:
        print("Setting can_run to:", value, "comment:", comment)
        self.can_run = value

    @workflow.update
    def go(self) -> None:
        self._set_can_run(True, "update")

    @workflow.run
    async def run(self) -> None:
        self._set_can_run(True, "workflow start")

        def wait_condition() -> bool:
            print("Checking condition. result:", self.can_run)
            return self.can_run

        while True:
            await workflow.wait_condition(wait_condition)
            await workflow.execute_activity(job, start_to_close_timeout=datetime.timedelta(seconds=5))
            self._set_can_run(False, "activity completed")


async def main():
    client = await Client.connect("localhost:7233")

    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:
        worker = Worker(
            client,
            task_queue="my-task-queue",
            workflows=[Workflow],
          activities=[job],
          activity_executor=activity_executor,
        )
    await worker.run()


if __name__ == "__main__":
    asyncio.run(main())
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- Temporal Version: 1.27.1
- SDK Version: 1.11.0



#### Comments (2)

<details>
<summary><strong>dandavison</strong> commented on 2025-04-24 13:14:10.000 UTC</summary>

Thanks for this @ndtretyak. It does look like a bug; we're looking into it.

</details>

<details>
<summary><strong>Sushisource</strong> commented on 2025-05-07 00:37:33.000 UTC</summary>

Fixed in Core in https://github.com/temporalio/sdk-core/pull/910 - Python will need Core update & release

</details>


---

<a id="824"></a>

### #824: [Release] Cut a new release to address CVE-2025-29787

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/824 |
| **State** | CLOSED |
| **Author** | AlecRosenbaum (Alec Rosenbaum) |
| **Created** | 2025-04-10 12:56:58.000 UTC (9 months ago) |
| **Updated** | 2025-04-29 17:31:02.000 UTC |
| **Closed** | 2025-04-29 17:31:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸŽ‰ 1 |

#### Description

The version of the rust sdk in the most recent release includes a version of the `zip` crate affected by [CVE-2025-29787](https://nvd.nist.gov/vuln/detail/CVE-2025-29787).

While I don't suspect from an application-usage perspective that the temporal core sdk is extracting untrusted zip files, automated vulnerability scanning tools still pick up on the vulnerable version and prompt us to respond in some form.

Current `master` of this repository already has the core sdk bumped to a version that is not vulnerable, there just hasn't been a release uploaded to pypi since it was patched. The `zip` patch was included with https://github.com/temporalio/sdk-python/pull/802 I think just as a side-effect of the other work done in that change.

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2025-04-10 13:45:03.000 UTC</summary>

We plan on making our next release soon, though no exact date

Reactions: ðŸ‘ 2

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-04-29 17:31:00.000 UTC</summary>

This was released in https://github.com/temporalio/sdk-python/releases/tag/1.11.0

Reactions: ðŸŽ‰ 1

</details>


---

<a id="791"></a>

### #791: [Bug] We assume an element is present in an error details list

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/791 |
| **State** | CLOSED |
| **Author** | GSmithApps (Grant) |
| **Created** | 2025-03-24 13:49:48.000 UTC (9 months ago) |
| **Updated** | 2025-03-27 22:27:43.000 UTC |
| **Closed** | 2025-03-27 22:27:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | THardy98 |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Avoid getting an `IndexError` when we could have something more descriptive.

### Describe the bug

The SDK assumes [this list](https://github.com/temporalio/sdk-python/blob/49ca10e413ba67e1adfeed3b5577cb4f5b007e54/temporalio/client.py#L6076) is populated, but it is empty when getting an error of the form `RPCError('something went wrong, please retry (XXXXXXXX)')`

### Additional context

We believe this happened when the server was throwing errors -- in this particular case, we believe it was because a deployment of Temporal Cloud


#### Comments (2)

<details>
<summary><strong>GSmithApps</strong> commented on 2025-03-24 18:50:11.000 UTC</summary>

Big shoutout to @tysonmote for finding and logging this issue. Thank you, Tyson!

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>THardy98</strong> commented on 2025-03-27 22:27:42.000 UTC</summary>

@GSmithApps this has been fixed if you want to get back to the customer

</details>


---

<a id="790"></a>

### #790: [Feature Request] Helpers to prevent accidental non-passed-through imports

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/790 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-03-21 17:36:16.000 UTC (9 months ago) |
| **Updated** | 2025-10-28 21:59:03.000 UTC |
| **Closed** | 2025-10-28 21:59:03.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Describe the solution you'd like

Sometimes third party libraries, even passed through ones, import things lazily at runtime which causes them to be imported in the sandbox which users can't see and, if they don't do anything illegal, just cause memory overhead. Add a opt-out-able warning if we see any imports after the workflow file is loaded.

We should also consider a way for people to opt-in to warning if any imports occur that are not passed through. But of course they could then mark the ones that are safe explicitly, e.g. `with workflow.unsafe.imports_intentionally_reloaded():`.


---

<a id="786"></a>

### #786: [Feature Request] Initial Nexus implementation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/786 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-03-11 14:15:03.000 UTC (10 months ago) |
| **Updated** | 2025-08-26 15:26:11.000 UTC |
| **Closed** | 2025-08-26 15:26:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | dandavison |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Alongside Nexus Python SDK, need Temporal-specific implementation in this SDK. See https://github.com/temporalio/features/issues/609

#### Comments (2)

<details>
<summary><strong>prasek</strong> commented on 2025-05-06 18:56:37.000 UTC</summary>

- https://github.com/temporalio/proposals/pull/97

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-08-26 15:26:11.000 UTC</summary>

Closed by #813 

</details>


---

<a id="780"></a>

### #780: [Bug] Not able to debug code [python]

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/780 |
| **State** | CLOSED |
| **Author** | jasvirsinghdhillon |
| **Created** | 2025-03-01 07:02:46.000 UTC (10 months ago) |
| **Updated** | 2025-03-01 23:29:42.000 UTC |
| **Closed** | 2025-03-01 20:14:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

This is the code that I have written, which I want to put a debug point and test. 

from temporalio.client import Client
from temporalio.worker import Worker, UnsandboxedWorkflowRunner
from temporalio import activity, workflow
from datetime import timedelta
import asyncio
from dataclasses import dataclass

@dataclass
class Message:
    content: str

@activity.defn(name="chat_activity")
async def chat_activity(message: Message) -> Message:
    print(f"Message Received: {message.content}")
    return Message(content="Hello World!")


@workflow.defn(name="chat_workflow", sandboxed=False)
class ChatWorkflow:

    @workflow.run
    async def chat_workflow(self, query: str) -> Message:
        return await workflow.execute_activity(chat_activity, Message(query), start_to_close_timeout=timedelta(seconds=10))


async def main():
    client = await Client.connect("localhost:7233")

    print("Starting worker!")

    worker = Worker(
        client,
        task_queue="chat-queue",
        workflows=[ChatWorkflow],
        activities=[chat_activity],
        debug_mode=True,
        workflow_runner=UnsandboxedWorkflowRunner()
    )

    print("Worker started!")

    await worker.run()

if __name__ == '__main__':
    asyncio.run(main())

I work all great until I don't try to attach the debugger.  The moment I run in debug mode I start getting this exception: 

import sys; print('Python %s on %s' % (sys.version, sys.platform))
C:\Users\jasvi\PycharmProjects\practice_python_project\.venv\Scripts\python.exe -X pycache_prefix=C:\Users\jasvi\AppData\Local\JetBrains\PyCharm2024.2\cpython-cache "C:/Program Files/JetBrains/PyCharm 2024.2.4/plugins/python-ce/helpers/pydev/pydevd.py" --multiprocess --qt-support=auto --client 127.0.0.1 --port 49915 --file C:\Users\jasvi\PycharmProjects\practice_python_project\temporal.py 
Connected to pydev debugger (build 242.23726.102)
Starting worker!
Worker started!
Exception in callback <Task pending name='run (workflow: chat_workflow, id: b14c8089-9d71-4acf-a41f-d7bd623578ee, run: 01955078-5420-74c5-acb3-56539efc74f3)' coro=<_WorkflowInstanceImpl._run_top_level_workflow_function() running at C:\Users\jasvi\PycharmProjects\practice_python_project\.venv\Lib\site-packages\temporalio\worker\_workflow_instance.py:1873> cb=[set.remove()]>()
handle: <Handle <Task pending name='run (workflow: chat_workflow, id: b14c8089-9d71-4acf-a41f-d7bd623578ee, run: 01955078-5420-74c5-acb3-56539efc74f3)' coro=<_WorkflowInstanceImpl._run_top_level_workflow_function() running at C:\Users\jasvi\PycharmProjects\practice_python_project\.venv\Lib\site-packages\temporalio\worker\_workflow_instance.py:1873> cb=[set.remove()]>()>
Traceback (most recent call last):
  File "C:\Users\jasvi\AppData\Local\Programs\Python\Python311\Lib\asyncio\events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
TypeError: 'Task' object is not callable
Exception in callback <Task pending name='query: __temporal_workflow_metadata (workflow: chat_workflow, id: b14c8089-9d71-4acf-a41f-d7bd623578ee, run: 01955078-5420-74c5-acb3-56539efc74f3)' coro=<_WorkflowInstanceImpl._apply_query_workflow.<locals>.run_query() running at C:\Users\jasvi\PycharmProjects\practice_python_project\.venv\Lib\site-packages\temporalio\worker\_workflow_instance.py:642> cb=[set.remove()]>()
handle: <Handle <Task pending name='query: __temporal_workflow_metadata (workflow: chat_workflow, id: b14c8089-9d71-4acf-a41f-d7bd623578ee, run: 01955078-5420-74c5-acb3-56539efc74f3)' coro=<_WorkflowInstanceImpl._apply_query_workflow.<locals>.run_query() running at C:\Users\jasvi\PycharmProjects\practice_python_project\.venv\Lib\site-packages\temporalio\worker\_workflow_instance.py:642> cb=[set.remove()]>()>
Traceback (most recent call last):
  File "C:\Users\jasvi\AppData\Local\Programs\Python\Python311\Lib\asyncio\events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
TypeError: 'Task' object is not callable
Failed running eviction job, not evicting. Since eviction could not be processed, this worker cannot complete and the slot will remain forever used.
Traceback (most recent call last):
  File "C:\Users\jasvi\PycharmProjects\practice_python_project\.venv\Lib\site-packages\temporalio\worker\_workflow.py", line 262, in _handle_activation
    completion = await asyncio.wait_for(
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jasvi\AppData\Local\Programs\Python\Python311\Lib\asyncio\tasks.py", line 442, in wait_for
    return await fut
           ^^^^^^^^^
  File "C:\Users\jasvi\AppData\Local\Programs\Python\Python311\Lib\asyncio\futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "C:\Users\jasvi\AppData\Local\Programs\Python\Python311\Lib\asyncio\tasks.py", line 339, in __wakeup
    future.result()
  File "C:\Users\jasvi\AppData\Local\Programs\Python\Python311\Lib\asyncio\futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "C:\Users\jasvi\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jasvi\PycharmProjects\practice_python_project\.venv\Lib\site-packages\temporalio\worker\_workflow_instance.py", line 419, in activate
    raise RuntimeError(

It would be of great help if you could help me fix this issue. Thanks


#### Comments (2)

<details>
<summary><strong>dandavison</strong> commented on 2025-03-01 20:14:51.000 UTC</summary>

Hi @jasvirsinghdhillon, thanks for the report. You're absolutely right that this experience needs to be improved. We'll try to get to it soon. For now, could you take a look at the tips posted by @mnichols in https://github.com/temporalio/sdk-python/issues/603? In fact, if you don't mind, let's close this ticket and discuss in #603.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>jasvirsinghdhillon</strong> commented on 2025-03-01 23:29:40.000 UTC</summary>

Hi @dandavison ,

Thanks for your quick response! I really appreciate you looking into this issue. I'll go through the tips shared in #603 and continue the discussion there if needed.

Yes, I'm okay with closing this ticket. Thanks again for your support!



</details>


---

<a id="743"></a>

### #743: [Bug] ResourceBasedSlotOptions fields not consistent with the underlying Rust implementation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/743 |
| **State** | CLOSED |
| **Author** | florin-chelaru (Florin Chelaru) |
| **Created** | 2025-01-21 22:09:44.000 UTC (11 months ago) |
| **Updated** | 2025-01-22 13:16:57.000 UTC |
| **Closed** | 2025-01-22 12:42:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | â¤ï¸ 1 |

#### Description

### What are you really trying to do?

Trying to start a worker with a resource-based tuner rather than a fixed one. When setting `target_memory_usage` and `target_cpu_usage` to values between 0 and 1, the resulting worker only allocates the minimum number of slots for workflows and activities. 

To work around the issue, values between 0 and 100 have to be used instead.

### Describe the bug

Using the resource-based tuner for the worker as indicated [in the documentation](https://github.com/temporalio/sdk-python/blob/154aab9999bad7fc58e3e481d5a5986aa08d72db/temporalio/worker/_tuning.py#L37-L39) ends up creating a worker which always allocates the minimum for the Workflow and Activity slots.

```py
Worker(
    client,
    tuner=WorkerTuner.create_resource_based(
        target_memory_usage=0.80,  # 80% of available memory
        target_cpu_usage=0.8,
    ),
    ...
)
```

This is because the underlying Rust implementation interprets `target_memory_usage` and `target_cpu_usage` as percentages with values between 0 and 100 (rather than between 0 and 1): https://github.com/temporalio/sdk-core/blob/f0418936c4eec664faab931d0b174b9e57c65668/core/src/worker/tuner/resource_based.rs#L206-L218.

When setting values as percentages, i.e. 80.0 rather than 0.8, the tuner behaves as expected.

### Environment/Versions

- OS and processor: M2 Mac
- Temporal Version: 1.9.0
- Deploying Temporal to Kubernetes using the default Helm chart



#### Comments (2)

<details>
<summary><strong>Sushisource</strong> commented on 2025-01-21 23:02:28.000 UTC</summary>

So that line isn't saying it's a value between 0 and 100. It's saying the absolute limit for the PID controller is 100, which is not related to the underlying value's range.

What's relevant is that the PID attempts to reach the set point (the value you provide, between 0 and 1) and that it is comparing that to the measured value, which comes from [here](https://github.com/temporalio/sdk-core/blob/8fd1499ca9069328ef65c78b9638c1806f5e67ba/core/src/worker/tuner/resource_based.rs#L252) and is also between 0 and 1.

> ends up creating a worker which always allocates the minimum for the Workflow and Activity slots.

If this is indeed true it would be good to know what else is going on - but the problem is not the range of this value.

</details>

<details>
<summary><strong>florin-chelaru</strong> commented on 2025-01-22 12:42:33.000 UTC</summary>

My bad. I did a closer read of the code and now I understand it. The issue was likely lack of memory in my container. When I set the `target_memory_usage` to `80.0`, it effectively set a target to 8000%, which immediately triggered a spike in available slots. That's what made me suspect this was the issue in the first place. 

Thanks for the quick response!

Reactions: ðŸ‘ 1

</details>


---

<a id="732"></a>

### #732: [Feature Request] Please add workflow.execute_activity which has a typed resonse

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/732 |
| **State** | CLOSED |
| **Author** | spacether (Justin Black) |
| **Created** | 2025-01-15 20:12:41.000 UTC (11 months ago) |
| **Updated** | 2025-01-15 23:58:05.000 UTC |
| **Closed** | 2025-01-15 23:57:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

I want to know in my ide what the response type of workflow.execute_activity is
Right now your code defines it as Any which is too vague

### Describe the solution you'd like

Please use python generics to pass in activity with the generic output type
The return the output type / callable / couroutine etc

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2025-01-15 22:03:44.000 UTC</summary>

> Right now your code defines it as Any which is too vague

This should not be the case. The typesafe overloads for this extract the return type from from the activity passed in. For instance if you have:

```python
@activity.defn
def my_activity() -> str:
```

Then the result of `await workflow.execute_activity(my_activity, ...)` will be seen to type checkers as `str`. Are you referring to the non-type-safe overload?

</details>

<details>
<summary><strong>spacether</strong> commented on 2025-01-15 23:57:53.000 UTC</summary>

Hmm this may be a pycharm bug
I pass in a coroutine with no inputs which returns a uuid. Overloads should cover it, thanks! 

</details>


---

<a id="726"></a>

### #726: [Feature Request] Pydantic V2 support in converter

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/726 |
| **State** | CLOSED |
| **Author** | mattylight22 (Matt Nick) |
| **Created** | 2025-01-11 04:35:15.000 UTC (12 months ago) |
| **Updated** | 2025-02-18 17:49:08.000 UTC |
| **Closed** | 2025-01-13 13:09:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Currently the converter looks for the `parse_obj` method for Pydantic conversions, this method has been deprecated in favor of `model_validate`.

The converter should default to looking for the `model_validate` method and if it doesn't exist fallback to `parse_obj`. This would allow for an improvement without a breaking change.


### Suggested Code Update ðŸ‘€
Replace lines 1526-1536 of `converter.py`
```python
# Handle converting to Pydantic models supporting both V1 and V2 syntax.
# Default to V2's model_validate method, else fall back to V1's parse_obj.
# Methods must be either @staticmethod or @classmethod to be used.
for method_name in ("model_validate", "parse_obj"):
    attr = inspect.getattr_static(hint, method_name, None)
    if isinstance(attr, (classmethod, staticmethod)):
        if not isinstance(value, dict):
            raise TypeError(
                f"Cannot convert to {hint}, value is {type(value)} not dict"
            )
        return getattr(hint, method_name)(value)
```

ðŸ™ (love the work y'all do)

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2025-01-13 13:09:55.000 UTC</summary>

> Currently the converter looks for the parse_obj method for Pydantic conversions, this method has been deprecated in favor of model_validate.

The old behavior of the SDK working with Pydantic out of the box has been deprecated in favor of the custom converter approach shown in the sample. Re https://github.com/temporalio/sdk-python?tab=readme-ov-file#data-conversion:

> * Any class with a `dict()` method and a static `parse_obj()` method, e.g. [Pydantic models](https://pydantic-docs.helpmanual.io/usage/models)
>   * The default data converter is deprecated for Pydantic models and will warn if used since not all fields work. See [this sample](https://github.com/temporalio/samples-python/tree/main/pydantic_converter) for the recommended approach.

But this is just V1. Overall this issue is a duplicate of #621. We hope to build official in-SDK Pydantic support in the near term because we recognize Pydantic is far too difficult to use currently.

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-02-18 17:49:07.000 UTC</summary>

Hi @mattylight22, the latest version of the SDK now has official Pydantic (v2) support: https://github.com/temporalio/sdk-python/releases/tag/1.10.0

</details>


---

<a id="677"></a>

### #677: Question: Logging of uncaught exceptions when running under a threadpool executor

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/677 |
| **State** | CLOSED |
| **Author** | sarnikowski (Philip Tamimi-Sarnikowski) |
| **Created** | 2024-10-28 11:47:24.000 UTC (1y 2m ago) |
| **Updated** | 2024-10-30 12:44:58.000 UTC |
| **Closed** | 2024-10-30 12:44:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

When one normally wants to override the writing of uncaught exceptions in python, you can use the `excepthook` of sys.
Below is an example of how one can do this in temporal:
```python
import asyncio
import datetime as dt
import logging
import sys
import traceback

from temporalio import activity, workflow
from temporalio.client import Client
from temporalio.common import RetryPolicy
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import Worker

logging.basicConfig(
    level=logging.ERROR,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

logger = logging.getLogger(__name__)

def log_exception(exc_type, exc_value, exc_traceback):
    tb_lines = traceback.format_exception(exc_type, exc_value, exc_traceback)
    tb_text = "".join(tb_lines)
    logger.error("Uncaught exception:\n%s", tb_text)

@activity.defn
async def say_hello(name: str) -> str:
    raise Exception("ohh no, I could not say hello")

@workflow.defn
class GreetingWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        return await workflow.execute_activity(
            say_hello,
            name,
            start_to_close_timeout=dt.timedelta(seconds=10),
            retry_policy=RetryPolicy(
                initial_interval=dt.timedelta(seconds=0),
                maximum_attempts=1,
            ),
        )

async def start_temporal_server():
    return await WorkflowEnvironment.start_local(namespace="default", dev_server_log_level="error")

async def run_workflow(client: Client):
    async with Worker(
        client,
        task_queue="hello-task-queue",
        workflows=[GreetingWorkflow],
        activities=[say_hello],
    ):
        result = await client.execute_workflow(
            GreetingWorkflow.run,
            "World",
            id="greeting-workflow",
            task_queue="hello-task-queue",
        )
        print(f"Workflow result: {result}")

async def main():
    sys.excepthook = log_exception
    server = await start_temporal_server()
    await run_workflow(server.client)

if __name__ == "__main__":
    asyncio.run(main())
```

However, if the workers are running using for example a threadpool executor, this does not work because the hook of  `sys.excepthook` is not propagated to these threads (ref: https://github.com/python/cpython/pull/13515). 

I have an application where we do not want to print exceptions and their stacktraces to stderr, but rather log all uncaught exceptions. The problem is that if I want to do this for threads, the usual approach is to simply catch the exception and log the errors instead of raising an exception. However, if I want the temporal workflow to pick up the fact that an activity has failed, I believe I cannot do this ? So in essence I would like:
1. To log all uncaught exceptions instead of printing these to stderr.
2. Have temporal read the exceptions and stacktrace etc. so that it is readable on a workflow in the UI for example.
3. Have activities and workflows behave as usual, with errors triggering retries etc.

Is there a way to achieve this ?

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-10-28 13:12:29.000 UTC</summary>

Yes, you should look into interceptors. For instance, [this sample](https://github.com/temporalio/samples-python/tree/main/sentry) sends errors to Sentry. You can make an activity interceptor that logs and re-raises.

Also, feel free to join us on our forums or `#python-sdk` on [Slack](https://t.mp/slack) with general questions.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>sarnikowski</strong> commented on 2024-10-30 12:44:58.000 UTC</summary>

For posterity, I ended up with the following solution:

I added a structlog interceptor, as suggested by @cretz:
```python
from typing import Any, override

import structlog
from temporalio.worker import (
    ActivityInboundInterceptor,
    ExecuteActivityInput,
    ExecuteWorkflowInput,
    Interceptor,
    WorkflowInboundInterceptor,
    WorkflowInterceptorClassInput,
)

logger = structlog.get_logger()


class _StructlogActivityInboundInterceptor(ActivityInboundInterceptor):
    @override
    async def execute_activity(self, input: ExecuteActivityInput) -> Any:
        try:
            return await super().execute_activity(input)
        except Exception as e:
            logger.exception("uncaught exception in activity")
            raise e


class _StructlogWorkflowInboundInterceptor(WorkflowInboundInterceptor):
    @override
    async def execute_workflow(self, input: ExecuteWorkflowInput) -> Any:
        try:
            return await super().execute_workflow(input)
        except Exception as e:
            logger.exception("uncaught exception in workflow")
            raise e


class StructlogInterceptor(Interceptor):
    """Temporal Interceptor class which will log workflow & activity exceptions to stdout."""

    @override
    def intercept_activity(self, next: ActivityInboundInterceptor) -> ActivityInboundInterceptor:
        return _StructlogActivityInboundInterceptor(super().intercept_activity(next))

    @override
    def workflow_interceptor_class(
        self, input: WorkflowInterceptorClassInput
    ) -> type[WorkflowInboundInterceptor] | None:
        return _StructlogWorkflowInboundInterceptor
```

However, this only ensured that uncaught exceptions would be written to stdout, using structlog. To suppress the exception being written to stderr, I ended up redirecting stderr to `/dev/null`, which I don't think is an ideal solution. To limit the scope of the redirect, I made a context manager, that I am using on my threadpool executor. The manager looks like this:

```python
class RedirectStdErrToDevNull:
    """A context manager that pipes stderr output to /dev/null.

    Example:
        >>> import sys
        >>> with StdErrToDevNull():
        >>>     print("quack", file=sys.stderr)
        >>>     # No quacking will be written to stderr.
    """

    def __init__(self) -> None:
        self._devnull: TextIO | None = None
        self._stderr_redirector: contextlib.redirect_stderr[TextIO] | None = None

    def __enter__(self) -> "RedirectStdErrToDevNull":
        self._devnull = open(os.devnull, "w")
        self._stderr_redirector = contextlib.redirect_stderr(self._devnull)
        self._stderr_redirector.__enter__()
        return self

    def __exit__(
        self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None
    ) -> None:
        if self._stderr_redirector is not None:
            self._stderr_redirector.__exit__(exc_type, exc_val, exc_tb)
        if self._devnull is not None:
            self._devnull.close()
```

</details>


---

<a id="673"></a>

### #673: [Bug] NondeterminismError when replaying overlapping signals and updates

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/673 |
| **State** | CLOSED |
| **Author** | cbrewster (Connor Brewster) |
| **Created** | 2024-10-16 16:23:21.000 UTC (1y 2m ago) |
| **Updated** | 2024-10-23 18:43:14.000 UTC |
| **Closed** | 2024-10-23 18:43:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | â¤ï¸ 1 |

#### Description

### What are you really trying to do?

I have a workflow which runs various activities depending on signals/updates from the user. The workflow keeps track of the current state (running, stopped) using a counter to keep track of how many activities are currently running. There is a resume update which we only want to start an activity if there isn't already an activity running.

I've implemented this but during replays we are seeing non-determinism errors. I've made a minimal reproduction of this.

### Describe the bug

We're seeing workflow replays that cause events to be processed in a different order than how the events originally came in:

Original order of events:
```
- start workflow
- start timer
- signal start
- start activity 1 (state = "running", running_count = 1)
- activity 1 finishes (state = "stopped", running_count = 0)
- update resume
- start activity 2 (state = "running", running_count = 1)
- activity 2 finishes (state = "stopped", running_count = 0)
- update returns
- timer completes, workflow done
```

During replay we get a different order which causes the NDE:

```
- start workflow
- start timer
- signal start
- start activity 1 (state = "running", running_count = 1)
- update resume <--- This update gets ran before we process the completion of activity 1
- resume returns early because state is "running"
- activity 1 finishes (state = "stopped", running_count = 0)
- NDE because we didn't end up scheduling activity 2
```

### Minimal Reproduction

This reproduction runs the workflow and then automatically replays the workflow to demonstrate the nondeterminism error.

```python
import asyncio
from datetime import timedelta
from uuid import uuid4

from temporalio import workflow
from temporalio import activity
from temporalio.client import Client
from temporalio.worker import Replayer, Worker


@activity.defn
async def test_activity() -> None:
    return


@workflow.defn
class TestWorkflow:
    def __init__(self) -> None:
        self.running_count = 0
        self.state = "stopped"

    @workflow.run
    async def run(self) -> None:
        await asyncio.sleep(1)

    @workflow.signal
    async def start(self) -> None:
        await self.run_one()

    @workflow.update
    async def resume(self) -> None:
        if self.state == "running":
            return
        await self.run_one()

    async def run_one(self):
        self.running_count += 1
        self.state = "running"
        await workflow.execute_activity(
            test_activity,
            start_to_close_timeout=timedelta(seconds=1),
        )
        self.running_count -= 1
        if self.running_count == 0:
            self.state = "stopped"


async def main() -> None:
    id = str(uuid4())
    client = await Client.connect("localhost:7233")
    async with Worker(
        client=client,
        task_queue="test",
        workflows=[TestWorkflow],
        activities=[test_activity],
    ):
        workflow_handle = await client.start_workflow(
            TestWorkflow.run,
            id=id,
            task_queue="test",
        )
        await workflow_handle.signal(TestWorkflow.start)
        await asyncio.sleep(0.5)
        await workflow_handle.execute_update(TestWorkflow.resume)
        await workflow_handle.result()

    workflows = client.list_workflows(f"WorkflowId = '{id}'")
    histories = workflows.map_histories()
    replayer = Replayer(workflows=[TestWorkflow])
    await replayer.replay_workflows(histories)


if __name__ == "__main__":
    asyncio.run(main())

```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Linux
- Temporal Version: Server 1.24.2, UI 2.28.0, sdk-python 1.8.0
- Using temporal CLI

### Additional Comments

Replacing the update with a signal instead does not have the same issue.

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-10-16 17:18:15.000 UTC</summary>

Thanks for the detailed report! The activity completion definitely should be delivered before update on replay if it was originally. We will likely treat this as a high priority (granted updates are not yet GA for these kinds of reasons).

Reactions: â¤ï¸ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-10-23 13:02:22.000 UTC</summary>

While the linked PR there fixes the issue, note that the fix will not be available until the next Python release that includes the next Core upgrade.

Reactions: ðŸ‘ 1

</details>


---

<a id="671"></a>

### #671: [Feature Request] Fix tonic Vulnerability by Updating Rust Package to Version 0.12.3

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/671 |
| **State** | CLOSED |
| **Author** | rbtbar (Robert Bar) |
| **Created** | 2024-10-15 17:12:02.000 UTC (1y 2m ago) |
| **Updated** | 2024-11-22 18:36:57.000 UTC |
| **Closed** | 2024-11-22 18:36:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Security scans have identified a vulnerability in one of the Rust dependencies. The current version of the package is affected by the following advisories:

[GHSA-4jwc-w2hc-78qv](https://github.com/advisories/GHSA-4jwc-w2hc-78qv)
[CVE-2024-47609](https://security-tracker.debian.org/tracker/CVE-2024-47609)

If possible, package should be updated to version 0.12.3, which contains the necessary patches.

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-10-16 12:28:20.000 UTC</summary>

:+1: We will do this as part of our next Core upgrade

</details>

<details>
<summary><strong>Sushisource</strong> commented on 2024-11-22 18:36:57.000 UTC</summary>

This got finished in https://github.com/temporalio/sdk-python/pull/683

</details>


---

<a id="643"></a>

### #643: UpdateWithStart SDK API

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/643 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-09-12 21:47:43.000 UTC (1y 3m ago) |
| **Updated** | 2025-01-07 17:37:37.000 UTC |
| **Closed** | 2025-01-07 17:37:37.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

Expose update-with-start operations via the SDK


---

<a id="633"></a>

### #633: Cannot execute workflow update from a temporal activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/633 |
| **State** | CLOSED |
| **Author** | sytdas (Tan Du) |
| **Created** | 2024-09-04 04:50:57.000 UTC (1y 4m ago) |
| **Updated** | 2024-09-04 14:21:40.000 UTC |
| **Closed** | 2024-09-04 05:07:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I have 2 workflows, one run by schedule to fetch updates and send it to a long running workflow which hold some state. When I tried to send update using activity, I got following error.

```
Error: ({'activity_id': '2', 'activity_type': 'update_workflow', 'attempt': 1, 'namespace': 'default', 'task_queue': 'hello-cron-task-queue-schedule', 'workflow_id': 'hello-cron-workflow-id-scheduler', 'workflow_run_id': 'b3495343-a347-4706-8b62-0be845b46ded', 'workflow_type': 'GreetingCronWorkflow'})
Traceback (most recent call last):
  File "/Users/sytd/Code/sytd/temporal-update/.venv/lib/python3.12/site-packages/temporalio/service.py", line 1064, in _rpc_call
    resp = await client.call(
           ^^^^^^^^^^^^^^^^^^
  File "/Users/sytd/Code/sytd/temporal-update/.venv/lib/python3.12/site-packages/temporalio/bridge/client.py", line 151, in call
    resp.ParseFromString(await resp_fut)
                         ^^^^^^^^^^^^^^
temporal_sdk_bridge.RPCError: (9, 'limit on number of total updates has been reached (2000)', b'')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sytd/Code/sytd/temporal-update/main.py", line 98, in update_workflow
    return await handler.execute_update(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sytd/Code/sytd/temporal-update/.venv/lib/python3.12/site-packages/temporalio/client.py", line 1862, in execute_update
    handle = await self._start_update(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sytd/Code/sytd/temporal-update/.venv/lib/python3.12/site-packages/temporalio/client.py", line 2013, in _start_update
    return await self._client._impl.start_workflow_update(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sytd/Code/sytd/temporal-update/.venv/lib/python3.12/site-packages/temporalio/client.py", line 5294, in start_workflow_update
    resp = await self._client.workflow_service.update_workflow_execution(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sytd/Code/sytd/temporal-update/.venv/lib/python3.12/site-packages/temporalio/service.py", line 991, in __call__
    return await self.service_client._rpc_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sytd/Code/sytd/temporal-update/.venv/lib/python3.12/site-packages/temporalio/service.py", line 1079, in _rpc_call
    raise RPCError(message, RPCStatusCode(status), details)
temporalio.service.RPCError: limit on number of total updates has been reached (2000)
```
Here is the code https://github.com/sytdas/temporal-update

#### Comments (2)

<details>
<summary><strong>sytdas</strong> commented on 2024-09-04 04:52:05.000 UTC</summary>

I can trigger the update from temporal cli. Now I need to make it work with the scheduled workflow activity.

</details>

<details>
<summary><strong>sytdas</strong> commented on 2024-09-04 05:07:21.000 UTC</summary>

There's a bug in my code when sending invalid data. 

</details>


---

<a id="618"></a>

### #618: [Feature Request] Improve wait_condition semantics

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/618 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-08-21 12:09:17.000 UTC (1y 4m ago) |
| **Updated** | 2025-04-10 13:34:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Ideally, the following would not happen:

```python
@workflow.defn
class WaitForSharedMutableStateWorkflow:
    def __init__(self) -> None:
        self.cond = False

    @workflow.run
    async def run(self):
        self.cond = True
        await asyncio.gather(self.coro("1"), self.coro("2"))

    async def coro(self, id: str):
        await workflow.wait_condition(lambda: self.cond)
        print(f"after wait coro {id} sees {self.cond}")  # â—â— <== coro2 sees Falseâ—
        self.cond = False
        await asyncio.sleep(1)
        self.cond = True
```

Completely eliminating the possibility would be ideal, but best-effort improvements would also be welcome.

#### Comments (2)

<details>
<summary><strong>GSmithApps</strong> commented on 2025-04-09 21:50:15.000 UTC</summary>

Hey @dandavison,

I definitely agree on this.

As an interim code-around, it worked on my end if I replace

```python
        await asyncio.gather(self.coro("1"), self.coro("2"))
```

with

```python
        for task in [self.coro("1"), self.coro("2")]:
            asyncio.create_task(task)
            await asyncio.sleep(0.0000000001)
```

On the downside, it creates actions because it creates a temporal timer for each coro

I'm curious to hear what you think!

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-04-10 13:34:03.000 UTC</summary>

No, because the wait condition is resolved after a second which can be on another task. And even if it did work, it's not a general solution to the problem because the concern is that wait conditions _all_ wake up if the predicate is satisfied (as opposed to other SDKs that only wake up one). Whether that happens before or after a sleep is immaterial. The snippet is a trivial example showing the issue, but that's not real world code. In real world code, you just have to make sure you code for the fact that the line of code after a wait condition that may be waited on multiple times concurrently may not match the condition.

</details>


---

<a id="601"></a>

### #601: [Feature Request] Reset workflow feature

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/601 |
| **State** | OPEN |
| **Author** | burbaljaka (Timur Kapitonov) |
| **Created** | 2024-08-05 10:06:36.000 UTC (1y 5m ago) |
| **Updated** | 2024-08-05 13:32:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

There is a missing feature in Python SDK for workflow reset

### Describe the solution you'd like

I have implemented this on my project and would like to share my findings
### Additional context

<!-- Add any other context or screenshots about the feature request here. -->


#### Comments (2)

<details>
<summary><strong>burbaljaka</strong> commented on 2024-08-05 10:07:38.000 UTC</summary>

I would like to create a PR with go-live ready code, let me know if you could be interested in this kind of assistance :)

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-08-05 13:32:38.000 UTC</summary>

We intentionally don't expose workflow reset as high-level in most SDKs, but you can access the raw gRPC call for workflow reset via `workflow_service`. So `await my_client.workflow_service.reset_workflow_execution(...)` is available for advanced users, but it is considered an advanced call and not one people use regularly (it's for getting out of bad situations, if it's being used as part of regular flow, an alternative should be considered).

</details>


---

<a id="592"></a>

### #592: [Bug] Activity logger context not populated when `activity_info_on_message` False

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/592 |
| **State** | CLOSED |
| **Author** | TheCodeWrangler (Nathan Price) |
| **Created** | 2024-07-24 14:10:27.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-24 16:30:15.000 UTC |
| **Closed** | 2024-07-24 16:30:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I would like to have my logs include activity info on extra but not in the message
```
activity.logger.activity_info_on_message = False
activity.logger.activity_info_on_extra = True
```

However in practice after enabling a debugger when
https://github.com/temporalio/sdk-python/blob/main/temporalio/activity.py#L462
is reached the value of `context._logger_details` is None/null

Running the same debug again with
```
activity.logger.activity_info_on_message = True
activity.logger.activity_info_on_extra = True
```
results in the debugger showing that `context._logger_details` is populated

### Describe the bug

It should be possible to include extra context on activities without including them in the string of the message however in practice it does not seem to work.

### Minimal Reproduction

Prior to running hello world on running the workers 
```
from temporal import activity

activity.logger.activity_info_on_message = False
activity.logger.activity_info_on_extra = True
```

Run an activity which inclues `activity.logger.info("some message") ` and observe that the logger results in  "temporal_activity": null



#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-07-24 14:20:42.000 UTC</summary>

Thanks for the report! This is a bug. We need to change https://github.com/temporalio/sdk-python/blob/c4b1a019ad2627edb20a954edfd57c9ba414f3d5/temporalio/activity.py#L462 to access `logger_details` instead of `_logger_details` since the former will lazily populate it (which was occurring when `activity_info_on_message` was `True` so the bug wasn't obvious before).

</details>

<details>
<summary><strong>TheCodeWrangler</strong> commented on 2024-07-24 14:39:10.000 UTC</summary>

Thank you for quickly (nearly immediately) identifying the fix!

I posted a PR
https://github.com/temporalio/sdk-python/pull/593

Reactions: â¤ï¸ 1

</details>


---

<a id="576"></a>

### #576: [Feature Request] better workflow testing in multi-worker scenarios

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/576 |
| **State** | CLOSED |
| **Author** | reith (amir) |
| **Created** | 2024-07-09 18:38:17.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-10 01:12:22.000 UTC |
| **Closed** | 2024-07-10 01:12:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

I have a workflow that runs activities in an explicitly set task queue. One reason for this is that I have two different task queues but the issue affects any workflows that set task queues explicitly. In workflow tests, in which I merely mock activities, I don't really need to run multiple workers, one per task queue. My current approach is to subclass my workflow in the test suite, override task queues names and forward `run` to the actual superclass but I don't like it because I have to deal with workflow and data validation for my dummy workflow that I only added for tests.

Note that, there is no straightforward way to run multiple workers in test suites, or I haven't found it yet. The bellow code:

```python
async with Worker(task_queue=queue_a, activities=[activitiy_a], workflows=Workflow):
    async with Worker(task_queue=queue_b, activities=[activity_b], workflows=[]):
       await env.client.execute_workflow(Workflow.run, task_queue=queue_a) 
```

errors _activity function activity_b is not registered on this worker_. I can run the other worker in a separate thread though.

### Describe the solution you'd like

I can think of some solutions:
- `testing.WorkflowEnvironment` can provide a `with_overriden_task_queue` which modifies the client to run all workflows in a single queue.
- support nested Workers() (above code)
- provide a helper, `WorkerGroup`, that can act as an async context provider for multiple workers.

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-07-09 21:34:14.000 UTC</summary>

> In workflow tests, in which I merely mock activities, I don't really need to run multiple workers, one per task queue

It should be harmless to do so and it's helpful; to possibly get errors about misconfigured task queue name and such during tests

> My current approach is to subclass my workflow in the test suite, override task queues names and forward run to the actual superclass but I don't like it because I have to deal with workflow and data validation for my dummy workflow that I only added for tests.

The best approach is to just run as many workers as you need in tests. But if you are sure you don't want to run multiple workers in tests, consider using workflow input to drive the task queue name instead of subclassing.

> Note that, there is no straightforward way to run multiple workers in test suites, or I haven't found it yet.

Running multiple workers is no different in tests or production. The library code is intentionally the same.

> The bellow code: errors activity function activity_b is not registered on this worker. I can run the other worker in a separate thread though.

This sounds like you are trying to call `activity_b` in your workflow from `queue_a`.

> testing.WorkflowEnvironment can provide a with_overriden_task_queue which modifies the client to run all workflows in a single queue.

The environment is unrelated to the workers you start on it. It has no knowledge of workers, it is simply a built-in Temporal server. Some people use the environment for 

> support nested Workers() (above code)

These are supported. If it is not working there must be something else incorrect such as improperly calling an activity on a task queue worker that it is not registered on.

> provide a helper, WorkerGroup, that can act as an async context provider for multiple workers.

This is easy enough for anyone to write. You don't have to nest, you can just use `run`, e.g. maybe something like:

```
workers = [
    Worker(task_queue=queue_a, activities=[activity_a], workflows=[WorkflowA]),
    Worker(task_queue=queue_b, activities=[activity_b], workflows=[WorkflowB]),
    Worker(task_queue=queue_c, activities=[activity_c], workflows=[WorkflowC])
]
async with asyncio.TaskGroup() as tg:
    worker_tasks = [tg.create_task(w.run()) for w in workers]
    # Do stuff
    for w in worker_tasks:
        w.cancel()
```

That is untested (just typed here in GitHub), but the idea is there. Use Python asyncio utilities like task group or `gather` to run multiple things concurrently (but nested is fine too like your previous code). Feel free to come join us on [Slack](https://t.mp/slack) in `#python-sdk` to discuss general test questions/approaches.

</details>

<details>
<summary><strong>reith</strong> commented on 2024-07-10 01:12:22.000 UTC</summary>

Thanks @cretz for the detailed response.

> This sounds like you are trying to call `activity_b` in your workflow from `queue_a`.

This is the intended usage. I run my workflow in `queue_a,` along with some activities, but that workflow also executes some specific activities in task queue `queue_b`. 

> This is easy enough for anyone to write. You don't have to nest, you can just use run, e.g. maybe something like.

I'll try this. Thanks!


</details>


---

<a id="564"></a>

### #564: [Bug] Converting payloads and other exception-possible actions done after command creation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/564 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-06-27 13:45:30.000 UTC (1y 6m ago) |
| **Updated** | 2024-08-09 18:19:45.000 UTC |
| **Closed** | 2024-08-09 18:19:45.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Describe the bug

When scheduling an activity (and presumably in many cases), we create the command _before_ we try to serialize the parameters. This can cause an issue where if it fails, the command is still in the activations commands list but incomplete. This is usually not a big deal since it's supposed to be a task failure which does not send commands and waits for code fix. But some users are not allowing these failures to be task failures and therefore you get a half-complete command going to Core.

To replicate, simply convert a serialization error for a parameter to an activity to an application error.

There are a couple of approaches to fix. We can take the more drastic approach of not creating the command until we have built it (changes a lot since we are doing a proto-lib-encouraged approach of creating the thing before populating it today). Or we can just make sure to just do things that can fail before creating the command.

Relates to and closes https://github.com/temporalio/sdk-python/issues/540.


---

<a id="531"></a>

### #531: [Feature Request] Include stack trace on deadlock detection exception

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/531 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-05-22 20:08:35.000 UTC (1y 7m ago) |
| **Updated** | 2024-08-27 21:35:32.000 UTC |
| **Closed** | 2024-08-27 21:35:32.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Describe the solution you'd like

We raise a deadlock detection error but we don't include any stack trace info. We should try to include this. If we can do an individual task stack trace that would be great but maybe not possible. If possible, have the stack trace replace the existing deadlock stack trace but this may not be possible if it's lots of coroutine stack traces instead of one.


---

<a id="523"></a>

### #523: [Bug] Make sure `is_replaying` is `True` upon eviction, and only throw certain exception types out on eviction

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/523 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-05-09 18:49:49.000 UTC (1y 8m ago) |
| **Updated** | 2024-05-23 12:13:54.000 UTC |
| **Closed** | 2024-05-15 14:50:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Today, anyone in a `finally` or `except Exception` can catch exceptions occurring during eviction and do side-effecting things like logging. We need to set `is_replaying` to `True` during eviction (or maybe Core does) and we need to make a new `class WorkflowEvictingException(BaseException)` that we use when we see stuff happening during delete.

#### Comments (2)

<details>
<summary><strong>jpcarlino</strong> commented on 2024-05-22 18:42:35.000 UTC</summary>

@cretz I recently upgraded our pipeline workers to use the Temporal SDK 1.6.0. Since then I see this error being printed in our logs: `Ignoring add while deleting`, which seems to come from [this line](https://github.com/temporalio/sdk-python/blob/1.6.0/temporalio/worker/_workflow_instance.py#L1517). I see you did change it to [this](https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/_workflow_instance.py#L1520-L1521).

My question is: should I catch that exception and ignore it? If not, what should I do on my side to avoid that. Unfortunately it is flooding our logs right now. Thank you.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-05-23 12:13:54.000 UTC</summary>

Yes, this issue is meant to fix that problem because logging should not occur during is_replaying. In the meantime you can ignore these exceptions, they are only on cache eviction.

Reactions: ðŸ‘ 2

</details>


---

<a id="497"></a>

### #497: [Feature Request] Check Protobuf 5.x Support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/497 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-04-01 14:31:08.000 UTC (1y 9m ago) |
| **Updated** | 2025-01-13 13:31:19.000 UTC |
| **Closed** | 2025-01-13 13:31:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

5.x just dropped a few weeks ago and we don't CI against it. We either need to confirm it is easy to support, or set `<5` as a dependency constraint for now while we build support.

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-12-11 16:25:33.000 UTC</summary>

This is basically being solved by #694 because 3.13 only works with 5.x and CI is passing

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-01-13 13:31:19.000 UTC</summary>

This was done as part of #694 (the lock file now uses proto 5.x)

</details>


---

<a id="489"></a>

### #489: [Feature Request] Make log "extra" have dict instead of info

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/489 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-03-19 15:41:23.000 UTC (1y 9m ago) |
| **Updated** | 2024-04-02 17:36:37.000 UTC |
| **Closed** | 2024-04-02 17:36:37.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Describe the solution you'd like

Today we set `activity_info` and `workflow_info` on `extra` as entire info data classes which doesn't work well with loggers. We should stop doing this by default, and start setting `temporal_activity` and `temporal_workflow` as the logger details dict


---

<a id="482"></a>

### #482: [Feature Request] API key client option

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/482 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-02-27 15:22:14.000 UTC (1y 10m ago) |
| **Updated** | 2024-03-13 21:43:13.000 UTC |
| **Closed** | 2024-03-13 21:43:13.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

**Describe the solution you'd like**

See https://github.com/temporalio/features/issues/426


---

<a id="463"></a>

### #463: [Feature Request] Support disabling host verification

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/463 |
| **State** | CLOSED |
| **Author** | BrianKopp (Brian Kopp) |
| **Created** | 2024-01-19 18:16:56.000 UTC (1y 11m ago) |
| **Updated** | 2025-06-09 16:56:36.000 UTC |
| **Closed** | 2024-11-19 17:24:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I am unable to use the python SDK to connect to a temporal cluster when using self-generated TLS certificates due to domain name validation. We use generic self-generated TLS certificates as part of our testing process against various temporal clusters. In other SDK's & the CLI, we usually disable domain name validation in order to enable this standardization.  E.g. by using the `TEMPORAL_TLS_DISABLE_HOST_VERIFICATION` environment variable [in the CLI](https://docs.temporal.io/cli) or the `SimpleSslContextBuilder setUseInsecureTrustManager` method [in java](https://community.temporal.io/t/java-client-with-tls/5313). In the python SDK, I get the following error:

```
  File "/usr/local/lib64/python3.11/site-packages/temporalio/client.py", line 164, in connect
    await temporalio.service.ServiceClient.connect(connect_config),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib64/python3.11/site-packages/temporalio/service.py", line 184, in connect
    return await _BridgeServiceClient.connect(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib64/python3.11/site-packages/temporalio/service.py", line 711, in connect
    await client._connected_client()
  File "/usr/local/lib64/python3.11/site-packages/temporalio/service.py", line 724, in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib64/python3.11/site-packages/temporalio/bridge/client.py", line 87, in connect
    await temporalio.bridge.temporal_sdk_bridge.connect_client(
RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, Custom { kind: InvalidData, error: InvalidCertificate(NotValidForName) }))
```

### Describe the solution you'd like

It would be great to be able to disable hostname validation as part of the [TlsConfig class](https://python.temporal.io/temporalio.service.TLSConfig.html). It appears that the client TLS is configured [here in the rust SDK](https://github.com/temporalio/sdk-core/blob/f673f1da17ded3cf3679f4a87db27d9d7d316e88/client/src/lib.rs#L406). Perhaps a hostname validation disable flag could drive the behavior down here somewhere.

### Additional context

I'm happy to help contribute on this if that would be welcome. Thanks!


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-01-20 02:01:54.000 UTC</summary>

We can look into having a disable host verification option

> NotValidForName

So it looks like the cert for the server is for a different name than the host you're connecting to. You should be able to set `TlsConfig.domain` as an expected domain from the cert the server uses instead of it default to the given host (this is SNI override).



Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>burnhamd</strong> commented on 2025-06-09 16:56:35.000 UTC</summary>

Bringing this back up.  We have same issue where regulatory requirements dictate that we encrypt in transit even within vpc in AWS.  This becomes difficult when trying to fake a CA in our environment also opened a issue for the rust client https://github.com/temporalio/sdk-core/issues/932

</details>


---

<a id="462"></a>

### #462: [Bug] Handle possibility of update-before-start

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/462 |
| **State** | OPEN |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2024-01-12 17:39:57.000 UTC (1y 12m ago) |
| **Updated** | 2025-02-07 03:29:12.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug
Because of the fact that we re-order updates to come before other things a-la signals in Core, it's currently possible to handle an update before running the first bit of workflow code if an update comes in same task as start workflow.

We'll likely need to buffer things as we do in other SDKs (in lieu of making some other choice)

### Minimal Reproduction
See the `test_workflow_update_handlers_unhappy` test and discussion here

### Environment/Versions
<!-- Please complete the following information where relevant. -->

* OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
* Temporal Version: [e.g. 1.14.0?] and/or SDK version
* Are you using Docker or Kubernetes or building Temporal from source?

### Additional context
<!-- Add any other context about the problem here. -->



#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-01-12 17:45:02.000 UTC</summary>

Also, we need to stop reordering of activation jobs inside of Python. Right now this moves signals first and does not account for updates. But we need to make very sure that the Core ordering matches what is already happening (besides update).

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-01-12 18:40:00.000 UTC</summary>

.NET equivalent: https://github.com/temporalio/sdk-dotnet/issues/176. We need to make sure we have a test that does a start then update _then_ starts a worker and confirms behavior with dynamically registered handlers.

Reactions: ðŸ‘ 1

</details>


---

<a id="452"></a>

### #452: [Feature Request] Decode json/plain as json/protobuf if type_hint is a subclass of protobuf.Message

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/452 |
| **State** | CLOSED |
| **Author** | ndtretyak (Nikolay Tretyak) |
| **Created** | 2023-12-25 20:08:09.000 UTC (2 years ago) |
| **Updated** | 2024-02-20 21:34:28.000 UTC |
| **Closed** | 2024-02-20 21:34:28.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

All my workflows accepts protobufs as arguments and it works fine if I start them using SDK's client since it adds `json/protobuf` as payload's encoding. But sometimes it is useful to start workflows using `tctl` that uses`json/plain` encoding.

Now, `JSONPlainPayloadConverter`, which is used for the `json/plain` encoding, does not handle `proto.Message`s in type_hints and just fails.

### Describe the solution you'd like

I'd like `JSONPlainPayloadConverter` to handle json-encoded protobufs.



#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-01-03 16:40:49.000 UTC</summary>

> sometimes it is useful to start workflows using tctl that usesjson/plain encoding.

This is a problem with `tctl` not the SDK. We need the ability to support saying the metadata encoding of inputs (https://github.com/temporalio/tctl/issues/185). I believe once the newer incarnation of the Temporal CLI is developed, it will support setting the encoding.

> I'd like JSONPlainPayloadConverter to handle json-encoded protobufs.

You can do this with a custom converter, but I hesitate to do this by default. We only want the JSON proto converter to have special rules about proto.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-02-20 21:34:28.000 UTC</summary>

Closing (not adding proto support to general json/plain converter), but be advised that the new Temporal CLI before it reaches 1.0 GA will support supplying protos.

</details>


---

<a id="450"></a>

### #450: pathlib RestrictedWorkflowAccessError

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/450 |
| **State** | CLOSED |
| **Author** | vamsu |
| **Created** | 2023-12-21 05:41:14.000 UTC (2 years ago) |
| **Updated** | 2024-01-03 16:43:46.000 UTC |
| **Closed** | 2024-01-03 16:43:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Execute a workflow 

### Describe the bug

Running into: Evicting workflow with run ID 2cc67004-c6a9-473b-bb84-78be1b3dad30, message: Workflow activation completion failed: Failure { failure: Some(Failure { message: \"Cannot access pathlib.Path.resolve.__call__ from inside a workflow. If this is code from a module not used in a workflow or known to only be used deterministically from a workflow, mark the import as pass through

So far, I tried the below, but no luck

```
with workflow.unsafe.imports_passed_through():
    import pathlib
```
anyway to disable the restrictions on sandbox and let the developers ensure deterministic behavior ?


#### Comments (2)

<details>
<summary><strong>jackklika</strong> commented on 2023-12-24 19:42:14.000 UTC</summary>

A quick fix if you don't mind the entire workflow being deterministic is to disable sandboxing on the entire workflow via `@workflow.defn(sandboxed=False)`.

Reactions: ðŸ‘ 3

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-01-03 16:43:46.000 UTC</summary>

> anyway to disable the restrictions on sandbox and let the developers ensure deterministic behavior ?

Sure, see the README: https://github.com/temporalio/sdk-python#avoiding-the-sandbox. You can:

```python
with temporalio.workflow.unsafe.sandbox_unrestricted():
    my_path.resolve();
```

But I would strongly discourage using disk-based calls from inside a workflow.

(closing issue as just a question, but feel free to keep responding here or join us on `#python-sdk` on Slack)

Reactions: ðŸ‘ 2

</details>


---

<a id="446"></a>

### #446: [Feature Request] Configurable workflow failure exception types

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/446 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-12-11 12:57:11.000 UTC (2y 1m ago) |
| **Updated** | 2024-04-30 17:33:46.000 UTC |
| **Closed** | 2024-04-30 17:33:46.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Describe the solution you'd like

Per https://github.com/temporalio/features/issues/322, need:

* `workflow_failure_exception_types: Set[Type[Exception]]` on Worker constructor
* `failure_exception_types: Set[Type[Exception]]` on `@workflow.defn`
* Disallow `NondeterminismError` to be included until https://github.com/temporalio/sdk-core/issues/656 is done



---

<a id="401"></a>

### #401: [Feature Request] Trigger Activity Retry when workflow got signal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/401 |
| **State** | CLOSED |
| **Author** | devery59 (Kris) |
| **Created** | 2023-10-16 09:38:11.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-16 12:10:54.000 UTC |
| **Closed** | 2023-10-16 12:10:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

I'm trying to make pattern when i do activity and wait activity done(using signal).
According to signal state i want to trigger activity retry logic will be executed.

Here's my current pattern

```
...
    await workflow.execute_activity(
        acvitiy_1,
        args = [
            arg1,
            arg2,
            arg3,
            ],
        start_to_close_timeout=timedelta(minutes=3),
    )
    await workflow.execute_activity(
        acvitiy_1,
        args = [
            arg1,
            arg2,
            arg3,
            ],
        start_to_close_timeout=timedelta(minutes=3),
    )
    try:
        await workflow.wait_condition(lambda: <condition>, timeout=timedelta(minutes=5))
    except:
        raise ApplicationError(<Error Message>)
...
``` 

In this pattern, i wait condition related to temporal signal and if condition makes True, next step will be executed.
But when i got signal failed so condition is False or timeout error is occured, i want to execute activity again which is failed using _**Temporal Retry Policy**_. 

### Describe the solution you'd like

I know i can add for loop or something else in python, but i want to use temporal pattern. Is there any solution pattern for me?



#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-10-16 11:57:48.000 UTC</summary>

> I know i can add for loop or something else in python, but i want to use temporal pattern

A for loop is very much a Temporal pattern

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-10-16 12:10:53.000 UTC</summary>

(closing as it's just a question, feel free to use forums or Slack)

Reactions: ðŸ‘ 1

</details>


---

<a id="390"></a>

### #390: PoC - Static analyzer for Python

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/390 |
| **State** | OPEN |
| **Author** | josh-berry (Josh Berry) |
| **Created** | 2023-09-26 18:15:04.000 UTC (2y 3m ago) |
| **Updated** | 2024-08-21 16:12:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

*No description provided.*

#### Comments (2)

<details>
<summary><strong>dandavison</strong> commented on 2024-08-21 15:40:00.000 UTC</summary>

Ultimately, we want static analysis for Temporal SDKs to be both runnable as a standalone process and easily integratable into LSP-based IDE tooling.

One possibility here would be [mypy plugins](https://mypy.readthedocs.io/en/stable/extending_mypy.html#extending-mypy-using-plugins). However, it's not clear that mypy is going to be dominant in the future of Python type-checking: Microsoft subsequently created Pyright, which is more closely associated with and widely deployed in developer LSP tooling due to VSCode (as well as a bit faster and perhaps has some other advantages). However, my impression is that Pyright [will not](https://github.com/microsoft/pyright/issues/607#issuecomment-873467941) provide extensibility of the sort that we need.

Now, [Astral](https://astral.sh/) are [creating](https://github.com/astral-sh/ruff/pulls?q=is%3Apr+label%3Ared-knot+is%3Aclosed) a new Python type-checker. Astral's `ruff` linter and formatter has been extremely well received, and they have created LSP tools to go with it. Clearly, their type checker will be integrated into their LSP solution. (They've also [released](https://astral.sh/blog/uv-unified-python-packaging) a competitor to pip/poetry/pyenv etc).

I think that we should watch [this space](https://github.com/astral-sh/ruff/pulls?q=is%3Apr+label%3Ared-knot+is%3Aclosed). In particular to understand whether Astral are creating type-checking technology that is extensible in a way that we can make use of, and if so when it would make sense to start prototyping. 

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-08-21 16:00:04.000 UTC</summary>

Basically there are two static analyzers needed IMO:

* General purpose Python static analyzer that catches people making thread-blocking calls (directly or transitively) in an `async def` function
* Temporal-specific Python static analyzer that catches people making illegal calls (directly or transitively) in a workflow

The first one should be a general product/repo IMO, but they both need a call graph so there is an opportunity for code sharing.

I have POC'd this with MyPy plugins by the way. You can build a reasonable call graph but storing it in cache is difficult because they only allow you to set some plugin metadata at a higher level not a per function level (read https://github.com/python/mypy/blob/master/mypy/plugin.py). It can be hacked, but I think we should contribute a very small PR to MyPy to allow plugins to store a metadata dict on more specific objects.

I have researched a good bit about Python call graph builders and there are no good/fast modern ones. And I've only found MyPy to provide extensibility _and_ semantic analysis to build proper call graphs (this can't be done without static analysis). It will be great if Ruff gets this analysis, we could definitely build on it.

</details>


---

<a id="371"></a>

### #371: [Bug] mTLS doesn't support non-EC private keys

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/371 |
| **State** | CLOSED |
| **Author** | dgjustice (Daniel) |
| **Created** | 2023-08-21 23:24:36.000 UTC (2y 4m ago) |
| **Updated** | 2023-08-31 14:09:52.000 UTC |
| **Closed** | 2023-08-31 14:09:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Using a PEM file beginning with `-----BEGIN EC PRIVATE KEY-----`:

#### Traceback

```shell
RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, PrivateKeyParseError)
```

#### Temporary remedy

Convert your pkey to pkcs8:

```bash
openssl pkcs8 -topk8 -nocrypt -in key.pem -out key-pkcs8.pem
```

### Describe the bug

`python-sdk` is pointing to an older commit in `sdk-core` which depends on an [older version](https://github.com/temporalio/sdk-core/blob/492d687be48ce6ee81ef7d55c825d366c1e3c4c5/client/Cargo.toml#L28) of `tonic`.
The key issue has been fixed in [this `tonic` PR](https://github.com/hyperium/tonic/pull/1145).

### Minimal Reproduction

```python
def setup_tls_config() -> TLSConfig:
    """Helper func to build TLSConfig struct."""
    with open(os.getenv("TEMPORAL_CLI_TLS_CERT"), "rb") as f:
        client_cert = f.read()
    with open(os.getenv("TEMPORAL_CLI_TLS_KEY"), "rb") as f:
        client_private_key = f.read()
    with open(os.getenv("TEMPORAL_CLI_TLS_CA"), "rb") as f:
        server_root_ca_cert = f.read()
    return TLSConfig(
        client_cert=client_cert,
        client_private_key=client_private_key,
        domain=os.getenv("TEMPORAL_CLI_TLS_SERVER_NAME"),
        server_root_ca_cert=server_root_ca_cert,
    )
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Linux i7
- Temporal Version: `temporalio==1.3.0`
- Are you using Docker or Kubernetes or building Temporal from source? Kubernetes

### Additional context

If updating to a more recent core SDK version is fairly straightforward, feel free to throw me some pointers, and I will put together a PR.


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-08-22 19:29:59.000 UTC</summary>

:+1: We just overcame a hurdle to going past 0.8 Tonic (OTel libs), so we will be upgrading Tonic and all of these libraries before next release.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>dgjustice</strong> commented on 2023-08-22 19:46:51.000 UTC</summary>

Awesome, good to know!

</details>


---

<a id="368"></a>

### #368: [Bug] Activity completions may be dropped on worker shutdown

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/368 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-08-15 19:43:31.000 UTC (2y 4m ago) |
| **Updated** | 2023-08-24 15:41:21.000 UTC |
| **Closed** | 2023-08-24 15:41:21.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Describe the bug

The activity completion, often a cancel failure as result of worker shutdown, may not get recorded before run/shutdown returns. This becomes evident if you `sys.exit()` immediately after shutdown returns.

### Minimal Reproduction

```python
import asyncio
import logging
import signal
import sys
import os
from datetime import timedelta
from uuid import uuid4

from temporalio import activity, workflow
from temporalio.client import Client
from temporalio.common import RetryPolicy
from temporalio.worker import Worker


async def handler_stop_signals():
    shutdowns = []
    shutdowns.append(worker.shutdown())
    await asyncio.gather(*shutdowns) 
    sys.exit()

@activity.defn
async def long_activity() -> None:
    activity.logger.info(
        f"Activity waiting for cancel, attempt: {activity.info().attempt}"
    )
    await asyncio.sleep(60)


@workflow.defn
class MyWorkflow:
    @workflow.run
    async def run(self):
        await workflow.execute_activity(
            long_activity,
            start_to_close_timeout=timedelta(hours=2),
            retry_policy=RetryPolicy(maximum_attempts=1),
        )


async def main():
    logging.basicConfig(level=logging.INFO)

    logging.info("Starting worker")
    client = await Client.connect("localhost:7233")
    task_queue = f"tq-{uuid4()}"
    global worker
    worker = Worker(
        client,
        task_queue=task_queue,
        workflows=[MyWorkflow],
        activities=[long_activity],
        graceful_shutdown_timeout=timedelta(minutes=24),
    )
    loop = asyncio.get_event_loop()
    run_task = asyncio.create_task(worker.run())

    logging.info("Starting workflow")
    handle = await client.start_workflow(
        MyWorkflow.run,
        id=f"wf-{uuid4()}",
        task_queue=task_queue,
    )
    logging.info("Started workflow with ID %s" % handle.id)
    logging.info(os.getpid())

    logging.info("Waiting for SIGTERM")
    loop.add_signal_handler(signal.SIGTERM, lambda: asyncio.ensure_future(handler_stop_signals()))
    await run_task

    logging.info(
        "Worker shutdown, activity will fail, but workflow will remain until a worker is available to process failure"
    )


if __name__ == "__main__":
    asyncio.run(main())
```

Sometimes this doesn't record the activity failure sometimes it does. We need to properly finish running activities before closing activity worker (we thought we were but looking at code, running activity completion may not get recorded).


---

<a id="319"></a>

### #319: [Bug] Continue as new doesn't work without specifying `task_queue` and `workflow`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/319 |
| **State** | CLOSED |
| **Author** | nathanielobrown (Nathaniel Brown) |
| **Created** | 2023-05-22 17:01:10.000 UTC (2y 7m ago) |
| **Updated** | 2023-05-22 19:10:42.000 UTC |
| **Closed** | 2023-05-22 19:10:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Use `workflow.continue_as_new()` in a workflow without specifying `task_queue` or `workflow` and have those be taken from the current workflow. The docstring implies I can do this.

### Describe the bug

When these arguments are not passed, I get a log error like:
```
test_continue_as_new.py::test_time_skipping_past_activity_timeout 2023-05-22T16:59:17.621391Z  WARN temporal_sdk_core::worker::workflow: Network error while completing workflow activation error=status: InvalidArgument, message: "Missing TaskQueue.", details: [], metadata: MetadataMap { headers: {"content-type": "application/grpc"} }
```
and my workflow hangs

### Minimal Reproduction
```python
import asyncio

from temporalio import workflow
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import Worker

started_event = asyncio.Event()


@workflow.defn
class Workflow:
    @workflow.run
    async def run(self, continue_as_new: bool):
        if continue_as_new:
            workflow.continue_as_new(
                False,
                # If these are set, test will pass:
                # task_queue=workflow.info().task_queue,
                # workflow=Workflow.run,
            )


async def test_continue_as_new_without_workflow_or_task_queue():
    task_queue = "test"
    async with await WorkflowEnvironment.start_time_skipping() as env, Worker(
        client=env.client,
        task_queue=task_queue,
        activities=[],
        workflows=[Workflow],
    ):
        await env.client.execute_workflow(
            Workflow.run,
            True,
            task_queue=task_queue,
            id="test",
        )
```

### Environment/Versions

- OS and processor:M1 Mac
- Temporal Version: 1.2.0


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-05-22 17:53:17.000 UTC</summary>

Thanks! Hrmm, we have a test at https://github.com/temporalio/sdk-python/blob/a4224a20189c8c8078edbc66482c31860193c1b3/tests/worker/test_workflow.py#L1284-L1328 which is passing. I will have to investigate.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-22 19:10:41.000 UTC</summary>

This is a known issue with the time-skipping test server. See https://github.com/temporalio/sdk-java/issues/1424.

(closing as duplicate)

Reactions: ðŸ‘ 1

</details>


---

<a id="312"></a>

### #312: [Feature Request] Validate type of timedelta() parameters in validate methods?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/312 |
| **State** | CLOSED |
| **Author** | mbernier (Matt Bernier) |
| **Created** | 2023-04-25 16:22:00.000 UTC (2y 8m ago) |
| **Updated** | 2023-05-22 19:14:33.000 UTC |
| **Closed** | 2023-05-22 19:14:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

When writing a RetryPolicy, I got an error
```
File "/opt/homebrew/lib/python3.10/site-packages/temporalio/client.py", line 407, in start_workflow
    return await self._impl.start_workflow(
  File "/opt/homebrew/lib/python3.10/site-packages/temporalio/client.py", line 2352, in start_workflow
    input.retry_policy.apply_to_proto(req.retry_policy)
  File "/opt/homebrew/lib/python3.10/site-packages/temporalio/common.py", line 75, in apply_to_proto
    self._validate()
  File "/opt/homebrew/lib/python3.10/site-packages/temporalio/common.py", line 96, in _validate
    if self.maximum_interval.total_seconds() < 0:
```
which is due to the fact that I forgot to wrap my interval as a timedelta rather than as an integer.

### Describe the solution you'd like

In the validate methods, such as [temporalio/common:line 75](https://github.com/temporalio/sdk-python/blob/main/temporalio/common.py#L86) - we could test the type of the parameters that should be timedeltas, to make sure - then throw an error to say something similar to "You passed an integer, but timedelta was expected, see this line of the README for more information ___" 

### Additional context

This would increase the developer experience, because customers can quickly get to the correct answer due to the error informing them of what is missing. 

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-04-25 16:29:36.000 UTC</summary>

I think it's a bit unreasonable to check the types of the hundreds of parameters/fields we have at runtime. We offer type hints and strongly encourage the use of MyPy or similar. Your IDE should even show you that you are using the wrong type. This same issue exists when using JS with our TypeScript SDK. If you are choosing to not use the typing facilities, it's a bit much to have us check them all at runtime.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-05-22 19:14:23.000 UTC</summary>

Closing as not reasonable to check these parameters' types at runtime. The existing Python tooling is built to catch these before runtime.

</details>


---

<a id="299"></a>

### #299: [Feature Request] tokio update for vuln

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/299 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-03-21 13:37:48.000 UTC (2y 9m ago) |
| **Updated** | 2023-04-07 21:00:34.000 UTC |
| **Closed** | 2023-04-07 21:00:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Vuln reported at https://github.com/temporalio/sdk-python/issues/244. Can see details there. While this can be fixed in that SDK, we should probably set a minimum Tokio version in this repo to 1.25 or 1.26.

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-03-21 16:04:18.000 UTC</summary>

This can be fixed via `cargo update` in Python bridge

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-04-07 21:00:33.000 UTC</summary>

Fixed in https://github.com/temporalio/sdk-python/pull/307

</details>


---

<a id="298"></a>

### #298: [Feature Request] bzip2 update for vuln

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/298 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-03-21 13:30:09.000 UTC (2y 9m ago) |
| **Updated** | 2023-04-07 21:00:21.000 UTC |
| **Closed** | 2023-04-07 21:00:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Vuln reported at https://github.com/temporalio/sdk-python/issues/61. Ref https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-22895.


Dependency Hierarchy:
  - temporal-sdk-core-0.1.0 (Root Library)
    - zip-0.6.3.crate
      - :x: **bzip2-0.4.3.crate** (Vulnerable Library)

Note, per https://github.com/zip-rs/zip/issues/335#issuecomment-1411543375 we will have to explicitly update bzip2 ourselves

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-03-21 16:03:57.000 UTC</summary>

This can be fixed via `cargo update` in Python bridge

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-04-07 21:00:20.000 UTC</summary>

Fixed in https://github.com/temporalio/sdk-python/pull/307

</details>


---

<a id="297"></a>

### #297: [Feature Request] remove_dir_all update for vuln

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/297 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-03-21 13:26:09.000 UTC (2y 9m ago) |
| **Updated** | 2023-04-07 20:59:09.000 UTC |
| **Closed** | 2023-04-07 20:59:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Vuln reported at https://github.com/temporalio/sdk-python/issues/54. Ref https://github.com/advisories/GHSA-mc8h-8q98-g5hr.

Dependency Hierarchy:
  - temporal-sdk-core-api-0.1.0 (Root Library)
    - temporal-client-0.1.0
      - temporal-sdk-core-protos-0.1.0
        - tonic-build-0.8.2.crate
          - prost-build-0.11.1.crate
            - tempfile-3.3.0.crate
              - :x: **remove_dir_all-0.5.3.crate** (Vulnerable Library)

(may have already been fixed since last Python release, sorry if so, I did not check)

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-03-21 16:03:36.000 UTC</summary>

This can be fixed via `cargo update` in Python bridge

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-04-07 20:59:09.000 UTC</summary>

Fixed in https://github.com/temporalio/sdk-python/pull/307

</details>


---

<a id="296"></a>

### #296: [Feature Request] bumpalo update for vuln

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/296 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-03-21 13:23:48.000 UTC (2y 9m ago) |
| **Updated** | 2023-04-07 20:58:55.000 UTC |
| **Closed** | 2023-04-07 20:58:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Vuln reported at https://github.com/temporalio/sdk-python/issues/54. Ref https://rustsec.org/advisories/RUSTSEC-2022-0078.html.

Dependency Hierarchy:
  - temporal-sdk-core-api-0.1.0 (Root Library)
    - temporal-client-0.1.0
      - temporal-sdk-core-protos-0.1.0
        - tonic-0.8.2.crate
          - tokio-rustls-0.23.4.crate
            - rustls-0.20.6.crate
              - webpki-0.22.0.crate
                - ring-0.16.20.crate
                  - web-sys-0.3.60.crate
                    - wasm-bindgen-0.2.83.crate
                      - wasm-bindgen-macro-0.2.83.crate
                        - wasm-bindgen-macro-support-0.2.83.crate
                          - wasm-bindgen-backend-0.2.83.crate
                            - :x: **bumpalo-3.11.0.crate** (Vulnerable Library)

(may have already been fixed since last Python release, sorry if so, I did not check)

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-03-21 16:03:18.000 UTC</summary>

This can be fixed via `cargo update` in Python bridge

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-04-07 20:58:55.000 UTC</summary>

Fixed in https://github.com/temporalio/sdk-python/pull/307

</details>


---

<a id="288"></a>

### #288: [Bug] Enum HistoryEventFilterType has no value defined for name 'ValueType'

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/288 |
| **State** | CLOSED |
| **Author** | grizzlybearer |
| **Created** | 2023-02-24 13:23:35.000 UTC (2y 10m ago) |
| **Updated** | 2023-02-24 15:08:21.000 UTC |
| **Closed** | 2023-02-24 13:49:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Trying to run this example: https://github.com/temporalio/samples-python/blob/main/hello/hello_activity_multiprocess.py

### Describe the bug
Code sample crashes, but the workflow continues to run

Getting the following error:
```
Traceback (most recent call last):
  File "hello_activity_multiprocess.py", line 85, in <module>
    asyncio.run(main())
  File "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py", line 616, in run_until_complete
    return future.result()
  File "hello_activity_multiprocess.py", line 75, in main
    result = await client.execute_workflow(
  File "/Users/grizzlybear/.pyenvs/python3.8/lib/python3.8/site-packages/temporalio/client.py", line 556, in execute_workflow
    return await (
  File "/Users/grizzlybear/.pyenvs/python3.8/lib/python3.8/site-packages/temporalio/client.py", line 870, in result
    async for event in self._fetch_history_events_for_run(
  File "/Users/grizzlybear/.pyenvs/python3.8/lib/python3.8/site-packages/temporalio/client.py", line 1970, in __anext__
    await self.fetch_next_page()
  File "/Users/grizzlybear/.pyenvs/python3.8/lib/python3.8/site-packages/temporalio/client.py", line 1944, in fetch_next_page
    history_event_filter_type=temporalio.api.enums.v1.HistoryEventFilterType.ValueType(
  File "/Users/grizzlybear/.pyenvs/python3.8/lib/python3.8/site-packages/google/protobuf/internal/enum_type_wrapper.py", line 114, in __getattr__
    raise AttributeError('Enum {} has no value defined for name {!r}'.format(
AttributeError: Enum HistoryEventFilterType has no value defined for name 'ValueType'
```

### Minimal Reproduction

https://github.com/temporalio/samples-python/blob/main/hello/hello_activity_multiprocess.py

### Environment/Versions

- OS and processor: Mac
- Temporal Version: [e.g. 1.14.0?] and/or SDK version -> 2.10.3 (from UI)
- Are you using Docker or Kubernetes or building Temporal from source? Docker

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-02-24 13:39:07.000 UTC</summary>

Which protobuf version? This was added in 3.20+: https://github.com/protocolbuffers/protobuf/commit/0707f2e7f556c8396d6027d0533ec3a56d1061db. Make sure you are using at least the minimum version we require.



</details>

<details>
<summary><strong>grizzlybearer</strong> commented on 2023-02-24 13:48:59.000 UTC</summary>

That was it. I upgraded to version 3.20.0 and that resolved the issue. Thanks for the quick turnaround!
I'm not sure how I was able to use temporal without it meeting this requirement.

</details>


---

<a id="277"></a>

### #277: types-protobuf dependency is still on 3.x, while package supports protobuf 4.x

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/277 |
| **State** | CLOSED |
| **Author** | schibrikov (Eugeny Schibrikov) |
| **Created** | 2023-02-15 14:41:51.000 UTC (2y 10m ago) |
| **Updated** | 2023-02-15 15:58:41.000 UTC |
| **Closed** | 2023-02-15 15:58:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi, the title says it all.
The problem with types-protobuf 3.x for me is this issue https://github.com/python/typeshed/issues/5800.
In short: using types-protobuf 3.x overrides google cloud SDK typings, so I want to upgrade.

Can we change 
```
protobuf = ">=3.20"
types-protobuf = "~3.20"
```
to
```
protobuf = ">=3.20"
types-protobuf = ">=3.20"
```
?


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-02-15 14:43:49.000 UTC</summary>

Definitely can change that. I should have done so when backporting this library to work with proto 3.x library. Feel free to PR or wait a bit and I'll get to it. Thanks for the report!

</details>

<details>
<summary><strong>schibrikov</strong> commented on 2023-02-15 14:45:04.000 UTC</summary>

Good, I guess I'll try to make a PR

</details>


---

<a id="275"></a>

### #275: [Feature Request] Access to invoking workflow in activity method

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/275 |
| **State** | CLOSED |
| **Author** | bbkgh (Babak Ghadiri) |
| **Created** | 2023-02-14 11:06:14.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-17 14:20:32.000 UTC |
| **Closed** | 2023-02-17 14:20:32.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi. Is there any way to access invoking workflow in activity?
sth like:
```
    @activity.defn
    async def compose_greeting() -> str:
        print(workflow.name)
```

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-02-14 12:40:49.000 UTC</summary>

You can call [temporalio.activity.info()](https://python.temporal.io/temporalio.activity.html#info) which returns an [Info](https://python.temporal.io/temporalio.activity.Info.html) class containing information about the activity including workflow information.

If you are unsure how to access this or other information, feel free to use our forums or public Slack.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-17 14:20:32.000 UTC</summary>

Closing issue as it was just a question

</details>


---

<a id="264"></a>

### #264: [Feature Request] Support JSON Type Converters

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/264 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-02-01 13:22:55.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-08 20:14:44.000 UTC |
| **Closed** | 2023-02-08 20:14:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

People need to customize `temporal.converters.value_to_type` for use with their types during JSON conversion

### Describe the solution you'd like

Add a `JSONTypeConverter` ABC that converts to and from a user type to JSON-able type. Change `JSONPlainPayloadConverter` to accept a collection of these and make that easier to set in construction of default payload converter.

#### Comments (2)

<details>
<summary><strong>michael-cybrid</strong> commented on 2023-02-03 20:24:03.000 UTC</summary>

Big ðŸ‘ from us on this.

We have conversion working  [OpenAPI generated models](https://github.com/OpenAPITools/openapi-generator) working by inheriting from `JSONPlainPayloadConverter` and doing something like the following but it has limitations (around optional and collections, etc.) and first party support would be awesome:

```python
 def from_payload(
        self,
        payload: Payload,
        type_hint: Type[Any] | None = None,
    ) -> Any:
        """See base class."""
        try:
            obj = json.loads(payload.data, cls=self._decoder)
            if type_hint:
                if service_one.is_openapi_model_subclass(type_hint):
                    obj = service_one.dict_to_model(obj, type_hint)

                elif service_two.is_openapi_model_subclass(type_hint):
                    obj = service_two.dict_to_model(obj, type_hint)

                else:
                    obj = value_to_type(type_hint, obj)
            return obj
        except json.JSONDecodeError as err:  # pragma: no cover
            raise RuntimeError("Failed parsing") from err
```


</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-08 16:33:50.000 UTC</summary>

> it has limitations (around optional and collections, etc.) 

@michael-cybrid - I think for custom input/output types it can be clearer to use your own encoding payload converter with your own encoding (e.g. `json/open-api`) instead of reusing the existing encoding. This makes it more portable/obvious to other Temporal clients/users that custom logic is needed. Granted I understand this doesn't help with collections and such, but usually you know which types you need to support for your own conversion.

Regardless, I have just opened a PR for JSON type converters and README updates that can help your use case if reusing existing JSON conversion logic.

</details>


---

<a id="259"></a>

### #259: [Feature Request] Workflow Versioning Documentation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/259 |
| **State** | CLOSED |
| **Author** | bbkgh (Babak Ghadiri) |
| **Created** | 2023-01-20 13:56:14.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-07 13:52:40.000 UTC |
| **Closed** | 2023-02-07 13:52:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi. Is there any way to set/get version of Workflow in python-sdk?
I can contribute to implement this feature if you give me some hints for changing  code sections.
https://docs.temporal.io/workflows#workflow-versioning

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-01-23 13:33:23.000 UTC</summary>

Versioning is already supported in Python via the https://python.temporal.io/temporalio.workflow.html#patched call in mostly the same way as TypeScript. See https://legacy-documentation-sdks.temporal.io/typescript/patching.

We are still working on our documentation in this area. I will update the issue title.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-07 13:52:39.000 UTC</summary>

Closing in favor of https://github.com/temporalio/documentation/issues/1925

</details>


---

<a id="245"></a>

### #245: [Bug] BrokenProcessPool completely disables ProcessPoolExecutor 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/245 |
| **State** | CLOSED |
| **Author** | wilsoniya (Michael Wilson) |
| **Created** | 2023-01-06 18:50:57.000 UTC (3 years ago) |
| **Updated** | 2023-01-13 18:28:26.000 UTC |
| **Closed** | 2023-01-13 18:28:26.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm trying use the multiprocessing/other activity execution scheme to run synchronous activities and be able to recover from undesirable but inevitable operational problems (e.g., segfault, oomkill) which result in the termination of worker child processes.

### Describe the bug

When using a [multiprocessing/other](https://github.com/temporalio/samples-python/blob/e8e8ee60900791348a2eca160824d3173395310f/hello/hello_activity_multiprocess.py)-type activity executor, termination of worker processes comprising the `ProcessPoolExecutor` apparently renders the worker unable to service further activity or workflow execution. Stack traces indicate a `BrokenProcessPool`, e.g.: 

```
Completing activity as failed ({'activity_id': '1', 'activity_type': 'compose_greeting', 'attempt': 1, 'namespace': 'default', 'task_queue': 'hello-activity-multiprocess-task-queue', 'workflow_id': 'hello-activity-multiprocess-workflow-id', 'workflow_run_id': 'a0d0cf4e-3cb4-4b9d-98a8-98d4ee67ccbf', 'workflow_type': 'GreetingWorkflow'})
Traceback (most recent call last):
  File "/home/wilsoniya/ia/devel/github.com/temporalio/samples-python/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py", line 402, in _run_activity
    result = await impl.execute_activity(input)
  File "/home/wilsoniya/ia/devel/github.com/temporalio/samples-python/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py", line 638, in execute_activity
    return await loop.run_in_executor(
concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.
Completing activity as failed ({'activity_id': '1', 'activity_type': 'compose_greeting', 'attempt': 2, 'namespace': 'default', 'task_queue': 'hello-activity-multiprocess-task-queue', 'workflow_id': 'hello-activity-multiprocess-workflow-id', 'workflow_run_id': 'a0d0cf4e-3cb4-4b9d-98a8-98d4ee67ccbf', 'workflow_type': 'GreetingWorkflow'})
Traceback (most recent call last):
  File "/home/wilsoniya/ia/devel/github.com/temporalio/samples-python/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py", line 402, in _run_activity
    result = await impl.execute_activity(input)
  File "/home/wilsoniya/ia/devel/github.com/temporalio/samples-python/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py", line 638, in execute_activity
    return await loop.run_in_executor(
  File "/home/wilsoniya/ia/devel/github.com/temporalio/samples-python/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py", line 638, in execute_activity
    return await loop.run_in_executor(
...
```

At the minimum, the top-level worker process should be terminated to allow for the service/pod to be restarted (e.g., by systemd or k8s), though ideally the temporal `Worker` could cope with a `ProcessPoolExecutor` that has entered a bad state.

### Minimal Reproduction


1. clone samples-python
2. increase the number of iterations in `compose_greeting` to allow time to kill a worker process
```
diff --git a/hello/hello_activity_multiprocess.py b/hello/hello_activity_multiprocess.py
index 49395c3..ee0e7b7 100644
--- a/hello/hello_activity_multiprocess.py
+++ b/hello/hello_activity_multiprocess.py
@@ -24,7 +24,7 @@ class ComposeGreetingInput:
 def compose_greeting(input: ComposeGreetingInput) -> str:
     # We'll wait for 3 seconds, heartbeating in between (like all long-running
     # activities should do), then return the greeting
-    for _ in range(0, 3):
+    for _ in range(0, 10000):
         print(f"Heartbeating activity on PID {os.getpid()}")
         activity.heartbeat()
         time.sleep(1)
```
3. `poetry run python hello/hello_activity_multiprocess.py`
4. discover pid of running worker process (it's helpful to use a nested view e.g., as provided by htop, so as to select a process at the deepest level of nesting under the top `hello_activity_multiprocess.py` python process)
5. kill worker process
6. 
```
temporalio/samples-python â€¹main*â€º Â» poetry run python hello/hello_activity_multiprocess.py                                                                                                       
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652971
Heartbeating activity on PID 1652969
Heartbeating activity on PID 1652971
Heartbeating activity on PID 1652969
Completing activity as failed ({'activity_id': '1', 'activity_type': 'compose_greeting', 'attempt': 1, 'namespace': 'default', 'task_queue': 'hello-activity-multiprocess-task-queue', 'workflow_id': 'hello-activity-multiprocess-workflow-id', 'workflow_run_id': 'a0d0cf4e-3cb4-4b9d-98a8-98d4ee67ccbf', 'workflow_type': 'GreetingWorkflow'})
Traceback (most recent call last):
  File "/home/wilsoniya/ia/devel/github.com/temporalio/samples-python/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py", line 402, in _run_activity
    result = await impl.execute_activity(input)
  File "/home/wilsoniya/ia/devel/github.com/temporalio/samples-python/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py", line 638, in execute_activity
    return await loop.run_in_executor(
concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.
Completing activity as failed ({'activity_id': '1', 'activity_type': 'compose_greeting', 'attempt': 2, 'namespace': 'default', 'task_queue': 'hello-activity-multiprocess-task-queue', 'workflow_id': 'hello-activity-multiprocess-workflow-id', 'workflow_run_id': 'a0d0cf4e-3cb4-4b9d-98a8-98d4ee67ccbf', 'workflow_type': 'GreetingWorkflow'})
Traceback (most recent call last):
  File "/home/wilsoniya/ia/devel/github.com/temporalio/samples-python/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py", line 402, in _run_activity
    result = await impl.execute_activity(input)
  File "/home/wilsoniya/ia/devel/github.com/temporalio/samples-python/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py", line 638, in execute_activity
    return await loop.run_in_executor(
  File "/home/wilsoniya/ia/devel/github.com/temporalio/samples-python/venv/lib/python3.8/site-packages/temporalio/worker/_activity.py", line 638, in execute_activity
    return await loop.run_in_executor(
concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.

# ... apparently forever
```

### Environment/Versions

- OS and processor:
```
uname -a                                                                                                                                                                                                                                                                                                                                                                           
Linux 6.1.1-arch1-1 #1 SMP PREEMPT_DYNAMIC Wed, 21 Dec 2022 22:27:55 +0000 x86_64 GNU/Linux

lscpu
...
  Model name:            AMD Ryzen 7 3700X 8-Core Processor
```
- sdk-python version
```
python --version                                                                                                                                                                                                                                                                                                                                                                   
Python 3.8.12

pip freeze | grep temporalio
temporalio==1.0.0
-e git+ssh://git@github.com/temporalio/samples-python.git@af94df067f2d0afc5fd4f2433e83ed0ad85467f5#egg=temporalio_samples
# and also
pip freeze | grep temporalio                                                                                                                                                                                                                                                                                                                                                                                       
temporalio==0.1b3
```

- Are you using Docker or Kubernetes or building Temporal from source?
I'm using a docker distribution for local development but I don't think that should affect the reproduction of this bug.

### Additional context

Thanks for the help and congrats on reaching 1.0.0 on sdk-python :tada: !


#### Comments (2)

<details>
<summary><strong>wilsoniya</strong> commented on 2023-01-09 17:36:25.000 UTC</summary>

~Looks like this `ProcessPoolExecutor` implementation might be a workaround: https://github.com/joblib/loky~

EDIT: I tested this out and it can't cope with killed worker procs.

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-01-09 18:08:54.000 UTC</summary>

Did some research. I think it's pretty clear we should fatally error the worker if we get any `concurrent.futures.BrokenExecutor` from `run_in_executor`. Thanks for the report!

Reactions: ðŸ‘ 1

</details>


---

<a id="242"></a>

### #242: [Feature Request] Support dynamic workflows, activities, signals, and queries

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/242 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-01-05 13:50:38.000 UTC (3 years ago) |
| **Updated** | 2023-07-14 20:27:57.000 UTC |
| **Closed** | 2023-07-14 20:27:57.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

Need opt-in catch-alls for workflows, activities, signals, and queries.

### Describe the solution you'd like

Should be able to register only one workflow as "dynamic" and it is given the name as a string as the first arg, then `*args: Payload` as second arg. This means we also need to provide the users of a workflow access to their payload converter to convert those.

Need to do the same for activity.

For signal and query handlers, we already have `dynamic=True`, but we foolishly allow already-decoded args. Ideally, we would change to require payloads. But since that may be a breaking change, we should just args hinted as payloads pass through unconverted.


---

<a id="223"></a>

### #223: [Bug] Deprecation Warning During Install

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/223 |
| **State** | CLOSED |
| **Author** | mbernier (Matt Bernier) |
| **Created** | 2022-12-07 18:24:11.000 UTC (3y 1m ago) |
| **Updated** | 2023-02-07 13:44:38.000 UTC |
| **Closed** | 2023-02-07 13:44:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

`pip3 install temporalio`

### Describe the bug

Received Deprecation warning:
`DEPRECATION: grpcio is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559`

### Minimal Reproduction

On a machine without temporalio installed via pip, run: `pip3 install temporalio`

### Environment/Versions

Brand new MBP
Homebrew install of python3 which included pip3

Edit: Pip3 installed these versions of the libs:
`Successfully installed grpcio-1.51.1 protobuf-4.21.10 temporalio-0.1b4 types-protobuf-3.20.4.6 typing-extensions-4.4.0`

requirements.txt file:
```
grpcio==1.51.1
protobuf==4.21.10
temporalio==0.1b4
types-protobuf==3.20.4.6
typing_extensions==4.4.0

```

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2022-12-07 18:46:37.000 UTC</summary>

What version of Python and PIP are you using? Can you try with the latest and confirm whether this still exists? Are you also getting a pip-out-of-date warning? I am unable to replicate in my environment using latest pip, but it may be because I _do_ have `wheel` installed.

(semi-relatedly, `grpcio` is supposed to be an optional dependency, I have opened #224 to fix that)

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-02-07 13:44:37.000 UTC</summary>

grpcio is no longer installed by default

</details>


---

<a id="216"></a>

### #216: [Bug] Importing gevent causes sandbox errors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/216 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-29 20:20:40.000 UTC (3y 1m ago) |
| **Updated** | 2022-12-02 21:13:04.000 UTC |
| **Closed** | 2022-12-02 21:13:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

```python
from gevent.os import fork
```

Seems that causes the sandbox to fail



#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2022-12-01 17:30:47.000 UTC</summary>

This is happening due to https://github.com/gevent/gevent/blob/1e412d35526183b26c1abf2eb658cbef661f5f70/src/gevent/os.py#L539 where https://github.com/gevent/gevent/blob/1e412d35526183b26c1abf2eb658cbef661f5f70/src/gevent/_util.py#L90 is accessing `__dict__.items()` of the module

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-12-01 17:58:00.000 UTC</summary>

This is caused by `dict.items(our_restricted_proxy_wrapper_of_a_dict)` which raises:

> TypeError: descriptor 'items' for 'dict' objects doesn't apply to a '_RestrictedProxy' object

This is checked in C code and we can't fix it. Will suggest work arounds.

</details>


---

<a id="200"></a>

### #200: [Bug] Identity equality for int enum fails inside sandbox

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/200 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-14 23:24:19.000 UTC (3y 1m ago) |
| **Updated** | 2022-11-15 22:00:14.000 UTC |
| **Closed** | 2022-11-15 22:00:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

When a `is` check is done on a type like `IntEnum` that comes as a workflow arg, it fails because the arg conversion uses the type from the definition type hint which is technically different than the type _inside_ the sandbox. This may also occur on `isinstance` for data classes, need to check.

Not exactly sure what to do here. Maybe re-reference the full type's qualified name when converting instead of reusing the type hint itself?

#### Comments (2)

<details>
<summary><strong>bergundy</strong> commented on 2022-11-14 23:29:37.000 UTC</summary>

We've had similar issues in TS when passing around object between v8 contexts.
Our solution was to instantiate everything in the proper context but I understand that's not feasible with the Python sandboxing approach.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-14 23:40:44.000 UTC</summary>

It might be feasible. I have the qualified name, so I can re-reference it. But really, I should use the type hints inside the sandbox, not from the definition. Will have to see.

</details>


---

<a id="195"></a>

### #195: [Feature Request] Iterable Intermediate Results

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/195 |
| **State** | CLOSED |
| **Author** | mikedh (Michael Dawson-Haggerty) |
| **Created** | 2022-11-11 20:43:32.000 UTC (3y 1m ago) |
| **Updated** | 2022-11-14 17:28:07.000 UTC |
| **Closed** | 2022-11-14 17:28:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Thanks for the great work on this project! I'm not sure if this is possible using temporal, but if it is not precluded by design it would be nice to have intermediate results from long-running iterable functions. I think this has a number of applications, i.e. something like someone uploads a zip file of images and a processing engine iterates through them generating thumbnails. 

I tried a whole bunch of ways (unsuccessfully), using both the latest wheels on `pip` and bleeding edge ones from an actions build (`temporalio-0.1b2-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl`, post-#175). I was able to get it to work just fine returning `list` after completion, but was not able to have intermediate results returned. Happy to PR an example if this is at all possible!

### Describe the solution you'd like

It would be nice to have a few examples in the repo, like `examples/iterable/serve.py`. Here's a working snippet based off the one in the readme: 

`serve.py`:
```
import asyncio
from datetime import timedelta
from temporalio import workflow, activity
from temporalio.client import Client
from temporalio.worker import Worker

import time
from typing import AsyncIterator


@activity.defn
async def say_hello(name: str) -> AsyncIterator[str]:
    for letter in name:
        # some long-running task, probably synchronous
        time.sleep(1.0)
        # this example works with a "return" but it would
        # be nice to be able to do:
        # `yield letter`
        return letter


@workflow.defn
class SayHello:
    @workflow.run
    async def run(self, name: str) -> str:
        return await workflow.execute_activity(
            say_hello,
            name,
            schedule_to_close_timeout=timedelta(seconds=5))


async def main():
    # Create client connected to server at the given address
    client = await Client.connect("localhost:7233")

    # Run the worker
    worker = Worker(
        client,
        task_queue="my-task-queue",
        workflows=[SayHello],
        activities=[say_hello])
    await worker.run()

if __name__ == "__main__":
    asyncio.run(main())
```

`run.py`:
```
import asyncio
from uuid import uuid4
from temporalio.client import Client

# Import the workflow from the previous code
from serve import SayHello


async def main():
    # Create client connected to server at the given address
    client = await Client.connect("localhost:7233")

    result = await client.execute_workflow(
        SayHello.run,
        "my name",
        task_queue='my-task-queue',
        id=str(uuid4()))
    print(f"Result: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2022-11-14 13:38:51.000 UTC</summary>

> I was able to get it to work just fine returning list after completion, but was not able to have intermediate results returned.

The way Temporal works for activities is to run the activity and serialize its results back to the workflow. There is no concept of "intermediate results" in Temporal I am afraid. You'll have to rework your code if you need something different.

If you must have a long-running activity communicate with the workflow before the activity is complete, some people resort to signalling back to the calling workflow from the activity using a Temporal client (can make the Temporal client a member of a class and the activity a method on the class and use start/execute_activity_method). Don't forget to set a heartbeat timeout and perform heartbeats on non-immediate activities.

</details>

<details>
<summary><strong>mikedh</strong> commented on 2022-11-14 17:28:07.000 UTC</summary>

> signalling back to the calling workflow 

Thanks for the suggestion! I'll give that a try.

</details>


---

<a id="162"></a>

### #162: [Bug] Wrap query RPC error with friendlier error

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/162 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-20 18:08:19.000 UTC (3y 2m ago) |
| **Updated** | 2022-11-07 14:52:21.000 UTC |
| **Closed** | 2022-11-07 14:52:21.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Describe the bug

In other SDKs, when a non-existent query is invoked a "query exception" in that lang is thrown. But in Python we just throw the RPC back out. Wrap that with a more clear error to the user.


---

<a id="157"></a>

### #157: [Feature Request] Integration test for Otel gRPC metrics collector

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/157 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-13 22:00:46.000 UTC (3y 2m ago) |
| **Updated** | 2022-10-27 18:33:18.000 UTC |
| **Closed** | 2022-10-27 18:32:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Confirm the Otel gRPC URL format (and if possible whether it works over TLS) via integration test

#### Comments (2)

<details>
<summary><strong>nrhodes</strong> commented on 2022-10-14 12:46:00.000 UTC</summary>

Copied from the [slack thread](https://temporalio.slack.com/archives/CTT84RS0P/p1665750534907139?thread_ts=1665697686.033889&cid=CTT84RS0P):
I modified the `hello_activity.py` sample in the `sample-python` repo to add a call to `init_telemetry`, specifying a localhost endpoint as follows:
```
async def main():
    # Uncomment the line below to see logging
    #logging.basicConfig(level=logging.DEBUG)
    open_telemetry_config = OtelCollectorConfig(
            url="localhost:9999", headers={
            }
        )
    init_telemetry(
        TelemetryConfig(
        otel_metrics=open_telemetry_config,
        otel_tracing=open_telemetry_config,
        )
    )
    # Start client
    client = await Client.connect("localhost:7233")
    # Run a worker for the workflow
    async with Worker(
        client,
        task_queue="hello-activity-task-queue",
        workflows=[GreetingWorkflow],
        activities=[compose_greeting],
    ):
        # While the worker is running, use the client to run the workflow and
        # print out its result. Note, in many production setups, the client
        # would be in a completely separate process from the worker.
        result = await client.execute_workflow(
            GreetingWorkflow.run,
            "World",
            id="hello-activity-workflow-id",
            task_queue="hello-activity-task-queue",
        )
        print(f"Result: {result}")

```

I then checked with Wireshark and saw that there were no connections being made to port 9999 on the loopback interface. There were connections to the temporal server at 7233, as Iâ€™d expect.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-10-27 18:32:47.000 UTC</summary>

There's not a good way to put an integration test for the collector side in the primary SDK repo, so I added it to the PR at https://github.com/temporalio/samples-python/pull/18.

The issue here I think is that you have to use a real URL like `http://localhost:9999`.

(closing as not completing because I won't do an integration test, but can reopen if there's a bug or can discuss on the sample)

</details>


---

<a id="150"></a>

### #150: [Bug] Potential issue with parallel activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/150 |
| **State** | CLOSED |
| **Author** | nediamond |
| **Created** | 2022-10-11 18:06:44.000 UTC (3y 3m ago) |
| **Updated** | 2022-10-11 18:34:31.000 UTC |
| **Closed** | 2022-10-11 18:34:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I am trying to execute activities in parallel with retries disabled, following the pattern from [this example](https://github.com/temporalio/samples-python/blob/main/hello/hello_parallel_activity.py)

When one activity execution fails, I would expect that successful activity completions are still saved to workflow history

### Describe the bug

I am observing that when an activity from the `asyncio.gather(` fails, it then fails the workflow & causes the remaining activity results to be dropped / not saved to workflow history.

It also gives this warning on subsequent activity completion after the initial failure:
```
WARN temporal_sdk_core::worker::activities: Activity not found on completion. This may happen if the activity has already been cancelled but completed anyway., task_token: TaskToken(CiQwNmI4MTAyZC02NzUzLTQxODctYjI3My1jNDVkMzVhOGIxMGESEXdlaWJvLXByb2ZpbGUtNTk3GiQ5ODdlMjAxMy0yNGYwLTRiMmMtOGFhNi1lODUyMjIxMjE5NGMgEygBMgE1Qgxwcm9jZXNzX2l0ZW1KCAgBEMWAQBgB), details: Status { code: NotFound, message: "workflow execution already completed", details: b"\x08\x05\x12$workflow execution already completed\x1aB\n@type.googleapis.com/temporal.api.errordetails.v1.NotFoundFailure", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
```

In Temporal UI, this presents as having both `ActivityTaskFailed` and `WorkflowExecutionFailed` in the event history but many pending activities are still listed indefinitely at the top.

### Minimal Reproduction
Based on https://github.com/temporalio/samples-python/blob/main/hello/hello_parallel_activity.py
Disabled retries, increased activity duration, added failing activity
```
import asyncio
from datetime import timedelta
from typing import List

from temporalio import activity, workflow
from temporalio.client import Client
from temporalio.common import RetryPolicy
from temporalio.worker import Worker


NO_RETRY = RetryPolicy(maximum_attempts=1)


@activity.defn
async def say_hello_activity(name: str) -> str:
    await asyncio.sleep(20)
    return f"Hello, {name}!"


@activity.defn
async def bad_activity() -> str:
    return 1/0


@workflow.defn
class SayHelloWorkflow:
    @workflow.run
    async def run(self) -> List[str]:
        # Run 5 activities at the same time
        results = await asyncio.gather(
            workflow.execute_activity(
                bad_activity, retry_policy=NO_RETRY, start_to_close_timeout=timedelta(minutes=20)
            ),
            workflow.execute_activity(
                say_hello_activity, "user2", retry_policy=NO_RETRY, start_to_close_timeout=timedelta(minutes=20)
            ),
            workflow.execute_activity(
                say_hello_activity, "user3", retry_policy=NO_RETRY, start_to_close_timeout=timedelta(minutes=20)
            ),
            workflow.execute_activity(
                say_hello_activity, "user4", retry_policy=NO_RETRY, start_to_close_timeout=timedelta(minutes=20)
            ),
            workflow.execute_activity(
                say_hello_activity, "user5", retry_policy=NO_RETRY, start_to_close_timeout=timedelta(minutes=20)
            ),
        )
        # Sort the results because they can complete in any order
        return list(sorted(results))


async def main():
    # Start client
    client = await Client.connect("temporal:7233")

    # Run a worker for the workflow
    async with Worker(
        client,
        task_queue="hello-parallel-activity-task-queue",
        workflows=[SayHelloWorkflow],
        activities=[say_hello_activity, bad_activity],
    ):

        # While the worker is running, use the client to run the workflow and
        # print out its result. Note, in many production setups, the client
        # would be in a completely separate process from the worker.
        result = await client.execute_workflow(
            SayHelloWorkflow.run,
            id="hello-parallel-activity-workflow-id",
            task_queue="hello-parallel-activity-task-queue",
        )
        print(f"Result: {result}")


if __name__ == "__main__":
    asyncio.run(main())

```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- i9 Mac
- Temporal Version: docker image `temporalio/auto-setup:1.18.0` 
- Running using docker compose

### Additional context

- Noticed when doing the minimal reproduction that activity latency is a significant factor


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2022-10-11 18:25:16.000 UTC</summary>

> When one activity execution fails, I would expect that successful activity completions are still saved to workflow history

When a workflow completes, all activities are cancelled. This is Temporal server behavior. The workflow is complete when an exception is thrown out of it. I think that WARN may be ignored, but I will check with that team (and we'll lower the log level if so).

From https://docs.python.org/3/library/asyncio-task.html#asyncio.gather:

> If return_exceptions is False (default), the first raised exception is immediately propagated to the task that awaits on gather().

This means in your code, while all 5 activities may start, `bad_activity` will raise, causing `gather` to raise, causing the workflow to complete with a failure, causing all other activities and anything outstanding on the workflow to be "cancelled".

Let me know if I am misunderstanding the concern.

Reactions: ðŸ‘ 1

</details>

<details>
<summary><strong>nediamond</strong> commented on 2022-10-11 18:34:25.000 UTC</summary>

>>If return_exceptions is False (default), the first raised exception is immediately propagated to the task that awaits on gather().

>This means in your code, while all 5 activities may start, bad_activity will raise, causing gather to raise, causing the workflow to complete with a failure, causing all other activities and anything outstanding on the workflow to be "cancelled".

Ah I tried that option earlier but think I ruled it out quickly out of confusion - definitely seems appropriate. Thanks!



</details>


---

<a id="146"></a>

### #146: Clarify story around sync activity cancellation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/146 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-30 15:45:30.000 UTC (3y 3m ago) |
| **Updated** | 2022-12-01 14:48:25.000 UTC |
| **Closed** | 2022-12-01 14:48:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Our tests only capture cancel on sync activities, they don't prove that cancel can bubble out

### Describe the solution you'd like

Make sure there is a good sync way to bubble out a cancel

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2022-11-07 16:09:16.000 UTC</summary>

Reopening. We still want to discuss possible alternatives here to sync cancellation. Requiring the user to throw a `CancelledError` may not be the friendliest. Maybe we want https://docs.python.org/3/c-api/init.html#c.PyThreadState_SetAsyncExc?

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-07 19:34:58.000 UTC</summary>

After team discussion, we're going to:

* Use https://docs.python.org/3/c-api/init.html#c.PyThreadState_SetAsyncExc by default (Rust FFI at https://docs.rs/pyo3/0.17.3/pyo3/ffi/fn.PyThreadState_SetAsyncExc.html)
* Allow you to disable this in the decorator (e.g. `@activity.defn(no_thread_exception=True)`)
* Allow to you shield code from having this thrown within it (e.g. `with activity.no_thread_exception():`) that will throw immediately upon completion (of the last one in the stack if nested)

</details>


---

<a id="141"></a>

### #141: [Feature Request] Support Otel temporality config

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/141 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-22 19:34:28.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-07 14:52:22.000 UTC |
| **Closed** | 2022-11-07 14:52:22.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Describe the solution you'd like

Expose temporality configuration from https://github.com/temporalio/sdk-core/pull/402



---

<a id="129"></a>

### #129: [Bug] conditions are triggered when processing queries and patch notifications

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/129 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2022-09-07 23:24:06.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-19 13:00:20.000 UTC |
| **Closed** | 2022-09-19 13:00:20.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

We recently fixed this for TypeScript, Python should do the same.

Otherwise a `patched` statement in a `wait_condition` function might generate commands when it shouldn't.


---

<a id="123"></a>

### #123: Update Python proto library to latest

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/123 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-01 17:51:27.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-19 15:28:38.000 UTC |
| **Closed** | 2022-09-19 15:28:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Currently we're on 3.x because `grpcio` package which we support as optional requires 3.x. But Pulumi seems to allow both: https://github.com/pulumi/pulumi/blob/master/sdk/python/requirements.txt. See if we can too. There is also (intentionally unlinked) PR 30382 on https://github.com/grpc/grpc that may let us upgrade.

#### Comments (2)

<details>
<summary><strong>brightsparc</strong> commented on 2022-09-12 22:53:48.000 UTC</summary>

Note the gRPC version that this library currently targets is `^1.48.0` however there is a [known memory leak](https://github.com/grpc/grpc/issues/28513) for `>=1.43.0` so suggest until this has been resolved pinning the gRPC dependency to `1.41.0` which is know to be stable. 

</details>

<details>
<summary><strong>bergundy</strong> commented on 2022-09-12 23:05:22.000 UTC</summary>

We don't invoke grpc directly from Python, it all goes through Rust Core using tonic so I this memory leak probably doesn't apply to this SDK.

</details>


---

<a id="109"></a>

### #109: Support timedelta | float | int for all user-facing duration input

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/109 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-18 19:03:26.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-06 15:33:07.000 UTC |
| **Closed** | 2022-09-06 15:30:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Some input requires `timedelta`, but Python devs are used to just passing in a float of seconds. Right now only on `wait_condition` do we accept floats, everywhere else we require timedeltas.

### Describe the solution you'd like

Everywhere that we accept duration input we should accept a timedelta or a float/int of seconds. But keep timedeltas as the way we show durations.

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2022-09-06 15:30:49.000 UTC</summary>

This is really changing a lot of stuff. I am not sure this is ok, In addition to confusion of `Union[timedelta, float]` everywhere (represented as `temporalio.common.Duration: TypeAlias = Union[timedelta, float]`), some bidirectional values in places like `RetryPolicy` will have this which means I will be asking everyone to call either `temporalio.common.duration_to_timedelta()` or `temporalio.common.duration_to_total_seconds()` which is a bit unwieldy. It's clearer leaving these all as one type everywhere.

Closing as won't fix.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-09-06 15:33:07.000 UTC</summary>

But I will be changing everywhere we accept a float to match Python API to also take a timedelta for clarity.

</details>


---

<a id="100"></a>

### #100: Linux aarch64 wheel

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/100 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-08 12:51:42.000 UTC (3y 5m ago) |
| **Updated** | 2022-10-31 20:12:47.000 UTC |
| **Closed** | 2022-10-31 20:12:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

The Linux aarch64 wheel on the builds needs to be tested and confirm it works. Unfortunately it takes a really long time to build in CI (disabled except on main merge).

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2022-08-17 11:24:27.000 UTC</summary>

Also, we can't run `protoc` in this environment because https://pypi.org/project/protoc-wheel-0 does not have a Linux aarch64 wheel. We need to make sure we don't try to install it if possible.

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-10-25 16:59:14.000 UTC</summary>

Can now set `runs-on: buildjet-4vcpu-ubuntu-2204-arm` since we now have a runner

</details>


---

<a id="87"></a>

### #87: High level API for schedules

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/87 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2022-07-30 00:22:17.000 UTC (3y 5m ago) |
| **Updated** | 2023-02-17 19:10:35.000 UTC |
| **Closed** | 2023-02-17 19:10:34.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | None |
| **Assignees** | cretz |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

*No description provided.*


---

<a id="36"></a>

### #36: Publish to pip

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/36 |
| **State** | CLOSED |
| **Author** | vincent-lecrubier-skydio (Vincent Lecrubier) |
| **Created** | 2022-06-02 21:52:09.000 UTC (3y 7m ago) |
| **Updated** | 2022-07-05 13:57:34.000 UTC |
| **Closed** | 2022-07-05 13:57:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi, congrats on the progress, this is amazing. I've seen you published this package to pypi once in the past. Would you mind publishing it once more ? We need it currently in my team to experiment, but having to build everything from scratch is not ideal to integrate in our main repo.

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2022-06-02 21:56:18.000 UTC</summary>

Yes, I have or two more issues to solve, then I'll publish.

Reactions: ðŸŽ‰ 2

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-07-05 13:57:34.000 UTC</summary>

This has been published

Reactions: ðŸŽ‰ 1

</details>


---

<a id="23"></a>

### #23: Stress tests

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/23 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-05-26 13:14:18.000 UTC (3y 7m ago) |
| **Updated** | 2022-11-01 22:32:35.000 UTC |
| **Closed** | 2022-11-01 22:32:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | beta-3 |

#### Description

### Is your feature request related to a problem? Please describe.

We need to know reasonable default maximums and resource utilization of heavy workflow use

### Describe the solution you'd like

Make a repeatable and parameterized stress test. While it should run in CI to confirm it always works, the parameters for CI should be much lower than one might want when actually stressing a system.

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2022-05-31 22:37:23.000 UTC</summary>

On hold pending https://github.com/temporalio/sdk-core/issues/336 so we don't have to rewrite this for each SDK

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-10-10 18:37:07.000 UTC</summary>

Try to use https://github.com/temporalio/xk6-temporal instead.

</details>


---

<a id="9"></a>

### #9: Support macOS arm64

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/9 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-03-15 19:56:19.000 UTC (3y 10m ago) |
| **Updated** | 2022-07-01 15:26:04.000 UTC |
| **Closed** | 2022-07-01 15:26:04.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

Need a prebuilt macOS aarch64 wheel. Don't currently have it because cross-compilation is a bit of a pain with pyo3 and we may get a mac m1 runner soon.

### Describe the solution you'd like

Update CI build build and test on this platform


---

<a id="1238"></a>

### #1238: [Bug] `create_schedule` doesn't use static user metadata

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1238 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-12-04 20:36:00.000 UTC (1 months ago) |
| **Updated** | 2025-12-10 00:14:42.000 UTC |
| **Closed** | 2025-12-10 00:14:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

```
async def create_schedule(
    self,
    ...
    static_summary: str | None = None,
    static_details: str | None = None,
    ...):

    return await self._impl.create_schedule(
            CreateScheduleInput(
                id=id,
                schedule=schedule,
                trigger_immediately=trigger_immediately,
                backfill=backfill,
                memo=memo,
                search_attributes=search_attributes,
                rpc_metadata=rpc_metadata,
                rpc_timeout=rpc_timeout,
            )
```

The `static_` values should probably be used. Minimally they should be deprecated or documented somehow.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-12-04 20:39:18.000 UTC</summary>

The server API doesn't include it, the details are included as part of the action embedded in the schedule. Should be removed from the API.

</details>


---

<a id="1220"></a>

### #1220: [Feature Request] Add typing for signal for PyCharm

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1220 |
| **State** | CLOSED |
| **Author** | trajano (Archimedes Trajano) |
| **Created** | 2025-11-23 22:12:41.000 UTC (1 months ago) |
| **Updated** | 2025-12-02 16:52:18.000 UTC |
| **Closed** | 2025-12-02 16:52:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

PyCharm complains on a no-arg signal

```
await handler.signal(PlaceholderWorkflow.terminate)
```

### Describe the solution you'd like

Can this be added to `WorkflowHandle` in client.py
```
    @overload
    async def signal(
        self,
        signal: Callable[SelfType, None],
        *,
        rpc_metadata: Mapping[str, Union[str, bytes]] = {},
        rpc_timeout: Optional[timedelta] = None,
    ) -> None: ...
```

### Additional context

PyRight works, just PyCharm isn't working as expected but having that line works in both PyRight and PyCharm


#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-12-02 16:52:18.000 UTC</summary>

I actually wasn't able to reproduce this in my version of PyCharm. Potentially that's configuration or version dependent. Regardless, this overload doesn't actually expand the range of valid types, as `Callable` would fit the protocol of `MethodSyncOrAsyncNoParam`. That would indicate it is a PyCharm bug, not one for which we should introduce a new overload for all users.

</details>


---

<a id="1212"></a>

### #1212: Add pascalcase to WorkflowExecutionStatus

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1212 |
| **State** | CLOSED |
| **Author** | jhammarstedt (Johan Hammarstedt) |
| **Created** | 2025-11-13 17:08:41.000 UTC (1 months ago) |
| **Updated** | 2025-12-05 19:22:45.000 UTC |
| **Closed** | 2025-12-05 19:22:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

When filtering for search attributes ExecutionStatus excepts PascalCase but the WorkflowExecutionStatus cannot be converted as is right now. There should be a method for this

#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-12-05 19:22:45.000 UTC</summary>

Courtesy of @cretz:
> Even though it's the case that this enumerate returned from describe/list happens to have the same values in a different format as the search attribute, not sure we want to guarantee that will always be the case. We intentionally document the explicit set of enumerate strings at https://docs.temporal.io/search-attribute#default-search-attribute without reference to this enumerate, but it's technically possible that list could change unrelated to this enumerate (even if it happens to match today even in server code). Most (all?) parts of the list filter/query don't have non-string-literal representations in SDKs at this time.

</details>


---

<a id="1202"></a>

### #1202: [Feature Request] Advanced, unsafe ability to close the client

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1202 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-10-31 18:31:46.000 UTC (2 months ago) |
| **Updated** | 2025-12-10 16:50:31.000 UTC |
| **Closed** | 2025-12-10 16:50:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

For advanced users that need very explicitly-timed closing of TCP connections (i.e. not waiting on GC or having to run it), we should offer a way to close the client. This should probably be exposed as `ServiceClient.unsafe_close()` and be documented with wording like "users should not use this, and the behavior of any use of clients or things relying on clients after this call is undefined". Whether this drops the client from a Rust POV (e.g. we put it in an option and _take_ it on close) or we add a Core method to help with this is up to the implementer/discussion.

This should also include a test (even if platform specific) that confirms the socket is closed on return from that call.

#### Comments (1)

<details>
<summary><strong>VegetarianOrc</strong> commented on 2025-12-10 16:50:31.000 UTC</summary>

After some internal discussion, we'll be looking to provide a way to cleanly exit powered by some server side changes.

</details>


---

<a id="1180"></a>

### #1180: [Bug] OpenAI Agents SDK serialization fails with complex type annotations

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1180 |
| **State** | CLOSED |
| **Author** | nitishagar (Nitish Agarwal) |
| **Created** | 2025-10-23 06:10:31.000 UTC (2 months ago) |
| **Updated** | 2025-10-23 13:17:36.000 UTC |
| **Closed** | 2025-10-23 13:17:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

We're integrating the OpenAI Agents SDK with Temporal Python SDK to build AI-powered workflows that can execute activities through an agent interface. When using the OpenAI Agents SDK with Temporal Python SDK (`temporalio.contrib.openai_agents`), workflows fail with serialization errors when trying to deserialize complex generic type annotations that Temporal's default data converter cannot handle.

### Describe the bug

When attempting to execute a workflow that uses the OpenAI Agents SDK integration (temporalio.contrib.openai_agents), the workflow fails immediately during the first agent operation with a serialization error:

```python
TypeError: Unserializable type during conversion: typing_extensions.Required[typing.Union[str, list[typing.Union[openai.types.responses.easy_input_message_param.EasyInputMessageParam, openai.types.responses.response_input_item_param.Message, ...]]]]
```

### Minimal Reproduction


```python
from temporalio import workflow
from temporalio.common import RetryPolicy
from agents import Agent, Runner
from temporalio.contrib import openai_agents
from datetime import timedelta

@workflow.defn
class MyWorkflow:
    @workflow.run
    async def run(self) -> None:
        # Create a tool from an activity
        my_tool = openai_agents.workflow.activity_as_tool(
            my_activity,
            start_to_close_timeout=timedelta(seconds=10),
        )

        # Create an agent
        agent = Agent(
            name="Test Agent",
            instructions="Do something",
            tools=[my_tool],
        )

        # Run the agent - THIS FAILS with serialization error
        result = await Runner.run(agent, input="test prompt")
```


### Environment/Versions
- **Temporal Python SDK**: `1.8.0`
- **OpenAI Agents SDK**: `0.0.5`
- **OpenAI**: `1.59.4`
- **Python**: `3.9`
- **Operating System**: macOS (Darwin 24.6.0)

### Additional context

- The error occurs during activity deserialization in the worker
- The `ActivityModelInput` is created internally by the OpenAI Agents Plugin
- The issue is specific to the interaction between Temporal's type conversion system and OpenAI's complex type annotations
- Similar issues may affect other third-party libraries that use complex generic type hints

#### Workaround

Create a custom data converter that skips type validation for `ActivityModelInput`:

```python
"""Custom data converter for handling OpenAI Agents SDK types."""
import pickle
from typing import Any, List, Optional, Sequence, Type
from temporalio.api.common.v1 import Payload
from temporalio.converter import (
    DataConverter,
    DefaultPayloadConverter,
    PayloadConverter,
)


class PicklePayloadConverter(PayloadConverter):
    """Payload converter that handles OpenAI Agents SDK types."""

    def __init__(self):
        self._default = DefaultPayloadConverter()

    def to_payloads(self, values: Sequence[Any]) -> List[Payload]:
        return [self._to_payload_single(value) for value in values]

    def from_payloads(
        self, payloads: Sequence[Payload], type_hints: Optional[List[Type]] = None
    ) -> List[Any]:
        results = []
        for i, payload in enumerate(payloads):
            type_hint = type_hints[i] if type_hints and i < len(type_hints) else None
            results.append(self._from_payload_single(payload, type_hint))
        return results

    @property
    def encoding(self) -> str:
        return "binary/pickle+json"

    def _to_payload_single(self, value: Any) -> Payload:
        # Check if this is an OpenAI or Temporal OpenAI Agents type
        if hasattr(value, '__module__'):
            module_name = str(value.__module__)
            if 'openai' in module_name or 'temporalio.contrib.openai_agents' in module_name:
                return Payload(
                    metadata={"encoding": b"binary/pickle"},
                    data=pickle.dumps(value),
                )

        # Try default converter
        try:
            result = self._default.to_payloads([value])
            if result and len(result) > 0:
                return result[0]
        except (TypeError, AttributeError):
            pass

        # Fallback to pickle
        return Payload(
            metadata={"encoding": b"binary/pickle"},
            data=pickle.dumps(value),
        )

    def _from_payload_single(self, payload: Payload, type_hint: Optional[Type] = None) -> Any:
        encoding = payload.metadata.get("encoding", b"").decode()

        if encoding == "binary/pickle":
            return pickle.loads(payload.data)

        # Skip problematic type hints for OpenAI Agents SDK types
        skip_type_hint = False
        if type_hint:
            type_str = str(type_hint)
            if ('temporalio.contrib.openai_agents' in type_str or
                'ActivityModelInput' in type_str or
                'typing_extensions.Required' in type_str):
                skip_type_hint = True

        # Try default converter
        try:
            if skip_type_hint:
                result = self._default.from_payloads([payload], None)
            else:
                result = self._default.from_payloads([payload], [type_hint] if type_hint else None)

            if result and len(result) > 0:
                return result[0]
        except Exception:
            if payload.data:
                try:
                    return pickle.loads(payload.data)
                except Exception:
                    pass
            raise


def create_openai_data_converter() -> DataConverter:
    """Create a data converter that can handle OpenAI Agents SDK types."""
    return DataConverter(
        payload_converter_class=PicklePayloadConverter,
    )
```

**Usage:**

```python
from temporalio.client import Client
from temporalio.worker import Worker
from my_converters import create_openai_data_converter

# For worker
async def main():
    data_converter = create_openai_data_converter()

    client = await Client.connect(
        "localhost:7233",
        data_converter=data_converter,
    )

    worker = Worker(
        client,
        task_queue="my-queue",
        workflows=[MyWorkflow],
        activities=[my_activity],
        plugins=[openai_plugin],
    )

    await worker.run()

# For client
async def run_workflow():
    data_converter = create_openai_data_converter()

    client = await Client.connect(
        "localhost:7233",
        data_converter=data_converter,
    )

    result = await client.execute_workflow(
        MyWorkflow.run,
        id="my-workflow-id",
        task_queue="my-queue",
    )
```

#### Limitations of Workaround

The workaround bypasses the serialization error, but introduces a new issue: when type hints are skipped, the deserialized data becomes plain Python dicts instead of proper typed objects. The OpenAI Agents SDK code expects objects with attribute access (e.g., `tool.name`) rather than dict key access (e.g., `tool["name"]`), leading to `AttributeError: 'dict' object has no attribute 'name'`.



#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-23 13:17:36.000 UTC</summary>

This is because the `OpenAIAgentsPlugin` registers its own `DataConverter` which is a slight variation of the `pydantic_data_converter`. If you look at all the samples, for instance https://github.com/temporalio/samples-python/blob/main/openai_agents/basic/run_hello_world_workflow.py, the plugin is provided to the `Client`, not the `Worker`. It isn't actually super important in your `main` function (assuming `openai_plugin` is a `OpenAIAgentsPlugin`, but its definition is missing), because the client is only used inside a worker there. In `run_workflow` however, the plugin is simply not provided at all and therefore doesn't change the `DataConverter` as is required.

</details>


---

<a id="1162"></a>

### #1162: [Feature Request] [OpenAI] Allow activity_as_tool to execute as local_activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1162 |
| **State** | CLOSED |
| **Author** | kishan-paceix |
| **Created** | 2025-10-15 08:29:15.000 UTC (2 months ago) |
| **Updated** | 2025-10-27 15:49:27.000 UTC |
| **Closed** | 2025-10-27 15:49:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Currently activities defined as tool cannot be executed as local activities as we use `temporal_workflow.execute_activity` always. 
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->
Introduce additional boolean parameter (for ex `as_local_activity`) to `activity_as_tool` function and use `execute_local_activity` method when set to true

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->


#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-10-15 15:19:18.000 UTC</summary>

We'll consider that additional functionality. In the meantime, you are welcome to define a function tool which runs your specific local activity. It's much simpler to write than our implementation of `activity_as_tool` since it doesn't need to be generic. 
```
@function_tool
async def some_tool(something):
    return await workflow.execute_local_activity(some_activity, something, start_to_close_timeout...)
```

Reactions: ðŸ‘ 1

</details>


---

<a id="1130"></a>

### #1130: [Feature Request] Support for custom tracing providers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1130 |
| **State** | OPEN |
| **Author** | johnyang101 (John Yang) |
| **Created** | 2025-09-29 20:26:23.000 UTC (3 months ago) |
| **Updated** | 2025-09-30 14:41:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I'd like to add a custom tracing provider for the OpenAI Agents SDK. Importantly, this tracing provider is supported by the OpenAI Agents SDK itself.

### The solution I'd like

I'd like to be able to pass the tracing provider to the OpenAI Agents Plugin when initializing the client e.g. 

```python
from temporalio.client import Client
from temporalio.contrib.openai_agents import OpenAIAgentsPlugin
from custom_tracing_provider import CustomTraceProvider

plugin = OpenAIAgentsPlugin(
     trace_provider=CustomTraceProvider()
)


client_config = {
    "namespace": NAMESPACE,
    "plugins": [plugin],
    "API_KEY": os.environ.get("TEMPORAL_API_KEY")
}

_client_instance = await Client.connect(ADDRESS, **client_config)

```

Given the temporal tracing provider today is implemented on top of the `DefaultTracingProvider` class provided by OpenAI as shown below, perhaps it wouldn't be too hard to just create a factory that takes any supported tracing provider and returns a Temporal supported one.
```python

class TemporalTraceProvider(DefaultTraceProvider):
    """A trace provider that integrates with Temporal workflows."""

    def __init__(self, auto_close_in_workflows: bool = False):
        """Initialize the TemporalTraceProvider."""
        super().__init__()
        self._original_provider = cast(DefaultTraceProvider, get_trace_provider())
        self._multi_processor = _TemporalTracingProcessor(
            self._original_provider._multi_processor,
            auto_close_in_workflows,
        )

    def time_iso(self) -> str:
        """Return the current deterministic time in ISO 8601 format."""
        if workflow.in_workflow():
            return workflow.now().isoformat()
        return super().time_iso()

    def gen_trace_id(self) -> str:
        """Generate a new trace ID."""
        if workflow.in_workflow():
            try:
                """Generate a new trace ID."""
                return f"trace_{_workflow_uuid()}"
            except ReadOnlyContextError:
                return f"trace_{uuid.uuid4().hex}"
        return super().gen_trace_id()

    def gen_span_id(self) -> str:
        """Generate a span ID."""
        if workflow.in_workflow():
            try:
                """Generate a deterministic span ID."""
                return f"span_{_workflow_uuid()}"
            except ReadOnlyContextError:
                return f"span_{uuid.uuid4().hex[:24]}"
        return super().gen_span_id()

    def gen_group_id(self) -> str:
        """Generate a group ID."""
        if workflow.in_workflow():
            try:
                """Generate a deterministic group ID."""
                return f"group_{_workflow_uuid()}"
            except ReadOnlyContextError:
                return f"group_{uuid.uuid4().hex[:24]}"
        return super().gen_group_id()

    def __enter__(self):
        """Enter the context of the Temporal trace provider."""
        return self

    def __exit__(
        self,
        exc_type: type[BaseException],
        exc_val: BaseException,
        exc_tb: TracebackType,
    ):
        """Exit the context of the Temporal trace provider."""
        self._multi_processor.shutdown()
```

#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-30 14:41:55.000 UTC</summary>

Can you elaborate on the scenario? For what reason do you want to override the trace provider? Typically I've seen folks implement a `TracingProcessor`. 

</details>


---

<a id="1112"></a>

### #1112: [Feature Request] Add WorkflowIdConflictPolicy for child workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1112 |
| **State** | CLOSED |
| **Author** | aaronnhan-headway |
| **Created** | 2025-09-22 04:47:31.000 UTC (3 months ago) |
| **Updated** | 2025-09-22 13:45:27.000 UTC |
| **Closed** | 2025-09-22 13:45:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.
I appreciate the addition of WorkflowIdConflictPolicy for parent workflows added in [this issue](https://github.com/temporalio/sdk-python/issues/504), but I would also love to have it available in child workflows


### Describe the solution you'd like
Similar to how the `id_conflict_policy: temporalio.common.WorkflowIDConflictPolicy` parameter was added to the Temporal client's execute_workflow() and start_workflow() methods, I would love for start_child_workflow() and execute_child_workflow()


#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-22 13:45:27.000 UTC</summary>

This is an open request on the server to support. http://github.com/temporalio/temporal/issues/6799

</details>


---

<a id="1109"></a>

### #1109: [Bug] calling workflow.random().randint before workflow.uuid4 breaks replay with [TMPRL1100] Nondeterminism error

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1109 |
| **State** | CLOSED |
| **Author** | dpr-synth (David Pribil) |
| **Created** | 2025-09-19 09:19:38.000 UTC (3 months ago) |
| **Updated** | 2025-09-22 13:36:27.000 UTC |
| **Closed** | 2025-09-22 13:36:26.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I have a long running workflow that uses `workflow.uuid4` to set an ID for a child workflow, then sleeps. I modified the retry policy of an activity and set `initial_interval` with `workflow.random().randint`. The modified activity is called before the child workflow is started. I deployed the new workflow.

### Describe the bug

All the running workflows failed with `[TMPRL1100] Nondeterminism` error after they woke up from the sleep. All the errors were complaining about the ID of the child workflow. Example error message:
```
[TMPRL1100] Nondeterminism error: Child workflow id of scheduled event 'child_workflow_6bfd6073-2403-413f-acc3-7090cf837d0a' does not match child workflow id of command 'child_workflow_96a033ac-6730-4f6d-abfd-60732403113f
```

If I make any other change in the retry policy, e.g., changing `initial_interval` from one number to another, I don't get the above error. It only happens, if a `workflow.random()` function call is added.

### Minimal Reproduction

I managed to reproduce the issue with the simple workflow code below.

1. Deploy the worker
2. Start the workflow
3. Uncomment `workflow.random().randint`
4. Re-deploy the worker
5. After workflow wakes up it dies with non-determinism error

I also got the same error with other random functions, e.g., `workflow.random().uniform`. However, just adding  `workflow.random()` doesn't cause workflow fail with non-determinism.If `workflow.uuid4` is called BEFORE `workflow.random().randint`, then the workflow doesn't fail with non-determinism after `workflow.random().randint` was added.

```python
from datetime import timedelta

from temporalio import workflow


@workflow.defn
class ChildWorkflowUuid:
    @workflow.run
    async def run(self) -> str:
        return workflow.info().workflow_id


@workflow.defn
class WorkflowUuid:
    @workflow.run
    async def run(self) -> str:
        # IF CHILD ID IS SET HERE, THE WORKFLOW COMPLETES WITHOUT ANY ERRORS AFTER ADDING `workflow.random().randint(1, 5)`
        # child_workflow_id = str(workflow.uuid4())
        # UNCOMMENT `workflow.random().randint(1, 5)` WHILE WORKFLOW IS SLEEPING (just `workflow.random()` does not cause non-determinism error)
        # workflow.random().randint(1, 5)
        # IF CHILD ID IS SET HERE, WORKFLOW FAILS WITH
        # [TMPRL1100] Nondeterminism error: Child workflow id of scheduled event '0d4bdc56-4950-4478-94ad-076b61f06fb1' does not match child workflow id of command 'ab05243a-0d4b-4c56-8950-547854ad076b
        child_workflow_id = str(workflow.uuid4())
        child_workflow_uuid = await workflow.execute_child_workflow(
            ChildWorkflowUuid.run,
            id=child_workflow_id,
        )
        await workflow.sleep(timedelta(seconds=30))
        return child_workflow_uuid
```

### Environment/Versions

- OS and processor: M1 Mac Pro and x86 Linux
- Temporal Version: 1.17.0
- Are you using Docker or Kubernetes or building Temporal from source: no

### Additional context

I am not sure if this is a bug or expected behaviour. I assumed that `workflow.uuid4` seed only depends on the run ID, which should be unchanged after the workflow was modified.


#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-22 13:36:26.000 UTC</summary>

This is expected. All the randomly generated objects on the workflow are based on `workflow.random()`. Hence, when you add a new call to it, it will change the result of the second call.

Reactions: ðŸ‘ 1

</details>


---

<a id="1103"></a>

### #1103: [Bug] There are no Workers polling the X Task Queue.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1103 |
| **State** | CLOSED |
| **Author** | bbkgh (Babak Ghadiri) |
| **Created** | 2025-09-13 05:54:31.000 UTC (3 months ago) |
| **Updated** | 2025-09-22 14:47:25.000 UTC |
| **Closed** | 2025-09-22 14:47:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

After some times without any changes in worker or server configs, in the web ui i see:

"There are no Workers polling the X Task Queue."

Environment is k8s.

After rollout restart of worker, it works again and error disappears.

How can we fix this issue? is there any retry/connect mechanism built in sdk?


### Environment/Versions

- Temporal Version: server: temporalio/auto-setup:1.28, sdk: 1.17

<img width="1649" height="410" alt="Image" src="https://github.com/user-attachments/assets/5a491335-f6f8-4d46-9e7d-a87e6560d4ba" />


worker configs:

```
client = await TemporalService().get_client()
worker = Worker(
        client,
        task_queue=TemporalTaskQueues.billing.value,
        workflows=[
...        ],
        activities=[
...        ],
        workflow_runner=UnsandboxedWorkflowRunner(),
        debug_mode=True,
        activity_executor=ThreadPoolExecutor(20),
        graceful_shutdown_timeout=timedelta(seconds=30),
        max_heartbeat_throttle_interval=timedelta(seconds=10),
        default_heartbeat_throttle_interval=timedelta(seconds=5),
    )

```

#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-16 14:34:16.000 UTC</summary>

It sounds like you are potentially running out of slots on your worker(s). You'll need to use metrics and logs to diagnose why. I would recommend you go to the community slack if you need help from our customer success folks: https://t.mp/slack.

</details>


---

<a id="1087"></a>

### #1087: [Bug] Unable to pickle proto classes, module name is 'temporal' instead of 'temporalio'

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1087 |
| **State** | CLOSED |
| **Author** | martinxsliu (Martin Liu) |
| **Created** | 2025-09-08 18:09:58.000 UTC (4 months ago) |
| **Updated** | 2025-10-14 19:12:52.000 UTC |
| **Closed** | 2025-10-14 19:12:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi team,

I'm trying to run workflow unit tests with a process pool and I'm getting the following error:

> multiprocessing.pool.MaybeEncodingError: Error sending result: '<multiprocessing.pool.ExceptionWithTraceback object at 0x7efc28c0ba10>'. Reason: 'PicklingError("Can't pickle <class 'temporal.api.failure.v1.message_pb2.Failure'>: import of module 'temporal.api.failure.v1.message_pb2' failed")'

Seems like maybe it's because the module names in the generated proto classes are prefixes with `temporal` instead of `temporalio`, for example: https://github.com/temporalio/sdk-python/blob/1.17.0/temporalio/api/failure/v1/message_pb2.py#L60

### Minimal Reproduction

Will try to create a repro..

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Mac
- Temporal Version: 1.17
- Are you using Docker or Kubernetes or building Temporal from source? Using wheel from PyPI

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-09-09 19:45:10.000 UTC</summary>

This is a bug on our part, we will be fixing it to correct the module (likely updating the protos to have a proper python path)

</details>


---

<a id="1081"></a>

### #1081: [Feature Request] Expose `strict_json_schema` argument in `activity(nexus)_as_tool` in OpenAI SDK integration

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1081 |
| **State** | CLOSED |
| **Author** | antlai-temporal (Antonio Lain) |
| **Created** | 2025-09-05 18:44:32.000 UTC (4 months ago) |
| **Updated** | 2025-09-29 16:15:31.000 UTC |
| **Closed** | 2025-09-29 16:15:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description


Currently, the OpenAI SDK integration hardwires `strict_json_schema=True` in https://github.com/temporalio/sdk-python/blob/main/temporalio/contrib/openai_agents/workflow.py#L152 and L238 . 

The current workaround is to use `dataclasses.replace` on the result, but it could be cleaner to pass it as an argument to `activity_as_tool`, as suggested by a Temporal customer... 


#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-05 19:09:55.000 UTC</summary>

Seems reasonable and easy. We'll put it in the pipeline.

</details>


---

<a id="1054"></a>

### #1054: [Bug] OTEL metadata missing in `__init__` method of a workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1054 |
| **State** | CLOSED |
| **Author** | SF-300 (Vladimir Lebedev) |
| **Created** | 2025-08-26 12:36:04.000 UTC (4 months ago) |
| **Updated** | 2025-09-03 18:11:22.000 UTC |
| **Closed** | 2025-09-03 18:11:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hello,

### What are you really trying to do?

I want to see proper OTEL metadata (spans and traces) propagated from the client invocation site all the way through workflows and activities, so I can visualize the full path in something like Datalust Seq.

### Describe the bug
When a workflow starts an activity from its `__init__` method, the OpenTelemetry context is not propagated. As a result, the activity runs with a missing/empty span context, even though the client call is wrapped in a valid span.

### Minimal Reproduction
```python
# /// script
# requires-python = ">=3.13"
# dependencies = [
#     "temporalio[opentelemetry]>=1.16.0",
# ]
# ///
import asyncio
from datetime import timedelta

from temporalio import activity, workflow
from temporalio.worker import Worker
from temporalio.contrib.opentelemetry import TracingInterceptor
from temporalio.testing import WorkflowEnvironment

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor


# ---------- OpenTelemetry setup ----------
provider = TracerProvider()
provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))
trace.set_tracer_provider(provider)
tracer = trace.get_tracer(__name__)


@activity.defn
async def get_trace_id() -> int:
    return trace.get_current_span().get_span_context().trace_id


# ---------- Workflow ----------
@workflow.defn
class DemoWorkflow:
    @workflow.init
    def __init__(self) -> None:
        self._result = workflow.start_activity(
            get_trace_id,
            start_to_close_timeout=timedelta(seconds=5),
        )

    @workflow.run
    async def run(self) -> int:
        return await self._result


# ---------- Main ----------
async def main():
    async with await WorkflowEnvironment.start_local(
        interceptors=[TracingInterceptor()],
    ) as env:
        queue_name = "some-queue"
        async with Worker(
            env.client,
            task_queue=queue_name,
            workflows=[DemoWorkflow],
            activities=[get_trace_id],
        ):
            with tracer.start_as_current_span("invocation-site-span") as span:
                invocation_trace_id = span.get_span_context().trace_id
                activity_trace_id = await env.client.execute_workflow(
                    DemoWorkflow.run,
                    id="otel-update-workflow-id-2",
                    task_queue=queue_name,
                )
                assert activity_trace_id == invocation_trace_id


if __name__ == "__main__":
    asyncio.run(main())
```

### Additional context

I think that [WorkflowOutboundInterceptor](https://github.com/temporalio/sdk-python/blob/a9c71aafd8c9905e64e8767f5dcfe917df1dc79a/temporalio/worker/_interceptor.py#L405) should have a hook to wrap an `__init__` of a workflow, but looking at how/where `_instantiate_workflow_object` [is called](https://github.com/temporalio/sdk-python/blob/a9c71aafd8c9905e64e8767f5dcfe917df1dc79a/temporalio/worker/_workflow_instance.py#L2125), I don't think that it was planned for. So fixing this might require a refactoring of unclear scale to me.
If this issue is considered worth fixing, I'm willing to try fixing this and would greatly appreciate any clues (e.g. whether or not introducing another hook is the way to go here... or any other advice, really :)

#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-03 18:11:22.000 UTC</summary>

In most SDKs we have specifically disallowed taking substantial action inside of the workflow constructors. I would recommend you refrain from setting up commands inside `__init__` and do so during `run` instead.

</details>


---

<a id="1053"></a>

### #1053: [Bug] OTEL metadata missing for execute_update_with_start_workflow 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1053 |
| **State** | CLOSED |
| **Author** | SF-300 (Vladimir Lebedev) |
| **Created** | 2025-08-26 11:28:28.000 UTC (4 months ago) |
| **Updated** | 2025-10-16 19:41:38.000 UTC |
| **Closed** | 2025-10-16 19:41:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hello,

### What are you really trying to do?

I want to see proper OTEL metadata (spans and traces) propagated from the client invocation site all the way through workflows and activities, so I can visualize the full path in something like Datalust Seq.

### Describe the bug

Propagation works correctly for most workflow client operations, but fails for `execute_update_with_start_workflow` (and likely also for `start_update_with_start_workflow`).  

From inspecting the SDK codebase, it looks like `start_update_with_start_workflow` was not included in
[`_TracingClientOutboundInterceptor`](https://github.com/temporalio/sdk-python/blob/607641bf1e8250699b9c7ae0e87230d43b26c2ff/temporalio/contrib/opentelemetry.py#L219) after [this PR](https://github.com/temporalio/sdk-python/pull/702).  
When I add it manually, propagation works as expected.

**Expected:** OTEL trace and span context from the client are propagated through the workflow update handler and into an activity.  
**Actual:** Trace and span context are missing inside the activity.

### Minimal Reproduction
```python
# /// script
# requires-python = ">=3.13"
# dependencies = [
#     "temporalio[opentelemetry]>=1.16.0",
# ]
# ///
import asyncio
from typing import Any
from datetime import timedelta

import temporalio
import opentelemetry
from temporalio import activity, workflow
from temporalio.client import WithStartWorkflowOperation
from temporalio.worker import Worker
from temporalio.contrib.opentelemetry import TracingInterceptor, _TracingClientOutboundInterceptor
from temporalio.common import WorkflowIDConflictPolicy
from temporalio.testing import WorkflowEnvironment

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor


# ---------- OpenTelemetry setup ----------
provider = TracerProvider()
provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))
trace.set_tracer_provider(provider)
tracer = trace.get_tracer(__name__)


@activity.defn
async def get_trace_id() -> int:
    return trace.get_current_span().get_span_context().trace_id


# ---------- Workflow ----------
@workflow.defn
class DemoWorkflow:
    @workflow.init
    def __init__(self) -> None:
        self._done = asyncio.Event()

    @workflow.run
    async def run(self) -> None:
        await self._done.wait()

    @workflow.update
    async def read_trace_id_from_activity(self) -> int:
        try:
            return await workflow.execute_activity(
                get_trace_id,
                start_to_close_timeout=timedelta(seconds=5),
            )
        finally:
            self._done.set()


class _PatchedTracingClientOutboundInterceptor(_TracingClientOutboundInterceptor):
    async def start_update_with_start_workflow(
        self, input: temporalio.client.StartWorkflowUpdateWithStartInput
    ) -> temporalio.client.WorkflowUpdateHandle[Any]:
        with self.root._start_as_current_span(
            f"StartUpdateWithStartWorkflow:{input.start_workflow_input.workflow}",
            attributes={"temporalWorkflowID": input.start_workflow_input.id},
            input=input.start_workflow_input,
            kind=opentelemetry.trace.SpanKind.CLIENT,
        ):
            return await super().start_update_with_start_workflow(input)


class _PatchedTracingInterceptor(TracingInterceptor):
    def intercept_client(
        self, next: temporalio.client.OutboundInterceptor
    ) -> temporalio.client.OutboundInterceptor:
        return _PatchedTracingClientOutboundInterceptor(next, self)


# ---------- Main ----------
async def main():
    async with await WorkflowEnvironment.start_local(
        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        interceptors=[_PatchedTracingInterceptor()],
        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        # interceptors=[TracingInterceptor()],
        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    ) as env:
        queue_name = "some-queue"
        async with Worker(
            env.client,
            task_queue=queue_name,
            workflows=[DemoWorkflow],
            activities=[get_trace_id],
        ):
            with tracer.start_as_current_span("invocation-site-span") as span:
                invocation_trace_id = span.get_span_context().trace_id
                activity_trace_id = await env.client.execute_update_with_start_workflow(
                    DemoWorkflow.read_trace_id_from_activity,
                    start_workflow_operation=WithStartWorkflowOperation(
                        DemoWorkflow.run,
                        id="otel-update-workflow-id-2",
                        task_queue=queue_name,
                        id_conflict_policy=WorkflowIDConflictPolicy.USE_EXISTING,
                    ),
                )
                assert activity_trace_id == invocation_trace_id


if __name__ == "__main__":
    asyncio.run(main())
```

### Additional context

I would kindly like someone more familiar with the codebase to sanity-check this assessment and my fix. If those are correct, I would gladly make a PR with the fix.

#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-09-03 18:15:30.000 UTC</summary>

I agree, it wasn't updated to account for the new methods. We would be happy to accept a PR for this!

</details>


---

<a id="1033"></a>

### #1033: [Bug] heartbeat_timeout=_proto_to_non_zero_timedelta(start.WAIT_TIMEOUT)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1033 |
| **State** | CLOSED |
| **Author** | AlmogBaku (Almog Baku) |
| **Created** | 2025-08-19 02:55:49.000 UTC (4 months ago) |
| **Updated** | 2025-08-19 09:56:11.000 UTC |
| **Closed** | 2025-08-19 09:56:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Simple naive test

### Describe the bug

Code below shows this weird error:
```console
============================= test session starts ==============================
platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0 -- /Users/almogbaku/projects/poc/workflows/.venv/bin/python3
cachedir: .pytest_cache
rootdir: /Users/almogbaku/projects/poc/workflows
configfile: pyproject.toml
plugins: asyncio-1.1.0, anyio-4.10.0, cov-6.2.1, langsmith-0.4.14, logfire-4.3.3
asyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=session, asyncio_default_test_loop_scope=function
collecting ... collected 1 item

src/test.py::test_workflow_basic_startup 
-------------------------------- live log call ---------------------------------
WARNING  temporalio.activity:_activity.py:365 Completing activity as failed
Traceback (most recent call last):
  File "/Users/almogbaku/projects/poc/workflows/.venv/lib/python3.12/site-packages/temporalio/worker/_activity.py", line 296, in _handle_start_activity_task
    result = await self._execute_activity(start, running_activity, task_token)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/almogbaku/projects/poc/workflows/.venv/lib/python3.12/site-packages/temporalio/worker/_activity.py", line 519, in _execute_activity
    heartbeat_timeout=_proto_to_non_zero_timedelta(start.WAIT_TIMEOUT)
                                                   ^^^^^^^^^^^^^^^^^^
AttributeError: WAIT_TIMEOUT
WARNING  temporalio.activity:_activity.py:365 Completing activity as failed
Traceback (most recent call last):
  File "/Users/almogbaku/projects/poc/workflows/.venv/lib/python3.12/site-packages/temporalio/worker/_activity.py", line 296, in _handle_start_activity_task
    result = await self._execute_activity(start, running_activity, task_token)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/almogbaku/projects/poc/workflows/.venv/lib/python3.12/site-packages/temporalio/worker/_activity.py", line 519, in _execute_activity
    heartbeat_timeout=_proto_to_non_zero_timedelta(start.WAIT_TIMEOUT)
                                                   ^^^^^^^^^^^^^^^^^^
AttributeError: WAIT_TIMEOUT
WARNING  temporalio.activity:_activity.py:365 Completing activity as failed
Traceback (most recent call last):
  File "/Users/almogbaku/projects/poc/workflows/.venv/lib/python3.12/site-packages/temporalio/worker/_activity.py", line 296, in _handle_start_activity_task
    result = await self._execute_activity(start, running_activity, task_token)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/almogbaku/projects/poc/workflows/.venv/lib/python3.12/site-packages/temporalio/worker/_activity.py", line 519, in _execute_activity
    heartbeat_timeout=_proto_to_non_zero_timedelta(start.WAIT_TIMEOUT)
                                                   ^^^^^^^^^^^^^^^^^^
AttributeError: WAIT_TIMEOUT
PASSED

============================== 1 passed in 0.39s ===============================
```

### Minimal Reproduction

```python
from datetime import timedelta

import pytest
from temporalio import activity, workflow
from temporalio.client import WorkflowExecutionStatus
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import Worker


@workflow.defn
class BasicFake:
    def __init__(self) -> None:
        self._internal_state = {}

    @workflow.run
    async def run(self, input_params) -> None:
        self._internal_state = await workflow.execute_activity(
            'do_something',
            input_params,
            start_to_close_timeout=timedelta(seconds=30),
        )

        while True:
            await workflow.sleep(1)


@activity.defn(name="do_something")
async def mock_do_something(input_params: dict) -> str:
    return input_params['name']


@pytest.mark.asyncio
async def test_workflow_basic_startup():
    async with await WorkflowEnvironment.start_time_skipping() as env:
        async with Worker(
            env.client,
            task_queue="test-queue",
            workflows=[BasicFake],
            activities=[mock_do_something],
        ):
            handle = await env.client.start_workflow(
                'BasicFake',
                {"name": "test"},
                id="test-id",
                task_queue="test-queue",
            )
            await env.sleep(5)

            try:
                status = (await handle.describe()).status
                assert WorkflowExecutionStatus.RUNNING == status
            finally:
                await handle.terminate()
```

### Environment/Versions

python sdk 1.15.0


#### Comments (1)

<details>
<summary><strong>AlmogBaku</strong> commented on 2025-08-19 09:56:11.000 UTC</summary>

removing .venv and reinstalling fixed this. damn, sent until 6am on this

</details>


---

<a id="1028"></a>

### #1028: [Documentation Request] GetLastCompletionResult equivalent in Python SDK

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1028 |
| **State** | CLOSED |
| **Author** | yakupt-canonical |
| **Created** | 2025-08-14 10:52:25.000 UTC (4 months ago) |
| **Updated** | 2025-08-15 14:22:04.000 UTC |
| **Closed** | 2025-08-15 14:22:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I am using Python SDK for Temporal schedules, when I check the Go and Java SDK, I see that there is a [GetLastCompletionResult](https://pkg.go.dev/go.temporal.io/sdk/workflow#GetLastCompletionResult) method for getting previous run result of a schedule and it can be called [within workfow](https://github.com/temporalio/samples-go/blob/e2db4f5f4291536e4f5e0ea382431fc7fb8550eb/cron/workflow.go#L31-L36), however I couldn't find the same method in Python SDK. 

Does Python SDK have a equivalent way for getting the last completion result of a schedule?


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-08-15 14:22:03.000 UTC</summary>

This is a duplicate of #383. We will look into increasing the prioritization of this.

Reactions: ðŸ‘ 1 â¤ï¸ 1

</details>


---

<a id="1018"></a>

### #1018: [Bug] Replayer not working when workflow_runner is set to UnsandboxedWorkflowRunner().

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1018 |
| **State** | CLOSED |
| **Author** | ankitpaudel20 (Ankit Paudel) |
| **Created** | 2025-08-04 15:56:54.000 UTC (5 months ago) |
| **Updated** | 2025-08-04 16:06:14.000 UTC |
| **Closed** | 2025-08-04 15:58:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I am trying to replay a workflow from a history file.

### Describe the bug

I have a history json downloaded from temporal-ui and I want to debug it using replayer.

this is my code:
```
import argparse
import asyncio
import json
import logging
from typing import Any

from temporalio.client import WorkflowHistory
from temporalio.contrib.pydantic import pydantic_data_converter
from temporalio.worker import Replayer, UnsandboxedWorkflowRunner

from app.workflows.workflow import TestWorkflow



def get_workflow_id_from_history(history: dict[str, Any]) -> str | None:
    """Extract workflow ID from history events."""
    events = history.get("events")
    if not isinstance(events, list):
        return None

    for event in events:
        if event.get("eventType") == "EVENT_TYPE_WORKFLOW_EXECUTION_STARTED":
            attributes = event.get("workflowExecutionStartedEventAttributes")
            if attributes and attributes.get("workflowId"):
                return attributes["workflowId"]
    return None


async def main():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    logging.info("Starting workflow replay")

    parser = argparse.ArgumentParser(description="Replay a workflow from a history file.")
    parser.add_argument(
        "--history-file",
        help="Path to the workflow history JSON file.",
    )
    args = parser.parse_args()

    history_file_path = args.history_file or "/home/user/Downloads/01986512-a694-7802-9ec5-7bfd91daac2f_events.json"
    if not history_file_path:
        logging.info("History file not provided via CLI, asking for user input.")
        history_file_path = input("Enter the path to the history file: ")

    logging.info(f"Loading history from: {history_file_path}")

    replayer = Replayer(
        workflows=[TestWorkflow],
        data_converter=pydantic_data_converter,
        debug_mode=True,
        workflow_runner=UnsandboxedWorkflowRunner(),
    )
    with open(history_file_path) as f:
        history = json.load(f)

    logging.info("History loaded.")

    workflow_id = get_workflow_id_from_history(history)
    if not workflow_id:
        logging.warning("Could not find workflow ID in history. Please enter it manually.")
        workflow_id = input("Could not find workflow ID in history. Please enter it manually: ")

    logging.info(f"Replaying workflow with ID: {workflow_id}")

    await replayer.replay_workflow(
        history=WorkflowHistory.from_json(
            workflow_id,
            history,
        ),
    )

    logging.info("Workflow replay finished.")


if __name__ == "__main__":
    asyncio.run(main())
```


### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: x86 Linux
- Python SDK version: 1.14.1
- Are you using Docker or Kubernetes or building Temporal from source?: NO

### Additional context

It works in this specific runner though:
```        workflow_runner=SandboxedWorkflowRunner(restrictions=SandboxRestrictions.default.with_passthrough_all_modules()),
```

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-08-04 16:06:13.000 UTC</summary>

I assume this was solved without needing anything from the SDK? If so, all good.

</details>


---

<a id="1013"></a>

### #1013: [Bug] Missing overload for start causing Pyright type check issue

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1013 |
| **State** | CLOSED |
| **Author** | btruhand (Btara Truhandarien) |
| **Created** | 2025-07-31 05:06:03.000 UTC (5 months ago) |
| **Updated** | 2025-08-04 23:43:20.000 UTC |
| **Closed** | 2025-08-04 23:43:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Want to call `start_activity` with `summary` supplied while passing type checks

### Describe the bug

When calling `start_activity` with `summary`, Pyright complains that no overloads match the provided arguments

<img width="885" height="181" alt="Image" src="https://github.com/user-attachments/assets/238ec9f8-f914-4e2b-9969-ee44f3901769" />

### Minimal Reproduction

```
from temporalio import workflow

workflow.start_activity(
            activity="Some name",
            summary="Some summary",
)
```

### Environment/Versions

- Temporal version: `1.11.1`
- Python version: `3.11.12`


#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-07-31 14:37:57.000 UTC</summary>

Thanks! It does look like this was missed in the original implementation. 

</details>


---

<a id="1002"></a>

### #1002: Over 1000 commits in workflow non active state

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1002 |
| **State** | CLOSED |
| **Author** | akametall |
| **Created** | 2025-07-28 13:20:57.000 UTC (5 months ago) |
| **Updated** | 2025-07-28 16:37:15.000 UTC |
| **Closed** | 2025-07-28 16:37:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Good day,
Have an issue with temp ver 1.22.4 and postgres - when there is no workflows temporal dont stop commiting. Maybe we do somth wrong? 

#### Comments (1)

<details>
<summary><strong>dandavison</strong> commented on 2025-07-28 16:37:15.000 UTC</summary>

Hi @akametall, are you reporting a problem you've had with the Temporal server? The repo for that is https://github.com/temporalio/temporal. If you do open another issue, could you give a slightly longer explanation of exactly how to reproduce the problem you're encountering? I'll close this one since this repo is for problems specifically with the Python SDK. But feel free to re-open if you feel that the problem is related to the Python SDK.

</details>


---

<a id="989"></a>

### #989: [Feature Request] Support for specifying WorkflowIDConflictPolicy in child workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/989 |
| **State** | CLOSED |
| **Author** | wyattrees (Wyatt Rees) |
| **Created** | 2025-07-23 14:20:46.000 UTC (5 months ago) |
| **Updated** | 2025-07-23 18:57:59.000 UTC |
| **Closed** | 2025-07-23 18:57:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

I need to spawn a number of child workflows and abandon them. Later, the same parent workflow may have a new run and attempt to spawn more child workflows. I'd like to be able to check if any of the new children are workflows that have already been spawned and are still running via use of `WorkflowIDConflictPolicy.USE_EXISTING`. As `WorkflowIDConflictPolicy` cannot be specified in `temporalio.workflow.start_child_workflow`, this is currently not possible.

The workaround is to simply use the default and except the error that is thrown when an existing workflow has the same ID, however, this does not allow you to retrieve the workflow handle.

### Describe the solution you'd like

Add an argument to `temporalio.workflow.start_child_workflow` that specifies a `WorkflowIDConflictPolicy`.

### Additional context

n/a


#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-07-23 18:57:59.000 UTC</summary>

This isn't an issue specific to the Python SDK, the server does not support this mechanism. We do have an item tracking this request at a higher level. https://github.com/temporalio/features/issues/558

</details>


---

<a id="961"></a>

### #961: [Feature Request] Add optional headers parameter to WorkflowHandle.signal() and Workflow.execute_update()/Workflow.start_update()

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/961 |
| **State** | CLOSED |
| **Author** | DmitryZap (Dmitry Zaporozhchenko) |
| **Created** | 2025-07-14 04:26:02.000 UTC (5 months ago) |
| **Updated** | 2025-08-05 12:29:21.000 UTC |
| **Closed** | 2025-08-05 10:09:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Currently, the signal() method in Temporal's Python SDK WorkflowHandle doesn't allow passing custom headers to the workflow signal. While there's an rpc_metadata parameter for RPC-level headers, there's no way to pass workflow-level headers that would be accessible within the workflow code. 


### Describe the solution you'd like

I'd like to add an optional headers parameter to the signal() method that would be passed through to the SignalWorkflowInput. The method signature would become:

```python3
async def signal(
    self,
    signal: Union[str, Callable],
    arg: Any = temporalio.common._arg_unset,
    *,
    args: Sequence[Any] = [],
    headers: Mapping[str, temporalio.api.common.v1.Payloads] = {},  # New parameter
    rpc_metadata: Mapping[str, str] = {},
    rpc_timeout: Optional[timedelta] = None,
) -> None:
    ...
```

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-07-14 14:33:48.000 UTC</summary>

Temporal headers are intentionally only set via interceptors because they are meant for implicit setting and not explicit by callers. See https://github.com/temporalio/samples-python/tree/main/context_propagation for an example use case of this where context propagation of a context var is done via headers. Our OpenTelemetry contrib implementation/interceptor also uses headers to relay serialized spans.

Reactions: ðŸ‘ 1

</details>


---

<a id="951"></a>

### #951: [Bug] OpenAI Agents - trace spans sometimes fail to complete

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/951 |
| **State** | CLOSED |
| **Author** | jssmith (Johann Schleier-Smith) |
| **Created** | 2025-07-09 22:23:00.000 UTC (6 months ago) |
| **Updated** | 2025-07-16 17:57:40.000 UTC |
| **Closed** | 2025-07-16 17:57:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

When running the [samples](https://github.com/temporalio/samples-python/tree/main/openai_agents) and viewing the traces in the OpenAI Platform Dashboard, some spans fail to close.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction

Run the research workflow:
```
uv run openai_agents/run_research_workflow.py
```

Then [review traces](https://platform.openai.com/traces/).

### Environment/Versions

M4 Mac, Python 3.13

### Additional context

<!-- Add any other context about the problem here. -->

<img width="2984" height="2080" alt="Image" src="https://github.com/user-attachments/assets/a7eb6d2e-c744-43bc-b387-e40ac88abf0a" />


#### Comments (1)

<details>
<summary><strong>tconley1428</strong> commented on 2025-07-16 17:57:22.000 UTC</summary>

Turns out that this is UI behavior in OpenAI. The load more button at the bottom will pull in additional events, including the closes on these 'ongoing' spans.

</details>


---

<a id="925"></a>

### #925: [Feature Request] Opt-in support for applying codec to headers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/925 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-06-26 21:30:41.000 UTC (6 months ago) |
| **Updated** | 2025-07-08 18:42:13.000 UTC |
| **Closed** | 2025-07-08 18:42:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/331. Today, headers are not passed through the codec by default here like they are in .NET and Ruby and such. We should provide an opt-in somewhere to allow this.

Users using activities and such can just use the data converter w/ the codec, but inside workflows they can't easily, only the payload converter. Ideally all non-search-attributes go through codec, but we can't blindly flip this on.

Asks:

* Opt-in for all non-search-attribute payloads to go through codec (may need to be configured at client level)
  * Will have to update client and `temporalio.bridge.worker.py`'s `decode_activation` and `encode_completion`
    * Bonus points if a payload visitor utility is created that does this and can also be used for other purposes, ref https://github.com/temporalio/features/issues/468
* Confirmation via tests that all payloads go through the codec including activation/completion

#### Comments (1)

<details>
<summary><strong>Sushisource</strong> commented on 2025-07-01 16:59:53.000 UTC</summary>

Letâ€™s consider potentially making this opt-out rather than opt-in by default (even though thatâ€™s a breaking change) so SDKs are consistent.

</details>


---

<a id="890"></a>

### #890: [Bug] pydantic_data_converter change the value of SecretStr to "*"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/890 |
| **State** | CLOSED |
| **Author** | yeger00 (Yeger) |
| **Created** | 2025-06-06 17:35:24.000 UTC (7 months ago) |
| **Updated** | 2025-06-07 20:07:35.000 UTC |
| **Closed** | 2025-06-07 20:07:28.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Hello,
First of all thank you for maintaining this library.
I wanted to pass "secrets" to an activity by using the [`SecretStr`](https://docs.pydantic.dev/2.0/usage/types/secrets/) type from Pydantic. I use the `pydantic_data_converter`.

### Describe the bug

Before calling the `execute_workflow` the value is as expected, but in the activity the value is all "*", instead of the value itself.


### Minimal Reproduction
I created a minimal reproduction of the bug in the samples-python repo:
The code is [here](https://github.com/yeger00/samples-python/tree/secretstr-bug/pydantic_converter
) (the commit is [here](https://github.com/yeger00/samples-python/commit/95d2d738b0c4ab708f7ef25e5ea43cbabae824f2))

The logs - as you can see before the value is printed as expected, and after the value is all "*":
INFO:root:models secret: super secret
INFO:root:Got secret from client: **********

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: M1 Mac
- Temporal Version: temporal version 1.3.0 (Server 1.27.1, UI 2.36.0)
- Are you using Docker or Kubernetes or building Temporal from source? 

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-06-06 19:41:28.000 UTC</summary>

From the linked doc:

> The SecretStr and SecretBytes will be formatted as either '**********' or '' on conversion to json.

It seems Pydantic intentionally doesn't allow serialization of this value. You may need to choose another data type if you want it serializable.

</details>


---

<a id="873"></a>

### #873: [Bug] Workflow Task Timed out

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/873 |
| **State** | CLOSED |
| **Author** | hebrd (hebrd) |
| **Created** | 2025-05-18 11:23:58.000 UTC (7 months ago) |
| **Updated** | 2025-05-20 00:23:55.000 UTC |
| **Closed** | 2025-05-20 00:23:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

There are many "workflow task timed out" error on my temporal (with python sdk) after 2 minutes.

workflow definition:
```python
@activity.defn
async def execute_action(action_input: dict[str, Any]):
    try:
        return await do_execute_action(action_input)
    except Exception as e:
        if action_input is not None and 'execution' in action_input:
            from utils.log_helper import get_loguru_traceback
            if action_input['execution']['execution_log'] is None:
                action_input['execution']['execution_log'] = ""
            from utils.log_helper import get_current_traceback
            action_input['execution']['execution_log'] += "\r\n" + get_current_traceback()


async def do_execute_action(action_input: dict[str, Any]):
    """
    æ‰§è¡ŒAction
    :param action_input:
    :return:
    """
    if action_input is None:
        raise RuntimeError("action_input is None")
    action = action_input.get("action", {})  # type: dict
    task_handler = action.get("action_handler_name", "")
    if task_handler is None or len(task_handler) <= 0:
        raise RuntimeError("action_handler_name is None or empty")
    func = get_worker_handler_function(task_handler)
    action_output = await func(action_input)
    return action_output


@workflow.defn
class BotActionWorkflow:

    @workflow.run
    async def run(self, wf_input: dict[str, Any]) -> dict[str, Any]:
        return await workflow.execute_activity(
            execute_action,
            wf_input,
            start_to_close_timeout=timedelta(seconds=1800),
            retry_policy=RetryPolicy(maximum_attempts=1),
        )


```
I need helps:
1. Why does this happen? My business code already done, why still so many workflow tasks taking so long to finish
2. How to prevent this timeout problem so the workflow can be finished more quick

![Image](https://github.com/user-attachments/assets/63a1faee-7b77-4c1e-89e1-def0a9700055)

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-05-19 12:38:01.000 UTC</summary>

This may be more of a support question than a bug (can join us on Slack or forums). Usually this means your code is taking 10s to handle the workflow task which only takes milliseconds usually. This history looks like you may have tried to raise this timeout to the max of 120s, but regardless, something is blocking the event loop or workflow thread somehow.

There may be something missing from your sample letting me replicate. Can you provide a full runnable replication? Can take https://github.com/temporalio/samples-python/blob/main/hello/hello_activity.py or similar and alter to replicate your issue, or can take your issue while you can replicate and continually reduce it smaller and smaller while it still is breaking.

This is usually caused by blocking the asyncio event loop with accidental thread-blocking logic.

</details>


---

<a id="856"></a>

### #856: Integration of Temporal Python SDK Metrics with FastAPI and Prometheus FastAPI Instrumentator

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/856 |
| **State** | CLOSED |
| **Author** | serious-den |
| **Created** | 2025-05-07 11:36:48.000 UTC (8 months ago) |
| **Updated** | 2025-05-07 12:46:47.000 UTC |
| **Closed** | 2025-05-07 12:46:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I am using the Temporal Python SDK along with FastAPI and Prometheus FastAPI Instrumentator for metrics. However, I am facing difficulties in integrating the metrics from the Temporal Python SDK into the FastAPI app's metrics. Specifically, I would like to see all metrics available under a single endpoint.

Here is my current setup:

FastAPI for the web framework
Temporal Python SDK for workflow orchestration
Prometheus FastAPI Instrumentator for exposing metrics
Despite following the standard setup for Prometheus FastAPI Instrumentator, the metrics from the Temporal Python SDK are appearing under the different port and endpoint.

Is there a recommended solution or best practice for achieving this integration so that all metrics, including those from the Temporal Python SDK, are available under a single endpoint?

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-05-07 12:46:21.000 UTC</summary>

> Is there a recommended solution or best practice for achieving this integration so that all metrics, including those from the Temporal Python SDK, are available under a single endpoint?

Yes. Our metrics are emitted from our Rust-based Core. However, you can create a global `temporalio.runtime.Runtime` that you pass to every client (or even better, that you pass to `temporalio.runtime.Runtime.set_default` to be the proper default). Usually people create this with telemetry metrics configured for OpenTelemetry or Prometheus, but there is an advanced metric buffer option. When creating the runtime, you can pass `telemetry` config that has `metrics` as a `temporalio.runtime.MetricBuffer` you create. Then you can repeatedly call `retrieve_updates` to continually get new metrics updates as they happen and do what you need with those metrics.

Feel free to ask general questions on our [Slack](https://t.mp/slack) instance in the `#python-sdk` channel, or our [forums](https://community.temporal.io/). Closing this issue as not an issue, but rather a question.

</details>


---

<a id="852"></a>

### #852: [Bug] Serializing Literal fails

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/852 |
| **State** | CLOSED |
| **Author** | brandonchinn178 (Brandon Chinn) |
| **Created** | 2025-04-29 21:45:14.000 UTC (8 months ago) |
| **Updated** | 2025-05-14 16:55:11.000 UTC |
| **Closed** | 2025-05-14 16:55:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Use a dataclass as activity input with a field of type `Literal["success", "failure"]`

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

When using the Literal type_hint, value_to_type shows an error

```
Unserializable type during conversion: typing.Literal['success', 'failure']
```

This seems to happen because temporal checks referential equality with Literal from typing_extensions, but typing_extensions doesnt reuse the one from typing unless 3.10+

Workaroundable by using Literal from typing_extensions, but this isnt documented. It would be nice if temporal just used the normal Literal from typing (and perhaps replicate the deduplication logic in 3.10+)

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

```
python3.9 -m venv venv
venv/bin/pip install temporalio
venv/bin/python

from temporalio.converter import value_to_type
from typing import *
value_to_type(Literal["a", "b"], "a")
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-04-30 14:53:54.000 UTC</summary>

Yes, this is a bug as we need to also check the stdlib `Literal ` instead of just `typing_extensions`. Thanks for reporting, we will fix.

> and perhaps replicate the deduplication logic in 3.10+

For our use at https://github.com/temporalio/sdk-python/blob/e3603989f081bb812d883189be5f5ac9aa127089/temporalio/converter.py#L1426-L1430 the dedupe isn't really needed I think since we just do a validation check on the collection (regardless of whether it's in there multiple times) before accepting the raw value.

</details>


---

<a id="851"></a>

### #851: [Bug] Test server in time-skipping mode does return userMetadata.summary

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/851 |
| **State** | OPEN |
| **Author** | yunmanger1 (German Ilin) |
| **Created** | 2025-04-25 22:44:24.000 UTC (8 months ago) |
| **Updated** | 2025-04-28 13:07:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I want to leverage `userMetadata.summary` and show it in UI, and test the functionality.

### Describe the bug

My tests fail when running tests with test server in time-skipping mode.
I am passing `summary` attribute when executing workflow and activities. Then I query workflow history I can see `userMetadata.summary` populated on the `HistoryEvent` when running in dev mode, but it is missing in time-skipping mode. 

### Minimal Reproduction

```
@workflow.defn
class WorkflowSample:

    @workflow.run
    async def run(self, name: str) -> list[str]:
        result = await workflow.execute_activity(ActivitySample.greeting, name, summary=f"Calculating greeting for {name}")
        return [result]
```

```
    async def get_payload_value(self, *args: Payload | None) -> str | None:
        decoded_summary = await self.data_converter.decode(non_empty_payloads(*args))
        if len(decoded_summary) == 0:
            return None
        return decoded_summary[0]

    async def get_summary(self, history_event: HistoryEvent) -> str | None:
        logger.debug("Getting summary for event %s", MessageToJson(history_event))
        return await self.get_payload_value(
            history_event.user_metadata.summary)
    
    async def run(self):
        async for event in workflow_handle.fetch_history_events(wait_new_event=wait_new_event):
            summary = await self.get_summary(event)
            print(summary)
```

### Environment/Versions

- OS and processor: M1 Mac
- Temporal Version:  1.10.0



#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-04-28 13:05:26.000 UTC</summary>

Hrmm, this was added in https://github.com/temporalio/sdk-java/pull/2441. We will test and confirm. In the meantime, can you clear out any `temporal-` prefixed files in `$TMPDIR` and try again and confirm this is still failing?

(transferring to SDK)

</details>


---

<a id="850"></a>

### #850: [Bug] _RestrictedProxy does not invoke __format__ correctly when called on class

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/850 |
| **State** | CLOSED |
| **Author** | brandonchinn178 (Brandon Chinn) |
| **Created** | 2025-04-26 18:53:23.000 UTC (8 months ago) |
| **Updated** | 2025-04-27 03:28:14.000 UTC |
| **Closed** | 2025-04-27 03:28:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

*Note: this issue came about because we implemented our own `from_payload` from scratch. I just rewrote the code to reuse JSONPlainPayloadConverter and just define our own `custom_type_converters` instead of reimplementing the whole thing. So this doesn't affect us anymore, but it's still a confusing error to get*

We're defining our own custom `from_payload` function that deserializes according to the `type_hint`. If the `type_hint` isn't supported, we throw an error like
```py
raise ValueError(f"{val} does not match {type_hint}")
```

When using a dataclass with a `datetime` field, this errors with
```
TypeError: descriptor '__format__' for 'datetime.date' objects doesn't apply to a 'str' object
```

Related: #301

### Describe the bug

Normally, running
```py
from datetime import datetime
format(datetime, "")
```
will desugar as `type(datetime).__format__(datetime, "")`, where `type(datetime)` is `type` and `type.__format__` just calls `str()`.

But when `datetime` is a `_RestrictedProxy`, `format(proxy_datetime, "")` will run `_RestrictedProxy.__format__(proxy_datetime, "")`, which will run `real_datetime.__format__("")`, which throws the above error.

Current workaround is to explicitly convert to str like `f"{type_hint!s}"`, but the error is very confusing

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (1)

<details>
<summary><strong>brandonchinn178</strong> commented on 2025-04-27 03:28:13.000 UTC</summary>

Whoops I guess this was fixed in 1.10
https://github.com/temporalio/sdk-python/pull/757#discussion_r1949629666

Sorry, we're on an old version because we're still on Python 3.8. Didn't think to check newer versions.

</details>


---

<a id="839"></a>

### #839: [Feature Request] Warn when the server-provided task start time is fairly different than time SDK begins processing the task

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/839 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-04-22 12:22:01.000 UTC (8 months ago) |
| **Updated** | 2025-09-23 20:07:57.000 UTC |
| **Closed** | 2025-09-23 20:07:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/616


#### Comments (1)

<details>
<summary><strong>Sushisource</strong> commented on 2025-09-23 20:07:57.000 UTC</summary>

We're not gonna do this. Too niche.

</details>


---

<a id="816"></a>

### #816: [Feature Request] Upgrade PyO3 to latest and use pyo3-async-runtimes instead of pyo3-asyncio

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/816 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-04-03 15:07:48.000 UTC (9 months ago) |
| **Updated** | 2025-06-12 16:32:11.000 UTC |
| **Closed** | 2025-06-12 16:32:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

https://github.com/PyO3/pyo3-async-runtimes is now available as a successor to https://github.com/awestlake87/pyo3-asyncio, we should try to use it. This also may solve #300.

Note at https://github.com/temporalio/sdk-python/blob/92b77589204a0733f49b5d081942fc9303e436c4/temporalio/bridge/src/runtime.rs#L366-L408 we had to copy over some pyo3-asyncio code because it only supported a spawning from a global Tokio runtime and we needed to spawn from the current one. Make sure that pyo3-async-runtimes can handle this properly.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-04-03 15:11:21.000 UTC</summary>

See https://community.temporal.io/t/support-for-graalpy/17029 for context on why this was created. Also see related issue at https://github.com/oracle/graalpython/issues/494 that encouraged this to be opened.

</details>


---

<a id="814"></a>

### #814: [Bug] Python client not able to connect to self-hosted Temporal server via proxy using authorization header

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/814 |
| **State** | OPEN |
| **Author** | dipali-bhatt |
| **Created** | 2025-04-03 01:24:22.000 UTC (9 months ago) |
| **Updated** | 2025-04-03 13:14:32.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->
Weâ€™re trying to connect Python client to our self-hosted Temporal server via a proxy using an authorization header.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->
We see that it is able to connect to the worker at startup, but after exactly 60 seconds, it fails with the below error. Note that we do see some transient 503 errors while connecting to the server; but after that itâ€™s able to connect successfully (verified in server logs).

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->
Hereâ€™ the set of errors we see:
1.
```
Error: Client connection not established Failed client connect: `get_system_info` call error after connection: Status { code: PermissionDenied, message: "encountered c2c service api error response 503", metadata: MetadataMap { headers: {"content-type": "application/grpc", "date": "Wed, 19 Mar 2025 02:11:39 GMT"} }, source: None }.
Traceback (most recent call last):
  File "/home/temporal/temporal_sample_worker_python/src/run_worker.py", line 202, in <module>
    asyncio.run(main())
  File "/usr/lib64/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib64/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/home/temporal/temporal_sample_worker_python/src/run_worker.py", line 157, in main
    await conn.create_client()
  File "/home/temporal/temporal_sample_worker_python/src/utils.py", line 105, in create_client
    self.client = await Client.connect("dev.api.data.falcon.sfdcfc.net:7443",
  File "/usr/local/lib64/python3.9/site-packages/temporalio/client.py", line 176, in connect
    await temporalio.service.ServiceClient.connect(connect_config),
  File "/usr/local/lib64/python3.9/site-packages/temporalio/service.py", line 209, in connect
    return await _BridgeServiceClient.connect(config)
  File "/usr/local/lib64/python3.9/site-packages/temporalio/service.py", line 1117, in connect
    await client._connected_client()
  File "/usr/local/lib64/python3.9/site-packages/temporalio/service.py", line 1130, in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
  File "/usr/local/lib64/python3.9/site-packages/temporalio/bridge/client.py", line 97, in connect
    await temporalio.bridge.temporal_sdk_bridge.connect_client(
RuntimeError: Failed client connect: `get_system_info` call error after connection: Status { code: PermissionDenied, message: "encountered c2c service api error response 503", metadata: MetadataMap { headers: {"content-type": "application/grpc", "date": "Wed, 19 Mar 2025 02:11:39 GMT"} }, source: None }
```
2.
```
2025-03-19T02:17:48.974543Z  WARN temporal_sdk_core::worker::workflow::wft_poller: Error while polling for workflow tasks error=Status { code: Unimplemented, metadata: MetadataMap { headers: {"content-type": "application/grpc", "date": "Wed, 19 Mar 2025 02:17:48 GMT"} }, source: None }
2025-03-19T02:17:48.974547Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: Unimplemented, metadata: MetadataMap { headers: {"content-type": "application/grpc", "date": "Wed, 19 Mar 2025 02:17:48 GMT"} }, source: None })
2025-03-19T02:17:48.974551Z ERROR temporal_sdk_core::worker::workflow::workflow_stream: Workflow processing encountered fatal error and must shut down TonicError(Status { code: Unimplemented, metadata: MetadataMap { headers: {"content-type": "application/grpc", "date": "Wed, 19 Mar 2025 02:17:48 GMT"} }, source: None })
2025-03-19 02:17:48,975 - ERROR - Worker failed, shutting down
Traceback (most recent call last):
  File "/usr/local/lib64/python3.9/site-packages/temporalio/worker/_workflow.py", line 154, in run
    act = await self._bridge_worker().poll_workflow_activation()
  File "/usr/local/lib64/python3.9/site-packages/temporalio/bridge/worker.py", line 147, in poll_workflow_activation
    await self._ref.poll_workflow_activation()
RuntimeError: Poll failure: Unhandled grpc error when polling: Status { code: Unimplemented, metadata: MetadataMap { headers: {"content-type": "application/grpc", "date": "Wed, 19 Mar 2025 02:17:48 GMT"} }, source: None }

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib64/python3.9/site-packages/temporalio/worker/_workflow.py", line 164, in run
    raise RuntimeError("Workflow worker failed") from err
RuntimeError: Workflow worker failed
2025-03-19 02:17:48,975 - INFO - Beginning worker shutdown, will wait 0:00:00 before cancelling activities

Traceback (most recent call last):
  File "/usr/local/lib64/python3.9/site-packages/temporalio/worker/_workflow.py", line 154, in run
    act = await self._bridge_worker().poll_workflow_activation()
  File "/usr/local/lib64/python3.9/site-packages/temporalio/bridge/worker.py", line 147, in poll_workflow_activation
    await self._ref.poll_workflow_activation()
RuntimeError: Poll failure: Unhandled grpc error when polling: Status { code: Unimplemented, metadata: MetadataMap { headers: {"content-type": "application/grpc", "date": "Wed, 19 Mar 2025 02:17:48 GMT"} }, source: None }

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/temporal/temporal_sample_worker_python/src/run_worker.py", line 202, in <module>
    asyncio.run(main())
  File "/usr/lib64/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib64/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/home/temporal/temporal_sample_worker_python/src/run_worker.py", line 179, in main
    await run_worker(client, taskqueue_name, args.worker_type)
  File "/home/temporal/temporal_sample_worker_python/src/run_worker.py", line 142, in run_worker
    await worker.run()
  File "/usr/local/lib64/python3.9/site-packages/temporalio/worker/_worker.py", line 571, in run
    raise exception
  File "/usr/local/lib64/python3.9/site-packages/temporalio/worker/_workflow.py", line 164, in run
    raise RuntimeError("Workflow worker failed") from err
RuntimeError: Workflow worker failed
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- Temporal Version: Server v1.26.2, SDK v1.10.0

### Additional context

<!-- Add any other context about the problem here. -->
Our issue looks similar to this this open bug: [[Bug] Worker hangs after polling workflow task queue Â· Issue #631 Â· temporalio/sdk-python Â· GitHub](https://github.com/temporalio/sdk-python/issues/631)
Wanted to check if it is same issue and if not we need help in understanding the fixing it.


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-04-03 13:12:54.000 UTC</summary>

Can you confirm that this is not an issue when connecting directly to Temporal and therefore an issue specific to your proxy? What kind of proxy is this?

Also note that the proxy has to be setup with the proper parameters, specifically it should allow individual calls for more than 80s, have a soft max connection age (i.e. http/2 go away) of no less than 6m and hard mas connection age (i.e. connection kill) of 8m. This is because on Temporal server we allow 70s, 5m, and 6m10s for these values respectively so the proxy needs to not intrude on that. The other issue was also an improper ingress/proxy configuration issue where it was failing requests that need to stay open for 70s.

</details>


---

<a id="803"></a>

### #803: Run different worker types in separate threads

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/803 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2025-03-27 20:44:35.000 UTC (9 months ago) |
| **Updated** | 2025-03-28 14:06:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently, when workflow and activity workers are run in the same process, they share the the same thread (same event loop):

https://github.com/temporalio/sdk-python/blob/b0dfaef0f029244a44f56c4c1970cda8621b4e68/temporalio/worker/_worker.py#L489-L492

Running these (and Nexus workers when they arrive) in different threads would prevent an async activity that incorrectly does blocking I/O from blocking workflow progress. In addition to limiting blast radius it should make it easier to understand the failure modes.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-03-28 13:48:56.000 UTC</summary>

This is going to be tough to do. Notes:

* We have to be able to use the primary/user-controlled event loop for running (some) activities/operations anyways, so maybe this is a workflow worker only thing?
* Workflow workers will have to remove async and go threaded which will cost threads. There is the blocking call for polling (currently async, so not using Python thread). Then the code backgrounds the work when received which uses async today. And then _that_ work uses a thread on a pool so that it can impose a deadlock timeout on it. This means every workflow activation uses 1 thread from the thread pool today. We either have to double the in-use thread count or run a single thread for the life of the workflow worker that runs its own asyncio event loop so it can background activations as received (not to be confused with the next point).
* Workflow workers will always need to use primary/user-controlled event loop for user codecs anyways (e.g. via `asyncio.run_coroutine_threadsafe`) which means it is subject to the same blocking concerns. So any benefits are only for those not using codecs.

We will have to decide whether the benefit is worth the costs here.

</details>


---

<a id="779"></a>

### #779: [Feature Request] Adding type checking on workflow_execute activity when a variable number of positional args exist in the callable activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/779 |
| **State** | CLOSED |
| **Author** | spacether (Justin Black) |
| **Created** | 2025-03-01 03:58:56.000 UTC (10 months ago) |
| **Updated** | 2025-03-10 13:50:51.000 UTC |
| **Closed** | 2025-03-09 03:40:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

How about adding type checking on workflow_execute activity when a variable number of positional args exist in the callable activity?
Right now the args are not type checked, Sequence[Any] is allowed in

### Describe the solution you'd like

Use TypedVarTupe to define *args, delete the arg input.

### Additional context

I got this working with this code which would go in the workflow module
```
Ts = typing.TypeVarTuple("Ts")

async def execute_activity(
    activity: Union[
        typing.Callable[[*Ts], ReturnType],
        typing.Callable[[], ReturnType],
    ],
    *args: *Ts,
    task_queue: Optional[str] = None,
    result_type: Optional[Type] = None,
    schedule_to_close_timeout: Optional[timedelta] = None,
    schedule_to_start_timeout: Optional[timedelta] = None,
    start_to_close_timeout: Optional[timedelta] = None,
    heartbeat_timeout: Optional[timedelta] = None,
    retry_policy: Optional[temporalio.common.RetryPolicy] = None,
    cancellation_type: ActivityCancellationType = ActivityCancellationType.TRY_CANCEL,
    activity_id: Optional[str] = None,
    versioning_intent: Optional[VersioningIntent] = None,
) -> ReturnType:
```

Pros:
- this works and enforces type checking for one or more positional argument inputs

Cons:
- it only works for Python 3.11 or 3.12 and higher
- it is a breaking change and I think requires remove of the arg argument in workflow.execute_activity

See https://stackoverflow.com/questions/79476595/is-it-possible-to-type-hint-a-callable-that-takes-positional-arguments-only-usin

### Other Apllications
- a fix like this would be great for signals, updates, and queries too


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-03-07 12:15:51.000 UTC</summary>

> it only works for Python 3.11 or 3.12 and higher [...] it is a breaking change and I think requires remove of the arg argument in workflow.execute_activity

These limitations unfortunately mean we cannot do them. But we suggest only using single-argument items anyways as dataclasses that can get default fields added to them in a compatible manner. This is why our type-safe overloads only focus on no-param/single-param items across the codebase.

Reactions: ðŸ‘ 1

</details>


---

<a id="778"></a>

### #778: [Bug] strange workflow task timeout

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/778 |
| **State** | OPEN |
| **Author** | hebrd (hebrd) |
| **Created** | 2025-03-01 03:39:57.000 UTC (10 months ago) |
| **Updated** | 2025-03-07 12:19:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I've set the activity timeout to 3600 seconds in the settings. Most tasks complete normally, but some are not polled and executed by the worker until several hours later.

Below is the event historyâ€”I'm unsure what happened. Could anyone help diagnose this issue?

Thanks in advance!

![Image](https://github.com/user-attachments/assets/5d6b5d7f-e2b4-44da-8ebc-082efe988188)

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-03-07 12:18:27.000 UTC</summary>

Workflow task timeout means something worker side is preventing that workflow task from responding within 10 seconds. It could be any number of things (e.g. blocking the thread in `async def` functions somewhere in the system such as activities). Event history alone cannot give us enough information about the code. If at all possible to provide a small, standalone replication (even if it has to be run several times to see it or be put in some kind of loop), that could help.

Also, you can use our traditional support methods instead of GitHub if you'd like: https://temporal.io/community.

</details>


---

<a id="776"></a>

### #776: [Feature Request] Implement run_in_executor on _WorkflowInstanceImpl

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/776 |
| **State** | CLOSED |
| **Author** | funkindy (Alexey Trofimov) |
| **Created** | 2025-02-26 09:37:07.000 UTC (10 months ago) |
| **Updated** | 2025-02-26 18:10:27.000 UTC |
| **Closed** | 2025-02-26 18:10:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I am implementing async logging inside my workflow. I use structlog lib async methods which use thread executor for default logging methods (.info -> await .ainfo).

The problem now is that run_in_executor is not implemented on Temporal's event loop, so invoking `await asyncio.get_running_loop().run_in_executor(...)` in structlog's internals causes NotImplementedError


Is it possible to implement  `_WorkflowInstanceImpl.run_in_executor`, or is it pointless on python-SDK architecture?

Thank you.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-02-26 13:48:41.000 UTC</summary>

The async loop in workflows is a custom deterministic loop with no thread capabilities by intention, because threads are non-deterministic.

If you need to violate the determinism rules to issue log statements, you cannot use the deterministic loop to do it, you will have to take the same approach as calling your asyncio loop from outside the loop in any other fashion. This is usually `asyncio.run_coroutine_threadsafe(my_async_call(), my_primary_loop).result()`.

But what is the use case for _async_  logging from inside a workflow instead of synchronous logging? Also, you may consider replacing the base logger of `workflow.logger` (i.e. `workflow.logger.logger`) with this other equivalent and use `workflow.logger`, or at the least, make sure you don't log during replay in your logger (meaning check `not workflow.unsafe.is_replaying()`).

Reactions: â¤ï¸ 1

</details>


---

<a id="765"></a>

### #765: [Feature Request] Allow failures to have the traditional pre-3.11 traceback with less information

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/765 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-02-12 21:58:45.000 UTC (11 months ago) |
| **Updated** | 2025-04-03 20:41:35.000 UTC |
| **Closed** | 2025-04-03 20:41:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Failure sizes are limited server side. Since Python 3.11, https://peps.python.org/pep-0657/ has been adding a bunch of extra stuff to tracebacks like the code that is failing and the ascii underlines of where. This bloats the traceback considerably causing it to be more likely to overflow the server side limit.

We should allow people to not add this extra data. Unfortunately a quick glance seems to show this cannot be done on a case-by-case basis, the setting is global. So we may have to walk the frames and build our own stack trace. The implementer will have to research this and will have to decide whether this is opt-in behavior or opt-out.

This is separate but related to the feature to partially truncate stack traces at https://github.com/temporalio/features/issues/597.


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-04-03 20:41:35.000 UTC</summary>

After research, this just adds `^` and `~` under code lines which we don't believe contributes _too_ significantly to stack trace size. If this is a concern for users, per https://docs.python.org/3/whatsnew/3.11.html#whatsnew311-pep657, they can set `PYTHONNODEBUGRANGES` env var or `-X no_debug_ranges` cmd arg.

Closing as won't do.

</details>


---

<a id="731"></a>

### #731: [Feature Request] Special behavior for Temporal built-in prefixes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/731 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-01-15 13:14:09.000 UTC (11 months ago) |
| **Updated** | 2025-01-21 18:13:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/576

#### Comments (1)

<details>
<summary><strong>josh-berry</strong> commented on 2025-01-21 18:13:34.000 UTC</summary>

Note: double-underscores have some special meaning in Python for method and property names on a class. Specifically, Python munges `__foo` -> `_ClassName__foo` (or something like that, I don't exactly remember).

We don't think this is relevant, because we're only reserving well-known/already-established names, and we think `__temporal_` is specific enough that it's unlikely to collide with any class name.

</details>


---

<a id="721"></a>

### #721: [Feature Request] Provide client from activity context

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/721 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-01-07 14:33:33.000 UTC (1 years ago) |
| **Updated** | 2025-07-16 13:16:04.000 UTC |
| **Closed** | 2025-07-16 13:16:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Allow users to access Temporal client from within an activity. This is likely a `client() -> Client` function in the `activity` module. See https://github.com/temporalio/features/issues/203.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-06-27 15:16:51.000 UTC</summary>

I am reopening this as I believe it got inadvertently closed. There is a pending PR on this at #740

</details>


---

<a id="719"></a>

### #719: [Feature Request] Provide "workflow local"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/719 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-01-07 13:47:42.000 UTC (1 years ago) |
| **Updated** | 2025-07-24 15:57:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Users need a workflow-local var that is scoped only to the current workflow when accessed via code. See https://github.com/temporalio/features/issues/571.

Rough sketch: a user can have `myValue = workflow.LocalVar("myValue")`, and all methods and typesafety and such work like `contextvars.ContextVar`.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2025-07-24 15:57:07.000 UTC</summary>

Note, this can already be done be users using `setattr`/`getattr` on `workflow.instance()`.

</details>


---

<a id="699"></a>

### #699: [Bug] `workflow.wait` broken with `asyncio.FIRST_COMPLETED` and local activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/699 |
| **State** | OPEN |
| **Author** | andmis (Andrey Mishchenko) |
| **Created** | 2024-12-09 19:04:53.000 UTC (1y 1m ago) |
| **Updated** | 2024-12-09 20:34:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

Start N local activities, wait for the first one to finish, cancel the others.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

If I start N local activities, then use `workflow.wait(tasks, return_when=asyncio.FIRST_COMPLETED)`, the `workflow.wait` coro blocks until all of the activities actually finish, instead of returning when the first one finishes. If I use non-local activities, this bug does not happen.

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

https://github.com/andmis/snippets/tree/temporal-python-sdk-local-activity-workflow-wait-bug

```
# Worker output after `python run_workflow.py`
2024-12-09 19:02:30.750775+00:00 (Workflow) Starting, local=False
2024-12-09 19:02:30.750775+00:00 (Workflow) Waiting for first completed activity
2024-12-09 11:02:30.760073 (Activity 0) Running sandbox_activity
2024-12-09 11:02:30.760565 (Activity 1) Running sandbox_activity
2024-12-09 11:02:32.875597 (Activity 1) Exiting sandbox_activity
2024-12-09 19:02:32.891753+00:00 (Workflow) Got first completed activity, cancelling others and exiting
2024-12-09 19:02:32.891753+00:00 (Workflow) Exiting
2024-12-09 11:02:35.328912 (Activity 0) Cancelling sandbox_activity

# Worker output after `python run_workflow.py -l`
2024-12-09 19:03:48.421271+00:00 (Workflow) Starting, local=True
2024-12-09 19:03:48.421271+00:00 (Workflow) Waiting for first completed activity
2024-12-09 11:03:48.425911 (Activity 0) Running sandbox_activity
2024-12-09 11:03:48.426165 (Activity 1) Running sandbox_activity
2024-12-09 11:03:49.817020 (Activity 1) Exiting sandbox_activity
2024-12-09 11:03:56.397142 (Activity 0) Exiting sandbox_activity
2024-12-09 19:03:56.394210+00:00 (Workflow) Got first completed activity, cancelling others and exiting
2024-12-09 19:03:56.394210+00:00 (Workflow) Exiting
# Note the timestamps
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: macOS, M1
- Temporal version: 1.1.2
- Python SDK version: 1.8.0
- Are you using Docker or Kubernetes or building Temporal from source? No

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-12-09 20:34:46.000 UTC</summary>

Thanks! We will investigate.

</details>


---

<a id="655"></a>

### #655: [Bug] KeyError: 'warnings' due to sandbox

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/655 |
| **State** | OPEN |
| **Author** | leonard-henriquez (LÃ©onard Henriquez) |
| **Created** | 2024-09-29 08:46:03.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-30 13:07:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## What are you really trying to do?

Use Sentry with Temporal to track and manage warnings within workflows.

## Describe the bug

Sentryâ€™s catch_warnings context manager, which relies on the standard Python warnings module, raises a KeyError in Temporal workflows because the warnings module is not available. This happens in environments managed by Temporal, which restricts access to certain standard Python modules like warnings.

This restriction leads to a failure when using libraries like Sentry that depend on this module for capturing and managing Python warnings.

Error Example:
```
KeyError: 'warnings'
```

Cause: The warnings module is not present or accessible within the Temporal workflow environment, causing the code to fail when it tries to reference sys.modules['warnings'].

## Minimal Reproduction

To reproduce this issue:

	1.	Set up a Temporal workflow in Python.
	2.	Integrate Sentryâ€™s catch_warnings context manager or any other library that uses the warnings module.
	3.	Trigger a Python warning within the workflow (e.g., by using warnings.warn()).
	4.	The workflow fails with a KeyError: 'warnings' due to the restricted access.

Hereâ€™s a minimal code snippet:

```
def workflow_function():
    with warnings.catch_warnings(record=True) as w:
        warnings.warn("This is a test warning")
```

## Environment/Versions
	â€¢	OS and processor: MacBook Pro M1
	â€¢	Temporal Version: 1.7.1
	â€¢	Are you using Docker or Kubernetes?: Using Docker to run Temporal

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-09-30 13:07:24.000 UTC</summary>

> 2.	Integrate Sentryâ€™s catch_warnings context manager or any other library that uses the warnings module.

You're going to have to pass through the imports and disable the sandbox I believe. `warnings` is not a determinism-safe module (and Sentry likely isn't either). Using non-deterministic modules like Sentry require pass through and sandbox avoidance. https://github.com/temporalio/sdk-python?tab=readme-ov-file#workflow-sandbox explains how the sandbox works and how to disable for parts of code. Similar pass through and avoidance techniques are applied in our Sentry interceptor in the sample at https://github.com/temporalio/samples-python/tree/main/sentry.

</details>


---

<a id="654"></a>

### #654: MultiProcess executor not working as expected

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/654 |
| **State** | CLOSED |
| **Author** | giuliohome (Giulio) |
| **Created** | 2024-09-28 11:07:33.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-30 13:08:10.000 UTC |
| **Closed** | 2024-09-28 11:21:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

This is a follow up to [this message](https://github.com/temporalio/sdk-go/issues/1651#issuecomment-2377474385), in particular I'm simply running this official sample https://github.com/temporalio/samples-python/blob/main/hello/hello_activity_multiprocess.py


### Describe the bug

I see this output

```
(.venv) C:\Users\giulio365cloud\dev-test>python multiprocessor_giulio.py
Heartbeating activity on PID 5104
Heartbeating activity on PID 5104
Heartbeating activity on PID 5104
Heartbeating activity on PID 5104
Heartbeating activity on PID 5104
Heartbeating activity on PID 5104
Result on PID 7072: Hello, World from activity on PID 5104! and then Hello, World from activity on PID 5104!
```

**I would have expected to see a different PID when the activity runs the second time!**

### Minimal Reproduction

I have taken the official sample and simply repeated the activity execution two times inside the workflow

```py
@workflow.defn
class GreetingWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        ret1 = await workflow.execute_activity(
            compose_greeting,
            ComposeGreetingInput("Hello", name),
            start_to_close_timeout=timedelta(seconds=10),
            # Always set a heartbeat timeout for long-running activities
            heartbeat_timeout=timedelta(seconds=2),
        )
        ret2 = await workflow.execute_activity(
            compose_greeting,
            ComposeGreetingInput("Hello", name),
            start_to_close_timeout=timedelta(seconds=10),
            # Always set a heartbeat timeout for long-running activities
            heartbeat_timeout=timedelta(seconds=2),
        )
        return ret1 + " and then " + ret2
```
and I have also modified the activity to print its PID in the hello message

```py
@activity.defn
def compose_greeting(input: ComposeGreetingInput) -> str:
    # We'll wait for 3 seconds, heartbeating in between (like all long-running
    # activities should do), then return the greeting
    for _ in range(0, 3):
        print(f"Heartbeating activity on PID {os.getpid()}")
        activity.heartbeat()
        time.sleep(1)
    return f"{input.greeting}, {input.name} from activity on PID {os.getpid()}!"
```

### Environment/Versions

- OS and processor:  Windows 11 Enterprise 16 vCPU 32 GB RAM
Device name	Windows11Pro
Processor	Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz   2.79 GHz
Installed RAM	32.0 GB
System type	64-bit operating system, x64-based processor

![image](https://github.com/user-attachments/assets/140d720d-a801-436b-9e77-6025f4ab1736)

- Temporal Version: Latest temporal version, latest SDK version
temporal server start-dev
CLI 1.1.0 (Server 1.25.0, UI 2.30.3)

- Are you using Docker or Kubernetes or building Temporal from source? Nope


#### Comments (1)

<details>
<summary><strong>giuliohome</strong> commented on 2024-09-28 11:21:52.000 UTC</summary>

Uhm ... I see... they need to run in parallel

```
@workflow.defn
class GreetingWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        task1 = asyncio.create_task( workflow.execute_activity(
            compose_greeting,
            ComposeGreetingInput("Hello", name),
            start_to_close_timeout=timedelta(seconds=10),
            # Always set a heartbeat timeout for long-running activities
            heartbeat_timeout=timedelta(seconds=2),
        ))
        task2 = asyncio.create_task( workflow.execute_activity(
            compose_greeting,
            ComposeGreetingInput("Hello", name),
            start_to_close_timeout=timedelta(seconds=10),
            # Always set a heartbeat timeout for long-running activities
            heartbeat_timeout=timedelta(seconds=2),
        ) )
        ret1 = await task1
        ret2 = await task2
        return ret1 + " and then " + ret2
```
in that case

```
python multiprocessor_giulio.py
Heartbeating activity on PID 4952
Heartbeating activity on PID 5904
Heartbeating activity on PID 4952
Heartbeating activity on PID 5904
Heartbeating activity on PID 4952
Heartbeating activity on PID 5904
Result on PID 11484: Hello, World from activity on PID 4952! and then Hello, World from activity on PID 5904!
```

</details>


---

<a id="652"></a>

### #652: [Bug] `ScheduleOverlapPolicy` has broken __eq__

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/652 |
| **State** | OPEN |
| **Author** | andersio (Anders Ha) |
| **Created** | 2024-09-27 08:53:30.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-27 12:08:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

In a deployment script, compare a `Schedule` on the server to a proposed `Schedule`, and skip schedule updates as needed.

### Describe the bug

When comparing two `Schedule`s that differ only in their `ScheduleOverlapPolicy`, `Schedule.__eq__` returns true.

This seems to be caused by `ScheduleOverlapPolicy`, where every enum case apparently equals to any cases according to `ScheduleOverlapPolicy.__eq__`, even though the underlying values are different.

For some reason, `ScheduleOverlapPolicy` is both a dataclass and an IntEnum. Seems like the rare combination might have caused the behaviour here?

### Minimal Reproduction

```python
from temporalio.client import ScheduleOverlapPolicy

print(ScheduleOverlapPolicy.SKIP == ScheduleOverlapPolicy.TERMINATE_OTHER)
# console: True

print(int(ScheduleOverlapPolicy.SKIP) == int(ScheduleOverlapPolicy.TERMINATE_OTHER))
# console: False

print(ScheduleOverlapPolicy.SKIP.value == ScheduleOverlapPolicy.TERMINATE_OTHER.value)
# console: False
```

### Environment/Versions

Python 3.11.7

```
 name         : temporalio             
 version      : 1.7.0                  
 description  : Temporal.io Python SDK 

dependencies
 - protobuf >=3.20
 - types-protobuf >=3.20
 - typing-extensions >=4.2.0,<5.0.0
```

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-09-27 12:08:07.000 UTC</summary>

> For some reason, ScheduleOverlapPolicy is both a dataclass and an IntEnum

This was a mistake on our part and was fixed in #628 and therefore the fix is in 1.7.1. Can you confirm?

</details>


---

<a id="632"></a>

### #632: [Feature Request] OpenTelemetry export fails over TLS

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/632 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-09-03 12:33:07.000 UTC (1y 4m ago) |
| **Updated** | 2024-09-11 14:31:20.000 UTC |
| **Closed** | 2024-09-11 14:31:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸŽ‰ 1 |

#### Description

### Describe the solution you'd like

Reported and fixed at https://github.com/temporalio/sdk-core/issues/801, this is for tracking updating core to fix it.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-09-11 14:31:20.000 UTC</summary>

This should now be fixed with the 1.7.1 release

</details>


---

<a id="629"></a>

### #629: [Bug] workflow.upsert_search_attributes() will bork SDK on empty inputs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/629 |
| **State** | OPEN |
| **Author** | irdbl (Kevin Pham) |
| **Created** | 2024-09-02 22:14:28.000 UTC (1y 4m ago) |
| **Updated** | 2024-09-03 13:02:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

In the middle of a workflow, if you call `upsert_search_attributes` with an empty input:

```python
empty = []
workflow.upsert_search_attributes(empty)
```

You'll get a completely inscrutable and hard to debug error like so:

```
Failed completing activation on workflow with run ID 27fa7d71-76a8-4b3b-be33-1e0d11afc5a4
Traceback (most recent call last):
  File ".../.venv/lib/python3.12/site-packages/temporalio/worker/_workflow.py", line 320, in _handle_activation
    await self._bridge_worker().complete_workflow_activation(completion)
  File ".../.venv/lib/python3.12/site-packages/temporalio/bridge/worker.py", line 117, in complete_workflow_activation
    await self._ref.complete_workflow_activation(comp.SerializeToString())
RuntimeError: Completion failure: Lang SDK sent us a malformed workflow completion for run (27fa7d71-76a8-4b3b-be33-1e0d11afc5a4): At least one workflow command in the completion contained an empty variant
```

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-09-03 13:02:51.000 UTC</summary>

Saw the attached PR, thanks!

</details>


---

<a id="625"></a>

### #625: [Feature Request] Fix pyright type-checking violations

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/625 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-08-27 17:24:59.000 UTC (1y 4m ago) |
| **Updated** | 2025-03-25 15:07:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

#420 started requiring pyright in CI, but with a substantial exclusion list. Fix violations and remove exclusion list.

#### Comments (1)

<details>
<summary><strong>dandavison</strong> commented on 2025-03-25 15:07:51.000 UTC</summary>

E.g. https://github.com/temporalio/sdk-python/issues/795

</details>


---

<a id="624"></a>

### #624: [Bug] RPCStatusCode.ALREADY_EXISTS can be swallowed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/624 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-08-26 16:35:30.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-27 16:35:52.000 UTC |
| **Closed** | 2024-08-27 16:35:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

RPCStatusCode.ALREADY_EXISTS can be swallowed due to an unhandled branch of execution here:

https://github.com/temporalio/sdk-python/blob/927abdcf923a803da9cb7202c0dcd410561e6ef2/temporalio/client.py#L5086-L5093

#### Comments (1)

<details>
<summary><strong>YoungAgile</strong> commented on 2024-08-26 16:54:54.000 UTC</summary>

> Download \<redacted\>: changeme In the installer menu, select "gcc."

Virus, please do not click on the link

Reactions: ðŸ‘ 1

</details>


---

<a id="620"></a>

### #620: [Bug] Workflow cancellation can be swallowed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/620 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-08-26 11:55:11.000 UTC (1y 4m ago) |
| **Updated** | 2025-07-02 21:12:20.000 UTC |
| **Closed** | 2025-07-02 21:12:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

In Python if a cancellation is requested of a workflow with a short-lived activity, and that activity completes during cancellation (unsure if you have to set cancellation type as wait on the activity), the cancellation is effectively swallowed. This occurs for any Python asyncio where cancellation is used, if the awaiting thing swallows cancellation it's like it never happened.

This is probably just existing, unfortunate Python behavior we have to document. We need to at least confirm if it is only for wait-cancel cancellation types or try-cancel too.

#### Comments (1)

<details>
<summary><strong>lambyqq</strong> commented on 2024-08-26 17:17:57.000 UTC</summary>

Here's some test code that could reproduce this:
```
@activity.defn
async def short_activity_async():
   delay = random.uniform(0.05, 0.15)  # 50~150ms delay
   await asyncio.sleep(delay)


@activity.defn
def short_activity_sync():
   delay = random.uniform(0.05, 0.15)  # 50~150ms delay
   sleep(delay)


@workflow.defn
class ShortActivityWorkflow:
   @workflow.run
   async def run(self, total_seconds: float = 10.0):
       end = workflow.now() + timedelta(seconds=total_seconds)
       while True:
           await workflow.execute_activity(
               short_activity_async,
               schedule_to_close_timeout=timedelta(seconds=10))
           await workflow.execute_activity(
               short_activity_sync,
               schedule_to_close_timeout=timedelta(seconds=10))
           if workflow.now() > end:
               break


@pytest.mark.asyncio
async def test_workflow_cancellation():
   ...
   client: Client = ...
   async with Worker(
           client,
           task_queue='test-wf-cancellation-task-queue',
           workflows=[ShortActivityWorkflow],
           activities=[short_activity_async, short_activity_sync],
           activity_executor=ThreadPoolExecutor(max_workers=1)
   ):
       for i in range(10):
           wf_duration = random.uniform(5.0, 15.0)
           wf_handle = await client.start_workflow(
               ShortActivityWorkflow.run,
               id=f'short_activity_wf_id-{i}',
               args=[wf_duration],
               task_queue=test_worker_task_queue,
               execution_timeout=timedelta(minutes=1)
           )

           # Cancel wf
           await asyncio.sleep(1.0)
           await wf_handle.cancel()

           with pytest.raises(WorkflowFailureError) as err_info:
               await wf_handle.result()  # failed
           cause = err_info.value.cause
           assert isinstance(cause, CancelledError)
           assert cause.message == 'Workflow cancelled'
```

</details>


---

<a id="597"></a>

### #597: [Bug] Cant build wheel temporalio when installing with pip

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/597 |
| **State** | OPEN |
| **Author** | ichsansaid (Muhammad Al Ichsan Nur Rizqi Said) |
| **Created** | 2024-08-01 02:53:32.000 UTC (1y 5m ago) |
| **Updated** | 2024-08-05 16:25:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?
pip install temporalio
<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

### Describe the bug
![image](https://github.com/user-attachments/assets/820ca4e3-dddc-4744-886a-e55fde0bf3d4)
![image](https://github.com/user-attachments/assets/194ecca3-efe3-4470-a2f4-ca03f5709e23)
![image](https://github.com/user-attachments/assets/c29f9d50-d1a0-45a1-8274-3f066481fe6c)
When i installed the temporalio, these errors is comes up, i also already installed rust compiler in my windows but still error

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Windows
- Temporal Version: Latest
- Are you using Docker or Kubernetes or building Temporal from source? not using docker and kubernetes

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-08-05 16:24:50.000 UTC</summary>

For some reason your `pip` install is not using a prebuilt wheel. Can you show the output of `pip debug --verbose` (or at least the "Compatible tags" section)? If it is Windows, I wonder if it's Windows ARM or 32-bit, both of which we don't prebuild wheels for.

</details>


---

<a id="590"></a>

### #590: [Feature Request] Update schedule default values

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/590 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-07-22 16:08:53.000 UTC (1y 5m ago) |
| **Updated** | 2024-09-25 13:15:36.000 UTC |
| **Closed** | 2024-09-25 13:15:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Per https://github.com/temporalio/api/pull/308, https://github.com/temporalio/sdk-python/blob/3796ec3e4039b61eb79e143aee815c6448134d39/temporalio/client.py#L3579 should now be 1 year. Also, consider if we can have defaults be server-side defaults instead of client-side (changes types to make it seem like this value is optional when it's not). Will discuss with team.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-07-22 19:12:31.000 UTC</summary>

For now it was decided to just change this default value to match server default

</details>


---

<a id="583"></a>

### #583: Tweak error message on workflow cancellation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/583 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-07-17 15:13:27.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-17 15:40:24.000 UTC |
| **Closed** | 2024-07-17 15:40:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Workflow cancellation results in `WorkflowFailureError("workflow execution failed")`. It would make sense for that to be `WorkflowFailureError("workflow execution cancelled")`.

#### Comments (1)

<details>
<summary><strong>dandavison</strong> commented on 2024-07-17 15:40:24.000 UTC</summary>

Actually, this suggestion isn't appropriate: it makes sense that the outer message has a natural language string matching its class name; the cancellation exception is contained within the outer exception as the `__cause__` (exposed as `.cause`)

</details>


---

<a id="563"></a>

### #563: [Feature Request] Update test running to replay

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/563 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-06-27 13:10:49.000 UTC (1y 6m ago) |
| **Updated** | 2025-05-16 15:02:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

We need to replay our test workflows more. Today we don't replay nor do we run without cache, both of which could catch issues. At the least we need to try to auto-replay workflows that complete in tests.

#### Comments (1)

<details>
<summary><strong>avinashtanniru</strong> commented on 2025-05-16 15:02:18.000 UTC</summary>

There is a featue : https://docs.temporal.io/develop/python/testing-suite#replay But not sure how it works. 
I'm unable to test this. 

</details>


---

<a id="561"></a>

### #561: [Bug] Workflow Task Timeout only supports upto 2 minutes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/561 |
| **State** | CLOSED |
| **Author** | mrafay0 (Rafay Awan) |
| **Created** | 2024-06-25 16:35:08.000 UTC (1y 6m ago) |
| **Updated** | 2024-06-25 16:42:52.000 UTC |
| **Closed** | 2024-06-25 16:42:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Trying to set task timeout for a workflow.

### Describe the bug

Can be less than 2 minutes but not more than that, is it expected?
### Minimal Reproduction

```
result = await temporal_client.execute_workflow(
               ....
                task_timeout=timedelta(seconds=60*15), # gets set to 2 minute instead of 15 minutes
)
```

![2024-06-25 at 21 35 26](https://github.com/temporalio/sdk-python/assets/168713959/70bb8671-f6e9-44b1-b400-3f0fa75c69d7)



#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-06-25 16:42:52.000 UTC</summary>

This is the maximum timeout of this value as enforced on the server. It is unrelated to Python, see https://docs.temporal.io/encyclopedia/detecting-workflow-failures#workflow-task-timeout. Closing issue as by intention.

Note, it is usually a bad idea to change this value and if you're finding yourself changing it, there is likely something else wrong. Workflow tasks should only take milliseconds. Feel free to join us on #python-sdk in Slack at https://t.mp/slack or on the forums at https://community.temporal.io.

Reactions: ðŸ‘ 1

</details>


---

<a id="560"></a>

### #560: [Feature Request] Propagate changes made with updated Rust dependencies

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/560 |
| **State** | CLOSED |
| **Author** | kelkawi-a (Ali Kelkawi) |
| **Created** | 2024-06-25 12:05:31.000 UTC (1y 6m ago) |
| **Updated** | 2024-06-27 08:05:50.000 UTC |
| **Closed** | 2024-06-27 08:05:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

The changes in #520 have not yet propagated to `pip`, leading to the vulnerabilities previously identified to still exist in the latest version of the `temporalio` SDK. 

### Describe the solution you'd like

For the `temporalio` SDK installed using `pip` to have the latest Rust dependencies installed in #520 .

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-06-25 12:59:40.000 UTC</summary>

This will happen next release (should not be too long, but does not need to have a GH issue opened). In the meantime you can build and install the SDK itself.

Reactions: ðŸ‘ 1

</details>


---

<a id="529"></a>

### #529: Wrap GRPC::CANCELED and DEADLINE_EXCEEDED in an SDK Timeout exception for Update

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/529 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-05-18 00:43:03.000 UTC (1y 7m ago) |
| **Updated** | 2024-06-20 20:25:43.000 UTC |
| **Closed** | 2024-06-20 20:25:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

So that users only have to catch one predictable exception

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-06-05 20:01:30.000 UTC</summary>

See https://github.com/temporalio/features/issues/483#issuecomment-2150832036 for implementation details

</details>


---

<a id="518"></a>

### #518: [Feature Request] Investigate asyncio.as_completed and asyncio.wait usage of `set` as non-deterministic

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/518 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-05-06 12:26:01.000 UTC (1y 8m ago) |
| **Updated** | 2024-06-04 14:07:10.000 UTC |
| **Closed** | 2024-06-04 14:07:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

asyncio.as_completed and asyncio.wait may be non-deterministic in their use of `set`. See https://github.com/python/cpython/blob/v3.12.3/Lib/asyncio/tasks.py#L609 and https://github.com/python/cpython/blob/v3.12.3/Lib/asyncio/tasks.py#L458. See if anything can be done and if not, document that they are dangerous and warn on their use. May need to provide deterministic alternatives.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-05-06 13:23:15.000 UTC</summary>

See also #429

</details>


---

<a id="504"></a>

### #504: Support for WorkflowIdConflictPolicy

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/504 |
| **State** | CLOSED |
| **Author** | josh-berry (Josh Berry) |
| **Created** | 2024-04-11 00:40:12.000 UTC (1y 9m ago) |
| **Updated** | 2025-02-13 14:55:12.000 UTC |
| **Closed** | 2024-07-12 18:16:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See https://github.com/temporalio/features/issues/437

#### Comments (1)

<details>
<summary><strong>CarlosBorroto</strong> commented on 2025-02-13 14:55:10.000 UTC</summary>

Are there plans to support `WorkflowIdConflictPolicy` for child workflows?

Reactions: ðŸ‘ 3

</details>


---

<a id="494"></a>

### #494: [Bug] Commands sent during finally of a cache eviction may cross workflow contexts

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/494 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-03-26 16:26:46.000 UTC (1y 9m ago) |
| **Updated** | 2024-04-05 14:00:14.000 UTC |
| **Closed** | 2024-04-05 14:00:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

It appears during GC, finally clauses in Python will end up on a different workflow's even loop. This is very bad. Here is a replication:

```python
import asyncio
import logging
import time
from concurrent.futures import ThreadPoolExecutor
from datetime import timedelta
from uuid import uuid4

from temporalio import activity, workflow
from temporalio.client import Client, WorkflowHandle
from temporalio.worker import Worker


@activity.defn
def waiting_activity() -> str:
    time.sleep(1)
    return "done"


@activity.defn
def unrelated_activity() -> str:
    return "I should only be in FinallyWorkflow"


@workflow.defn
class DummyWorkflow:
    @workflow.run
    async def run(self) -> None:
        await workflow.start_activity(
            waiting_activity, start_to_close_timeout=timedelta(seconds=10)
        )
        await workflow.start_activity(
            waiting_activity, start_to_close_timeout=timedelta(seconds=10)
        )


@workflow.defn
class FinallyWorkflow:
    @workflow.run
    async def run(self) -> None:
        try:
            await workflow.start_activity(
                waiting_activity, start_to_close_timeout=timedelta(seconds=10)
            )
            await workflow.start_activity(
                waiting_activity, start_to_close_timeout=timedelta(seconds=10)
            )
        finally:
            await workflow.start_activity(
                unrelated_activity, start_to_close_timeout=timedelta(seconds=10)
            )


async def main():
    logging.basicConfig(level=logging.INFO)

    task_queue = f"tq-{uuid4()}"
    logging.info(f"Starting on task queue {task_queue}")

    # Connect
    client = await Client.connect("localhost:7233")

    # Run with worker
    with ThreadPoolExecutor(100) as activity_executor:
        async with Worker(
            client=client,
            task_queue=task_queue,
            workflows=[FinallyWorkflow, DummyWorkflow],
            activities=[waiting_activity, unrelated_activity],
            activity_executor=activity_executor,
            max_concurrent_workflow_tasks=5,
            max_cached_workflows=10,
        ):

            # Create 1000 dummy and finally workflows
            dummy_handles: list[WorkflowHandle] = []
            logging.info("Starting dummy and finally workflows")
            for i in range(1000):
                dummy_handles.append(
                    await client.start_workflow(
                        DummyWorkflow.run, id=f"dummy-{uuid4()}", task_queue=task_queue
                    )
                )
                await client.start_workflow(
                    FinallyWorkflow.run, id=f"finally-{uuid4()}", task_queue=task_queue
                )

            # Wait on every dummy handle, which basically waits forever if/when
            # one hits non-determinism
            logging.info("Checking dummy and finally workflows")
            for _, handle in enumerate(dummy_handles):
                logging.info(f"Checking dummy result for {handle.id}")
                await handle.result()
            logging.info("No dummy workflows had finally activities")


if __name__ == "__main__":
    asyncio.run(main())
```

Running that against a localhost server will start to spit out non-determinism errors. And after a few seconds, if you look at the still-running `DummyWorkflow`s, you'll see they run `unrelated_activity` which is not even in their code. It is suspected this is caused by `GeneratorExit` happening with finally upon cache eviction, and that is being interleaved in the same thread as another workflow that has `asyncio._set_running_loop` on the thread.

We may need to implement a special async gen capture/finalizer, see https://peps.python.org/pep-0525/#finalization.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-03-29 22:39:01.000 UTC</summary>

An update - so basically there's no way for us to intercept GC to make sure it only runs on a certain thread. There are ways if we wanted to "stop the world" (i.e. disable GC and such), but nothing specific to a workflow. Can't use any form of weakref ref/finalize before/after on the higher level event loop to confirm GC of transitive coroutines. Can't disable the `GeneratorExit` behavior on GC of Coroutine in any way in Python that we can find (coroutine wrapper long since deprecated/removed, event loop not told about every coroutine, etc). Async generator capture/finalizer does nothing for general coroutines.

So what is clear is that we need to force tasks to complete before we try to GC. This is similar to what we do in other SDKs on eviction where we tell coroutines to complete and just ignore any commands or side effects they may have. The first attempt at this is promising on [this commit](https://github.com/cretz/temporal-sdk-python/commit/13d06adfd677bc661917bb7f6b6b35bc95f87d17), but it is a scary change, because if for some reason a user's code can't have its tasks torn down, it will never get evicted which means the memory never gets reclaimed and a slot is used on that worker forever and it can't run anything for that run ID again either. This is technically preferable to running code on the wrong workflow of course, but we may need a better approach here (e.g. moving the workflow to a different place).

We are still researching here.

</details>


---

<a id="493"></a>

### #493: [Feature Request] Support metric option for using seconds (and other recent core metric changes)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/493 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-03-26 15:17:59.000 UTC (1y 9m ago) |
| **Updated** | 2024-04-18 11:57:04.000 UTC |
| **Closed** | 2024-04-18 11:57:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Need to at least expose `use_seconds_for_duration` option on Prometheus metrics from https://github.com/temporalio/sdk-core/pull/706, may also have to update other things due to changes made by that PR.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-04-01 19:30:14.000 UTC</summary>

Half of this was done at #498, still need:

* Tests for duration values as seconds
* Ability to set duration values in custom metrics

</details>


---

<a id="453"></a>

### #453: [Bug] Parent packages of a package are automatically imported via the Sandbox

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/453 |
| **State** | CLOSED |
| **Author** | rcomesan |
| **Created** | 2024-01-01 23:50:48.000 UTC (2 years ago) |
| **Updated** | 2024-01-03 16:10:51.000 UTC |
| **Closed** | 2024-01-03 16:10:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I'm trying to import a workflow defined in a module placed in a nested package.

### Describe the bug

If I import my workflow using `from myapp.subpackage.workflows import MyWorkflow`, and `myapp` or `myapp.subpackage` imports non-deterministic libraries or modules that import non-deterministic libraries (e.g. httpx, torch), I get a `RuntimeError: Failed validating workflow {defn.name}`.

I'm expecting only the module `myapp.subpackage.workflows.__init__.py` that contains my deterministic code/classes to be imported via the sandbox. However, it seems that modules  `myapp.__init__.py`  and  `myapp.subpackage.__init__.py` also get imported somehow, and they break the sandbox as they contain non-deterministic code unrelated to workflows. 

### Minimal Reproduction

My repository contains 2 main files (main_failing.py and main_working.py) that illustrate this behavior.:
https://github.com/rcomesan/sandbox-intermediate-packages-repro/tree/main

### Environment/Versions

- Debian amd64
- Python 3.11.2
- Temporal SDK: 1.4.0
 - Docker image: temporalio/auto-setup:1.22.3

### Additional context
If I disable the sandbox either passing sandboxed=False to the decorator or at worker construction time using an UnsandboxedWorkflowRunner this behavior no longer occurs, and I'm able to start my workers without issues.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-01-03 16:10:50.000 UTC</summary>

This is the proper behavior according to Python specification. From https://docs.python.org/3/reference/import.html#regular-packages:

> Importing `parent.one` will implicitly execute `parent/__init__.py` and `parent/one/__init__.py`.

We use normal `importlib` behavior by intention. Closing the issue as by intention, but feel free to continue discussion here or on `#python-sdk` on Slack.

Reactions: ðŸ‘ 1

</details>


---

<a id="451"></a>

### #451: upsert_search_attribute does not insert new attributes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/451 |
| **State** | CLOSED |
| **Author** | ndtretyak (Nikolay Tretyak) |
| **Created** | 2023-12-21 10:45:47.000 UTC (2 years ago) |
| **Updated** | 2023-12-21 11:11:30.000 UTC |
| **Closed** | 2023-12-21 11:11:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Adding new SearchAttribute using `workflow.upsert_search_attribute` fails with the `StopIteration` error [here](https://github.com/temporalio/sdk-python/blob/6f966c7ca59d01c02243561d186867e199f252a6/temporalio/worker/_workflow_instance.py#L1245) since this list contains only previously set attributes.

Adding new SearchAttribute via new typed API raises the same error [here](https://github.com/temporalio/sdk-python/blob/6f966c7ca59d01c02243561d186867e199f252a6/temporalio/worker/_workflow_instance.py#L1287).

### Minimal Reproduction

Start this workflow without `SomeKey` in the initial SearchAttributes set.

```
@workflow.defn
class TestWorkflow:
    @workflow.run
    async def run(self):
        workflow.upsert_search_attributes({
            "SomeKey": [True],
        })
```

### Environment/Versions

Works fine in SDK 1.3.0 but fails in 1.4.0


#### Comments (1)

<details>
<summary><strong>ndtretyak</strong> commented on 2023-12-21 11:11:28.000 UTC</summary>

Fixed in https://github.com/temporalio/sdk-python/pull/440

</details>


---

<a id="447"></a>

### #447: [Bug] Wrong line number in workflow.logger

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/447 |
| **State** | CLOSED |
| **Author** | ndtretyak (Nikolay Tretyak) |
| **Created** | 2023-12-13 12:42:50.000 UTC (2 years ago) |
| **Updated** | 2024-01-10 16:36:06.000 UTC |
| **Closed** | 2024-01-10 16:36:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

All the log records emitted by `workflow.logger` has the same `file`, `path` and `lineno` pointing to `workflow.LoggingAdapter.log`.

### Minimal Reproduction

```python
import logging
import sys

from temporalio import workflow


class Formatter(logging.Formatter):
    def format(self, record) -> str:
        return f"path: {record.pathname}"


formatter = Formatter()
handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(formatter)
log = logging.getLogger("")
log.setLevel(logging.INFO)
log.addHandler(handler)


@workflow.defn
class Workflow:
    @workflow.run
    async def run(self) -> None:
        workflow.logger.info("message")
```

This prints `<...>/temporalio/workflow.py` instead of the file where workflow is defined.



#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-12-13 14:13:33.000 UTC</summary>

Thanks for the report!

I have confirmed this is due to us overriding `LogAdapter.log` to prevent logging during replay. The best fix is probably for us to remove the `log` overload and replace it with:

```python
    def isEnabledFor(self, level: int) -> bool:
        return super().isEnabledFor(level) and (self.log_during_replay or not unsafe.is_replaying())
```

</details>


---

<a id="443"></a>

### #443: [Bug] Coroutine Raised StopIteration when triggering  workflow execution from CLI for workflows which call workflow.upsert_search_attributes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/443 |
| **State** | CLOSED |
| **Author** | alonwengierko (Alon) |
| **Created** | 2023-12-09 00:25:28.000 UTC (2y 1m ago) |
| **Updated** | 2023-12-11 13:51:47.000 UTC |
| **Closed** | 2023-12-11 13:51:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I modified poetry file to use temporalio version 1.4.0 and then ran worker.py of your_visibility folder of the DOCUMENTATION_SAMPLES_PYTHON repository (https://github.com/temporalio/documentation-samples-python) 

Then I use cli command to trigger workflow  

temporal workflow start   --task-queue search-attributes-task-queue   --type GreetingWorkflow

After doing that I get an error:
"coroutine raised StopIteration"

This error doesn't happen for version Python SDK 1.3.0 when I trigger workflow from cli

### Describe the bug
I found that when you trigger a workflow who has calls to workflow.upsert_search_attributes  using cli for for Temporalio Python SDK version 1.4.0, you get error Coroutine Raised StopIteration

File "/Users/awengierko/Documents/Projects/documentation-samples-python/.venv/lib/python3.11/site-packages/temporalio/worker/_workflow_instance.py", line 1888, in execute_workflow
    return await input.run_fn(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: coroutine raised StopIteration
Failed activation on workflow GreetingWorkflow with ID 9e14f703-b1de-4031-9213-fb43b405a660 and run ID cf74cb1b-0bb2-4d08-a711-312c3f050cfb

<img width="1618" alt="Screenshot 2023-12-08 at 21 11 10" src="https://github.com/temporalio/sdk-python/assets/91084345/34b354c8-451e-450f-a006-41b5254c6c91">
<img width="1538" alt="Screenshot 2023-12-08 at 21 11 18" src="https://github.com/temporalio/sdk-python/assets/91084345/1b44a973-20a6-4762-a131-71294c423bff">

### Minimal Reproduction
1. Clone repo https://github.com/temporalio/documentation-samples-python
2. Update pyproject.toml to use temporalio = "^1.4.0" instead of temporalio = "^1.3.0"
3. Terminal 1 -> Run worker: python worker.py
4. Terminal 2 -> Trigger Workflow exeuction:  temporal workflow start   --task-queue search-attributes-task-queue   --type GreetingWorkflow
5. See in Terminal 2: Error message "coroutine raised StopIteration"

### Environment/Versions

- OS and processor: M2 Mac
- Temporal Version: 1.4.0
- Are you using Docker or Kubernetes or building Temporal from source? No



#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-12-11 13:51:47.000 UTC</summary>

Thanks! I believe this was just fixed in https://github.com/temporalio/sdk-python/pull/440 but not yet released. In the meantime, I recommend using typed search attributes for `upsert_search_attributes` (you should be getting a warning if you use this deprecated form). I will see if I can have the docs updated to match.

(closing since the primary issue was fixed, but feel free to continue to discuss here, on the forums, or on Slack)

</details>


---

<a id="425"></a>

### #425: [Feature Request] Add usedforsecurity=False for build-ID MD5 use

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/425 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-13 13:39:47.000 UTC (2y 1m ago) |
| **Updated** | 2024-02-20 21:37:09.000 UTC |
| **Closed** | 2024-02-20 21:37:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

For Python 3.9+, add `usedforsecurity=False` to `md5` call for build ID

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-02-20 21:37:09.000 UTC</summary>

This was solved with #472

</details>


---

<a id="407"></a>

### #407: [Feature Request] Add utility or at least sample to incorporate core logs into Python logging

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/407 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-10-24 22:28:55.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-31 15:45:19.000 UTC |
| **Closed** | 2023-10-31 15:45:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Need to demonstrate how to use the new log forwarding feature (#405) to push logs into Python loggers automatically. If a good enough utility can be created to poll and forward quickly, we can make it part of the SDK. Otherwise, use a sample. Also research what TS does wrt log ordering.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-10-31 15:45:19.000 UTC</summary>

Ok, this is being folded back into #311 as part of the MVP requirement.

</details>


---

<a id="397"></a>

### #397: [Bug] Client may not be reconnecting on server shutdown/restart

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/397 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-10-09 20:58:24.000 UTC (2y 3m ago) |
| **Updated** | 2023-12-07 19:31:12.000 UTC |
| **Closed** | 2023-12-07 19:31:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

There are reports of clients not auto-reconnecting on server shutdown/restart. Confirm that clients for direct use and for worker use are properly auto-reconnecting. This may be solved by #396, but confirm.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-12-07 19:31:11.000 UTC</summary>

I have done the following test:

* Start a server
* Start a worker
* Start another process that creates a single client at the beginning and runs a workflow every second
* Stop the server
  * This will get core warnings on worker like "WARN temporal_client::retry: gRPC call poll_workflow_task_queue retried 6 times" but will not fail the worker
  * This will get core warnings on the client like "WARN temporal_client::retry: gRPC call start_workflow_execution retried 7 time" and after some retries it will raise an exception to the caller (which we trap for this test)
* Start the server
  * The worker automatically recovers
  * The same exact client instance is usable again for starting workflows

I do not believe this is an issue unless I can get more details on how to replicate

</details>


---

<a id="382"></a>

### #382: tonic-0.9.2.crate: 1 vulnerabilities (highest severity is: 7.5) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/382 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2023-08-31 14:12:00.000 UTC (2y 4m ago) |
| **Updated** | 2023-11-06 17:41:47.000 UTC |
| **Closed** | 2023-11-06 17:41:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘€ 1 |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>tonic-0.9.2.crate</b></p></summary>

<p></p>
<p>Path to dependency file: /temporalio/bridge/Cargo.toml</p>
<p>Path to vulnerable library: /temporalio/bridge/Cargo.toml</p>
<p>


<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/aa829d30b57cf7865a3b1ef30d98625f6e3fe5c4">aa829d30b57cf7865a3b1ef30d98625f6e3fe5c4</a></p></details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in (tonic version) | Remediation Possible** |
| ------------- | ------------- | ----- | ----- | ----- | ------------- | --- |
| [WS-2023-0305](https://github.com/rustls/webpki/commit/4ea052366f342a06344aab589565179b59b342d3) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png?' width=19 height=20> High | 7.5 | rustls-webpki-0.101.4.crate | Transitive | N/A* | &#10060; |
<p>*For some transitive vulnerabilities, there is no version of direct dependency with a fix. Check the "Details" section below to see if there is a version of transitive dependency where vulnerability is fixed.</p><p>**In some cases, Remediation PR cannot be created automatically for a vulnerability despite the availability of remediation</p>

## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png?' width=19 height=20> WS-2023-0305</summary>


###  Vulnerable Library - <b>rustls-webpki-0.101.4.crate</b></p>

<p>Web PKI X.509 Certificate Verification.</p>
<p>Library home page: <a href="https://static.crates.io/crates/rustls-webpki/rustls-webpki-0.101.4.crate">https://static.crates.io/crates/rustls-webpki/rustls-webpki-0.101.4.crate</a></p>
<p>Path to dependency file: /temporalio/bridge/Cargo.toml</p>
<p>Path to vulnerable library: /temporalio/bridge/Cargo.toml</p>
<p>

Dependency Hierarchy:
  - tonic-0.9.2.crate (Root Library)
    - tokio-rustls-0.24.1.crate
      - rustls-0.21.6.crate
        - :x: **rustls-webpki-0.101.4.crate** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/aa829d30b57cf7865a3b1ef30d98625f6e3fe5c4">aa829d30b57cf7865a3b1ef30d98625f6e3fe5c4</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
rustls-webpki: CPU denial of service in certificate path building

<p>Publish Date: 2023-08-22
<p>URL: <a href=https://github.com/rustls/webpki/commit/4ea052366f342a06344aab589565179b59b342d3>WS-2023-0305</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://github.com/advisories/GHSA-fh2r-99q2-6mmg">https://github.com/advisories/GHSA-fh2r-99q2-6mmg</a></p>
<p>Release Date: 2023-08-22</p>
<p>Fix Resolution: rustls-webpki - 0.100.2,0.101.4</p>

</p>

<p></p>

</details>

#### Comments (1)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-11-06 17:41:47.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="381"></a>

### #381: [Feature Request] Typed search attributes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/381 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-08-29 12:43:08.000 UTC (2y 4m ago) |
| **Updated** | 2023-11-03 19:16:25.000 UTC |
| **Closed** | 2023-11-03 19:16:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Implementation of [typed search attributes](https://github.com/temporalio/proposals/blob/master/sdk-typed-search-attributes.md). Work started at #366.


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-11-03 19:16:25.000 UTC</summary>

Merged

</details>


---

<a id="360"></a>

### #360: [Feature Request] Allow use of type hints even if arg count mismatches in case of default params

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/360 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-08-01 21:54:50.000 UTC (2y 5m ago) |
| **Updated** | 2025-12-11 19:46:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Today we only use a type hint if the arg count matches the input count. But the args may have default values. So we need to at least use type hints when the required arg count matches the input count. Maybe we should just remove the check altogether and just use as many type hints as we can.

#### Comments (1)

<details>
<summary><strong>asavartsov</strong> commented on 2025-12-11 19:46:02.000 UTC</summary>

I guess not a lot changed? I stumbled into this in very funny way: I have a workflow which first argument is a model, and some rest have default values for single reason: being able to quickly "Start Workflow Like This One" from UI because it supports only one argument.

As you guess, in the programmatic flow all values are provided and everything works fine, but when starting from UI, the model becomes a `dict`. 

There are a lot of options of course, from the wrapping to checking if a dict and `model_validating` in manually. Anyway, not knowing about type-hinting limitations, I attributed this sudden appearance of dict to incorrect data converter configuration, and was digging in wrong direction.

So maybe this limitation worth to be mentioned in README, somewhere around `@workflow.run` and  Pydantic sections.

</details>


---

<a id="358"></a>

### #358: [Feature Request] Default to suggesting threaded activities (and other async helper docs)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/358 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-08-01 17:05:55.000 UTC (2y 5m ago) |
| **Updated** | 2023-10-06 16:35:49.000 UTC |
| **Closed** | 2023-10-06 16:35:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

* Let people know that if they block an `async def` call in Python it stops all async code
* Document `set_debug(True)` which also warns about tasks taking longer than 0.1 seconds
* Encourage more use of threaded activities unless the user is sure their calls are non-thread-blocking

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-09-18 12:59:52.000 UTC</summary>

> Encourage more use of threaded activities unless the user is sure their calls are non-thread-blocking

To add to this, we're going to change the quickstart and maybe samples to use threaded activities

</details>


---

<a id="357"></a>

### #357: [Feature Request] TracingInterceptor span kinds

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/357 |
| **State** | CLOSED |
| **Author** | northpowered (Oleg Romanov) |
| **Created** | 2023-07-27 07:12:39.000 UTC (2y 5m ago) |
| **Updated** | 2023-08-01 13:23:57.000 UTC |
| **Closed** | 2023-08-01 13:23:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

This`s an issue for #356 

Now, TracingInterceptor creates all spans with kind=INTERNAL

This PR changes this to CLIENT/SERVER model for convenient service graphs in APM (Tempo for ex.)

Before:
![bad_sg](https://github.com/temporalio/sdk-python/assets/95244614/a3091c7f-f5f6-4ede-be79-90cebac8668e)

After:
![good_sg](https://github.com/temporalio/sdk-python/assets/95244614/cc02137c-0646-49fc-96bd-1f75e0b87691)


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-07-31 14:29:23.000 UTC</summary>

Thanks for the PR! Commented there. I completely agree we need to set this properly.

</details>


---

<a id="354"></a>

### #354: [Feature Request] Metric fowarding support and custom metric support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/354 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-07-24 14:31:37.000 UTC (2y 5m ago) |
| **Updated** | 2023-09-08 15:16:26.000 UTC |
| **Closed** | 2023-09-08 15:16:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Need a way to, independently, support metric forwarding and allow people to create/record custom metrics. See [this TypeScript issue](https://github.com/temporalio/sdk-typescript/issues/1177) and [this core issue](https://github.com/temporalio/sdk-core/pull/544).

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-09-08 15:16:25.000 UTC</summary>

This is being handled in #369

</details>


---

<a id="345"></a>

### #345: [Feature Request] Support in-workflow OpenTelemetry span creation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/345 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-07-13 14:29:41.000 UTC (2y 6m ago) |
| **Updated** | 2023-07-13 14:32:22.000 UTC |
| **Closed** | 2023-07-13 14:32:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

We currently have a concept of a "completed span" for use by workflows for the OTel interceptor, but we don't expose this to users. We should expose this to users but document the caveats (e.g. that it uses the original tracer, that it is marked completed immediately, that it is not done on replay, etc).

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-07-13 14:32:22.000 UTC</summary>

My mistake, this is already present at `temporalio.contrib.opentelemetry.workflow.completed_span`.

</details>


---

<a id="336"></a>

### #336: [Python] Workflow update support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/336 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2023-07-05 16:57:11.000 UTC (2y 6m ago) |
| **Updated** | 2023-10-25 21:05:13.000 UTC |
| **Closed** | 2023-10-25 21:05:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | Sushisource |
| **Milestone** | None |

#### Description

*No description provided.*

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-08-15 20:16:58.000 UTC</summary>

This is waiting on https://github.com/temporalio/sdk-core/issues/582

</details>


---

<a id="331"></a>

### #331: [Bug] Local activity without schedule_to_close_timeout is failing immediately

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/331 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-06-19 13:33:25.000 UTC (2y 6m ago) |
| **Updated** | 2023-07-05 20:57:24.000 UTC |
| **Closed** | 2023-07-05 20:57:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Only tested so far on 1.2.0. Example worker:

```python
import asyncio
import logging
import time
from datetime import timedelta

from temporalio import activity, workflow
from temporalio.client import Client
from temporalio.worker import Worker


@activity.defn
async def get_time() -> float:
    activity.logger.info("Called get_time")
    return time.time()


@workflow.defn
class GreetingWorkflow:
    @workflow.run
    async def run(self) -> float:
        workflow.logger.info("Calling local activity")
        result = await workflow.execute_local_activity(
            get_time,
            # schedule_to_close_timeout=timedelta(seconds=10),
            start_to_close_timeout=timedelta(seconds=10),
        )
        workflow.logger.info("Local activity result: {result}")
        return result


async def main():
    logging.basicConfig(level=logging.INFO)
    # Start client
    client = await Client.connect("localhost:7233")

    # Run a worker for the workflow
    worker = Worker(
        client,
        task_queue="hello-local-activity-task-queue",
        workflows=[GreetingWorkflow],
        activities=[get_time],
    )
    await worker.run()


if __name__ == "__main__":
    asyncio.run(main())
```

Then when run via CLI like:

```
temporal workflow execute -t hello-local-activity-task-queue --type GreetingWorkflow
```

Result:

```
Running execution:
  WorkflowId  bb7a7e16-f979-4d0f-9ec9-9d4ce6e40021
  RunId       e7a9202b-d56c-4d6d-b473-e7555b306f0d
  Type        GreetingWorkflow
  Namespace   default
  TaskQueue   hello-local-activity-task-queue
  Args        []

Progress:
  ID          Time                    Type
   1  2023-06-19T13:30:17Z  WorkflowExecutionStarted
   2  2023-06-19T13:30:17Z  WorkflowTaskScheduled
   3  2023-06-19T13:30:17Z  WorkflowTaskStarted
   4  2023-06-19T13:30:17Z  WorkflowTaskCompleted
   5  2023-06-19T13:30:17Z  MarkerRecorded
   6  2023-06-19T13:30:17Z  WorkflowExecutionFailed

Result:
  Run Time: 1 seconds
  Status: FAILED
  Failure: &Failure{Message:Local Activity timed out,Source:,StackTrace:,Cause:&Failure{Message:Activity timed out,Source:,StackTrace:,Cause:nil,FailureType:Failure_TimeoutFailureInfo: ScheduleToClose,},FailureType:Failure_ActivityFailureInfo,}
```

But if `schedule_to_close_timeout` is uncommented it works.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-06-19 13:46:38.000 UTC</summary>

The issue is that the Go SDK client sets 0 seconds execution timeout instead of null which meant immediate timeout. This was solved by https://github.com/temporalio/sdk-core/pull/569. We need to upgrade core and put this in the next release.

</details>


---

<a id="327"></a>

### #327: Use stable pydoctor version instead of devel version

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/327 |
| **State** | CLOSED |
| **Author** | tristanlatr |
| **Created** | 2023-06-09 22:55:32.000 UTC (2y 7m ago) |
| **Updated** | 2023-07-05 14:01:46.000 UTC |
| **Closed** | 2023-07-05 14:01:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Pydoctor's version in use is a custom development version. Since support for `@overload` has now been released, the stable version should be used instead.


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-06-12 14:46:19.000 UTC</summary>

Thanks for carrying that over the finish line! I really appreciate it. We will upgrade shortly.

</details>


---

<a id="325"></a>

### #325: [Bug] GeneratorExit inadvertently caught, and also core timer mismatch 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/325 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-06-05 21:55:08.000 UTC (2y 7m ago) |
| **Updated** | 2023-07-11 19:18:58.000 UTC |
| **Closed** | 2023-07-11 19:18:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

There are two problems here:

#### GeneratorExit is caught somewhere

When a coroutine is closed, Python [throws a `GeneratorExit` from the coroutine](https://github.com/python/cpython/blob/a7a450f84a087421603170c2dad226bb881d4d9a/Lib/_collections_abc.py#L146-L154) and expects that to be bubbled out. But somewhere we are not bubbling that out so Python throws a runtime exception on this. So we're getting:

> RuntimeError: coroutine ignored GeneratorExit
> Exception ignored in: <coroutine object _WorkflowInstanceImpl._apply_start_workflow.<locals>.run_workflow at 0x000001F7B495E340>

With a stack trace.

#### Core timer error

When workflow is evicted, sometimes when it reruns from scratch it gets something like:

> 2023-06-05T22:04:56.500788Z  WARN temporal_sdk_core::worker::workflow: Failing workflow task run_id=b4c01744-339b-4d39-98f8-10e0c7d3846c failure=Failure { failure: Some(Failure { message: "Fatal(\"Timer fired event did not have expected timer id 8, it was 2!\")", source: "", stack_trace: "", encoded_attributes: None, cause: None, failure_info: Some(ApplicationFailureInfo(ApplicationFailureInfo { r#type: "", non_retryable: false, details: None })) }), force_cause: Unspecified }

Unsure if this is due to `GeneratorExit` above or if that failure just surfaced the problem.

### Replication

```python
import asyncio
import logging
from uuid import uuid4

from temporalio import workflow
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import Worker


@workflow.defn
class MyWorkflow:
    @workflow.run
    async def run(self) -> None:
        try:
            for _ in range(20):
                await asyncio.sleep(0.5)
        finally:
            await asyncio.sleep(1)

    @workflow.signal
    async def my_signal(self) -> None:
        pass


async def main():
    logging.basicConfig(level=logging.INFO)

    async with await WorkflowEnvironment.start_local() as env:
        logging.info("Starting worker")
        task_queue = f"tq-{uuid4()}"
        async with Worker(
            env.client,
            task_queue=task_queue,
            workflows=[MyWorkflow],
            max_cached_workflows=0,
        ):

            logging.info("Starting workflow")
            handle = await env.client.start_workflow(
                MyWorkflow.run,
                id=f"wf-{uuid4()}",
                task_queue=task_queue,
            )

            logging.info("Signalling every 300ms")
            for _ in range(20):
                await asyncio.sleep(0.3)
                await handle.signal(MyWorkflow.my_signal)

            logging.info("Waiting for workflow completion")
            await handle.result()
            logging.info("Workflow done")


if __name__ == "__main__":
    asyncio.run(main())
```

This is based on a user repo. This disables cache and makes a bunch of timers. But this causes constant evictions for each timer and signal sent which is causing the `GeneratorExit` issue. But that `GeneratorExit` issue is _also_ causing this core error. I have not yet determined whether the `GeneratorExit` issue is surfacing a core bug or is just the bug itself w/ an eviction problem and core is somehow showing it.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-06-06 22:09:10.000 UTC</summary>

Here's the issue for `GeneratorExit` as comments on the original workflow:

```python
@workflow.defn
class MyWorkflow:
    @workflow.run
    async def run(self) -> None:
        try:
            for _ in range(20):
                # With cache disabled, this is evicted which when GC'd causes
                # this to throw a GeneratorExit (which is usually ok)
                await asyncio.sleep(0.5)
        finally:
            # This tries to run on GeneratorExit, but fails with a RuntimeError
            # of "no running event loop". Since this finally was not written well
            # it swallows the GeneratorExit error making Python mad it didn't
            # receive the error
            await asyncio.sleep(1)
```

So, the `finally` block is causing an error which swallows the actual `GeneratorExit` making Python upset. No matter how many ways I've tried (e.g. trying to overwrite `close` on `coroutine` which are read only), I cannot prevent the `coroutine.close()` which does the `GeneratorExit` dance. From https://docs.python.org/3/reference/datamodel.html#coroutine.close:

> Causes the coroutine to clean itself up and exit. If the coroutine is suspended, this method first delegates to the [close()](https://docs.python.org/3/reference/expressions.html#generator.close) method of the iterator that caused the coroutine to suspend, if it has such a method. Then it raises [GeneratorExit](https://docs.python.org/3/library/exceptions.html#GeneratorExit) at the suspension point, causing the coroutine to immediately clean itself up. Finally, the coroutine is marked as having finished executing, even if it was never started.
>
>   Coroutine objects are automatically closed using the above process when they are about to be destroyed.

I am still investigating how I can try to tear down the event loop without it waking up. I am also still investigating how core timers can get out of sync here. My first thought was this is an old activation completing, but that doesn't seem accurate.

</details>


---

<a id="316"></a>

### #316: [Bug] ScheduleState.limited_actions doc is invalid

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/316 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-05-02 14:02:59.000 UTC (2y 8m ago) |
| **Updated** | 2023-06-16 14:33:12.000 UTC |
| **Closed** | 2023-06-16 14:33:12.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

`ScheduleState.limited_actions` says it's not allowed on create. Not only is that false, but we need to say it is required for remaining actions to take effect. Confirming whether we should fail if they try to create a schedule with remaining actions non-zero and this value as false.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-05-02 18:55:19.000 UTC</summary>

After discussion, we should error _on create_ if `(limited_actions == True) != (remaining_actions > 0)`. Can allow the mismatched fields on update though just in case they only want to update one.

</details>


---

<a id="302"></a>

### #302: [Feature Request] Fully qualify README logo image URL

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/302 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-03-23 22:33:57.000 UTC (2y 9m ago) |
| **Updated** | 2023-03-30 23:24:52.000 UTC |
| **Closed** | 2023-03-30 23:24:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Fully qualify README logo image URL so it will appear in PyPI

#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2023-03-23 22:50:53.000 UTC</summary>

@lorensr is working on uploading the logo to a CDN.

</details>


---

<a id="295"></a>

### #295: [Feature Request] Support for dependency injection (pinject)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/295 |
| **State** | CLOSED |
| **Author** | grizzlybearer |
| **Created** | 2023-03-10 13:37:09.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-10 13:53:54.000 UTC |
| **Closed** | 2023-03-10 13:53:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | question |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### In order to use Temporal in our systems, we need the ability to inject dependencies such as grpc connections, db connections, kafka connections, auth data into temporal workflows and activities, using pinject, that are scoped at different levels (global, customer specific).

Here's an example:
Say you have a python class to fetch customer scoped data from redis:
```
from pinject import new_object_graph
import redis

class CustomerDataAccess:
    def __init__(self, customer_partition):
       # this is simplified. IRL there would be nested dependencies injected
        self.customer_partition = customer_partition
        self.redis_client = redis.Redis()

    def get_data(self):
        # Access data from the specified customer partition in Redis
        data = self.redis_client.get(self.customer_partition)
        return data
```
 and then you have a workflow and an activity that needs this dependency:

```
# Define the Workflow interface
class CustomerWorkflow:
    @workflow_method(task_queue="customer-data")
    def get_customer_data(self) -> str:
        pass

# Implement the Workflow
class CustomerWorkflowImpl(CustomerWorkflow):
    def __init__(self, customer_data_access: CustomerDataAccess):
        self.data_access = customer_data_access

    def get_customer_data(self) -> str:
        # Access data from a specific customer partition using the Activity
        data = self.data_access.get_data("customer1")
        # do business logic on this data
        return result
```


### Describe the solution you'd like

Proposed solution: Add support for pinject, similar to how you folks added support for Spring Boot in Java: https://github.com/temporalio/sdk-java/issues/745

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-03-10 13:53:53.000 UTC</summary>

Closing as question/duplicate. Duplicate post at https://community.temporal.io/t/python-support-for-dependency-injection-pinject/7538 is a better place to discuss.

</details>


---

<a id="292"></a>

### #292: Support dynamic activity and workflow implementations

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/292 |
| **State** | CLOSED |
| **Author** | vidath (Vida) |
| **Created** | 2023-03-02 20:06:10.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-02 20:06:52.000 UTC |
| **Closed** | 2023-03-02 20:06:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

s your feature request related to a problem? Please describe.
In some cases, activity and workflow implementations should be loaded dynamically. It is similar to the way the Java app engines load applications using class loaders.

Describe the solution you'd like
Expose an extension point for dynamic activity loading. While the SDK might not provide the explicit loading mechanism it should provide a factory interface that is called with activity or workflow type to provide an untyped activity implementation.

Additional context
This feature has been added to java sdk as part of issue #245
https://github.com/temporalio/sdk-java/issues/245

https://community.temporal.io/t/dynamic-activities-registration-at-runtime-and-worker-splitting/930/7

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-03-02 20:06:52.000 UTC</summary>

Closing as duplicate of #242 

Reactions: ðŸ‘ 1

</details>


---

<a id="244"></a>

### #244: tokio-1.21.2.crate: 2 vulnerabilities (highest severity is: 9.8) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/244 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2023-01-05 18:36:42.000 UTC (3 years ago) |
| **Updated** | 2023-03-30 23:05:59.000 UTC |
| **Closed** | 2023-03-30 23:05:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>tokio-1.21.2.crate</b></p></summary>

<p>An event-driven, non-blocking I/O platform for writing asynchronous I/O
backed applications.
</p>
<p>Library home page: <a href="https://crates.io/api/v1/crates/tokio/1.21.2/download">https://crates.io/api/v1/crates/tokio/1.21.2/download</a></p>
<p>


<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/10bf259ce4fd670cea2c26f88e83368d4fd375ed">10bf259ce4fd670cea2c26f88e83368d4fd375ed</a></p></details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in (tokio version) | Remediation Available |
| ------------- | ------------- | ----- | ----- | ----- | ------------- | --- |
| [WS-2023-0027](https://github.com/tokio-rs/tokio/commit/d6ea7a742b92d3e516035a584ab9347a96be363a) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 9.8 | tokio-1.21.2.crate | Direct | tokio - 1.18.5,1.20.4,1.24.2 | &#10060; |
| [CVE-2023-22466](https://www.mend.io/vulnerability-database/CVE-2023-22466) | <img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> Medium | 5.4 | tokio-1.21.2.crate | Direct | tokio - 1.18.4,1.20.3,1.23.1 | &#10060; |


## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> WS-2023-0027</summary>


###  Vulnerable Library - <b>tokio-1.21.2.crate</b></p>

<p>An event-driven, non-blocking I/O platform for writing asynchronous I/O
backed applications.
</p>
<p>Library home page: <a href="https://crates.io/api/v1/crates/tokio/1.21.2/download">https://crates.io/api/v1/crates/tokio/1.21.2/download</a></p>
<p>

Dependency Hierarchy:
  - :x: **tokio-1.21.2.crate** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/10bf259ce4fd670cea2c26f88e83368d4fd375ed">10bf259ce4fd670cea2c26f88e83368d4fd375ed</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
A soundness issue was discovered in tokio. tokio::io::ReadHalf<T>::unsplit can violate the Pin contract. Specific set of conditions needed to trigger an issue (a !Unpin type in ReadHalf) is unusual, combined with the difficulty of making any arbitrary use-after-free exploitable in Rust without doing a lot of careful alignment of data types in the surrounding code. The tokio feature io-util is also required to be enabled to trigger this soundness issue.

<p>Publish Date: 2023-02-02
<p>URL: <a href=https://github.com/tokio-rs/tokio/commit/d6ea7a742b92d3e516035a584ab9347a96be363a>WS-2023-0027</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>9.8</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: High
  - Integrity Impact: High
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://rustsec.org/advisories/RUSTSEC-2023-0005.html">https://rustsec.org/advisories/RUSTSEC-2023-0005.html</a></p>
<p>Release Date: 2023-02-02</p>
<p>Fix Resolution: tokio - 1.18.5,1.20.4,1.24.2</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> CVE-2023-22466</summary>


###  Vulnerable Library - <b>tokio-1.21.2.crate</b></p>

<p>An event-driven, non-blocking I/O platform for writing asynchronous I/O
backed applications.
</p>
<p>Library home page: <a href="https://crates.io/api/v1/crates/tokio/1.21.2/download">https://crates.io/api/v1/crates/tokio/1.21.2/download</a></p>
<p>

Dependency Hierarchy:
  - :x: **tokio-1.21.2.crate** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/10bf259ce4fd670cea2c26f88e83368d4fd375ed">10bf259ce4fd670cea2c26f88e83368d4fd375ed</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
Tokio is a runtime for writing applications with Rust. Starting with version 1.7.0 and prior to versions 1.18.4, 1.20.3, and 1.23.1, when configuring a Windows named pipe server, setting `pipe_mode` will reset `reject_remote_clients` to `false`. If the application has previously configured `reject_remote_clients` to `true`, this effectively undoes the configuration. Remote clients may only access the named pipe if the named pipe's associated path is accessible via a publicly shared folder (SMB). Versions 1.23.1, 1.20.3, and 1.18.4 have been patched. The fix will also be present in all releases starting from version 1.24.0. Named pipes were introduced to Tokio in version 1.7.0, so releases older than 1.7.0 are not affected. As a workaround, ensure that `pipe_mode` is set first after initializing a `ServerOptions`.

<p>Publish Date: 2023-01-04
<p>URL: <a href=https://www.mend.io/vulnerability-database/CVE-2023-22466>CVE-2023-22466</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>5.4</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: Low
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: Low
  - Integrity Impact: Low
  - Availability Impact: None
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://github.com/tokio-rs/tokio/security/advisories/GHSA-7rrj-xr53-82p7">https://github.com/tokio-rs/tokio/security/advisories/GHSA-7rrj-xr53-82p7</a></p>
<p>Release Date: 2023-01-04</p>
<p>Fix Resolution: tokio - 1.18.4,1.20.3,1.23.1</p>

</p>

<p></p>

</details>

#### Comments (1)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-03-30 23:05:59.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="241"></a>

### #241: [Feature Request] Opt-in to having stack trace query use custom data converter

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/241 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-01-05 13:20:41.000 UTC (3 years ago) |
| **Updated** | 2023-03-27 17:40:42.000 UTC |
| **Closed** | 2023-03-27 17:40:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Make a new boolean worker option like `stack_trace_query_uses_converter`. If false, the `__stack_trace` query response should not be converted/encoded/etc w/ custom converter, but if true it should be. We accept that this changes today's default behavior so we need to make it loud and clear on release notes.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-03-27 17:40:42.000 UTC</summary>

Closing since the feature was abandoned

</details>


---

<a id="233"></a>

### #233: setuptools-65.3.0-py3-none-any.whl: 1 vulnerabilities (highest severity is: 5.9) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/233 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2022-12-23 21:03:10.000 UTC (3 years ago) |
| **Updated** | 2023-02-08 11:21:39.000 UTC |
| **Closed** | 2023-02-08 11:21:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>setuptools-65.3.0-py3-none-any.whl</b></p></summary>

<p>Easily download, build, install, upgrade, and uninstall Python packages</p>
<p>Library home page: <a href="https://files.pythonhosted.org/packages/d9/5f/2daccd14278b6b780ae6799f85998377c06019354982391245f4b58a927d/setuptools-65.3.0-py3-none-any.whl">https://files.pythonhosted.org/packages/d9/5f/2daccd14278b6b780ae6799f85998377c06019354982391245f4b58a927d/setuptools-65.3.0-py3-none-any.whl</a></p>
<p>

</details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in (setuptools version) | Remediation Available |
| ------------- | ------------- | ----- | ----- | ----- | ------------- | --- |
| [CVE-2022-40897](https://www.mend.io/vulnerability-database/CVE-2022-40897) | <img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> Medium | 5.9 | setuptools-65.3.0-py3-none-any.whl | Direct | setuptools - 65.5.1 | &#10060; |


## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> CVE-2022-40897</summary>


###  Vulnerable Library - <b>setuptools-65.3.0-py3-none-any.whl</b></p>

<p>Easily download, build, install, upgrade, and uninstall Python packages</p>
<p>Library home page: <a href="https://files.pythonhosted.org/packages/d9/5f/2daccd14278b6b780ae6799f85998377c06019354982391245f4b58a927d/setuptools-65.3.0-py3-none-any.whl">https://files.pythonhosted.org/packages/d9/5f/2daccd14278b6b780ae6799f85998377c06019354982391245f4b58a927d/setuptools-65.3.0-py3-none-any.whl</a></p>
<p>

Dependency Hierarchy:
  - :x: **setuptools-65.3.0-py3-none-any.whl** (Vulnerable Library)
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
Python Packaging Authority (PyPA) setuptools before 65.5.1 allows remote attackers to cause a denial of service via HTML in a crafted package or custom PackageIndex page. There is a Regular Expression Denial of Service (ReDoS) in package_index.py.

<p>Publish Date: 2022-12-23
<p>URL: <a href=https://www.mend.io/vulnerability-database/CVE-2022-40897>CVE-2022-40897</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>5.9</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: High
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://pyup.io/posts/pyup-discovers-redos-vulnerabilities-in-top-python-packages/">https://pyup.io/posts/pyup-discovers-redos-vulnerabilities-in-top-python-packages/</a></p>
<p>Release Date: 2022-12-23</p>
<p>Fix Resolution: setuptools - 65.5.1</p>

</p>

<p></p>

</details>

#### Comments (1)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2023-02-08 11:21:39.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="229"></a>

### #229: [Feature Request] Confirm PyThreadState_SetAsyncExc is ok on non-GIL threads

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/229 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-12-14 18:35:32.000 UTC (3 years ago) |
| **Updated** | 2023-02-07 13:43:55.000 UTC |
| **Closed** | 2023-02-07 13:43:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Saw issue with Ruby, just want to confirm if we cancel a sync activity that is running non-GIL code that it's ok

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-02-07 13:43:55.000 UTC</summary>

Appears good enough in practice so far, not easy enough to add test cases for

</details>


---

<a id="227"></a>

### #227: [Feature Request] Dynamic activity/workflow support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/227 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-12-13 23:43:02.000 UTC (3 years ago) |
| **Updated** | 2023-01-13 14:36:22.000 UTC |
| **Closed** | 2023-01-13 14:36:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

```python
@activity.defn(dynamic=True)
async def my_activity(name: str, *args: Any) -> Any:
    ...

@workflow.defn(dynamic=True)
class MyWorkflow:
    @workflow.run
    async def run(self, name: str, *args: Any) ->Any:
        ...
```

Something like that. Notes:

* Make sure worker can only have one dynamic activity and/or only one dynamic workflow
* Do we need to support payload passthrough in cases of unknown param/response type?
  * Maybe the signature should _only_ allow `*args: Any` or `payloads: *Payload`? Same for return type?

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-01-13 14:36:22.000 UTC</summary>

Closing in favor of duplicate #242

</details>


---

<a id="214"></a>

### #214: [Feature Request] RPC error improvements

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/214 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-28 15:21:29.000 UTC (3y 1m ago) |
| **Updated** | 2022-12-05 19:33:11.000 UTC |
| **Closed** | 2022-12-05 19:33:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

* Make RPC-based errors extend `temporalio.service.RPCError`
* Have error details that are in https://github.com/temporalio/api/blob/master/temporal/api/errordetails/v1/message.proto show up properly typed (unsure whether to put in `service` or `client` package, try to determine which is most reasonable)

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-12-05 15:45:36.000 UTC</summary>

After some thought, for now we'll just make the gRPC status available with the `Any` details they can use if they must

</details>


---

<a id="210"></a>

### #210: [Bug] Sandboxed classes should be able to extend restricted classes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/210 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-22 13:06:40.000 UTC (3y 1m ago) |
| **Updated** | 2022-12-02 21:13:03.000 UTC |
| **Closed** | 2022-12-02 21:13:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Known limitation right now

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-12-01 20:17:32.000 UTC</summary>

We'll workaround where the class extension works, but using does not for now

</details>


---

<a id="208"></a>

### #208: [Bug] Sandbox importer not respecting relative levels for pass through

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/208 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-18 16:52:24.000 UTC (3y 1m ago) |
| **Updated** | 2022-12-02 21:13:04.000 UTC |
| **Closed** | 2022-12-02 21:13:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Some relative imports are respected, but ones that call `__import__` with a level > 0 are not properly. This surfaced during a non-binary install of Pydantic for #207. We need to mimic https://github.com/python/cpython/blob/c8c6113398ee9a7867fe9b08bc539cceb61e2aaa/Lib/importlib/_bootstrap.py#L1255-L1258.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-11-18 17:00:38.000 UTC</summary>

Here is the helper needed to build the fully qualified name:

```python
def _resolve_module_name(name: str, globals: Optional[Mapping[str, object]], level: int) -> str:
    if level == 0:
        return name
    # Calc the package from globals
    package = _calc___package__(globals or {})
    # Logic taken from importlib._resolve_name
    bits = package.rsplit(".", level - 1)
    if len(bits) < level:
        raise ImportError("Attempted relative import beyond top-level package")
    base = bits[0]
    return f"{base}.{name}" if name else base

# Copied from importlib._calc__package__
def _calc___package__(globals: Mapping[str, object]) -> str:
    """Calculate what __package__ should be.
    __package__ is not guaranteed to be defined or could be set to None
    to represent that its proper value is unknown.
    """
    package = globals.get("__package__")
    spec = globals.get("__spec__")
    if package is not None:
        if spec is not None and package != spec.parent:
            warnings.warn("__package__ != __spec__.parent "
                           f"({package!r} != {spec.parent!r})",
                           DeprecationWarning, stacklevel=3)
        return package
    elif spec is not None:
        return spec.parent
    else:
        warnings.warn("can't resolve package from __spec__ or __package__, "
                       "falling back on __name__ and __path__",
                       ImportWarning, stacklevel=3)
        package = globals["__name__"]
        if "__path__" not in globals:
            package = package.rpartition('.')[0]
    return package
```

</details>


---

<a id="201"></a>

### #201: [Bug] Cannot even import concurrent.futures.ProcessPoolExecutor in sandbox

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/201 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-14 23:31:19.000 UTC (3y 1m ago) |
| **Updated** | 2022-11-15 22:00:14.000 UTC |
| **Closed** | 2022-11-15 22:00:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

`from concurrent.futures import ProcessPoolExecutor` causes workflow failure even if unused. Need to see why.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-11-14 23:33:10.000 UTC</summary>

Same for `concurrent.futures.ThreadPoolExecutor`

</details>


---

<a id="182"></a>

### #182: [Feature Request] Unit test for protos as input/output of workflows in sandbox

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/182 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-01 22:48:03.000 UTC (3y 2m ago) |
| **Updated** | 2022-12-01 23:17:55.000 UTC |
| **Closed** | 2022-12-01 23:17:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Protobuf static C state is a problem reloading import modules until https://github.com/protocolbuffers/protobuf/issues/10075 is merged

### Describe the solution you'd like

Explain that protobufs have to be added as passthrough modules until that is fix, and add a unit test that we can clean up when that's fixed.

Also relates to #181 because 3.x will never work with reloading it seems (ref https://github.com/protocolbuffers/protobuf/issues/10143)

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-12-01 23:17:55.000 UTC</summary>

This was merged with #215

</details>


---

<a id="173"></a>

### #173: V1 - is there any esitimation date for V1 ?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/173 |
| **State** | CLOSED |
| **Author** | niros1 |
| **Created** | 2022-10-29 08:39:58.000 UTC (3y 2m ago) |
| **Updated** | 2022-10-31 12:42:22.000 UTC |
| **Closed** | 2022-10-31 12:42:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

*No description provided.*

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-10-31 12:42:15.000 UTC</summary>

Hopefully soon! :-) Closing this as it is not an issue. Feel free to come discuss via Slack or forum: https://temporal.io/community.

</details>


---

<a id="171"></a>

### #171: [Bug] Investigate "legacy query response along with other commands"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/171 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-28 15:11:32.000 UTC (3y 2m ago) |
| **Updated** | 2022-11-10 18:47:54.000 UTC |
| **Closed** | 2022-11-10 18:47:54.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

During `test_workflow_dataclass_typed` (especially so if you run with the Java test server), the following error occurs:

> RuntimeError: Completion failure: Lang SDK sent us a malformed workflow completion for run (77f79520-d7cc-4fbc-beda-fc41aa6450f2): Workflow completion had a legacy query response along with other commands. This is not allowed and constitutes an error in the lang SDK

Originally this was thought to be https://github.com/temporalio/sdk-core/issues/390 but it occurs very rarely even against Temporal(ite). Investigate what is happening here.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-11-02 20:11:28.000 UTC</summary>

It is https://github.com/temporalio/sdk-core/issues/390

</details>


---

<a id="163"></a>

### #163: [Bug] Query return type hint not being validated against

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/163 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-20 18:22:47.000 UTC (3y 2m ago) |
| **Updated** | 2022-10-25 20:05:20.000 UTC |
| **Closed** | 2022-10-25 20:04:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

If the called return type of a query is different than actual return type of query, does not seem to be failing on decode.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-10-20 18:25:29.000 UTC</summary>

Nevermind, looks ok

</details>


---

<a id="161"></a>

### #161: github.com/temporalio/temporalite-v0.0.0-20220801155940-a4b633da82bc: 1 vulnerabilities (highest severity is: 7.5) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/161 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2022-10-19 20:08:49.000 UTC (3y 2m ago) |
| **Updated** | 2022-10-28 12:24:47.000 UTC |
| **Closed** | 2022-10-28 12:24:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>github.com/temporalio/temporalite-v0.0.0-20220801155940-a4b633da82bc</b></p></summary>

<p></p>
<p>


<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/efc1c7babb9a81afdd5a8915f64787c5c5ea4f98">efc1c7babb9a81afdd5a8915f64787c5c5ea4f98</a></p></details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in | Remediation Available |
| ------------- | ------------- | ----- | ----- | ----- | --- | --- |
| [CVE-2022-27664](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-27664) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | golang.org/x/net-v0.0.0-20220708220712-1185a9018129 | Transitive | N/A | &#10060; |


## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2022-27664</summary>


###  Vulnerable Library - <b>golang.org/x/net-v0.0.0-20220708220712-1185a9018129</b></p>

<p></p>
<p>Library home page: <a href="https://proxy.golang.org/golang.org/x/net/@v/v0.0.0-20220708220712-1185a9018129.zip">https://proxy.golang.org/golang.org/x/net/@v/v0.0.0-20220708220712-1185a9018129.zip</a></p>
<p>

Dependency Hierarchy:
  - github.com/temporalio/temporalite-v0.0.0-20220801155940-a4b633da82bc (Root Library)
    - go.temporal.io/sdk-v1.15.0
      - github.com/grpc-ecosystem/go-grpc-middleware-v1.3.0
        - :x: **golang.org/x/net-v0.0.0-20220708220712-1185a9018129** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/efc1c7babb9a81afdd5a8915f64787c5c5ea4f98">efc1c7babb9a81afdd5a8915f64787c5c5ea4f98</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In net/http in Go before 1.18.6 and 1.19.x before 1.19.1, attackers can cause a denial of service because an HTTP/2 connection can hang during closing if shutdown were preempted by a fatal error.

<p>Publish Date: 2022-09-06
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-27664>CVE-2022-27664</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

</details>

#### Comments (1)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-10-28 12:24:46.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="155"></a>

### #155: [Feature Request] Add metric temporality to telemetry config

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/155 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-13 12:29:53.000 UTC (3y 2m ago) |
| **Updated** | 2022-10-13 12:45:29.000 UTC |
| **Closed** | 2022-10-13 12:45:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/sdk-core/pull/402 (and https://github.com/temporalio/sdk-core/pull/404)

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-10-13 12:45:29.000 UTC</summary>

Dupe of https://github.com/temporalio/sdk-python/issues/141

</details>


---

<a id="154"></a>

### #154: [Feature Request] Support multi-workflow replayer

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/154 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-13 12:28:40.000 UTC (3y 2m ago) |
| **Updated** | 2022-11-01 21:40:20.000 UTC |
| **Closed** | 2022-11-01 21:40:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

* Support lazy history fetching
  * Should be able to stream-map on top of lazy list from https://github.com/temporalio/sdk-python/issues/153
  * Need to make sure workflow ID is included with history
* Change replayer to accept iterable of histories instead of a single one (maybe keep the single shortcut) and use https://github.com/temporalio/sdk-core/pull/400 to stream them in

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-10-18 22:32:46.000 UTC</summary>

Also, come back to #158 and do https://github.com/temporalio/sdk-python/pull/158#discussion_r998487416

</details>


---

<a id="152"></a>

### #152: [Feature Request] Custom failure converters

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/152 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-13 12:23:09.000 UTC (3y 2m ago) |
| **Updated** | 2022-10-22 00:16:05.000 UTC |
| **Closed** | 2022-10-13 12:45:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/sdk-typescript/pull/887 and https://github.com/temporalio/sdk-go/pull/924

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-10-13 12:45:43.000 UTC</summary>

Dupe of https://github.com/temporalio/sdk-python/issues/142

</details>


---

<a id="151"></a>

### #151: [Bug] Telemetry not initialized for replayer

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/151 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-12 15:04:09.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-03 14:36:07.000 UTC |
| **Closed** | 2022-11-03 14:36:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

> pyo3_runtime.PanicException: Tried to use a metric but telemetry has not been initialized

Caused by not initializing process wide Telemetry when using replayer without client.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-11-03 14:36:06.000 UTC</summary>

This was solved in #158 

</details>


---

<a id="149"></a>

### #149: [Bug] "History is out of order" reported when running two workers for the same queue

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/149 |
| **State** | CLOSED |
| **Author** | imsut (Ken Kawamoto) |
| **Created** | 2022-10-10 13:59:02.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-10 18:47:54.000 UTC |
| **Closed** | 2022-11-10 18:47:54.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->
I'm running two `Worker`s in a single Python process to achieve task priority. (e.g. running two workers for a queue X and one for a queue Y if I want relative priorities being 2:1.)

As a start, I run just two workers for the same queue with an activity that randomly raises an Exception to emulate transient failures.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

The worker process shows an error message:
```
  2022-10-10T13:28:26.616940Z ERROR temporal_sdk_core::worker::workflow::managed_run: Error in run machines, error: RunUpdateErr { source: Fatal("History is out of order. Last processed event: 33, event id: 44"), complete_resp_chan: None }

  2022-10-10T13:28:26.617278Z  WARN temporal_sdk_core::worker::workflow::workflow_stream: Error while updating workflow, error: Fatal("History is out of order. Last processed event: 33, event id: 44"), run_id: f31c2102-7b86-4fa1-8822-96c0b2c3b648
```


<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

"worker.py" (entry point)
```
import asyncio

from temporalio.worker import Worker

from playground.workflow import GreetingWorkflow, compose_greeting
from playground.client import create_client


async def run_worker():
    client = await create_client()

    worker1 = Worker(
        client,
        task_queue="my-task-queue",
        workflows=[GreetingWorkflow],
        activities=[compose_greeting],
    )

    worker2 = Worker(
        client,
        task_queue="my-task-queue",
        workflows=[GreetingWorkflow],
        activities=[compose_greeting],
    )

    await asyncio.gather(worker1.run(), worker2.run())


if __name__ == "__main__":
    asyncio.run(run_worker())
```

"client.py"
```
import os
from temporalio.client import Client
from temporalio.service import TLSConfig


async def create_client() -> Client:
    with open("/opt/keys/temporal-poc/client-cert.pem", "rb") as f:
        client_cert = f.read()
    with open("/opt/keys/temporal-poc/client-key.pem", "rb") as f:
        client_key = f.read()

    # Start client
    return await Client.connect(
        os.getenv("TEMPORAL_HOST"),
        namespace=os.getenv("TEMPORAL_NS"),
        tls=TLSConfig(
            client_cert=client_cert,
            client_private_key=client_key,
        ),
    )
```

"workflow.py"
```
import asyncio
from dataclasses import dataclass
from datetime import timedelta
import sys

import random

from temporalio import activity, workflow


@dataclass
class ComposeGreetingInput:
    greeting: str
    name: str


@activity.defn
async def compose_greeting(input: ComposeGreetingInput) -> str:
    activity.logger.info("Running activity with parameter %s" % input)

    r = random.randint(0, 4)
    if r <= 1:
        raise Exception("sorry!!")

    sys.stderr.write("processing...\n")
    return f"{input.greeting}, {input.name}!"


@workflow.defn
class GreetingWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        workflow.logger.info("Running workflow with parameter %s" % name)

        activities = [
            workflow.execute_activity(
                compose_greeting,
                ComposeGreetingInput("Hello", name),
                start_to_close_timeout=timedelta(seconds=3),
            )
            for i in range(8)
        ]

        tpl = await asyncio.gather(*activities)
        return " and ".join(tpl)
```

"invoker.py" (a process that triggers a workflow execution.)
```
import asyncio
import logging

from temporalio import activity, workflow

from playground.workflow import GreetingWorkflow
from playground.client import create_client


async def main():
    # Uncomment the line below to see logging
    # logging.basicConfig(level=logging.INFO)

    # Start client
    client = await create_client()

    result = await client.execute_workflow(
        GreetingWorkflow.run,
        "World",
        id="hello-activity-workflow-id",
        task_queue="my-task-queue",
    )
    print(f"Result: {result}")


if __name__ == "__main__":
    asyncio.run(main())
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: M1 Mac (with Temporal Cloud)
- Temporal Version: temporalio 0.1b2 with Python 3.7.13
- Are you using Docker or Kubernetes or building Temporal from source?: Temporal Cloud

### Additional context

<!-- Add any other context about the problem here. -->


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-11-03 15:16:46.000 UTC</summary>

After some time, I have finally been able to replicate. I have opened https://github.com/temporalio/sdk-core/issues/428 to track this.

In the meantime, it is rarely ever necessary to run the multiple workers for the same task queue in the same process. There may be a misunderstanding about what workers do. Can you share your use case more? I recall our Slack discussion and would like to continue it there or here to understand what you are trying to do.



</details>


---

<a id="147"></a>

### #147: Clarify story around using a client from a different asyncio loop than it was created in

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/147 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-30 18:34:08.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-07 16:08:36.000 UTC |
| **Closed** | 2022-11-07 16:08:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Users reporting that the client may not work if created in different event loop

### Describe the solution you'd like

Confirm this is the case, document if necessary, error if reasonable.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-11-04 14:51:04.000 UTC</summary>

I have confirmed that a client can run in a different event loop than it was created, but it is _not_ thread safe and therefore should not be run in a separate event loop (or separate thread). I will document this.

</details>


---

<a id="145"></a>

### #145: [Bug] Cancelled workflow returns WorkflowExecutionStatus.FAILED status in workflow description

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/145 |
| **State** | CLOSED |
| **Author** | lambyqq (Siyang Xie) |
| **Created** | 2022-09-28 19:30:21.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-07 14:52:22.000 UTC |
| **Closed** | 2022-11-07 14:52:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Cancel a running workflow and check the status in `wf_handle.describe().status`

### Describe the bug

Expect the status to be `WorkflowExecutionStatus.CANCELED`, but got `WorkflowExecutionStatus.FAILED`

### Minimal Reproduction

cancel a workflow and get its status

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- Temporal Version: [e.g. 1.14.0?] and/or SDK version
python-SDK: temporalio==0.1b1


### Additional context

I have to use a workaround to tell if workflow is cancelled:
```
wf_description.status == WorkflowExecutionStatus.FAILED
            and isinstance(wf_error.cause, CancelledError)
```


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-09-28 19:51:49.000 UTC</summary>

Thanks, I have confirmed at https://github.com/temporalio/sdk-python/blob/49ccd563c1120425fc78b792c7c9e02bf3670f1c/temporalio/worker/workflow_instance.py#L1229-L1236 we are not doing the right thing. We will solve this shortly.

Note to self/implementer:

* Change to `cancel_workflow_execution` instead of `fail_workflow_execution` and confirm in tests
* Confirm in tests what happens if a cancellation error is raised outside of a cancellation request

</details>


---

<a id="142"></a>

### #142: [Feature Request] Support FailureConverter concept

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/142 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-26 16:43:25.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-07 14:17:03.000 UTC |
| **Closed** | 2022-11-07 14:17:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/sdk-typescript/pull/887. Also relates to #131.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-10-25 18:52:20.000 UTC</summary>

This is waiting on #164 

</details>


---

<a id="140"></a>

### #140: [Feature Request] Better title and favicon for python.temporal.io

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/140 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2022-09-19 22:16:12.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-07 14:52:22.000 UTC |
| **Closed** | 2022-11-07 14:52:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

This is what it currently looks like:

<img width="78" alt="image" src="https://user-images.githubusercontent.com/52304/191129217-d4307a97-46b7-4a97-8543-4d95184b4a55.png">


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-11-03 17:36:29.000 UTC</summary>

Am adding favicon but not altering title. It is not easy to do and it's basically just as good as others. Here is Python, TypeScript, Go, and Java:

![image](https://user-images.githubusercontent.com/206396/199794085-6a5e70e8-14fc-417a-ac35-6dcea5bec749.png)

Yes, Python is missing the suffix, but it's not worth the effort of a custom template.


</details>


---

<a id="139"></a>

### #139: Expose default sequence of payload converters for custom converter creators

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/139 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-19 19:28:31.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-07 14:17:04.000 UTC |
| **Closed** | 2022-11-07 14:17:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Right now this is buried in the default constructor, but people may want this list for other reasons

### Describe the solution you'd like

Make the list available

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-10-12 21:12:54.000 UTC</summary>

Also, just make creating a data converter (and payload converter) with a different encoding payload converter prepended to the default list easier. Maybe something like `temporalio.converter.default().with_encoding_payload_converter(...`.

</details>


---

<a id="133"></a>

### #133: temporal-sdk-core-protos-0.1.0: 1 vulnerabilities (highest severity is: 7.5) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/133 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2022-09-15 16:27:25.000 UTC (3y 3m ago) |
| **Updated** | 2022-10-19 20:08:52.000 UTC |
| **Closed** | 2022-10-19 20:08:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>temporal-sdk-core-protos-0.1.0</b></p></summary>

<p></p>
<p>

</details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in | Remediation Available |
| ------------- | ------------- | ----- | ----- | ----- | --- | --- |
| [CVE-2022-3212](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-3212) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | detected in multiple dependencies | Transitive | N/A | &#10060; |


## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2022-3212</summary>


###  Vulnerable Libraries - <b>axum-core-0.2.7.crate</b>, <b>axum-0.5.15.crate</b></p>
<p>


### <b>axum-core-0.2.7.crate</b></p>

<p>Core types and traits for axum</p>
<p>Library home page: <a href="https://crates.io/api/v1/crates/axum-core/0.2.7/download">https://crates.io/api/v1/crates/axum-core/0.2.7/download</a></p>
<p>

Dependency Hierarchy:
  - temporal-sdk-core-protos-0.1.0 (Root Library)
    - tonic-0.8.0.crate
      - axum-0.5.15.crate
        - :x: **axum-core-0.2.7.crate** (Vulnerable Library)


### <b>axum-0.5.15.crate</b></p>

<p>Web framework that focuses on ergonomics and modularity</p>
<p>Library home page: <a href="https://crates.io/api/v1/crates/axum/0.5.15/download">https://crates.io/api/v1/crates/axum/0.5.15/download</a></p>
<p>

Dependency Hierarchy:
  - temporal-sdk-core-protos-0.1.0 (Root Library)
    - tonic-0.8.0.crate
      - :x: **axum-0.5.15.crate** (Vulnerable Library)


<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
<bytes::Bytes as axum_core::extract::FromRequest>::from_request would not, by default, set a limit for the size of the request body. That meant if a malicious peer would send a very large (or infinite) body your server might run out of memory and crash. This also applies to these extractors which used Bytes::from_request internally: axum::extract::Form axum::extract::Json String

<p>Publish Date: 2022-09-14
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-3212>CVE-2022-3212</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://rustsec.org/advisories/RUSTSEC-2022-0055.html">https://rustsec.org/advisories/RUSTSEC-2022-0055.html</a></p>
<p>Release Date: 2022-09-14</p>
<p>Fix Resolution: axum-core - 0.2.8</p>

</p>

<p></p>

</details>

#### Comments (1)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-10-19 20:08:52.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="131"></a>

### #131: [Feature Request] Apply codecs to `Failure.encodedAttributes`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/131 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2022-09-14 21:05:34.000 UTC (3y 3m ago) |
| **Updated** | 2022-11-07 14:17:04.000 UTC |
| **Closed** | 2022-11-07 14:17:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

Recently added Payload to the `Failure` message, needs to be run through the payload codecs.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-10-25 18:52:38.000 UTC</summary>

This is waiting on #164

</details>


---

<a id="120"></a>

### #120: Convert all tests to use test framework and multiple server forms

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/120 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-30 22:19:35.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-16 20:23:23.000 UTC |
| **Closed** | 2022-09-16 20:23:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Currently all tests use a dynamically-built Go binary that wraps Temporalite with TLS and non-TLS.

### Describe the solution you'd like

* Change all client/workflow tests to inject a test environment instead of the Go-based one
* Change all activity tests to either use a local non-Go workflow or to use the activity test environment
* Make CI run all tests on both the time-skipping test server and the Temporalite test server (will leave docker-compose test server for the future)
* Remove all Go from CI and code

Depends on #81 

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-09-01 20:52:36.000 UTC</summary>

Also, from #121, document that `on_heartbeat` for activity tests is per call but we actually don't send to the server every heartbeat call

</details>


---

<a id="115"></a>

### #115: [Bug] Setting memo on executing child flow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/115 |
| **State** | CLOSED |
| **Author** | adrianbeloqui (Adrian Beloqui) |
| **Created** | 2022-08-29 21:03:22.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-07 22:29:17.000 UTC |
| **Closed** | 2022-09-07 22:29:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I am writing some example workflows and activities to test the capabilities of this library. As I was doing so, while trying to execute a child flow with the memo argument set I get an exception that based on my very short Google search it is due to the way the value of the memo is being set on the internal object.

### Describe the bug

Setting a value for the memo parameter on `workflow.execute_child_worflow` function throws the exception `ValueError: Direct assignment of submessage not allowed`

Here is the stacktrace:

```bash
2022-08-29 20:48:27,646 - WARNING - Failed activation on workflow EventAutomation with ID event-automation and run ID 9dca34d6-af85-4992-8abc-dd18d5a7b6da
Traceback (most recent call last):
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_instance.py", line 266, in activate
    self._run_once()
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_instance.py", line 1153, in _run_once
    raise self._current_activation_error
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_instance.py", line 1170, in _run_top_level_workflow_function
    await coro
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_instance.py", line 601, in run_workflow
    result = await self._inbound.execute_workflow(input)
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_instance.py", line 1405, in execute_workflow
    return await input.run_fn(*args)
  File "/workspaces/saul-sample-temporal-python/temporal_python/workflows/event_automation.py", line 84, in run
    incident = await self.create_incident(params)
  File "/workspaces/saul-sample-temporal-python/temporal_python/workflows/event_automation.py", line 35, in create_incident
    incident = await workflow.execute_child_workflow(
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/workflow.py", line 3095, in execute_child_workflow
    handle = await _Runtime.current().workflow_start_child_workflow(
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_instance.py", line 846, in workflow_start_child_workflow
    return await self._outbound.start_child_workflow(
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_instance.py", line 1475, in start_child_workflow
    return await self._instance._outbound_start_child_workflow(input)
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_instance.py", line 1052, in _outbound_start_child_workflow
    handle._apply_start_command(self._add_command())
  File "/home/vscode/.cache/pypoetry/virtualenvs/temporal-python--nLrvWXT-py3.10/lib/python3.10/site-packages/temporalio/worker/workflow_instance.py", line 1739, in _apply_start_command
    v.memo[k] = self._instance._payload_converter.to_payloads([val])[0]
ValueError: Direct assignment of submessage not allowed
```

### Minimal Reproduction

```python
await workflow.execute_child_workflow(
            "ChildWorkflow",
            {"key1": 1, "key2": 2},
            id="child-workflow-1",
            task_timeout=timedelta(seconds=120),
            memo={"my_memo_key": "my memo value"},
            retry_policy=RetryPolicy(
                backoff_coefficient=2.0,
                maximum_interval=timedelta(seconds=120),
                maximum_attempts=5,
                non_retryable_error_types=["MyCustomNonRetriableException"],
            ),
            search_attributes={"search_attribute1": [""], "search_attribute2": [""]}
)
```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Mac running Colima
- Temporal Version: `temporalio = "^0.1b1"`
- Are you using Docker or Kubernetes or building Temporal from source? Using Temporal's docker-compose provided environment. Executing workflow from a VSCode Dev Container using Python 3.10.6

### Additional context

Nothing to add at this point


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-08-29 21:15:25.000 UTC</summary>

Thanks! This is definitely a bug where I am not using the proto library properly at https://github.com/temporalio/sdk-python/blob/4db14cc7cdc01b83be656b9086ecf77aeacfac8b/temporalio/worker/workflow_instance.py#L1739 and I did not properly cover this in tests. To give background, with the Python proto SDK, you can't set messages directly on maps/lists. See https://developers.google.com/protocol-buffers/docs/reference/python-generated#map-fields in this case. You have to lazily set scalar fields only via lazy message field access. I will fix this, thanks!

Reactions: ðŸ‘ 1

</details>


---

<a id="114"></a>

### #114: Add max task queue activities per second

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/114 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-25 14:21:01.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-07 22:29:17.000 UTC |
| **Closed** | 2022-09-07 22:29:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

This is in TypeScript and needs to be in Python.

#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2022-08-25 15:41:44.000 UTC</summary>

Max activities per second is also missing in Python.

Reactions: ðŸ‘ 1

</details>


---

<a id="111"></a>

### #111: Change child workflow cancel to issue the one command

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/111 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-19 19:09:23.000 UTC (3y 4m ago) |
| **Updated** | 2022-10-31 19:32:48.000 UTC |
| **Closed** | 2022-10-31 19:32:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Currently we issue different child workflow commands based on whether child workflow has started or not

### Describe the solution you'd like

With https://github.com/temporalio/sdk-core/pull/379 we can now just use one command

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-10-31 19:32:48.000 UTC</summary>

This was done as part of #158 

</details>


---

<a id="110"></a>

### #110: Ensure accepted iterables are iterated eagerly and stored as lists

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/110 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-18 21:56:54.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-07 22:29:17.000 UTC |
| **Closed** | 2022-09-07 22:29:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

We accept iterables but sometimes store the iterables which makes little sense for ones that can only iterate once.

### Describe the solution you'd like

Convert all iterables into lists eagerly when accepted.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-08-31 15:48:40.000 UTC</summary>

Rather, just accept `Sequence` instead of `Iterable` everywhere. This is what other Python libraries seem to do.

</details>


---

<a id="105"></a>

### #105: go.temporal.io/server-v1.17.1: 6 vulnerabilities (highest severity is: 7.5) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/105 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2022-08-12 12:20:21.000 UTC (3y 5m ago) |
| **Updated** | 2022-10-28 13:23:29.000 UTC |
| **Closed** | 2022-10-28 13:23:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>go.temporal.io/server-v1.17.1</b></p></summary>

<p></p>
<p>


<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/efc1c7babb9a81afdd5a8915f64787c5c5ea4f98">efc1c7babb9a81afdd5a8915f64787c5c5ea4f98</a></p></details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in (go.temporal.io/server-v1.17.1 version) | Remediation Available |
| ------------- | ------------- | ----- | ----- | ----- | ------------- | --- |
| [CVE-2019-0205](https://www.mend.io/vulnerability-database/CVE-2019-0205) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | detected in multiple dependencies | Transitive | N/A* | &#10060; |
| [CVE-2022-27664](https://www.mend.io/vulnerability-database/CVE-2022-27664) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | golang.org/x/net-v0.0.0-20220708220712-1185a9018129 | Transitive | N/A* | &#10060; |
| [CVE-2022-32149](https://www.mend.io/vulnerability-database/CVE-2022-32149) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | golang.org/x/text-v0.3.7 | Transitive | N/A* | &#10060; |
| [CVE-2019-0210](https://www.mend.io/vulnerability-database/CVE-2019-0210) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | detected in multiple dependencies | Transitive | N/A* | &#10060; |
| [CVE-2018-11798](https://www.mend.io/vulnerability-database/CVE-2018-11798) | <img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> Medium | 6.5 | github.com/apache/thrift-0.10.0 | Transitive | N/A* | &#10060; |
| [CVE-2020-28928](https://www.mend.io/vulnerability-database/CVE-2020-28928) | <img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> Medium | 5.5 | modernc.org/libc-v1.16.10 | Transitive | N/A* | &#10060; |
<p>*For some transitive vulnerabilities, there is no version of direct dependency with a fix. Check the section "Details" below to see if there is a version of transitive dependency where vulnerability is fixed.</p>

## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2019-0205</summary>


###  Vulnerable Libraries - <b>github.com/apache/thrift-0.10.0</b>, <b>github.com/uber-go/tally/v4-v4.1.2</b></p>
<p>


### <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.17.1 (Root Library)
    - github.com/uber/tchannel-go-v1.22.3
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)


### <b>github.com/uber-go/tally/v4-v4.1.2</b></p>

<p>A Go metrics interface with fast buffered metrics and third party reporters</p>
<p>Library home page: <a href="https://proxy.golang.org/github.com/uber-go/tally/v4/@v/v4.1.2.zip">https://proxy.golang.org/github.com/uber-go/tally/v4/@v/v4.1.2.zip</a></p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.17.1 (Root Library)
    - :x: **github.com/uber-go/tally/v4-v4.1.2** (Vulnerable Library)


<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/efc1c7babb9a81afdd5a8915f64787c5c5ea4f98">efc1c7babb9a81afdd5a8915f64787c5c5ea4f98</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In Apache Thrift all versions up to and including 0.12.0, a server or client may run into an endless loop when feed with specific input data. Because the issue had already been partially fixed in version 0.11.0, depending on the installed version it affects only certain language bindings.

<p>Publish Date: 2019-10-29
<p>URL: <a href=https://www.mend.io/vulnerability-database/CVE-2019-0205>CVE-2019-0205</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0205">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0205</a></p>
<p>Release Date: 2019-10-29</p>
<p>Fix Resolution: org.apache.thrift:libthrift:0.13.0</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2022-27664</summary>


###  Vulnerable Library - <b>golang.org/x/net-v0.0.0-20220708220712-1185a9018129</b></p>

<p></p>
<p>Library home page: <a href="https://proxy.golang.org/golang.org/x/net/@v/v0.0.0-20220708220712-1185a9018129.zip">https://proxy.golang.org/golang.org/x/net/@v/v0.0.0-20220708220712-1185a9018129.zip</a></p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.17.1 (Root Library)
    - golang.org/x/oauth2-v0.0.0-20220622183110-fd043fe589d2
      - :x: **golang.org/x/net-v0.0.0-20220708220712-1185a9018129** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/efc1c7babb9a81afdd5a8915f64787c5c5ea4f98">efc1c7babb9a81afdd5a8915f64787c5c5ea4f98</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In net/http in Go before 1.18.6 and 1.19.x before 1.19.1, attackers can cause a denial of service because an HTTP/2 connection can hang during closing if shutdown were preempted by a fatal error.

<p>Publish Date: 2022-09-06
<p>URL: <a href=https://www.mend.io/vulnerability-database/CVE-2022-27664>CVE-2022-27664</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2022-32149</summary>


###  Vulnerable Library - <b>golang.org/x/text-v0.3.7</b></p>

<p></p>
<p>Library home page: <a href="https://proxy.golang.org/golang.org/x/text/@v/v0.3.7.zip">https://proxy.golang.org/golang.org/x/text/@v/v0.3.7.zip</a></p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.17.1 (Root Library)
    - golang.org/x/oauth2-v0.0.0-20220622183110-fd043fe589d2
      - golang.org/x/net-v0.0.0-20220708220712-1185a9018129
        - :x: **golang.org/x/text-v0.3.7** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/efc1c7babb9a81afdd5a8915f64787c5c5ea4f98">efc1c7babb9a81afdd5a8915f64787c5c5ea4f98</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
An attacker may cause a denial of service by crafting an Accept-Language header which ParseAcceptLanguage will take significant time to parse.

<p>Publish Date: 2022-10-14
<p>URL: <a href=https://www.mend.io/vulnerability-database/CVE-2022-32149>CVE-2022-32149</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://www.cve.org/CVERecord?id=CVE-2022-32149">https://www.cve.org/CVERecord?id=CVE-2022-32149</a></p>
<p>Release Date: 2022-10-14</p>
<p>Fix Resolution: v0.3.8</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2019-0210</summary>


###  Vulnerable Libraries - <b>github.com/uber-go/tally/v4-v4.1.2</b>, <b>github.com/apache/thrift-0.10.0</b></p>
<p>


### <b>github.com/uber-go/tally/v4-v4.1.2</b></p>

<p>A Go metrics interface with fast buffered metrics and third party reporters</p>
<p>Library home page: <a href="https://proxy.golang.org/github.com/uber-go/tally/v4/@v/v4.1.2.zip">https://proxy.golang.org/github.com/uber-go/tally/v4/@v/v4.1.2.zip</a></p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.17.1 (Root Library)
    - :x: **github.com/uber-go/tally/v4-v4.1.2** (Vulnerable Library)


### <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.17.1 (Root Library)
    - github.com/uber/tchannel-go-v1.22.3
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)


<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/efc1c7babb9a81afdd5a8915f64787c5c5ea4f98">efc1c7babb9a81afdd5a8915f64787c5c5ea4f98</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In Apache Thrift 0.9.3 to 0.12.0, a server implemented in Go using TJSONProtocol or TSimpleJSONProtocol may panic when feed with invalid input data.

<p>Publish Date: 2019-10-29
<p>URL: <a href=https://www.mend.io/vulnerability-database/CVE-2019-0210>CVE-2019-0210</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="http://mail-archives.apache.org/mod_mbox/thrift-dev/201910.mbox/%3C277A46CA87494176B1BBCF5D72624A2A%40HAGGIS%3E">http://mail-archives.apache.org/mod_mbox/thrift-dev/201910.mbox/%3C277A46CA87494176B1BBCF5D72624A2A%40HAGGIS%3E</a></p>
<p>Release Date: 2019-10-29</p>
<p>Fix Resolution: 0.13.0</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> CVE-2018-11798</summary>


###  Vulnerable Library - <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.17.1 (Root Library)
    - github.com/uber/tchannel-go-v1.22.3
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/efc1c7babb9a81afdd5a8915f64787c5c5ea4f98">efc1c7babb9a81afdd5a8915f64787c5c5ea4f98</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
The Apache Thrift Node.js static web server in versions 0.9.2 through 0.11.0 have been determined to contain a security vulnerability in which a remote user has the ability to access files outside the set webservers docroot path.

<p>Publish Date: 2019-01-07
<p>URL: <a href=https://www.mend.io/vulnerability-database/CVE-2018-11798>CVE-2018-11798</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>6.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: Low
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: High
  - Integrity Impact: None
  - Availability Impact: None
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11798">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11798</a></p>
<p>Release Date: 2019-01-07</p>
<p>Fix Resolution: v0.12.0</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> CVE-2020-28928</summary>


###  Vulnerable Library - <b>modernc.org/libc-v1.16.10</b></p>

<p></p>
<p>Library home page: <a href="https://proxy.golang.org/modernc.org/libc/@v/v1.16.10.zip">https://proxy.golang.org/modernc.org/libc/@v/v1.16.10.zip</a></p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.17.1 (Root Library)
    - modernc.org/sqlite-v1.17.3
      - :x: **modernc.org/libc-v1.16.10** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/efc1c7babb9a81afdd5a8915f64787c5c5ea4f98">efc1c7babb9a81afdd5a8915f64787c5c5ea4f98</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In musl libc through 1.2.1, wcsnrtombs mishandles particular combinations of destination buffer size and source character limit, as demonstrated by an invalid write access (buffer overflow).

<p>Publish Date: 2020-11-24
<p>URL: <a href=https://www.mend.io/vulnerability-database/CVE-2020-28928>CVE-2020-28928</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>5.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Local
  - Attack Complexity: Low
  - Privileges Required: Low
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://nvd.nist.gov/vuln/detail/CVE-2020-28928">https://nvd.nist.gov/vuln/detail/CVE-2020-28928</a></p>
<p>Release Date: 2020-11-24</p>
<p>Fix Resolution: musl - 1.2.2-1,1.2.2-1,1.1.16-3+deb9u1</p>

</p>

<p></p>

</details>

#### Comments (1)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-10-28 13:23:29.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="103"></a>

### #103: [Feature Request] `retry_policy` on `continue_as_new`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/103 |
| **State** | CLOSED |
| **Author** | patte (Patrick Recher) |
| **Created** | 2022-08-09 20:28:10.000 UTC (3y 5m ago) |
| **Updated** | 2022-09-07 22:29:17.000 UTC |
| **Closed** | 2022-09-07 22:29:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

From https://github.com/temporalio/sdk-java/issues/1200 and https://github.com/temporalio/sdk-java/issues/1201:
> Right now continueAsNew is implemented in a way that assumes that most parameters are carried over by the server if not specified on the continueAsNew request. While in fact, it's the opposite. Server doesn't carry over most parameters.

If in the following code:
- raise on `iteration == 0`: The workflow is run (retried) 3 time before ending with `FAILD` (retryState: `MaximumAttemptsReached`)
- raise on `iteration == 1`: The workflow runs one time successfully `Continued as New`. The next run it ends with `Failed` (retryState: `RetryPolicyNotSet`).

I'd expect the workflow to retry 3 time before giving up.

```python
import asyncio
import logging
from dataclasses import dataclass
from typing import NoReturn
from datetime import timedelta

from temporalio import workflow, activity
from temporalio.client import Client
from temporalio.worker import Worker
from temporalio.common import RetryPolicy

@dataclass
class ComposeGreetingInput:
    greeting: str
    name: str

@activity.defn
async def compose_greeting(input: ComposeGreetingInput) -> NoReturn:
    # Always raise exception
    raise RuntimeError(f"Greeting exception: {input.greeting}, {input.name}!")


@workflow.defn(name="LoopingWorkflow")
class LoopingWorkflow:
    @workflow.run
    async def run(self, iteration: int) -> None:
        if iteration == 40:
            return
        workflow.logger.info("Running workflow iteration %s", iteration)
        await asyncio.sleep(1)
        
        # on which iteration to raise
        if (iteration == 0):
            await workflow.execute_activity(
                compose_greeting,
                ComposeGreetingInput("Hello", f"{iteration}"),
                start_to_close_timeout=timedelta(seconds=10),
                retry_policy=RetryPolicy(maximum_attempts=1),
            )

        workflow.continue_as_new(
            iteration + 1,
            #TypeError: continue_as_new() got an unexpected keyword argument 'retry_policy'
            #retry_policy=RetryPolicy(maximum_attempts=3),
        )



async def main():
    # Enable logging for this sample
    logging.basicConfig(level=logging.INFO)

    # Start client
    client = await Client.connect("localhost:7233")

    # Run a worker for the workflow
    async with Worker(
        client,
        task_queue="hello-continue-as-new-task-queue",
        workflows=[LoopingWorkflow],
        activities=[compose_greeting],
    ):

        await client.start_workflow(
            "LoopingWorkflow",
            0,
            id="hello-continue-as-new-workflow-id",
            task_queue="hello-continue-as-new-task-queue",
            retry_policy=RetryPolicy(maximum_attempts=3),
        )

        await asyncio.Future()


if __name__ == "__main__":
    asyncio.run(main())

```

### Describe the solution you'd like

The real solution might be for temporal server to carry over workflow options on continue_as_new. For the time being, we could work around it by adding `retry_policy` to `continue_as_new` (similar to `search_attributes`).  


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-08-09 20:37:47.000 UTC</summary>

Thanks for the report! I have opened an issue to first implement this in our shared core code at https://github.com/temporalio/sdk-core/issues/372 and then we can implement in Python. I will update when complete.

Reactions: â¤ï¸ 1

</details>


---

<a id="88"></a>

### #88: github.com/temporalio/temporal-v1.17.1: 3 vulnerabilities (highest severity is: 7.5) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/88 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2022-08-01 15:41:31.000 UTC (3y 5m ago) |
| **Updated** | 2022-08-12 12:20:24.000 UTC |
| **Closed** | 2022-08-12 12:20:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>github.com/temporalio/temporal-v1.17.1</b></p></summary>

<p></p>
<p>


<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/2288f4128e7e1dfd80ecbde2df89d8e81810860e">2288f4128e7e1dfd80ecbde2df89d8e81810860e</a></p></details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in | Remediation Available |
| ------------- | ------------- | ----- | ----- | ----- | --- | --- |
| [CVE-2019-0205](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0205) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | github.com/apache/thrift-0.10.0 | Transitive | N/A | &#10060; |
| [CVE-2019-0210](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0210) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | github.com/apache/thrift-0.10.0 | Transitive | N/A | &#10060; |
| [CVE-2018-11798](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2018-11798) | <img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> Medium | 6.5 | github.com/apache/thrift-0.10.0 | Transitive | N/A | &#10060; |


## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2019-0205</summary>


###  Vulnerable Library - <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - github.com/temporalio/temporal-v1.17.1 (Root Library)
    - github.com/temporalio/ringpop-go-6f91b5915e95e7b08817aae78fea3a38ea1b5bd2
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/2288f4128e7e1dfd80ecbde2df89d8e81810860e">2288f4128e7e1dfd80ecbde2df89d8e81810860e</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In Apache Thrift all versions up to and including 0.12.0, a server or client may run into an endless loop when feed with specific input data. Because the issue had already been partially fixed in version 0.11.0, depending on the installed version it affects only certain language bindings.

<p>Publish Date: 2019-10-29
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0205>CVE-2019-0205</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0205">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0205</a></p>
<p>Release Date: 2019-10-29</p>
<p>Fix Resolution: org.apache.thrift:libthrift:0.13.0</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2019-0210</summary>


###  Vulnerable Library - <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - github.com/temporalio/temporal-v1.17.1 (Root Library)
    - github.com/temporalio/ringpop-go-6f91b5915e95e7b08817aae78fea3a38ea1b5bd2
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/2288f4128e7e1dfd80ecbde2df89d8e81810860e">2288f4128e7e1dfd80ecbde2df89d8e81810860e</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In Apache Thrift 0.9.3 to 0.12.0, a server implemented in Go using TJSONProtocol or TSimpleJSONProtocol may panic when feed with invalid input data.

<p>Publish Date: 2019-10-29
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0210>CVE-2019-0210</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="http://mail-archives.apache.org/mod_mbox/thrift-dev/201910.mbox/%3C277A46CA87494176B1BBCF5D72624A2A%40HAGGIS%3E">http://mail-archives.apache.org/mod_mbox/thrift-dev/201910.mbox/%3C277A46CA87494176B1BBCF5D72624A2A%40HAGGIS%3E</a></p>
<p>Release Date: 2019-10-29</p>
<p>Fix Resolution: 0.13.0</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> CVE-2018-11798</summary>


###  Vulnerable Library - <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - github.com/temporalio/temporal-v1.17.1 (Root Library)
    - github.com/temporalio/ringpop-go-6f91b5915e95e7b08817aae78fea3a38ea1b5bd2
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/2288f4128e7e1dfd80ecbde2df89d8e81810860e">2288f4128e7e1dfd80ecbde2df89d8e81810860e</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
The Apache Thrift Node.js static web server in versions 0.9.2 through 0.11.0 have been determined to contain a security vulnerability in which a remote user has the ability to access files outside the set webservers docroot path.

<p>Publish Date: 2019-01-07
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2018-11798>CVE-2018-11798</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>6.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: Low
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: High
  - Integrity Impact: None
  - Availability Impact: None
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11798">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11798</a></p>
<p>Release Date: 2019-01-07</p>
<p>Fix Resolution: v0.12.0</p>

</p>

<p></p>

</details>

#### Comments (1)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-08-12 12:20:23.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="84"></a>

### #84: [Bug] M1 Mac grpcio error

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/84 |
| **State** | CLOSED |
| **Author** | ashwin-borneo (Ashwin) |
| **Created** | 2022-07-22 13:06:04.000 UTC (3y 5m ago) |
| **Updated** | 2022-07-22 13:15:56.000 UTC |
| **Closed** | 2022-07-22 13:15:56.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Running temporal python workers on M1 Mac 

### Describe the bug

import cygrpc error when trying to run sample python code 

<img width="1678" alt="Screenshot 2022-07-22 at 6 29 45 PM" src="https://user-images.githubusercontent.com/86338859/180444319-7ced3ed1-f696-40bf-8f1f-cdf6d2f573f3.png">

### Minimal Reproduction

1. create conda env for temporal with python 3.82. 
2. `pip install --no-binary :all: grpcio --ignore-installed`3. 
3. `pip install --no-binary :all: grpcio-tools --ignore-installed`4. 
4. `pip install temporalio` 5. 
Run the sample run_worker.py 

<!-- Please complete the following information where relevant. -->

- OS and processor:  M1 Mac OS 12.4 
- Temporal Version: latest and/or SDK version 0.1a2
- Are you using Docker or Kubernetes or building Temporal from source? Docker Compose to run Temporal

### Additional context




#### Comments (1)

<details>
<summary><strong>ashwin-borneo</strong> commented on 2022-07-22 13:15:56.000 UTC</summary>

FIXED: uninstall grpcio from pip and install via conda. 

</details>


---

<a id="83"></a>

### #83: Add README info for how to disable deadlock detector

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/83 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-07-21 17:43:15.000 UTC (3y 5m ago) |
| **Updated** | 2022-07-25 21:46:29.000 UTC |
| **Closed** | 2022-07-25 21:46:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Put something in the README for this

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-07-25 21:46:29.000 UTC</summary>

Moving to #82 

</details>


---

<a id="81"></a>

### #81: Test framework

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/81 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-07-21 13:14:14.000 UTC (3y 5m ago) |
| **Updated** | 2022-09-01 20:53:15.000 UTC |
| **Closed** | 2022-09-01 20:53:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | beta-2 |

#### Description

### Describe the solution you'd like

Need test framework that leverages the test server

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-08-03 17:54:33.000 UTC</summary>

Needs https://github.com/temporalio/sdk-core/issues/368

</details>


---

<a id="79"></a>

### #79: Accept `host:port` for client instead of only URL

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/79 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-07-20 17:24:56.000 UTC (3y 5m ago) |
| **Updated** | 2022-08-01 15:38:47.000 UTC |
| **Closed** | 2022-08-01 15:38:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | beta-1 |

#### Description

### Is your feature request related to a problem? Please describe.

All other SDKs use `host:port`, we should too

### Describe the solution you'd like

* Change `tls_config: Optional[TLSConfig] = None` to `tls_config: Union[bool, TLSConfig] = False`
* If a scheme is not present (meaning we're backwards compatible here), before sending to core, set it as `http` if `tls_config` is false or `https` otherwise

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-07-20 17:26:35.000 UTC</summary>

Maybe I should rename `tls_config` to `tls`? I think I will... (that's backwards incompat but who cares)

</details>


---

<a id="40"></a>

### #40: Add more robust docs to docstrings

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/40 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-06-07 13:04:01.000 UTC (3y 7m ago) |
| **Updated** | 2022-09-07 22:29:18.000 UTC |
| **Closed** | 2022-09-07 22:29:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

* Module/package level docs should have more docs in the docstrings explaining usage of that package
* Maybe even examples, but don't go overboard

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-09-07 13:26:46.000 UTC</summary>

After some thought, these won't be robust, just minor improvements.

</details>


---

<a id="37"></a>

### #37: Put public doc links in docstrings

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/37 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-06-03 11:25:03.000 UTC (3y 7m ago) |
| **Updated** | 2022-09-07 12:54:30.000 UTC |
| **Closed** | 2022-09-07 12:54:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

When public Python docs are available, sprinkle the specific links throughout the docstrings.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-09-07 12:54:30.000 UTC</summary>

I am afraid these headings are changing to often to put into code.

</details>


---

<a id="30"></a>

### #30: github.com/temporalio/temporal-v1.14.4: 4 vulnerabilities (highest severity is: 7.5) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/30 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2022-05-27 21:54:20.000 UTC (3y 7m ago) |
| **Updated** | 2022-06-10 21:27:05.000 UTC |
| **Closed** | 2022-06-10 21:27:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>github.com/temporalio/temporal-v1.14.4</b></p></summary>

<p></p>
<p>


<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/f1aa1c91b664fedd68e65b2f6ce9727e24e5f2af">f1aa1c91b664fedd68e65b2f6ce9727e24e5f2af</a></p></details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in | Remediation Available |
| ------------- | ------------- | ----- | ----- | ----- | --- | --- |
| [CVE-2019-0205](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0205) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | github.com/apache/thrift-0.10.0 | Transitive | N/A | &#10060; |
| [CVE-2022-21698](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-21698) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | github.com/prometheus/client_golang-v1.11.0 | Transitive | N/A | &#10060; |
| [CVE-2019-0210](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0210) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | github.com/apache/thrift-0.10.0 | Transitive | N/A | &#10060; |
| [CVE-2018-11798](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2018-11798) | <img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> Medium | 6.5 | github.com/apache/thrift-0.10.0 | Transitive | N/A | &#10060; |


## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2019-0205</summary>


###  Vulnerable Library - <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - github.com/temporalio/temporal-v1.14.4 (Root Library)
    - github.com/temporalio/ringpop-go-6f91b5915e95e7b08817aae78fea3a38ea1b5bd2
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/f1aa1c91b664fedd68e65b2f6ce9727e24e5f2af">f1aa1c91b664fedd68e65b2f6ce9727e24e5f2af</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In Apache Thrift all versions up to and including 0.12.0, a server or client may run into an endless loop when feed with specific input data. Because the issue had already been partially fixed in version 0.11.0, depending on the installed version it affects only certain language bindings.

<p>Publish Date: 2019-10-29
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0205>CVE-2019-0205</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0205">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0205</a></p>
<p>Release Date: 2019-10-29</p>
<p>Fix Resolution: org.apache.thrift:libthrift:0.13.0</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2022-21698</summary>


###  Vulnerable Library - <b>github.com/prometheus/client_golang-v1.11.0</b></p>

<p>Prometheus instrumentation library for Go applications</p>
<p>

Dependency Hierarchy:
  - github.com/temporalio/temporal-v1.14.4 (Root Library)
    - :x: **github.com/prometheus/client_golang-v1.11.0** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/f1aa1c91b664fedd68e65b2f6ce9727e24e5f2af">f1aa1c91b664fedd68e65b2f6ce9727e24e5f2af</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
client_golang is the instrumentation library for Go applications in Prometheus, and the promhttp package in client_golang provides tooling around HTTP servers and clients. In client_golang prior to version 1.11.1, HTTP server is susceptible to a Denial of Service through unbounded cardinality, and potential memory exhaustion, when handling requests with non-standard HTTP methods. In order to be affected, an instrumented software must use any of `promhttp.InstrumentHandler*` middleware except `RequestsInFlight`; not filter any specific methods (e.g GET) before middleware; pass metric with `method` label name to our middleware; and not have any firewall/LB/proxy that filters away requests with unknown `method`. client_golang version 1.11.1 contains a patch for this issue. Several workarounds are available, including removing the `method` label name from counter/gauge used in the InstrumentHandler; turning off affected promhttp handlers; adding custom middleware before promhttp handler that will sanitize the request method given by Go http.Request; and using a reverse proxy or web application firewall, configured to only allow a limited set of methods.

<p>Publish Date: 2022-02-15
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-21698>CVE-2022-21698</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://github.com/prometheus/client_golang/security/advisories/GHSA-cg3q-j54f-5p7p">https://github.com/prometheus/client_golang/security/advisories/GHSA-cg3q-j54f-5p7p</a></p>
<p>Release Date: 2022-02-15</p>
<p>Fix Resolution: v1.11.1</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2019-0210</summary>


###  Vulnerable Library - <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - github.com/temporalio/temporal-v1.14.4 (Root Library)
    - github.com/temporalio/ringpop-go-6f91b5915e95e7b08817aae78fea3a38ea1b5bd2
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/f1aa1c91b664fedd68e65b2f6ce9727e24e5f2af">f1aa1c91b664fedd68e65b2f6ce9727e24e5f2af</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In Apache Thrift 0.9.3 to 0.12.0, a server implemented in Go using TJSONProtocol or TSimpleJSONProtocol may panic when feed with invalid input data.

<p>Publish Date: 2019-10-29
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0210>CVE-2019-0210</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="http://mail-archives.apache.org/mod_mbox/thrift-dev/201910.mbox/%3C277A46CA87494176B1BBCF5D72624A2A%40HAGGIS%3E">http://mail-archives.apache.org/mod_mbox/thrift-dev/201910.mbox/%3C277A46CA87494176B1BBCF5D72624A2A%40HAGGIS%3E</a></p>
<p>Release Date: 2019-10-29</p>
<p>Fix Resolution: 0.13.0</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> CVE-2018-11798</summary>


###  Vulnerable Library - <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - github.com/temporalio/temporal-v1.14.4 (Root Library)
    - github.com/temporalio/ringpop-go-6f91b5915e95e7b08817aae78fea3a38ea1b5bd2
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)
<p>Found in HEAD commit: <a href="https://github.com/temporalio/sdk-python/commit/f1aa1c91b664fedd68e65b2f6ce9727e24e5f2af">f1aa1c91b664fedd68e65b2f6ce9727e24e5f2af</a></p>
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
The Apache Thrift Node.js static web server in versions 0.9.2 through 0.11.0 have been determined to contain a security vulnerability in which a remote user has the ability to access files outside the set webservers docroot path.

<p>Publish Date: 2019-01-07
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2018-11798>CVE-2018-11798</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>6.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: Low
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: High
  - Integrity Impact: None
  - Availability Impact: None
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11798">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11798</a></p>
<p>Release Date: 2019-01-07</p>
<p>Fix Resolution: v0.12.0</p>

</p>

<p></p>

</details>
<!-- <REMEDIATE>[{"isOpenPROnVulnerability":true,"isPackageBased":true,"isDefaultBranch":true,"packages":[{"packageType":"GO","packageName":"github.com/apache/thrift","packageVersion":"0.10.0","packageFilePaths":[],"isTransitiveDependency":true,"dependencyTree":"github.com/temporalio/temporal:v1.14.4;github.com/temporalio/ringpop-go:6f91b5915e95e7b08817aae78fea3a38ea1b5bd2;github.com/apache/thrift:0.10.0","isMinimumFixVersionAvailable":true,"minimumFixVersion":"org.apache.thrift:libthrift:0.13.0","isBinary":true}],"baseBranches":["main"],"vulnerabilityIdentifier":"CVE-2019-0205","vulnerabilityDetails":"In Apache Thrift all versions up to and including 0.12.0, a server or client may run into an endless loop when feed with specific input data. Because the issue had already been partially fixed in version 0.11.0, depending on the installed version it affects only certain language bindings.","vulnerabilityUrl":"https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0205","cvss3Severity":"high","cvss3Score":"7.5","cvss3Metrics":{"A":"High","AC":"Low","PR":"None","S":"Unchanged","C":"None","UI":"None","AV":"Network","I":"None"},"extraData":{}},{"isOpenPROnVulnerability":true,"isPackageBased":true,"isDefaultBranch":true,"packages":[{"packageType":"GO","packageName":"github.com/prometheus/client_golang","packageVersion":"v1.11.0","packageFilePaths":[],"isTransitiveDependency":true,"dependencyTree":"github.com/temporalio/temporal:v1.14.4;github.com/prometheus/client_golang:v1.11.0","isMinimumFixVersionAvailable":true,"minimumFixVersion":"v1.11.1","isBinary":true}],"baseBranches":["main"],"vulnerabilityIdentifier":"CVE-2022-21698","vulnerabilityDetails":"client_golang is the instrumentation library for Go applications in Prometheus, and the promhttp package in client_golang provides tooling around HTTP servers and clients. In client_golang prior to version 1.11.1, HTTP server is susceptible to a Denial of Service through unbounded cardinality, and potential memory exhaustion, when handling requests with non-standard HTTP methods. In order to be affected, an instrumented software must use any of `promhttp.InstrumentHandler*` middleware except `RequestsInFlight`; not filter any specific methods (e.g GET) before middleware; pass metric with `method` label name to our middleware; and not have any firewall/LB/proxy that filters away requests with unknown `method`. client_golang version 1.11.1 contains a patch for this issue. Several workarounds are available, including removing the `method` label name from counter/gauge used in the InstrumentHandler; turning off affected promhttp handlers; adding custom middleware before promhttp handler that will sanitize the request method given by Go http.Request; and using a reverse proxy or web application firewall, configured to only allow a limited set of methods.","vulnerabilityUrl":"https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-21698","cvss3Severity":"high","cvss3Score":"7.5","cvss3Metrics":{"A":"High","AC":"Low","PR":"None","S":"Unchanged","C":"None","UI":"None","AV":"Network","I":"None"},"extraData":{}},{"isOpenPROnVulnerability":true,"isPackageBased":true,"isDefaultBranch":true,"packages":[{"packageType":"GO","packageName":"github.com/apache/thrift","packageVersion":"0.10.0","packageFilePaths":[],"isTransitiveDependency":true,"dependencyTree":"github.com/temporalio/temporal:v1.14.4;github.com/temporalio/ringpop-go:6f91b5915e95e7b08817aae78fea3a38ea1b5bd2;github.com/apache/thrift:0.10.0","isMinimumFixVersionAvailable":true,"minimumFixVersion":"0.13.0","isBinary":true}],"baseBranches":["main"],"vulnerabilityIdentifier":"CVE-2019-0210","vulnerabilityDetails":"In Apache Thrift 0.9.3 to 0.12.0, a server implemented in Go using TJSONProtocol or TSimpleJSONProtocol may panic when feed with invalid input data.","vulnerabilityUrl":"https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0210","cvss3Severity":"high","cvss3Score":"7.5","cvss3Metrics":{"A":"High","AC":"Low","PR":"None","S":"Unchanged","C":"None","UI":"None","AV":"Network","I":"None"},"extraData":{}},{"isOpenPROnVulnerability":true,"isPackageBased":true,"isDefaultBranch":true,"packages":[{"packageType":"GO","packageName":"github.com/apache/thrift","packageVersion":"0.10.0","packageFilePaths":[],"isTransitiveDependency":true,"dependencyTree":"github.com/temporalio/temporal:v1.14.4;github.com/temporalio/ringpop-go:6f91b5915e95e7b08817aae78fea3a38ea1b5bd2;github.com/apache/thrift:0.10.0","isMinimumFixVersionAvailable":true,"minimumFixVersion":"v0.12.0","isBinary":true}],"baseBranches":["main"],"vulnerabilityIdentifier":"CVE-2018-11798","vulnerabilityDetails":"The Apache Thrift Node.js static web server in versions 0.9.2 through 0.11.0 have been determined to contain a security vulnerability in which a remote user has the ability to access files outside the set webservers docroot path.","vulnerabilityUrl":"https://vuln.whitesourcesoftware.com/vulnerability/CVE-2018-11798","cvss3Severity":"medium","cvss3Score":"6.5","cvss3Metrics":{"A":"None","AC":"Low","PR":"Low","S":"Unchanged","C":"High","UI":"None","AV":"Network","I":"None"},"extraData":{}}]</REMEDIATE> -->

#### Comments (1)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-06-10 21:27:04.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="25"></a>

### #25: Propagate fatal worker errors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/25 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-05-27 20:15:17.000 UTC (3y 7m ago) |
| **Updated** | 2022-11-07 15:29:59.000 UTC |
| **Closed** | 2022-11-07 15:29:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Make sure that fatal worker errors bubble out of `run()` and there is some way for people to add a "fatal error handler" for `async with` which has no way to otherwise send an error.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-05-31 22:37:00.000 UTC</summary>

May be pending https://github.com/temporalio/sdk-core/issues/325 so we have a reliable fatal error

</details>


---

<a id="19"></a>

### #19: Roadmap

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/19 |
| **State** | CLOSED |
| **Author** | Xakt |
| **Created** | 2022-04-06 14:36:05.000 UTC (3y 9m ago) |
| **Updated** | 2022-04-06 15:04:38.000 UTC |
| **Closed** | 2022-04-06 15:03:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hello there! 

We'll be looking into using Temporal in the upcoming quarter. We have quite a few Python services running in our backend. So I was wondering, can you maybe share a bit on the roadmap for this Python SDK? Among other things, I'm wondering how soon it will be mature enough to put on production, or at least leave the alpha stage.

Thanks!

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-04-06 15:03:49.000 UTC</summary>

Sure. Basically we've split into three phases. See the proposals at https://github.com/temporalio/proposals/tree/master/python for the first two. Of course implementation changes the proposal details. Phase 1 has already been implemented. Phase 2 is under active development and hopefully can get to beta shortly thereafter (I'm afraid I don't have specific dates). The API will hopefully not move too much after that.

We admittedly are missing pre-built wheels for macOS M1 and Linux ARM, but we'll get those soon. To chat about the Python SDK, feel free join `#python-sdk` on Slack at https://temporal.io/slack or our forum at https://community.temporal.io/.

(closing as not an outstanding issue, but feel free to continue commenting)

</details>


---

<a id="18"></a>

### #18: temporal-client-0.1.0: 1 vulnerabilities (highest severity is: 7.5) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/18 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2022-04-02 01:03:09.000 UTC (3y 9m ago) |
| **Updated** | 2022-06-01 13:19:21.000 UTC |
| **Closed** | 2022-06-01 13:19:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>temporal-client-0.1.0</b></p></summary>

<p></p>
<p>

</details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in | Remediation Available |
| ------------- | ------------- | ----- | ----- | ----- | --- | --- |
| [CVE-2022-24713](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-24713) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | regex-1.5.4.crate | Transitive | N/A | &#10060; |


## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2022-24713</summary>


###  Vulnerable Library - <b>regex-1.5.4.crate</b></p>

<p>An implementation of regular expressions for Rust. This implementation uses
finite automata and guarantees linear time matching on all inputs.
</p>
<p>Library home page: <a href="https://crates.io/api/v1/crates/regex/1.5.4/download">https://crates.io/api/v1/crates/regex/1.5.4/download</a></p>
<p>

Dependency Hierarchy:
  - temporal-client-0.1.0 (Root Library)
    - temporal-sdk-core-protos-0.1.0
      - tonic-build-0.6.2.crate
        - prost-build-0.9.0.crate
          - :x: **regex-1.5.4.crate** (Vulnerable Library)
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
regex is an implementation of regular expressions for the Rust language. The regex crate features built-in mitigations to prevent denial of service attacks caused by untrusted regexes, or untrusted input matched by trusted regexes. Those (tunable) mitigations already provide sane defaults to prevent attacks. This guarantee is documented and it's considered part of the crate's API. Unfortunately a bug was discovered in the mitigations designed to prevent untrusted regexes to take an arbitrary amount of time during parsing, and it's possible to craft regexes that bypass such mitigations. This makes it possible to perform denial of service attacks by sending specially crafted regexes to services accepting user-controlled, untrusted regexes. All versions of the regex crate before or equal to 1.5.4 are affected by this issue. The fix is include starting from regex 1.5.5. All users accepting user-controlled regexes are recommended to upgrade immediately to the latest version of the regex crate. Unfortunately there is no fixed set of problematic regexes, as there are practically infinite regexes that could be crafted to exploit this vulnerability. Because of this, it us not recommend to deny known problematic regexes.

<p>Publish Date: 2022-03-08
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-24713>CVE-2022-24713</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://github.com/rust-lang/regex/security/advisories/GHSA-m5pq-gvj9-9vr8">https://github.com/rust-lang/regex/security/advisories/GHSA-m5pq-gvj9-9vr8</a></p>
<p>Release Date: 2022-03-08</p>
<p>Fix Resolution: regex - 1.5.5</p>

</p>

<p></p>

</details>
<!-- <REMEDIATE>[{"isOpenPROnVulnerability":true,"isPackageBased":true,"isDefaultBranch":true,"packages":[{"packageType":"Crate","packageName":"regex","packageVersion":"1.5.4","packageFilePaths":[],"isTransitiveDependency":true,"dependencyTree":"temporal-client:0.1.0;temporal-sdk-core-protos:0.1.0;tonic-build:0.6.2;prost-build:0.9.0;regex:1.5.4","isMinimumFixVersionAvailable":true,"minimumFixVersion":"regex - 1.5.5","isBinary":true}],"baseBranches":["main"],"vulnerabilityIdentifier":"CVE-2022-24713","vulnerabilityDetails":"regex is an implementation of regular expressions for the Rust language. The regex crate features built-in mitigations to prevent denial of service attacks caused by untrusted regexes, or untrusted input matched by trusted regexes. Those (tunable) mitigations already provide sane defaults to prevent attacks. This guarantee is documented and it\u0027s considered part of the crate\u0027s API. Unfortunately a bug was discovered in the mitigations designed to prevent untrusted regexes to take an arbitrary amount of time during parsing, and it\u0027s possible to craft regexes that bypass such mitigations. This makes it possible to perform denial of service attacks by sending specially crafted regexes to services accepting user-controlled, untrusted regexes. All versions of the regex crate before or equal to 1.5.4 are affected by this issue. The fix is include starting from regex 1.5.5. All users accepting user-controlled regexes are recommended to upgrade immediately to the latest version of the regex crate. Unfortunately there is no fixed set of problematic regexes, as there are practically infinite regexes that could be crafted to exploit this vulnerability. Because of this, it us not recommend to deny known problematic regexes.","vulnerabilityUrl":"https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-24713","cvss3Severity":"high","cvss3Score":"7.5","cvss3Metrics":{"A":"High","AC":"Low","PR":"None","S":"Unchanged","C":"None","UI":"None","AV":"Network","I":"None"},"extraData":{}}]</REMEDIATE> -->

#### Comments (1)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-06-01 13:19:21.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.

</details>


---

<a id="17"></a>

### #17: go.temporal.io/server-v1.14.4: 4 vulnerabilities (highest severity is: 7.5) - autoclosed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/17 |
| **State** | CLOSED |
| **Author** | app/mend-for-github-com |
| **Created** | 2022-04-02 01:03:07.000 UTC (3y 9m ago) |
| **Updated** | 2022-05-27 21:54:27.000 UTC |
| **Closed** | 2022-05-27 21:54:26.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | Mend: dependency security vulnerability |
| **Assignees** | None |
| **Milestone** | None |

#### Description

<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>go.temporal.io/server-v1.14.4</b></p></summary>

<p></p>
<p>

</details>

## Vulnerabilities

| CVE | Severity | <img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS | Dependency | Type | Fixed in | Remediation Available |
| ------------- | ------------- | ----- | ----- | ----- | --- | --- |
| [CVE-2019-0205](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0205) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | github.com/apache/thrift-0.10.0 | Transitive | N/A | &#10060; |
| [CVE-2022-21698](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-21698) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | github.com/prometheus/client_golang-v1.11.0 | Transitive | N/A | &#10060; |
| [CVE-2019-0210](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0210) | <img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> High | 7.5 | github.com/apache/thrift-0.10.0 | Transitive | N/A | &#10060; |
| [CVE-2018-11798](https://vuln.whitesourcesoftware.com/vulnerability/CVE-2018-11798) | <img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> Medium | 6.5 | github.com/apache/thrift-0.10.0 | Transitive | N/A | &#10060; |


## Details

<details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2019-0205</summary>


###  Vulnerable Library - <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.14.4 (Root Library)
    - github.com/uber/tchannel-go-v1.21.0
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In Apache Thrift all versions up to and including 0.12.0, a server or client may run into an endless loop when feed with specific input data. Because the issue had already been partially fixed in version 0.11.0, depending on the installed version it affects only certain language bindings.

<p>Publish Date: 2019-10-29
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0205>CVE-2019-0205</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0205">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0205</a></p>
<p>Release Date: 2019-10-29</p>
<p>Fix Resolution: org.apache.thrift:libthrift:0.13.0</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2022-21698</summary>


###  Vulnerable Library - <b>github.com/prometheus/client_golang-v1.11.0</b></p>

<p>Prometheus instrumentation library for Go applications</p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.14.4 (Root Library)
    - :x: **github.com/prometheus/client_golang-v1.11.0** (Vulnerable Library)
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
client_golang is the instrumentation library for Go applications in Prometheus, and the promhttp package in client_golang provides tooling around HTTP servers and clients. In client_golang prior to version 1.11.1, HTTP server is susceptible to a Denial of Service through unbounded cardinality, and potential memory exhaustion, when handling requests with non-standard HTTP methods. In order to be affected, an instrumented software must use any of `promhttp.InstrumentHandler*` middleware except `RequestsInFlight`; not filter any specific methods (e.g GET) before middleware; pass metric with `method` label name to our middleware; and not have any firewall/LB/proxy that filters away requests with unknown `method`. client_golang version 1.11.1 contains a patch for this issue. Several workarounds are available, including removing the `method` label name from counter/gauge used in the InstrumentHandler; turning off affected promhttp handlers; adding custom middleware before promhttp handler that will sanitize the request method given by Go http.Request; and using a reverse proxy or web application firewall, configured to only allow a limited set of methods.

<p>Publish Date: 2022-02-15
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-21698>CVE-2022-21698</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://github.com/prometheus/client_golang/security/advisories/GHSA-cg3q-j54f-5p7p">https://github.com/prometheus/client_golang/security/advisories/GHSA-cg3q-j54f-5p7p</a></p>
<p>Release Date: 2022-02-15</p>
<p>Fix Resolution: v1.11.1</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/high_vul.png' width=19 height=20> CVE-2019-0210</summary>


###  Vulnerable Library - <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.14.4 (Root Library)
    - github.com/uber/tchannel-go-v1.21.0
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
In Apache Thrift 0.9.3 to 0.12.0, a server implemented in Go using TJSONProtocol or TSimpleJSONProtocol may panic when feed with invalid input data.

<p>Publish Date: 2019-10-29
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0210>CVE-2019-0210</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>7.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: None
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: None
  - Integrity Impact: None
  - Availability Impact: High
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="http://mail-archives.apache.org/mod_mbox/thrift-dev/201910.mbox/%3C277A46CA87494176B1BBCF5D72624A2A%40HAGGIS%3E">http://mail-archives.apache.org/mod_mbox/thrift-dev/201910.mbox/%3C277A46CA87494176B1BBCF5D72624A2A%40HAGGIS%3E</a></p>
<p>Release Date: 2019-10-29</p>
<p>Fix Resolution: 0.13.0</p>

</p>

<p></p>

</details><details>

<summary><img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> CVE-2018-11798</summary>


###  Vulnerable Library - <b>github.com/apache/thrift-0.10.0</b></p>

<p>Apache Thrift</p>
<p>

Dependency Hierarchy:
  - go.temporal.io/server-v1.14.4 (Root Library)
    - github.com/uber/tchannel-go-v1.21.0
      - :x: **github.com/apache/thrift-0.10.0** (Vulnerable Library)
<p>Found in base branch: <b>main</b></p>
</p>

<p></p>

###  Vulnerability Details
<p>  
  
The Apache Thrift Node.js static web server in versions 0.9.2 through 0.11.0 have been determined to contain a security vulnerability in which a remote user has the ability to access files outside the set webservers docroot path.

<p>Publish Date: 2019-01-07
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2018-11798>CVE-2018-11798</a></p>
</p>

<p></p>

###  CVSS 3 Score Details (<b>6.5</b>)
<p>

Base Score Metrics:
- Exploitability Metrics:
  - Attack Vector: Network
  - Attack Complexity: Low
  - Privileges Required: Low
  - User Interaction: None
  - Scope: Unchanged
- Impact Metrics:
  - Confidentiality Impact: High
  - Integrity Impact: None
  - Availability Impact: None
</p>
For more information on CVSS3 Scores, click <a href="https://www.first.org/cvss/calculator/3.0">here</a>.
</p>

<p></p>

###  Suggested Fix
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11798">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11798</a></p>
<p>Release Date: 2019-01-07</p>
<p>Fix Resolution: v0.12.0</p>

</p>

<p></p>

</details>
<!-- <REMEDIATE>[{"isOpenPROnVulnerability":true,"isPackageBased":true,"isDefaultBranch":true,"packages":[{"packageType":"GO","packageName":"github.com/apache/thrift","packageVersion":"0.10.0","packageFilePaths":[],"isTransitiveDependency":true,"dependencyTree":"server:v1.14.4;github.com/uber/tchannel-go:v1.21.0;github.com/apache/thrift:0.10.0","isMinimumFixVersionAvailable":true,"minimumFixVersion":"org.apache.thrift:libthrift:0.13.0","isBinary":true}],"baseBranches":["main"],"vulnerabilityIdentifier":"CVE-2019-0205","vulnerabilityDetails":"In Apache Thrift all versions up to and including 0.12.0, a server or client may run into an endless loop when feed with specific input data. Because the issue had already been partially fixed in version 0.11.0, depending on the installed version it affects only certain language bindings.","vulnerabilityUrl":"https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0205","cvss3Severity":"high","cvss3Score":"7.5","cvss3Metrics":{"A":"High","AC":"Low","PR":"None","S":"Unchanged","C":"None","UI":"None","AV":"Network","I":"None"},"extraData":{}},{"isOpenPROnVulnerability":true,"isPackageBased":true,"isDefaultBranch":true,"packages":[{"packageType":"GO","packageName":"github.com/prometheus/client_golang","packageVersion":"v1.11.0","packageFilePaths":[],"isTransitiveDependency":true,"dependencyTree":"server:v1.14.4;github.com/prometheus/client_golang:v1.11.0","isMinimumFixVersionAvailable":true,"minimumFixVersion":"v1.11.1","isBinary":true}],"baseBranches":["main"],"vulnerabilityIdentifier":"CVE-2022-21698","vulnerabilityDetails":"client_golang is the instrumentation library for Go applications in Prometheus, and the promhttp package in client_golang provides tooling around HTTP servers and clients. In client_golang prior to version 1.11.1, HTTP server is susceptible to a Denial of Service through unbounded cardinality, and potential memory exhaustion, when handling requests with non-standard HTTP methods. In order to be affected, an instrumented software must use any of `promhttp.InstrumentHandler*` middleware except `RequestsInFlight`; not filter any specific methods (e.g GET) before middleware; pass metric with `method` label name to our middleware; and not have any firewall/LB/proxy that filters away requests with unknown `method`. client_golang version 1.11.1 contains a patch for this issue. Several workarounds are available, including removing the `method` label name from counter/gauge used in the InstrumentHandler; turning off affected promhttp handlers; adding custom middleware before promhttp handler that will sanitize the request method given by Go http.Request; and using a reverse proxy or web application firewall, configured to only allow a limited set of methods.","vulnerabilityUrl":"https://vuln.whitesourcesoftware.com/vulnerability/CVE-2022-21698","cvss3Severity":"high","cvss3Score":"7.5","cvss3Metrics":{"A":"High","AC":"Low","PR":"None","S":"Unchanged","C":"None","UI":"None","AV":"Network","I":"None"},"extraData":{}},{"isOpenPROnVulnerability":true,"isPackageBased":true,"isDefaultBranch":true,"packages":[{"packageType":"GO","packageName":"github.com/apache/thrift","packageVersion":"0.10.0","packageFilePaths":[],"isTransitiveDependency":true,"dependencyTree":"server:v1.14.4;github.com/uber/tchannel-go:v1.21.0;github.com/apache/thrift:0.10.0","isMinimumFixVersionAvailable":true,"minimumFixVersion":"0.13.0","isBinary":true}],"baseBranches":["main"],"vulnerabilityIdentifier":"CVE-2019-0210","vulnerabilityDetails":"In Apache Thrift 0.9.3 to 0.12.0, a server implemented in Go using TJSONProtocol or TSimpleJSONProtocol may panic when feed with invalid input data.","vulnerabilityUrl":"https://vuln.whitesourcesoftware.com/vulnerability/CVE-2019-0210","cvss3Severity":"high","cvss3Score":"7.5","cvss3Metrics":{"A":"High","AC":"Low","PR":"None","S":"Unchanged","C":"None","UI":"None","AV":"Network","I":"None"},"extraData":{}},{"isOpenPROnVulnerability":true,"isPackageBased":true,"isDefaultBranch":true,"packages":[{"packageType":"GO","packageName":"github.com/apache/thrift","packageVersion":"0.10.0","packageFilePaths":[],"isTransitiveDependency":true,"dependencyTree":"server:v1.14.4;github.com/uber/tchannel-go:v1.21.0;github.com/apache/thrift:0.10.0","isMinimumFixVersionAvailable":true,"minimumFixVersion":"v0.12.0","isBinary":true}],"baseBranches":["main"],"vulnerabilityIdentifier":"CVE-2018-11798","vulnerabilityDetails":"The Apache Thrift Node.js static web server in versions 0.9.2 through 0.11.0 have been determined to contain a security vulnerability in which a remote user has the ability to access files outside the set webservers docroot path.","vulnerabilityUrl":"https://vuln.whitesourcesoftware.com/vulnerability/CVE-2018-11798","cvss3Severity":"medium","cvss3Score":"6.5","cvss3Metrics":{"A":"None","AC":"Low","PR":"Low","S":"Unchanged","C":"High","UI":"None","AV":"Network","I":"None"},"extraData":{}}]</REMEDIATE> -->

#### Comments (1)

<details>
<summary><strong>mend-for-github-com</strong> commented on 2022-05-27 21:54:27.000 UTC</summary>

:heavy_check_mark: This issue was automatically closed by WhiteSource because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the WhiteSource inventory.

</details>


---

<a id="12"></a>

### #12: Multiprocess heartbeat and cancel improvement research

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/12 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-03-17 16:53:11.000 UTC (3y 9m ago) |
| **Updated** | 2022-03-22 13:27:29.000 UTC |
| **Closed** | 2022-03-22 13:27:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | â¤ï¸ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

Right now we're using `multiprocessing.Manager()`'s queue for heartbeating and event for cancellation. Then we move from the sync queue onto the sync heartbeat which schedules an async heartbeat task that we try to ensure runs when the activity completes.

### Describe the solution you'd like

This is potentially flaky and/or confusing. Try to streamline the threaded/multiprocess heartbeat queuing.


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2022-03-17 18:24:13.000 UTC</summary>

Also, heartbeating too fast can hold up the event loop which we don't like

</details>


---

<a id="1268"></a>

### #1268: [Bug] Activity/workflow pollers drop to 0 ignoring minimum/maximum settings

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1268 |
| **State** | OPEN |
| **Author** | sashaneb (Sasha Lopashev) |
| **Created** | 2026-01-09 00:42:34.000 UTC (0 days ago) |
| **Updated** | 2026-01-09 00:42:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Run Temporal workers that reliably poll for workflow and activity tasks. Workers should maintain active pollers as long as task slots are available.

### Describe the bug

Worker pollers (both activity and workflow) randomly drop to 0 despite available task slots and explicit poller configuration. This causes workflow/activity task timeouts as no worker is polling for tasks.

**Tested configurations - all fail:**

1. `PollerBehaviorAutoscaling(minimum=5, maximum=20, initial=10)` - pollers drop to 0 despite `minimum=5`
2. Default behavior (no poller config) - pollers drop to 0
3. `PollerBehaviorSimpleMaximum(maximum=5)` - pollers still drop to 0

**Evidence from Prometheus metrics:**

Worker with 14 available activity slots but 0 pollers:
```
temporal_num_pollers{poller_type="activity_task"} 0
temporal_num_pollers{poller_type="sticky_workflow_task"} 3
temporal_num_pollers{poller_type="workflow_task"} 1
temporal_worker_task_slots_available{worker_type="ActivityWorker"} 14
temporal_worker_task_slots_used{worker_type="ActivityWorker"} 1
```

The docs for `PollerBehaviorSimpleMaximum` state:
> "A poller behavior that will attempt to poll as long as a slot is available, up to the provided maximum."

This is, however, not what we're observing. With 14 available slots, there should be active pollers.

**Timeline:**
- Workers start healthy with expected poller counts
- After 10-20 minutes, pollers drop to 0 on some/all workers
- No errors in logs - worker appears healthy but doesn't poll
- Happens randomly, affects different workers at different times

**Impact:**
- Workflow Task Timeouts: New workflows can't start
- Activity Task Timeouts: Activities can't execute
- Silent Failures: No errors logged, worker appears healthy

### Minimal Reproduction

```python
import asyncio
from temporalio.client import Client
from temporalio.worker import Worker, PollerBehaviorSimpleMaximum
from temporalio import workflow, activity

@activity.defn
async def my_activity() -> str:
    return "done"

@workflow.defn
class MyWorkflow:
    @workflow.run
    async def run(self) -> str:
        return await workflow.execute_activity(
            my_activity,
            start_to_close_timeout=timedelta(seconds=60),
        )

async def main():
    client = await Client.connect("your-temporal-address")

    worker = Worker(
        client,
        task_queue="test-queue",
        workflows=[MyWorkflow],
        activities=[my_activity],
        max_concurrent_activities=15,
        max_concurrent_workflow_tasks=10,
        # Any of these configurations fail:
        workflow_task_poller_behavior=PollerBehaviorSimpleMaximum(maximum=5),
        activity_task_poller_behavior=PollerBehaviorSimpleMaximum(maximum=5),
    )

    # Run worker and monitor temporal_num_pollers metric
    # After 10-20 minutes, activity_task pollers drop to 0
    await worker.run()

if __name__ == "__main__":
    asyncio.run(main())
```

To observe:
1. Start worker with Prometheus metrics enabled
2. Run some workflows
3. Monitor `temporal_num_pollers` metric over 15-20 minutes
4. Observe `poller_type="activity_task"` drop to 0 despite available slots

### Environment/Versions

- OS and processor: Linux x86_64 (Kubernetes/EKS), also tested on M1 Mac
- SDK version: `temporalio==1.21.1` (latest), as well as previously used `1.13.0`
- Temporal: Temporal Cloud (us-east-1)
- Python: 3.12
- Using Kubernetes (EKS on AWS) with multiple worker pods autoscaled with KEDA

### Additional context

**Release notes reviewed:**
- SDK 1.11.0 introduced `PollerBehaviorAutoscaling`
- SDK 1.12.0 had fix for "low wft slots/pollers issue" (#877) but only for workflow tasks
- SDK 1.19.0 removed "experimental" tag from worker tuners

**Questions:**
1. What causes pollers to drop to 0 despite available slots?
2. Why does `minimum=5` in autoscaling not enforce the minimum?
3. Is there a known issue with the Rust core's poller management?

**Current workaround:**
- Use `PollerBehaviorSimpleMaximum` (slightly more stable than autoscaling)
- Run multiple workers and hope not all drop to 0 simultaneously
- Consider periodic worker restarts to reset pollers (not ideal)



---

<a id="1262"></a>

### #1262: [Bug] Add ability to obtain `ApplicationError` details with type hint

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1262 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-12-29 15:19:00.000 UTC (11 days ago) |
| **Updated** | 2025-12-29 15:19:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

For users of deserialized `ApplicationError`, today we only offer the deserialized `details` in raw Python form (e.g. dicts and such). For `ApplicationErrors` that are deserialized, we should make the `details` property lazy, have a property for obtaining the raw details (that only works if raw details were set on create, i.e. in deserialization situations), and have a method that lets you access a detail by index and type hint.


---

<a id="1255"></a>

### #1255: [Bug] Update openai-agents dependency

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1255 |
| **State** | CLOSED |
| **Author** | ilovepb (Henry) |
| **Created** | 2025-12-18 00:06:00.000 UTC (22 days ago) |
| **Updated** | 2025-12-18 18:31:08.000 UTC |
| **Closed** | 2025-12-18 18:31:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

i just want the openai agents sdk updated to include some bug fixes + compatibility with the new gpt 5.1 + 5.2 models.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

the temporal openai agents plugin dependency is about [2 minor versions](https://pypi.org/project/openai-agents/) behind.

https://github.com/temporalio/sdk-python/blob/main/pyproject.toml#L31

```
openai-agents = ["openai-agents>=0.3,<0.5", "mcp>=1.9.4, <2"]
```

this means the max version we can have is `0.4.2` i think.


thanks! 



---

<a id="1254"></a>

### #1254: [Bug] `SandboxImportNotificationPolicy.WARN_ON_UNINTENTIONAL_PASSTHROUGH` warns on import of the workflow itself to the sandbox

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1254 |
| **State** | OPEN |
| **Author** | VegetarianOrc (Alex Mazzeo) |
| **Created** | 2025-12-17 23:35:07.000 UTC (22 days ago) |
| **Updated** | 2025-12-17 23:35:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Leverage `SandboxImportNotificationPolicy.WARN_ON_UNINTENTIONAL_PASSTHROUGH` to identify imports that aren't intentionally passed through to the sandbox.

### Describe the bug

Using a sandboxed runner with the workflows defined in a separate from the worker triggers the unintentional passthrough warning. The workflow must be loaded into the sandbox each time and therefore should be exempt from this warning.

### Minimal Reproduction

```python
# workflows.py
from datetime import timedelta

from temporalio import workflow

from warning.activities import echo_activity


@workflow.defn
class ExampleWorkflow:
    @workflow.run
    async def run(self) -> str:
        activity_result = await workflow.execute_activity(
            echo_activity,
            "example",
            task_queue="example-task-queue",
            start_to_close_timeout=timedelta(seconds=10),
        )

        return activity_result
```

```python
# activities.py
from temporalio import activity


@activity.defn
async def echo_activity(input: str) -> str:
    return f"hello {input}"

```

```python
# worker.py
import asyncio

from temporalio.client import Client
from temporalio.envconfig import ClientConfig
from temporalio.worker import Worker
from temporalio.worker.workflow_sandbox import (
    SandboxedWorkflowRunner,
    SandboxRestrictions,
)
from temporalio.workflow import SandboxImportNotificationPolicy

from warning.activities import echo_activity
from warning.workflows import ExampleWorkflow


async def main():
    config = ClientConfig.load_client_connect_config()
    config.setdefault("target_host", "localhost:7233")

    # Start client
    client = await Client.connect(**config)

    worker = Worker(
        client,
        task_queue="example-task-queue",
        workflows=[ExampleWorkflow],
        activities=[echo_activity],
        workflow_runner=SandboxedWorkflowRunner(
            restrictions=SandboxRestrictions.default.with_import_notification_policy(
                SandboxImportNotificationPolicy.WARN_ON_DYNAMIC_IMPORT
                | SandboxImportNotificationPolicy.WARN_ON_UNINTENTIONAL_PASSTHROUGH
            )
        ),
    )

    await worker.run()


if __name__ == "__main__":
    asyncio.run(main())
```

Starting the above worker yields this log on startup and workflow execution:

> uv run warning/worker.py
/.../temporalio/worker/workflow_sandbox/_importer.py:323: UserWarning: Module warning.workflows was not intentionally passed through to the sandbox.
  warnings.warn(
/.../temporalio/worker/workflow_sandbox/_importer.py:323: UserWarning: Module warning.activities was not intentionally passed through to the sandbox.
  warnings.warn(

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- Temporal Version: SDK 1.20

### Additional context

<!-- Add any other context about the problem here. -->



---

<a id="1244"></a>

### #1244: [Bug] Build Binaries Github Actions Workflow Fails on nexus-rpc mismatch

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1244 |
| **State** | CLOSED |
| **Author** | VegetarianOrc (Alex Mazzeo) |
| **Created** | 2025-12-09 00:39:31.000 UTC (1 months ago) |
| **Updated** | 2025-12-10 22:25:46.000 UTC |
| **Closed** | 2025-12-10 22:25:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Merge a change when the Temporal SDK is referencing a version of nexus-rpc that is not latest.

### Describe the bug

The Build Binaries action installs the latest version of nexus-rpc (via `nexus-rpc>=1.1.0`) when testing the built wheel in a new virtual environment. If the Temporal SDK relies on an older version, the build binaries job fails as it is unable to find a suitable version of nexus-rpc.

Example sequence of events:
* nexus-rpc versions: 1.1.0, 1.2.0, 1.3.0
* Temporal depends on `nexus-rpc==1.2.0`
* build-binaries action installs `nexus-rpc==1.3.0`
* Test wheel step fails b/c the dependency mismatch




---

<a id="1239"></a>

### #1239: [Bug] `WorkflowHistoryEventAsyncIterator` page_size override is ignored.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1239 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-12-04 20:41:07.000 UTC (1 months ago) |
| **Updated** | 2025-12-10 00:14:42.000 UTC |
| **Closed** | 2025-12-10 00:14:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

`WorkflowHistoryEventAsyncIterator.fetch_next_page` never uses the `page_size` argument.

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?

### Additional context

<!-- Add any other context about the problem here. -->



---

<a id="1237"></a>

### #1237: [Bug] Converter returns bad value converting `dict[None, Any]`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1237 |
| **State** | OPEN |
| **Author** | tconley1428 |
| **Created** | 2025-12-03 21:58:21.000 UTC (1 months ago) |
| **Updated** | 2025-12-03 21:58:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I don't know why this would be a thing.

### Describe the bug

In `test_converter.py`, 
`ok(dict[None, str], {'null': "1"})` passes, but that doesn't really seem like the correct return object.

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?

### Additional context

<!-- Add any other context about the problem here. -->



---

<a id="1232"></a>

### #1232: [Feature Request] Add BasedPyright as a required typecheck

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1232 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-12-02 20:02:33.000 UTC (1 months ago) |
| **Updated** | 2026-01-05 14:53:49.000 UTC |
| **Closed** | 2026-01-05 14:53:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

The codebase should pass checks from BasedPyright as it gains user adoption.

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="1230"></a>

### #1230: [Feature Request] Support non-workflow activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1230 |
| **State** | OPEN |
| **Author** | maciejdudko (Maciej Dudkowski) |
| **Created** | 2025-12-02 17:20:15.000 UTC (1 months ago) |
| **Updated** | 2025-12-02 17:20:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Allow executing activities without a workflow. The clients must support the new (upcoming) server APIs for starting, managing and getting results of activities. The workers must support running activities outside of workflow context.

### Additional context

Server API PR (WIP): https://github.com/temporalio/api/pull/640


---

<a id="1225"></a>

### #1225: [Bug] Client.config() and Worker.config() should represent immutable copies of exact kwargs passed in

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1225 |
| **State** | CLOSED |
| **Author** | yuandrew (Andrew Yuan) |
| **Created** | 2025-11-25 17:41:54.000 UTC (1 months ago) |
| **Updated** | 2025-12-02 19:09:44.000 UTC |
| **Closed** | 2025-12-02 19:09:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Issue brought up by Chad from https://github.com/temporalio/sdk-python/pull/1157#discussion_r2542287950, 

> Config for most of the options needs to represent the configuration passed in (or mutated by plugins). A client's config contains the options it was configured with, and so does a workers. The combined plugin list should not be in config IMO. People use config to change-and-resplat, e.g. new_config = old_worker.config() + Worker(**new_config).

> Arguably we should not be storing the plugin-mutated config in self._config for that reason too.


Also see https://github.com/temporalio/sdk-python/pull/1157#pullrequestreview-3505399150 for more context.

> Client.config() and Worker.config() need to represent immutable copies of the exact kwargs passed in to constructor or at least be sure they can be passed in to another constructor


---

<a id="1209"></a>

### #1209: [Feature Request] Allow overriding of the Worker Deployment Version when invoking a Child Workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1209 |
| **State** | OPEN |
| **Author** | chris-olszewski (Chris Olszewski) |
| **Created** | 2025-11-12 16:25:55.000 UTC (1 months ago) |
| **Updated** | 2025-11-12 16:25:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Users want to override the target version on a workflow, especially when doing pre-deployment testing.  This should work the same as it does when creating a Workflow from a client.

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="1201"></a>

### #1201: [Feature Request] Better prevention of accidental `Runtime` use over forks

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1201 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-10-31 18:28:09.000 UTC (2 months ago) |
| **Updated** | 2025-11-17 20:22:51.000 UTC |
| **Closed** | 2025-11-17 20:22:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Runtimes/clients/workers/etc do not work across forks. We need to do two things here:
* Prevent creating things with a runtime created in a different process
  * Basically do the exact same thing we did in Ruby: https://github.com/temporalio/sdk-ruby/pull/301
* Allow users to opt-out of default runtime use
  * This means add something like `Runtime.prevent_default()` that will prevent any invocation of `Runtime.default()`
  * Clients and replayers and test environments use `Runtime.default` today which lazily creates runtime, but advanced users want to prevent accidentally doing this
  * `Runtime.prevent_default()` should error if it's called after a default has been created


---

<a id="1191"></a>

### #1191: [Bug] Failures in activites are not decoded in workflow when custom codec is used

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1191 |
| **State** | CLOSED |
| **Author** | romank0 (Roman Konoval) |
| **Created** | 2025-10-27 11:00:19.000 UTC (2 months ago) |
| **Updated** | 2025-10-27 17:22:20.000 UTC |
| **Closed** | 2025-10-27 17:22:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I use a custom codec to encrypt payload data.

### Describe the bug

If the activity raises an exception with details failure is properly encoded and stored. But in the worker when the workflow is activated the following error happens and workflow cannot proceed and eventually time outs:
```
Failed activation on workflow MyWorkflow with ID 9c556743-9ebf-4c3b-9800-3c56f02248a7 and run ID 3aeb4af6-97b6-43df-9c3e-a8b0f4326cd6
Traceback (most recent call last):
  File "..venv/lib/python3.11/site-packages/temporalio/worker/_workflow_instance.py", line 413, in activate
    self._apply(job)
  File "..venv/lib/python3.11/site-packages/temporalio/worker/_workflow_instance.py", line 512, in _apply
    self._apply_resolve_activity(job.resolve_activity)
  File "..venv/lib/python3.11/site-packages/temporalio/worker/_workflow_instance.py", line 769, in _apply_resolve_activity
    self._failure_converter.from_failure(
  File "..venv/lib/python3.11/site-packages/temporalio/converter.py", line 1096, in from_failure
    err.__cause__ = self.from_failure(failure.cause, payload_converter)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "..venv/lib/python3.11/site-packages/temporalio/converter.py", line 992, in from_failure
    *payload_converter.from_payloads_wrapper(app_info.details),
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "..venv/lib/python3.11/site-packages/temporalio/converter.py", line 139, in from_payloads_wrapper
    return self.from_payloads(payloads.payloads)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "..venv/lib/python3.11/site-packages/temporalio/converter.py", line 309, in from_payloads
    raise KeyError(f"Unknown payload encoding {encoding.decode()}")
KeyError: 'Unknown payload encoding binary/encrypted'
```

### Minimal Reproduction

This test when added to `tests/worker/tests_workflow.py` fails because of the timeout:
```
@activity.defn
async def activity_that_fails_with_details() -> str:
    raise ApplicationError(
        "Activity failed intentionally",
        "detail1",
        {"error_code": "NOT_FOUND", "id": "test-123"},
        non_retryable=True,
    )


@workflow.defn
class WorkflowWithFailingActivityAndCodec:
    @workflow.run
    async def run(self) -> str:
        try:
            return await workflow.execute_activity(
                activity_that_fails_with_details,
                schedule_to_close_timeout=timedelta(seconds=3),
                retry_policy=RetryPolicy(maximum_attempts=1),
            )
        except ActivityError as err:
            return "Handled encrypted failure successfully"


async def test_activity_failure_with_encoded_payload_is_decoded_in_workflow(client: Client):
    config = client.config()
    config["data_converter"] = dataclasses.replace(
        temporalio.converter.default(), payload_codec=EncryptionCodec()
    )
    client = Client(**config)

    async with new_worker(
        client,
        WorkflowWithFailingActivityAndCodec,
        activities=[activity_that_fails_with_details],
    ) as worker:
        result = await client.execute_workflow(
            WorkflowWithFailingActivityAndCodec.run,
            id=f"workflow-{uuid.uuid4()}",
            task_queue=worker.task_queue,
            run_timeout=timedelta(seconds=5),
        )
        assert result == "Handled encrypted failure successfully"
```

### Environment/Versions

- OS and processor: fails both on M1 Mac and Linux, docker and native.
- Temporal Version: 2.31.2, python sdk 1.18.0, 1.18.1 and master

### Additional context

No.


---

<a id="1188"></a>

### #1188: [Bug] converter.py: TypeError on Python 3.13 when using isinstance with typing.Literal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1188 |
| **State** | CLOSED |
| **Author** | BrunodsLilly (Bruno) |
| **Created** | 2025-10-26 16:15:16.000 UTC (2 months ago) |
| **Updated** | 2025-10-30 20:33:18.000 UTC |
| **Closed** | 2025-10-30 20:33:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

I am running a workflow that passes data containing a dictionary. The dictionary's type hint uses typing.Literal for its keys. The workflow fails during data conversion when attempting to convert this dictionary.

### Describe the bug

When temporalio.converter.value_to_type attempts to convert a dictionary that has a typing.Literal as its key_type, it incorrectly uses isinstance(key, key_type) for validation.

In Python 3.13, using isinstance() with subscripted generics like typing.Literal is no longer allowed and raises a **TypeError: Subscripted generics cannot be used with class and instance checks.**

The value_to_type function catches this TypeError and re-raises its own TypeError (e.g., "Failed converting key 'AA' to type..."), which hides the root cause.

This bug appears to be in temporalio/converter.py around line 1601 (in v1.18.1) inside the "mapping" logic of converter.py::value_to_type:

```python
is_newtype = getattr(key_type, "__supertype__", None)
                      if is_newtype or not isinstance(key, key_type): # <--- THIS IS THE BUG
                          key = value_to_type(key_type, key, custom_converters)
```

Stacktrace:
```
Failing workflow task run_id=019a2143-d61f-76f5-95c8-c4b9e79ecd51 failure=Failure { failure: Some(Failure { message: "Failed decoding arguments", source: "", stack_trace: "  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py\", line 413, in activate\n    self._apply(job)\n    ~~~~~~~~~~~^^^^^\n\n  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py\", line 512, in _apply\n    self._apply_resolve_activity(job.resolve_activity)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py\", line 761, in _apply_resolve_activity\n    ret_vals = self._convert_payloads(\n        [job.result.completed.result],\n        ret_types,\n    )\n\n  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py\", line 2000, in _convert_payloads\n    raise RuntimeError(\"Failed decoding arguments\") from err\n", encoded_attributes: None, cause: Some(Failure { message: "Failed converting field stats on dataclass <class 'core.app.domain.models.experiments.RunSummary'>", source: "", stack_trace: "  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/worker/_workflow_instance.py\", line 1990, in _convert_payloads\n    return self._payload_converter.from_payloads(\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        payloads,\n        ^^^^^^^^^\n        type_hints=types,\n        ^^^^^^^^^^^^^^^^^\n    )\n    ^\n\n  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/converter.py\", line 311, in from_payloads\n    values.append(converter.from_payload(payload, type_hint))\n                  ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n\n  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/converter.py\", line 594, in from_payload\n    obj = value_to_type(type_hint, obj, self._custom_type_converters)\n\n  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/converter.py\", line 1645, in value_to_type\n    raise TypeError(\n        f\"Failed converting field {field.name} on dataclass {hint}\"\n    ) from err\n", encoded_attributes: None, cause: Some(Failure { message: "Failed converting key 'AA' to type typing.Literal['Clone', 'Chain', 'Plate', 'Well', 'DNA', 'AA', 'Clonaltype Count'] in mapping dict[typing.Literal['Clone', 'Chain', 'Plate', 'Well', 'DNA', 'AA', 'Clonaltype Count'], dict[typing.Literal['Mean', 'Median', 'Range', 'Count of Missing Values', 'Count of Unique Values', 'Data Type', 'Total Records'], str | int | float | None]]", source: "", stack_trace: "  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/converter.py\", line 1641, in value_to_type\n    field_values[field.name] = value_to_type(\n                               ~~~~~~~~~~~~~^\n        field_hints[field.name], field_value, custom_converters\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n\n  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/converter.py\", line 1604, in value_to_type\n    raise TypeError(\n        f\"Failed converting key {repr(key)} to type {key_type} in mapping {hint}\"\n    ) from err\n", encoded_attributes: None, cause: Some(Failure { message: "**Subscripted generics cannot be used with class and instance checks**", source: "", stack_trace: "  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/converter.py\", line 1601, in value_to_type\n    if is_newtype or not isinstance(key, key_type):\n                         ~~~~~~~~~~^^^^^^^^^^^^^^^\n\n  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/worker/workflow_sandbox/_importer.py\", line 497, in __call__\n    return self.current(*args, **kwargs)\n           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n\n  File \"<my_repo>/.venv/lib/python3.13/site-packages/temporalio/worker/workflow_sandbox/_importer.py\", line 120, in unwrap_second_param\n    return orig(a, b)\n\n  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/typing.py\", line 1375, in __instancecheck__\n    return self.__subclasscheck__(type(obj))\n           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n\n  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/typing.py\", line 1378, in __subclasscheck__\n    raise TypeError(\"**Subscripted generics cannot be used with\"\n                    \" class and instance checks**\")\n", encoded_attributes: None, cause: None, failure_info: Some(ApplicationFailureInfo(ApplicationFailureInfo { r#type: "TypeError", non_retryable: false, details: None, next_retry_delay: None, category: Unspecified })) }), failure_info: Some(ApplicationFailureInfo(ApplicationFailureInfo { r#type: "TypeError", non_retryable: false, details: None, next_retry_delay: None, category: Unspecified })) }), failure_info: Some(ApplicationFailureInfo(ApplicationFailureInfo { r#type: "TypeError", non_retryable: false, details: None, next_retry_delay: None, category: Unspecified })) }), failure_info: Some(ApplicationFailureInfo(ApplicationFailureInfo { r#type: "RuntimeError", non_retryable: false, details: None, next_retry_delay: None, category: Unspecified })) }), force_cause: Unspecified }
```


### Minimal Reproduction

The following pytest unit test reliably reproduces the bug by isolating the value_to_type function. This test expects the "Failed converting key" TypeError, confirming the bug's presence.

```python
import pytest
import typing
from temporalio.converter import value_to_type

def test_value_to_type_literal_key_bug():
    """
    Reproduces the bug where value_to_type fails on a dict 
    with a typing.Literal key on Python 3.13+.
    """
    
    # 1. Define the types
    KeyHint = typing.Literal["Key1", "Key2"]
    InnerKeyHint = typing.Literal["Inner1", "Inner2"]
    InnerValueHint = str | int | float | None
    ValueHint = dict[InnerKeyHint, InnerValueHint]

    # 2. Define the hint and value that cause the crash
    hint_with_bug = dict[KeyHint, ValueHint]
    value_to_convert = {"Key1": {"Inner1": 123.45, "Inner2": 10}}
    custom_converters = []

    # 3. Assert that the function raises the specific TypeError
    # This test PASSES if the bug is present.
    # It will FAIL if the bug is fixed (as the error won't be raised).
    with pytest.raises(TypeError, match="Failed converting key 'Key1' to type"):
        value_to_type(hint_with_bug, value_to_convert, custom_converters)
```


### Environment/Versions

- OS and processor: macOS (Apple Silicon, based on /opt/homebrew/)
- Python Version: 3.13.2
- Temporal Version: 1.18.1
- Using Docker, K8s, and building from source

### Additional context

The fix is to add a check to see if key_type is a "real" class (like int or str) before calling isinstance(), as isinstance(typing.Literal[...], type) returns False.

Proposed Fix:

Change this line:

```
if is_newtype or not isinstance(key, key_type):
```

To this:

```python
if (
    is_newtype
    or not isinstance(key_type, type)  # Check if key_type is a class
    or not isinstance(key, key_type)
):
```

This short-circuits the logic and avoids calling isinstance(key, key_type) when key_type is a generic, correctly falling through to the value_to_type call which handles Literal properly.



---

<a id="1176"></a>

### #1176: [Feature Request] Clean up pyright exclude list

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1176 |
| **State** | CLOSED |
| **Author** | yuandrew (Andrew Yuan) |
| **Created** | 2025-10-21 21:36:14.000 UTC (2 months ago) |
| **Updated** | 2026-01-05 14:55:30.000 UTC |
| **Closed** | 2026-01-05 14:55:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.
Ran into the CI lint failure mentioned in `pyproject.toml`
```
  # TODO: these pass locally but fail in CI with
  # error: Import "temporalio.bridge.temporal_sdk_bridge" could not be resolved
```

ended up switching the order of `poe lint` and `poe build-develop` in CI to resolve this issue. 

### Describe the solution you'd like
The list of excludes is quite long, with some substantial files. We should go through and clean up the list of files that need to be excluded here.

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="1171"></a>

### #1171: [Feature Request] Check integration support with agents 0.4.0

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1171 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-10-20 15:42:50.000 UTC (2 months ago) |
| **Updated** | 2025-10-24 15:28:01.000 UTC |
| **Closed** | 2025-10-24 15:28:01.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

See if there are any breaking changes, update requirements.

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="1160"></a>

### #1160: Modify code comment for Worker build_id field

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1160 |
| **State** | CLOSED |
| **Author** | ntaylorkhan |
| **Created** | 2025-10-14 21:59:12.000 UTC (2 months ago) |
| **Updated** | 2025-10-22 16:08:38.000 UTC |
| **Closed** | 2025-10-22 16:08:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

https://github.com/temporalio/sdk-python/blob/162cff7cc290979974e7c44000c9dd6da9e3faf9/temporalio/worker/_worker.py#L206

Current comment reads:

>  build_id: Unique identifier for the current runtime. This is best
>                 set as a hash of all code and should change only when code does.
>                 If unset, a best-effort identifier is generated.

On initialization the best-effort process results in walking and MD5 hashing every module in the codebase, this is normally fast but in very large codebases (15M+ lines of code) this can result in a very long wait for worker to boot. 

Code comment should include a warning about long initialization times for large codebases, here is a potential replacement for inspiration:


> A unique identifier for the current runtime. This value should ideally be provided as a hash of the complete source code and must change only when the code itself changes.
> 
> If not explicitly set, the system automatically generates a best-effort identifier by traversing and computing hashes of all modules in the codebase. In very large codebases this automatic process may significantly increase initialization time.


---

<a id="1159"></a>

### #1159: [Feature Request] Create test to verify OpenAI plugin works in activity-only and workflow-only modes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1159 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-10-14 18:58:52.000 UTC (2 months ago) |
| **Updated** | 2025-10-27 22:35:35.000 UTC |
| **Closed** | 2025-10-27 22:35:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Many Python users, perhaps more than any other SDK, are splitting up workers to activity-only workers and workflow-only workers. We need to make sure the OpenAI plugin can support this setup instead of forcing activities and workflows to run in the same process.


---

<a id="1133"></a>

### #1133: [Feature Request] Provide an easier way to create simple Plugins

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1133 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-09-29 23:26:03.000 UTC (3 months ago) |
| **Updated** | 2025-10-07 17:33:40.000 UTC |
| **Closed** | 2025-10-07 17:33:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Currently, you have to implement quite a few things in order to create a plugin, even if it only does a few simple things.

### Describe the solution you'd like

Provide a function for creating a simple plugin from a set of static configurations: activities, workflows, dataconverter, etc.

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="1132"></a>

### #1132: [Feature Request] Properly set Python programming language versions and license in wheel/PyPI

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1132 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-09-29 21:18:11.000 UTC (3 months ago) |
| **Updated** | 2025-10-20 23:51:47.000 UTC |
| **Closed** | 2025-10-20 23:51:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

With the move from poetry to uv, we have lost the programming language classifier I think. Compare the bottom left of https://pypi.org/project/temporalio/1.10.0/ to https://pypi.org/project/temporalio/1.11.0/. This is causing our README badge to be invalid, e.g. compare https://img.shields.io/pypi/pyversions/pydantic with https://img.shields.io/pypi/pyversions/temporalio.

It looks like we also need to set the license properly for that badge. Compare https://img.shields.io/pypi/l/pydantic.svg?style=for-the-badge with https://img.shields.io/pypi/l/temporalio.svg?style=for-the-badge.


---

<a id="1121"></a>

### #1121: [Feature Request] Document runtime telemetry config fields

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1121 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-09-25 14:14:35.000 UTC (3 months ago) |
| **Updated** | 2025-09-25 14:14:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

`OpenTelemetryConfig` and `PrometheusConfig` should have their fields documented clearly


---

<a id="1090"></a>

### #1090: [Feature Request] Support `mcp_server` in OpenAI integration

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1090 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-09-09 19:59:36.000 UTC (4 months ago) |
| **Updated** | 2025-09-22 18:30:01.000 UTC |
| **Closed** | 2025-09-22 18:30:01.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

We don't currently provide a supported `MCPServer` implementation.

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="1084"></a>

### #1084: [Feature Request] Do not clobber user payload codec in OpenAI plugin

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1084 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-09-08 15:23:40.000 UTC (4 months ago) |
| **Updated** | 2025-09-09 17:18:38.000 UTC |
| **Closed** | 2025-09-09 17:18:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

OpenAI plugin replaces the entire data converter instead of just the payload converter. Also, per https://github.com/temporalio/sdk-python/pull/1083#pullrequestreview-3194906671, we should consider erroring if the data converter is provided with a non-default payload converter (since we are overriding it). Or we could error only if the user-provided payload converter is not what we need (Pydantic with a setting set). There could be discussions on advanced user payload converter customization on top of the one OpenAI plugin sets, but that can wait until needed.


---

<a id="1080"></a>

### #1080: [Bug] Unable to import generated protocol buffer code

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1080 |
| **State** | CLOSED |
| **Author** | SpencerC (Spencer Connaughton) |
| **Created** | 2025-09-05 17:19:16.000 UTC (4 months ago) |
| **Updated** | 2025-09-16 16:01:49.000 UTC |
| **Closed** | 2025-09-16 16:01:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Depend on generated protocol buffer files.

### Describe the bug

Iâ€™m trying to start using the Temporal python SDK and Iâ€™m getting the following error when I try to import 
>google.protobuf.runtime_version.VersionError: Detected mismatched Protobuf Gencode/Runtime major versions when loading chat/v1/types.proto: gencode 6.31.1 runtime 5.29.5. Same major version is required. See Protobuf version guarantees at https://protobuf.dev/support/cross-version-runtime-guarantee.

We have other dependencies that need a minimum protobuf version 31.1. Can you please update the underlying protobuf/grpc versions so that it works with newer gencode?

### Minimal Reproduction

1. Generate a Python protocol buffer library using the latest version protoc.
2. Attempt to load that library into a module that also depends on the Temporal python SDK.

### Environment/Versions

- Temporal Version: 1.17.0

### Additional context

<!-- Add any other context about the problem here. -->



---

<a id="1079"></a>

### #1079: [Feature Request] Ensure tests exist to confirm custom slot supplier slot info has proper fields

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1079 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-09-05 12:31:34.000 UTC (4 months ago) |
| **Updated** | 2025-09-05 12:31:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/672



---

<a id="1077"></a>

### #1077: [Feature Request] Extend typing improvements from #938

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1077 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2025-09-04 02:14:04.000 UTC (4 months ago) |
| **Updated** | 2025-09-04 02:14:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

#938 improved type inference. Check whether the analogous improvement can be made elsewhere, as suggested in the comment:

> Should this also be done with workflow things like execute_activity, start_activity, execute_child_workflow, and start_child_workflow?


---

<a id="1071"></a>

### #1071: [Bug] Ensure every inbound and outbound workflow payload goes through codec

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1071 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-09-02 13:54:30.000 UTC (4 months ago) |
| **Updated** | 2025-09-10 21:13:22.000 UTC |
| **Closed** | 2025-09-10 21:13:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Today it seems user metadata (and maybe other things) aren't encoded outbound. Not only must we make sure _every_ non-search attribute payload is decoded on activation and encoded on completion, we must write a test to ensure this for all future payloads.

The ideal way forward here would be to write a general code generated payload visitor utility that everyone can use, even ourselves. We have done this in Go and Ruby. The visitor could be used for this. It would also help solve https://github.com/temporalio/features/issues/468.


---

<a id="1063"></a>

### #1063: [Feature Request] Ensure gRPC binary metadata headers are supported

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1063 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-08-27 19:42:44.000 UTC (4 months ago) |
| **Updated** | 2025-09-12 18:27:04.000 UTC |
| **Closed** | 2025-09-12 18:27:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/671. Probably just need to accept `bytes` as metadata value and send as binary to Tonic for `-bin` keys.



---

<a id="1058"></a>

### #1058: [Bug] OpenAI Agent SDK wraps Temporal ApplicationError preventing workflow acting on it

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1058 |
| **State** | CLOSED |
| **Author** | DonForbes (Donald Forbes) |
| **Created** | 2025-08-27 14:05:02.000 UTC (4 months ago) |
| **Updated** | 2025-09-03 19:00:30.000 UTC |
| **Closed** | 2025-09-03 19:00:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?
Using the openai agent SDK integration I was looking to have an agent invoke a tool (activity_as_tool) and the invocation identified a non-retryable error.  I want to raise an ApplicationError and get this back to the workflow so it can decide on appropriate next steps to take.

### Describe the bug
My understanding is that the Agent SDK wraps/catches the ApplicationError and raises it as a UserError.  This causes the workflow task after the failed activity to also fail and gets stuck like this without ever moving forward.

### Minimal Reproduction
Starting from the https://github.com/temporal-community/openai-agents-demos repository look to run the tools workflow (run_tools_workflow.py)  having edited the get_weather_activity.py to make it raise an ApplicationError that is non-retryable.

### Environment/Versions
Temporal python SDK 1.15.0
openai-agents 0.2.7

### Additional context

None



---

<a id="1056"></a>

### #1056: [Feature Request] Add workflow MCP client

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1056 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2025-08-26 15:54:29.000 UTC (4 months ago) |
| **Updated** | 2025-08-26 17:02:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

The SDK should contain a component allowing MCP to be used from native workflow code (i.e. when not using a 3rd-party agent SDK).

For the SDK API, one possibility is that this will be an `mcp.ClientSession`.

The initial version of this should allow use of existing `stdio` and `http` transport MCP servers. Regarding implementation, one possibility is that this will involve a custom transport that responds to `tools/list` and `tools/call` calls by making activity calls, making use of an activity-local activity worker to implement stateful sessions, as implemented in https://github.com/temporalio/sdk-python/pull/1021. This may be similar to [nexus-mcp-python:nexusmcp/workflow_transport.py](https://github.com/bergundy/nexus-mcp-python/blob/main/nexusmcp/workflow_transport.py), adapted to use activities.

Subsequent versions should allow fully durable/fault-tolerant tool calls / client sessions, i.e. making use of durable execution primitives such as workflows exposed as MCP tools. This may involve adding options to make the transport use nexus operations in addition to activities. 


---

<a id="1025"></a>

### #1025: [Feature Request] Add first execution run ID to workflow info

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/1025 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-08-11 14:27:21.000 UTC (5 months ago) |
| **Updated** | 2025-08-15 15:54:06.000 UTC |
| **Closed** | 2025-08-15 15:54:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/29


---

<a id="994"></a>

### #994: [Feature Request] Worker plugin support for replayers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/994 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-07-24 18:24:31.000 UTC (5 months ago) |
| **Updated** | 2025-08-07 00:04:03.000 UTC |
| **Closed** | 2025-08-07 00:04:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Plugins need to be able to be provided to replayers. The problem is worker plugins have a lot of config that a replayer does not, but there are important shared things like interceptors. We're going to have to decide which option we want, here are a few:

* Add `configure_replayer` + `run_replayer` on `worker.Plugin` making every plugin author to add extra code to configure common things like interceptors
* Try to reuse `configure_worker` + `run_worker` knowing there are a ton of config options and worker instance stuff that doesn't overlap
* Have a completely separate `worker.ReplayerPlugin` which has same problem as first bullet, but at least will give type error when trying to register when creating replayer

This same problem will need to be solved in other languages too. The third is probably the best so we can keep API clean and plugin authors can just choose not to support replayer if they want.


---

<a id="978"></a>

### #978: [Feature Request] Add Summary to LocalActivityConfig

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/978 |
| **State** | CLOSED |
| **Author** | yuandrew (Andrew Yuan) |
| **Created** | 2025-07-18 00:04:08.000 UTC (5 months ago) |
| **Updated** | 2025-09-04 21:54:01.000 UTC |
| **Closed** | 2025-09-04 21:54:01.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

See `features` repo issue for more info

### Describe the solution you'd like



### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="965"></a>

### #965: [Bug] pydantic_core._pydantic_core.PydanticSerializationError: Error serializing to JSON: TypeError: 'MockValSer' object cannot be converted to 'SchemaSerializer'

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/965 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-07-14 19:13:00.000 UTC (5 months ago) |
| **Updated** | 2025-07-22 17:09:23.000 UTC |
| **Closed** | 2025-07-22 17:09:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | tconley1428 |
| **Milestone** | None |

#### Description

### What are you really trying to do?

Use openai_agents across multiple workers.

### Describe the bug

In some cases openai models will not have been built. 

### Minimal Reproduction
`export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1`
Then run the hello world sample

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: [e.g. M1 Mac, x86 Windows, Linux]
- Temporal Version: [e.g. 1.14.0?] and/or SDK version
- Are you using Docker or Kubernetes or building Temporal from source?

### Additional context

<!-- Add any other context about the problem here. -->



---

<a id="950"></a>

### #950: [Feature Request] Plugins to support controlling multiple configuration points at once

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/950 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-07-09 15:43:02.000 UTC (6 months ago) |
| **Updated** | 2025-07-21 16:29:31.000 UTC |
| **Closed** | 2025-07-21 16:29:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

See parent https://github.com/temporalio/features/issues/652
 
### Describe the solution you'd like

Should support OpenAI integration as an example.

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="921"></a>

### #921: Remove eval_type_backport dependency

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/921 |
| **State** | OPEN |
| **Author** | tconley1428 |
| **Created** | 2025-06-25 16:07:36.000 UTC (6 months ago) |
| **Updated** | 2025-06-25 16:07:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | tconley1428 |
| **Milestone** | None |

#### Description

When python 3.9 is EOL, we should remove the dependency on `eval_type_backport` in the `open-ai` extra. 


---

<a id="916"></a>

### #916: Add tests for the tracing intercepter to OpenAI tests

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/916 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-06-23 18:13:11.000 UTC (6 months ago) |
| **Updated** | 2025-07-18 17:11:20.000 UTC |
| **Closed** | 2025-07-18 17:11:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

`OpenAIAgentsTracingInterceptor` is currently unused in the SDK code base, it should have tests which verify its output


---

<a id="915"></a>

### #915: Add tests for heartbeating in OpenAI tests

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/915 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-06-23 18:12:18.000 UTC (6 months ago) |
| **Updated** | 2025-07-23 18:02:16.000 UTC |
| **Closed** | 2025-07-23 18:02:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

*No description provided.*


---

<a id="914"></a>

### #914: Fix OpenAI extension issues on python 3.9-3.10

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/914 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-06-23 18:11:47.000 UTC (6 months ago) |
| **Updated** | 2025-06-25 17:01:28.000 UTC |
| **Closed** | 2025-06-25 17:01:28.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Previously errors were seen on older python versions, ensure those problems are gone and run the tests on all CI versions.


---

<a id="913"></a>

### #913: Move OpenAI overrides to a worker interecepter

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/913 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-06-23 18:10:52.000 UTC (6 months ago) |
| **Updated** | 2025-07-15 16:53:02.000 UTC |
| **Closed** | 2025-07-15 16:53:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently the overrides are provided by a contextmanager, they should be configured on the worker instead to narrow the scope and make it easier to use.


---

<a id="912"></a>

### #912: Validate OpenAI data converter necessity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/912 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-06-23 18:09:30.000 UTC (6 months ago) |
| **Updated** | 2025-07-03 20:06:32.000 UTC |
| **Closed** | 2025-07-03 20:06:32.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See if there's a way to remove the data converter entirely. If not, justify that reason in code.


---

<a id="911"></a>

### #911: Add named parameters to OpenAI APIs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/911 |
| **State** | CLOSED |
| **Author** | tconley1428 |
| **Created** | 2025-06-23 18:08:33.000 UTC (6 months ago) |
| **Updated** | 2025-06-25 17:57:46.000 UTC |
| **Closed** | 2025-06-25 17:57:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

APIs which subsequently configure activity executions should have named parameters instead of `kwargs`


---

<a id="892"></a>

### #892: [Feature Request] Support `merge_extra` in custom adapters

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/892 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-06-06 20:26:24.000 UTC (7 months ago) |
| **Updated** | 2025-06-06 20:26:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/sdk-python/issues/276#issuecomment-2949910553.

For `workflow.LoggerAdapter` and `activity.LoggerAdapter`, consider supporting `merge_extra` to allow users to set adapter-level extra data. Note, the stdlib's form of this is 3.13 only so we may need our own form.


---

<a id="889"></a>

### #889: [Bug] Improperly accessing current activity context in outbound heartbeat

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/889 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-06-06 16:10:51.000 UTC (7 months ago) |
| **Updated** | 2025-06-27 14:50:03.000 UTC |
| **Closed** | 2025-06-27 14:50:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

First found in Ruby at https://github.com/temporalio/sdk-ruby/issues/279, we are accessing the current context in the outbound activity heartbeater to get the task token. We should not, because we allow heartbeating outside of context. So we should give the interceptor the task token another way.


---

<a id="874"></a>

### #874: [Bug] Workflow interceptors initialized before some attributes set in instance constructor

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/874 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-05-19 13:55:47.000 UTC (7 months ago) |
| **Updated** | 2025-06-05 21:57:53.000 UTC |
| **Closed** | 2025-06-05 21:57:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

People need to be able to do things in inbound `init` (or outbound constructor which is often in there) such as `set_signal_handler`, but this fails on the `_assert_not_read_only` of that call because it checks `self._deleting` which is not created until after interceptor init. Move the interceptor create/init to the _end_ of the workflow instance constructor.


---

<a id="870"></a>

### #870: [Python] Worker Versioning high level client

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/870 |
| **State** | OPEN |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2025-05-14 22:46:38.000 UTC (7 months ago) |
| **Updated** | 2025-05-14 22:47:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Create the high-level client for control plane operations concerning worker deployments. See the Go one as an example (though it also needs some updating as of this writing)


---

<a id="865"></a>

### #865: [Bug] Flaky test: `tests/worker/test_workflow.py::test_workflow_return_is_honored_when_it_precedes_signal_completion_command`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/865 |
| **State** | CLOSED |
| **Author** | maciejdudko (Maciej Dudkowski) |
| **Created** | 2025-05-12 21:55:31.000 UTC (8 months ago) |
| **Updated** | 2025-09-30 17:20:00.000 UTC |
| **Closed** | 2025-09-30 17:20:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

The following test sometimes fails with `temporalio.exceptions.ApplicationError: Client should never see this error!`

`tests/worker/test_workflow.py::test_workflow_return_is_honored_when_it_precedes_signal_completion_command`

### Minimal Reproduction

Running the test 10,000 times reproduces it fairly reliably:
```
zsh
set -e
for i in $(seq 10000); do; poe test tests/worker/test_workflow.py -k test_workflow_return_is_honored_when_it_precedes_signal_completion_command; done
```

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: ARM Ubuntu (CI), x86 Mac (CI), M4 Mac (local) 
- Temporal Version: [SDK main @ 257f143](https://github.com/temporalio/sdk-python/commit/257f14380aff6a5bf8e11be19061617b4a446303)
- Are you using Docker or Kubernetes or building Temporal from source? Both

### Additional context

<!-- Add any other context about the problem here. -->

CI jobs with failure:
https://github.com/temporalio/sdk-python/actions/runs/14937905636/job/41969535426
https://github.com/temporalio/sdk-python/actions/runs/14980669571/job/42083795290

<details>

<summary>Terminal output:</summary>

```
=================================== FAILURES ===================================
__ test_workflow_return_is_honored_when_it_precedes_signal_completion_command __
temporalio.exceptions.ApplicationError: Client should never see this error!

The above exception was the direct cause of the following exception:

client = <temporalio.client.Client object at 0xfff417c82210>
main_workflow_returns_before_signal_completions = True

    async def _do_first_completion_command_is_honored_test(
        client: Client, main_workflow_returns_before_signal_completions: bool
    ):
        workflow_cls: Union[
            Type[FirstCompletionCommandIsHonoredPingPongWorkflow],
            Type[FirstCompletionCommandIsHonoredWorkflow],
        ] = (
            FirstCompletionCommandIsHonoredPingPongWorkflow
            if main_workflow_returns_before_signal_completions
            else FirstCompletionCommandIsHonoredWorkflow
        )
        async with Worker(
            client,
            task_queue="tq",
            workflows=[workflow_cls],
        ) as worker:
            handle = await client.start_workflow(
                workflow_cls.run,
                id=f"wf-{uuid.uuid4()}",
                task_queue=worker.task_queue,
            )
            await handle.signal(workflow_cls.this_signal_executes_second)
            await handle.signal(workflow_cls.this_signal_executes_first)
            try:
>               result = await handle.result()

tests/worker/test_workflow.py:6060: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <temporalio.client.WorkflowHandle object at 0xfff40eb1cf80>

    async def result(
        self,
        *,
        follow_runs: bool = True,
        rpc_metadata: Mapping[str, str] = {},
        rpc_timeout: Optional[timedelta] = None,
    ) -> ReturnType:
        """Wait for result of the workflow.
    
        This will use :py:attr:`result_run_id` if present to base the result on.
        To use another run ID, a new handle must be created via
        :py:meth:`Client.get_workflow_handle`.
    
        Args:
            follow_runs: If true (default), workflow runs will be continually
                fetched, until the most recent one is found. If false, the first
                result is used.
            rpc_metadata: Headers used on the RPC call. Keys here override
                client-level RPC metadata keys.
            rpc_timeout: Optional RPC deadline to set for each RPC call. Note,
                this is the timeout for each history RPC call not this overall
                function.
    
        Returns:
            Result of the workflow after being converted by the data converter.
    
        Raises:
            WorkflowFailureError: Workflow failed, was cancelled, was
                terminated, or timed out. Use the
                :py:attr:`WorkflowFailureError.cause` to see the underlying
                reason.
            Exception: Other possible failures during result fetching.
        """
        # We have to maintain our own run ID because it can change if we follow
        # executions
        hist_run_id = self._result_run_id
        while True:
            async for event in self._fetch_history_events_for_run(
                hist_run_id,
                wait_new_event=True,
                event_filter_type=WorkflowHistoryEventFilterType.CLOSE_EVENT,
                skip_archival=True,
                rpc_metadata=rpc_metadata,
                rpc_timeout=rpc_timeout,
            ):
                if event.HasField("workflow_execution_completed_event_attributes"):
                    complete_attr = event.workflow_execution_completed_event_attributes
                    # Follow execution
                    if follow_runs and complete_attr.new_execution_run_id:
                        hist_run_id = complete_attr.new_execution_run_id
                        break
                    # Ignoring anything after the first response like TypeScript
                    type_hints = [self._result_type] if self._result_type else None
                    results = await self._client.data_converter.decode_wrapper(
                        complete_attr.result,
                        type_hints,
                    )
                    if not results:
                        return cast(ReturnType, None)
                    elif len(results) > 1:
                        warnings.warn(f"Expected single result, got {len(results)}")
                    return cast(ReturnType, results[0])
                elif event.HasField("workflow_execution_failed_event_attributes"):
                    fail_attr = event.workflow_execution_failed_event_attributes
                    # Follow execution
                    if follow_runs and fail_attr.new_execution_run_id:
                        hist_run_id = fail_attr.new_execution_run_id
                        break
>                   raise WorkflowFailureError(
                        cause=await self._client.data_converter.decode_failure(
                            fail_attr.failure
                        ),
E                       temporalio.client.WorkflowFailureError: Workflow execution failed

temporalio/client.py:[163](https://github.com/temporalio/sdk-python/actions/runs/14937905636/job/41969535426#step:14:164)7: WorkflowFailureError

During handling of the above exception, another exception occurred:

client = <temporalio.client.Client object at 0xfff417c82210>

    async def test_workflow_return_is_honored_when_it_precedes_signal_completion_command(
        client: Client,
    ):
>       await _do_first_completion_command_is_honored_test(
            client, main_workflow_returns_before_signal_completions=True
        )

tests/worker/test_workflow.py:6031: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

client = <temporalio.client.Client object at 0xfff417c82210>
main_workflow_returns_before_signal_completions = True

    async def _do_first_completion_command_is_honored_test(
        client: Client, main_workflow_returns_before_signal_completions: bool
    ):
        workflow_cls: Union[
            Type[FirstCompletionCommandIsHonoredPingPongWorkflow],
            Type[FirstCompletionCommandIsHonoredWorkflow],
        ] = (
            FirstCompletionCommandIsHonoredPingPongWorkflow
            if main_workflow_returns_before_signal_completions
            else FirstCompletionCommandIsHonoredWorkflow
        )
        async with Worker(
            client,
            task_queue="tq",
            workflows=[workflow_cls],
        ) as worker:
            handle = await client.start_workflow(
                workflow_cls.run,
                id=f"wf-{uuid.uuid4()}",
                task_queue=worker.task_queue,
            )
            await handle.signal(workflow_cls.this_signal_executes_second)
            await handle.signal(workflow_cls.this_signal_executes_first)
            try:
                result = await handle.result()
            except WorkflowFailureError as err:
                if main_workflow_returns_before_signal_completions:
>                   raise RuntimeError(
                        "Expected no error due to main workflow coroutine returning first"
                    )
E                   RuntimeError: Expected no error due to main workflow coroutine returning first

tests/worker/test_workflow.py:6063: RuntimeError
------------------------------ Captured log call -------------------------------
21:25:24 [   DEBUG] Workflow raised failure with run ID 0196b6f0-4e20-78dc-9011-350d605a0452 (_workflow_instance.py:2003)
Traceback (most recent call last):
  File "/home/runner/work/sdk-python/sdk-python/temporalio/worker/_workflow_instance.py", line 1982, in _run_top_level_workflow_function
    await coro
  File "/home/runner/work/sdk-python/sdk-python/temporalio/testing/_workflow.py", line 522, in handle_signal
    return await super().handle_signal(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/sdk-python/sdk-python/temporalio/worker/_interceptor.py", line 337, in handle_signal
    return await self.next.handle_signal(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/sdk-python/sdk-python/temporalio/worker/_workflow_instance.py", line 2372, in handle_signal
    await handler(*input.args)
  File "/home/runner/work/sdk-python/sdk-python/temporalio/workflow.py", line [177](https://github.com/temporalio/sdk-python/actions/runs/14937905636/job/41969535426#step:14:178)4, in with_object
    return await fn(obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/sdk-python/sdk-python/tests/worker/test_workflow.py", line 6001, in this_signal_executes_second
    raise ApplicationError("Client should never see this error!")
temporalio.exceptions.ApplicationError: Client should never see this error!
21:25:24 [   DEBUG] Workflow raised failure with run ID 0196b6f0-4e20-78dc-9011-350d605a0452 (_workflow_instance.py:2003)
Traceback (most recent call last):
  File "/home/runner/work/sdk-python/sdk-python/temporalio/worker/_workflow_instance.py", line 1982, in _run_top_level_workflow_function
    await coro
  File "/home/runner/work/sdk-python/sdk-python/temporalio/testing/_workflow.py", line 522, in handle_signal
    return await super().handle_signal(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/sdk-python/sdk-python/temporalio/worker/_interceptor.py", line 337, in handle_signal
    return await self.next.handle_signal(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/sdk-python/sdk-python/temporalio/worker/_workflow_instance.py", line 2372, in handle_signal
    await handler(*input.args)
  File "/home/runner/work/sdk-python/sdk-python/temporalio/workflow.py", line 1774, in with_object
    return await fn(obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/sdk-python/sdk-python/tests/worker/test_workflow.py", line 5990, in this_signal_executes_first
    raise ApplicationError(
    ...<2 lines>...
    )
temporalio.exceptions.ApplicationError: Client should see this error unless doing ping-pong (in which case main coroutine returns first)
21:25:24 [   DEBUG] Evicting workflow with run ID 0[196](https://github.com/temporalio/sdk-python/actions/runs/14937905636/job/41969535426#step:14:197)b6f0-4e20-78dc-9011-350d605a0452, message: Workflow completed (_workflow.py:365)
21:25:24 [    INFO] Beginning worker shutdown, will wait 0:00:00 before cancelling activities (_worker.py:640)
=============================== warnings summary ===============================
tests/worker/test_activity.py::test_sync_activity_process_non_picklable
tests/worker/test_activity.py::test_sync_activity_process_failure
tests/worker/test_activity.py::test_sync_activity_process_cancel
tests/worker/test_activity.py::test_sync_activity_process_cancel_uncaught
tests/worker/test_activity.py::test_sync_activity_process_heartbeat_details
tests/worker/test_activity.py::test_sync_activity_process_non_picklable_heartbeat_details
tests/worker/test_activity.py::test_sync_activity_dynamic_process
  /home/runner/work/sdk-python/sdk-python/tests/worker/test_activity.py:1331: UserWarning: Worker max_concurrent_activities is 50 but activity_executor's max_workers is only 2
    async with Worker(**worker_config):

tests/worker/test_activity.py::test_sync_activity_process_worker_shutdown_graceful
  /home/runner/work/sdk-python/sdk-python/tests/worker/test_activity.py:934: UserWarning: Worker max_concurrent_activities is 50 but activity_executor's max_workers is only 2
    act_worker = Worker(

tests/worker/test_activity.py::test_sync_activity_process_executor_crash
  /home/runner/work/sdk-python/sdk-python/tests/worker/test_activity.py:991: UserWarning: Worker max_concurrent_activities is 50 but activity_executor's max_workers is only 2
    act_worker = Worker(

tests/worker/workflow_sandbox/test_runner.py::test_workflow_sandbox_global_state[sandboxed_passthrough_modules0]
  /home/runner/work/sdk-python/sdk-python/temporalio/worker/workflow_sandbox/_importer.py:[234](https://github.com/temporalio/sdk-python/actions/runs/14937905636/job/41969535426#step:14:235): RuntimeWarning: coroutine '_WorkflowInstanceImpl._apply_initialize_workflow.<locals>.run_workflow' was never awaited
    mod = importlib.__import__(name, globals, locals, fromlist, level)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /home/runner/work/sdk-python/sdk-python/junit-xml/3.13--ubuntu-arm.xml -
=========================== short test summary info ============================
FAILED tests/worker/test_workflow.py::test_workflow_return_is_honored_when_it_precedes_signal_completion_command - RuntimeError: Expected no error due to main workflow coroutine returning first
====== 1 failed, 386 passed, 8 skipped, 10 warnings in 271.45s (0:04:31) =======
Error: Process completed with exit code 1.
```
</details>


---

<a id="864"></a>

### #864: [Feature Request] Add some kind of extra on some logs for easier identification

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/864 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-05-12 16:27:41.000 UTC (8 months ago) |
| **Updated** | 2025-06-10 16:08:51.000 UTC |
| **Closed** | 2025-06-10 16:08:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Some users in their custom log handlers want to identify certain log records so they can do something else with them (e.g. increase/decrease the log level). We need some kind of clear string they can use as a conditional instead of checking English message patrterns. Research the best way to provide this (e.g. on `extra`?). This only _needs_ to be logging done on activity failures and workflow task failures at this time, but may be able to apply to other `warning` logs or similar situations.

Also a sample showing how to customize log level for workflow task failure would be helpful here once this issue is done.


---

<a id="849"></a>

### #849: [Feature Request] Add workflow.Info.workflow_start_time

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/849 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-04-24 15:01:08.000 UTC (8 months ago) |
| **Updated** | 2025-05-13 17:20:25.000 UTC |
| **Closed** | 2025-05-13 17:20:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

The current start time was (incorrectly) set as the first task start time instead of the workflow start time. After discussion, we feel it's best to clarify that the existing start time is first task start time and add a new `workflow_start_time` that has the actual workflow start time from the initialize job.



---

<a id="847"></a>

### #847: [python] SDK support for activity reset 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/847 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2025-04-23 19:08:07.000 UTC (8 months ago) |
| **Updated** | 2025-09-03 23:30:40.000 UTC |
| **Closed** | 2025-09-03 23:30:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See https://github.com/temporalio/features/issues/620


---

<a id="845"></a>

### #845: [Bug] Missing Local Activites should fail WFT rather than retry-looping

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/845 |
| **State** | CLOSED |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2025-04-22 23:04:14.000 UTC (8 months ago) |
| **Updated** | 2025-05-12 18:04:30.000 UTC |
| **Closed** | 2025-05-12 18:04:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Right now if an LA is invoked that doesn't exist, it will just retry at the _activity_ level which doesn't make sense and will never work. It should fail the WFT. See https://github.com/temporalio/features/issues/221

### Minimal Reproduction

```python
@workflow.defn
class MissingLocalActivityWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        return await workflow.execute_local_activity(
            "whatever", name, schedule_to_close_timeout=timedelta(seconds=5)
        )


async def test_workflow_missing_local_activity(client: Client):
    async with new_worker(
        client, MissingLocalActivityWorkflow, activities=[say_hello]
    ) as worker:
        result = await client.execute_workflow(
            MissingLocalActivityWorkflow.run,
            "Temporal",
            id=f"workflow-{uuid.uuid4()}",
            task_queue=worker.task_queue,
        )
        assert result == "Hello, Temporal!"


```


---

<a id="840"></a>

### #840: [Feature Request] Change get static summary/details on workflow in client to be lazy

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/840 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-04-22 12:29:28.000 UTC (8 months ago) |
| **Updated** | 2025-05-15 15:51:00.000 UTC |
| **Closed** | 2025-05-15 15:51:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Today we eagerly decode these, change to be lazy upon request


---

<a id="835"></a>

### #835: [Feature Request] Environment configuration

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/835 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-04-21 17:19:57.000 UTC (8 months ago) |
| **Updated** | 2025-06-26 16:52:30.000 UTC |
| **Closed** | 2025-06-26 16:52:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/441 and https://github.com/temporalio/proposals/blob/master/all-sdk/external-client-configuration.md


---

<a id="834"></a>

### #834: [Bug] Python 3.13.3 on macOS Intel hangs post-test

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/834 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-04-18 15:08:16.000 UTC (8 months ago) |
| **Updated** | 2025-04-18 15:08:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

On latest Python (3.13.3 as of this writing), macOS Intel sometimes hangs post `pytest` run in our CI, with things like:

```
 2025-04-17T16:21:46.354310Z  WARN temporal_client::retry: gRPC call poll_nexus_task_queue retried 6 times error=Status { code: Unavailable, message: "tcp connect error: Connection refused (os error 61)", source: Some(tonic::transport::Error(Transport, ConnectError(ConnectError("tcp connect error", Os { code: 61, kind: ConnectionRefused, message: "Connection refused" })))) }
2025-04-17T16:21:46.373735Z  WARN temporal_client::retry: gRPC call poll_workflow_task_queue retried 6 times error=Status { code: Unavailable, message: "tcp connect error: Connection refused (os error 61)", source: Some(tonic::transport::Error(Transport, ConnectError(ConnectError("tcp connect error", Os { code: 61, kind: ConnectionRefused, message: "Connection refused" })))) }
```

and 

```
2025-04-17T22:02:54.232127Z  WARN temporal_client::retry: gRPC call get_workflow_execution_history retried 5 times error=Status { code: Unavailable, message: "tcp connect error: Connection refused (os error 61)", source: Some(tonic::transport::Error(Transport, ConnectError(ConnectError("tcp connect error", Os { code: 61, kind: ConnectionRefused, message: "Connection refused" })))) }
2025-04-17T22:02:55.213783Z  WARN temporal_client::retry: gRPC call get_workflow_execution_history retried 6 times error=Status { code: Unavailable, message: "tcp connect error: Connection refused (os error 61)", source: Some(tonic::transport::Error(Transport, ConnectError(ConnectError("tcp connect error", Os { code: 61, kind: ConnectionRefused, message: "Connection refused" })))) }
```

This may be some worker in some test unable to shutdown properly in this specific situation.


---

<a id="831"></a>

### #831: [Feature Request] Show retry policy on activity info

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/831 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-04-17 12:23:48.000 UTC (8 months ago) |
| **Updated** | 2025-09-09 19:12:36.000 UTC |
| **Closed** | 2025-09-09 19:12:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

See https://github.com/temporalio/features/issues/615


---

<a id="827"></a>

### #827: [Bug] Importing uvicorn causes exception

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/827 |
| **State** | CLOSED |
| **Author** | yh-0 (Jakub Kucharski) |
| **Created** | 2025-04-11 16:39:03.000 UTC (9 months ago) |
| **Updated** | 2025-04-11 19:22:02.000 UTC |
| **Closed** | 2025-04-11 19:22:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I want to use temporalio with FastAPI server. Importing uvicorn causes exception.

Steps to reproduce:

Create a new project that uses temporalio python sdk
Write code to run worker and start workflow (code works)
Import uvicorn causes the exception

### Minimal Reproduction
```py
import asyncio
from datetime import timedelta
from temporalio.worker import Worker
from temporalio.client import Client
from temporalio import workflow, activity

import uvicorn # this causes the problem

@activity.defn
async def activity_return_user_id(user_id: str) -> str:
    return f"Returned user: {user_id}"

@workflow.defn
class WorkflowReturnUserId:
    @workflow.run
    async def run(self, user_id: str) -> str:
        return await workflow.execute_activity(activity_return_user_id, user_id, schedule_to_close_timeout=timedelta(seconds=10))

async def worker_coroutine() -> None:
    client = await Client.connect("localhost:7233")

    worker = Worker(
        client,
        task_queue="feed-tasks",
        workflows=[WorkflowReturnUserId],
        activities=[activity_return_user_id],
    )
    asyncio.create_task(worker.run())

    handle = await client.start_workflow(
        WorkflowReturnUserId.run,
        "some-user-id",
        id="wflow-id",
        task_queue="feed-tasks",
    )

    x = await handle.result()
    print(x)


def run_worker():
    asyncio.run(worker_coroutine())

if __name__ == "__main__":
    run_worker()
```

```
Traceback (most recent call last):
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\_workflow.py", line 137, in __init__
    workflow_runner.prepare_workflow(defn)
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_runner.py", line 77, in prepare_workflow
    self.create_instance(
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_runner.py", line 94, in create_instance
    return _Instance(det, self._runner_class, self._restrictions)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_runner.py", line 122, in __init__
    self._create_instance()
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_runner.py", line 133, in _create_instance
    self._run_code(
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_runner.py", line 176, in _run_code
    exec(code, self.globals_and_locals, self.globals_and_locals)
  File "<string>", line 2, in <module>
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_importer.py", line 442, in __call__
    return self.current(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_importer.py", line 232, in _import
    new_spec.loader.exec_module(new_mod)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "c:\Users\kubak\source\repos\tesmporal_test\temporal_test.py", line 6, in <module>
    import uvicorn
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_importer.py", line 442, in __call__
    return self.current(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_importer.py", line 234, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1466, in __import__
  File "<frozen importlib._bootstrap>", line 1387, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1360, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1331, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\uvicorn\__init__.py", line 2, in <module>
    from uvicorn.main import Server, main, run
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_importer.py", line 442, in __call__
    return self.current(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_importer.py", line 234, in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1466, in __import__
  File "<frozen importlib._bootstrap>", line 1387, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1360, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1331, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\uvicorn\main.py", line 85, in <module>
    type=click.Path(exists=True),
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\click\types.py", line 830, in __init__
    self.name = _("path")
                ^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_restrictions.py", line 1024, in __call__
    state.assert_child_not_restricted("__call__")
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\workflow_sandbox\_restrictions.py", line 838, 
in assert_child_not_restricted
    raise RestrictedWorkflowAccessError(
temporalio.worker.workflow_sandbox._restrictions.RestrictedWorkflowAccessError: Cannot access gettext.gettext.__call__ from inside a workflow. If this is code from a module not used in a workflow or known to only be used deterministically from a workflow, mark the import as pass through.  

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "c:\Users\kubak\source\repos\tesmporal_test\temporal_test.py", line 44, in <module>
    run_worker()
  File "c:\Users\kubak\source\repos\tesmporal_test\temporal_test.py", line 41, in run_worker
    asyncio.run(worker_coroutine())
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\asyncio\runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\asyncio\base_events.py", line 685, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\asyncio\base_events.py", line 685, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\asyncio\base_events.py", line 685, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
    return future.result()
           ^^^^^^^^^^^^^^^
  File "c:\Users\kubak\source\repos\tesmporal_test\temporal_test.py", line 21, in worker_coroutine
    worker = Worker(
           ^^^^^^^^^^^^^^^
  File "c:\Users\kubak\source\repos\tesmporal_test\temporal_test.py", line 21, in worker_coroutine
    worker = Worker(
  File "c:\Users\kubak\source\repos\tesmporal_test\temporal_test.py", line 21, in worker_coroutine
    worker = Worker(
    worker = Worker(
             ^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\_worker.py", line 309, in __init__
    self._workflow_worker = _WorkflowWorker(
                            ^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\_workflow.py", line 141, in __init__
    raise RuntimeError(f"Failed validating workflow {defn.name}") from err
RuntimeError: Failed validating workflow WorkflowReturnUserId
             ^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\_worker.py", line 309, in __init__
    self._workflow_worker = _WorkflowWorker(
                            ^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\_workflow.py", line 141, in __init__
    raise RuntimeError(f"Failed validating workflow {defn.name}") from err
RuntimeError: Failed validating workflow WorkflowReturnUserId

    self._workflow_worker = _WorkflowWorker(
                            ^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\_workflow.py", line 141, in __init__
    raise RuntimeError(f"Failed validating workflow {defn.name}") from err
RuntimeError: Failed validating workflow WorkflowReturnUserId
                            ^^^^^^^^^^^^^^^^
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\_workflow.py", line 141, in __init__
    raise RuntimeError(f"Failed validating workflow {defn.name}") from err
RuntimeError: Failed validating workflow WorkflowReturnUserId
  File "C:\Users\kubak\AppData\Local\Programs\Python\Python312\Lib\site-packages\temporalio\worker\_workflow.py", line 141, in __init__
    raise RuntimeError(f"Failed validating workflow {defn.name}") from err
RuntimeError: Failed validating workflow WorkflowReturnUserId
```

### Environment/Versions

- OS and processor: x86 Windows
- Temporal Version: 1.10.0


---

<a id="826"></a>

### #826: [Bug] test_unfinished_handler_on_workflow_termination fails in some situations with time skipping and update

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/826 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-04-10 16:53:11.000 UTC (9 months ago) |
| **Updated** | 2025-04-10 16:54:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

As part of #644 we were trying to unskip these tests. But they still fail.

Specifically, when running in Python 3.13 (and maybe others) with time skipping test server, all permutations of `test_unfinished_handler_on_workflow_termination` that use `handler_type` `update`, `handler_waiting` `no-wait-all-handlers-finish`, and `workflow_termination_type` `continue-as-new` fail with "workflow execution already completed".

To replicate, run:

    poe test -s --log-cli-level=DEBUG --workflow-environment time-skipping -k test_unfinished_handler_on_workflow_termination

To see the failure. Leaving tests skipped for now.


---

<a id="820"></a>

### #820: Apply application failure logging and metrics behaviour according to ApplicationErrorCategory

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/820 |
| **State** | CLOSED |
| **Author** | THardy98 (Thomas Hardy) |
| **Created** | 2025-04-09 18:27:09.000 UTC (9 months ago) |
| **Updated** | 2025-05-15 23:09:29.000 UTC |
| **Closed** | 2025-05-15 23:09:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See corresponding features issue:
https://github.com/temporalio/features/issues/614

Will need to wait for core:
https://github.com/temporalio/sdk-core/issues/897

Core-based SDKs only need to apply corresponding logging behaviour, core handles metrics.



---

<a id="812"></a>

### #812: Heartbeating activities should be interrupted when the activities are paused.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/812 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2025-04-01 23:43:10.000 UTC (9 months ago) |
| **Updated** | 2025-05-29 13:31:02.000 UTC |
| **Closed** | 2025-05-29 13:31:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | THardy98 |
| **Milestone** | None |

#### Description

See https://github.com/temporalio/features/issues/602


---

<a id="810"></a>

### #810: [Feature Request] Implement proper behavior for `cancelled`, `uncancel`, and `cancelling` of activities and child workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/810 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-04-01 13:12:44.000 UTC (9 months ago) |
| **Updated** | 2025-04-01 13:12:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Implement proper behavior for `cancelled`, `uncancel`, and `cancelling` of activities and child workflows. The behavior probably looks like:

* `cancelled` - return true once it comes back as canceled
* `uncancel` - raise an (non-Temporal) exception saying that uncancel is not supported
* `cancelling` - probably just works already


---

<a id="798"></a>

### #798: [Feature Request] Consider TimeoutError as a workflow failure exception

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/798 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-03-25 18:21:50.000 UTC (9 months ago) |
| **Updated** | 2025-04-10 19:39:07.000 UTC |
| **Closed** | 2025-04-10 19:39:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Today if a `TimeoutError` occurs (e.g. from `wait_condition` or `asyncio.wait_for`) it will fail the task. We should allow `TimeoutError` to be considered a standard workflow failure exception. We consider this a backwards compatible change because turning what would have been task failures to no longer be task failures is considered safe.


---

<a id="794"></a>

### #794: [Feature Request] Make option for OTel workflow spans even if client span not present

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/794 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-03-25 14:10:12.000 UTC (9 months ago) |
| **Updated** | 2025-04-10 18:25:40.000 UTC |
| **Closed** | 2025-04-10 18:25:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Today, if the client-created span is not present and on the headers, we don't create any workflow spans because they may be orphaned (especially on replay). But some users starting from schedules or CLI want the workflow spans even if they may be orphaned, so we should allow this opt-in on the tracing interceptor.


---

<a id="793"></a>

### #793: [Feature Request] Support New Worker Versioning API

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/793 |
| **State** | CLOSED |
| **Author** | antlai-temporal (Antonio Lain) |
| **Created** | 2025-03-24 18:07:05.000 UTC (9 months ago) |
| **Updated** | 2025-04-16 22:30:54.000 UTC |
| **Closed** | 2025-04-16 22:30:54.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.
We need to add annotations to workflows to support the new versioning APIs. Refer to temporalio/features#548 for the Go equivalents.

### Describe the solution you'd like

There are some designs for all SDK languages (including Python) here: https://docs.google.com/document/d/1QGacjnlFwrfFtAvv9besx6sEdr36kJ3zQ9J7bGLRwk4/edit?usp=sharing





---

<a id="787"></a>

### #787: [Feature Request] Add summary to Nexus operation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/787 |
| **State** | CLOSED |
| **Author** | Quinn-With-Two-Ns (Quinn Klassen) |
| **Created** | 2025-03-11 18:40:55.000 UTC (10 months ago) |
| **Updated** | 2025-11-24 20:04:07.000 UTC |
| **Closed** | 2025-11-24 20:04:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

https://github.com/temporalio/features/issues/610


---

<a id="784"></a>

### #784: [Bug] Avoid losing worker slot on error while processing cache eviction

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/784 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2025-03-06 20:23:06.000 UTC (10 months ago) |
| **Updated** | 2025-04-04 19:27:45.000 UTC |
| **Closed** | 2025-04-04 19:27:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Improve the situation described here:

https://github.com/temporalio/sdk-python/blob/49ca10e413ba67e1adfeed3b5577cb4f5b007e54/temporalio/worker/_workflow.py#L270-L277


---

<a id="774"></a>

### #774: [Feature Request] Expose root workflow execution

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/774 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-02-25 14:17:47.000 UTC (10 months ago) |
| **Updated** | 2025-03-31 19:54:32.000 UTC |
| **Closed** | 2025-03-31 19:54:32.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | THardy98 |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/605, but needs to wait on https://github.com/temporalio/sdk-core/issues/881


---

<a id="773"></a>

### #773: [Feature Request] Built-in query responses should use "RawValue"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/773 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-02-21 20:34:19.000 UTC (10 months ago) |
| **Updated** | 2025-02-21 20:39:54.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/604. Make sure built-in queries return a raw value


---

<a id="772"></a>

### #772: [Bug] Investigate unused bind_f function

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/772 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2025-02-21 14:20:49.000 UTC (10 months ago) |
| **Updated** | 2025-02-21 14:20:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Investigate purpose of unused `bind_f` function. See https://github.com/temporalio/sdk-python/pull/771/files/a4803fc18aa4c93ccc1d00709f759c6143d8f185#r1960041327


---

<a id="764"></a>

### #764: Expose Workflow cancel cause/reason

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/764 |
| **State** | OPEN |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2025-02-11 20:44:19.000 UTC (11 months ago) |
| **Updated** | 2025-02-11 20:44:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

WorkflowExecutionCancelRequestedEventAttributes has a cause field that should be exposed when workflows receive cancels, and we should additionally expose this as an input when users are issuing cancels from clients or commands


---

<a id="763"></a>

### #763: [Feature Request] Remove experimental warning from a few no-longer-experimental APIs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/763 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-02-07 13:28:09.000 UTC (11 months ago) |
| **Updated** | 2025-02-13 17:36:30.000 UTC |
| **Closed** | 2025-02-13 17:36:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

The following items should no longer be considered experimental:

* Local activities
* Eager workflow start
* Eager activity execution
* Runtime module (top of module says experimental)
* Runtime logging and log-forwarding config
* Workflow update decorator


---

<a id="762"></a>

### #762: [Feature Request] Enforce no-mutable-defaults lint rule

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/762 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2025-02-06 23:46:03.000 UTC (11 months ago) |
| **Updated** | 2025-02-06 23:46:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

https://google.github.io/styleguide/pyguide.html#2124-decision
https://docs.astral.sh/ruff/rules/mutable-argument-default/


---

<a id="761"></a>

### #761: [Feature Request] Publish musl wheel

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/761 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-02-05 22:37:16.000 UTC (11 months ago) |
| **Updated** | 2025-02-05 22:37:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/594


---

<a id="760"></a>

### #760: [Feature Request] Support Priority Annotations

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/760 |
| **State** | CLOSED |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2025-02-03 18:21:41.000 UTC (11 months ago) |
| **Updated** | 2025-03-28 20:58:34.000 UTC |
| **Closed** | 2025-03-28 20:58:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Add support for attaching priority to workflows/activities

**Describe the solution you'd like**
See https://github.com/temporalio/features/issues/593 for details



---

<a id="754"></a>

### #754: [Feature Request] Add explicit memoization support to the `patched()` API

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/754 |
| **State** | OPEN |
| **Author** | mjameswh (James Watkins-Harvey) |
| **Created** | 2025-01-29 23:44:27.000 UTC (11 months ago) |
| **Updated** | 2025-01-29 23:44:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

In short:

- Add a `memoized` argument to the `patched` and `deprecate_patch` APIs.

- Keep backward compatibility on those functions for non-specifying the `memoized` argument, but deprecate those signatures. Make sure documentation clearly indicate how existing calls may safely get converted.

- Delegate handling of the `patched` logic to Core.

See temporalio/features#591 for details.


---

<a id="748"></a>

### #748: [Feature Request] Allow customization of dev server UI port

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/748 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-01-24 13:26:15.000 UTC (11 months ago) |
| **Updated** | 2025-01-24 13:26:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/588


---

<a id="728"></a>

### #728: [Feature Request] Update Ruff target version to 3.9

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/728 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-01-14 19:38:21.000 UTC (11 months ago) |
| **Updated** | 2025-01-24 21:13:58.000 UTC |
| **Closed** | 2025-01-24 21:13:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Per comment at https://github.com/temporalio/sdk-python/pull/694#issuecomment-2590851299, we forgot to do this on 3.8 upgrade


---

<a id="722"></a>

### #722: [Feature Request] Stack trace on deadlock exception improvements

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/722 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-01-08 22:38:17.000 UTC (1 years ago) |
| **Updated** | 2025-01-08 22:38:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

https://github.com/temporalio/sdk-python/pull/626 added a stack traces from the deadlocking thread, but there are three improvements we need to make to make sure this is working properly:

1. Make this also work for users that have opted out of sandboxing (if, as suspected reading the PR, it doesn't actually work already)
2. Add a test to confirm the deadlock stack trace represents (couldn't see such a test in the PR)
3. We append `(no frames available)` or `(failed getting frames: {err})` when we couldn't capture the threads trace, but we should also add something like `(no thread ID available)` if the earlier condition occurs where for some reason we don't have a thread ID at all.


---

<a id="720"></a>

### #720: [Feature Request] Access to workflow instance from context

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/720 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-01-07 13:49:41.000 UTC (1 years ago) |
| **Updated** | 2025-01-23 23:02:51.000 UTC |
| **Closed** | 2025-01-23 23:02:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

The created instance of the workflow class should be made available as `workflow.instance()`. See https://github.com/temporalio/features/issues/572.


---

<a id="708"></a>

### #708: [Bug] Stop skipping update tests under the Java test server

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/708 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-12-19 13:21:41.000 UTC (1 years ago) |
| **Updated** | 2025-04-10 17:34:20.000 UTC |
| **Closed** | 2025-04-10 17:34:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Either these pass, in which case they should not be skipped, or they don't, in which case the test server must be fixed.


---

<a id="704"></a>

### #704: [Bug] Run ID should be set in update handle

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/704 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-12-16 18:27:50.000 UTC (1 years ago) |
| **Updated** | 2024-12-19 18:06:02.000 UTC |
| **Closed** | 2024-12-19 18:06:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

When receiving an update response, the Python SDK does not set in the `UpdateHandle` the run ID of the run that the update landed on:

https://github.com/temporalio/sdk-python/blob/main/temporalio/client.py#L5339

In contrast, Typescript and Go and presumably others, all set this:

https://github.com/temporalio/sdk-typescript/blob/main/packages/client/src/workflow-client.ts#L852-L853

https://github.com/temporalio/sdk-go/blob/master/internal/internal_workflow_client.go#L2356


One consequence is that the `PollWorkflowUpdate` requests issued by the Python update handle do not contain a run ID (and so will land on any run), whereas for other SDKs those requests are constrained to the run that the update landed on.

It's possible that there are bugs here. For example, it seems that in Python a poll request might be able to target a different update with the same update ID, that exists in a post-CAN or post-reset run.


---

<a id="689"></a>

### #689: [Feature Request] Do runtime check to ensure update validator has the same parameter signature as update it validates

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/689 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-11-13 18:55:34.000 UTC (1y 1m ago) |
| **Updated** | 2025-01-18 22:43:43.000 UTC |
| **Closed** | 2025-01-18 22:43:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

We need to make sure at runtime the parameter portion of the signature of the validator matches the parameter portion of the signature of the update method it is validating.


---

<a id="685"></a>

### #685: [Bug] Issue when failing to build activity failure

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/685 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-11-06 16:33:05.000 UTC (1y 2m ago) |
| **Updated** | 2025-01-21 16:08:57.000 UTC |
| **Closed** | 2025-01-21 16:08:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

When activity fails, but the failure can't be converted for whatever reason, we build a general failure at https://github.com/temporalio/sdk-python/blob/723d234865f9e6674b44a2071521f8e99b0eb2b4/temporalio/worker/_activity.py#L507-L510. But we aren't properly building this by putting application failure info in it. We fixed the same issue in .NET recently: https://github.com/temporalio/sdk-dotnet/pull/353. We need to do the same here.


---

<a id="682"></a>

### #682: [Bug] Update requests should set first_execution_run_id

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/682 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-11-01 13:09:40.000 UTC (1y 2m ago) |
| **Updated** | 2024-12-19 16:42:37.000 UTC |
| **Closed** | 2024-12-19 16:42:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Update requests do not send `first_execution_run_id`: https://github.com/temporalio/sdk-python/blob/0b327b04a95fc7954ed2c9e4e1a6c5de47c51ebd/temporalio/client.py#L5252-L5285

However, they should, and other SDKs do.

Also, is it correct that we don't use the run ID from the response when constructing the returned update handle?


---

<a id="678"></a>

### #678: [Bug] Logic bug for create_schedule + backfill w/out trigger immediately

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/678 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-10-29 16:18:30.000 UTC (1y 2m ago) |
| **Updated** | 2024-11-22 17:45:41.000 UTC |
| **Closed** | 2024-11-22 17:45:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Looking at code while developing Ruby SDK, it seems at https://github.com/temporalio/sdk-python/blob/b16215104069f798b79592679d8a16dd3d702883/temporalio/client.py#L5499-L5507 we are always setting trigger immediately even it user set it to false but provided backfills. .NET properly does not set this.


---

<a id="675"></a>

### #675: [Feature Request] Auto-skip time in time-skipping testing environment when waiting on update result

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/675 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-10-21 16:31:03.000 UTC (1y 2m ago) |
| **Updated** | 2024-10-21 16:31:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

We auto-skip time when waiting on workflow result, we need to do the same when waiting on update result. See https://github.com/temporalio/features/issues/551.



---

<a id="670"></a>

### #670: [Feature Request] Support user metadata

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/670 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-10-15 12:27:11.000 UTC (1y 2m ago) |
| **Updated** | 2024-12-16 17:30:18.000 UTC |
| **Closed** | 2024-12-16 17:30:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/486. Specifically, after https://github.com/temporalio/sdk-core/issues/830:

* Accept optional "static summary " and "static details" on all ways to start a workflow (start, signal with start, schedule, and child workflow) and populate user metadata with them
* Accept optional "summary" when creating timers and populate user metadata with it
* Accept optional "summary" when creating activities and populate user metadata with it
* Handle `__temporal_workflow_metadata` query (see features issue and https://github.com/temporalio/sdk-go/pull/1597)
* Mark experimental


---

<a id="669"></a>

### #669: [Feature Request] Support / provide guidance on using OpenTelemetry logging + metrics SDKs with process-pool workers 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/669 |
| **State** | OPEN |
| **Author** | gregbrowndev (Greg Brown) |
| **Created** | 2024-10-14 20:46:25.000 UTC (1y 2m ago) |
| **Updated** | 2024-10-14 20:46:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

[Slack discussion](https://temporalio.slack.com/archives/CTT84RS0P/p1728929473099159)

Hi,

I've been rolling out an OpenTelemetry-based observability solution for my Temporal app. The reason for using OTel is partly due to the Temporal Python SDK already using OTel for traces and metrics (in the SDK), so I want to adopt those SDKs for custom metrics and logging.

Everything works great in async activities. I've been able to use OTel tooling for logging and tracing (using the `TracingInterceptor` as [seen in this example](https://github.com/temporalio/samples-python/blob/main/open_telemetry/worker.py)), I can see traces with their correlated logs for each activity in my backend (Grafana, Loki, Tempo). The Temporal SDK provides `activity.metric_meter()`, which I've used to add custom metrics to async activities.

However, I'm having several issues with sync activities running on process-pool-based workers (I'm happy to split them into separate issues):

The `trace_id` and `span_id` injected into the logs are incorrect for all except the first activity that runs on that worker. It seems that the first activity's IDs are injected into all activities that follow it.
- Temporal SDK's `activity.metric_meter()` doesn't work in process-pools, which is clearly a known but probably related issue. I planned to set up a separate OTel `MetricProvider` to support custom metrics in my sync activities. However, this looks to have the same limitations (I haven't tried everything exhaustively yet).

Note: I suspect initialising the `MeterProvider` for each process/activity will be a lot more simple because it isn't attached to a global root logger.

While these issues are likely inherently within the OTel SDKs, the same issues are also known to be true for the TracingProvider ([ref](https://opentelemetry-python.readthedocs.io/en/latest/examples/fork-process-model/README.html)), yet Temporal managed to get that to work. 

Please provide guidance on setting up OTel logging and custom metrics in process-pool-based workers or support them natively like you do OTel tracing.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

### Describe the solution you'd like

Support for the remaining OpenTelemetry SDKs (metric and logging) natively in both async, thread-pool, and process-pool workers. 

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->




---

<a id="657"></a>

### #657: [Bug] Cannot replace worker client if default Runtime is used

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/657 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-09-30 18:11:41.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-30 18:11:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Our check that the runtime is the same on the replaced client as is on the worker does not work if the runtime is not set on the client (i.e. lazily uses the default). We need to either more eagerly set the runtime default on the client config for the service, or use a different way to obtain the runtime of the bridge service client when comparing.

Replication:

```python
import asyncio
from dataclasses import dataclass
from datetime import timedelta

from temporalio import activity, workflow
from temporalio.client import Client
from temporalio.worker import Worker


# While we could use multiple parameters in the activity, Temporal strongly
# encourages using a single dataclass instead which can have fields added to it
# in a backwards-compatible way.
@dataclass
class ComposeGreetingInput:
    greeting: str
    name: str


# Basic activity that logs and does string concatenation
@activity.defn
async def compose_greeting(input: ComposeGreetingInput) -> str:
    activity.logger.info("Running activity with parameter %s" % input)
    return f"{input.greeting}, {input.name}!"


# Basic workflow that logs and invokes an activity
@workflow.defn
class GreetingWorkflow:
    @workflow.run
    async def run(self, name: str) -> str:
        workflow.logger.info("Running workflow with parameter %s" % name)
        return await workflow.execute_activity(
            compose_greeting,
            ComposeGreetingInput("Hello", name),
            start_to_close_timeout=timedelta(seconds=10),
        )


async def main():
    # Uncomment the lines below to see logging output
    # import logging
    # logging.basicConfig(level=logging.INFO)

    # Start client
    client = await Client.connect("localhost:7233")

    client_2 = await Client.connect("localhost:7233")


    # Run a worker for the workflow
    async with Worker(
        client,
        task_queue="hello-activity-task-queue",
        workflows=[GreetingWorkflow],
        activities=[compose_greeting],
    ) as worker:

        worker.client = client_2

        # While the worker is running, use the client to run the workflow and
        # print out its result. Note, in many production setups, the client
        # would be in a completely separate process from the worker.
        result = await client.execute_workflow(
            GreetingWorkflow.run,
            "World",
            id="hello-activity-workflow-id",
            task_queue="hello-activity-task-queue",
        )
        print(f"Result: {result}")


if __name__ == "__main__":
    asyncio.run(main())
```

In the meantime, simply using `default=temporalio.runtime.Runtime.default()` in the client `connect` calls will work around the issue.


---

<a id="648"></a>

### #648: Show update handler and ID in logging context

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/648 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-09-24 16:37:38.000 UTC (1y 3m ago) |
| **Updated** | 2024-11-06 16:17:31.000 UTC |
| **Closed** | 2024-11-06 16:17:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See https://github.com/temporalio/features/issues/466 for more details


---

<a id="642"></a>

### #642: [Feature Request] Fix quinn-proto Vulnerability by Updating Rust Package to Version 0.11.7 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/642 |
| **State** | CLOSED |
| **Author** | rbtbar (Robert Bar) |
| **Created** | 2024-09-12 09:14:53.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-25 16:29:55.000 UTC |
| **Closed** | 2024-09-25 16:29:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Security scans have identified a vulnerability in one of the Rust dependencies. The current version of the package is affected by the following advisories:

[GHSA-vr26-jcq5-fjj8](https://github.com/advisories/GHSA-vr26-jcq5-fjj8)
[RUSTSEC-2024-0373](https://rustsec.org/advisories/RUSTSEC-2024-0373.html)

If possible, package should be updated to version 0.11.7, which contains the necessary patches.


---

<a id="637"></a>

### #637: [Bug] Windows 3.12.5 builds are broken

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/637 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-09-06 13:17:38.000 UTC (1y 4m ago) |
| **Updated** | 2024-09-06 13:17:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See e.g. https://github.com/temporalio/sdk-python/actions/runs/10721545848/job/29731379405

```
error: linking with `link.exe` failed: exit code: 1181
  |
  = note: "C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.41.34120\\bin\\HostX64\\x64\\link.exe"
  
  ...
  
= note: LINK : fatal error LNK1181: cannot open input file 'python3.lib'
```


---

<a id="636"></a>

### #636: [Feature Request] Add type-level tests

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/636 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-09-05 15:11:15.000 UTC (1y 4m ago) |
| **Updated** | 2024-09-05 15:11:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

We'd like a good way to write type-level tests to confirm that mypy and pyright are enforcing what we want them to enforce and to prevent regressions.


---

<a id="613"></a>

### #613: [Feature Request] Investigate adding allure to GitHub Actions CI workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/613 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-08-19 15:18:00.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-19 15:18:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

https://allurereport.org/
https://allurereport.org/docs/pytest/
https://allurereport.org/docs/integrations-github/

The feature providing correlation of results across historical runs sounds particularly interesting, if it works well.

I was able to generate a nice-looking report locally via

```
brew install allure
pip install allure-pytest
python -m pytest --alluredir allure-results tests/worker/test_workflow.py
allure serve allure-results
```


---

<a id="611"></a>

### #611: [Feature Request] Better error message when trying to upsert a datetime search attribute with non-tz-aware datetime.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/611 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-08-16 03:34:20.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-16 03:34:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See https://github.com/temporalio/sdk-python/issues/572

The SDK should raise an exception with a helpful error message if the datetime object is invalid for use as a datetime search attribute (e.g. lacks a timezone), rather than waiting for the server to fail the WFT Completion.


---

<a id="610"></a>

### #610: [Feature Request] Support a `limit` parameter in `replay_workflows` and `workflow_replay_iterator`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/610 |
| **State** | CLOSED |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2024-08-15 21:57:44.000 UTC (1y 4m ago) |
| **Updated** | 2024-12-06 20:02:43.000 UTC |
| **Closed** | 2024-12-06 20:02:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Python stdlib is unfortunately very lacking when it comes to ways to work with `AsyncIterator`s. Add this to the fact that our API doesn't support a LIMIT clause, but only page sizes, we have no good way to stop these functions early.

Support that with a `limit` parameter.


---

<a id="608"></a>

### #608: [Feature Request] Make payload conversion errors on workflow input more discriminable

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/608 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-08-14 15:34:32.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-14 15:34:32.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

On invalid workflow input, if a bad value is called, the task fails with a runtime error. This should be a better error type that can be set on `workflow_failure_exception_types`. Consider reusing this exception in any other place that has payload conversion errors provided we are comfortable with the compatibility break of changing an error type.


---

<a id="607"></a>

### #607: [Bug] Prevent hot-reload of workflow definitions from disk

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/607 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-08-14 14:40:20.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-14 14:40:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

A Python worker reloads the workflow definition from disk every time a new workflow run is started, i.e. without requiring the worker process to be shutdown and restarted. So for example, a long-running workflow that CANs will "hot reload" like this on every CAN, and a non-sticky worker will hot-reload on every WFT.

This is a side-effect of implementation (worker sandboxing involves disabling import cache which results in re-reading from disk) rather than being desired behavior, and isn't appropriate in production worker deployments since users are likely to expect that during deployments they can overwrite files without any modifications to running processes until process restart.

So, let's find a good way to prevent the behavior. For example, one possibility might involve permitting the re-read-from-disk to continue, but having the worker copy the relevant files somewhere private, and importing workflows from there.


---

<a id="606"></a>

### #606: [Feature Request] Consider aligning activation job application with TS changes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/606 |
| **State** | OPEN |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2024-08-13 21:33:26.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-13 21:33:26.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Python currently applies jobs in a few different batches: https://github.com/temporalio/sdk-python/blob/main/temporalio/worker/_workflow_instance.py#L325

In TS, we changed this to only do one batch, and then run the event loop: https://github.com/temporalio/sdk-typescript/pull/1488

This model is a bit more intuitive and also fits in with what I have come to realize is really the most appropriate model for Core based languages, where everything that happens in a workflow task is "applied to state" synchronously, and then the way routines proceed to run is largely up to the language runtime (or our deterministic version thereof). Ideally, all Core langs (really, all of them, but we won't go back to change that) follow that model - state is updated synchronously, then stuff runs until we're blocked.

There is a really nice test [here](https://github.com/temporalio/sdk-typescript/pull/1488/files#diff-edcae2d23017bc07e3a859b68e980d151947323c04128f2bc08147bda4a1d7deR2512) @mjameswh added that I think all async languages should have some form of that helps us understand exactly how routines will proceed.

I'm not sure if making this change will actually have consequences to the _realized_ python behavior. In any case it should probably be protected with a flag if we make it.


---

<a id="600"></a>

### #600: Workflow-init support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/600 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-08-02 04:06:12.000 UTC (1y 5m ago) |
| **Updated** | 2024-09-24 16:32:58.000 UTC |
| **Closed** | 2024-09-24 16:32:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

We should have a method that's guaranteed to initialize before signals and updates come in so handlers can use initialized values.


---

<a id="594"></a>

### #594: [Feature Request] Support schedule search attribute update

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/594 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-07-24 19:24:52.000 UTC (1y 5m ago) |
| **Updated** | 2025-01-27 18:23:55.000 UTC |
| **Closed** | 2025-01-27 18:23:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/512 and the Go implementation at https://github.com/temporalio/sdk-go/issues/1561



---

<a id="585"></a>

### #585: [Bug] Test flake, may be sandbox import issue

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/585 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-07-17 17:40:48.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-17 17:40:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Sometimes on Python 3.8, there is this test failure:

```
tests/worker/test_replayer.py::test_replayer_workflow_incomplete - RuntimeError: Failed validating workflow SayHelloWorkflow
```

Here is the full logged exception:

```
______________________ test_replayer_workflow_incomplete _______________________

>   import asyncio

tests/worker/test_replayer.py:1: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
temporalio/worker/workflow_sandbox/_importer.py:441: in __call__
    return self.current(*args, **kwargs)
temporalio/worker/workflow_sandbox/_importer.py:234: in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)
<frozen importlib._bootstrap>:1093: in __import__
    ???
<frozen importlib._bootstrap>:1014: in _gcd_import
    ???
<frozen importlib._bootstrap>:991: in _find_and_load
    ???
<frozen importlib._bootstrap>:961: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:219: in _call_with_frames_removed
    ???
<frozen importlib._bootstrap>:1014: in _gcd_import
    ???
<frozen importlib._bootstrap>:991: in _find_and_load
    ???
<frozen importlib._bootstrap>:975: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:671: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:843: in exec_module
    ???
<frozen importlib._bootstrap>:219: in _call_with_frames_removed
    ???
../../../.cache/pypoetry/virtualenvs/temporalio-s8Ez09FR-py3.8/lib/python3.8/site-packages/_pytest/assertion/__init__.py:9: in <module>
    from _pytest.assertion import rewrite
temporalio/worker/workflow_sandbox/_importer.py:441: in __call__
    return self.current(*args, **kwargs)
temporalio/worker/workflow_sandbox/_importer.py:234: in _import
    mod = importlib.__import__(name, globals, locals, fromlist, level)
<frozen importlib._bootstrap>:1113: in __import__
    ???
<frozen importlib._bootstrap>:1042: in _handle_fromlist
    ???
<frozen importlib._bootstrap>:219: in _call_with_frames_removed
    ???
<frozen importlib._bootstrap>:1014: in _gcd_import
    ???
<frozen importlib._bootstrap>:991: in _find_and_load
    ???
<frozen importlib._bootstrap>:975: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:671: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:843: in exec_module
    ???
<frozen importlib._bootstrap>:219: in _call_with_frames_removed
    ???
../../../.cache/pypoetry/virtualenvs/temporalio-s8Ez09FR-py3.8/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:32: in <module>
    from _pytest._io.saferepr import DEFAULT_REPR_MAX_SIZE
        self._on_eviction_hook = on_eviction_hook
        self._disable_safe_eviction = disable_safe_eviction
        self._throw_after_activation: Optional[Exception] = None
    
        # If there's a debug mode or a truthy TEMPORAL_DEBUG env var, disable
        # deadlock detection, otherwise set to 2 seconds
        self._deadlock_timeout_seconds = (
            None if debug_mode or os.environ.get("TEMPORAL_DEBUG") else 2
        )
    
        # Keep track of workflows that could not be evicted
        self._could_not_evict_count = 0
    
        # Set the worker-level failure exception types into the runner
        workflow_runner.set_worker_level_failure_exception_types(
            workflow_failure_exception_types
        )
    
        # Validate and build workflow dict
        self._workflows: Dict[str, temporalio.workflow._Definition] = {}
        self._dynamic_workflow: Optional[temporalio.workflow._Definition] = None
        for workflow in workflows:
            defn = temporalio.workflow._Definition.must_from_class(workflow)
            # Confirm name unique
            if defn.name in self._workflows:
                raise ValueError(f"More than one workflow named {defn.name}")
            # Prepare the workflow with the runner (this will error in the
            # sandbox if an import fails somehow)
            try:
                if defn.sandboxed:
                    workflow_runner.prepare_workflow(defn)
                else:
                    unsandboxed_workflow_runner.prepare_workflow(defn)
            except Exception as err:
>               raise RuntimeError(f"Failed validating workflow {defn.name}") from err
E               RuntimeError: Failed validating workflow SayHelloWorkflow

temporalio/worker/_workflow.py:130: RuntimeError
```

I suspect this is the previously-known issue in older Python with reimporting (https://github.com/python/cpython/issues/91351). Just opening issue for tracking.


---

<a id="584"></a>

### #584: Unfinished handlers: continue-as-new and dynamic handlers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/584 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-07-17 16:54:44.000 UTC (1y 5m ago) |
| **Updated** | 2024-08-19 18:59:37.000 UTC |
| **Closed** | 2024-08-19 18:59:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

https://github.com/temporalio/sdk-python/pull/556 omitted to
- address dynamic handlers
- test that warnings are emitted on continue-as-new



---

<a id="577"></a>

### #577: [Bug] cibuildwheel using older manylinux 2014 images using out of date Centos 7 repo

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/577 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-07-09 21:24:07.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-19 12:49:05.000 UTC |
| **Closed** | 2024-07-19 12:49:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

The build binaries job (not run on PR by default) is using `quay.io/pypa/manylinux2014_x86_64:2024-03-10-4935fcc` image via `cibuildwheel` which doesn't have updated manylinux 2014 image with proper repos, so it gets:

```
   Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=container error was
  14: curl#6 - "Could not resolve host: mirrorlist.centos.org; Unknown error"
```

We probably just need to update cibuildwheel. Can make that CI job run during draft of PR to test fix before turning it back off again and marking PR ready for review.


---

<a id="575"></a>

### #575: [Feature Request] Workflow cancellation should cancel handler executions

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/575 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-07-09 18:21:34.000 UTC (1y 6m ago) |
| **Updated** | 2025-01-07 20:38:24.000 UTC |
| **Closed** | 2025-01-07 20:38:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently, workflow cancellation does not result in cancellation of in-progress signal/update handler executions.

For example, consider the case of update and see the sample below: the client waiting on the update gets an exception, but the exception is `workflow execution already completed`. This ticket calls for that exception to inform the client that the update was canceled due to cancellation of the workflow.

Doing this for signal handlers would be backwards-incompatible and hence will need to use an SDK workflow logic flag.

```python
@workflow.defn
class UpdateCancellationWorkflow:
    def __init__(self) -> None:
        self.non_terminating_operation_has_started = False

    @workflow.run
    async def run(self) -> str:
        await workflow.wait_condition(lambda: False)
        return "unreachable"

    @workflow.update
    async def wait_until_non_terminating_operation_has_started(self) -> None:
        await workflow.wait_condition(
            lambda: self.non_terminating_operation_has_started
        )

    @workflow.update
    async def non_terminating_operation(self) -> str:
        self.non_terminating_operation_has_started = True
        await workflow.wait_condition(lambda: False)
        return "unreachable"


async def test_update_cancellation(client: Client):
    async with new_worker(client, UpdateCancellationWorkflow) as worker:
        wf_handle = await client.start_workflow(
            UpdateCancellationWorkflow.run,
            id=str(uuid.uuid4()),
            task_queue=worker.task_queue,
        )
        # Asynchronously run an update that will never complete
        non_terminating_update_task = asyncio.create_task(
            wf_handle.execute_update(
                UpdateCancellationWorkflow.non_terminating_operation
            )
        )
        print("wait until handler started...")
        # Wait until we know the update handler has started executing
        await wf_handle.execute_update(
            UpdateCancellationWorkflow.wait_until_non_terminating_operation_has_started
        )
        print("cancel the workflow")
        await wf_handle.cancel()
        print("await non_terminating_update_task...")
        await non_terminating_update_task
```


---

<a id="573"></a>

### #573: [Feature Request] Support "query" when listing schedules

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/573 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-07-08 14:24:11.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-17 17:54:19.000 UTC |
| **Closed** | 2024-07-17 17:54:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Need to support the query option when listing schedules same as we do when listing workflows.



---

<a id="565"></a>

### #565: [Feature Request] Replace BuildJet ARM runner with GH runner

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/565 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-06-27 16:38:06.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-18 18:12:57.000 UTC |
| **Closed** | 2024-07-18 18:12:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Now that GH has ARM runners, might as well


---

<a id="558"></a>

### #558: [Feature Request] Accept search attributes in dev server startup

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/558 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-06-20 20:33:13.000 UTC (1y 6m ago) |
| **Updated** | 2025-02-07 13:59:29.000 UTC |
| **Closed** | 2025-02-07 13:59:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/494. With https://github.com/temporalio/cli/pull/593 deployed, users should be allowed to set search attributes in dev server option that translate to `--search-attribute` calls.



---

<a id="553"></a>

### #553: Testing

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/553 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-06-18 03:54:08.000 UTC (1y 6m ago) |
| **Updated** | 2024-06-19 15:18:40.000 UTC |
| **Closed** | 2024-06-19 15:18:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

testing


---

<a id="552"></a>

### #552: Testing

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/552 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-06-18 03:53:01.000 UTC (1y 6m ago) |
| **Updated** | 2024-06-19 15:18:44.000 UTC |
| **Closed** | 2024-06-19 15:18:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

testing


---

<a id="543"></a>

### #543: [Bug] Generating protos via docker and manually are causing different whitespace inside of multi-paragraph docs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/543 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-06-04 19:23:33.000 UTC (1y 7m ago) |
| **Updated** | 2024-06-04 19:23:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

When using the docker-based gen proto method, for whatever reason it is slightly different than the manual way outside of the container which causes CI diff check to break


---

<a id="542"></a>

### #542: Expose UpdateID in an update handler

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/542 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-06-04 16:17:06.000 UTC (1y 7m ago) |
| **Updated** | 2024-06-06 21:36:59.000 UTC |
| **Closed** | 2024-06-06 21:36:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

This will allow users to use it when they want a unique idempotent primary key for handlers


---

<a id="539"></a>

### #539: Provide methods to wait for all handlers to finish

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/539 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-06-04 04:39:17.000 UTC (1y 7m ago) |
| **Updated** | 2024-08-13 16:52:43.000 UTC |
| **Closed** | 2024-08-13 16:52:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Users want to easily await (or ignore) all their remaining handlers when they continue as new or exit the workflow.  Since we can't tell what they want, they should specify.


---

<a id="538"></a>

### #538: Warn or error when update handlers dangle across CAN or workflow exit

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/538 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-06-04 04:21:57.000 UTC (1y 7m ago) |
| **Updated** | 2024-06-26 17:57:03.000 UTC |
| **Closed** | 2024-06-26 17:57:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description


We can recommend the await-all-handlers or drop-all-handlers methods. TBD whether error or warning.




---

<a id="536"></a>

### #536: [Feature Request] Clarify exception on return type decoding failure

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/536 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-05-31 12:58:16.000 UTC (1y 7m ago) |
| **Updated** | 2024-05-31 12:58:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Today in child workflow for instance if the return type is not the expected, it says "Failed decoding arguments". It should say "Failed to decode return value on <child workflow>". Same for activities (and make sure it's clear what child/activity failed to _encode_ if needed too).


---

<a id="535"></a>

### #535: [Feature Request] Turn invalid asyncio call warnings into errors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/535 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-05-29 22:25:00.000 UTC (1y 7m ago) |
| **Updated** | 2024-05-29 22:25:01.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

As part of #533, we are warning if `asyncio.as_completed()` or `asyncio.wait()` are invoked in workflows because they are non-deterministic. After a release and a good period of time, we should fail when these are called the same way we do with other invalid workflow calls.


---

<a id="532"></a>

### #532: [Feature Request] Call Core worker.validate

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/532 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-05-28 12:10:57.000 UTC (1y 7m ago) |
| **Updated** | 2024-06-04 22:22:03.000 UTC |
| **Closed** | 2024-06-04 22:22:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

`validate` added in https://github.com/temporalio/sdk-core/pull/750, make sure to call it on worker start/run and if it errors, error to the user


---

<a id="528"></a>

### #528: [Feature Request] Do not set workflow completion until after all coroutines have settled in the task 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/528 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-05-17 19:30:22.000 UTC (1y 7m ago) |
| **Updated** | 2024-08-05 18:19:38.000 UTC |
| **Closed** | 2024-08-05 18:19:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/481


---

<a id="527"></a>

### #527: [Bug] Can crash SDK with non-halting workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/527 |
| **State** | CLOSED |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-05-14 16:26:29.000 UTC (1y 8m ago) |
| **Updated** | 2024-05-28 22:48:39.000 UTC |
| **Closed** | 2024-05-28 22:48:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

The workflow below crashes the SDK. The fix may be to take a copy of this set with `list(self._tasks)`, but I haven't investigated yet:

https://github.com/temporalio/sdk-python/blob/f96679b803803f607eb2586a63905d921347bc75/temporalio/worker/_workflow_instance.py#L1774

```
wait_until_positive
wait_until_negative
wait_until_positive
wait_until_negative
wait_until_positive
wait_until_negative
Failed running eviction job, not evicting
Traceback (most recent call last):
  File "/Users/dan/src/temporalio/samples-python/.venv/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 172, in _run_code
    exec(code, self.globals_and_locals, self.globals_and_locals)
  File "<string>", line 2, in <module>
  File "/Users/dan/src/temporalio/samples-python/.venv/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_in_sandbox.py", line 81, in activate
    return self.instance.activate(act)
  File "/Users/dan/src/temporalio/samples-python/.venv/lib/python3.10/site-packages/temporalio/worker/_workflow_instance.py", line 361, in activate
    + f"Stack traces below:\n\n{self._stack_trace()}"
  File "/Users/dan/src/temporalio/samples-python/.venv/lib/python3.10/site-packages/temporalio/worker/_workflow_instance.py", line 1774, in _stack_trace
    for task in self._tasks:
RuntimeError: Set changed size during iteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/dan/src/temporalio/samples-python/.venv/lib/python3.10/site-packages/temporalio/worker/_workflow.py", line 249, in _handle_activation
    completion = await asyncio.wait_for(
  File "/Users/dan/.pyenv/versions/3.10.13/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/Users/dan/.pyenv/versions/3.10.13/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/Users/dan/src/temporalio/samples-python/.venv/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 157, in activate
    self._run_code(
  File "/Users/dan/src/temporalio/samples-python/.venv/lib/python3.10/site-packages/temporalio/worker/workflow_sandbox/_runner.py", line 176, in _run_code
    del self.globals_and_locals[k]
KeyError: '__temporal_importer'
```


```python
import asyncio

from temporalio import common, workflow
from temporalio.client import Client
from temporalio.worker import Worker


@workflow.defn
class StackTraceBugRepro:

    def __init__(self) -> None:
        self.val = 1

    async def wait_until_positive(self):
        while True:
            print("wait_until_positive")
            await workflow.wait_condition(lambda: self.val > 0)
            self.val = -self.val

    async def wait_until_negative(self):
        while True:
            print("wait_until_negative")
            await workflow.wait_condition(lambda: self.val < 0)
            self.val = -self.val

    @workflow.run
    async def run(self):
        await asyncio.gather(self.wait_until_negative(), self.wait_until_positive())


async def main():
    client = await Client.connect("localhost:7233")
    async with Worker(
        client,
        task_queue="tq",
        workflows=[StackTraceBugRepro],
    ):
        await client.execute_workflow(
            StackTraceBugRepro.run,
            id="wid",
            task_queue="tq",
            id_reuse_policy=common.WorkflowIDReusePolicy.TERMINATE_IF_RUNNING,
        )


if __name__ == "__main__":
    asyncio.run(main())
```



---

<a id="525"></a>

### #525: [Bug] Returning given payload object in codec causes issues

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/525 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-05-09 21:35:34.000 UTC (1y 8m ago) |
| **Updated** | 2024-05-09 22:22:20.000 UTC |
| **Closed** | 2024-05-09 22:22:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

If a codec returns the same payload object it was given on decode, it can cause issues due to a bug with how we update the original payload


---

<a id="522"></a>

### #522: [Feature Request] Cloud Operations API Client

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/522 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-05-09 15:02:39.000 UTC (1y 8m ago) |
| **Updated** | 2024-07-10 17:07:57.000 UTC |
| **Closed** | 2024-07-10 17:07:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/features/issues/440 and https://github.com/temporalio/sdk-go/pull/1426. We need a separate cloud client using the cloud protos (they are separate today). The client should be marked experimental. This is dependent upon https://github.com/temporalio/sdk-core/issues/737.



---

<a id="514"></a>

### #514: [Feature Request] Make start_update users aware that it's synchronous w/ worker

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/514 |
| **State** | CLOSED |
| **Author** | drewhoskins-temporal (Drew Hoskins) |
| **Created** | 2024-04-25 01:34:06.000 UTC (1y 8m ago) |
| **Updated** | 2024-05-20 20:23:49.000 UTC |
| **Closed** | 2024-05-20 20:23:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.
See https://github.com/temporalio/features/issues/469 which I discovered using the python SDK.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

### Additional context

<!-- Add any other context or screenshots about the feature request here. -->



---

<a id="513"></a>

### #513: [Feature Request] Client replacement in worker

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/513 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-04-24 13:26:56.000 UTC (1y 8m ago) |
| **Updated** | 2024-05-03 22:16:03.000 UTC |
| **Closed** | 2024-05-03 22:16:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Need to be able to replace the worker client with a new client that may be connected with different options (e.g. a new TLS cert)


---

<a id="509"></a>

### #509: Expose defaultConnection() method to switch between localhost and cloud based on env

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/509 |
| **State** | CLOSED |
| **Author** | josh-berry (Josh Berry) |
| **Created** | 2024-04-12 23:54:40.000 UTC (1y 9m ago) |
| **Updated** | 2025-10-06 21:10:27.000 UTC |
| **Closed** | 2025-10-06 21:04:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Expose a defaultConnection() method or similar that switches between localhost (default) and cloud based on some environment variable, so that we can use it in the samples repos to allow people to easily run samples in the cloud. See: https://github.com/temporalio/features/issues/454




---

<a id="503"></a>

### #503: [Bug] Do not mutate `extra` in activity/workflow loggers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/503 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-04-09 20:19:03.000 UTC (1y 9m ago) |
| **Updated** | 2024-04-09 20:19:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

If there is an existing `extra` dictionary passed to the logger, copy it before mutating it


---

<a id="500"></a>

### #500: [Feature Request] Add extra context on task failure logs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/500 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-04-02 15:32:11.000 UTC (1y 9m ago) |
| **Updated** | 2024-04-12 14:52:18.000 UTC |
| **Closed** | 2024-04-12 14:52:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

For workflow and activities, on task failure we need to provide `extra` with Temporal workflow/activity info like regular workflow/activity.logger calls do.


---

<a id="496"></a>

### #496: [Bug] Fix explanation of Pydantic datetime issue

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/496 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-03-29 21:34:44.000 UTC (1y 9m ago) |
| **Updated** | 2024-03-29 22:23:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Today the README says to mark datetime as passthrough, but the right answer is to mark it as unrestricted. Fix this in README, and also consider providing snippet from https://github.com/temporalio/samples-python/blob/4303a9b15f4ddc4cd770bc0ba33afef90a25d3ae/pydantic_converter/worker.py#L45-L65.

Also clarify in README that you should pass through non-workflow imports, not just third party.


---

<a id="495"></a>

### #495: [Feature Request] Use macOS M1/ARM runners for all SDK CI

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/495 |
| **State** | CLOSED |
| **Author** | Quinn-With-Two-Ns (Quinn Klassen) |
| **Created** | 2024-03-26 17:52:38.000 UTC (1y 9m ago) |
| **Updated** | 2024-04-12 19:47:50.000 UTC |
| **Closed** | 2024-04-12 19:47:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See: https://github.com/temporalio/features/issues/396


---

<a id="488"></a>

### #488: [Bug] Fix typo in restriction set

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/488 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-03-15 21:49:38.000 UTC (1y 9m ago) |
| **Updated** | 2024-03-15 21:49:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

https://github.com/temporalio/sdk-python/blob/f3d1b8511a3ef33b36fa806a9fc56f96a4a70b84/temporalio/worker/workflow_sandbox/_restrictions.py#L602 needs to be a comma instead of space


---

<a id="485"></a>

### #485: [Feature Request] SDK should not return an update handle if the update has not reached the desired state

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/485 |
| **State** | CLOSED |
| **Author** | Quinn-With-Two-Ns (Quinn Klassen) |
| **Created** | 2024-03-06 18:21:18.000 UTC (1y 10m ago) |
| **Updated** | 2024-05-20 20:23:49.000 UTC |
| **Closed** | 2024-05-20 20:23:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See https://github.com/temporalio/features/issues/432


---

<a id="484"></a>

### #484: [Feature Request] SDK clients should set a UpdateID on any update request even if the user did not specify one

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/484 |
| **State** | CLOSED |
| **Author** | Quinn-With-Two-Ns (Quinn Klassen) |
| **Created** | 2024-03-06 17:34:11.000 UTC (1y 10m ago) |
| **Updated** | 2024-05-20 20:23:48.000 UTC |
| **Closed** | 2024-05-20 20:23:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See: https://github.com/temporalio/features/issues/431



---

<a id="479"></a>

### #479: [Feature Request] Reflection-based payload codec test to ensure all non-search-attribute payloads are covered

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/479 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-02-26 15:05:24.000 UTC (1y 10m ago) |
| **Updated** | 2024-02-26 15:07:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

In .NET we have [these tests](https://github.com/temporalio/sdk-dotnet/blob/main/tests/Temporalio.Tests/Worker/WorkflowCodecHelperTests.cs) which make sure any new payload added in gRPC is made part of codec conversion. But we don't have this for Python. We should.

(this would have caught #477)


---

<a id="477"></a>

### #477: [Bug] Updates not run through data converters

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/477 |
| **State** | CLOSED |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2024-02-22 23:19:58.000 UTC (1y 10m ago) |
| **Updated** | 2024-02-23 01:13:05.000 UTC |
| **Closed** | 2024-02-23 01:13:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | Sushisource |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->

### Describe the bug

The input to and responses to updates are not currently being run through the payload converters / codecs.


---

<a id="473"></a>

### #473: [Feature Request] Clarify in README that --all-extras is needed for build

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/473 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-02-15 13:11:22.000 UTC (1y 10m ago) |
| **Updated** | 2024-02-27 15:12:47.000 UTC |
| **Closed** | 2024-02-27 15:12:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Need to change install command to `poetry install --no-root --all-extras`. Also may need to make clear newer version of `pip` required.


---

<a id="470"></a>

### #470: [Feature Request] Clarify option immutability for application failures

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/470 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-02-12 14:46:31.000 UTC (1y 11m ago) |
| **Updated** | 2024-02-27 15:12:48.000 UTC |
| **Closed** | 2024-02-27 15:12:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Clarify that options on application failures are not mutated by the server (e.g. non-retryable is an error option, not a server-provided failure)


---

<a id="468"></a>

### #468: Expose next retry delay on Application Failure in all SDKs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/468 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2024-02-02 22:35:16.000 UTC (1y 11m ago) |
| **Updated** | 2024-08-19 17:37:33.000 UTC |
| **Closed** | 2024-08-19 17:37:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

We recently added the ability for an activity to specify the next retry delay on failure via a [field on `ApplicationFailureInfo`](https://github.com/temporalio/api/blob/822966d29758cba3462d6f5ff198257fbaa426f6/temporal/api/failure/v1/message.proto#L49).

Expose this in the SDK.


---

<a id="467"></a>

### #467: [Feature Request] Support workflow metadata query

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/467 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-02-02 21:29:42.000 UTC (1y 11m ago) |
| **Updated** | 2024-02-02 21:29:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See https://github.com/temporalio/features/issues/51


---

<a id="439"></a>

### #439: [Feature Request] Allow continue as new from update handler

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/439 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-29 21:26:16.000 UTC (2y 1m ago) |
| **Updated** | 2023-11-29 21:26:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Allow continue as new from update handler


---

<a id="438"></a>

### #438: [Feature Request] Allow `RawValue` to be used for Search Attibute Upsert

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/438 |
| **State** | OPEN |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2023-11-29 19:26:11.000 UTC (2y 1m ago) |
| **Updated** | 2023-11-29 19:26:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

All in the title. There's no way at the moment to pass `RawValues` as a search attribute value. This is useful for ourselves for certain testing purposes, but could also be desirable for users shuttling SAs from one workflow to another, etc.


---

<a id="436"></a>

### #436: [Bug] Ensure continue as new raised from update handler is a task failure

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/436 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-28 20:49:21.000 UTC (2y 1m ago) |
| **Updated** | 2023-11-28 20:49:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Right now reading code, this may end up being swallowed (it's a base exception that I think is re-raised and uncaught in a task).


---

<a id="435"></a>

### #435: [Bug] Overload checking not catching bad parameters anymore

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/435 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-27 21:26:59.000 UTC (2y 1m ago) |
| **Updated** | 2023-11-27 21:26:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

It appears sometime recently or in some MyPy version recently, you can execute a single-param workflow with no param and MyPy won't catch it. Investigate this and other places where type checking should catch issues but is not. Ideally we can write tests to confirm type checking failures.


---

<a id="433"></a>

### #433: [Feature Request] Remove activation job sorting and confirm update order

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/433 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-27 13:46:03.000 UTC (2y 1m ago) |
| **Updated** | 2023-11-27 13:46:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Inside `_workflow_instance.py` we are sorting activation jobs which core does now. After https://github.com/temporalio/sdk-core/pull/639 is merged, remove that sorting, and write a test confirming that interleaved updates and signals remain interleaved.


---

<a id="427"></a>

### #427: [Bug] dataclasses require even defaulted fields in the JSON dict

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/427 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-13 20:41:00.000 UTC (2y 1m ago) |
| **Updated** | 2023-11-13 20:41:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

It appears, at first code glance, that a dataclass cannot deserialize a JSON dict that doesn't have _all_ fields including ones with defaults. This is a bug, we should not require all fields of a dataclass be present.


---

<a id="424"></a>

### #424: [Feature Request] add get update handle and set run ID

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/424 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-13 13:18:33.000 UTC (2y 1m ago) |
| **Updated** | 2024-05-20 20:23:48.000 UTC |
| **Closed** | 2024-05-20 20:23:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Should be able to get an update handle from a workflow handle. Also since updates are always run-specific, when using "start", the update handle should use the returned run ID regardless of what user passed in (but can still leave optional when using getter). And for the update-handle getter, we should document very clearly the consequences of getting an update handle on a workflow handle with no run ID.


---

<a id="421"></a>

### #421: [Feature Request] Move from black/isort to Ruff

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/421 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-09 15:20:21.000 UTC (2y 2m ago) |
| **Updated** | 2023-11-09 17:29:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | rachfop |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

We did this at https://github.com/temporalio/samples-python/pull/91 with success, we should do it here too


---

<a id="419"></a>

### #419: [Bug] README issues

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/419 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-08 22:11:45.000 UTC (2y 2m ago) |
| **Updated** | 2024-02-27 15:12:48.000 UTC |
| **Closed** | 2024-02-27 15:12:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

* Class `__init__()` doesn't take `self`
* Running tests should have `--all-extras` on `poetry install`
* Clarify in dev section that you need latest pip


---

<a id="416"></a>

### #416: [Feature Request] Ensure activity failure warning log has contextual workflow information

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/416 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-11-06 21:42:08.000 UTC (2y 2m ago) |
| **Updated** | 2024-02-20 22:57:16.000 UTC |
| **Closed** | 2024-02-20 22:57:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Today for not-found activities since they never enter activity context, they don't have workflow info. Make sure they get workflow info.


---

<a id="409"></a>

### #409: Remove OutboundInterceptor.poll_workflow_update

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/409 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-10-27 19:46:12.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-30 17:49:49.000 UTC |
| **Closed** | 2023-10-30 17:49:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

This does not need to be interceptable any more than getting a workflow result does



---

<a id="408"></a>

### #408: Remove timeout from WorkflowUpdateHandle.result

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/408 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-10-27 19:43:53.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-30 17:49:50.000 UTC |
| **Closed** | 2023-10-30 17:49:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Users can do this with `asyncio.wait_for` if they want, no reason to have this if we're not also gonna have it on `WorkflowHandle.result`


---

<a id="404"></a>

### #404: [Feature Request] Add StartDelay to Workflow Options

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/404 |
| **State** | CLOSED |
| **Author** | tlalfano (Tasha Alfano) |
| **Created** | 2023-10-23 19:16:21.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-24 22:40:22.000 UTC |
| **Closed** | 2023-10-24 22:40:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Expose StartDelay in Workflow Options



---

<a id="398"></a>

### #398: [Feature Request] Drop 3.7 support, update CI to Python 3.12, and other build improvements

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/398 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-10-10 15:14:17.000 UTC (2y 3m ago) |
| **Updated** | 2023-11-16 13:38:42.000 UTC |
| **Closed** | 2023-11-16 13:38:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

* Remove support for 3.7 (i.e. update everything to 3.8)
* Update build tooling versions of setuptools-rust, cibuildwheel, and others
* Re-investigate needs for setup.py, fix-wheel, etc that we have had to hack
* Add 3.12 to CI


---

<a id="395"></a>

### #395: [Feature Request] Client keep alive support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/395 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-10-06 15:44:54.000 UTC (2y 3m ago) |
| **Updated** | 2023-10-09 20:15:41.000 UTC |
| **Closed** | 2023-10-09 20:15:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Already enabled by default in latest SDK core, just need to expose the options from https://github.com/temporalio/sdk-core/issues/585


---

<a id="388"></a>

### #388: [Feature Request] Eager workflow start

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/388 |
| **State** | CLOSED |
| **Author** | antlai-temporal (Antonio Lain) |
| **Created** | 2023-09-25 19:29:24.000 UTC (2y 3m ago) |
| **Updated** | 2023-11-27 17:59:44.000 UTC |
| **Closed** | 2023-11-27 17:59:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Support for Eager Workflow Start was added in the server in 1.20, and is currently supported in Java and Go SDKs. It 
reduces the latency to start a workflow by allowing the client to directly schedule the first workflow task to a local worker. See https://github.com/temporalio/features/issues/242 for details.

The goal is to make Python the first core-based SDK to support Eager Workflow Start after is enabled in Core. 

### Describe the solution you'd like

This mechanism is mostly transparent to the Python SDK client, but we need to add a flag `enable_eager_start` to `client.start_workflow` to use it. It will default to `False` while the core implementation is considered experimental.

### Additional context

This depends on https://github.com/temporalio/sdk-core/issues/606



---

<a id="379"></a>

### #379: Document that time skipping test server does not currently work on ARM

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/379 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-08-24 19:26:11.000 UTC (2y 4m ago) |
| **Updated** | 2023-10-24 13:20:42.000 UTC |
| **Closed** | 2023-10-24 13:20:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | â¤ï¸ 1 |

#### Description

### Describe the solution you'd like

This is a known issue. See https://github.com/temporalio/sdk-java/issues/1407. In the meantime, people have to use the auto intel/amd-to-arm translation with macOS or use non-ARM Linux.

We need to document this.


---

<a id="377"></a>

### #377: [Feature Request] Warn when non-static activity method registered as static

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/377 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-08-23 18:03:59.000 UTC (2y 4m ago) |
| **Updated** | 2024-03-18 18:00:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like
If possible, if an activity's first parameter is `self` but it's not an instance method, warn when creating the worker. This means the instance wasn't instantiated.




---

<a id="369"></a>

### #369: [Feature Request] Advanced metrics support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/369 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-08-18 14:23:49.000 UTC (2y 4m ago) |
| **Updated** | 2023-10-05 15:46:22.000 UTC |
| **Closed** | 2023-10-05 15:46:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

With https://github.com/temporalio/sdk-core/pull/544 landed, we need to:

* [x] Support any missing prom options (done in #380)
* [x] Support any missing otel options (done in #380)
* [ ] Support user-accessible metric buffer-based forwarding
* ~Support Python provided metric implementation~ We're gonna consider this too expensive at this time and not write this
* [x] Support access to metrics from runtime (done in #384)
* [x] Support access to metrics from activity with default activity tags (done in #384)
* [x] Support access to metrics from workflow with default workflow tags and ignore metric updates during replay by default (done in #384)


---

<a id="363"></a>

### #363: [Bug] Testing environment overrides set interceptors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/363 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-08-07 13:16:25.000 UTC (2y 5m ago) |
| **Updated** | 2023-08-07 22:28:09.000 UTC |
| **Closed** | 2023-08-07 22:28:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

https://github.com/temporalio/sdk-python/blob/24fea4cc31eded8b32e3619490b68554c0b52376/temporalio/testing/_workflow.py#L565 should be `config["interceptors"] = config_interceptors`.


---

<a id="355"></a>

### #355: [Feature Request] Log and drop signals that pass the wrong number of arguments

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/355 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-07-24 22:22:52.000 UTC (2y 5m ago) |
| **Updated** | 2023-07-24 22:22:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

In #349 we now drop signals whose arguments are of the wrong type. But we still fail the workflow task if the arity is off, e.g. with Python exceptions like "missing 1 required positional argument". Try to make sure there is one argument for every required parameter and no more than accepted (varargs notwithstanding). May be easiest to have a min/max param count on the `_SignalDefinition` and just check against that.


---

<a id="352"></a>

### #352: [Bug] Cancel received in between awaitables not cancelling workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/352 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-07-18 18:08:21.000 UTC (2y 5m ago) |
| **Updated** | 2023-07-18 18:10:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Given workflow logic like:

```python
await workflow.execute_activity(some_activity, ...)
await workflow.execute_child_workflow(SomeChildWorkflow.run, ...)
```

There may be an issue where cancellation is sent after activity is marked complete but before the next task is processed. So an event history like:

* `ActivityTaskCompleted`
* `WorkflowExecutionCancelRequested`

Where both of those are in a single task. To replicate, the best way may be to:

* Create separate workflow and activity workers
* After activity starts, kill the workflow worker
* Complete activity and confirm completion is in history
* Send cancellation to workflow
* Start back up the workflow worker and confirm it processes activity completion, then cancellation

This has not been tested, so it may not be a bug. If this is indeed a bug, this may be a gap in our asyncio implementation. One would expect that issuing a cancel on the workflow asyncio task would automatically propagate to next non-shielded await but maybe it does not do that automatically because it never expects to receive a cancel when not awaiting on something (i.e. it relies on GIL to know that asyncio does not yield to outside until waiting on coroutine). But [the docs](https://docs.python.org/3/library/asyncio-task.html#task-cancellation) do say:

> When a task is cancelled, [asyncio.CancelledError](https://docs.python.org/3/library/asyncio-exceptions.html#asyncio.CancelledError) will be raised in the task at the next opportunity.

Need to investigate...


---

<a id="348"></a>

### #348: [Feature Request] Make all exposed modules be packages

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/348 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-07-17 18:05:59.000 UTC (2y 5m ago) |
| **Updated** | 2023-07-17 18:06:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

`temporalio.activity`, `temporalio.workflow`, etc are all modules right now. Besides the obvious benefit of properly separating code in a package into multiple files, the other benefit is that we can control `__all__` a bit more clearly so that users' intellisense doesn't display things we've imported (and in general they aren't able to access the imported things).


---

<a id="347"></a>

### #347: [Bug] Signals that can't be deserialized should be dropped/ignored

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/347 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-07-13 22:39:27.000 UTC (2y 6m ago) |
| **Updated** | 2023-07-18 11:07:17.000 UTC |
| **Closed** | 2023-07-18 11:07:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

In Go and Java we drop signals whose args can't be deserialized, we should here too. This is likely not considered a backwards incompatible behavior change.


---

<a id="335"></a>

### #335: Worker versioning support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/335 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2023-07-05 16:52:34.000 UTC (2y 6m ago) |
| **Updated** | 2023-07-10 22:14:00.000 UTC |
| **Closed** | 2023-07-10 22:14:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

*No description provided.*


---

<a id="328"></a>

### #328: Remove "similar to Pedantic" from no-generic part of README

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/328 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-06-11 12:28:33.000 UTC (2y 7m ago) |
| **Updated** | 2023-07-05 14:01:45.000 UTC |
| **Closed** | 2023-07-05 14:01:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

They do have limited support now


---

<a id="326"></a>

### #326: [Feature Request] Add helper for checking whether exception represents cancellation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/326 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-06-09 15:08:07.000 UTC (2y 7m ago) |
| **Updated** | 2023-07-05 14:01:46.000 UTC |
| **Closed** | 2023-07-05 14:01:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Logic is something like:

```python
is_cancelled =
    isinstance(err, asyncio.CancelledError) or
    isinstance(err, temporalio.exceptions.CancelledError) or
    (
        (isinstance(err, temporalio.exceptions.ActivityError) or isinstance(err, temporalio.exceptions.ChildWorkflowError)) and
        isinstance(err.cause, temporalio.exceptions.CancelledError)
    )
```

The internal workflow code can also use this


---

<a id="320"></a>

### #320: [Feature Request] Remove experimental warning from Schedules

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/320 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-05-22 22:24:42.000 UTC (2y 7m ago) |
| **Updated** | 2023-06-16 14:33:11.000 UTC |
| **Closed** | 2023-06-16 14:33:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Already removed in Go at https://github.com/temporalio/sdk-go/pull/1064.


---

<a id="318"></a>

### #318: [Bug] If self parameter is type hinted, it is not properly ignored when deserializing parameters

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/318 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-05-17 22:16:10.000 UTC (2y 7m ago) |
| **Updated** | 2023-07-18 11:07:18.000 UTC |
| **Closed** | 2023-07-18 11:07:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

`async def run(self: Self, arg: ArgType)` will not remove `self` when deserializing types because of https://github.com/temporalio/sdk-python/blob/a4224a20189c8c8078edbc66482c31860193c1b3/temporalio/common.py#L235

We need to see if we can be smarter about ignoring `self`.


---

<a id="301"></a>

### #301: [Bug] Proxied classes are not properly hashable

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/301 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-03-23 13:31:29.000 UTC (2y 9m ago) |
| **Updated** | 2023-03-23 13:31:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Getting `unhashable type: '_RestrictedProxy'` possibly on a datetime in a workflow even though we define `__hash__` I am unsure why this is, need to investigate.


---

<a id="294"></a>

### #294: Add API to count workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/294 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2023-03-03 22:05:12.000 UTC (2y 10m ago) |
| **Updated** | 2024-04-19 12:50:15.000 UTC |
| **Closed** | 2024-04-19 12:50:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See parent issue for more information.


---

<a id="293"></a>

### #293: [Feature Request] Clarify activity_id parameter is not recommended

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/293 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-03-03 15:06:39.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-30 19:04:26.000 UTC |
| **Closed** | 2023-03-30 19:04:26.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Go SDK docs say activity ID should not be set in most cases and to contact Temporal team to confirm you need to set this. We should do the same here.


---

<a id="291"></a>

### #291: Add disclaimer about gevent incompatibility

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/291 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-03-02 18:57:39.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-30 19:04:28.000 UTC |
| **Closed** | 2023-03-30 19:04:28.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Until #59 is solved, we should call out gevent's monkey patching messes up asyncio and our custom event loop


---

<a id="290"></a>

### #290: [Feature Request] Support result_type on child workflows and activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/290 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-02-27 19:50:00.000 UTC (2y 10m ago) |
| **Updated** | 2023-07-06 13:20:13.000 UTC |
| **Closed** | 2023-07-06 13:20:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

`Client.start_workflow` accepts this for string-name-based workflows, we should do the same for activities and child workflows when invoked from workflows. Maybe queries too.


---

<a id="289"></a>

### #289: [Bug] JSONTypeConverter not applied recursively

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/289 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-02-24 17:00:25.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-30 17:51:06.000 UTC |
| **Closed** | 2023-03-30 17:51:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Recursive calls in `value_to_type` are not passing the JSON type converters.


---

<a id="285"></a>

### #285: [Bug] temporalio.worker.WorkflowReplayResult(s) not visible

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/285 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-02-21 17:47:24.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-30 19:04:27.000 UTC |
| **Closed** | 2023-03-30 19:04:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Need to re-export from private module


---

<a id="282"></a>

### #282: [Feature Request] Clarify requirement of heartbeat timeout wrt cancellation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/282 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-02-16 16:49:35.000 UTC (2y 10m ago) |
| **Updated** | 2023-02-21 17:37:08.000 UTC |
| **Closed** | 2023-02-21 17:37:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Docs in addition to saying heartbeat is required need to say heartbeat timeout is required


---

<a id="273"></a>

### #273: [Feature Request] Improve workflow sandboxing suggestions

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/273 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-02-13 16:05:42.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-21 17:37:07.000 UTC |
| **Closed** | 2023-02-21 17:37:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

* In README, encourage putting workflows in separate files from all other code including activities
* Add clearer direction on exception messages
* Add reference to exception messages inside README for better SEO
* With the exception of "hello" code, change the samples to do this


---

<a id="267"></a>

### #267: [Bug] Health proto clashes with grpc_health library

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/267 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-02-07 12:49:19.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-09 23:04:58.000 UTC |
| **Closed** | 2023-02-09 23:04:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

When importing `grpc_health.v1` from the gRPC library you get:

> TypeError: Conflict register for file "health/v1/health.proto": grpc.health.v1.HealthCheckRequest is already defined in file "grpc_health/v1/health.proto". Please fix the conflict by adding package name on the proto file, or use different name for the duplication.

We need to change out health proto package so it doesn't clash with gRPC's.


---

<a id="266"></a>

### #266: [Feature Request] Confirm async activity completion can give null value for Optional-return activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/266 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-02-02 16:36:06.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-21 17:37:08.000 UTC |
| **Closed** | 2023-02-21 17:37:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Reading the code, it looks like we just don't put a value instead of putting a single null when the result is given `None`. Make a test to confirm that is ok for a workflow expecting an activity response of `Optional[str]` or similar.


---

<a id="265"></a>

### #265: [Feature Request] Properly list queries in query-handler-not-found error

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/265 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-02-02 14:47:36.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-21 17:37:07.000 UTC |
| **Closed** | 2023-02-21 17:37:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

UI not showing possible queries.

### Describe the solution you'd like

Need to put known queries in space-delimited form surrounded by brackets for use in the UI. E.g. "Some query error, queries: [query1 query2]".

Maybe one day we'll get to https://github.com/temporalio/features/issues/51.


---

<a id="263"></a>

### #263: [Feature Request] Populate contextvars for threaded activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/263 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-01-30 13:24:49.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-08 11:19:43.000 UTC |
| **Closed** | 2023-02-08 11:19:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Currently we are calling https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor but we are not propagating the contextvars.

### Describe the solution you'd like

Since there is no way with `run_in_executor` and we don't want to use `to_thread` as suggested at https://github.com/python/cpython/issues/78195 because it doesn't use a custom executor, we need to just populate the context ourselves. This of course won't work with process pool executor.


---

<a id="258"></a>

### #258: [Bug] Make sure start workflows throws WorkflowAlreadyStartedError if already running

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/258 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-01-19 14:13:50.000 UTC (2y 11m ago) |
| **Updated** | 2023-02-17 19:10:35.000 UTC |
| **Closed** | 2023-02-17 19:10:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Needs to be friendly error returned in this case


---

<a id="255"></a>

### #255: [Bug] PEP 604 style optionals not working properly

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/255 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-01-16 12:06:56.000 UTC (2y 11m ago) |
| **Updated** | 2023-01-23 13:58:21.000 UTC |
| **Closed** | 2023-01-23 13:58:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

https://github.com/temporalio/sdk-python/blob/1a260c389a8daa1e6378b113cf2301736da320e2/temporalio/converter.py#L1159 needs to change to:

```python
if origin is Union or isinstance(origin, types.UnionType):
```

Of course it's a bit more complicated than that since `types.UnionType` is only 3.10+. Also add tests confirming PEP 604 style unions for optional and others.


---

<a id="254"></a>

### #254: [Bug] with_child_unrestricted not properly removing restriction

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/254 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-01-13 16:44:53.000 UTC (2y 12m ago) |
| **Updated** | 2023-02-08 11:19:42.000 UTC |
| **Closed** | 2023-02-08 11:19:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

https://github.com/temporalio/sdk-python/blob/ddbb92fdb052f6e1eeecc25bec37e654eabed4fd/temporalio/worker/workflow_sandbox/_restrictions.py#L311

Should be:

```python
if child_path[0] in self.children: 
```




---

<a id="250"></a>

### #250: [Feature Request] Warn/error on invalid query use

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/250 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-01-12 16:32:24.000 UTC (2y 12m ago) |
| **Updated** | 2023-07-18 16:30:33.000 UTC |
| **Closed** | 2023-07-18 16:30:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

* If a query is an `async def` we should warn by default but allow overriding/silencing that warning
* If a query uses workflow constructs like `wait_condition` we should error
* If core is not already doing it for us, if a query does anything to create commands we should error


---

<a id="247"></a>

### #247: [Bug] Failure.encoded_attributes not being subjected to codec

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/247 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-01-10 17:35:47.000 UTC (2y 12m ago) |
| **Updated** | 2023-01-23 16:35:51.000 UTC |
| **Closed** | 2023-01-23 16:35:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Seems `PayloadCodec.encode_failure` and `PayloadCodec.decode_failure` missed this field as part of #185


---

<a id="246"></a>

### #246: [Feature Request] Mark local activities as experimental

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/246 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-01-09 15:01:18.000 UTC (3 years ago) |
| **Updated** | 2023-01-13 18:52:45.000 UTC |
| **Closed** | 2023-01-13 18:52:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Local activities are still experimental in core but we have not documented this as such in Python docstrings.


---

<a id="237"></a>

### #237: [Bug] Activity cancellation error issues

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/237 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-01-03 19:29:57.000 UTC (3 years ago) |
| **Updated** | 2023-01-04 17:47:07.000 UTC |
| **Closed** | 2023-01-04 17:47:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

* We cannot send a cancelled error to server if not requested, so convert to application error if not requested (such as thread cancel)
* Need to start setting `ActivityExecutionResult.cancelled` on cancel


---

<a id="236"></a>

### #236: [Bug] Optional[X] type not being handled properly as parameter

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/236 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-12-30 18:44:03.000 UTC (3 years ago) |
| **Updated** | 2023-01-04 17:47:07.000 UTC |
| **Closed** | 2023-01-04 17:47:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Our check at https://github.com/temporalio/sdk-python/blob/1f72c982aa6153ac5b50efb1ee2248e18d9af083/temporalio/common.py#L247 that does `inspect.isclass` is `False` for `typing.Optional` (which is an alias for `typing.Union`). Remove that check. And make sure to write a test accepting an optional parameter (ideally using a Pydantic model as the inner optional value since this is what was originally reported).


---

<a id="234"></a>

### #234: [Bug] Generic Nested Dataclasses not desrialized correctly

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/234 |
| **State** | CLOSED |
| **Author** | ferntheplant (fern) |
| **Created** | 2022-12-29 01:15:39.000 UTC (3 years ago) |
| **Updated** | 2023-01-04 17:47:06.000 UTC |
| **Closed** | 2023-01-04 17:47:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### What are you really trying to do?

<!-- 
Tell us at a high level what you're doing, to avoid XY problem (https://en.wikipedia.org/wiki/XY_problem) 
-->
Create activities corresponding to hitting different API endpoints. The endpoint responses are paginated and I want a separate iterator that controls pagination on behalf of the endpoint caller. So each activity would accept the params for the endpoint provided by the user + a cursor provided by the iterator. So an activity might look like:

```python
P = TypeVar("P")
@dataclass
class ApiParams(Generic[P]):
    args: P
    cursor: str

@dataclass 
class EndpointFooParams:
    foo: str
    bar: str

@activity.defn
async def hit_endpoint_foo(param: ApiParams[EndpointFooParams]) -> list:
    args = params.args
    cursor = params.cursor
    response = await api.foo(args.foo, args.bar, cursor)
    return response.data
```

### Describe the bug

<!-- A clear and concise description of what the bug is. -->

<!-- If applicable, add screenshots or code blocks to help explain your problem. You can also use [Loom](http://loom.com/) to do short, free video bug reports. -->

When running an activity defined like above I get a runtime error 
```  
    a = param.args
    ^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'args'
```

This indicates that the activity params dataclass was deserialized into a plain dictionary instead of a proper dataclass.

### Minimal Reproduction

<!-- 
Modify our hello world templates to demonstrate:

- TypeScript: https://github.com/temporalio/samples-typescript/tree/main/hello-world
- Go: https://github.com/temporalio/money-transfer-project-template-go
- Java: https://github.com/temporalio/money-transfer-project-template-java
- PHP: https://github.com/temporalio/samples-php#samples
-->
```python
import asyncio

from dataclasses import dataclass
from typing import Generic, TypeVar
from datetime import timedelta

from temporalio import activity, workflow
from temporalio.client import Client
from temporalio.worker import Worker

P = TypeVar("P")

@dataclass
class A:
    foo: str

@dataclass
class B(Generic[P]):
    bar: str
    nested_a: P

@activity.defn
async def demo_activity(param: B[A]) -> str:
    a = param.nested_a
    return a.foo + param.bar

@workflow.defn()
class BasicWorkflow:
    @workflow.run
    async def run(self, foo: str) -> str:
        return await workflow.execute_activity(
            demo_activity,
            B(bar=foo, nested_a=A(foo=foo)),
            start_to_close_timeout=timedelta(seconds=10)
        )

async def main():
    client = await Client.connect("localhost:7233")

    async with Worker(
        client,
        task_queue="basic",
        workflows=[BasicWorkflow],
        activities=[demo_activity],
    ):
        await client.execute_workflow(
            BasicWorkflow.run,
            "hello",
            id="basic",
            task_queue="basic",
        )


if __name__ == "__main__":
    asyncio.run(main())

```

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: M1 Mac running macOS 12.5.1
- Temporal Version: python sdk 0.1b4
- Are you using Docker or Kubernetes or building Temporal from source? Docker compose

### Additional context

<!-- Add any other context about the problem here. -->
As per [slack discussion](https://temporalio.slack.com/archives/CTT84RS0P/p1672273325314819) resolving dataclass generics for deserialization is not expected of temporal data converters. Resolution is to update readme to warn future users



---

<a id="230"></a>

### #230: [Feature Request] Move the user-facing parts in temporalio.bridge.runtime to somewhere stable

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/230 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-12-15 13:26:44.000 UTC (3 years ago) |
| **Updated** | 2023-01-04 21:48:42.000 UTC |
| **Closed** | 2023-01-04 21:48:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Right now `temporalio.bridge.runtime` is used for telemetry and other things and we have intentionally left it in the unstable package

### Describe the solution you'd like

Move it somewhere stable, probably either `temporalio` top level or `temporalio.runtime` or `temporalio.common`.


---

<a id="228"></a>

### #228: [Feature Request] Eagerly error when attempting to register a callable activity class instead of its instance

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/228 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-12-14 13:28:47.000 UTC (3 years ago) |
| **Updated** | 2023-02-21 17:37:06.000 UTC |
| **Closed** | 2023-02-21 17:37:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Right now we get a failure at activity runtime. We should fail at worker registration time.


---

<a id="226"></a>

### #226: [Bug] Protobuf version 3.19 does not have ValueType enum wrapper

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/226 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-12-08 20:41:18.000 UTC (3y 1m ago) |
| **Updated** | 2023-01-03 21:24:43.000 UTC |
| **Closed** | 2023-01-03 21:24:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

`HistoryEventFilterType.ValueType` fails on protobuf 3.19 because https://github.com/protocolbuffers/protobuf/commit/0707f2e7f556c8396d6027d0533ec3a56d1061db didn't come until 3.20. Update the minimum dependency required.


---

<a id="224"></a>

### #224: [Bug] grpcio not being treated as optional dependency

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/224 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-12-07 18:46:05.000 UTC (3y 1m ago) |
| **Updated** | 2023-01-03 21:24:44.000 UTC |
| **Closed** | 2023-01-03 21:24:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

METADATA inside of wheel has:

```
Provides-Extra: grpc
Provides-Extra: opentelemetry
Requires-Dist: grpcio (>=1.48.0,<2.0.0)
Requires-Dist: opentelemetry-api (>=1.11.1,<2.0.0); extra == "opentelemetry"
Requires-Dist: opentelemetry-sdk (>=1.11.1,<2.0.0); extra == "opentelemetry"
Requires-Dist: protobuf (>=4.21,<4.22)
Requires-Dist: python-dateutil (>=2.8.2,<3.0.0); python_version < "3.11"
Requires-Dist: types-protobuf (>=3.20,<3.21)
Requires-Dist: typing-extensions (>=4.2.0,<5.0.0)
```

Why does `grpcio` not have an extra listed? I think it's because it doesn't reference the `grpcio` dependency properly in `pyproject.toml` (calls it `grpc` instead).


---

<a id="218"></a>

### #218: [Feature Request] Disallow cross-namespace child workflow calls

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/218 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-12-01 16:52:33.000 UTC (3y 1m ago) |
| **Updated** | 2022-12-05 19:33:11.000 UTC |
| **Closed** | 2022-12-05 19:33:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Remove `namespace` option from child workflow calls. Temporal does not want to support this approach for cross-namespace calling in the future (we have other plans).


---

<a id="212"></a>

### #212: [Bug] Replayer README inaccurate

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/212 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-24 13:55:07.000 UTC (3y 1m ago) |
| **Updated** | 2022-12-05 19:33:10.000 UTC |
| **Closed** | 2022-12-05 19:33:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

README for replayer out of date about how to load from JSON. Update with new client history fetching details.


---

<a id="204"></a>

### #204: [Feature Request] Make it easier to pass through third party modules

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/204 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-16 21:21:36.000 UTC (3y 1m ago) |
| **Updated** | 2022-12-02 21:13:03.000 UTC |
| **Closed** | 2022-12-02 21:13:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Right now you have to configure the sandbox runner w/ unstable APIs. Needs to have a simpler way.

### Describe the solution you'd like

Probably a simple `with`, so something like:

```python
from temporalio import activity, workflow

with workflow.unsafe.imports_passed_through():
    import requests
    import urllib3
```

Any concern that users would abuse it? I could also do something w/ AST/comments, e.g.

```python
import requests # temporal: pass-through
import urllib3 # temporal: pass-through
```

Finally, maybe I could (also) just make this easier at the worker level by accepting a set of `workflow_sandbox_passthrough_modules` on the worker constructor?


---

<a id="203"></a>

### #203: [Bug] Fix README to use workflow_runner instead of runner in sandbox examples

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/203 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-16 20:16:05.000 UTC (3y 1m ago) |
| **Updated** | 2022-12-02 21:13:02.000 UTC |
| **Closed** | 2022-12-02 21:13:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Wrong arg name in sample


---

<a id="199"></a>

### #199: [Bug] OpenTelemetry sandbox issue on first load inside sandbox

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/199 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-14 23:09:13.000 UTC (3y 1m ago) |
| **Updated** | 2022-11-15 22:00:14.000 UTC |
| **Closed** | 2022-11-15 22:00:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

If the first use of OpenTelemetry is inside a workflow, you can get (stack trace elided):

```
    return _RUNTIME_CONTEXT.attach(context)  # type:ignore
AttributeError: 'NoneType' object has no attribute 'attach'
```

OpenTelemetry uses https://setuptools.pypa.io/en/latest/pkg_resources.html to lazily load its global runtime context. See https://github.com/open-telemetry/opentelemetry-python/blob/787e499aaadb5db6e0eb308a9fa7dca58139e44c/opentelemetry-api/src/opentelemetry/context/__init__.py#L34-L69.

The first time this is run, `pkg_resources` issues an `open()` at https://github.com/pypa/setuptools/blob/a0e8e53cecc238e3bce2247308fe7dd94114ef35/pkg_resources/__init__.py#L1635. We don't allow `open()` in a sandbox. This wasn't being triggered in unit tests because the OpenTelemetry context was being used outside of the sandbox first for client calls. But when separating worker and client (like our examples do), this shows up.

Easiest fix for now is just add this to the top of the `opentelemetry` contrib Python file:

```
opentelemetry.context.get_current()
```

This means when first imported _outside_ the sandbox, it'll lazily load instead of trying _inside_ of the sandbox.


---

<a id="198"></a>

### #198: [Bug] Bad metrics config type

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/198 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-14 21:39:23.000 UTC (3y 1m ago) |
| **Updated** | 2022-11-15 22:00:15.000 UTC |
| **Closed** | 2022-11-15 22:00:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

https://github.com/temporalio/sdk-python/blob/7dedda32d4ba544fe30bffc002f07187783702dd/temporalio/bridge/runtime.py#L150 should be `MetricsConfig`


---

<a id="187"></a>

### #187: [Feature Request] Allow client headers to be mutated after creation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/187 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-04 14:03:33.000 UTC (3y 2m ago) |
| **Updated** | 2022-11-10 18:47:55.000 UTC |
| **Closed** | 2022-11-10 18:47:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

https://github.com/temporalio/sdk-core/pull/326 supports a `set_headers` call, so we should support a `Client.set_rpc_metadata` method


---

<a id="178"></a>

### #178: [Feature Request] Prefix private files/modules' filenames with underscore

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/178 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-01 12:59:19.000 UTC (3y 2m ago) |
| **Updated** | 2022-11-04 21:34:38.000 UTC |
| **Closed** | 2022-11-04 21:34:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Auto complete in IDE keeps bringing up things like `temporalio.worker.activity` even though we expose what we want via `__all__` in `temporalio.worker`. Make those private modules properly private with underscore-prefixed filenames.


---

<a id="176"></a>

### #176: [Feature Request] Support string-based enum

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/176 |
| **State** | CLOSED |
| **Author** | KMilhan (Milhan) |
| **Created** | 2022-11-01 00:15:21.000 UTC (3y 2m ago) |
| **Updated** | 2022-11-03 12:51:00.000 UTC |
| **Closed** | 2022-11-03 12:51:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently, this is how str-based enum works in sdk-python

```python
import asyncio
from datetime import timedelta
from enum import Enum

from temporalio import activity, workflow
from temporalio.client import Client
from temporalio.worker import Worker


class Status(str, Enum):
    REQUESTED = "requested"
    STARTED = "started"
    FAILED_TO_START = "failed_to_start"
    IN_PROGRESS = "in_progress"
    ERROR = "error"
    COMPLETED = "completed"


@activity.defn
async def say_hello(name: Status) -> str:
    return str(type(name))


@workflow.defn
class SayHello:
    @workflow.run
    async def run(self) -> str:
        return await workflow.execute_activity(
            say_hello,
            "completed",
            schedule_to_close_timeout=timedelta(seconds=5),
        )


async def main():
    # Create client connected to server at the given address
    client = await Client.connect("localhost:7233")

    # Run the worker
    async with Worker(
        client,
        task_queue="my-task-queue",
        workflows=[SayHello],
        activities=[say_hello],
    ):
        print(
            await client.execute_workflow(
                SayHello.run, id="test", task_queue="my-task-queue"
            )
        )


if __name__ == "__main__":
    asyncio.run(main())
```
This code prints
`<class 'list'>`
which can break your underlying activities.


Changing activity's signature to
```
@activity.defn
async def say_hello(name: Status | str) -> str:
    return str(type(name))
```
gives you `<class 'str'>` . Now you can enjoy Temporal with str-based Enum.

But ideally, python-sdk can support string-based enums


---

<a id="167"></a>

### #167: [Bug] Do not require timer to be present when fire-timer received

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/167 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-26 14:45:27.000 UTC (3y 2m ago) |
| **Updated** | 2022-10-28 15:53:51.000 UTC |
| **Closed** | 2022-10-28 15:53:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Same as https://github.com/temporalio/sdk-typescript/pull/466 in TS. Basically if a timer is fired in the same job that a signal is received that cancels it, the timer won't be present when fired. We require a timer's presence when fired, but we shouldn't just in case its failed earlier that activation.


---

<a id="165"></a>

### #165: [Feature Request] Tests for Python 3.11 support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/165 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-25 16:14:00.000 UTC (3y 2m ago) |
| **Updated** | 2022-10-31 20:12:48.000 UTC |
| **Closed** | 2022-10-31 20:12:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Confirm in CI we support 3.11. Do not do this until after #164 lands.


---

<a id="159"></a>

### #159: [Bug] Function type hint lookup cache key is bad

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/159 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-18 16:40:33.000 UTC (3y 2m ago) |
| **Updated** | 2022-10-28 16:50:14.000 UTC |
| **Closed** | 2022-10-28 16:50:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

A function's `__qualname__` is not unique enough for caching type hints (we found this out while writing Python sample tests w/ same-named workflow classes). The name is only qualified to the class level, not higher. We can find a way to qualify this (can get complicated with inheritance and modules and such), or just remove the type hint cache. If we start storing the type hints on the definitions of things themselves, the need for a cache may be obviated.

Also, please test that `typing.Set[foo]` as a return type works with client. Preliminary review shows that we are excluded return type hints that don't satisfy `inspect.isclass` which these typing wrappers do not.


---

<a id="156"></a>

### #156: [Feature Request] Remove custom Go server

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/156 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-10-13 12:46:43.000 UTC (3y 2m ago) |
| **Updated** | 2022-10-28 13:21:55.000 UTC |
| **Closed** | 2022-10-28 13:21:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Now that Temporalite is released w/ search attribute cache disabling and proper TLS base config override, we can just use it instead


---

<a id="125"></a>

### #125: Mark global tracing options as experimental

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/125 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-02 14:10:49.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-07 22:29:15.000 UTC |
| **Closed** | 2022-09-07 22:29:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Currently we have a top-level call for preparing telemetry in `temporalio.bridge.telemetry`. This implies that telemetry is global, meaning you have to configure things for the entire process without the option of having worker-specific settings.

### Describe the solution you'd like

I would hope this doesn't last forever and therefore we can remove this top-level call (though there is a default that is still global). For now we need to make it clear on the API that this may not last forever. We have also resisted moving this to a more public-facing package for this reason.

See: https://github.com/temporalio/sdk-core/issues/384


---

<a id="124"></a>

### #124: Do not require ID for child workflow invocation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/124 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-02 13:50:17.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-07 22:29:15.000 UTC |
| **Closed** | 2022-09-07 22:29:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Today on clients and in workflows we require explicit workflow IDs

### Describe the solution you'd like

For in-workflow child workflow invocation, we should not require a workflow ID but instead make it optional and default to a UUID.


---

<a id="122"></a>

### #122: Remove abbreviations in options

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/122 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-01 17:33:05.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-07 22:29:16.000 UTC |
| **Closed** | 2022-09-07 22:29:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

In worker options, `max_concurrent_wft_polls` should be `max_concurrent_workflow_task_polls` and `max_concurrent_at_polls` should be `max_concurrent_activity_task_polls`. Check for other mistakes like this.


---

<a id="119"></a>

### #119: Remove callable as accepted client interceptor type

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/119 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-30 22:15:48.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-07 22:29:16.000 UTC |
| **Closed** | 2022-09-07 22:29:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Currently a client interceptor is `Union[temporalio.client.Interceptor, Callable[[temporalio.client.OutboundInterceptor], temporalio.client.OutboundInterceptor]]` but there's no need for the shortcut, just accept `temporalio.client.Interceptor`.


---

<a id="118"></a>

### #118: Do not convert history JSON enums if already done

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/118 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-30 14:42:30.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-07 22:29:16.000 UTC |
| **Closed** | 2022-09-07 22:29:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See https://github.com/temporalio/sdk-features/issues/121. We need to not convert if already converted.


---

<a id="117"></a>

### #117: [Bug] Client.get_workflow_handle_for not properly setting result type

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/117 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-30 13:55:12.000 UTC (3y 4m ago) |
| **Updated** | 2022-09-07 22:29:16.000 UTC |
| **Closed** | 2022-09-07 22:29:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Describe the bug

When using `get_workflow_handle_for` with a workflow, the return type is not extracted and set on the handle. Also allow `result_type` to be explicitly set on `get_workflow_handle` for those that know the type.


---

<a id="101"></a>

### #101: Per-call gRPC options

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/101 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-08 19:10:38.000 UTC (3y 5m ago) |
| **Updated** | 2022-08-16 22:34:36.000 UTC |
| **Closed** | 2022-08-16 22:34:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Need to be able to set timeouts and headers per gRPC call

### Describe the solution you'd like

* From the Rust side, change the macro to alter the tonic request as needed
* From the Python side, maybe have a `RpcConfig` option that could have a `timeout: Optional[timedelta]` and a `metadata: Mapping[str, str]` field (technically `Mapping[str, Iterable[str]]` matches HTTP spec, but we can add that functionality when dupe values are needed later).


---

<a id="93"></a>

### #93: Expose operator service raw grpc API

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/93 |
| **State** | CLOSED |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2022-08-03 17:42:05.000 UTC (3y 5m ago) |
| **Updated** | 2022-08-16 22:34:36.000 UTC |
| **Closed** | 2022-08-16 22:34:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

*No description provided.*


---

<a id="91"></a>

### #91: Immediate start-then-cancel not working properly

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/91 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-03 13:49:49.000 UTC (3y 5m ago) |
| **Updated** | 2022-08-16 11:54:02.000 UTC |
| **Closed** | 2022-08-04 15:53:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | cretz |
| **Milestone** | beta-1 |

#### Description

### Describe the bug

When you start-then-cancel, i.e. this event set: 
![image](https://user-images.githubusercontent.com/206396/182624437-58dc306e-7e51-494f-b8cc-9fdc8a2c3fdf.png)

then the primary task is getting cancelled before even running which means the workflow code never runs which means the cancelled event is never caught. We need to defer to `self._primary_task.cancel()` so it is put on the event loop after the start so the user's code still runs and can react to cancellation.


---

<a id="89"></a>

### #89: Timed out wait_condition futures still attempt setting result

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/89 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-08-01 19:00:12.000 UTC (3y 5m ago) |
| **Updated** | 2022-08-01 20:01:10.000 UTC |
| **Closed** | 2022-08-01 20:01:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | cretz |
| **Milestone** | None |

#### Description

### Describe the bug

When using `workflow.wait_condition`, if a wait condition times out _then_ gets a bool success, we attempt to set the future which causes an `InvalidStateError.

### Minimal Reproduction

Just timeout a wait_condition and let its callback become true


---

<a id="80"></a>

### #80: Create workflow sandbox

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/80 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-07-21 13:12:59.000 UTC (3y 5m ago) |
| **Updated** | 2022-10-28 12:22:27.000 UTC |
| **Closed** | 2022-10-28 12:22:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | beta-3 |

#### Description

### Describe the solution you'd like

We need to run workflows in a sandbox with some things disabled to prevent non-determinism. Research has already started on this. More will continue.


---

<a id="70"></a>

### #70: Properly treat activity/child cancellation errors bubbled as workflow cancel

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/70 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-07-12 20:33:05.000 UTC (3y 6m ago) |
| **Updated** | 2022-07-14 20:22:19.000 UTC |
| **Closed** | 2022-07-14 20:22:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

Workflow cancellation not reported as such when bubbling the activity cancellation. TypeScript properly converts activity error and child workflow error that are caused by cancellation into workflow cancellation when a cancellation is requested. We need to do the same.

Technically it means that if a user wraps the error the workflow will show as failed not cancelled, but there's not much we can do about that.


---

<a id="60"></a>

### #60: Interceptor header and OpenTelemetry support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/60 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-06-29 17:47:53.000 UTC (3y 6m ago) |
| **Updated** | 2022-07-21 15:21:59.000 UTC |
| **Closed** | 2022-07-21 15:21:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

* Provide headers on all interceptors where applicable
* Create `opentelemetry` module w/ optional dependency on https://opentelemetry.io/docs/instrumentation/python/ that has an interceptor for properly handling spans (see TS otel interceptor)
* Test that otel spans flow throughout and, if reasonable, also through Go


---

<a id="56"></a>

### #56: Remove gRPC dependency

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/56 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-06-22 22:38:48.000 UTC (3y 6m ago) |
| **Updated** | 2022-08-16 22:34:36.000 UTC |
| **Closed** | 2022-08-16 22:34:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | cretz |
| **Milestone** | None |
| **Reactions** | ðŸ‘€ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

We generate pure Python gRPC unnecessarily since we have Core client. 

### Describe the solution you'd like

Remove gRPC from the regular dependencies, and we can still use it as a dev dependency during tests


---

<a id="47"></a>

### #47: Properly report version

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/47 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-06-10 22:53:07.000 UTC (3y 7m ago) |
| **Updated** | 2022-07-13 11:47:35.000 UTC |
| **Closed** | 2022-07-13 11:47:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

https://github.com/temporalio/sdk-python/blob/8ffa58e03e115478a4cdc315db7cd1dc98a01df2/temporalio/bridge/client.py#L47-L48


---

<a id="45"></a>

### #45: Improve workflow stack trace query result

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/45 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-06-10 15:28:25.000 UTC (3y 7m ago) |
| **Updated** | 2022-06-10 15:28:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ðŸ‘€ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

Right now the stack traces are not very good

### Describe the solution you'd like

Make them better hopefully without unnecessary performance penalty


---

<a id="39"></a>

### #39: Add build docs to README

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/39 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-06-03 21:07:47.000 UTC (3y 7m ago) |
| **Updated** | 2022-06-09 15:53:29.000 UTC |
| **Closed** | 2022-06-09 15:53:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

It is not clear from the README how to build the repo from source for use as a dependency. We only have local build instructions.

### Describe the solution you'd like

Add a section for building from source and even an example for using after built from source. Also make it clear that users must clone recursively because `temporalio/bridge/sdk-core` is a git submodule.


---

<a id="29"></a>

### #29: Move from Sphinx API docs to pydoctor

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/29 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-05-27 20:19:41.000 UTC (3y 7m ago) |
| **Updated** | 2022-06-03 11:26:35.000 UTC |
| **Closed** | 2022-06-03 11:26:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

We already have `gen-docs-pydoctor` alternative, we just need to make that the default and remove Sphinx. This requires upstreaming support for `@overload` to pydoctor.


---

<a id="28"></a>

### #28: More workflow tests

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/28 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-05-27 20:18:16.000 UTC (3y 7m ago) |
| **Updated** | 2022-06-09 15:53:29.000 UTC |
| **Closed** | 2022-06-09 15:53:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

See the list of TODO's at the bottom of test files


---

<a id="27"></a>

### #27: Versioning/Patching

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/27 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-05-27 20:17:22.000 UTC (3y 7m ago) |
| **Updated** | 2022-06-03 17:38:13.000 UTC |
| **Closed** | 2022-06-03 17:38:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Basically https://docs.temporal.io/typescript/patching in Python


---

<a id="26"></a>

### #26: Search attributes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/26 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-05-27 20:16:01.000 UTC (3y 7m ago) |
| **Updated** | 2022-06-10 21:25:22.000 UTC |
| **Closed** | 2022-06-10 21:25:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Full search attribute support on client and inside workflows. Make sure to type the search attributes properly.


---

<a id="24"></a>

### #24: Async activity support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/24 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-05-27 20:13:54.000 UTC (3y 7m ago) |
| **Updated** | 2022-06-01 19:49:10.000 UTC |
| **Closed** | 2022-06-01 19:49:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the solution you'd like

Need async activity support in client and activity itself


---

<a id="11"></a>

### #11: Test client/worker/core behavior across fork

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/11 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-03-15 20:27:43.000 UTC (3y 10m ago) |
| **Updated** | 2022-03-15 20:27:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | â¤ï¸ 1 ðŸ‘€ 1 |

#### Description

### Is your feature request related to a problem? Please describe.

Right now, it is basically undefined how core will work across forks. We do know it fails. For example, the client var, when copied across the fork, has Rust file descriptors copied for the gRPC stuff that fail during Rust drop when the child process's version of the var is GC'd.

### Describe the solution you'd like

We need to, without leak, make sure that `fork` does not fail unless you explicitly use something from across the fork, at which point it should fail. No surprise failures like on implicit Python GC failure during drop.

I don't know how yet to do this.


---

<a id="10"></a>

### #10: Investigate more generic manylinux support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/sdk-python/issues/10 |
| **State** | CLOSED |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-03-15 20:06:27.000 UTC (3y 10m ago) |
| **Updated** | 2022-07-01 15:26:04.000 UTC |
| **Closed** | 2022-07-01 15:26:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Right now our Linux wheel is named `temporalio-0.1a1.dev1-cp37-abi3-manylinux_2_31_x86_64.whl`. This means it only works on glibc 2.31+ per https://packaging.python.org/en/latest/specifications/platform-compatibility-tags/.

### Describe the solution you'd like

We can use manylinux container to get Linux builds that work with a wider range of linuxes (e.g. `manylinux2010` or `manylinux2014`). We need to investigate the effort/value of this.


