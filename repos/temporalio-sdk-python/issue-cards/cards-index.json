{
  "generatedAt": "2026-01-22T20:30:08.622Z",
  "totalCards": 598,
  "cards": [
    {
      "summary": "Worker intermittently stops polling after completing long-running synchronous activities, causing a 5-15 minute delay before auto-reconnection. The gRPC connection drops after activity completion even though the CompleteActivity RPC succeeds, suggesting a race condition in the SDK's handling of synchronous activity completion.",
      "category": "bug",
      "subcategory": "worker-polling",
      "apis": [
        "Worker",
        "CompleteActivity"
      ],
      "components": [
        "worker",
        "activity-executor",
        "grpc-connection",
        "polling-loop"
      ],
      "concepts": [
        "race-condition",
        "connection-management",
        "activity-completion",
        "thread-pool",
        "polling",
        "reconnection"
      ],
      "severity": "high",
      "userImpact": "Activities complete successfully but workers stop polling for new tasks, causing processing delays of 5-15 minutes until automatic reconnection occurs.",
      "rootCause": "Race condition in SDK's handling of synchronous activity completion after ThreadPoolExecutor returns result and CompleteActivity RPC succeeds, preventing polling from resuming properly.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Worker stops polling - no new poll requests after activity completes, CompleteActivity RPC succeeds, but queue closes exactly 5 min after activity completion",
      "number": 1295,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-22T20:30:03.902Z"
    },
    {
      "summary": "Update the `lru` Rust dependency in the Python SDK's bridge from 0.16.0 to 0.16.3 to patch a soundness vulnerability (GHSA-rhfx-m35p-ff5j) where IterMut iterator methods violate Rust's Stacked Borrows rules.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "bridge",
        "cargo-dependencies"
      ],
      "concepts": [
        "security-vulnerability",
        "soundness",
        "dependency-management",
        "rust-safety"
      ],
      "severity": "low",
      "userImpact": "Users are exposed to a soundness vulnerability in the Rust bridge that could potentially cause undefined behavior in edge cases.",
      "rootCause": "The `IterMut` iterator's `next` and `next_back` methods temporarily create an exclusive reference to the key when dereferencing the internal node pointer, violating Rust's Stacked Borrows rules.",
      "proposedFix": "Update the `lru` dependency to version 0.16.3 or later in the Rust bridge's Cargo.lock.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Dependency updated via PR #1266",
      "related": [
        1266
      ],
      "keyQuote": "The `IterMut` iterator's `next` and `next_back` methods temporarily create an exclusive reference to the key when dereferencing the internal node pointer, violating Rust's Stacked Borrows rules.",
      "number": 1294,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-22T20:30:08.517Z"
    },
    {
      "summary": "Feature request to change temporalio.CancelledError to inherit from BaseException instead of Exception, to prevent accidental suppression of cancellation attempts by broad exception handlers. The issue highlights that Temporal cancellation errors can surface at arbitrary points in user code due to low-level C-extension API injection, making them easy to inadvertently catch with broad `except Exception` clauses.",
      "category": "feature",
      "subcategory": "exception-hierarchy",
      "apis": [
        "activity.defn",
        "activity.is_cancelled",
        "activity.wait_for_cancelled_sync"
      ],
      "components": [
        "exceptions",
        "activity-executor",
        "thread-management"
      ],
      "concepts": [
        "cancellation",
        "exception-handling",
        "thread-interruption",
        "error-semantics",
        "backward-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users migrating existing code to Temporal may inadvertently suppress cancellation attempts with broad exception handlers, causing workflows to continue running when they should be cancelled.",
      "rootCause": "CancelledError inherits from Exception rather than BaseException, allowing it to be caught by broad exception handlers that don't intend to catch cancellation errors, similar to the issue Python's asyncio.CancelledError faced.",
      "proposedFix": "Change CancelledError to inherit from BaseException instead of Exception, or allow customization of the exception type raised during thread cancellation via decorator and worker options.",
      "workaround": "Disable thread-based cancellation injection with `@activity.defn(no_thread_cancel_exception=True)` and explicitly check cancellation status using `activity.is_cancelled()` or `activity.wait_for_cancelled_sync()`.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This creates a scenario where it's easy for error handling logic in random application code to unintentionally ignore Temporal Cancellation attempts",
      "number": 1292,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-22T20:30:07.684Z"
    },
    {
      "summary": "Request to add SDK warning when workflow history exceeds a certain size threshold. This is a feature request to help developers detect potential performance issues early.",
      "category": "feature",
      "subcategory": "workflow-history",
      "apis": [],
      "components": [
        "workflow-executor",
        "history-management",
        "logging"
      ],
      "concepts": [
        "workflow-history",
        "size-limit",
        "performance-warning",
        "developer-experience",
        "debugging"
      ],
      "severity": "medium",
      "userImpact": "Developers will receive warnings when workflow history grows too large, helping them identify potential memory and performance issues earlier.",
      "rootCause": null,
      "proposedFix": "Implement SDK-level detection and warning when workflow history size exceeds a configurable threshold",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        705
      ],
      "keyQuote": "Warn if SDK detects a workflow history over a certain size",
      "number": 1289,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-22T20:29:46.865Z"
    },
    {
      "summary": "The workflow sandbox fails to restrict urllib3 version 1.26.19 when used with requests, allowing HTTP requests that should be blocked. Later versions like 2.6.2 correctly trigger sandbox restriction errors.",
      "category": "bug",
      "subcategory": "workflow-sandbox",
      "apis": [],
      "components": [
        "workflow_sandbox",
        "sandbox_restrictions"
      ],
      "concepts": [
        "sandbox",
        "import_restrictions",
        "urllib3",
        "http_requests",
        "determinism"
      ],
      "severity": "high",
      "userImpact": "Users can accidentally bypass workflow sandbox restrictions depending on their urllib3 version, allowing non-deterministic HTTP requests that could corrupt workflow state.",
      "rootCause": "urllib3 1.26.19 defers the restricted http.client.IncompleteRead access to a later point in execution, allowing it to evade sandbox detection at import time unlike version 2.6.2.",
      "proposedFix": null,
      "workaround": "Mark requests as pass-through import if intentionally using it in workflows, or use urllib3 2.6.2+ which properly triggers sandbox errors.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer confirmed sandbox is best-effort; users must explicitly mark imports as pass-through if they intend to use restricted libraries deterministically in workflows.",
      "related": [],
      "keyQuote": "The sandbox is best effort, we don't guarantee it will prevent every potential issue. Additionally, importing `requests` as pass through indicates that you are trying to use it in the workflow",
      "number": 1287,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-22T20:29:51.317Z"
    },
    {
      "summary": "SDK should fail workflow tasks when payload size exceeds known limits to prevent silent failures or partial execution.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "ExecuteWorkflow"
      ],
      "components": [
        "workflow-executor",
        "payload-validation",
        "task-processor"
      ],
      "concepts": [
        "payload-size",
        "validation",
        "error-handling",
        "limits",
        "workflow-task",
        "failure-detection"
      ],
      "severity": "high",
      "userImpact": "Workflows may silently fail or behave unexpectedly when payloads exceed size limits, requiring explicit validation logic.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": null,
      "number": 1285,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-22T20:29:47.489Z"
    },
    {
      "number": 1284,
      "summary": "Add a warning when the Python SDK attempts to send a payload that exceeds a specific size threshold. This helps developers identify and address potential issues with oversized payloads before they cause problems.",
      "category": "feature",
      "subcategory": "payload-validation",
      "apis": [],
      "components": [
        "client",
        "serialization",
        "payload-handler"
      ],
      "concepts": [
        "payload-size",
        "validation",
        "warning",
        "limits",
        "diagnostics"
      ],
      "severity": "medium",
      "userImpact": "Developers will receive warnings when attempting to send oversized payloads, helping them optimize their data transmission and avoid runtime issues.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        701
      ],
      "keyQuote": "Warn if the SDK tried to send a payload above a specific size",
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-22T20:29:30.615Z"
    },
    {
      "summary": "Unable to cancel child workflows in tests due to an invalid state transition in the activity state machine. When cancelling one child workflow while others are executing activities, the SDK core logs a fatal warning about ActivityMachine transitioning from ScheduledActivityCancelCommandCreated state upon ActivityTaskStarted event, causing the cancellation to fail silently.",
      "category": "bug",
      "subcategory": "child-workflow-cancellation",
      "apis": [
        "execute_child_workflow",
        "cancel"
      ],
      "components": [
        "workflow-engine",
        "activity-state-machine",
        "child-workflow-handler"
      ],
      "concepts": [
        "cancellation",
        "state-machine",
        "race-condition",
        "activity-execution",
        "workflow-coordination"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably test child workflow cancellation scenarios as the cancellation operation fails silently with state machine warnings, making it impossible to verify correct behavior when cancelling individual child workflows.",
      "rootCause": "Activity state machine receives ActivityTaskStarted event while in ScheduledActivityCancelCommandCreated state, which is an invalid transition. This appears to be a race condition where activity cancellation and activity start events occur in an unexpected order during concurrent child workflow execution.",
      "proposedFix": null,
      "workaround": "Ensure cancelled child workflows do not complete before cancellation is requested, or upgrade to SDK version 1.21.0+ where the issue has reduced occurrence rate. Remove early status checks that create race conditions in test code.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "ActivityMachine in state ScheduledActivityCancelCommandCreated says the transition is invalid during event EventInfo { event_id: 10, event_type: ActivityTaskStarted }",
      "number": 1280,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-22T20:29:34.768Z"
    },
    {
      "summary": "The Python SDK currently uses pydoctor for API documentation generation, but it has limitations in handling dataclasses and is unmaintained. The team should evaluate alternative API doc generators like pdoc or other ecosystem options to improve documentation quality.",
      "category": "feature",
      "subcategory": "documentation-tooling",
      "apis": [],
      "components": [
        "documentation-generation",
        "api-docs",
        "build-system"
      ],
      "concepts": [
        "documentation",
        "api-reference",
        "tooling",
        "maintenance",
        "dataclass-support",
        "overload-handling"
      ],
      "severity": "medium",
      "userImpact": "Users may encounter incomplete or inaccurate API documentation when reference docs are missing fields or misrepresent dataclass attributes.",
      "rootCause": "pydoctor lacks proper understanding of Python dataclasses and is not actively maintained, leading to missing fields and incorrect rendering of class structures.",
      "proposedFix": "Evaluate and migrate to alternative API documentation generators such as pdoc.dev or other actively maintained tools that better support modern Python patterns.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It doesn't really get updated much and is a bit buggy...missing `default_versioning_behavior` that is in source with a default.",
      "number": 1275,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-22T20:29:31.384Z"
    },
    {
      "summary": "Worker pollers randomly drop to 0 despite available task slots and explicit poller configuration, causing workflow and activity task timeouts with no logged errors. This occurs across all poller behavior configurations and randomly affects workers 10-20 minutes after startup.",
      "category": "bug",
      "subcategory": "poller-behavior",
      "apis": [
        "Worker",
        "PollerBehaviorSimpleMaximum",
        "PollerBehaviorAutoscaling"
      ],
      "components": [
        "worker",
        "poller-management",
        "task-scheduling",
        "metrics"
      ],
      "concepts": [
        "poller-behavior",
        "task-slots",
        "autoscaling",
        "worker-reliability",
        "silent-failure",
        "configuration-ignored"
      ],
      "severity": "critical",
      "userImpact": "Workflows and activities fail with timeouts due to workers stopping polling, while appearing healthy with no error logs.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Use PollerBehaviorSimpleMaximum instead of autoscaling, run multiple workers to reduce simultaneous failure impact, or periodically restart workers to reset pollers.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        877
      ],
      "keyQuote": "With 14 available slots, there should be active pollers. Workers start healthy with expected poller counts, then after 10-20 minutes, pollers drop to 0.",
      "number": 1268,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:37:28.033Z"
    },
    {
      "summary": "Logs emitted within workflow query functions are being dropped even when using the replay-safe logger. The issue appears to be that `workflow.unsafe.is_replaying()` always returns true during query execution, causing the logger to discard all log messages.",
      "category": "bug",
      "subcategory": "logging",
      "apis": [
        "workflow.logger",
        "workflow.query",
        "workflow.unsafe.is_replaying"
      ],
      "components": [
        "logger",
        "query-handler",
        "replay-detection"
      ],
      "concepts": [
        "logging",
        "replay-safety",
        "query-execution",
        "side-effects",
        "log-filtering"
      ],
      "severity": "medium",
      "userImpact": "Users cannot emit logs from query functions, limiting debugging and observability for query logic.",
      "rootCause": "The is_replaying() function returns true during query context, causing the replay-safe logger to drop logs as an unintended side effect.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It's intentional that `is_replaying` is true, but the side effect of not emitting logs isn't.",
      "number": 1267,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:37:17.907Z"
    },
    {
      "summary": "Add a method to obtain ApplicationError details with type hints for deserialized errors. Currently, details are only available in raw Python form (dicts, etc.); users need a typed accessor.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [
        "ApplicationError"
      ],
      "components": [
        "error-handling",
        "deserialization",
        "exceptions"
      ],
      "concepts": [
        "type-hints",
        "error-details",
        "deserialization",
        "type-safety",
        "accessor-methods"
      ],
      "severity": "medium",
      "userImpact": "Users working with ApplicationError details lack convenient typed access to deserialized error information.",
      "rootCause": null,
      "proposedFix": "Make details property lazy, add property for raw details access, and add method to access detail by index with type hint",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "For users of deserialized ApplicationError, today we only offer the deserialized details in raw Python form",
      "number": 1262,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:37:14.484Z"
    },
    {
      "summary": "The openai-agents dependency in the Python SDK is pinned to version <0.5, limiting it to 0.4.2, which prevents access to newer versions with bug fixes and support for GPT 5.1 and 5.2 models.",
      "category": "bug",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "pyproject.toml",
        "openai-agents-plugin",
        "dependency-resolver"
      ],
      "concepts": [
        "dependency-versioning",
        "gpt-models",
        "compatibility",
        "bug-fixes",
        "openai-integration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot access the latest openai-agents features, bug fixes, and support for newer GPT models due to version constraints.",
      "rootCause": "The version constraint in pyproject.toml (openai-agents>=0.3,<0.5) is too restrictive, capping at 0.4.2 instead of allowing newer 0.5+ versions.",
      "proposedFix": "Update the openai-agents version constraint in pyproject.toml to allow newer versions that include bug fixes and GPT 5.1/5.2 model support.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The dependency constraint was updated to allow newer versions of openai-agents.",
      "related": [],
      "keyQuote": "the temporal openai agents plugin dependency is about 2 minor versions behind",
      "number": 1255,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:37:16.556Z"
    },
    {
      "summary": "The `SandboxImportNotificationPolicy.WARN_ON_UNINTENTIONAL_PASSTHROUGH` policy incorrectly warns when workflow modules are imported into the sandbox during execution, even though these imports are necessary and intentional for the sandboxed workflow runner to function.",
      "category": "bug",
      "subcategory": "workflow-sandbox",
      "apis": [
        "SandboxedWorkflowRunner",
        "SandboxRestrictions",
        "SandboxImportNotificationPolicy"
      ],
      "components": [
        "workflow-sandbox",
        "importer",
        "sandbox-restrictions"
      ],
      "concepts": [
        "sandbox-isolation",
        "import-policy",
        "module-loading",
        "false-positives",
        "workflow-execution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use `WARN_ON_UNINTENTIONAL_PASSTHROUGH` policy effectively because it generates false warnings for legitimate workflow module imports.",
      "rootCause": "The sandbox import notification policy doesn't distinguish between intentional workflow module loads (required for sandbox execution) and unintentional passthrough imports.",
      "proposedFix": "Exempt workflow modules from the unintentional passthrough warning since they must be loaded into the sandbox for execution to occur.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The workflow must be loaded into the sandbox each time and therefore should be exempt from this warning.",
      "number": 1254,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:58.510Z"
    },
    {
      "summary": "Feature request to enable sync activities running on ThreadPoolExecutor to access the activity client. Currently, the client is only available in async activities, but the requester argues there's no technical reason to restrict it for thread pool-based sync activities since they don't have the pickling issues of process pool executors.",
      "category": "feature",
      "subcategory": "activity-client",
      "apis": [
        "activity.client",
        "activity.info"
      ],
      "components": [
        "activity-executor",
        "sync-activities",
        "client"
      ],
      "concepts": [
        "thread-pool-executor",
        "async-interop",
        "activity-progress",
        "concurrency",
        "event-loop"
      ],
      "severity": "medium",
      "userImpact": "Users cannot implement progress updates in sync activities run via ThreadPoolExecutor because they lack client access, forcing them to use only async activities or find workarounds.",
      "rootCause": "Client is tied to the main event loop and reusing it from a different thread would cause concurrency issues with asyncio synchronization primitives. The restriction applies uniformly to all sync activities regardless of executor type.",
      "proposedFix": "Allow client access in sync activities when using ThreadPoolExecutor by ensuring proper event loop handling, or provide guidance on creating a new client in the worker thread.",
      "workaround": "Users can create a new client within the sync activity worker thread, or implement progress tracking through other mechanisms not requiring client access.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer explained that reusing a client from a different event loop creates concurrency issues with asyncio primitives. The requester accepted this explanation and understood the architectural constraint.",
      "related": [],
      "keyQuote": "Because the client was created on a different event loop than the one you would be producing inside your sync activity. This potentially leads to concurrency errors since asyncio synchronization is not guaranteed across multiple threads.",
      "number": 1252,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:37:03.270Z"
    },
    {
      "summary": "Python SDK fails intermittently when downloading the Temporal test server from https://temporal.download, resulting in RuntimeError. The issue appears to be network-related and non-deterministic, sometimes working fine and other times failing.",
      "category": "bug",
      "subcategory": "test-server-download",
      "apis": [
        "WorkflowEnvironment.start_time_skipping",
        "Worker",
        "execute_workflow"
      ],
      "components": [
        "test-server",
        "download-manager",
        "workflow-environment",
        "worker"
      ],
      "concepts": [
        "network-request",
        "intermittent-failure",
        "server-download",
        "http-error",
        "test-infrastructure",
        "initialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably run tests with the Temporal test server, blocking test execution workflows.",
      "rootCause": "Network request to temporal.download endpoint fails intermittently, potentially due to server-side issues (500 errors reported) or network connectivity problems.",
      "proposedFix": null,
      "workaround": "Manually download the test server and configure WorkflowEnvironment to use the local binary.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "RuntimeError: Failed starting test server: error sending request for url (https://temporal.download/temporal-test-server/default?arch=amd64&platform=linux&sdk-name=sdk-python&sdk-version=1.17.0)",
      "number": 1250,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:59.969Z"
    },
    {
      "summary": "GitHub Actions Build Binaries workflow fails when the Temporal SDK pins an older version of nexus-rpc than the latest available. The workflow installs the latest nexus-rpc version during testing, causing dependency mismatches that break the wheel testing step.",
      "category": "bug",
      "subcategory": "ci-cd",
      "apis": [],
      "components": [
        "build-binaries",
        "github-actions",
        "wheel-testing",
        "dependency-management"
      ],
      "concepts": [
        "dependency-version-mismatch",
        "ci-cd-workflow",
        "virtual-environment",
        "package-constraints",
        "testing"
      ],
      "severity": "high",
      "userImpact": "Developers cannot merge changes to the SDK when working with older pinned versions of nexus-rpc, blocking release workflows.",
      "rootCause": "The Build Binaries GitHub Actions workflow installs nexus-rpc with a loose constraint (>=1.1.0) that resolves to the latest available version, conflicting with SDK pinned dependencies during wheel testing.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the nexus-rpc version constraint or workflow logic was updated to handle pinned dependency versions.",
      "related": [],
      "keyQuote": "The Build Binaries action installs the latest version of nexus-rpc (via `nexus-rpc>=1.1.0`) when testing the built wheel in a new virtual environment. If the Temporal SDK relies on an older version, the build binaries job fails",
      "number": 1244,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:45.943Z"
    },
    {
      "summary": "The `page_size` parameter in `WorkflowHistoryEventAsyncIterator.fetch_next_page()` is ignored and not used during pagination. This prevents users from controlling the batch size of history events retrieved.",
      "category": "bug",
      "subcategory": "workflow-history",
      "apis": [
        "WorkflowHistoryEventAsyncIterator"
      ],
      "components": [
        "workflow-history",
        "async-iterator",
        "pagination"
      ],
      "concepts": [
        "pagination",
        "batch-size",
        "page-size",
        "history-events",
        "async-iteration",
        "parameter-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot control the page size when fetching workflow history events, limiting their ability to optimize memory usage and performance for large histories.",
      "rootCause": "The `fetch_next_page()` method does not pass or utilize the `page_size` argument when constructing pagination requests.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "`WorkflowHistoryEventAsyncIterator.fetch_next_page` never uses the `page_size` argument.",
      "number": 1239,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:43.926Z"
    },
    {
      "summary": "The `create_schedule` method in the Python SDK accepts `static_summary` and `static_details` parameters but never uses them when constructing the CreateScheduleInput. These parameters should either be forwarded to the API call or removed.",
      "category": "bug",
      "subcategory": "schedule-creation",
      "apis": [
        "create_schedule",
        "CreateScheduleInput"
      ],
      "components": [
        "schedule-client",
        "api-binding"
      ],
      "concepts": [
        "parameter-handling",
        "api-completeness",
        "static-metadata"
      ],
      "severity": "medium",
      "userImpact": "Users who pass static_summary and static_details to create_schedule will have these values silently ignored, potentially leading to incomplete schedule metadata.",
      "rootCause": "The static_summary and static_details parameters are accepted in the method signature but not passed to CreateScheduleInput during construction.",
      "proposedFix": "Remove the unused static_summary and static_details parameters from the create_schedule method signature, as the server API doesn't support them and details are included as part of the action embedded in the schedule.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Parameters were removed from the API as they are not supported by the server API and details are included as part of the action embedded in the schedule.",
      "related": [],
      "keyQuote": "The server API doesn't include it, the details are included as part of the action embedded in the schedule. Should be removed from the API.",
      "number": 1238,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:42.234Z"
    },
    {
      "summary": "The converter incorrectly handles `dict[None, Any]` types, returning `{'null': \"1\"}` which doesn't match the expected return object structure when converting dictionaries with None keys.",
      "category": "bug",
      "subcategory": "converter",
      "apis": [],
      "components": [
        "converter",
        "type-conversion",
        "test-framework"
      ],
      "concepts": [
        "type-mapping",
        "null-handling",
        "dict-conversion",
        "serialization"
      ],
      "severity": "medium",
      "userImpact": "Users may encounter incorrect value conversion when working with dictionaries that have None as keys, leading to unexpected data structures.",
      "rootCause": "The converter appears to be mapping None keys to the string 'null' instead of properly handling None as a dictionary key type.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "In `test_converter.py`, `ok(dict[None, str], {'null': \"1\"})` passes, but that doesn't really seem like the correct return object.",
      "number": 1237,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:30.326Z"
    },
    {
      "summary": "Feature request to add BasedPyright as a required type checker for the Python SDK codebase as it gains user adoption.",
      "category": "feature",
      "subcategory": "type-checking",
      "apis": [],
      "components": [
        "type-checker",
        "ci-pipeline",
        "development-tools"
      ],
      "concepts": [
        "type-safety",
        "static-analysis",
        "code-quality",
        "validation",
        "tooling"
      ],
      "severity": "low",
      "userImpact": "Users would benefit from improved type safety and code quality assurance in the Python SDK through comprehensive type checking.",
      "rootCause": null,
      "proposedFix": "Integrate BasedPyright as a required type check in the development workflow and CI/CD pipeline.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the feature was implemented and BasedPyright was added as a required type checker.",
      "related": [],
      "keyQuote": "The codebase should pass checks from BasedPyright as it gains user adoption.",
      "number": 1232,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:26.155Z"
    },
    {
      "summary": "Request to support executing activities outside of workflow context. Clients need new server APIs for starting, managing, and retrieving activity results, while workers must be enhanced to run activities independently.",
      "category": "feature",
      "subcategory": "activity-execution",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "client",
        "activity-manager"
      ],
      "concepts": [
        "activities",
        "non-workflow-execution",
        "activity-management",
        "result-retrieval",
        "server-apis"
      ],
      "severity": "medium",
      "userImpact": "Enables users to execute and manage activities independently without requiring a workflow context, expanding use cases for activity-based processing.",
      "rootCause": null,
      "proposedFix": "Implement client support for new server APIs for activity lifecycle management and worker support for running activities outside workflow context.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        640
      ],
      "keyQuote": "Allow executing activities without a workflow. The clients must support the new (upcoming) server APIs for starting, managing and getting results of activities.",
      "number": 1230,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:27.954Z"
    },
    {
      "summary": "Client.config() and Worker.config() should return immutable copies of the exact kwargs passed to their constructors, not modified versions with combined plugin lists, to allow config reuse patterns like `new_config = old_worker.config() + Worker(**new_config)`.",
      "category": "bug",
      "subcategory": "configuration",
      "apis": [
        "Client",
        "Worker"
      ],
      "components": [
        "client",
        "worker",
        "configuration",
        "plugin-system"
      ],
      "concepts": [
        "immutability",
        "kwargs",
        "configuration-copy",
        "plugin-mutation",
        "constructor-params"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably reuse config objects from Client/Worker instances because the stored config includes plugin-mutated values that differ from original kwargs.",
      "rootCause": "Client.config() and Worker.config() store plugin-mutated configuration instead of the exact kwargs passed to the constructor.",
      "proposedFix": "Store immutable copies of the exact kwargs passed to constructor and return those from config(), excluding plugin-combined values.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation likely addressed to return exact kwargs as immutable copies without plugin mutations.",
      "related": [
        1157
      ],
      "keyQuote": "Config for most of the options needs to represent the configuration passed in (or mutated by plugins). People use config to change-and-resplat.",
      "number": 1225,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:13.357Z"
    },
    {
      "summary": "Workflow cancellation race condition causes CancelledError to not be raised reliably (~10% of the time), preventing cleanup handlers from executing when workflows are cancelled between activity completion and workflow task start.",
      "category": "bug",
      "subcategory": "cancellation-handling",
      "apis": [
        "workflow.execute_activity",
        "workflow.cancel",
        "is_cancelled_exception"
      ],
      "components": [
        "cancellation-scope",
        "workflow-task-handler",
        "exception-propagation"
      ],
      "concepts": [
        "race-condition",
        "cancellation",
        "cleanup",
        "exception-handling",
        "task-boundary",
        "signal-alternative",
        "asyncio"
      ],
      "severity": "high",
      "userImpact": "Critical cleanup code (notifications, state cleanup, event logging) fails to execute when workflows are cancelled, leaving systems in inconsistent states.",
      "rootCause": "Race condition where Temporal marks workflow as cancelled before CancelledError can be raised during the workflow task when cancellation arrives at task boundaries.",
      "proposedFix": "Use signals to move workflow into cleanup state rather than relying on exception propagation, or ensure CancelledError is raised during workflow task execution.",
      "workaround": "Use signals instead of try/except patterns to handle cancellation and trigger cleanup state transitions.",
      "resolution": "wontfix",
      "resolutionDetails": "SDK maintainer confirmed this is a known limitation of asyncio-based exception propagation in Python. Recommended using signals pattern instead, which is more reliable for cleanup in Temporal workflows.",
      "related": [],
      "keyQuote": "no CancelledError exception was raised during the workflow task execution, so my except block never ran to call handle_cancellation()",
      "number": 1224,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:15.575Z"
    },
    {
      "summary": "PyCharm IDE doesn't recognize type hints for no-argument signal handlers in WorkflowHandle.signal(). A proposed overload using Callable type annotation would improve IDE support, though the maintainer noted this may be a PyCharm bug rather than an SDK limitation.",
      "category": "feature",
      "subcategory": "type-hints",
      "apis": [
        "signal"
      ],
      "components": [
        "client",
        "WorkflowHandle",
        "typing"
      ],
      "concepts": [
        "type-checking",
        "IDE-support",
        "overload",
        "PyCharm",
        "type-hints"
      ],
      "severity": "low",
      "userImpact": "PyCharm users experience incorrect IDE type checking warnings when calling signal() with no-argument signal handlers, though the code runs correctly.",
      "rootCause": "PyCharm's type inference doesn't recognize the existing Callable[SelfType, None] protocol as matching the no-argument signal pattern, while PyRight handles it correctly.",
      "proposedFix": "Add an overload to WorkflowHandle.signal() that explicitly accepts Callable[SelfType, None] parameters with optional rpc_metadata and rpc_timeout arguments.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer determined the proposed overload doesn't expand valid type ranges as Callable already fits MethodSyncOrAsyncNoParam protocol, suggesting this is a PyCharm bug rather than an SDK issue.",
      "related": [],
      "keyQuote": "That would indicate it is a PyCharm bug, not one for which we should introduce a new overload for all users.",
      "number": 1220,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:36:11.508Z"
    },
    {
      "summary": "Python worker crashes with 'must only be set once' panic when TaskLocals are initialized twice, typically caused by calling both the async context manager and manually running the worker.",
      "category": "bug",
      "subcategory": "worker-initialization",
      "apis": [
        "Worker",
        "Client.connect"
      ],
      "components": [
        "worker",
        "task-locals",
        "python-bridge"
      ],
      "concepts": [
        "initialization",
        "task-locals",
        "context-manager",
        "concurrency",
        "resource-cleanup",
        "panic"
      ],
      "severity": "high",
      "userImpact": "Users following the quickstart guide get a panic crash when using the Worker context manager incorrectly, blocking the worker process and hanging the client connection.",
      "rootCause": "TaskLocals being set multiple times when both the async context manager runs the worker and the user manually calls await worker.run().",
      "proposedFix": "Remove the manual await worker.run() call and rely solely on the async context manager.",
      "workaround": "Do not call await worker.run() when using the async context manager pattern; use the context manager for cleanup.",
      "resolution": "fixed",
      "resolutionDetails": "User corrected their code by removing the redundant worker.run() call. The issue was user error rather than a SDK bug, but the error message could be clearer.",
      "related": [],
      "keyQuote": "The context manager is really just for use when you have things you want to do with that worker...if you just want the process to be a perpetual worker, there's no reason to use the context manager.",
      "number": 1215,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:58.290Z"
    },
    {
      "summary": "WorkflowExecutionStatus enum needs a method to convert to PascalCase format required by search attributes. Currently, the enum values cannot be directly used for filtering search attributes that expect PascalCase.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [
        "WorkflowExecutionStatus"
      ],
      "components": [
        "search-attributes",
        "workflow-execution",
        "enum-conversion"
      ],
      "concepts": [
        "case-conversion",
        "search-filtering",
        "enum-formatting",
        "search-attributes",
        "type-conversion"
      ],
      "severity": "medium",
      "userImpact": "Users must manually convert WorkflowExecutionStatus values to PascalCase when filtering search attributes, creating a friction point in the API.",
      "rootCause": "WorkflowExecutionStatus enum values are in a different format than the PascalCase strings expected by search attribute filtering APIs.",
      "proposedFix": "Add a method to convert WorkflowExecutionStatus to PascalCase format compatible with search attributes.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "A conversion method was added to handle WorkflowExecutionStatus to PascalCase conversion for search attributes.",
      "related": [],
      "keyQuote": "When filtering for search attributes ExecutionStatus excepts PascalCase but the WorkflowExecutionStatus cannot be converted as is right now.",
      "number": 1212,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:58.854Z"
    },
    {
      "summary": "Users need the ability to override the Worker Deployment Version when invoking a Child Workflow, similar to how it works when creating workflows from a client. This feature would support pre-deployment testing scenarios.",
      "category": "feature",
      "subcategory": "child-workflow-deployment",
      "apis": [
        "ChildWorkflow",
        "Worker"
      ],
      "components": [
        "worker",
        "child-workflow",
        "deployment-versioning"
      ],
      "concepts": [
        "deployment-version",
        "pre-deployment-testing",
        "workflow-invocation",
        "version-override",
        "child-workflows"
      ],
      "severity": "medium",
      "userImpact": "Users cannot test pre-deployment scenarios with child workflows by overriding deployment versions, limiting their testing capabilities.",
      "rootCause": null,
      "proposedFix": "Implement version override capability for child workflow invocations, mirroring the existing functionality available in client-side workflow creation.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Users want to override the target version on a workflow, especially when doing pre-deployment testing. This should work the same as it does when creating a Workflow from a client.",
      "number": 1209,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:55.341Z"
    },
    {
      "summary": "User requests a convenient way to extract workflow failure reasons and current state information on the client side without needing to dig through history or handle exceptions. Currently requires either inspecting history or parsing exceptions.",
      "category": "feature",
      "subcategory": "workflow-handle-api",
      "apis": [
        "WorkflowHandle.result",
        "WorkflowHandle.describe"
      ],
      "components": [
        "workflow-handle",
        "client-api",
        "workflow-state"
      ],
      "concepts": [
        "failure-handling",
        "state-information",
        "client-query",
        "workflow-metadata",
        "current-status"
      ],
      "severity": "low",
      "userImpact": "Users must implement complex workarounds to expose workflow failure reasons and current state to client applications for UI display or decision-making.",
      "rootCause": "No direct API method exists to query current workflow state and failure details from the client; users must use queries or inspect history.",
      "proposedFix": null,
      "workaround": "Use `upsert_memo` inside the workflow and `handle.describe().memo()` on the client side to share state information.",
      "resolution": "wontfix",
      "resolutionDetails": "The SDK team clarified that failure/success is only available via result or history, and recommended using custom queries. A related server-side feature (temporal/temporal#8608) was opened to provide this info on describe.",
      "related": [
        8608
      ],
      "keyQuote": "You can write your own query that can return anything you want too and it can be called even after a workflow is closed.",
      "number": 1207,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:41.927Z"
    },
    {
      "summary": "The `retry_policy` field is not populated in `activity.info()` even when it's explicitly specified in the `workflow.execute_activity()` call. This prevents activities from accessing retry configuration, including non-retryable error types, despite the policy being defined at the workflow level.",
      "category": "bug",
      "subcategory": "activity-execution",
      "apis": [
        "execute_activity",
        "activity.info",
        "RetryPolicy"
      ],
      "components": [
        "activity-executor",
        "activity-context",
        "retry-policy-handling"
      ],
      "concepts": [
        "retry-policy",
        "activity-info",
        "non-retryable-errors",
        "activity-context",
        "execution-configuration",
        "policy-propagation"
      ],
      "severity": "medium",
      "userImpact": "Activities cannot access retry policy configuration at runtime, preventing them from making decisions based on configured retry behavior or non-retryable error types.",
      "rootCause": "The retry_policy is not being passed through to the activity context when activities are executed, despite being specified in the execute_activity call.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "retry_policy isn't set even though it's set on the workflow.execute_activity call.",
      "number": 1203,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:42.876Z"
    },
    {
      "summary": "Feature request to provide an unsafe_close() method on ServiceClient for advanced users who need explicit control over TCP connection closure timing, without relying on garbage collection.",
      "category": "feature",
      "subcategory": "client-lifecycle",
      "apis": [
        "ServiceClient"
      ],
      "components": [
        "client",
        "connection-management",
        "rust-interop"
      ],
      "concepts": [
        "resource-cleanup",
        "connection-lifecycle",
        "explicit-disposal",
        "tcp-socket",
        "gc-independence"
      ],
      "severity": "low",
      "userImpact": "Advanced users gain explicit control over client connection closure timing for performance-critical scenarios.",
      "rootCause": null,
      "proposedFix": "Implement ServiceClient.unsafe_close() method with Rust-level resource management (e.g., Option::take pattern), clearly documented as unsafe for production use.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Internal discussion led to alternative approach: server-side changes to provide clean exit mechanism instead of unsafe client-side closure.",
      "related": [],
      "keyQuote": "we'll be looking to provide a way to cleanly exit powered by some server side changes",
      "number": 1202,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:44.028Z"
    },
    {
      "summary": "Add safeguards to prevent accidental Runtime misuse across forks by preventing creation with runtimes from different processes and allowing users to opt-out of default runtime creation.",
      "category": "feature",
      "subcategory": "runtime-lifecycle",
      "apis": [
        "Runtime.default",
        "Runtime.prevent_default"
      ],
      "components": [
        "runtime",
        "client",
        "worker",
        "test-framework"
      ],
      "concepts": [
        "process-fork",
        "runtime-isolation",
        "default-initialization",
        "multi-process",
        "safety",
        "opt-out"
      ],
      "severity": "medium",
      "userImpact": "Users working with forked processes or advanced configurations could accidentally misuse Runtimes, leading to hard-to-debug failures.",
      "rootCause": null,
      "proposedFix": "Implement process fork detection similar to Ruby SDK PR #301, and add Runtime.prevent_default() method to block lazy default runtime creation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed after implementation of fork detection and prevent_default() mechanism.",
      "related": [],
      "keyQuote": "Prevent creating things with a runtime created in a different process... Allow users to opt-out of default runtime use",
      "number": 1201,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:26.852Z"
    },
    {
      "summary": "Enable worker heartbeating in the Python SDK by allowing users to specify the worker heartbeat interval when initializing Core Runtime. Worker heartbeating is already implemented in Core and needs to be exposed through the SDK API.",
      "category": "feature",
      "subcategory": "worker-heartbeat",
      "apis": [],
      "components": [
        "worker",
        "core-runtime",
        "heartbeat"
      ],
      "concepts": [
        "heartbeat",
        "worker-health",
        "connection-monitoring",
        "core-integration",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to configure worker heartbeat intervals to ensure worker health monitoring and detect stalled workers.",
      "rootCause": null,
      "proposedFix": "Expose worker heartbeat interval configuration parameter in Core Runtime initialization API",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented and merged into the SDK",
      "related": [],
      "keyQuote": "Worker heartbeating is implemented in Core, need to enable worker heartbeating by allowing users to specify the worker heartbeat interval when initializing Core Runtime",
      "number": 1196,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:27.432Z"
    },
    {
      "summary": "User inquired whether the Python SDK supports setting the Temporal-Namespace header on namespace-specific gRPC requests, as described in the features repository. The feature was confirmed to exist since version 1.7.0.",
      "category": "question",
      "subcategory": "grpc-headers",
      "apis": [],
      "components": [
        "grpc-client",
        "namespace-handling"
      ],
      "concepts": [
        "namespace-header",
        "grpc-requests",
        "sdk-feature"
      ],
      "severity": "low",
      "userImpact": "Users needed clarification on whether the Python SDK supports namespace headers in gRPC requests and which version introduced this functionality.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Confirmed that the feature exists in Python SDK version 1.7.0 and has been present for over a year.",
      "related": [
        475
      ],
      "keyQuote": "Yes, it does.",
      "number": 1194,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:28.617Z"
    },
    {
      "summary": "When a custom codec is used for encryption, activity failures with details are not properly decoded in the workflow, causing a KeyError ('Unknown payload encoding binary/encrypted') and workflow timeouts. The issue occurs during failure conversion when the workflow tries to process an encrypted activity failure.",
      "category": "bug",
      "subcategory": "codec-encryption",
      "apis": [
        "ExecuteActivity",
        "ActivityError"
      ],
      "components": [
        "converter",
        "failure-converter",
        "worker",
        "codec"
      ],
      "concepts": [
        "encryption",
        "codec",
        "failure-handling",
        "payload-encoding",
        "deserialization",
        "error-propagation"
      ],
      "severity": "high",
      "userImpact": "Workflows using custom encryption codecs cannot handle activity failures, causing workflows to hang and timeout when activities fail.",
      "rootCause": "The payload converter's from_payloads method does not recognize the 'binary/encrypted' encoding produced by custom codecs when decoding failure details from activities.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating it was resolved in a subsequent SDK version.",
      "related": [],
      "keyQuote": "KeyError: 'Unknown payload encoding binary/encrypted' - The from_payloads method fails to decode encrypted payloads in failure details.",
      "number": 1191,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:12.738Z"
    },
    {
      "summary": "OpenTelemetry spans exported by TracingInterceptor lack workflow input/output attributes, only showing workflow/run IDs. Users want serialized input and output values visible in downstream OTEL backends like Langfuse.",
      "category": "feature",
      "subcategory": "opentelemetry-tracing",
      "apis": [
        "TracingInterceptor"
      ],
      "components": [
        "opentelemetry-contrib",
        "tracing-interceptor",
        "span-export"
      ],
      "concepts": [
        "observability",
        "tracing",
        "span-attributes",
        "telemetry",
        "workflow-metadata",
        "serialization"
      ],
      "severity": "medium",
      "userImpact": "Users integrating with OpenTelemetry backends cannot see what workflows were invoked or what results they produced, limiting observability and debugging capabilities.",
      "rootCause": "TracingInterceptor does not extract and attach workflow input/output payloads to the exported spans despite having access to them from the Temporal history.",
      "proposedFix": null,
      "workaround": "Users can create their own interceptor to add this information to spans, or emit custom spans with the desired attributes.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers declined to add this as built-in behavior due to security concerns about exposing sensitive data to downstream systems and compatibility guarantees. Recommended users implement custom interceptors instead.",
      "related": [],
      "keyQuote": "It will often be surprisingly dangerous to output that much information to the tracing system. If you want to do this, we would currently recommend your own interceptor to add this information",
      "number": 1190,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:10.940Z"
    },
    {
      "summary": "In Python 3.13, using isinstance() with typing.Literal in the converter.py value_to_type function raises TypeError because subscripted generics cannot be used with class and instance checks. The function attempts isinstance(key, key_type) where key_type is typing.Literal, failing on dict conversions with Literal-typed keys.",
      "category": "bug",
      "subcategory": "type-conversion",
      "apis": [],
      "components": [
        "converter",
        "value_to_type",
        "payload-conversion"
      ],
      "concepts": [
        "type-checking",
        "isinstance",
        "typing.Literal",
        "Python-3.13-compatibility",
        "generic-types",
        "type-validation"
      ],
      "severity": "high",
      "userImpact": "Workflows fail to deserialize data containing dictionaries with typing.Literal-typed keys when running on Python 3.13.",
      "rootCause": "The code uses isinstance(key, key_type) directly with typing.Literal types. Python 3.13 disallows isinstance() checks on subscripted generic types like Literal, raising TypeError: 'Subscripted generics cannot be used with class and instance checks'.",
      "proposedFix": "Add type checking to prevent isinstance() from being called with subscripted generics. Check if key_type is a special typing construct (like Literal) before attempting isinstance() validation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by adding proper type checking to handle subscripted generics in the isinstance validation logic.",
      "related": [],
      "keyQuote": "In Python 3.13, using isinstance() with subscripted generics like typing.Literal is no longer allowed and raises a TypeError: Subscripted generics cannot be used with class and instance checks.",
      "number": 1188,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:35:14.838Z"
    },
    {
      "summary": "The _TemporalModelStub.get_response() method in the OpenAI agents plugin lacks default values for optional parameters (conversation_id, previous_response_id, prompt), causing TypeError despite being marked as Optional. A previous fix (PR #1092) added the parameters but didn't provide defaults, leaving them functionally required.",
      "category": "bug",
      "subcategory": "openai-agents-integration",
      "apis": [],
      "components": [
        "openai_agents",
        "_TemporalModelStub",
        "get_response",
        "contrib"
      ],
      "concepts": [
        "optional-parameters",
        "type-hints",
        "default-values",
        "backwards-compatibility",
        "integration-plugin",
        "method-signature"
      ],
      "severity": "high",
      "userImpact": "Users cannot use the OpenAI agents integration plugin with the Python SDK as get_response() throws TypeError when called without explicitly passing optional parameters.",
      "rootCause": "Parameters added to method signature without default values, causing them to be functionally required at runtime despite Optional type annotation.",
      "proposedFix": "Add default values (= None) to optional parameters: previous_response_id: Optional[str] = None, conversation_id: Optional[str] = None, prompt: Optional[ResponsePromptParam] = None",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Closed as invalid - issue was determined to be in openai-agents library, not the SDK. OpenAI Agents should be passing conversation_id parameter.",
      "related": [
        1091,
        1092
      ],
      "keyQuote": "the fix was incomplete - the parameters were added without default values.",
      "number": 1186,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:58.262Z"
    },
    {
      "summary": "Feature request to allow direct invocation of decorated activity functions within workflows (syntactic sugar over execute_activity), similar to Prefect's pattern. The request was closed with a detailed explanation of why the Python SDK intentionally requires explicit activity invocation for clarity and flexibility.",
      "category": "feature",
      "subcategory": "activity-invocation",
      "apis": [
        "execute_activity",
        "start_activity",
        "execute_local_activity",
        "start_local_activity"
      ],
      "components": [
        "activity-decorator",
        "workflow-execution",
        "activity-invocation"
      ],
      "concepts": [
        "syntactic-sugar",
        "decorator-pattern",
        "ergonomics",
        "explicitness",
        "determinism",
        "activity-options",
        "activity-semantics"
      ],
      "severity": "low",
      "userImpact": "Users unfamiliar with Temporal's design patterns expect decorated activities to be directly callable within workflows, leading to confusion and perceived boilerplate.",
      "rootCause": "Intentional API design decision to maintain explicit activity invocation for clarity, consistency, and flexibility in supporting multiple invocation modes (start vs execute, local vs non-local).",
      "proposedFix": "Create a custom decorator/utility outside the SDK that wraps activities to support both awaited and non-awaited invocation patterns with pre-configured options.",
      "workaround": "Users can create custom decorators/utilities to provide more ergonomic activity invocation if needed, as suggested in the maintainer response.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer explained that explicit activity invocation is intentional: it communicates intent, supports multiple invocation modes, allows flexible option specification at call sites, and maintains consistency with workflow invocation patterns. The design was deliberately backed away from in Java/TypeScript after initial experiments.",
      "related": [],
      "keyQuote": "We don't want activity invocation to look like regular function invocation. You need to understand that you are invoking an activity and not just calling a Python function.",
      "number": 1184,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:56.160Z"
    },
    {
      "summary": "OpenAI Agents SDK integration fails during serialization with complex type annotations like `typing_extensions.Required`. The Temporal Python SDK's default data converter cannot handle OpenAI's complex generic types, causing workflows to fail immediately when executing agent operations.",
      "category": "bug",
      "subcategory": "serialization",
      "apis": [
        "DataConverter",
        "PayloadConverter",
        "DefaultPayloadConverter"
      ],
      "components": [
        "data-converter",
        "openai-agents-plugin",
        "type-system"
      ],
      "concepts": [
        "serialization",
        "type-annotations",
        "generic-types",
        "data-conversion",
        "third-party-integration",
        "type-validation"
      ],
      "severity": "high",
      "userImpact": "Users cannot use OpenAI Agents SDK with Temporal Python SDK due to serialization failures on complex type annotations, blocking AI-powered workflow development.",
      "rootCause": "Temporal's default data converter performs strict type validation that fails on OpenAI's complex generic type annotations like `typing_extensions.Required[Union[...]]`, which contain nested generics the converter doesn't support.",
      "proposedFix": "Custom data converter implementation that handles OpenAI-specific types through pickle serialization and bypasses type validation for problematic type hints.",
      "workaround": "Use custom PicklePayloadConverter that detects OpenAI types and serializes them with pickle instead of JSON, while skipping type hints for complex generic types.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by clarifying that OpenAIAgentsPlugin must be registered with both Client and Worker to provide proper DataConverter. Plugin should be passed to both client initialization and worker creation.",
      "related": [],
      "keyQuote": "If you look at all the samples, the plugin is provided to the `Client`, not the `Worker`. The plugin is simply not provided at all and therefore doesn't change the `DataConverter` as is required.",
      "number": 1180,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:57.832Z"
    },
    {
      "summary": "When using LiteLLM with AWS Bedrock in the OpenAI agent SDK, rate limiting errors (503 Service Unavailable) are incorrectly classified as non-retryable instead of being retried. The issue is that the retry logic only checks for status code == 500, but should check for >= 500 like OpenAI's implementation does.",
      "category": "bug",
      "subcategory": "openai-agent-sdk",
      "apis": [],
      "components": [
        "openai-agent-sdk",
        "litellm-integration",
        "error-handling",
        "retry-logic"
      ],
      "concepts": [
        "retry",
        "rate-limiting",
        "error-classification",
        "http-status-codes",
        "service-availability"
      ],
      "severity": "high",
      "userImpact": "Users leveraging LiteLLM with AWS Bedrock experience activity failures during rate limiting instead of automatic retries, breaking the reliability of AI agent workflows.",
      "rootCause": "Retry logic incorrectly uses == 500 comparison instead of >= 500 for HTTP status codes, causing 503 Service Unavailable errors to not trigger retries despite being defined as retryable by OpenAI.",
      "proposedFix": "Change the status code comparison from == 500 to >= 500 to match OpenAI's retry behavior and properly handle 503 and other 5xx errors.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Contributor acknowledged the bug was a straightforward fix - the inequality comparison was missed when mimicking OpenAI's retry policies.",
      "related": [],
      "keyQuote": "your implementation does not correctly mimic the openAI retry policies. You only retry on 500 errors while the openAI code retries on >= 500",
      "number": 1177,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:41.848Z"
    },
    {
      "summary": "Pyright type checking has a long exclude list in pyproject.toml that prevents certain files from being checked due to import resolution issues. The feature request is to clean up and reduce this exclude list by resolving the underlying import issues.",
      "category": "feature",
      "subcategory": "type-checking",
      "apis": [],
      "components": [
        "pyright",
        "type-checker",
        "build-system"
      ],
      "concepts": [
        "type-checking",
        "import-resolution",
        "ci-failure",
        "development-workflow"
      ],
      "severity": "low",
      "userImpact": "Developers experience CI lint failures and must work around import resolution issues that prevent proper type checking of Python SDK modules.",
      "rootCause": "Import resolution failure for temporalio.bridge.temporal_sdk_bridge in CI environment despite passing locally, likely due to build order or environment differences.",
      "proposedFix": "Review and clean up the pyright exclude list by resolving the underlying import issues that necessitate the exclusions.",
      "workaround": "Reorder CI steps to run build-develop before lint to ensure imports are resolved.",
      "resolution": "fixed",
      "resolutionDetails": "The exclude list was cleaned up by addressing the underlying import resolution issues.",
      "related": [],
      "keyQuote": "The list of excludes is quite long, with some substantial files. We should go through and clean up the list of files that need to be excluded here.",
      "number": 1176,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:39.196Z"
    },
    {
      "summary": "Check and update integration support with agents framework 0.4.0, including identifying any breaking changes and update requirements.",
      "category": "feature",
      "subcategory": "agents-integration",
      "apis": [],
      "components": [
        "agents-framework",
        "sdk-compatibility"
      ],
      "concepts": [
        "integration",
        "compatibility",
        "version-support",
        "breaking-changes",
        "agents-framework"
      ],
      "severity": "medium",
      "userImpact": "Users need to know if the SDK supports agents framework 0.4.0 and what updates may be required.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue closed after verification of agents 0.4.0 compatibility and any necessary updates were completed.",
      "related": [],
      "keyQuote": "Check integration support with agents 0.4.0",
      "number": 1171,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:37.064Z"
    },
    {
      "summary": "Activities are timing out prematurely when task volume exceeds concurrent worker capacity. When more than 10 concurrent activities are requested with max_workers=10, subsequent activities get cancelled before reaching their start_to_close_timeout, even though the timeout is set to 120 seconds.",
      "category": "bug",
      "subcategory": "activity-execution-timeout",
      "apis": [
        "ExecuteActivity",
        "Worker"
      ],
      "components": [
        "worker",
        "activity-executor",
        "task-queue"
      ],
      "concepts": [
        "concurrency",
        "timeout",
        "thread-pool",
        "task-scheduling",
        "activity-cancellation"
      ],
      "severity": "high",
      "userImpact": "Users experience unexpected activity cancellations when workload exceeds configured concurrency limits, causing task failures and retries despite adequate timeout configuration.",
      "rootCause": "The timeout clock starts when poll_activity_task() receives the task, before the thread pool has capacity to execute it. When thread pool is full, tasks wait in queue while the timer continues counting, resulting in premature timeout before execution begins.",
      "proposedFix": "Use `max_concurrent_activities` parameter on the Worker to properly limit concurrent activity processing, or implement a fixed-size slot supplier for more complex scenarios.",
      "workaround": "Configure the `max_concurrent_activities` parameter on the Worker instead of relying solely on the ThreadPoolExecutor's max_workers.",
      "resolution": "wontfix",
      "resolutionDetails": "Clarified that start_to_close_timeout begins after task acceptance, not after user code execution. This is intended behavior. Users should use Temporal's built-in concurrency controls (max_concurrent_activities or slot suppliers) rather than thread pool sizing alone.",
      "related": [],
      "keyQuote": "start_to_close_timeout refers to the time allowed after a worker has accepted a task. In Python, you can use the max_concurrent_activities argument on a worker.",
      "number": 1168,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:24.870Z"
    },
    {
      "summary": "Feature request to improve type hints for `execute_activity` to support activity methods (class methods decorated as activities). The user initially thought the type hints didn't support this use case, but discovered that `execute_activity_method` already exists for this purpose.",
      "category": "feature",
      "subcategory": "type-hints",
      "apis": [
        "execute_activity",
        "execute_activity_method"
      ],
      "components": [
        "type-system",
        "activity-executor",
        "decorator"
      ],
      "concepts": [
        "type-hints",
        "activity-methods",
        "callable-signature",
        "class-methods",
        "type-checking"
      ],
      "severity": "low",
      "userImpact": "Users may be confused about type hint support for activity methods and may not discover the dedicated `execute_activity_method` API.",
      "rootCause": null,
      "proposedFix": "Update type hints for execute_activity to support activity methods, or provide better documentation/discovery of execute_activity_method",
      "workaround": "Use execute_activity_method for class-based activity methods instead of execute_activity",
      "resolution": "wontfix",
      "resolutionDetails": "The user discovered that execute_activity_method already exists and is the intended API for activity methods, making the feature request unnecessary",
      "related": [],
      "keyQuote": "Ah I missed that we gave execute_activity_method",
      "number": 1166,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:23.805Z"
    },
    {
      "summary": "Feature request to allow activities defined as tools to be executed as local activities by adding an optional parameter to the `activity_as_tool` function that would use `execute_local_activity` instead of `execute_activity`.",
      "category": "feature",
      "subcategory": "activity-tools",
      "apis": [
        "execute_activity",
        "execute_local_activity",
        "activity_as_tool"
      ],
      "components": [
        "activity-executor",
        "tool-integration",
        "workflow-execution"
      ],
      "concepts": [
        "local-activity",
        "tool-definition",
        "activity-execution",
        "openai-integration",
        "activity-configuration"
      ],
      "severity": "low",
      "userImpact": "Users cannot currently execute activities defined as OpenAI tools as local activities, limiting performance optimization options.",
      "rootCause": null,
      "proposedFix": "Add a boolean parameter (e.g., `as_local_activity`) to the `activity_as_tool` function that uses `execute_local_activity` when set to true.",
      "workaround": "Define a custom function tool that directly calls `workflow.execute_local_activity` with the specific local activity instead of using `activity_as_tool`.",
      "resolution": "wontfix",
      "resolutionDetails": "The maintainer indicated they would consider the functionality but suggested using a custom function tool as a simpler alternative, implying the feature was deprioritized.",
      "related": [],
      "keyQuote": "In the meantime, you are welcome to define a function tool which runs your specific local activity.",
      "number": 1162,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:20.869Z"
    },
    {
      "summary": "Worker initialization is slow in large codebases when build_id is not explicitly set because the system automatically generates an identifier by MD5 hashing all modules. The code comment should be updated to warn about this performance impact.",
      "category": "docs",
      "subcategory": "worker-initialization",
      "apis": [
        "Worker"
      ],
      "components": [
        "worker",
        "build_id-generation"
      ],
      "concepts": [
        "performance",
        "initialization-time",
        "codebase-hashing",
        "large-codebases"
      ],
      "severity": "medium",
      "userImpact": "Users with large codebases (15M+ lines) experience significantly longer worker initialization times if they don't explicitly set the build_id field.",
      "rootCause": "The automatic build_id generation process traverses and MD5 hashes all modules in the codebase, which becomes a bottleneck for large codebases.",
      "proposedFix": "Update the code comment to include a warning about potential long initialization times for large codebases when build_id is not explicitly set.",
      "workaround": "Explicitly set the build_id field to a pre-computed hash of the source code to avoid the automatic generation process.",
      "resolution": "fixed",
      "resolutionDetails": "Code comment was updated to warn about initialization time impact in large codebases.",
      "related": [],
      "keyQuote": "In very large codebases this automatic process may significantly increase initialization time.",
      "number": 1160,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:08.125Z"
    },
    {
      "summary": "Feature request to create a test verifying the OpenAI plugin works in activity-only and workflow-only worker configurations. Python users commonly split workers by type, and the plugin must support this setup instead of requiring activities and workflows in the same process.",
      "category": "feature",
      "subcategory": "plugin-testing",
      "apis": [],
      "components": [
        "openai-plugin",
        "worker",
        "activity-worker",
        "workflow-worker"
      ],
      "concepts": [
        "plugin-compatibility",
        "worker-separation",
        "activity-only-mode",
        "workflow-only-mode",
        "worker-configuration",
        "process-separation",
        "plugin-support"
      ],
      "severity": "medium",
      "userImpact": "Python users splitting workers by type need assurance that the OpenAI plugin functions correctly in activity-only and workflow-only configurations.",
      "rootCause": null,
      "proposedFix": "Create a test to verify OpenAI plugin works in both activity-only and workflow-only worker modes",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Test was created to verify OpenAI plugin compatibility with separated worker configurations",
      "related": [],
      "keyQuote": "Many Python users, perhaps more than any other SDK, are splitting up workers to activity-only workers and workflow-only workers.",
      "number": 1159,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:05.600Z"
    },
    {
      "summary": "Python 3.14 introduces InterpreterPoolExecutor for isolated subinterpreters, but the SDK's activity executor assumes ThreadPoolExecutor behavior, causing NotShareableError when attempting to use InterpreterPoolExecutor for activity execution due to cross-interpreter data incompatibility.",
      "category": "feature",
      "subcategory": "activity-executor",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "concurrent-executor"
      ],
      "concepts": [
        "subinterpreters",
        "thread-pool",
        "executor",
        "concurrency",
        "data-sharing",
        "python-3.14"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use Python 3.14's InterpreterPoolExecutor as an activity executor due to incompatible assumptions about data shareability across interpreter boundaries.",
      "rootCause": "The activity executor implementation assumes ThreadPoolExecutor semantics but InterpreterPoolExecutor enforces stricter isolation rules that prevent sharing certain objects (Queue, Context, etc.) across interpreter boundaries.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Python 3.14 adds InterpreterPoolExecutor, a subclass of ThreadPoolExecutor that runs tasks in isolated subinterpreters... does not support cross-interpreter data",
      "number": 1154,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:34:04.416Z"
    },
    {
      "summary": "Feature request to allow workflows to provide additional context data during MCP server construction, enabling context-specific server configuration instead of requiring factory callables with no arguments defined at worker level.",
      "category": "feature",
      "subcategory": "mcp-server",
      "apis": [],
      "components": [
        "mcp-server",
        "worker",
        "workflow"
      ],
      "concepts": [
        "context",
        "factory",
        "configuration",
        "initialization",
        "server-construction"
      ],
      "severity": "low",
      "userImpact": "Users cannot currently pass workflow-specific context to MCP server factories, limiting their ability to create servers with different configurations per workflow.",
      "rootCause": null,
      "proposedFix": "Ability to provide additional data to the mcp server reference which is then provided to the factory callable.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Ability to provide some additional data to the mcp server reference which is then provided to the factory callable.",
      "number": 1146,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:52.709Z"
    },
    {
      "summary": "Activity and Nexus worker error handling during shutdown may have issues including potential memory leaks. The issue questions whether the current behavior is appropriate and consistent across worker types.",
      "category": "bug",
      "subcategory": "worker-shutdown",
      "apis": [],
      "components": [
        "activity-worker",
        "nexus-worker",
        "worker-shutdown"
      ],
      "concepts": [
        "error-handling",
        "graceful-shutdown",
        "memory-leaks",
        "worker-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Users may experience improper error handling or memory leaks when activity and Nexus workers shut down.",
      "rootCause": "Potential inconsistency in error handling behavior between activity worker and Nexus worker during shutdown, with possible memory leak in activity worker.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Closed as won't do. Core handles graceful shutdown and Nexus worker behavior is consistent with activity worker behavior.",
      "related": [
        813
      ],
      "keyQuote": "Core handles graceful shutdown and the behavior of the Nexus worker is consistent with the activities worker.",
      "number": 1144,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:51.280Z"
    },
    {
      "summary": "Langfuse tracing instrumentation fails to work with Temporal OpenAI Agents Plugin, preventing observable tracing of agent execution. User found partial workarounds using OpenAIAgentsInstrumentor but encounters context detachment errors.",
      "category": "bug",
      "subcategory": "openai-agents-plugin-tracing",
      "apis": [],
      "components": [
        "openai-agents-plugin",
        "tracing",
        "instrumentation",
        "worker",
        "activities"
      ],
      "concepts": [
        "observability",
        "tracing",
        "langfuse",
        "opentelemetry",
        "context-propagation",
        "instrumentation",
        "agents"
      ],
      "severity": "high",
      "userImpact": "Users cannot properly trace and monitor OpenAI agent execution within Temporal workflows, blocking observability for production multi-tenant applications.",
      "rootCause": "Plugin-based agent execution creates context isolation issues with OpenTelemetry instrumentation; logfire's TracingProvider overrides conflict with Temporal's tracing setup, and context variables created in different contexts cause detachment failures.",
      "proposedFix": null,
      "workaround": "Use OpenAIAgentsInstrumentor().instrument() which provides traces but generates context detachment errors; or use logfire.instrument_openai() for partial tracing with null input/output.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1134,
        441
      ],
      "keyQuote": "Tracing is absolutely critical for us, we can't avoid it. The specific tracing tool doesn't matter as long as it works reliably with Temporal.",
      "number": 1136,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:51.455Z"
    },
    {
      "summary": "Temporal workflows with OpenAI agents enforce model names to be strings, preventing the use of custom model objects like LitellmModel that would enable per-agent configuration of API keys and base URLs for multi-tenant scenarios.",
      "category": "bug",
      "subcategory": "openai-agents",
      "apis": [],
      "components": [
        "openai-agents",
        "convert-agent",
        "model-provider"
      ],
      "concepts": [
        "multi-tenancy",
        "model-configuration",
        "api-key-management",
        "proxy-configuration",
        "serialization",
        "agent-isolation"
      ],
      "severity": "medium",
      "userImpact": "Users building multi-tenant applications cannot configure different API keys or proxy URLs per agent and must use a global OpenAI client configuration.",
      "rootCause": "The convert_agent function requires model to be a string because Model objects are not serializable and must be acquired in activities via a ModelProvider on the worker side.",
      "proposedFix": "Support custom ModelProvider implementations that map model names (potentially including tenant identifiers) to the appropriate model instances with different configurations.",
      "workaround": "Create a custom ModelProvider that constructs LitellmModel instances with the desired configuration based on tenant identifiers encoded in the model name string.",
      "resolution": "wontfix",
      "resolutionDetails": "The maintainers clarified that string model names are a hard requirement due to serialization constraints in activity execution, but custom ModelProviders can provide the needed flexibility.",
      "related": [],
      "keyQuote": "You can either use an existing ModelProvider if it gives you the semantics you need, or you can always create your own to provide an arbitrary mapping from model name to the model you desire.",
      "number": 1134,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:36.278Z"
    },
    {
      "summary": "Feature request to simplify plugin creation by providing a function that creates plugins from static configurations (activities, workflows, dataconverter) instead of requiring implementation of multiple interfaces.",
      "category": "feature",
      "subcategory": "plugin-system",
      "apis": [],
      "components": [
        "plugin-system",
        "plugin-factory",
        "configuration"
      ],
      "concepts": [
        "plugin-creation",
        "static-configuration",
        "developer-experience",
        "abstraction",
        "simplification"
      ],
      "severity": "low",
      "userImpact": "Developers must implement multiple interfaces to create plugins, making plugin development complex and time-consuming for simple use cases.",
      "rootCause": null,
      "proposedFix": "Provide a factory function that creates simple plugins from a set of static configurations instead of requiring full interface implementation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to simplify plugin creation for common use cases.",
      "related": [],
      "keyQuote": "Provide a function for creating a simple plugin from a set of static configurations: activities, workflows, dataconverter, etc.",
      "number": 1133,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:32.803Z"
    },
    {
      "summary": "After migrating from Poetry to uv, Python version classifiers and license metadata are missing from the PyPI package, causing README badges to fail. The package needs proper version and license information set in the wheel/PyPI metadata.",
      "category": "feature",
      "subcategory": "packaging-metadata",
      "apis": [],
      "components": [
        "packaging",
        "pypi",
        "build-system"
      ],
      "concepts": [
        "package-metadata",
        "version-classifier",
        "license-information",
        "build-configuration",
        "pypi-distribution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot accurately see Python version compatibility and license information on PyPI, breaking documentation badges and reducing discoverability.",
      "rootCause": "Migration from Poetry to uv build system removed Python version classifiers and license metadata configuration",
      "proposedFix": "Set Python programming language version classifiers and license metadata in the wheel/PyPI configuration for uv",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the packaging metadata was properly configured after uv migration",
      "related": [],
      "keyQuote": "With the move from poetry to uv, we have lost the programming language classifier I think.",
      "number": 1132,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:37.696Z"
    },
    {
      "summary": "User requests support for custom tracing providers in the OpenAI Agents Plugin, allowing integration with custom trace providers beyond the default implementation. The proposal suggests creating a factory pattern to wrap any OpenAI-supported tracing provider with Temporal compatibility.",
      "category": "feature",
      "subcategory": "tracing-providers",
      "apis": [
        "Client.connect",
        "OpenAIAgentsPlugin"
      ],
      "components": [
        "openai-agents-plugin",
        "tracing-provider",
        "trace-processor"
      ],
      "concepts": [
        "custom-tracing",
        "provider-integration",
        "openai-compatibility",
        "deterministic-time",
        "trace-id-generation",
        "workflow-context"
      ],
      "severity": "low",
      "userImpact": "Users cannot integrate custom tracing providers with the OpenAI Agents Plugin, limiting observability options for applications using custom tracing implementations.",
      "rootCause": null,
      "proposedFix": "Create a factory pattern that accepts any OpenAI-supported tracing provider and wraps it to be Temporal-compatible, similar to the existing TemporalTraceProvider implementation.",
      "workaround": "Users can implement their own trace provider by extending DefaultTraceProvider and overriding tracing methods (as shown in the TemporalTraceProvider example).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Perhaps it wouldn't be too hard to just create a factory that takes any supported tracing provider and returns a Temporal supported one.",
      "number": 1130,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:21.555Z"
    },
    {
      "summary": "Child workflow search attributes fail to decrypt in SDK 1.18.0 when using a custom payload codec, causing InvalidArgument errors. This is a regression from 1.17.0 where the same code worked correctly.",
      "category": "bug",
      "subcategory": "child-workflow",
      "apis": [
        "execute_child_workflow"
      ],
      "components": [
        "child-workflow",
        "payload-codec",
        "search-attributes",
        "converter"
      ],
      "concepts": [
        "encryption",
        "decryption",
        "search-attributes",
        "payload-codec",
        "child-workflow",
        "regression"
      ],
      "severity": "high",
      "userImpact": "Users cannot launch child workflows with search attributes when using custom payload codecs in SDK 1.18.0, breaking encryption-based workflows.",
      "rootCause": "Search attributes in child workflow commands are not being decrypted by the payload codec in version 1.18.0, likely due to changes in how search attributes are handled in the child workflow execution path.",
      "proposedFix": null,
      "workaround": "Downgrade to SDK 1.17.0 or wait for 1.18.1 release.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed and released as 1.18.1 patch version. User confirmed the fix resolves the issue.",
      "related": [],
      "keyQuote": "invalid SearchAttributes on StartChildWorkflowCommand: invalid value for search attribute show_name of type Keyword: value from <metadata:{key:\"encoding\"  value:\"binary/encrypted\"}",
      "number": 1129,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:19.846Z"
    },
    {
      "summary": "Request to add documentation for the field definitions and configuration options in OpenTelemetryConfig and PrometheusConfig classes in the Python SDK.",
      "category": "docs",
      "subcategory": "telemetry-configuration",
      "apis": [
        "OpenTelemetryConfig",
        "PrometheusConfig"
      ],
      "components": [
        "telemetry",
        "configuration",
        "documentation"
      ],
      "concepts": [
        "instrumentation",
        "observability",
        "configuration",
        "telemetry",
        "metrics",
        "documentation"
      ],
      "severity": "low",
      "userImpact": "Users cannot easily understand what each configuration field does, making it difficult to properly set up telemetry for their applications.",
      "rootCause": null,
      "proposedFix": "Add clear documentation for all fields in OpenTelemetryConfig and PrometheusConfig",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "OpenTelemetryConfig and PrometheusConfig should have their fields documented clearly",
      "number": 1121,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:16.544Z"
    },
    {
      "summary": "User requests LIFO (Last In First Out) scheduling logic for task queues or workflow types so that newer activities execute first. The request was addressed through discussion of existing priority mechanisms and task queue fairness, but no direct LIFO support currently exists.",
      "category": "feature",
      "subcategory": "task-queue-scheduling",
      "apis": [],
      "components": [
        "task-queue",
        "scheduler",
        "workflow-executor"
      ],
      "concepts": [
        "LIFO-scheduling",
        "task-ordering",
        "queue-fairness",
        "priority",
        "workflow-precedence"
      ],
      "severity": "low",
      "userImpact": "Users cannot configure task queues to prioritize newer workflows or activities over older ones, limiting scheduling flexibility.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Use the Priority mechanism to give activities different priority levels to achieve preference ordering",
      "resolution": "wontfix",
      "resolutionDetails": "Determined to be a feature request without explicit LIFO support; existing priority system was suggested as alternative",
      "related": [],
      "keyQuote": "We don't currently provide a way to prioritize newer runs. There is feature proposal which would help here",
      "number": 1120,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:03.352Z"
    },
    {
      "summary": "Client TLS connection fails after a new release due to a change in native roots handling. The issue was resolved by removing the server_root_ca_cert parameter to allow native roots to be used.",
      "category": "bug",
      "subcategory": "tls-configuration",
      "apis": [
        "Client.connect"
      ],
      "components": [
        "client",
        "tls-transport",
        "native-roots"
      ],
      "concepts": [
        "tls",
        "certificate",
        "connection",
        "native-roots",
        "authentication"
      ],
      "severity": "high",
      "userImpact": "Users with custom TLS certificates cannot connect to Temporal Server after upgrading to the latest SDK release.",
      "rootCause": "A change in sdk-core (PR #1007) modified the behavior to only enable native roots if the root CA certificate is not provided, causing custom root CA certificates to fail with UnknownIssuer errors.",
      "proposedFix": null,
      "workaround": "Remove the server_root_ca_cert parameter from TLSConfig to allow native roots to be used instead.",
      "resolution": "fixed",
      "resolutionDetails": "The issue was resolved by adjusting TLS configuration behavior in a previous release. Users should remove server_root_ca_cert if they want to use native roots.",
      "related": [],
      "keyQuote": "There was a change in the release to only enable native roots if the root ca cert is not provided.",
      "number": 1113,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:05.222Z"
    },
    {
      "summary": "Feature request to add WorkflowIdConflictPolicy support for child workflows, similar to the existing support in parent workflows. This would allow developers to specify conflict handling policies when starting or executing child workflows.",
      "category": "feature",
      "subcategory": "child-workflows",
      "apis": [
        "start_child_workflow",
        "execute_child_workflow",
        "execute_workflow",
        "start_workflow"
      ],
      "components": [
        "child-workflow-client",
        "workflow-orchestration",
        "conflict-resolution"
      ],
      "concepts": [
        "workflow-id-conflict",
        "policy-configuration",
        "child-workflow-execution",
        "error-handling",
        "workflow-composition"
      ],
      "severity": "low",
      "userImpact": "Users cannot specify conflict policies for child workflows, limiting their ability to handle workflow ID conflicts in hierarchical workflow scenarios.",
      "rootCause": null,
      "proposedFix": "Add id_conflict_policy parameter to start_child_workflow() and execute_child_workflow() methods, consistent with parent workflow API.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "This is tracked as an open request on the server side (temporal#6799), indicating it requires server support before SDK implementation.",
      "related": [
        504,
        6799
      ],
      "keyQuote": "I would love for start_child_workflow() and execute_child_workflow() [to support WorkflowIDConflictPolicy]",
      "number": 1112,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:33:02.281Z"
    },
    {
      "summary": "Adding workflow.random() calls before workflow.uuid4() in a modified workflow causes nondeterminism errors on replay. The issue occurs when deploying workflow changes that add random function calls, breaking previously running instances that expected uuid4 to be called first.",
      "category": "bug",
      "subcategory": "workflow-replay",
      "apis": [
        "workflow.uuid4",
        "workflow.random",
        "execute_child_workflow"
      ],
      "components": [
        "workflow-replay",
        "determinism",
        "random-generation",
        "child-workflow"
      ],
      "concepts": [
        "nondeterminism",
        "replay",
        "deterministic-ordering",
        "random-seed",
        "workflow-versioning",
        "state-mismatch"
      ],
      "severity": "high",
      "userImpact": "Workflows fail with nondeterminism errors when re-deployed with new random() calls, causing running instances to crash and preventing workflow continuation.",
      "rootCause": "All random objects in workflows share a common seed based on workflow run ID. Adding new random() calls changes the sequence of random values, causing previously stored deterministic values (like child workflow IDs) to mismatch on replay.",
      "proposedFix": null,
      "workaround": "Call workflow.uuid4() before any workflow.random() calls to ensure consistent ordering, or avoid adding new random() calls in deployed workflows.",
      "resolution": "wontfix",
      "resolutionDetails": "Resolved as expected behavior by maintainer - this is a fundamental constraint of deterministic workflow execution where all random values depend on shared seed state.",
      "related": [],
      "keyQuote": "All the randomly generated objects on the workflow are based on workflow.random(). Hence, when you add a new call to it, it will change the result of the second call.",
      "number": 1109,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:32:48.777Z"
    },
    {
      "summary": "Update openai-agents dependency to v0.3.0 to incorporate bug fixes and improvements to the hooks mechanism. Current version causes workflow task failures with unexpected keyword argument errors.",
      "category": "feature",
      "subcategory": "dependency-update",
      "apis": [],
      "components": [
        "workflow-executor",
        "dependency-management"
      ],
      "concepts": [
        "dependency-upgrade",
        "hooks-mechanism",
        "openai-agents",
        "bug-fixes",
        "compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users utilizing openai-agents integration experience workflow task failures that can be resolved by updating to v0.3.0.",
      "rootCause": "openai-agents library version mismatch causing _TemporalModelStub.get_response() to receive unexpected keyword argument 'conversation_id'",
      "proposedFix": "Update openai-agents library dependency to version 0.3.0",
      "workaround": "Build the dependency from main branch in the meantime",
      "resolution": "fixed",
      "resolutionDetails": "Issue was already fixed in main branch; maintainers indicated a release would be published soon with the fix",
      "related": [],
      "keyQuote": "Openai new version fixes various bugs and add new improvements to the hooks mechanism that we would like to incorporate",
      "number": 1108,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:32:46.312Z"
    },
    {
      "summary": "Request to remove the upper bound constraint on protobuf dependency (currently pinned to <6) to allow usage of protobuf 6.x and later versions. This is important as protobuf 5.x reaches end of support in Q1 2026.",
      "category": "feature",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "dependency-resolver",
        "protobuf-integration",
        "package-configuration"
      ],
      "concepts": [
        "dependency-versioning",
        "compatibility",
        "end-of-support",
        "upstream-libraries",
        "version-constraints"
      ],
      "severity": "medium",
      "userImpact": "Users are blocked from using protobuf 6.x and later, which may cause dependency conflicts when other libraries require newer protobuf versions.",
      "rootCause": "Upper bound constraint (<6) was set for protobuf dependency, preventing adoption of newer versions.",
      "proposedFix": "Update protobuf dependency constraint from 'protobuf>=3.20,<6' to 'protobuf>=3.20' to allow any version from 3.20 onwards.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "PR #1082 is in progress to support protobuf 6.x, addressing this feature request.",
      "related": [
        1082
      ],
      "keyQuote": "Protobuf 5x will reach End of Support in Q1 2026 so it's better to upgrade asap",
      "number": 1107,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:32:50.085Z"
    },
    {
      "summary": "TLS certificate verification fails during poll_workflow_task_queue when connecting to Temporal server in Kubernetes. The client attempts to resolve the matching service pod IP instead of using the frontend service hostname, causing certificate SAN validation errors.",
      "category": "bug",
      "subcategory": "tls-certificate-validation",
      "apis": [
        "poll_workflow_task_queue"
      ],
      "components": [
        "client",
        "connection",
        "tls"
      ],
      "concepts": [
        "tls-certificate",
        "kubernetes-service-discovery",
        "hostname-resolution",
        "certificate-san",
        "internode-communication"
      ],
      "severity": "high",
      "userImpact": "Users deploying Python workers in Kubernetes with TLS cannot connect to Temporal server due to certificate verification failures on task queue polling.",
      "rootCause": "Missing server_name configuration under server.config.tls.internode.client in Helm chart configuration, causing gRPC client to use pod IP instead of frontend service hostname for TLS verification.",
      "proposedFix": "Add server_name parameter under server.config.tls.internode.client in Helm configuration to explicitly specify the hostname for TLS certificate verification.",
      "workaround": "Configure server_name in the internode TLS client settings to match the certificate SAN (e.g., temporal-frontend.main.svc).",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved by adding server_name configuration to Helm chart's TLS internode client settings, allowing proper hostname resolution during TLS handshake.",
      "related": [],
      "keyQuote": "I was missing server_name under server.config.tls.internode.client in my Helm config.",
      "number": 1106,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:32:28.540Z"
    },
    {
      "summary": "User cannot use Python's breakpoint() function in workflows even with debug_mode=True, as the sandboxing restrictions prevent access to __builtins__.breakpoint. Additionally, the user is confused about workflow-level retry policy configuration for unit tests versus activity-level retries.",
      "category": "bug",
      "subcategory": "debugging-sandbox",
      "apis": [
        "execute_workflow",
        "execute_activity"
      ],
      "components": [
        "workflow_sandbox",
        "workflow_instance",
        "importer",
        "worker"
      ],
      "concepts": [
        "debugging",
        "breakpoint",
        "sandboxing",
        "restrictions",
        "test-driven-development",
        "retry-policy"
      ],
      "severity": "high",
      "userImpact": "Developers cannot debug workflows using Python's debugger in unit tests even with debug_mode enabled, making TDD difficult for workflows with complex logic.",
      "rootCause": "The workflow sandbox restricts access to __builtins__.breakpoint even when debug_mode=True, and unsandboxed mode with UnsandboxedWorkflowRunner hangs instead of allowing interactive debugging.",
      "proposedFix": "Allow __builtins__.breakpoint to bypass sandbox restrictions when debug_mode=True, or ensure UnsandboxedWorkflowRunner allows proper pdb REPL interaction.",
      "workaround": "Use the Replayer class to debug workflow history replays instead of live debugging, though this requires pre-existing workflow history.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "However, notice in that screenshot that while it *does* break, it is not in the pdb repl and I cannot interact with the debugger in any way.",
      "number": 1104,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:32:33.901Z"
    },
    {
      "summary": "Worker appears to stop polling task queue after period of inactivity in Kubernetes environment. Issue resolves after worker restart, but recurs without configuration changes.",
      "category": "bug",
      "subcategory": "worker-polling",
      "apis": [
        "Worker"
      ],
      "components": [
        "worker",
        "task-queue-polling",
        "kubernetes-deployment"
      ],
      "concepts": [
        "connection-loss",
        "task-queue-polling",
        "worker-availability",
        "deployment-lifecycle",
        "graceful-shutdown"
      ],
      "severity": "high",
      "userImpact": "Users experience task processing interruptions where workers stop polling task queues without visible configuration changes, requiring manual worker restarts to restore functionality.",
      "rootCause": "Potentially running out of slots on worker(s) or connection/polling mechanism not recovering from network/timing issues in Kubernetes environment",
      "proposedFix": null,
      "workaround": "Restart worker deployment to restore task queue polling",
      "resolution": "invalid",
      "resolutionDetails": "Community team indicated this is a diagnostics issue requiring metrics/logs review rather than SDK bug; user directed to community Slack for support",
      "related": [],
      "keyQuote": "After some times without any changes in worker or server configs, in the web ui i see: 'There are no Workers polling the X Task Queue.'",
      "number": 1103,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:32:30.955Z"
    },
    {
      "summary": "Hello World workflow fails with TypeError when using OpenAI Agents SDK 0.3.0 due to breaking changes in the SDK API. The issue is caused by incompatibility between the Temporal SDK and OpenAI Agents SDK versions.",
      "category": "bug",
      "subcategory": "dependency-compatibility",
      "apis": [],
      "components": [
        "run_worker",
        "workflow-activation",
        "agent-integration"
      ],
      "concepts": [
        "dependency-management",
        "breaking-changes",
        "version-compatibility",
        "openai-integration",
        "workflow-execution",
        "error-handling"
      ],
      "severity": "high",
      "userImpact": "Users cannot run basic hello world workflows with OpenAI Agents SDK 0.3.0, requiring immediate version pinning workaround.",
      "rootCause": "Breaking changes in OpenAI Agents SDK 0.3.0 introduced an incompatible API (conversation_id parameter) that conflicts with the Temporal SDK's _TemporalModelStub.get_response() method.",
      "proposedFix": "Pin openai agents dependency to version 0.2.9 or lower until the fix is released in a newer version.",
      "workaround": "Pin openai agents to version 0.2.9 or lower",
      "resolution": "fixed",
      "resolutionDetails": "Breaking changes were fixed in main branch, with upper bound of <0.3.0 added to prevent using the incompatible version.",
      "related": [],
      "keyQuote": "Breaking changes were made in `0.2.11` and subsequently exist in `0.3.0`. It has been fixed in main, but an upper bound of `<0.3.0` has been added",
      "number": 1098,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:32:12.537Z"
    },
    {
      "summary": "_TemporalModelStub.get_response() is incompatible with the latest openai-agents-python library, which added a new `conversation_id` keyword argument. The method signature needs to be updated to accept this parameter or use kwargs passthrough to handle future upstream changes.",
      "category": "bug",
      "subcategory": "openai-agents-integration",
      "apis": [],
      "components": [
        "_TemporalModelStub",
        "openai-agents-integration",
        "contrib"
      ],
      "concepts": [
        "API-compatibility",
        "upstream-drift",
        "parameter-mismatch",
        "keyword-arguments",
        "serialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot run workflows with the latest openai-agents-python library versions due to TypeError when calling get_response().",
      "rootCause": "The _TemporalModelStub.get_response() method signature does not accept the `conversation_id` parameter that openai-agents-python 0.2.10+ now passes when calling model.get_response().",
      "proposedFix": "Update _TemporalModelStub.get_response to accept `conversation_id: Optional[str] = None` parameter. Consider adding **kwargs passthrough, but only for serializable arguments since parameters must pass through serialization.",
      "workaround": "Pin openai-agents-python to versions before 0.2.10 until the SDK is updated.",
      "resolution": "fixed",
      "resolutionDetails": "Updated _TemporalModelStub.get_response to accept the conversation_id parameter to maintain compatibility with openai-agents-python 0.2.10+.",
      "related": [
        1640
      ],
      "keyQuote": "TypeError: _TemporalModelStub.get_response() got an unexpected keyword argument 'conversation_id'",
      "number": 1091,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:32:14.218Z"
    },
    {
      "summary": "Request to add support for MCP (Model Context Protocol) server implementation in the Python SDK's OpenAI integration, enabling more advanced AI tooling capabilities.",
      "category": "feature",
      "subcategory": "openai-integration",
      "apis": [],
      "components": [
        "openai-integration",
        "mcp-server"
      ],
      "concepts": [
        "mcp-server",
        "openai-integration",
        "ai-tooling",
        "model-context-protocol"
      ],
      "severity": "low",
      "userImpact": "Users cannot currently use MCP server functionality with Temporal's OpenAI integration, limiting advanced AI capabilities.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented and released in a subsequent version",
      "related": [],
      "keyQuote": "We don't currently provide a supported `MCPServer` implementation.",
      "number": 1090,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:32:15.490Z"
    },
    {
      "summary": "The Python SDK lacks built-in Session implementations for use with OpenAI Agents in workflows. Currently no functional Session options are available since the built-in SQLite implementation isn't supported, requiring users to provide their own.",
      "category": "feature",
      "subcategory": "openai-agents",
      "apis": [
        "Session"
      ],
      "components": [
        "session-management",
        "agents",
        "workflow-execution"
      ],
      "concepts": [
        "session-persistence",
        "openai-integration",
        "agent-framework",
        "workflow-context",
        "in-box-support"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use OpenAI Agents in Temporal workflows without implementing their own Session solution.",
      "rootCause": null,
      "proposedFix": "Provide at least one built-in Session implementation that works within a workflow context.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Provide at least one in the box `Session` which works in a workflow context.",
      "number": 1089,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:57.279Z"
    },
    {
      "summary": "Proto classes generated in the Python SDK have incorrect module names prefixed with 'temporal' instead of 'temporalio', causing pickle serialization failures when using multiprocessing for workflow unit tests.",
      "category": "bug",
      "subcategory": "proto-generation",
      "apis": [],
      "components": [
        "proto-generation",
        "serialization",
        "multiprocessing"
      ],
      "concepts": [
        "pickling",
        "module-naming",
        "proto-compilation",
        "multiprocessing",
        "serialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot run workflow unit tests with process pools due to pickle serialization errors on proto classes.",
      "rootCause": "Generated proto class modules have incorrect Python path prefix 'temporal' instead of 'temporalio', preventing proper module resolution during pickling.",
      "proposedFix": "Update proto generation to use correct python_package path prefix.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Module naming in generated protos will be corrected to use proper python path.",
      "related": [],
      "keyQuote": "Can't pickle <class 'temporal.api.failure.v1.message_pb2.Failure'>: import of module 'temporal.api.failure.v1.message_pb2' failed",
      "number": 1087,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:59.530Z"
    },
    {
      "summary": "The OpenAI plugin replaces the entire data converter instead of just the payload converter, which clobbers user payload codec customization. The request is to either modify the plugin to only replace the payload converter or error if a non-default payload converter is provided.",
      "category": "feature",
      "subcategory": "openai-plugin",
      "apis": [],
      "components": [
        "openai-plugin",
        "data-converter",
        "payload-codec"
      ],
      "concepts": [
        "codec-customization",
        "plugin-architecture",
        "data-conversion",
        "user-configuration",
        "override-prevention"
      ],
      "severity": "medium",
      "userImpact": "Users cannot customize their payload codec when using the OpenAI plugin without it being overridden by the plugin's data converter replacement.",
      "rootCause": "OpenAI plugin replaces the entire data converter rather than composing with or respecting the user's existing payload converter configuration.",
      "proposedFix": "Either modify the plugin to only replace the payload converter component, or error if the data converter is provided with a non-default payload converter, or error only if the user's payload converter is incompatible with what OpenAI plugin needs.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was resolved by modifying the OpenAI plugin to respect user payload codec customization rather than clobbering it.",
      "related": [
        1083
      ],
      "keyQuote": "OpenAI plugin replaces the entire data converter instead of just the payload converter.",
      "number": 1084,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:57.232Z"
    },
    {
      "summary": "Feature request to expose the `strict_json_schema` argument in `activity_as_tool` and `nexus_as_tool` functions for the OpenAI SDK integration, allowing users to override the hardwired `strict_json_schema=True` setting instead of requiring a workaround with `dataclasses.replace`.",
      "category": "feature",
      "subcategory": "openai-integration",
      "apis": [
        "activity_as_tool",
        "nexus_as_tool"
      ],
      "components": [
        "openai_agents",
        "workflow",
        "tool-integration"
      ],
      "concepts": [
        "json-schema",
        "strict-validation",
        "openai-integration",
        "configuration",
        "api-flexibility"
      ],
      "severity": "low",
      "userImpact": "Users can now cleanly configure JSON schema strictness for OpenAI tool functions without needing workarounds like dataclasses.replace.",
      "rootCause": null,
      "proposedFix": "Expose `strict_json_schema` as an argument parameter in `activity_as_tool` and `nexus_as_tool` functions.",
      "workaround": "Use `dataclasses.replace` on the result to override the hardwired `strict_json_schema=True` setting.",
      "resolution": "fixed",
      "resolutionDetails": "Feature was accepted and planned for implementation.",
      "related": [],
      "keyQuote": "Seems reasonable and easy. We'll put it in the pipeline.",
      "number": 1081,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:44.830Z"
    },
    {
      "summary": "Python SDK fails to import generated protocol buffer code due to mismatched Protobuf versions between gencode (6.31.1) and runtime (5.29.5). User needs SDK to support newer protobuf versions that their other dependencies require.",
      "category": "bug",
      "subcategory": "protobuf-versioning",
      "apis": [],
      "components": [
        "protobuf",
        "grpc",
        "dependencies"
      ],
      "concepts": [
        "version-compatibility",
        "protobuf-gencode",
        "dependency-resolution",
        "import-error"
      ],
      "severity": "high",
      "userImpact": "Users cannot import the SDK when they have other dependencies requiring newer protobuf versions, blocking SDK adoption.",
      "rootCause": "SDK dependencies specify an older protobuf version (5.29.5) that conflicts with generated code compiled with newer protobuf (6.31.1).",
      "proposedFix": "Update underlying protobuf/grpc versions to support newer gencode versions.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "SDK dependencies were updated to support newer protobuf versions.",
      "related": [],
      "keyQuote": "Detected mismatched Protobuf Gencode/Runtime major versions when loading chat/v1/types.proto: gencode 6.31.1 runtime 5.29.5",
      "number": 1080,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:40.841Z"
    },
    {
      "summary": "Feature request to add tests confirming that custom slot supplier slot info objects have all proper fields set correctly.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "slot-supplier",
        "testing",
        "worker"
      ],
      "concepts": [
        "custom-slots",
        "slot-info",
        "test-coverage",
        "validation"
      ],
      "severity": "low",
      "userImpact": "Ensures that custom slot supplier implementations are properly validated through comprehensive test coverage.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        672
      ],
      "keyQuote": "Ensure tests exist to confirm custom slot supplier slot info has proper fields",
      "number": 1079,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:36.591Z"
    },
    {
      "summary": "Extend type inference improvements made in #938 to workflow-related functions like execute_activity, start_activity, execute_child_workflow, and start_child_workflow for consistent typing across the SDK.",
      "category": "feature",
      "subcategory": "type-inference",
      "apis": [
        "execute_activity",
        "start_activity",
        "execute_child_workflow",
        "start_child_workflow"
      ],
      "components": [
        "typing",
        "workflow-execution",
        "activity-execution"
      ],
      "concepts": [
        "type-inference",
        "type-safety",
        "developer-experience",
        "generic-types",
        "api-consistency"
      ],
      "severity": "low",
      "userImpact": "Developers would benefit from improved type inference when using workflow-related functions, leading to better IDE support and fewer type errors.",
      "rootCause": null,
      "proposedFix": "Apply the same type inference improvements from #938 to execute_activity, start_activity, execute_child_workflow, and start_child_workflow functions.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        938
      ],
      "keyQuote": "Should this also be done with workflow things like execute_activity, start_activity, execute_child_workflow, and start_child_workflow?",
      "number": 1077,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:24.408Z"
    },
    {
      "summary": "User metadata and other non-search attribute payloads are not being encoded outbound in the Python SDK. The issue requires ensuring all workflow payloads go through the codec both on inbound (activation/decoding) and outbound (completion/encoding), with comprehensive tests added.",
      "category": "bug",
      "subcategory": "codec-payloads",
      "apis": [],
      "components": [
        "codec",
        "workflow-payload",
        "user-metadata"
      ],
      "concepts": [
        "encoding",
        "decoding",
        "payload-processing",
        "serialization",
        "codec-validation"
      ],
      "severity": "high",
      "userImpact": "Users may experience data loss or corruption when using codecs with workflow metadata and other payloads that are not being properly encoded/decoded.",
      "rootCause": "Non-search attribute payloads, particularly user metadata, are not being routed through the codec during outbound transmission from the SDK.",
      "proposedFix": "Implement a general code-generated payload visitor utility (similar to Go and Ruby implementations) to ensure all payloads are consistently encoded/decoded, and add comprehensive tests for all future payloads.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [
        468
      ],
      "keyQuote": "Not only must we make sure _every_ non-search attribute payload is decoded on activation and encoded on completion, we must write a test to ensure this for all future payloads.",
      "number": 1071,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:24.192Z"
    },
    {
      "summary": "Add support for gRPC binary metadata headers in the Python SDK by accepting bytes as metadata values and sending them as binary to Tonic for keys ending in '-bin'.",
      "category": "feature",
      "subcategory": "gRPC-metadata",
      "apis": [],
      "components": [
        "gRPC",
        "metadata",
        "Tonic"
      ],
      "concepts": [
        "binary-metadata",
        "gRPC-headers",
        "interoperability",
        "protocol-compliance"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently send binary metadata through gRPC calls, limiting compatibility with gRPC binary metadata standards.",
      "rootCause": null,
      "proposedFix": "Accept bytes as metadata values and send as binary to Tonic for '-bin' suffixed keys",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented support for binary metadata headers in gRPC communications",
      "related": [
        671
      ],
      "keyQuote": "Probably just need to accept `bytes` as metadata value and send as binary to Tonic for `-bin` keys.",
      "number": 1063,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:26.008Z"
    },
    {
      "summary": "The OpenAI Agent SDK wraps ApplicationErrors raised by activities as UserErrors, preventing workflows from catching and handling non-retryable errors appropriately. This causes workflow tasks to fail and get stuck indefinitely.",
      "category": "bug",
      "subcategory": "agent-sdk-integration",
      "apis": [
        "ApplicationError"
      ],
      "components": [
        "agent-sdk",
        "activity-executor",
        "error-handling"
      ],
      "concepts": [
        "error-wrapping",
        "non-retryable-error",
        "agent-integration",
        "error-propagation",
        "workflow-execution"
      ],
      "severity": "high",
      "userImpact": "Users cannot properly handle non-retryable errors from activities invoked through the OpenAI Agent SDK, causing workflows to fail and become unrecoverable.",
      "rootCause": "The Agent SDK catches ApplicationError exceptions and re-raises them as UserError, breaking the error handling contract expected by workflows.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The Agent SDK wraps/catches the ApplicationError and raises it as a UserError. This causes the workflow task after the failed activity to also fail and gets stuck",
      "number": 1058,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:08.173Z"
    },
    {
      "summary": "Add support for MCP (Model Context Protocol) clients in native Python workflows, enabling use of existing stdio and http transport MCP servers. Implementation should initially use activity calls with an activity-local worker for stateful sessions, with future versions supporting fully durable/fault-tolerant tool calls using nexus operations.",
      "category": "feature",
      "subcategory": "mcp-integration",
      "apis": [],
      "components": [
        "mcp-client",
        "activity-worker",
        "workflow-runtime",
        "transport-layer"
      ],
      "concepts": [
        "mcp-protocol",
        "stateful-sessions",
        "tool-calling",
        "durable-execution",
        "activity-worker",
        "nexus-operations"
      ],
      "severity": "medium",
      "userImpact": "Allows workflow code to directly use MCP servers for tool calling and external integrations without requiring third-party agent SDKs.",
      "rootCause": null,
      "proposedFix": "Implement mcp.ClientSession component using custom transport responding to tools/list and tools/call via activity calls; use activity-local activity worker for stateful sessions; support stdio and http transport initially; future versions to support nexus operations for durable tool calls.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1021
      ],
      "keyQuote": "The SDK should contain a component allowing MCP to be used from native workflow code (i.e. when not using a 3rd-party agent SDK).",
      "number": 1056,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:10.482Z"
    },
    {
      "summary": "OpenTelemetry context is not propagated when activities are started from a workflow's `__init__` method, resulting in missing span context in the activity execution despite the client call being properly instrumented.",
      "category": "bug",
      "subcategory": "opentelemetry-integration",
      "apis": [
        "start_activity",
        "execute_workflow"
      ],
      "components": [
        "opentelemetry-interceptor",
        "workflow-interceptor",
        "workflow-instance"
      ],
      "concepts": [
        "tracing",
        "context-propagation",
        "workflow-initialization",
        "span-context",
        "instrumentation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot properly trace activities started during workflow initialization, breaking distributed tracing visibility for initialization-time activity invocations.",
      "rootCause": "WorkflowOutboundInterceptor lacks a hook to wrap workflow `__init__` execution, so OpenTelemetry context is not captured and propagated when activities are started during initialization.",
      "proposedFix": "Introduce a hook in WorkflowOutboundInterceptor to wrap workflow `__init__` method execution, similar to how `run` is wrapped, to ensure context propagation.",
      "workaround": "Move activity invocations from `__init__` to the `run` method where context propagation works correctly.",
      "resolution": "wontfix",
      "resolutionDetails": "SDK maintainers recommended against starting substantial actions in workflow constructors, aligning with practices across other Temporal SDKs. Users should defer activity invocations to the `run` method.",
      "related": [],
      "keyQuote": "In most SDKs we have specifically disallowed taking substantial action inside of the workflow constructors. I would recommend you refrain from setting up commands inside `__init` and do so during `run` instead.",
      "number": 1054,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:31:12.034Z"
    },
    {
      "summary": "OTEL trace and span context propagation is missing for `execute_update_with_start_workflow` and `start_update_with_start_workflow` methods in the Python SDK. The methods were not included in the `_TracingClientOutboundInterceptor` after PR #702, causing trace context to be lost when invoking workflow updates with start.",
      "category": "bug",
      "subcategory": "opentelemetry-instrumentation",
      "apis": [
        "execute_update_with_start_workflow",
        "start_update_with_start_workflow",
        "execute_activity"
      ],
      "components": [
        "opentelemetry-contrib",
        "client-interceptor",
        "workflow-update"
      ],
      "concepts": [
        "observability",
        "trace-propagation",
        "span-context",
        "instrumentation",
        "distributed-tracing"
      ],
      "severity": "medium",
      "userImpact": "Users cannot see complete distributed traces for workflow updates started with start_workflow because OTEL trace context is not propagated from the client through the update handler to activities.",
      "rootCause": "The `_TracingClientOutboundInterceptor` class in `temporalio/contrib/opentelemetry.py` was not updated to include methods for `execute_update_with_start_workflow` and `start_update_with_start_workflow` after PR #702 added these methods.",
      "proposedFix": "Add implementations of `execute_update_with_start_workflow` and `start_update_with_start_workflow` to `_TracingClientOutboundInterceptor` that wrap the calls with `_start_as_current_span()` to properly propagate trace context, similar to the pattern shown in the issue's patched version.",
      "workaround": "Users can subclass `_TracingClientOutboundInterceptor` and `TracingInterceptor` to manually add span wrapping for these methods, as demonstrated in the minimal reproduction.",
      "resolution": "fixed",
      "resolutionDetails": "The missing tracing interceptor methods were added to handle the new update-with-start-workflow operations.",
      "related": [
        702
      ],
      "keyQuote": "start_update_with_start_workflow was not included in _TracingClientOutboundInterceptor after this PR. When I add it manually, propagation works as expected.",
      "number": 1053,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:30:55.393Z"
    },
    {
      "summary": "Feature request to make the Python SDK's 2-second deadlock detection timeout configurable per worker, similar to Go SDK's DeadlockDetectionTimeout option. Request was closed after team discussion concluded that workflows needing longer timeouts should use activities instead.",
      "category": "feature",
      "subcategory": "worker-configuration",
      "apis": [
        "Worker"
      ],
      "components": [
        "worker",
        "deadlock-detection",
        "workflow-execution"
      ],
      "concepts": [
        "timeout",
        "deadlock-detection",
        "worker-configuration",
        "debug-mode",
        "CPU-bound-operations",
        "workflow-constraints"
      ],
      "severity": "low",
      "userImpact": "Users cannot configure deadlock detection timeout in production and must restructure code or use debug mode for workflows with longer CPU-bound sections.",
      "rootCause": "Python SDK enforces a fixed 2-second deadlock detection timeout without configuration option, unlike Go SDK which allows customization.",
      "proposedFix": "Add deadlock_detection_timeout parameter to Worker class accepting timedelta, with default of 2 seconds and option to disable with None or 0.",
      "workaround": "Run workflows in debug_mode or set TEMPORAL_DEBUG environment variable to disable deadlock detection, or restructure long-running CPU-bound code into activities.",
      "resolution": "wontfix",
      "resolutionDetails": "Team decided to keep the timeout fixed by design, as workflows legitimately needing longer timeouts should use activities instead of executing CPU-bound code in workflows.",
      "related": [],
      "keyQuote": "If you're doing things that extend the timeout beyond those values, there's a nearly 100% chance that you shouldn't be doing those things in a workflow context.",
      "number": 1050,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:30:49.889Z"
    },
    {
      "summary": "Feature request to enable custom metric support for activities running in multi-processed workers. Currently metrics cannot be accessed in child processes; the proposal is to forward metrics from child processes to the parent worker via a queue, similar to how heartbeating is handled.",
      "category": "feature",
      "subcategory": "metrics-multi-process",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "metric-meter",
        "process-pool"
      ],
      "concepts": [
        "metrics",
        "observability",
        "multi-processing",
        "metric-forwarding",
        "OTEL-integration",
        "heartbeating"
      ],
      "severity": "medium",
      "userImpact": "Users must manually configure complex OTEL tooling and metric aggregation to support metrics in multi-processed workers, adding significant complexity to projects using CPU-bound workloads.",
      "rootCause": "Metrics cannot be shared across process boundaries; the SDK currently lacks inter-process metric forwarding infrastructure for multi-processed workers.",
      "proposedFix": "Create an alternate MetricMeter implementation that queues metric actions to the parent process, which forwards them to the actual MetricMeter, similar to the existing heartbeat manager approach.",
      "workaround": "Set up a separate OTEL metrics toolchain with manual process ID tagging and external metric aggregation via OTEL collector's transform processor.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        384
      ],
      "keyQuote": "Ideally, I'd hope to see metrics wired up across the process pool out of the box. So following a similar approach to the existing heartbeat manager",
      "number": 1049,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:30:52.722Z"
    },
    {
      "summary": "Sync multiprocessed activities in the Python SDK cannot be cancelled, either voluntarily or due to missed heartbeats, leading to resource waste and potential data corruption. The feature request asks for best-effort cancellation support similar to async and multithreaded workers, along with improved documentation and guidance on handling side effects.",
      "category": "feature",
      "subcategory": "activity-cancellation",
      "apis": [
        "activity.wait_for_cancellation_sync()",
        "ProcessPoolExecutor"
      ],
      "components": [
        "worker",
        "activity-executor",
        "multiprocessing",
        "cancellation-handler"
      ],
      "concepts": [
        "cancellation",
        "multiprocessing",
        "heartbeat",
        "activity-timeout",
        "resource-management",
        "side-effects",
        "process-interruption"
      ],
      "severity": "high",
      "userImpact": "Users running sync activities with multiprocessed workers cannot stop long-running activities on cancellation, causing resource starvation, retry loops, and potential data corruption from duplicate side effects.",
      "rootCause": "Sync multiprocessed activities lack a mechanism to raise cancellation exceptions in child processes; while a multiprocessing Manager event exists to signal cancellation across process boundaries, there is no built-in way for activity code to react to it, unlike multithreaded activities which use _ThreadExceptionRaiser.",
      "proposedFix": "Implement best-effort cancellation for sync multiprocessed workers using heartbeats to signal cancellation, similar to async and multithreaded workers; provide documentation on limitations and guidance for handling side effects (unique resource locations, activity-level idempotency, separate queues for critical side effects).",
      "workaround": "Use async activities with asyncio.run_in_executor() to delegate to a process pool; or use regular sync activities with multithreading instead; or use copy_context() with wait_for_cancellation_sync() in a background thread to monitor cancellation state.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        217,
        1047
      ],
      "keyQuote": "Activity not found on completion. This may happen if the activity has already been cancelled but completed anyway.",
      "number": 1048,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:30:32.342Z"
    },
    {
      "summary": "Feature request to mark OpenTelemetry spans as failed when activities are cancelled due to heartbeat timeouts in the Python SDK. Currently, heartbeat timeout cancellations are not properly reflected in tracing data, making it difficult to identify and assess the scale of worker heartbeat issues.",
      "category": "feature",
      "subcategory": "activity-heartbeat",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "opentelemetry-interceptor",
        "activity-worker",
        "tracing"
      ],
      "concepts": [
        "heartbeat-timeout",
        "cancellation",
        "observability",
        "span-status",
        "activity-execution",
        "asyncio-cancellation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily identify and diagnose workers with heartbeat issues through OpenTelemetry tracing data, requiring alternative monitoring approaches.",
      "rootCause": "OpenTelemetry's TracingInterceptor only catches Exception-type errors, not BaseException subclasses like asyncio.CancelledError. Additionally, there is insufficient context in the interceptor to distinguish different cancellation reasons, and cancellation doesn't work at all in multi-process workers.",
      "proposedFix": "Enhance the TracingInterceptor to catch and record asyncio.CancelledError as span failures, update cancellation_details context when heartbeat errors occur, and ensure cancellation applies consistently across async, threaded, and multi-process activity types.",
      "workaround": "Authors can patch their own interceptor to catch asyncio.CancelledError and manually set span status to ERROR.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1048,
        1017,
        1041
      ],
      "keyQuote": "Heartbeat timeout failures can be a major pain point when working with async Temporal activities...mark spans as failed for cancellations due to heartbeat timeouts, so we can identify workers with heartbeat issues more easily",
      "number": 1047,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:30:33.781Z"
    },
    {
      "summary": "Benign application errors raised in activities with category=BENIGN are properly handled by Temporal's logging but incorrectly reported as errors in OpenTelemetry spans, causing false alerts in monitoring systems like Logfire. The request seeks configuration or SDK updates to prevent benign errors from being treated as real errors in OpenTelemetry instrumentation.",
      "category": "feature",
      "subcategory": "observability-opentelemetry",
      "apis": [
        "ApplicationError"
      ],
      "components": [
        "tracing-interceptor",
        "opentelemetry-integration",
        "activity-executor"
      ],
      "concepts": [
        "observability",
        "error-handling",
        "benign-errors",
        "span-status",
        "activity-retries",
        "monitoring-integration"
      ],
      "severity": "medium",
      "userImpact": "Users leveraging benign application errors in activity polling patterns receive false error alerts in observability tools, reducing signal quality and requiring workaround interceptors.",
      "rootCause": "TracingInterceptor treats all exceptions equally in OpenTelemetry spans regardless of ApplicationError category, conflicting with Temporal's debug-level logging for benign errors.",
      "proposedFix": "Update TracingInterceptor to check ApplicationError category and handle benign errors appropriately; suppress exception events and set span status to OK for benign errors, or add configuration option set_status_on_exception=False with explicit exception handling.",
      "workaround": "Implement a custom ActivityInboundInterceptor that catches benign ApplicationErrors, sets span attributes, adds events, and resets span status to OK before re-raising.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1047
      ],
      "keyQuote": "OpenTelemetry still reports the exception as an error just as dire as any other one... triggering alerts for 'expected behavior'",
      "number": 1041,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:30:35.301Z"
    },
    {
      "summary": "AttributeError when accessing start.WAIT_TIMEOUT in activity execution, causing activity tasks to fail repeatedly. Resolved by reinstalling the virtual environment.",
      "category": "bug",
      "subcategory": "activity-execution",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "worker",
        "activity-executor",
        "proto-conversion"
      ],
      "concepts": [
        "heartbeat",
        "timeout",
        "activity-task",
        "proto-conversion",
        "attribute-error"
      ],
      "severity": "medium",
      "userImpact": "Activities fail with AttributeError when the Python SDK tries to access WAIT_TIMEOUT during heartbeat_timeout calculation, breaking activity execution.",
      "rootCause": "Missing or corrupted WAIT_TIMEOUT attribute in the protobuf start message object, likely due to incomplete installation or cached bytecode.",
      "proposedFix": null,
      "workaround": "Reinstall the virtual environment to clear cached or corrupted files.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by removing and reinstalling the virtual environment, indicating the problem was environment-related (likely corrupted cache or installation).",
      "related": [],
      "keyQuote": "removing .venv and reinstalling fixed this. damn, sent until 6am on this",
      "number": 1033,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:30:09.917Z"
    },
    {
      "summary": "The value_to_type function fails when handling dictionaries with custom string types (string subclasses or NewType aliases) as keys, converting them to character lists or throwing isinstance errors.",
      "category": "bug",
      "subcategory": "converter",
      "apis": [
        "value_to_type"
      ],
      "components": [
        "converter",
        "data-converter",
        "type-conversion"
      ],
      "concepts": [
        "type-conversion",
        "dictionary-keys",
        "custom-types",
        "newtype",
        "string-subclass",
        "serialization"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use custom string types (subclasses or NewType aliases) as dictionary keys in dataclasses passed to workflows.",
      "rootCause": "The isinstance check added in commit 4933dc5 fails on NewType because NewType creates a callable, not a proper type. String subclasses are incorrectly treated as iterables.",
      "proposedFix": "Handle NewType by extracting the supertype for isinstance checks; treat string subclasses as strings rather than iterables.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "PR #1059 fixed the NewType case by properly handling the isinstance check. CustomStr subclass support was deferred as a potential future feature.",
      "related": [
        1059
      ],
      "keyQuote": "The value_to_type function has two bugs when handling dictionary keys with custom string types: String subclasses are converted to character lists, and NewType fails isinstance check.",
      "number": 1032,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:30:07.017Z"
    },
    {
      "summary": "User requests a GetLastCompletionResult method in Python SDK for retrieving previous schedule run results, similar to functionality available in Go and Java SDKs.",
      "category": "feature",
      "subcategory": "schedules",
      "apis": [
        "GetLastCompletionResult"
      ],
      "components": [
        "python-sdk",
        "schedules",
        "workflow"
      ],
      "concepts": [
        "schedule",
        "completion-result",
        "previous-run",
        "cron"
      ],
      "severity": "medium",
      "userImpact": "Python SDK users cannot easily access previous schedule run results within workflows, limiting schedule implementation capabilities.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #383; core request acknowledged with commitment to prioritize.",
      "related": [
        383
      ],
      "keyQuote": "I see that there is a GetLastCompletionResult method for getting previous run result of a schedule... however I couldn't find the same method in Python SDK.",
      "number": 1028,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:30:07.265Z"
    },
    {
      "summary": "Feature request to add the first execution run ID to the workflow info object in the Python SDK. This would allow workflows to access metadata about their initial execution, referenced from a cross-SDK features discussion.",
      "category": "feature",
      "subcategory": "workflow-info",
      "apis": [
        "WorkflowInfo"
      ],
      "components": [
        "workflow-context",
        "workflow-info",
        "metadata"
      ],
      "concepts": [
        "execution-history",
        "workflow-metadata",
        "run-id",
        "first-execution",
        "workflow-state"
      ],
      "severity": "low",
      "userImpact": "Users would be able to access the first execution run ID through workflow info, enabling better tracking of workflow execution lineage.",
      "rootCause": null,
      "proposedFix": "Add first_execution_run_id field to the WorkflowInfo class in the Python SDK",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented and merged to add first_execution_run_id to workflow info",
      "related": [
        29
      ],
      "keyQuote": "See https://github.com/temporalio/features/issues/29",
      "number": 1025,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:29:49.984Z"
    },
    {
      "summary": "Feature request to display only the agent name in the timeline instead of the Activity Type + Summary. The requester suggests modifying the _TemporalModelStub class to include agent_name information that could be used for timeline display.",
      "category": "feature",
      "subcategory": "agents-timeline-display",
      "apis": [],
      "components": [
        "openai_agents",
        "_openai_runner",
        "_temporal_model_stub"
      ],
      "concepts": [
        "agent-name",
        "timeline-display",
        "agent-handoff",
        "model-invocation",
        "agentic-loop"
      ],
      "severity": "low",
      "userImpact": "Users cannot currently display only agent names in timelines, making it difficult to track agent changes during multi-agent workflows.",
      "rootCause": "The Agent object in the runner is only the starting agent, while the model is reused across all invocations including handoffs. Additionally, model names are not specified at the agent level, preventing different models from being used in different portions of the same agentic loop.",
      "proposedFix": "Update _TemporalModelStub class to include agent_name field, with changes to the agents SDK to properly track agent context throughout the agentic loop.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by fixing the underlying issue where model names are not specified at agent level, which will enable different models in different portions of agentic loops and allow agent context tracking.",
      "related": [],
      "keyQuote": "The model names themselves are not specified at the agent level, so it is not currently possible to use different models in different portions of the same agentic loop.",
      "number": 1024,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:29:49.558Z"
    },
    {
      "summary": "TaskGroup-based child workflow execution fails to propagate ChildWorkflowError exceptions correctly. When child workflows fail within a TaskGroup, the error is wrapped in an ExceptionGroup which doesn't trigger workflow failure, causing the parent workflow to run indefinitely instead of failing as expected.",
      "category": "bug",
      "subcategory": "child-workflows",
      "apis": [
        "execute_child_workflow",
        "ChildWorkflowError"
      ],
      "components": [
        "workflow-instance",
        "error-handling",
        "exception-processing"
      ],
      "concepts": [
        "exception-group",
        "failure-propagation",
        "task-group",
        "child-workflow-errors",
        "async-task-management",
        "error-wrapping"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably use asyncio.TaskGroup for parallel child workflow execution because failures are not properly detected, causing workflows to hang indefinitely.",
      "rootCause": "The workflow exception handling code does not unwrap ExceptionGroup to check for nested FailureError instances. When TaskGroup wraps ChildWorkflowError in ExceptionGroup, the outer exception is not recognized as a failure exception.",
      "proposedFix": "Add logic to _is_workflow_failure_exception() to recursively check ExceptionGroup contents for FailureError instances. Implement _is_exception_group_failure() method that checks if any nested exception in an ExceptionGroup is itself a workflow failure exception.",
      "workaround": "Use asyncio.gather() instead of asyncio.TaskGroup for child workflow execution, as gather() raises the first exception directly without wrapping.",
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When a child workflow fails it raises a ChildWorkflowError. However, when using TaskGroup it is encapsulated in an ExceptionGroup and raised, which does not fail the calling workflow, causing it to run forever",
      "number": 1020,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:29:49.375Z"
    },
    {
      "summary": "Replayer fails when workflow_runner is set to UnsandboxedWorkflowRunner() but works with SandboxedWorkflowRunner. User is debugging workflow history files from Temporal UI and needs the unsandboxed runner for their use case.",
      "category": "bug",
      "subcategory": "workflow-replay",
      "apis": [
        "Replayer",
        "UnsandboxedWorkflowRunner",
        "SandboxedWorkflowRunner",
        "WorkflowHistory"
      ],
      "components": [
        "replayer",
        "workflow-runner",
        "sandbox"
      ],
      "concepts": [
        "workflow-replay",
        "debugging",
        "sandboxing",
        "workflow-execution",
        "history-replay"
      ],
      "severity": "medium",
      "userImpact": "Users cannot debug workflows using the unsandboxed runner with the replayer, limiting debugging options for certain use cases.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Use SandboxedWorkflowRunner with default restrictions and passthrough_all_modules instead of UnsandboxedWorkflowRunner",
      "resolution": "fixed",
      "resolutionDetails": "Maintainer assumed the issue was resolved by the user without requiring SDK changes",
      "related": [],
      "keyQuote": "It works in this specific runner though: workflow_runner=SandboxedWorkflowRunner(restrictions=SandboxRestrictions.default.with_passthrough_all_modules())",
      "number": 1018,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:29:31.317Z"
    },
    {
      "summary": "Missing overload for `start_activity` with `summary` parameter causes Pyright type checking failures. Users cannot pass the `summary` argument without triggering type checker errors.",
      "category": "bug",
      "subcategory": "activity-api",
      "apis": [
        "start_activity"
      ],
      "components": [
        "activity-executor",
        "type-stubs",
        "api-signatures"
      ],
      "concepts": [
        "type-checking",
        "overloads",
        "pyright",
        "method-signature",
        "parameter-handling"
      ],
      "severity": "medium",
      "userImpact": "Users with strict type checking enabled cannot use the `summary` parameter with `start_activity`, blocking production use in type-safe codebases.",
      "rootCause": "Missing overload definition in the `start_activity` method signature for the `summary` parameter",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Overload for start_activity with summary parameter was added to the implementation",
      "related": [],
      "keyQuote": "When calling `start_activity` with `summary`, Pyright complains that no overloads match the provided arguments",
      "number": 1013,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:29:32.012Z"
    },
    {
      "summary": "Activity does not retry immediately after worker process exits for the second time; instead waits for the full start_to_close_timeout (30 minutes). User expects faster retry behavior when a worker restarts.",
      "category": "bug",
      "subcategory": "activity-retry",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "activity-executor",
        "worker",
        "retry-policy"
      ],
      "concepts": [
        "retry",
        "timeout",
        "worker-failure",
        "process-exit",
        "heartbeat",
        "activity-failure"
      ],
      "severity": "medium",
      "userImpact": "Activities take significantly longer to retry (30 minutes) after worker process exits on second attempt instead of retrying immediately.",
      "rootCause": "Worker process exit is not automatically detected; the system waits for the full start_to_close_timeout before retrying rather than detecting the failure through heartbeat mechanism.",
      "proposedFix": "Add heartbeating to activities to enable automatic detection of worker failures, or implement automatic heartbeating feature.",
      "workaround": "Manually add heartbeating to your activity as documented in the Temporal encyclopedia on detecting activity failures.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue was not marked as a bug but rather identified as missing heartbeating in user's implementation. Recommended using the heartbeat mechanism for failure detection. Auto-heartbeating request tracked separately in features repo.",
      "related": [
        229
      ],
      "keyQuote": "We don't currently support automatic detection of the worker's process exit. The recommended solution is to add heartbeating to your activity.",
      "number": 1012,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:29:34.944Z"
    },
    {
      "summary": "Feature request for streaming support in OpenAI Agents integration. Users need run_streamed() to update chat progress during long-running reasoning agent operations.",
      "category": "feature",
      "subcategory": "openai-agents-streaming",
      "apis": [
        "run_streamed"
      ],
      "components": [
        "openai-agents-plugin",
        "activity-executor",
        "streaming-support"
      ],
      "concepts": [
        "streaming",
        "real-time-updates",
        "agent-execution",
        "long-running-tasks",
        "progress-tracking",
        "async-communication"
      ],
      "severity": "high",
      "userImpact": "Users cannot implement streaming agents that provide real-time progress updates to clients, blocking use cases that require incremental response feedback.",
      "rootCause": "Streaming updates from activities back to workflows require non-trivial implementation to propagate model outputs asynchronously during agent execution.",
      "proposedFix": "Implement streaming support in the OpenAI Agents plugin to handle run_streamed() calls and propagate updates back through the workflow.",
      "workaround": "Use two separate workers - one with the plugin for agent workflows and one without for regular workflows; or implement manual polling/callback mechanisms.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Streaming is not currently supported, as it is non-trivial to bring those updates back to the workflow from the activity in which the models are running.",
      "number": 1009,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:29:13.888Z"
    },
    {
      "summary": "User reports that Temporal 1.22.4 continues writing commits to PostgreSQL even when no workflows are active. The issue was redirected to the Temporal server repository as potentially server-related rather than Python SDK-specific.",
      "category": "question",
      "subcategory": "server-interaction",
      "apis": [],
      "components": [
        "persistence",
        "postgres-backend"
      ],
      "concepts": [
        "workflow-state",
        "inactive-workflows",
        "commit-behavior",
        "database-writes"
      ],
      "severity": "low",
      "userImpact": "Users may experience unexpected database commits and increased write volume when no workflows are running.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was redirected to temporalio/temporal repository as it appears to be a server-side concern rather than Python SDK-specific.",
      "related": [],
      "keyQuote": "when there is no workflows temporal dont stop commiting",
      "number": 1002,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:29:16.840Z"
    },
    {
      "summary": "Worker plugins need to support replayers with shared configuration like interceptors. The issue discusses three design options for providing plugin support to replayers while maintaining a clean API and avoiding redundant configuration code across plugin authors.",
      "category": "feature",
      "subcategory": "worker-plugins",
      "apis": [],
      "components": [
        "worker",
        "replayer",
        "plugins",
        "interceptors"
      ],
      "concepts": [
        "plugin-architecture",
        "configuration-reuse",
        "replayer-support",
        "interceptor-integration",
        "api-design",
        "backward-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Plugin authors cannot currently provide plugins to replayers, forcing them to duplicate configuration logic or choose not to support replay functionality.",
      "rootCause": "Worker plugins have configuration options that don't apply to replayers, but share important common functionality like interceptors, creating a design challenge.",
      "proposedFix": "Create a separate worker.ReplayerPlugin interface to maintain API clarity and allow plugin authors to opt-in to replayer support without forcing unnecessary configuration code.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented separate ReplayerPlugin interface allowing plugin authors to choose replayer support independently.",
      "related": [],
      "keyQuote": "Have a completely separate `worker.ReplayerPlugin` which...will give type error when trying to register when creating replayer",
      "number": 994,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:29:16.138Z"
    },
    {
      "summary": "User requests ability to specify WorkflowIDConflictPolicy when starting child workflows in Python SDK, allowing them to check for and reuse existing child workflows instead of getting an error.",
      "category": "feature",
      "subcategory": "child-workflows",
      "apis": [
        "start_child_workflow"
      ],
      "components": [
        "child-workflow-executor",
        "workflow-client",
        "api-bindings"
      ],
      "concepts": [
        "workflow-id-conflict",
        "workflow-reuse",
        "child-workflow-management",
        "policy-configuration",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot gracefully handle scenarios where parent workflow restarts and needs to check for existing child workflows with the same ID.",
      "rootCause": "WorkflowIDConflictPolicy parameter is not exposed in the Python SDK's start_child_workflow function, though the server supports it.",
      "proposedFix": "Add a WorkflowIDConflictPolicy argument to temporalio.workflow.start_child_workflow function.",
      "workaround": "Catch the error thrown when an existing workflow has the same ID, but this does not allow retrieving the workflow handle.",
      "resolution": "wontfix",
      "resolutionDetails": "Server does not support this mechanism; tracked at higher level in features repository.",
      "related": [
        558
      ],
      "keyQuote": "Add an argument to temporalio.workflow.start_child_workflow that specifies a WorkflowIDConflictPolicy.",
      "number": 989,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:57.812Z"
    },
    {
      "summary": "Type annotation issue when instantiating NamespaceSpec with search_attributes parameter. The SearchAttributeType needs to be exposed as a public class instead of a private nested type to allow proper type checking.",
      "category": "bug",
      "subcategory": "api-types",
      "apis": [
        "NamespaceSpec"
      ],
      "components": [
        "api-generation",
        "type-stubs",
        "cloud-namespace-api"
      ],
      "concepts": [
        "type-checking",
        "proto-generation",
        "private-types",
        "type-exposure",
        "cloud-api"
      ],
      "severity": "low",
      "userImpact": "Users cannot pass type checking when using NamespaceSpec with search_attributes without accessing private types.",
      "rootCause": "Proto code generation creates nested private types that should be exposed as public classes for proper type annotation support.",
      "proposedFix": "Expose SearchAttributeType as a public non-private class in the generated code.",
      "workaround": "Use the public subclass NamespaceSpec.SearchAttributeType.SEARCH_ATTRIBUTE_TYPE_UNSPECIFIED directly instead of accessing the private _SearchAttributeType.ValueType.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue marked as a proto generation problem. Workaround provided using public subclass.",
      "related": [],
      "keyQuote": "Can SearchAttributeType please be exposed as a non-private name?",
      "number": 980,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:59.165Z"
    },
    {
      "summary": "User reported that `invoke_model_activity` was missing from the latest SDK release, causing OpenAI examples to fail. The issue was resolved when the user discovered they were using an outdated SDK version in their devcontainer.",
      "category": "bug",
      "subcategory": "release-packaging",
      "apis": [],
      "components": [
        "release",
        "packaging",
        "openai-examples"
      ],
      "concepts": [
        "version-mismatch",
        "release-management",
        "sample-compatibility"
      ],
      "severity": "low",
      "userImpact": "Users with outdated SDK versions in their environment could not run the OpenAI example code.",
      "rootCause": "User was running an outdated SDK version (1.14.0) in their devcontainer instead of the latest release (1.14.1).",
      "proposedFix": null,
      "workaround": "Update to the latest SDK version (1.14.1 or later).",
      "resolution": "invalid",
      "resolutionDetails": "The feature was already available in the latest release. The user was using an outdated version in their development container.",
      "related": [],
      "keyQuote": "I just tested the sample against sdk version `1.14.1` which is the latest, and it ran without issues.",
      "number": 979,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:29:00.561Z"
    },
    {
      "summary": "Add Summary field to LocalActivityConfig to provide additional context for local activity executions.",
      "category": "feature",
      "subcategory": "local-activities",
      "apis": [
        "LocalActivityConfig"
      ],
      "components": [
        "local-activities",
        "activity-configuration"
      ],
      "concepts": [
        "activity-summary",
        "metadata",
        "configuration",
        "context"
      ],
      "severity": "low",
      "userImpact": "Users will be able to attach descriptive summary information to local activity configurations for better tracking and documentation.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented in a closed issue",
      "related": [],
      "keyQuote": null,
      "number": 978,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:36.743Z"
    },
    {
      "summary": "User requested a public API to configure logging behavior for workflow and activity loggers. They were manually patching logger attributes to control whether contextual information appears in log messages vs. logging extras, and wanted clarity on whether this was the intended approach.",
      "category": "feature",
      "subcategory": "logging-configuration",
      "apis": [
        "workflow.logger",
        "activity.logger"
      ],
      "components": [
        "logging",
        "workflow-logger",
        "activity-logger"
      ],
      "concepts": [
        "logging-configuration",
        "structured-logging",
        "logger-adapter",
        "contextual-information"
      ],
      "severity": "low",
      "userImpact": "Users need to manually patch logger attributes to configure logging behavior, when a dedicated public API would make this clearer and more maintainable.",
      "rootCause": null,
      "proposedFix": "Expose a public `configure_logging()` function for both workflow and activity loggers with explicit parameters for controlling log message content and replay logging.",
      "workaround": "Manually set attributes on `workflow.logger` and `activity.logger` instances (e.g., `workflow.logger.workflow_info_on_message = False`)",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers confirmed that the current approach of setting logger adapter attributes is the intended design pattern and is already well-documented. The attributes are intentionally exposed on the logger adapter for this purpose.",
      "related": [],
      "keyQuote": "It is, and is why we made these attributes visible on the logger adapter (and the logger adapter itself visible). We even clearly document them",
      "number": 971,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:41.162Z"
    },
    {
      "summary": "Pydantic serialization error occurs when using openai_agents with multiple workers, triggered by setting OPENAI_AGENTS_DONT_LOG_MODEL_DATA environment variable and running the hello world sample.",
      "category": "bug",
      "subcategory": "pydantic-serialization",
      "apis": [],
      "components": [
        "pydantic-serialization",
        "openai-agents",
        "worker"
      ],
      "concepts": [
        "serialization",
        "pydantic",
        "environment-variable",
        "multi-worker",
        "model-data",
        "json-encoding"
      ],
      "severity": "high",
      "userImpact": "Users cannot use openai_agents across multiple workers when the OPENAI_AGENTS_DONT_LOG_MODEL_DATA flag is set.",
      "rootCause": "Pydantic core's SchemaSerializer is receiving a MockValSer object that cannot be converted to SchemaSerializer, indicating a type mismatch during JSON serialization.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved; status shows CLOSED.",
      "related": [],
      "keyQuote": "pydantic_core._pydantic_core.PydanticSerializationError: Error serializing to JSON: TypeError: 'MockValSer' object cannot be converted to 'SchemaSerializer'",
      "number": 965,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:36.541Z"
    },
    {
      "summary": "Feature request to add an optional headers parameter to WorkflowHandle.signal() and Workflow.execute_update()/start_update() methods to allow passing custom workflow-level headers. The request was declined because Temporal headers are intentionally only set via interceptors for implicit setting and context propagation.",
      "category": "feature",
      "subcategory": "workflow-signaling",
      "apis": [
        "WorkflowHandle.signal",
        "Workflow.execute_update",
        "Workflow.start_update"
      ],
      "components": [
        "workflow-client",
        "signal-handling",
        "update-handling"
      ],
      "concepts": [
        "headers",
        "context-propagation",
        "rpc-metadata",
        "interceptors",
        "workflow-communication"
      ],
      "severity": "low",
      "userImpact": "Users cannot explicitly pass custom workflow-level headers through signal and update methods, though context propagation is possible via interceptors.",
      "rootCause": null,
      "proposedFix": "Add optional headers parameter to signal(), execute_update(), and start_update() methods accepting Mapping[str, temporalio.api.common.v1.Payloads]",
      "workaround": "Use interceptors for context propagation and implicit header setting, as demonstrated in the context_propagation sample.",
      "resolution": "wontfix",
      "resolutionDetails": "Temporal headers are intentionally only set via interceptors to support implicit setting and context propagation patterns, not explicit caller-provided headers.",
      "related": [],
      "keyQuote": "Temporal headers are intentionally only set via interceptors because they are meant for implicit setting and not explicit by callers.",
      "number": 961,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:22.077Z"
    },
    {
      "summary": "OpenAI trace spans sometimes fail to complete when running OpenAI Agents samples. The issue appears to be a UI behavior in OpenAI Platform Dashboard where additional events, including span closes, require loading more data.",
      "category": "bug",
      "subcategory": "observability-tracing",
      "apis": [],
      "components": [
        "openai-agents",
        "tracing",
        "spans"
      ],
      "concepts": [
        "tracing",
        "spans",
        "observability",
        "openai-integration",
        "ui-behavior",
        "event-loading"
      ],
      "severity": "low",
      "userImpact": "Users viewing traces in OpenAI Platform Dashboard see spans that appear incomplete until they load additional events.",
      "rootCause": "UI behavior in OpenAI Platform Dashboard - the load more button needs to be clicked to retrieve additional events including span closes",
      "proposedFix": null,
      "workaround": "Click the load more button at the bottom of the trace view to pull in additional events including the closes on 'ongoing' spans",
      "resolution": "invalid",
      "resolutionDetails": "Not a bug in the SDK - determined to be UI behavior in OpenAI Platform Dashboard where additional events are loaded on demand",
      "related": [],
      "keyQuote": "Turns out that this is UI behavior in OpenAI. The load more button at the bottom will pull in additional events, including the closes on these 'ongoing' spans.",
      "number": 951,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:20.833Z"
    },
    {
      "summary": "Feature request to add plugins that can control multiple configuration points at once, with OpenAI integration as an example use case.",
      "category": "feature",
      "subcategory": "plugins",
      "apis": [],
      "components": [
        "plugins",
        "configuration"
      ],
      "concepts": [
        "configuration",
        "integration",
        "plugins",
        "multi-point-control",
        "openai"
      ],
      "severity": "low",
      "userImpact": "Users would be able to manage multiple configuration settings through a single plugin interface, simplifying integrations like OpenAI.",
      "rootCause": null,
      "proposedFix": "Implement plugin system to support controlling multiple configuration points at once",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue closed, suggesting the feature may have been implemented or addressed",
      "related": [
        652
      ],
      "keyQuote": "Should support OpenAI integration as an example.",
      "number": 950,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:23.003Z"
    },
    {
      "summary": "Research and implement free-threading support for the Python SDK to make it compatible with Python's free-threading runtime. This requires validating Python Protobuf compatibility and establishing CI-based confirmation of compatibility.",
      "category": "feature",
      "subcategory": "runtime-compatibility",
      "apis": [],
      "components": [
        "runtime",
        "protobuf-bindings",
        "ci"
      ],
      "concepts": [
        "free-threading",
        "python-runtime",
        "concurrency",
        "compatibility",
        "protobuf",
        "research"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to run the SDK with Python's free-threading runtime for better concurrency performance once implemented.",
      "rootCause": null,
      "proposedFix": "Research free-threading opt-in via PyO3, validate Python Protobuf compatibility, and establish CI-based confirmation tests.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        863
      ],
      "keyQuote": "Per https://pyo3.rs/v0.25.1/free-threading.html this should be a simple opt-in, but we may have struggles with Python Protobuf",
      "number": 928,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:03.132Z"
    },
    {
      "summary": "The batch operation feature (specifically workflow reset) in the Temporal Python SDK fails with 'Unknown RPC call start_batch_operation' error. The issue stems from missing implementation in the Python bridge client that wraps the Rust core SDK.",
      "category": "bug",
      "subcategory": "batch-operations",
      "apis": [
        "StartBatchOperation",
        "WorkflowService"
      ],
      "components": [
        "python-bridge-client",
        "rust-core",
        "workflow-service",
        "batch-operations"
      ],
      "concepts": [
        "batch-operations",
        "workflow-reset",
        "rpc-call",
        "maintenance-operations",
        "scale-operations",
        "api-implementation"
      ],
      "severity": "high",
      "userImpact": "Users cannot perform batch workflow resets or other batch operations at scale, blocking maintenance and debugging workflows.",
      "rootCause": "The Python bridge client does not implement the StartBatchOperation RPC call, even though the Rust core has the underlying functionality.",
      "proposedFix": "Implement the StartBatchOperation RPC call in the Python bridge client that connects to the Rust core implementation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Core Rust implementation exists; Python bridge client needed to expose the API. Fix involves implementing the missing RPC call bridge.",
      "related": [],
      "keyQuote": "Core has it, rather, the Python bridge client does not. Tests for that will need the fix.",
      "number": 927,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:05.664Z"
    },
    {
      "summary": "Add opt-in support for applying codec to headers in Python SDK. Currently headers bypass the codec, unlike in .NET and Ruby SDKs. This feature would allow non-search-attribute payloads to be processed through the codec, particularly needed for workflows where the data converter with codec isn't easily accessible.",
      "category": "feature",
      "subcategory": "codec-headers",
      "apis": [],
      "components": [
        "client",
        "worker",
        "data-converter",
        "codec"
      ],
      "concepts": [
        "codec",
        "payload-processing",
        "headers",
        "data-conversion",
        "search-attributes",
        "encryption",
        "serialization"
      ],
      "severity": "medium",
      "userImpact": "Users can now encrypt/decrypt headers through codecs, enabling consistent data protection across all payload types including headers in workflows.",
      "rootCause": null,
      "proposedFix": "Implement opt-in configuration at client level to route non-search-attribute payloads through codec. Update decode_activation and encode_completion in temporalio.bridge.worker.py. Create payload visitor utility for reusable payload processing.",
      "workaround": "Users can only use data converter with codec for activities; workflows lack easy header codec access.",
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [
        331,
        468
      ],
      "keyQuote": "Opt-in for all non-search-attribute payloads to go through codec (may need to be configured at client level)",
      "number": 925,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:28:08.033Z"
    },
    {
      "summary": "Remove the eval_type_backport dependency from the open-ai extra when Python 3.9 reaches end-of-life. This dependency is only needed for Python versions that lack native support for certain type evaluation features.",
      "category": "feature",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "package-management",
        "python-sdk",
        "dependencies"
      ],
      "concepts": [
        "deprecation",
        "version-compatibility",
        "cleanup",
        "maintenance",
        "eol"
      ],
      "severity": "low",
      "userImpact": "Users will benefit from reduced dependencies and simplified package maintenance once Python 3.9 support is dropped.",
      "rootCause": null,
      "proposedFix": "Remove eval_type_backport from the open-ai extra dependencies once Python 3.9 is no longer supported",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When python 3.9 is EOL, we should remove the dependency on `eval_type_backport` in the `open-ai` extra.",
      "number": 921,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:27:48.722Z"
    },
    {
      "summary": "The OpenAIAgentsTracingInterceptor is currently unused in the SDK codebase and lacks test coverage. Tests need to be added to verify its output and functionality.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "OpenAIAgentsTracingInterceptor",
        "tracing",
        "testing"
      ],
      "concepts": [
        "tracing",
        "interceptor",
        "test-coverage",
        "openai",
        "agents",
        "instrumentation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot verify that the tracing interceptor functions correctly, leading to potential bugs or misuse in production environments.",
      "rootCause": null,
      "proposedFix": "Add tests to verify the output of OpenAIAgentsTracingInterceptor",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Tests were added for the OpenAIAgentsTracingInterceptor in the SDK test suite",
      "related": [],
      "keyQuote": "OpenAIAgentsTracingInterceptor is currently unused in the SDK code base, it should have tests which verify its output",
      "number": 916,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:27:50.603Z"
    },
    {
      "summary": "Add test coverage for heartbeating functionality in the OpenAI integration tests for the Python SDK.",
      "category": "feature",
      "subcategory": "activity-heartbeat",
      "apis": [
        "HeartbeatDetails"
      ],
      "components": [
        "activity-executor",
        "test-framework",
        "openai-integration"
      ],
      "concepts": [
        "heartbeat",
        "activity-progress",
        "testing",
        "integration-tests"
      ],
      "severity": "low",
      "userImpact": "Users need test coverage to ensure heartbeating works correctly with OpenAI activities.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Tests for heartbeating in OpenAI tests were added to ensure proper functionality.",
      "related": [],
      "keyQuote": null,
      "number": 915,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:27:46.862Z"
    },
    {
      "summary": "Fix compatibility issues with the OpenAI extension on Python 3.9 and 3.10. Ensure all previously identified errors on older Python versions are resolved and validated through CI testing across all supported versions.",
      "category": "bug",
      "subcategory": "openai-extension",
      "apis": [],
      "components": [
        "openai-extension",
        "python-runtime",
        "ci-tests"
      ],
      "concepts": [
        "version-compatibility",
        "python-3.9",
        "python-3.10",
        "extension-support",
        "regression-testing"
      ],
      "severity": "medium",
      "userImpact": "Users on Python 3.9 and 3.10 who use the OpenAI extension experience errors and reliability issues.",
      "rootCause": "Compatibility problems with the OpenAI extension on older Python versions (3.9-3.10).",
      "proposedFix": "Run tests on all CI versions to ensure the extension works correctly across Python 3.9-3.10.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Errors on Python 3.9-3.10 were identified and fixed; tests validated across all CI versions.",
      "related": [],
      "keyQuote": "Previously errors were seen on older python versions, ensure those problems are gone and run the tests on all CI versions.",
      "number": 914,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:27:30.555Z"
    },
    {
      "summary": "Move OpenAI interceptor configuration from a context manager to worker-level configuration for better scoping and ease of use.",
      "category": "feature",
      "subcategory": "worker-configuration",
      "apis": [],
      "components": [
        "worker",
        "interceptor",
        "openai-integration"
      ],
      "concepts": [
        "configuration",
        "scope",
        "interceptor",
        "worker-setup",
        "ease-of-use"
      ],
      "severity": "low",
      "userImpact": "Users will have an easier and more intuitive way to configure OpenAI overrides at the worker level instead of managing them through a context manager.",
      "rootCause": null,
      "proposedFix": "Configure OpenAI overrides directly on the worker instead of using a context manager approach.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Configuration was moved from context manager to worker-level interceptor.",
      "related": [],
      "keyQuote": "Currently the overrides are provided by a contextmanager, they should be configured on the worker instead to narrow the scope and make it easier to use.",
      "number": 913,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:27:31.681Z"
    },
    {
      "summary": "Investigation into whether the OpenAI data converter dependency can be removed from the Python SDK, or if it must be retained with proper justification in the code.",
      "category": "other",
      "subcategory": "data-converter",
      "apis": [],
      "components": [
        "data-converter",
        "openai-integration"
      ],
      "concepts": [
        "dependency-management",
        "code-cleanup",
        "optional-dependencies",
        "serialization"
      ],
      "severity": "low",
      "userImpact": "Determines whether users must have the OpenAI data converter as a required dependency or if it can be made optional.",
      "rootCause": null,
      "proposedFix": "Either remove the data converter entirely or add code comments justifying its necessity.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Investigation completed and decision made regarding data converter necessity.",
      "related": [],
      "keyQuote": "See if there's a way to remove the data converter entirely. If not, justify that reason in code.",
      "number": 912,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:27:33.904Z"
    },
    {
      "summary": "Request to replace `kwargs` with named parameters in OpenAI APIs that configure activity executions. This improves API usability and IDE support through explicit parameter declaration.",
      "category": "feature",
      "subcategory": "api-design",
      "apis": [],
      "components": [
        "activity-executor",
        "api-interface"
      ],
      "concepts": [
        "named-parameters",
        "api-usability",
        "developer-experience",
        "configuration",
        "activity-execution"
      ],
      "severity": "low",
      "userImpact": "Users will have better IDE autocompletion and clearer API documentation when configuring activity executions with named parameters instead of kwargs.",
      "rootCause": null,
      "proposedFix": "Replace kwargs with explicit named parameters in OpenAI API methods that configure activity executions",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Named parameters were added to OpenAI APIs for activity execution configuration",
      "related": [],
      "keyQuote": "APIs which subsequently configure activity executions should have named parameters instead of `kwargs`",
      "number": 911,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:27:12.397Z"
    },
    {
      "summary": "User requested the ability to install the Temporal Python SDK via conda/anaconda without requiring pip. The feature was successfully implemented by the community through conda-forge.",
      "category": "feature",
      "subcategory": "package-distribution",
      "apis": [],
      "components": [
        "package-manager",
        "conda-forge",
        "distribution"
      ],
      "concepts": [
        "installation",
        "package-management",
        "conda",
        "dependency-resolution",
        "version-management"
      ],
      "severity": "low",
      "userImpact": "Users can now install the Temporal Python SDK directly from conda-forge without needing pip, simplifying setup for conda-based environments.",
      "rootCause": null,
      "proposedFix": "Publish Temporal Python SDK to conda-forge repository to enable conda installation",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Community member (Achira via dgasmith) committed to maintaining the package on conda-forge. Temporalio is now available on conda-forge at https://github.com/conda-forge/temporalio-feedstock",
      "related": [],
      "keyQuote": "Temporalio is now on conda-forge: https://github.com/conda-forge/temporalio-feedstock",
      "number": 894,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:27:15.578Z"
    },
    {
      "summary": "Pyright in strict mode fails to properly type-check `execute_activity` and `start_activity`/`execute_workflow` calls due to complex overload signatures with partially unknown types. The type checker reports the function type as \"partially unknown\" despite being fully defined.",
      "category": "bug",
      "subcategory": "type-checking",
      "apis": [
        "execute_activity",
        "start_activity",
        "execute_workflow"
      ],
      "components": [
        "type-stubs",
        "activity-executor",
        "workflow-executor"
      ],
      "concepts": [
        "type-inference",
        "overloading",
        "strict-mode",
        "pyright",
        "generics"
      ],
      "severity": "medium",
      "userImpact": "Users with Pyright strict mode enabled see spurious type errors when calling activity and workflow methods, hindering strict type checking adoption and IDE experience.",
      "rootCause": "Complex overload definitions with multiple generic parameter combinations cause Pyright's type solver to report partial unknown types in strict mode validation.",
      "proposedFix": "Simplify overload signatures or provide TypeScript-style overload stubs that Pyright can more easily resolve in strict mode.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by addressing related issues #625 and #795 together to improve type stubs and overload definitions.",
      "related": [
        625,
        795
      ],
      "keyQuote": "Type of \"execute_activity\" is partially unknown",
      "number": 893,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:27:17.982Z"
    },
    {
      "summary": "Feature request to add `merge_extra` support to LoggerAdapter classes in the Python SDK, allowing users to set adapter-level extra data. This would follow the stdlib approach introduced in Python 3.13, requiring a custom implementation for broader compatibility.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [
        "LoggerAdapter"
      ],
      "components": [
        "workflow-logger-adapter",
        "activity-logger-adapter",
        "logging"
      ],
      "concepts": [
        "logging",
        "adapter",
        "extra-data",
        "configuration",
        "stdlib-compatibility"
      ],
      "severity": "low",
      "userImpact": "Users would be able to set adapter-level extra data in logging adapters, improving log contextual information and reducing boilerplate code.",
      "rootCause": null,
      "proposedFix": "Implement `merge_extra` method in both `workflow.LoggerAdapter` and `activity.LoggerAdapter` classes, with a custom implementation to support Python versions prior to 3.13.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        276
      ],
      "keyQuote": "For `workflow.LoggerAdapter` and `activity.LoggerAdapter`, consider supporting `merge_extra` to allow users to set adapter-level extra data.",
      "number": 892,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:57.383Z"
    },
    {
      "summary": "User requests the ability to execute Workflow code directly outside of a Temporal Worker to avoid duplicating orchestration logic. Currently, calling workflow methods directly throws a NotInWorkflowEventLoopError because workflow primitives like execute_activity only work in a worker-based durable environment.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "execute_activity",
        "in_workflow"
      ],
      "components": [
        "workflow-runtime",
        "event-loop",
        "activity-executor"
      ],
      "concepts": [
        "code-reuse",
        "durability",
        "non-durable-execution",
        "testing",
        "activity-orchestration"
      ],
      "severity": "low",
      "userImpact": "Users who want to reuse workflow orchestration logic outside a Temporal Worker must duplicate code or create custom abstractions.",
      "rootCause": "Workflow primitives are designed exclusively for durable execution within a Temporal server context and require a specially-crafted deterministic event loop.",
      "proposedFix": "Create a user-level abstraction that checks workflow.in_workflow() to fall back to non-durable execution when outside a worker context, or implement manual retry logic.",
      "workaround": "Use workflow.in_workflow() checks to branch code paths between durable (workflow) and non-durable (standard async) execution; keep a registry of available activities for programmatic access.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I just would like to avoid duplicating the \"activity\" orchestration logic in the workflow layer.",
      "number": 891,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:59.891Z"
    },
    {
      "summary": "The pydantic_data_converter incorrectly serializes Pydantic's SecretStr values as asterisks ('*') instead of preserving the actual secret value during activity execution. This prevents users from passing secrets through activities using the SecretStr type.",
      "category": "bug",
      "subcategory": "data-converter",
      "apis": [
        "execute_workflow"
      ],
      "components": [
        "pydantic_data_converter",
        "data-serialization",
        "activity-input"
      ],
      "concepts": [
        "serialization",
        "secrets",
        "data-conversion",
        "pydantic-types",
        "type-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use Pydantic's SecretStr type for passing sensitive data to activities because the values are serialized as asterisks, making them unusable.",
      "rootCause": "Pydantic intentionally formats SecretStr values as '**********' when converting to JSON to prevent accidental exposure of secrets in logs/serialization.",
      "proposedFix": null,
      "workaround": "Use an alternative data type instead of SecretStr for serialization purposes.",
      "resolution": "wontfix",
      "resolutionDetails": "The issue is a design limitation of Pydantic itself - SecretStr is intentionally not serializable to prevent secret exposure. Users should choose an alternative data type if they need serializable secrets.",
      "related": [],
      "keyQuote": "The SecretStr and SecretBytes will be formatted as either '**********' or '' on conversion to json. It seems Pydantic intentionally doesn't allow serialization of this value.",
      "number": 890,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:55.215Z"
    },
    {
      "summary": "Activity heartbeater incorrectly accesses the current context to retrieve the task token. This breaks the design that allows heartbeating outside of the activity context. The interceptor needs to receive the task token through an alternative mechanism.",
      "category": "bug",
      "subcategory": "activity-heartbeat",
      "apis": [],
      "components": [
        "activity-executor",
        "heartbeater",
        "interceptor"
      ],
      "concepts": [
        "context-access",
        "heartbeat",
        "task-token",
        "activity-execution",
        "design-flaw"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably heartbeat activities from outside the activity context, limiting the flexibility of heartbeat implementations.",
      "rootCause": "The outbound activity heartbeater accesses the current context to get the task token instead of receiving it as a parameter.",
      "proposedFix": "Pass the task token to the interceptor through an alternative mechanism rather than accessing the current context.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [
        279
      ],
      "keyQuote": "we allow heartbeating outside of context. So we should give the interceptor the task token another way.",
      "number": 889,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:38.897Z"
    },
    {
      "summary": "The workflow.in_workflow() function throws a RuntimeError when called from a sync activity because it expects an event loop, but sync activities run in a ThreadPoolExecutor without one. The function should safely return False when no event loop is present instead of raising an error.",
      "category": "bug",
      "subcategory": "activity-execution",
      "apis": [
        "in_workflow"
      ],
      "components": [
        "activity-executor",
        "workflow-context",
        "event-loop"
      ],
      "concepts": [
        "context-detection",
        "thread-pool",
        "event-loop",
        "activity-sync",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot safely check if code is running inside a workflow from sync activities, blocking utility functions that need to guard against workflow context.",
      "rootCause": "in_workflow() calls maybe_current() which expects a running event loop, but sync activities execute in ThreadPoolExecutor threads without event loops.",
      "proposedFix": "Return False (via None from maybe_current) when no running event loop exists instead of raising RuntimeError.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The in_workflow() function was fixed to handle the case where no event loop is running, returning False instead of raising an error.",
      "related": [],
      "keyQuote": "We expect this to be able to be called from all contexts and we need to return False (via None on maybe_current) if there is no running loop.",
      "number": 878,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:38.573Z"
    },
    {
      "summary": "Feature request to expose a worker metric for tracking when workflows fail to evict, allowing proactive detection and alerting on situations where workers get stuck in infinite eviction loops that can cause pod scaling issues and hung shutdowns.",
      "category": "feature",
      "subcategory": "worker-metrics",
      "apis": [],
      "components": [
        "worker",
        "eviction",
        "task-slots"
      ],
      "concepts": [
        "eviction-failure",
        "workflow-slot",
        "deadlock",
        "monitoring",
        "alerting",
        "resource-exhaustion"
      ],
      "severity": "medium",
      "userImpact": "Users cannot proactively detect when workers get stuck in eviction loops, leading to unexpected pod scaling and hung worker shutdowns.",
      "rootCause": "Unknown - issue reporter notes 'for reasons that we still need to root cause' but indicates eviction job failures cause workflows to remain unevicted indefinitely.",
      "proposedFix": "Expose `worker._count_not_evict_count` as a metric; alternatively suggest forcing worker shutdown after prolonged eviction loop or providing more threads than `max_concurrent_workflow_tasks`.",
      "workaround": "Use existing `temporal_worker_task_slots_available` and `temporal_worker_task_slots_used` metrics to infer if slots are never returned, though this is less direct.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Shutting down workflow worker, but 46 workflow(s) could not be evicted previously, so the shutdown may hang",
      "number": 875,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:42.995Z"
    },
    {
      "summary": "Workflow interceptors are initialized before all instance attributes are set in the constructor, causing failures when inbound init or outbound constructor calls methods like set_signal_handler that depend on attributes like _deleting. Interceptor initialization needs to be moved to the end of the constructor.",
      "category": "bug",
      "subcategory": "workflow-interceptors",
      "apis": [
        "set_signal_handler"
      ],
      "components": [
        "workflow-instance",
        "interceptors",
        "constructor"
      ],
      "concepts": [
        "initialization-order",
        "attribute-access",
        "constructor-timing",
        "interceptor-lifecycle",
        "read-only-state"
      ],
      "severity": "high",
      "userImpact": "Users cannot use interceptor init hooks to set up signal handlers or perform other operations that depend on fully initialized workflow attributes.",
      "rootCause": "Workflow interceptors are created and initialized before all instance attributes (_deleting, etc.) are initialized in the constructor, causing AttributeError or state validation failures when interceptor init code tries to access these attributes.",
      "proposedFix": "Move the interceptor create/init call to the end of the workflow instance constructor after all attributes are initialized.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Interceptor initialization was moved to the end of the workflow constructor to ensure all instance attributes are available.",
      "related": [],
      "keyQuote": "Move the interceptor create/init to the _end_ of the workflow instance constructor.",
      "number": 874,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:24.616Z"
    },
    {
      "summary": "Workflow tasks are timing out after 2 minutes despite the start-to-close timeout being set to 1800 seconds. The user suspects their business logic is blocking the asyncio event loop, causing workflow task processing to be slow.",
      "category": "bug",
      "subcategory": "workflow-timeout",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "workflow-engine",
        "event-loop",
        "activity-executor"
      ],
      "concepts": [
        "timeout",
        "event-loop",
        "blocking-operation",
        "workflow-task",
        "asyncio",
        "performance"
      ],
      "severity": "high",
      "userImpact": "Workflows fail unexpectedly when business logic inadvertently blocks the asyncio event loop, preventing timely workflow task processing.",
      "rootCause": "Blocking the asyncio event loop or workflow thread with synchronous/blocking code, preventing event loop from processing workflow tasks within the timeout window.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Determined to be a support/usage question rather than a bug. The timeout occurs because user code is blocking the event loop; the issue was redirected to community support channels (Slack/forums) for assistance.",
      "related": [],
      "keyQuote": "This usually means your code is taking 10s to handle the workflow task which only takes milliseconds usually. This is usually caused by blocking the asyncio event loop with accidental thread-blocking logic.",
      "number": 873,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:22.414Z"
    },
    {
      "summary": "Create a high-level client API for Python SDK to manage worker deployments and control plane operations, following the pattern established in the Go SDK implementation.",
      "category": "feature",
      "subcategory": "worker-versioning",
      "apis": [],
      "components": [
        "client",
        "worker",
        "deployment"
      ],
      "concepts": [
        "versioning",
        "control-plane",
        "deployment-management",
        "worker-lifecycle",
        "api-design"
      ],
      "severity": "medium",
      "userImpact": "Users need a standardized high-level API to manage worker versions and deployments through the control plane.",
      "rootCause": null,
      "proposedFix": "Implement high-level client following the Go SDK pattern as reference, with potential updates to align with current best practices.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Create the high-level client for control plane operations concerning worker deployments. See the Go one as an example",
      "number": 870,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:20.874Z"
    },
    {
      "summary": "A test that validates signal handling with workflow returns is flaky and sometimes fails with an internal error. The test `test_workflow_return_is_honored_when_it_precedes_signal_completion_command` sporadically fails with `ApplicationError: Client should never see this error!` when run repeatedly.",
      "category": "bug",
      "subcategory": "test-flakiness",
      "apis": [
        "start_workflow",
        "signal",
        "result"
      ],
      "components": [
        "worker",
        "signal-handler",
        "workflow-execution"
      ],
      "concepts": [
        "race condition",
        "signal completion",
        "workflow return",
        "timing",
        "flaky test"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably test signal handling with workflow returns due to intermittent test failures, indicating a potential underlying race condition in the SDK.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The following test sometimes fails with `temporalio.exceptions.ApplicationError: Client should never see this error!`",
      "number": 865,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:04.952Z"
    },
    {
      "summary": "Users need a way to identify specific log records (especially for activity and workflow task failures) in custom log handlers without relying on English message patterns. The request is to research and implement a clear identifier mechanism, potentially via the `extra` field on log records.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "logging",
        "activity-executor",
        "workflow-task-executor"
      ],
      "concepts": [
        "log-identification",
        "custom-handlers",
        "log-filtering",
        "activity-failures",
        "workflow-task-failures",
        "structured-logging"
      ],
      "severity": "low",
      "userImpact": "Users can now programmatically identify and handle specific types of log records without parsing English message patterns, enabling custom log level adjustments and specialized handling.",
      "rootCause": null,
      "proposedFix": "Add a clear identifier (potentially via `extra` field) to log records for activity failures and workflow task failures, with documentation showing how to customize log levels based on these identifiers.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented to provide identifiable markers on log records for activity and workflow task failures",
      "related": [],
      "keyQuote": "We need some kind of clear string they can use as a conditional instead of checking English message patterns.",
      "number": 864,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:06.586Z"
    },
    {
      "summary": "Add support for Python 3.14 once it becomes stable. The SDK needs to ensure compatibility with Python 3.14 beta and track blocking dependencies (pyo3, pydantic, fastuuid) for full support when the release is finalized.",
      "category": "feature",
      "subcategory": "python-version-support",
      "apis": [],
      "components": [
        "pyo3-bindings",
        "dependency-management",
        "ci-cd"
      ],
      "concepts": [
        "version-support",
        "python-compatibility",
        "dependency-upgrade",
        "beta-testing",
        "ci-pipeline"
      ],
      "severity": "low",
      "userImpact": "Users cannot use the SDK with Python 3.14 until explicit support is confirmed and blocking dependencies are upgraded.",
      "rootCause": "Blocking dependencies (pyo3 < 0.25, pydantic, fastuuid) do not yet support Python 3.14.",
      "proposedFix": "Upgrade pyo3 to version 0.25, wait for pydantic and fastuuid to support Python 3.14, then update CI and drop Python 3.9 support.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Python 3.14 support was confirmed and implemented by upgrading blocking dependencies.",
      "related": [
        816
      ],
      "keyQuote": "Python 3.14 is now beta. Please confirm support. If this doesn't include explicitly upgrading CI for fears of it not being stable yet, please open a separate issue to do that in October.",
      "number": 863,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:26:03.733Z"
    },
    {
      "summary": "User seeks guidance on integrating Temporal Python SDK metrics with FastAPI and Prometheus FastAPI Instrumentator to expose all metrics through a single endpoint, as Temporal metrics are appearing on a different port/endpoint.",
      "category": "question",
      "subcategory": "metrics-integration",
      "apis": [
        "Runtime",
        "MetricBuffer"
      ],
      "components": [
        "telemetry",
        "metrics",
        "runtime"
      ],
      "concepts": [
        "metrics",
        "prometheus",
        "fastapi",
        "instrumentation",
        "opentelemetry",
        "integration",
        "observability"
      ],
      "severity": "low",
      "userImpact": "Users struggle to consolidate Temporal SDK metrics with their FastAPI application metrics into a unified monitoring endpoint.",
      "rootCause": "Temporal metrics are emitted from Rust-based Core and appear on a different port/endpoint by default, not automatically integrated with FastAPI instrumentation.",
      "proposedFix": "Create a global `temporalio.runtime.Runtime` with telemetry config using `temporalio.runtime.MetricBuffer`, then repeatedly call `retrieve_updates` to get metric updates and expose them through the application's metrics endpoint.",
      "workaround": "Use an advanced metric buffer option in the Runtime telemetry configuration to manually retrieve and handle metrics.",
      "resolution": "invalid",
      "resolutionDetails": "Closed as not an issue but rather a support question, with guidance provided on recommended approaches for metrics integration.",
      "related": [],
      "keyQuote": "When creating the runtime, you can pass `telemetry` config that has `metrics` as a `temporalio.runtime.MetricBuffer` you create.",
      "number": 856,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:25:50.703Z"
    },
    {
      "summary": "Helper function for execute_activity fails with invalid TaskQueue error when using certain retry policy configurations. The issue appears to be triggered by specific retry policy settings (initial_interval >= 30 seconds) rather than the helper function itself.",
      "category": "bug",
      "subcategory": "activity-execution",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "activity-executor",
        "retry-policy",
        "workflow-context"
      ],
      "concepts": [
        "retry-policy",
        "task-queue",
        "helper-functions",
        "timeout-configuration",
        "configuration-inheritance"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to create helper functions for executing activities with certain retry policy configurations encounter task queue errors that don't occur without the helper wrapper.",
      "rootCause": "Unclear - appears to be related to retry policy configuration (initial_interval timing) affecting task queue resolution, but maintainer unable to reproduce consistently",
      "proposedFix": null,
      "workaround": "Use initial_interval=timedelta(seconds=1) instead of larger values like 30 seconds in the retry policy",
      "resolution": "fixed",
      "resolutionDetails": "Issue was determined to be caused by user-side bugs in the reproduction case (wrong activity reference, invalid retry policy with maximum_interval < initial_interval). When corrected, the helper pattern works as expected.",
      "related": [],
      "keyQuote": "when I set initial_interval=timedelta(seconds=1), it works perfectly. But if I change it to initial_interval=timedelta(seconds=30), it throws an \"invalid TaskQueue\" error",
      "number": 855,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:25:48.653Z"
    },
    {
      "summary": "The Python SDK fails to serialize Literal types from the standard typing module because it only checks against typing_extensions.Literal. This causes a \"Unserializable type\" error when using Literal type hints in dataclass activity inputs.",
      "category": "bug",
      "subcategory": "serialization",
      "apis": [
        "value_to_type"
      ],
      "components": [
        "converter",
        "type-validation",
        "serialization"
      ],
      "concepts": [
        "literal-types",
        "type-checking",
        "compatibility",
        "typing-module"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use Python's standard Literal type hints in activity inputs without switching to typing_extensions, creating an undocumented workaround requirement.",
      "rootCause": "The converter performs referential equality checks against typing_extensions.Literal instead of also checking the stdlib typing.Literal, which are separate objects in Python < 3.10.",
      "proposedFix": "Check both typing.Literal and typing_extensions.Literal during validation, or use the standard typing module instead of typing_extensions.",
      "workaround": "Import and use Literal from typing_extensions instead of typing.",
      "resolution": "fixed",
      "resolutionDetails": "The maintainer acknowledged the bug and committed to fixing it by also checking the stdlib Literal instead of just typing_extensions.",
      "related": [],
      "keyQuote": "we need to also check the stdlib `Literal` instead of just `typing_extensions`",
      "number": 852,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:25:45.687Z"
    },
    {
      "summary": "Test server in time-skipping mode does not populate userMetadata.summary on HistoryEvents, despite the summary attribute being passed when executing workflows and activities.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [
        "execute_activity",
        "fetch_history_events"
      ],
      "components": [
        "test-server",
        "time-skipping-mode",
        "history-events",
        "metadata-handling"
      ],
      "concepts": [
        "userMetadata",
        "summary",
        "time-skipping",
        "test-framework",
        "workflow-execution",
        "activity-execution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot test userMetadata.summary functionality in time-skipping mode, limiting test coverage for UI features that display workflow/activity summaries.",
      "rootCause": "Test server's time-skipping mode does not properly populate userMetadata.summary field on HistoryEvents, while dev mode handles it correctly.",
      "proposedFix": null,
      "workaround": "Clear temporal-prefixed files in $TMPDIR and retry; switch to dev mode for testing userMetadata.summary functionality.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2441
      ],
      "keyQuote": "I can see `userMetadata.summary` populated on the `HistoryEvent` when running in dev mode, but it is missing in time-skipping mode.",
      "number": 851,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:25:30.436Z"
    },
    {
      "summary": "_RestrictedProxy incorrectly invokes __format__ on class types, causing a TypeError when formatting type hints in error messages. The issue manifests when trying to format a proxied datetime class in f-strings.",
      "category": "bug",
      "subcategory": "payload-conversion",
      "apis": [
        "from_payload"
      ],
      "components": [
        "_RestrictedProxy",
        "JSONPlainPayloadConverter",
        "payload-converter"
      ],
      "concepts": [
        "type-hints",
        "format-protocol",
        "proxy-wrapper",
        "error-handling",
        "deserialization"
      ],
      "severity": "low",
      "userImpact": "Users encounter confusing TypeError messages when defining custom from_payload functions that format type hints in error messages.",
      "rootCause": "_RestrictedProxy.__format__ calls real_datetime.__format__(\"\") instead of delegating to type.__format__(proxy, \"\") which would convert to string appropriately.",
      "proposedFix": "Fix _RestrictedProxy.__format__ to properly handle class types by delegating to the metaclass __format__ method.",
      "workaround": "Explicitly convert to string using f\"{type_hint!s}\" instead of f\"{type_hint}\"",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in version 1.10 via PR #757",
      "related": [
        301,
        757
      ],
      "keyQuote": "format(proxy_datetime, \"\") will run _RestrictedProxy.__format__(proxy_datetime, \"\"), which will run real_datetime.__format__(\"\"), which throws the above error.",
      "number": 850,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:25:29.116Z"
    },
    {
      "summary": "Add a new `workflow_start_time` field to workflow.Info to provide the actual workflow start time from the initialize job. The existing start_time field was incorrectly representing the first task start time instead of the workflow start time.",
      "category": "feature",
      "subcategory": "workflow-info",
      "apis": [
        "workflow.Info"
      ],
      "components": [
        "workflow-runtime",
        "workflow-info",
        "initialize-job"
      ],
      "concepts": [
        "workflow-lifecycle",
        "timing",
        "initialization",
        "start-time",
        "task-scheduling"
      ],
      "severity": "medium",
      "userImpact": "Users can now distinguish between workflow start time and first task execution time, enabling more accurate workflow timing analysis.",
      "rootCause": "The existing start_time field was incorrectly set to the first task start time instead of the workflow start time from the initialize job.",
      "proposedFix": "Add a new workflow_start_time field to workflow.Info containing the actual workflow start time from the initialize job.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented to add workflow_start_time field to workflow.Info",
      "related": [],
      "keyQuote": "add a new `workflow_start_time` that has the actual workflow start time from the initialize job",
      "number": 849,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:25:30.797Z"
    },
    {
      "summary": "Updates are lost when a workflow execution is not cached on the worker after restart, requiring the update to be reissued. The issue occurs because updates are not properly replayed when the worker reconnects to an existing workflow.",
      "category": "bug",
      "subcategory": "workflow-replay",
      "apis": [
        "execute_activity",
        "wait_condition",
        "update"
      ],
      "components": [
        "worker",
        "workflow-execution",
        "update-handler",
        "replay-engine"
      ],
      "concepts": [
        "replay",
        "caching",
        "state-reconstruction",
        "update-handling",
        "worker-restart",
        "workflow-continuation"
      ],
      "severity": "high",
      "userImpact": "Users must manually reissue updates when workers restart, preventing reliable workflow state transitions and requiring workarounds in production systems.",
      "rootCause": "Updates are not properly applied during workflow replay when the execution is not in the worker's cache after restart.",
      "proposedFix": "Fix Core to properly handle updates during replay - resolved in sdk-core PR #910",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in Core in sdk-core PR #910; Python SDK requires Core update and release",
      "related": [
        910
      ],
      "keyQuote": "If an update occurs when the workflow execution is not cached on the worker, the update is effectively \"lost\", and I need to issue another update for it to be applied.",
      "number": 848,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:25:09.472Z"
    },
    {
      "summary": "Request to add SDK support for activity reset functionality in the Python SDK, tracking feature parity with other SDKs.",
      "category": "feature",
      "subcategory": "activity-reset",
      "apis": [
        "ActivityOptions"
      ],
      "components": [
        "activity-executor",
        "worker",
        "activity-context"
      ],
      "concepts": [
        "activity-reset",
        "state-management",
        "workflow-replay",
        "failure-recovery"
      ],
      "severity": "medium",
      "userImpact": "Python SDK users cannot reset activities to retry from a specific point, limiting failure recovery capabilities.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Activity reset support was implemented in the Python SDK to match feature parity across other SDKs.",
      "related": [
        620
      ],
      "keyQuote": "See https://github.com/temporalio/features/issues/620",
      "number": 847,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:25:11.428Z"
    },
    {
      "summary": "When a local activity is invoked that doesn't exist, the system retries at the activity level indefinitely instead of failing the workflow task. Local activities should fail the entire workflow task when the activity cannot be found.",
      "category": "bug",
      "subcategory": "local-activities",
      "apis": [
        "execute_local_activity"
      ],
      "components": [
        "local-activity-executor",
        "workflow-task-handler",
        "retry-logic"
      ],
      "concepts": [
        "local-activities",
        "error-handling",
        "workflow-failure",
        "activity-resolution",
        "retry-behavior"
      ],
      "severity": "high",
      "userImpact": "Developers cannot catch missing local activity errors; workflows hang in retry loops instead of failing gracefully.",
      "rootCause": "Local activity invocation retries at the activity level instead of propagating the error to fail the workflow task when the activity definition cannot be found.",
      "proposedFix": "Modify local activity execution to detect missing activities and fail the workflow task instead of retrying at the activity level.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [
        221
      ],
      "keyQuote": "Right now if an LA is invoked that doesn't exist, it will just retry at the _activity_ level which doesn't make sense and will never work.",
      "number": 845,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:25:10.740Z"
    },
    {
      "summary": "Lazy-load static workflow summary and details in the Python client instead of eagerly decoding them upon request.",
      "category": "feature",
      "subcategory": "client-workflow-api",
      "apis": [
        "GetWorkflowExecutionHistory"
      ],
      "components": [
        "client",
        "workflow-executor",
        "result-decoder"
      ],
      "concepts": [
        "lazy-loading",
        "performance",
        "memory-efficiency",
        "decoding",
        "client-side-optimization"
      ],
      "severity": "low",
      "userImpact": "Users will benefit from faster client initialization and reduced memory usage when accessing workflow summaries and details.",
      "rootCause": null,
      "proposedFix": "Implement lazy decoding of static summary and details fields to defer processing until explicitly requested.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the feature was implemented or decided upon.",
      "related": [],
      "keyQuote": "Today we eagerly decode these, change to be lazy upon request",
      "number": 840,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:49.963Z"
    },
    {
      "summary": "Feature request to warn developers when the server-provided task start time differs significantly from when the SDK begins processing the task, helping identify timing issues.",
      "category": "feature",
      "subcategory": "task-processing",
      "apis": [],
      "components": [
        "task-processor",
        "time-handling"
      ],
      "concepts": [
        "timing",
        "clock-skew",
        "task-start-time",
        "diagnostics",
        "warning"
      ],
      "severity": "low",
      "userImpact": "Users would receive warnings about timing discrepancies that could help debug synchronization issues between server and SDK.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers determined the feature request was too niche to implement.",
      "related": [
        616
      ],
      "keyQuote": "We're not gonna do this. Too niche.",
      "number": 839,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:51.532Z"
    },
    {
      "summary": "Temporal's logger adapter uses a Dict for the 'extra' field, which violates OpenTelemetry's format requirement for basic data types. Users need a way to flatten or customize the extra field to avoid OpenTelemetry warnings while retaining activity/workflow trace logs.",
      "category": "feature",
      "subcategory": "logging-opentelemetry",
      "apis": [],
      "components": [
        "activity-logger",
        "workflow-logger",
        "logger-adapter"
      ],
      "concepts": [
        "logging",
        "opentelemetry",
        "attributes",
        "nested-data",
        "compatibility",
        "tracing"
      ],
      "severity": "medium",
      "userImpact": "Users integrating with OpenTelemetry receive warnings when Temporal logger adapters include nested dicts in the extra field, complicating activity/workflow tracing.",
      "rootCause": "OpenTelemetry's Python SDK validates attribute types and removes non-basic types (dict, list) from attributes, as they don't match _VALID_ATTR_VALUE_TYPES, though nested maps are technically supported per the spec.",
      "proposedFix": "Add an option to flatten the logger's extra field by prefixing keys with 'temporal_activity.' and 'temporal_workflow.', or enhance temporalio.contrib.opentelemetry with a handler for nested extra data.",
      "workaround": "Disable default extra info (activity_info_on_extra = False, workflow_info_on_extra = False) and manually set logger extra fields using activity.info()._logger_details() or workflow.info()._logger_details() at the start of each activity/workflow. Alternatively, extend activity.LoggerAdapter and workflow.LoggerAdapter to override the process method.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        838
      ],
      "keyQuote": "OpenTelemetry's Python SDK verifies attribute types and removes non-basic types, complicating the ability to trace specific activities or workflows.",
      "number": 837,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:55.922Z"
    },
    {
      "summary": "Feature request to implement environment configuration for the Python SDK, aligning with cross-SDK standards for external client configuration.",
      "category": "feature",
      "subcategory": "configuration",
      "apis": [],
      "components": [
        "client",
        "configuration",
        "external-client"
      ],
      "concepts": [
        "environment-configuration",
        "client-setup",
        "external-client",
        "sdk-standards",
        "configuration-management"
      ],
      "severity": "medium",
      "userImpact": "Users need standardized environment-based configuration for external clients to match other SDK implementations.",
      "rootCause": null,
      "proposedFix": "Implement environment configuration following the external-client-configuration proposal for cross-SDK consistency.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented to support environment configuration for external client setup.",
      "related": [
        441
      ],
      "keyQuote": "See https://github.com/temporalio/features/issues/441 and https://github.com/temporalio/proposals/blob/master/all-sdk/external-client-configuration.md",
      "number": 835,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:34.588Z"
    },
    {
      "summary": "Python 3.13.3 on macOS Intel hangs post-test with workers unable to shutdown properly, resulting in connection refused errors from gRPC retry attempts.",
      "category": "bug",
      "subcategory": "test-infrastructure",
      "apis": [],
      "components": [
        "worker",
        "shutdown",
        "test-runner",
        "connection-pool"
      ],
      "concepts": [
        "hang",
        "worker-shutdown",
        "connection-refused",
        "test-cleanup",
        "threading",
        "macOS-specific",
        "Python-version-compatibility"
      ],
      "severity": "high",
      "userImpact": "Tests hang on Python 3.13.3 macOS Intel in CI, blocking test execution and requiring manual intervention.",
      "rootCause": "Worker unable to shutdown properly in specific Python 3.13.3 macOS Intel situation, causing lingering connections to attempt retries.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Python 3.13.3 on macOS Intel sometimes hangs post pytest run in our CI",
      "number": 834,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:37.741Z"
    },
    {
      "summary": "Feature request to display the retry policy on activity info, allowing developers to inspect retry configuration at runtime.",
      "category": "feature",
      "subcategory": "activity-info",
      "apis": [
        "ActivityInfo"
      ],
      "components": [
        "activity-executor",
        "activity-info"
      ],
      "concepts": [
        "retry-policy",
        "activity-metadata",
        "configuration-inspection",
        "runtime-introspection"
      ],
      "severity": "low",
      "userImpact": "Developers would be able to programmatically access retry policy information from ActivityInfo for debugging and monitoring purposes.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented to expose retry policy information in ActivityInfo",
      "related": [
        615
      ],
      "keyQuote": "Show retry policy on activity info",
      "number": 831,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:35.425Z"
    },
    {
      "summary": "Importing uvicorn in a file used by Temporal workflows causes a RestrictedWorkflowAccessError because uvicorn's initialization code calls gettext.gettext.__call__, which is restricted in the workflow sandbox.",
      "category": "bug",
      "subcategory": "workflow-sandbox",
      "apis": [
        "Worker",
        "Client",
        "start_workflow"
      ],
      "components": [
        "workflow-sandbox",
        "importer",
        "restrictions"
      ],
      "concepts": [
        "import-restriction",
        "third-party-library",
        "gettext",
        "sandbox-isolation",
        "determinism",
        "module-initialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot use FastAPI with uvicorn in the same codebase as Temporal workflows without restructuring their code.",
      "rootCause": "The workflow sandbox restricts access to gettext.gettext.__call__, which is called during uvicorn's module initialization. uvicorn's use of click.Path causes click to call gettext for localization, triggering the restriction.",
      "proposedFix": null,
      "workaround": "Mark the uvicorn import as pass-through in the workflow sandbox configuration to prevent it from being sandboxed.",
      "resolution": "fixed",
      "resolutionDetails": "Added uvicorn to the list of pass-through modules in the workflow sandbox importer so it bypasses determinism restrictions.",
      "related": [],
      "keyQuote": "Cannot access gettext.gettext.__call__ from inside a workflow. If this is code from a module not used in a workflow or known to only be used deterministically from a workflow, mark the import as pass through.",
      "number": 827,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:19.385Z"
    },
    {
      "summary": "Test `test_unfinished_handler_on_workflow_termination` fails in Python 3.13 with time skipping test server when using update handlers with continue-as-new workflow termination, reporting 'workflow execution already completed' error.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-framework",
        "workflow-termination",
        "update-handler",
        "time-skipping"
      ],
      "concepts": [
        "time-skipping",
        "workflow-termination",
        "continue-as-new",
        "update-handler",
        "test-server",
        "handler-completion"
      ],
      "severity": "medium",
      "userImpact": "Tests cannot run reliably on Python 3.13 with time skipping, blocking test coverage for handler termination scenarios.",
      "rootCause": "Interaction between time skipping, update handlers, continue-as-new workflow termination, and handler waiting logic causes premature completion detection.",
      "proposedFix": null,
      "workaround": "Tests remain skipped to avoid failure.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        644
      ],
      "keyQuote": "when running in Python 3.13 with time skipping test server, all permutations fail with 'workflow execution already completed'",
      "number": 826,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:19.274Z"
    },
    {
      "summary": "Python SDK release needed to address CVE-2025-29787 affecting the zip crate in the bundled Rust SDK. The vulnerability fix was already merged to master but no new release has been published to PyPI yet.",
      "category": "bug",
      "subcategory": "release-management",
      "apis": [],
      "components": [
        "rust-sdk",
        "zip-crate",
        "release-process"
      ],
      "concepts": [
        "vulnerability",
        "dependency",
        "CVE",
        "release",
        "security"
      ],
      "severity": "high",
      "userImpact": "Users with automated vulnerability scanning tools are flagged for vulnerable dependencies despite the fix being available in the codebase.",
      "rootCause": "The zip crate vulnerability (CVE-2025-29787) was included in the Rust SDK bundled with the Python SDK's last release.",
      "proposedFix": "Release a new version of the Python SDK to PyPI that includes the patched Rust SDK version already in the master branch.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Released in version 1.11.0 which includes the patched Rust SDK with the CVE-2025-29787 fix.",
      "related": [
        802
      ],
      "keyQuote": "Current `master` of this repository already has the core sdk bumped to a version that is not vulnerable, there just hasn't been a release uploaded to pypi since it was patched.",
      "number": 824,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:22.058Z"
    },
    {
      "summary": "User requested a workflow.disabledeadlockdetection context manager for data converters in Python SDK, similar to Go and Java. The request was closed after learning that deadlock detection is intentionally only applied to payload converters (not codecs), and the recommended approach is to move heavy I/O work to payload codecs instead.",
      "category": "feature",
      "subcategory": "data-converter",
      "apis": [],
      "components": [
        "data-converter",
        "payload-converter",
        "payload-codec",
        "sandbox"
      ],
      "concepts": [
        "deadlock-detection",
        "performance",
        "serialization",
        "workflow-task-processing",
        "timeout",
        "codec-converter-separation"
      ],
      "severity": "low",
      "userImpact": "Users encountering deadlock detection errors in custom data converters need to understand the architecture and move heavy work to payload codecs instead.",
      "rootCause": "Workflow tasks with heavy serialization/deserialization work in payload converters can exceed the 2-second deadlock detection threshold; the Python SDK deliberately applies deadlock detection only to payload converters (which run in the sandbox) and not codecs (which run on boundaries).",
      "proposedFix": "Move heavy I/O and async work from payload converters to custom payload codecs, which run outside the sandbox and are not subject to deadlock detection (only to the 10-second task timeout).",
      "workaround": "Implement custom payload codecs to handle serialization of generics and Pydantic objects, as these run on task boundaries before/after sandbox processing.",
      "resolution": "wontfix",
      "resolutionDetails": "The feature was intentionally not implemented because the Python SDK already has proper separation of payload codecs and converters, unlike Go/Java. Users should move heavy work to codecs.",
      "related": [],
      "keyQuote": "Move my data converter code into the codec to ensure I have enough time to serialize.",
      "number": 823,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:05.643Z"
    },
    {
      "summary": "Implement application failure logging and metrics behavior in Python SDK according to ApplicationErrorCategory, following the pattern established in core SDK. This requires alignment with core SDK changes and feature specifications.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "logging",
        "metrics",
        "error-handling"
      ],
      "concepts": [
        "application-failure",
        "error-category",
        "logging-behavior",
        "metrics-behavior",
        "error-classification",
        "failure-handling"
      ],
      "severity": "medium",
      "userImpact": "Users need consistent logging and metrics behavior for application failures across SDK implementations, improving observability and debugging capabilities.",
      "rootCause": null,
      "proposedFix": "Apply logging behavior according to ApplicationErrorCategory specification, with metrics handling delegated to core SDK",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented following core SDK specification and features issue guidelines",
      "related": [
        614,
        897
      ],
      "keyQuote": "Core-based SDKs only need to apply corresponding logging behaviour, core handles metrics.",
      "number": 820,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:02.064Z"
    },
    {
      "summary": "Upgrade PyO3 to the latest version and migrate from pyo3-asyncio to pyo3-async-runtimes, the official successor. This addresses limitations in spawning tasks from the current Tokio runtime instead of a global one, with potential benefits for GraalPy support.",
      "category": "feature",
      "subcategory": "async-runtime",
      "apis": [],
      "components": [
        "bridge",
        "runtime",
        "tokio-integration"
      ],
      "concepts": [
        "async-runtime",
        "pyo3-binding",
        "asyncio-interop",
        "tokio-spawning",
        "dependency-upgrade"
      ],
      "severity": "medium",
      "userImpact": "Users will benefit from an official, well-maintained async runtime integration that better supports custom Tokio runtime contexts and potential GraalPy compatibility.",
      "rootCause": "pyo3-asyncio is community-maintained and only supports spawning from global Tokio runtime; pyo3-async-runtimes is the official PyO3 successor with better flexibility",
      "proposedFix": "Upgrade to pyo3-async-runtimes and verify it properly handles spawning from the current Tokio runtime context instead of requiring a global one",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed by upgrading PyO3 and migrating to pyo3-async-runtimes",
      "related": [
        300
      ],
      "keyQuote": "pyo3-async-runtimes is now available as a successor to pyo3-asyncio, we should try to use it",
      "number": 816,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:24:00.111Z"
    },
    {
      "summary": "Feature request to fail workflows directly when pydantic.ValidationError occurs during payload decoding, instead of wrapping it in RuntimeError and keeping the workflow running indefinitely with a 'Failed decoding arguments' error.",
      "category": "feature",
      "subcategory": "data-conversion",
      "apis": [],
      "components": [
        "pydantic_data_converter",
        "workflow_instance",
        "worker"
      ],
      "concepts": [
        "payload-validation",
        "error-handling",
        "workflow-failure",
        "data-conversion",
        "exception-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot properly fail workflows when validation errors occur during pydantic payload decoding, leading to workflows stuck in a failed state that never complete.",
      "rootCause": "ValidationError from pydantic converter is wrapped as RuntimeError in _workflow_instance.py, preventing workflow_failure_exception_types from catching the original exception type.",
      "proposedFix": "Stop wrapping the exception (re-raise original) if the exception is already a workflow failure exception, or raise FailureError directly from the pydantic converter.",
      "workaround": "Specify RuntimeError in workflow_failure_exception_types, though this is overly broad and catches unrelated runtime errors.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        329,
        321
      ],
      "keyQuote": "I would like to see a workflow failing directly upon 'Failed decoding arguments' while not having to specify a RuntimeError as an error in workflow_failure_exception_types.",
      "number": 815,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:45.498Z"
    },
    {
      "summary": "Python client fails to connect to self-hosted Temporal server through a proxy with authorization headers, experiencing PermissionDenied and Unimplemented errors after 60 seconds of initial connection.",
      "category": "bug",
      "subcategory": "proxy-connection",
      "apis": [
        "Client.connect"
      ],
      "components": [
        "client",
        "bridge",
        "network-connection",
        "proxy"
      ],
      "concepts": [
        "proxy",
        "authorization",
        "connection-timeout",
        "gRPC",
        "503-error",
        "polling",
        "authentication"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably establish connections to self-hosted Temporal servers through proxies with authorization headers, causing worker failures and service unavailability.",
      "rootCause": "Proxy misconfiguration with improper timeout parameters (soft max connection age < 6m, hard max < 8m) causing connection interruption during long-lived gRPC calls that require 70+ seconds.",
      "proposedFix": "Configure proxy with proper parameters: allow calls for more than 80s, soft max connection age of no less than 6m, and hard max connection age of 8m to align with Temporal server expectations.",
      "workaround": "Reconfigure the proxy with appropriate timeout and connection age settings as per Temporal server requirements (70s, 5m, 6m10s).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        631
      ],
      "keyQuote": "proxy has to be setup with the proper parameters, specifically it should allow individual calls for more than 80s, have a soft max connection age of no less than 6m",
      "number": 814,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:48.146Z"
    },
    {
      "summary": "Heartbeating activities should be interrupted when activities are paused to properly handle pause state transitions. This ensures that heartbeat operations don't continue sending signals during pause periods.",
      "category": "bug",
      "subcategory": "activity-heartbeat",
      "apis": [],
      "components": [
        "activity-executor",
        "heartbeat-manager",
        "activity-state-manager"
      ],
      "concepts": [
        "heartbeat",
        "pause",
        "activity-lifecycle",
        "signal-interruption",
        "state-management",
        "resource-cleanup"
      ],
      "severity": "medium",
      "userImpact": "Users experience unexpected heartbeat signals continuing during paused activities, which can cause state inconsistencies and resource management issues.",
      "rootCause": "Heartbeat loops are not respecting or reacting to pause state changes in activities, continuing to send heartbeat signals even when activities should be paused.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Heartbeat interruption logic was implemented to stop heartbeating when activities transition to paused state.",
      "related": [
        602
      ],
      "keyQuote": "Heartbeating activities should be interrupted when the activities are paused.",
      "number": 812,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:45.106Z"
    },
    {
      "summary": "Implement proper cancellation handling for activities and child workflows in the Python SDK, including support for `cancelled`, `uncancel`, and `cancelling` methods with appropriate behavior for each operation.",
      "category": "feature",
      "subcategory": "activity-cancellation",
      "apis": [
        "cancelled",
        "uncancel",
        "cancelling"
      ],
      "components": [
        "activity-executor",
        "child-workflow-executor",
        "cancellation-handler"
      ],
      "concepts": [
        "cancellation",
        "activity-lifecycle",
        "exception-handling",
        "workflow-control"
      ],
      "severity": "medium",
      "userImpact": "Users cannot properly handle cancellation states for activities and child workflows, limiting control over workflow execution.",
      "rootCause": null,
      "proposedFix": "Implement `cancelled` to return true when activity is canceled, `uncancel` to raise a non-Temporal exception for unsupported operation, and ensure `cancelling` continues to work as expected.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Implement proper behavior for `cancelled`, `uncancel`, and `cancelling` of activities and child workflows.",
      "number": 810,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:29.050Z"
    },
    {
      "summary": "Stack trace information is lost when errors propagate through the Temporal distributed system, particularly in activity and child workflow errors. The Failure protobuf includes a stack_trace field that could be used to populate error tracking systems like Sentry with complete exception chains.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [
        "execute_activity",
        "execute_child_workflow"
      ],
      "components": [
        "converter",
        "failure-handling",
        "exception-chain"
      ],
      "concepts": [
        "stack-trace",
        "error-propagation",
        "distributed-exceptions",
        "sentry-integration",
        "failure-protobuf",
        "traceback",
        "error-chain"
      ],
      "severity": "medium",
      "userImpact": "Users cannot access complete stack traces for errors in activity and child workflow failures, hindering debugging and error tracking in monitoring systems like Sentry.",
      "rootCause": "Python exceptions don't support deserializing remote tracebacks from arbitrary strings (which can come from different languages/versions), and the Failure object's stack_trace field is not exposed through the exception chain.",
      "proposedFix": "Expose stack_trace from the failure object through the exception chain, or provide a utility function to collect stack traces from the failure object similar to the example code provided in comments.",
      "workaround": "Access raw failure object via FailureError.failure.stack_trace and manually construct stack trace information using utility functions.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The SDK already includes stack trace on the errors, it's just via `failure.stack_trace` instead of `__traceback__`",
      "number": 807,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:30.523Z"
    },
    {
      "summary": "Proposal to run workflow and activity workers in separate threads instead of sharing the same event loop. This would prevent blocking I/O in async activities from blocking workflow progress and make failure modes easier to understand.",
      "category": "feature",
      "subcategory": "worker-threading",
      "apis": [],
      "components": [
        "worker",
        "event-loop",
        "activity-executor",
        "workflow-worker"
      ],
      "concepts": [
        "threading",
        "concurrency",
        "event-loop",
        "blocking-io",
        "async",
        "resource-isolation"
      ],
      "severity": "medium",
      "userImpact": "Users could prevent misbehaving async activities from blocking workflow execution, reducing blast radius of failures.",
      "rootCause": "Workflow and activity workers currently share the same event loop, allowing blocking I/O in activities to affect workflow progress.",
      "proposedFix": "Run workflow workers in separate threads with their own event loop, potentially as a workflow-only feature to avoid complexity.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Running these (and Nexus workers when they arrive) in different threads would prevent an async activity that incorrectly does blocking I/O from blocking workflow progress.",
      "number": 803,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:31.488Z"
    },
    {
      "summary": "Feature request to treat TimeoutError as a standard workflow failure exception instead of causing task failures. This would allow workflows to handle timeout errors gracefully and is considered backwards compatible.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [
        "wait_condition",
        "asyncio.wait_for"
      ],
      "components": [
        "workflow-executor",
        "error-handling",
        "task-failure"
      ],
      "concepts": [
        "timeout",
        "exception-handling",
        "workflow-failure",
        "task-failure",
        "backwards-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to handle TimeoutError as a standard workflow failure exception instead of task failures, enabling more graceful timeout handling.",
      "rootCause": null,
      "proposedFix": "Allow TimeoutError to be considered a standard workflow failure exception rather than treating it as a task failure.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue has been addressed through implementation of TimeoutError handling as a standard workflow failure exception.",
      "related": [],
      "keyQuote": "We should allow `TimeoutError` to be considered a standard workflow failure exception. We consider this a backwards compatible change",
      "number": 798,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:16.874Z"
    },
    {
      "summary": "Feature request to add serialization context support for codecs and converters in the Python SDK. This would enable more flexible and context-aware serialization/deserialization of workflow data.",
      "category": "feature",
      "subcategory": "serialization",
      "apis": [],
      "components": [
        "codecs",
        "converters",
        "serialization"
      ],
      "concepts": [
        "serialization",
        "deserialization",
        "context",
        "codec",
        "converter",
        "data-handling"
      ],
      "severity": "medium",
      "userImpact": "Users would be able to pass context information to codecs and converters for more sophisticated serialization strategies.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to support serialization context in codecs and converters",
      "related": [
        434
      ],
      "keyQuote": "See https://github.com/temporalio/features/issues/434",
      "number": 796,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:14.778Z"
    },
    {
      "summary": "Pyright strict type checking returns partiallyUnknown error when using execute_activity due to an implicit Any type in the optional Type parameter at workflow.py line 2185. Adding an explicit Any annotation resolves the type checking issue.",
      "category": "bug",
      "subcategory": "type-checking",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "workflow",
        "type-stubs",
        "pyright-integration"
      ],
      "concepts": [
        "type-inference",
        "typing",
        "strict-mode",
        "generic-types",
        "optional-types"
      ],
      "severity": "low",
      "userImpact": "Users get type checking errors in their IDEs when using execute_activity with Pyright strict mode, requiring workarounds or disabling strict checking.",
      "rootCause": "Implicit Any on the optional Type parameter in workflow.py line 2185 prevents Pyright from properly inferring types in strict mode.",
      "proposedFix": "Add explicit Any annotation to the optional Type parameter at the problematic line.",
      "workaround": "Add explicit Any type annotation locally or disable Pyright strict mode.",
      "resolution": "fixed",
      "resolutionDetails": "Bug was acknowledged and PR was welcomed by maintainers, indicating it was fixed via contributed PR.",
      "related": [],
      "keyQuote": "Type checking with Pyright strict returns partiallyUnknown error when using Temporal execute_activity",
      "number": 795,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:14.412Z"
    },
    {
      "summary": "Feature request to add an option for OpenTelemetry workflow spans even when client spans are not present. Currently, spans are omitted to avoid orphaned spans during replay, but users starting workflows from schedules or CLI want workflow spans regardless.",
      "category": "feature",
      "subcategory": "tracing-otel",
      "apis": [],
      "components": [
        "tracing-interceptor",
        "otel-integration",
        "workflow-spans"
      ],
      "concepts": [
        "opentelemetry",
        "distributed-tracing",
        "span-context",
        "workflow-replay",
        "orphaned-spans",
        "opt-in"
      ],
      "severity": "low",
      "userImpact": "Users can now enable workflow span creation from schedules or CLI contexts where client spans may not be available.",
      "rootCause": null,
      "proposedFix": "Add an opt-in flag to the tracing interceptor to allow workflow span creation even without client-created spans in headers.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented as opt-in flag on tracing interceptor configuration.",
      "related": [],
      "keyQuote": "some users starting from schedules or CLI want the workflow spans even if they may be orphaned, so we should allow this opt-in on the tracing interceptor",
      "number": 794,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:00.197Z"
    },
    {
      "summary": "Add support for new Worker Versioning API through annotations on workflows. This feature aligns Python SDK with versioning capabilities available in other SDKs.",
      "category": "feature",
      "subcategory": "worker-versioning",
      "apis": [],
      "components": [
        "worker",
        "workflow-annotations",
        "versioning-api"
      ],
      "concepts": [
        "versioning",
        "worker-compatibility",
        "api-design",
        "sdk-alignment",
        "workflow-metadata"
      ],
      "severity": "medium",
      "userImpact": "Enables Python SDK users to leverage new worker versioning APIs for managing workflow code evolution and compatibility.",
      "rootCause": null,
      "proposedFix": "Add annotations to workflows to support the new versioning APIs, following the design documented in the shared Google Doc and implemented in the Go SDK.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented by adding worker versioning annotation support to Python SDK workflows.",
      "related": [
        548
      ],
      "keyQuote": "We need to add annotations to workflows to support the new versioning APIs. Refer to temporalio/features#548 for the Go equivalents.",
      "number": 793,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:02.446Z"
    },
    {
      "summary": "wait_condition with timeout causes test to hang indefinitely instead of timing out, preventing workflows with signal timeouts from being properly tested. The issue was traced to a missing asyncio import and unregistered activities in the test, but highlights a broader problem with timeout exception handling in workflow testing.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [
        "wait_condition"
      ],
      "components": [
        "workflow-testing",
        "wait-condition",
        "timeout-handling",
        "signal-management"
      ],
      "concepts": [
        "timeout",
        "signal",
        "test-framework",
        "asyncio",
        "workflow-execution",
        "exception-handling"
      ],
      "severity": "high",
      "userImpact": "Developers cannot properly test workflows that use signal timeouts, making it impossible to verify timeout behavior in test suites.",
      "rootCause": "Test failures in workflow code (unregistered activities, missing imports) do not cause the workflow to fail by default, leaving wait_condition indefinitely suspended.",
      "proposedFix": "Set `workflow_failure_exception_types=[Exception]` worker option to make workflow failures from exceptions visible. Related issue #798 opened to allow timeouts to properly cause workflow failures.",
      "workaround": "Wrap wait_condition in try-except to catch TimeoutError and handle it explicitly; ensure all activities are registered and imports are correct in workflow code.",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved through debugging guidance; root cause identified as test setup issues. Related issue #798 tracks proper timeout failure handling.",
      "related": [
        798
      ],
      "keyQuote": "The error you are getting now is... NameError: name 'asyncio' is not defined. This is the case where it is a legitimate task failure suspended waiting for a code fix",
      "number": 792,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:23:00.817Z"
    },
    {
      "summary": "SDK assumes an error details list is populated when it may be empty, causing an IndexError instead of a descriptive error message. This occurs when the server throws RPCErrors like 'something went wrong, please retry (XXXXXXXX)' with empty details.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "client",
        "error-handling"
      ],
      "concepts": [
        "error-details",
        "index-error",
        "error-message",
        "server-response",
        "rpc-error",
        "error-recovery"
      ],
      "severity": "medium",
      "userImpact": "Users receive unhelpful IndexError exceptions instead of meaningful error messages when encountering certain server-side RPC errors.",
      "rootCause": "The SDK unconditionally accesses error details list at a specific index without checking if the list is populated, particularly when server returns RPCError with empty details.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue has been fixed in the SDK. The maintainer confirmed the fix is available for customers.",
      "related": [],
      "keyQuote": "The SDK assumes this list is populated, but it is empty when getting an error of the form `RPCError('something went wrong, please retry (XXXXXXXX)')`",
      "number": 791,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:22:46.093Z"
    },
    {
      "summary": "Add warnings to help developers detect lazy imports from third-party libraries that execute in the workflow sandbox. Support both an opt-out warning for all imports and an opt-in strict mode that warns only when non-passed-through imports occur.",
      "category": "feature",
      "subcategory": "import-detection",
      "apis": [],
      "components": [
        "sandbox",
        "import-system",
        "workflow-loader"
      ],
      "concepts": [
        "lazy-loading",
        "import-detection",
        "sandbox-isolation",
        "third-party-libraries",
        "memory-overhead",
        "developer-experience"
      ],
      "severity": "low",
      "userImpact": "Developers will receive warnings about unexpected runtime imports that consume memory and may indicate compatibility issues with third-party libraries.",
      "rootCause": "Third-party libraries sometimes perform lazy imports at runtime, which execute in the workflow sandbox where users cannot control them, causing unexpected memory overhead.",
      "proposedFix": "Implement opt-out warning for any imports after workflow file load, and opt-in strict mode with context manager `workflow.unsafe.imports_intentionally_reloaded()` to mark safe imports.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Sometimes third party libraries, even passed through ones, import things lazily at runtime which causes them to be imported in the sandbox which users can't see",
      "number": 790,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:22:44.688Z"
    },
    {
      "summary": "Feature request to add a summary field to Nexus operations in the Python SDK, similar to existing Temporal operation summaries. This follows up on a cross-SDK feature discussion.",
      "category": "feature",
      "subcategory": "nexus-operations",
      "apis": [
        "Nexus"
      ],
      "components": [
        "nexus",
        "operations",
        "python-sdk"
      ],
      "concepts": [
        "nexus-operations",
        "operation-summary",
        "metadata",
        "cross-sdk-feature"
      ],
      "severity": "low",
      "userImpact": "Users would be able to add descriptive summaries to Nexus operations for better tracking and observability.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented to add summary support to Nexus operations in Python SDK",
      "related": [
        610
      ],
      "keyQuote": "Add summary to Nexus operation",
      "number": 787,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:22:46.791Z"
    },
    {
      "summary": "Implement Temporal-specific Nexus support in the Python SDK to complement the Nexus Python SDK library. This feature request tracks the need for Temporal-SDK-level Nexus integration.",
      "category": "feature",
      "subcategory": "nexus",
      "apis": [],
      "components": [
        "nexus",
        "sdk-integration"
      ],
      "concepts": [
        "service-mesh",
        "cross-workflow-communication",
        "integration",
        "rpc",
        "temporal-specific"
      ],
      "severity": "medium",
      "userImpact": "Enables Python SDK users to implement Nexus-based service interactions and cross-workflow communication patterns.",
      "rootCause": null,
      "proposedFix": "Implement Temporal-specific Nexus support in the Python SDK alongside the Nexus Python SDK library, as detailed in the features proposal.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Closed by issue #813, indicating the feature implementation was completed.",
      "related": [
        813,
        609
      ],
      "keyQuote": "Alongside Nexus Python SDK, need Temporal-specific implementation in this SDK.",
      "number": 786,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:22:29.012Z"
    },
    {
      "summary": "The worker can lose a processing slot when an error occurs during cache eviction in the workflow manager. This needs to be improved to prevent slot loss and maintain worker efficiency.",
      "category": "bug",
      "subcategory": "worker-cache",
      "apis": [],
      "components": [
        "worker",
        "workflow-manager",
        "cache-eviction"
      ],
      "concepts": [
        "slot-management",
        "error-handling",
        "resource-cleanup",
        "worker-efficiency",
        "cache-management"
      ],
      "severity": "medium",
      "userImpact": "Workers may lose processing slots when cache eviction errors occur, reducing workflow processing capacity.",
      "rootCause": "Error handling during cache eviction does not properly return the worker slot, causing it to be lost.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed by improving error handling in the cache eviction process to preserve worker slots.",
      "related": [],
      "keyQuote": "Avoid losing worker slot on error while processing cache eviction",
      "number": 784,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:22:27.898Z"
    },
    {
      "summary": "Worker shutdown with sticky execution causes workflow tasks to be scheduled to the now-unavailable sticky queue, resulting in timeout errors. Investigation revealed this is expected behavior where tasks are scheduled on both sticky and normal queues, with the timeout occurring only when no worker is available to pick up the task.",
      "category": "bug",
      "subcategory": "worker-lifecycle",
      "apis": [
        "Worker",
        "Client"
      ],
      "components": [
        "worker",
        "task-scheduling",
        "sticky-execution"
      ],
      "concepts": [
        "graceful-shutdown",
        "sticky-execution",
        "task-timeout",
        "worker-availability",
        "task-queue",
        "scheduling"
      ],
      "severity": "low",
      "userImpact": "Users see timeout errors in workflow history during worker restarts, though runtime behavior is not affected; cosmetic issue that may cause confusion.",
      "rootCause": "When a worker shuts down, pending workflow tasks scheduled to the sticky queue cannot be picked up. The task timeout occurs because there is no worker available to service the sticky queue, forcing rescheduling on the normal queue.",
      "proposedFix": null,
      "workaround": "Keep at least one worker running to pick up tasks from the sticky queue during other workers' shutdown, or disable sticky execution with max_cached_workflows=0.",
      "resolution": "wontfix",
      "resolutionDetails": "Determined to be expected behavior. A separate server-side issue was opened to improve the timeout presentation cosmetics.",
      "related": [
        7566
      ],
      "keyQuote": "The task timeout is only to re-schedule the task back on the normal queue explicitly instead of implicitly, and that only happens because there is no worker around.",
      "number": 783,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:22:32.445Z"
    },
    {
      "summary": "When a workflow task that is blocking on a timer gets cancelled, the SDK's internal callback for resolving the timer future fires after cancellation, causing asyncio.exceptions.InvalidStateError because the future is already done. The workflow succeeds despite the logged exception.",
      "category": "bug",
      "subcategory": "timer-callback",
      "apis": [
        "workflow.sleep"
      ],
      "components": [
        "workflow-instance",
        "timer-handling",
        "asyncio-integration"
      ],
      "concepts": [
        "cancellation",
        "future-state",
        "callback-timing",
        "asyncio-task",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users see logged exceptions when cancelling tasks that are sleeping on timers, even though the workflow succeeds, causing concern and log noise.",
      "rootCause": "The SDK sets a callback to resolve the sleep future at timer expiration time. If the task is cancelled before the timer fires, the future is already done when the callback executes, causing InvalidStateError because asyncio.Future.set_result() fails on already-done futures.",
      "proposedFix": "Check if the future is already done before calling set_result() on it, or find other locations where this pattern occurs and apply the same fix.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Should just not call `set_result` on an already done future (and find where else we might be doing that).",
      "number": 782,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:22:15.109Z"
    },
    {
      "summary": "Debugger attachment causes TypeError in asyncio event loop callbacks when running Temporal workflow with PyCharm debugger. The issue occurs when attempting to debug workflow code that uses UnsandboxedWorkflowRunner.",
      "category": "bug",
      "subcategory": "debugging-dev-experience",
      "apis": [
        "Client.connect",
        "Worker",
        "workflow.execute_activity",
        "UnsandboxedWorkflowRunner"
      ],
      "components": [
        "worker",
        "workflow-runner",
        "event-loop",
        "debugging"
      ],
      "concepts": [
        "debugging",
        "asyncio",
        "callback",
        "development-experience",
        "workflow-execution",
        "ide-integration"
      ],
      "severity": "high",
      "userImpact": "Developers cannot debug workflow code using standard Python debuggers like PyCharm, severely limiting development productivity.",
      "rootCause": "PyCharm debugger's callback handling interferes with asyncio event loop in the Temporal worker, causing 'Task' object is not callable errors in the event loop callback mechanism.",
      "proposedFix": null,
      "workaround": "Refer to tips in issue #603 for workaround strategies.",
      "resolution": "duplicate",
      "resolutionDetails": "Issue was marked as duplicate and discussion was redirected to #603 which contains debugging tips.",
      "related": [
        603
      ],
      "keyQuote": "The moment I run in debug mode I start getting this exception: TypeError: 'Task' object is not callable",
      "number": 780,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:22:15.698Z"
    },
    {
      "summary": "Feature request to add type checking for workflow_execute activity when activities accept variable positional arguments using TypeVarTuple, improving type safety for activity calls with multiple args.",
      "category": "feature",
      "subcategory": "type-checking",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "workflow-execution",
        "activity-executor",
        "type-system"
      ],
      "concepts": [
        "type-checking",
        "type-safety",
        "positional-arguments",
        "TypeVarTuple",
        "type-hints",
        "backward-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently enforce type checking on activities with multiple positional arguments, limiting type safety in workflow definitions.",
      "rootCause": "Current type hints use Sequence[Any] for args parameter, preventing TypeVarTuple-based type checking which is only available in Python 3.11+.",
      "proposedFix": "Use TypeVarTuple (*Ts) to define *args in execute_activity signature, removing the args input parameter for type-safe overloads.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Rejected due to Python 3.11+ requirement and breaking change needed (removal of args parameter). Maintainers recommend using single-argument items as dataclasses instead.",
      "related": [],
      "keyQuote": "These limitations unfortunately mean we cannot do them. But we suggest only using single-argument items anyways as dataclasses.",
      "number": 779,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:22:12.317Z"
    },
    {
      "summary": "Worker is experiencing workflow task timeouts, with some activities not being polled and executed until several hours after being scheduled. The issue manifests as workflow tasks timing out despite a 3600 second activity timeout setting.",
      "category": "bug",
      "subcategory": "workflow-task-timeout",
      "apis": [],
      "components": [
        "worker",
        "task-dispatcher",
        "activity-poller"
      ],
      "concepts": [
        "timeout",
        "task-execution",
        "polling",
        "scheduling",
        "worker-responsiveness"
      ],
      "severity": "high",
      "userImpact": "Activities are delayed by hours and workflow tasks fail with timeouts, preventing timely workflow execution and task completion.",
      "rootCause": "Workflow task timeout indicates worker-side blocking preventing workflow task response within 10 seconds; likely caused by blocking operations in async functions or activities.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Workflow task timeout means something worker side is preventing that workflow task from responding within 10 seconds.",
      "number": 778,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:59.217Z"
    },
    {
      "summary": "User requests ability to customize histogram bucket configuration for activity metrics, which currently max out at 60 seconds, limiting observability for long-running activities. Feature was implemented in sdk-core and successfully added to Python SDK.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "runtime",
        "prometheus-config",
        "metrics-bridge"
      ],
      "concepts": [
        "observability",
        "metrics",
        "histogram",
        "activity-duration",
        "prometheus"
      ],
      "severity": "low",
      "userImpact": "Users can now configure custom histogram buckets for activity metrics to improve observability on long-running operations.",
      "rootCause": null,
      "proposedFix": "Add histogram_bucket_overrides parameter to PrometheusConfig dataclass and pass it through to the bridge layer, similar to the sdk-core implementation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented via PR #781, which added histogram_bucket_overrides support to PrometheusConfig for custom metric bucket configuration.",
      "related": [
        781
      ],
      "keyQuote": "I would like to have a method of specifying a desired binning strategy on activity metrics.",
      "number": 777,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:58.311Z"
    },
    {
      "summary": "User requests implementation of `run_in_executor` on the workflow event loop to support async logging libraries like structlog that use thread executors. The maintainer explains that the deterministic workflow loop intentionally lacks thread capabilities and suggests alternatives.",
      "category": "feature",
      "subcategory": "workflow-logging",
      "apis": [],
      "components": [
        "event-loop",
        "workflow-instance",
        "deterministic-loop"
      ],
      "concepts": [
        "async-logging",
        "thread-executor",
        "determinism",
        "event-loop",
        "structlog",
        "non-blocking-io"
      ],
      "severity": "low",
      "userImpact": "Users cannot use async logging libraries with thread executors inside workflows due to NotImplementedError.",
      "rootCause": "The workflow event loop is a custom deterministic loop with no thread capabilities by design to maintain determinism guarantees.",
      "proposedFix": "Use `asyncio.run_coroutine_threadsafe(my_async_call(), my_primary_loop).result()` or replace the base logger of `workflow.logger` with async-compatible equivalent.",
      "workaround": "Call asyncio operations from outside the deterministic loop; consider wrapping with `asyncio.run_coroutine_threadsafe()` or use `workflow.logger` with modified base logger.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer clarified that implementing run_in_executor is not possible because the deterministic workflow loop intentionally excludes thread capabilities to preserve determinism. Recommended alternative approaches instead.",
      "related": [],
      "keyQuote": "The async loop in workflows is a custom deterministic loop with no thread capabilities by intention, because threads are non-deterministic.",
      "number": 776,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:57.164Z"
    },
    {
      "summary": "SDK metrics don't include tags added with telemetry.global_tags in version 1.4+, a regression from 1.3. The global_tags setting was supposed to apply custom tags to all emitted metrics but now only affects OTEL resource attributes.",
      "category": "bug",
      "subcategory": "metrics-telemetry",
      "apis": [
        "TelemetryConfig"
      ],
      "components": [
        "telemetry",
        "metrics-exporter",
        "otel-integration"
      ],
      "concepts": [
        "metrics",
        "tags",
        "telemetry",
        "global-configuration",
        "otel-resource",
        "regression"
      ],
      "severity": "high",
      "userImpact": "Users cannot add custom tags to SDK metrics for observability purposes, breaking existing monitoring configurations that relied on this functionality.",
      "rootCause": "global_tags changed from applying to individual metrics to only updating OTEL resource/Prometheus target_info, causing regression from version 1.3 behavior.",
      "proposedFix": "Restore global_tags to apply tags to all emitted metrics as originally intended. Consider adding separate configuration options for OTEL resource attributes and metric-level labels to accommodate both use cases.",
      "workaround": "Use downstream Alloy/Prometheus collectors to add labels from OTEL resource attributes to metrics after emission, though this is not ideal.",
      "resolution": "fixed",
      "resolutionDetails": "The regression was fixed to restore global_tags applying to all metrics as originally intended.",
      "related": [
        882
      ],
      "keyQuote": "The original intended behavior is that the tags would apply to all emitted metrics.",
      "number": 775,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:43.805Z"
    },
    {
      "summary": "Feature request to expose root workflow execution in the Python SDK. This depends on completing the core SDK implementation tracked in sdk-core#881 and the features specification in features#605.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "workflow-execution",
        "python-sdk"
      ],
      "concepts": [
        "root-workflow",
        "execution-context",
        "workflow-hierarchy"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to access root workflow execution details, enabling better context management and parent-child workflow relationship tracking.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue closed after dependent work was completed in sdk-core and features specification",
      "related": [
        605,
        881
      ],
      "keyQuote": "See https://github.com/temporalio/features/issues/605, but needs to wait on https://github.com/temporalio/sdk-core/issues/881",
      "number": 774,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:42.689Z"
    },
    {
      "summary": "Feature request to ensure built-in query responses use \"RawValue\" format, as specified in the features repository requirements.",
      "category": "feature",
      "subcategory": "queries",
      "apis": [],
      "components": [
        "query-handler",
        "serialization"
      ],
      "concepts": [
        "raw-value",
        "query-response",
        "serialization-format",
        "built-in-queries"
      ],
      "severity": "medium",
      "userImpact": "Users need built-in query responses to consistently use RawValue format for proper interoperability across SDKs.",
      "rootCause": null,
      "proposedFix": "Implement RawValue format for all built-in query response types",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        604
      ],
      "keyQuote": "Make sure built-in queries return a raw value",
      "number": 773,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:43.739Z"
    },
    {
      "summary": "Investigation needed into the purpose and necessity of an unused `bind_f` function in the Python SDK. The function appears to be dead code that should either be removed or its purpose clarified.",
      "category": "bug",
      "subcategory": "code-cleanup",
      "apis": [],
      "components": [
        "bind_f",
        "function-binding"
      ],
      "concepts": [
        "dead-code",
        "code-cleanup",
        "unused-functions",
        "refactoring"
      ],
      "severity": "low",
      "userImpact": "Unused code can increase maintenance burden and create confusion about the intended API surface.",
      "rootCause": "The `bind_f` function appears to be unreferenced code that may have been superseded by other functionality.",
      "proposedFix": "Either remove the unused `bind_f` function or document its intended purpose if it serves a future use case.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        771
      ],
      "keyQuote": "Investigate purpose of unused `bind_f` function.",
      "number": 772,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:24.655Z"
    },
    {
      "summary": "Python 3.11's enhanced traceback information (code lines and position markers) significantly increases failure payload sizes, potentially exceeding server limits. The feature request asks for a way to disable this extra traceback data, though research determined the setting is global and users can use environment variables to disable it.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "failure-handling",
        "traceback-formatting",
        "error-serialization"
      ],
      "concepts": [
        "traceback-size",
        "payload-limits",
        "Python-3.11",
        "error-reporting",
        "debugging-information",
        "serialization"
      ],
      "severity": "low",
      "userImpact": "Users running Python 3.11+ may experience failures that exceed server-side payload size limits due to enhanced traceback information.",
      "rootCause": "Python 3.11's PEP 657 adds code context and position markers (`^` and `~`) to tracebacks, increasing payload size.",
      "proposedFix": "Either provide opt-in/opt-out behavior to strip enhanced traceback data, or manually walk frames to build custom stack traces.",
      "workaround": "Set PYTHONNODEBUGRANGES environment variable or use -X no_debug_ranges command-line argument to disable debug ranges.",
      "resolution": "wontfix",
      "resolutionDetails": "After investigation, the enhanced traceback markers don't contribute significantly to size. Closing as won't do, recommending users use the environment variable workaround.",
      "related": [
        597
      ],
      "keyQuote": "If this is a concern for users, they can set PYTHONNODEBUGRANGES env var or -X no_debug_ranges cmd arg.",
      "number": 765,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:29.129Z"
    },
    {
      "summary": "Expose the cancel cause/reason from WorkflowExecutionCancelRequestedEventAttributes when workflows receive cancellations, and allow users to provide this information when issuing cancels from clients or commands.",
      "category": "feature",
      "subcategory": "workflow-cancellation",
      "apis": [
        "WorkflowExecutionCancelRequestedEventAttributes"
      ],
      "components": [
        "workflow-execution",
        "cancellation-handling",
        "client",
        "event-attributes"
      ],
      "concepts": [
        "cancellation",
        "workflow-lifecycle",
        "event-attributes",
        "cause-reason",
        "user-input"
      ],
      "severity": "medium",
      "userImpact": "Users cannot access or provide the reason why a workflow was cancelled, limiting their ability to handle cancellations appropriately and understand cancellation context.",
      "rootCause": null,
      "proposedFix": "Expose the cause field from WorkflowExecutionCancelRequestedEventAttributes and add it as an input parameter when users issue cancels from clients or commands.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "WorkflowExecutionCancelRequestedEventAttributes has a cause field that should be exposed when workflows receive cancels",
      "number": 764,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:25.717Z"
    },
    {
      "summary": "Remove experimental deprecation warnings from several mature APIs in the Python SDK that are no longer experimental, including local activities, eager workflow/activity execution, runtime module, logging configuration, and workflow update decorator.",
      "category": "feature",
      "subcategory": "api-stability",
      "apis": [
        "local_activities",
        "eager_start",
        "eager_activity_execution",
        "workflow_update"
      ],
      "components": [
        "runtime",
        "logger",
        "decorator",
        "api"
      ],
      "concepts": [
        "deprecation",
        "api-maturity",
        "experimental-flag",
        "api-stability",
        "backwards-compatibility"
      ],
      "severity": "low",
      "userImpact": "Users see unnecessary experimental warnings for stable APIs, creating confusion about API maturity status.",
      "rootCause": null,
      "proposedFix": "Remove experimental warnings from: local activities, eager workflow start, eager activity execution, runtime module, runtime logging/log-forwarding config, and workflow update decorator",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The following items should no longer be considered experimental: Local activities, Eager workflow start, Eager activity execution, Runtime module, Runtime logging and log-forwarding config, Workflow update decorator",
      "number": 763,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:10.062Z"
    },
    {
      "summary": "Feature request to enforce the no-mutable-defaults lint rule in the Python SDK to prevent common Python bugs where mutable default arguments are shared across function calls.",
      "category": "feature",
      "subcategory": "code-quality",
      "apis": [],
      "components": [
        "linter",
        "code-analysis",
        "development-tooling"
      ],
      "concepts": [
        "mutable-defaults",
        "code-quality",
        "lint-rules",
        "python-best-practices",
        "code-standards"
      ],
      "severity": "low",
      "userImpact": "Developers will catch mutable default argument bugs earlier through automated linting, preventing subtle bugs in their code.",
      "rootCause": null,
      "proposedFix": "Implement ruff's mutable-argument-default rule (RUF009) as part of the SDK's linting configuration, following Google's Python style guide recommendations.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": null,
      "number": 762,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:13.364Z"
    },
    {
      "summary": "Request to publish musl wheel for the Python SDK, enabling support for Alpine Linux and other musl-based distributions.",
      "category": "feature",
      "subcategory": "distribution-packaging",
      "apis": [],
      "components": [
        "build-system",
        "wheel-distribution",
        "packaging"
      ],
      "concepts": [
        "musl-libc",
        "alpine-linux",
        "platform-support",
        "wheel-publishing",
        "cross-platform",
        "distribution"
      ],
      "severity": "medium",
      "userImpact": "Users running on musl-based Linux distributions (like Alpine) currently cannot use pre-built wheels and must compile from source.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        594
      ],
      "keyQuote": "See https://github.com/temporalio/features/issues/594",
      "number": 761,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:21:11.234Z"
    },
    {
      "summary": "Feature request to add support for priority annotations on workflows and activities in the Python SDK. Implementation details are tracked in the cross-repository features issue.",
      "category": "feature",
      "subcategory": "priority-annotations",
      "apis": [],
      "components": [
        "workflow",
        "activity",
        "annotations"
      ],
      "concepts": [
        "priority",
        "workflow-execution",
        "activity-execution",
        "scheduling",
        "resource-allocation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently specify execution priority for workflows and activities, limiting control over task scheduling and resource allocation.",
      "rootCause": null,
      "proposedFix": "Implement priority annotation support as detailed in temporalio/features#593",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented and delivered in the Python SDK",
      "related": [
        593
      ],
      "keyQuote": "Add support for attaching priority to workflows/activities",
      "number": 760,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:55.465Z"
    },
    {
      "summary": "Request for utilities to automatically extract activities from classes, class instances, and modules, making it easier to register all activities with a worker without manually listing each one.",
      "category": "feature",
      "subcategory": "activity-collection",
      "apis": [
        "activity.defn",
        "Worker"
      ],
      "components": [
        "worker",
        "activity-registry",
        "activity-decorator"
      ],
      "concepts": [
        "activity-discovery",
        "activity-registration",
        "developer-experience",
        "code-organization",
        "module-introspection",
        "decorator-detection"
      ],
      "severity": "low",
      "userImpact": "Users must manually maintain an explicit list of all activity-decorated functions when registering them with workers, which is error-prone and violates separation of concerns.",
      "rootCause": "No public API or utility exists to programmatically discover activities by introspecting classes or modules.",
      "proposedFix": "Provide utility functions like get_activities_from_class(), get_activities_from_module(), and enhance Worker constructor to accept classes/modules directly for automatic activity extraction.",
      "workaround": "Users can manually wrap decorators or use AST parsing to discover activities, as shown in the issue and confirmed by multiple commenters who implemented similar solutions.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I would like: worker = Worker(activities=[*ActivitiesClass.get_activities(), *get_activities(activities_module)])",
      "number": 758,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:58.671Z"
    },
    {
      "summary": "Feature request to add explicit memoization support to the `patched()` API in the Python SDK. This involves adding a `memoized` argument while maintaining backward compatibility and delegating the patched logic to Core.",
      "category": "feature",
      "subcategory": "memoization",
      "apis": [
        "patched",
        "deprecate_patch"
      ],
      "components": [
        "patched-api",
        "core-delegation",
        "backward-compatibility"
      ],
      "concepts": [
        "memoization",
        "workflow-patching",
        "deprecation",
        "backward-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to explicitly control memoization behavior in patched workflows, improving API clarity and consistency with Core.",
      "rootCause": null,
      "proposedFix": "Add `memoized` argument to `patched` and `deprecate_patch` APIs, deprecate non-specifying signatures, and delegate logic to Core.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        591
      ],
      "keyQuote": "Add a `memoized` argument to the `patched` and `deprecate_patch` APIs.",
      "number": 754,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:56.086Z"
    },
    {
      "summary": "Feature request to allow customization of the development server UI port in the Python SDK. References a related features issue for implementation details.",
      "category": "feature",
      "subcategory": "dev-server",
      "apis": [],
      "components": [
        "dev-server",
        "ui",
        "cli"
      ],
      "concepts": [
        "customization",
        "port-configuration",
        "development-tools",
        "server-setup"
      ],
      "severity": "low",
      "userImpact": "Developers using the Temporal Python SDK dev server would have more flexibility in configuring the UI port for their local development environments.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        588
      ],
      "keyQuote": "Allow customization of dev server UI port",
      "number": 748,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:41.535Z"
    },
    {
      "summary": "User seeks a way to retrieve the worker ID that is executing the current activity, particularly within activity interceptors. After discussion, the solution involves passing worker state to interceptors at instantiation time rather than querying it at runtime.",
      "category": "question",
      "subcategory": "worker-identity",
      "apis": [
        "Worker",
        "activity.info"
      ],
      "components": [
        "worker",
        "activity-interceptor",
        "client"
      ],
      "concepts": [
        "worker-identity",
        "distributed-execution",
        "activity-context",
        "interceptor-state",
        "runtime-introspection",
        "worker-tracking"
      ],
      "severity": "low",
      "userImpact": "Users cannot directly query the worker ID executing an activity at runtime, requiring alternative patterns like passing state to interceptors.",
      "rootCause": "The activity context does not expose the executing worker's identity; worker information is available at Worker instantiation but not in the activity execution context.",
      "proposedFix": "Pass worker identity to the interceptor at instantiation time: ActivityStatusInterceptor(worker_name), since each worker has its own interceptor instance.",
      "workaround": "Instantiate activity interceptors with worker-specific state or use different activity instances per worker with different state.",
      "resolution": "fixed",
      "resolutionDetails": "User accepted the suggested pattern of passing worker identity to interceptor during instantiation, leveraging the fact that each worker has its own interceptor instance.",
      "related": [],
      "keyQuote": "You know that each worker has its own instance of the interceptor so you know if the interceptor for an activity is called, its state (e.g. worker identity) is only for the worker you're running on.",
      "number": 747,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:42.847Z"
    },
    {
      "summary": "ResourceBasedSlotOptions fields in the Python SDK documentation showed values between 0 and 1, but the underlying Rust implementation expected percentages between 0 and 100. This caused confusion and incorrect tuner behavior until the reporter realized the issue was insufficient container memory, not the value range itself.",
      "category": "docs",
      "subcategory": "worker-tuning",
      "apis": [
        "WorkerTuner.create_resource_based"
      ],
      "components": [
        "worker",
        "tuner",
        "resource-based-tuner"
      ],
      "concepts": [
        "resource-allocation",
        "memory-usage",
        "cpu-usage",
        "slot-allocation",
        "documentation-accuracy"
      ],
      "severity": "low",
      "userImpact": "Users following the documentation with normalized values (0-1) experienced incorrect slot allocation behavior, requiring them to debug and discover the correct percentage format (0-100).",
      "rootCause": "Documentation inconsistency between Python SDK documentation showing 0-1 range and underlying Rust implementation expecting 0-100 range. The actual issue was insufficient container memory, misdiagnosed by the reporter due to the documentation mismatch.",
      "proposedFix": null,
      "workaround": "Use percentage values (0-100) instead of normalized values (0-1) for target_memory_usage and target_cpu_usage.",
      "resolution": "self_resolved",
      "resolutionDetails": "Reporter realized the documentation was misleading but the actual root cause was insufficient container memory. The documentation should be clarified to explicitly state that values should be percentages (0-100).",
      "related": [],
      "keyQuote": "When setting values as percentages, i.e. 80.0 rather than 0.8, the tuner behaves as expected.",
      "number": 743,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:41.268Z"
    },
    {
      "summary": "OpenTelemetry SDK 1.29+ calls os.environ.get in workflows, which is blocked by Temporal's sandbox. Users need to either downgrade OTel or manually pass through the opentelemetry and ddtrace modules in SandboxRestrictions.",
      "category": "bug",
      "subcategory": "sandbox-restrictions",
      "apis": [
        "Worker",
        "SandboxedWorkflowRunner",
        "SandboxRestrictions"
      ],
      "components": [
        "workflow-sandbox",
        "interceptor-framework",
        "restrictions-engine"
      ],
      "concepts": [
        "opentelemetry",
        "ddtrace",
        "tracing",
        "sandbox-environment",
        "import-restrictions",
        "workflow-execution"
      ],
      "severity": "high",
      "userImpact": "Users enabling distributed tracing with OpenTelemetry/ddtrace cannot run workflows with SDK 1.9.0 when using OTel 1.29+.",
      "rootCause": "OpenTelemetry SDK 1.29 started calling os.environ.get at import time, which violates Temporal's workflow sandbox restrictions that prevent environment access.",
      "proposedFix": "Pass opentelemetry and ddtrace modules through SandboxRestrictions via with_passthrough_modules() when creating the Worker.",
      "workaround": "Either downgrade OpenTelemetry SDK to pre-1.29 version or use SandboxRestrictions.default.with_passthrough_modules('opentelemetry', 'ddtrace')",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "we need to get those imports preloaded and passed through. You may be able to do something like... with_passthrough_modules(\"opentelemetry\", \"ddtrace\")",
      "number": 733,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:26.318Z"
    },
    {
      "summary": "User requested typed return types for workflow.execute_activity() using Python generics instead of returning Any. Discussion revealed the SDK already supports type-safe overloads that extract return types from activity definitions, suggesting the issue may be an IDE limitation rather than an SDK bug.",
      "category": "question",
      "subcategory": "type-annotations",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "workflow",
        "activity",
        "type-hints"
      ],
      "concepts": [
        "type-safety",
        "generics",
        "ide-support",
        "type-inference",
        "overloads"
      ],
      "severity": "low",
      "userImpact": "Users may experience inadequate IDE type hints when calling execute_activity, making it harder to discover activity return types during development.",
      "rootCause": "Potential PyCharm IDE limitation with recognizing type-safe overloads rather than an SDK implementation issue.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "The requested feature already exists - the SDK provides type-safe overloads that extract return types from activity definitions. The issue was resolved when the author discovered PyCharm may have a limitation with recognizing these overloads.",
      "related": [],
      "keyQuote": "The typesafe overloads for this extract the return type from the activity passed in.",
      "number": 732,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:28.113Z"
    },
    {
      "summary": "Feature request to reserve Temporal-specific built-in prefixes in the Python SDK, following the approach defined in the cross-SDK features specification. Discussion addresses Python name mangling concerns with double-underscore prefixes.",
      "category": "feature",
      "subcategory": "naming-conventions",
      "apis": [],
      "components": [
        "sdk-core",
        "naming-system",
        "api-compatibility"
      ],
      "concepts": [
        "namespace-reservation",
        "naming-conventions",
        "python-compatibility",
        "built-in-prefixes",
        "api-design"
      ],
      "severity": "low",
      "userImpact": "Establishes reserved naming conventions to prevent conflicts between user code and Temporal SDK internal names.",
      "rootCause": null,
      "proposedFix": "Reserve `__temporal_` prefix for built-in Temporal names, which is specific enough to avoid collisions with Python's name mangling behavior.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        576
      ],
      "keyQuote": "We're only reserving well-known/already-established names, and we think `__temporal_` is specific enough that it's unlikely to collide with any class name.",
      "number": 731,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:26.675Z"
    },
    {
      "summary": "Update Ruff linter configuration to target Python 3.9 instead of 3.8, completing a previous upgrade that forgot to update this setting.",
      "category": "other",
      "subcategory": "build-configuration",
      "apis": [],
      "components": [
        "ruff",
        "linter-config",
        "build-system"
      ],
      "concepts": [
        "python-version",
        "linting",
        "configuration",
        "version-targeting",
        "tooling"
      ],
      "severity": "low",
      "userImpact": "Ensures the linter targets the correct Python version, preventing false positives or misaligned linting rules.",
      "rootCause": null,
      "proposedFix": "Update Ruff target version configuration from Python 3.8 to 3.9 in the project settings.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Ruff target version was updated to Python 3.9 as part of the version upgrade maintenance.",
      "related": [
        694
      ],
      "keyQuote": "we forgot to do this on 3.8 upgrade",
      "number": 728,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:09.288Z"
    },
    {
      "summary": "Feature request to add Pydantic V2 support in the data converter by preferring the `model_validate` method over the deprecated `parse_obj` method. The issue was marked as duplicate of #621 and subsequently resolved with official Pydantic v2 support in SDK version 1.10.0.",
      "category": "feature",
      "subcategory": "pydantic-converter",
      "apis": [],
      "components": [
        "converter",
        "data-conversion"
      ],
      "concepts": [
        "pydantic",
        "backward-compatibility",
        "deprecation",
        "serialization",
        "model-validation"
      ],
      "severity": "medium",
      "userImpact": "Users using Pydantic V2 can now use the converter without deprecation warnings and with full V2 support.",
      "rootCause": "Pydantic V2 deprecated `parse_obj` in favor of `model_validate`, but the SDK converter only recognized the old V1 method.",
      "proposedFix": "Modify converter.py lines 1526-1536 to check for `model_validate` first, then fall back to `parse_obj` for V1 compatibility.",
      "workaround": "Use the custom converter approach as documented in the samples.",
      "resolution": "fixed",
      "resolutionDetails": "Official Pydantic v2 support was implemented and released in SDK version 1.10.0",
      "related": [
        621
      ],
      "keyQuote": "The latest version of the SDK now has official Pydantic (v2) support",
      "number": 726,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:12.648Z"
    },
    {
      "summary": "Stack trace reporting for deadlock exceptions needs improvements: ensure it works for non-sandboxed environments, add tests to verify the feature, and improve error messaging for edge cases where thread IDs aren't available.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "deadlock-detection",
        "stack-trace-capture",
        "sandboxing",
        "threading"
      ],
      "concepts": [
        "stack-trace",
        "deadlock",
        "error-reporting",
        "debugging",
        "thread-safety",
        "exception-handling",
        "sandboxing"
      ],
      "severity": "medium",
      "userImpact": "Users debugging deadlock issues may not get proper stack traces if they've disabled sandboxing, and error messages may be unclear when thread information is unavailable.",
      "rootCause": "PR #626 added stack trace reporting for deadlocking threads but may not work consistently across all configurations and lacks adequate test coverage.",
      "proposedFix": "1) Verify stack trace capture works for non-sandboxed environments 2) Add tests to confirm deadlock stack traces are properly captured 3) Add '(no thread ID available)' message for cases where thread ID is missing",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        626
      ],
      "keyQuote": "Add something like `(no thread ID available)` if the earlier condition occurs where for some reason we don't have a thread ID at all.",
      "number": 722,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:20:12.051Z"
    },
    {
      "summary": "Feature request to provide access to the Temporal client from within an activity context. Users need a `client()` function in the activity module to enable activities to communicate with Temporal during execution.",
      "category": "feature",
      "subcategory": "activity-context",
      "apis": [
        "Client"
      ],
      "components": [
        "activity",
        "context",
        "client"
      ],
      "concepts": [
        "activity-execution",
        "client-access",
        "context-management",
        "inter-activity-communication"
      ],
      "severity": "medium",
      "userImpact": "Activities currently cannot access the Temporal client, limiting their ability to perform operations like calling other activities or workflows from within activity code.",
      "rootCause": null,
      "proposedFix": "Implement a `client() -> Client` function in the activity module to expose client access within activity context.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Reopened and addressed via pending PR #740",
      "related": [
        740,
        203
      ],
      "keyQuote": "Allow users to access Temporal client from within an activity. This is likely a `client() -> Client` function in the `activity` module.",
      "number": 721,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:57.780Z"
    },
    {
      "summary": "Feature request to expose the workflow instance from the workflow context, allowing developers to access `workflow.instance()` to get the created instance of the workflow class.",
      "category": "feature",
      "subcategory": "workflow-context",
      "apis": [],
      "components": [
        "workflow-context",
        "workflow-instance"
      ],
      "concepts": [
        "workflow-instance",
        "context-access",
        "developer-experience",
        "instance-management",
        "api-design"
      ],
      "severity": "low",
      "userImpact": "Developers would gain access to the workflow instance from the workflow context, enabling more flexible workflow implementations.",
      "rootCause": null,
      "proposedFix": "Add workflow.instance() method to expose the created instance of the workflow class",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        572
      ],
      "keyQuote": "The created instance of the workflow class should be made available as `workflow.instance()`",
      "number": 720,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:58.205Z"
    },
    {
      "summary": "Add a workflow-local variable mechanism that is scoped only to the current workflow instance, with type safety and API similar to contextvars.ContextVar. Users currently work around this by using setattr/getattr on workflow.instance().",
      "category": "feature",
      "subcategory": "workflow-context",
      "apis": [],
      "components": [
        "workflow",
        "context-management"
      ],
      "concepts": [
        "workflow-local-storage",
        "context-variables",
        "scoping",
        "type-safety",
        "workflow-isolation"
      ],
      "severity": "medium",
      "userImpact": "Users gain a standardized, type-safe way to store and access workflow-scoped variables instead of relying on undocumented setattr/getattr workarounds.",
      "rootCause": null,
      "proposedFix": "Implement a workflow.LocalVar() mechanism similar to contextvars.ContextVar that provides workflow-scoped variable storage with proper type safety and method chaining.",
      "workaround": "Users can currently use setattr/getattr on workflow.instance() to store and retrieve workflow-local values.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        571
      ],
      "keyQuote": "a user can have `myValue = workflow.LocalVar(\"myValue\")`, and all methods and typesafety and such work like `contextvars.ContextVar`",
      "number": 719,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:56.724Z"
    },
    {
      "summary": "Test server for ARM64 architecture on Linux is unavailable (404 error), preventing unit tests from running. The time-skipping test server does not work on ARM64, and there is no proper alternative available for Linux ARM environments.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [
        "WorkflowEnvironment.start_time_skipping"
      ],
      "components": [
        "test-server",
        "testing",
        "bridge"
      ],
      "concepts": [
        "ARM64",
        "architecture",
        "test-server",
        "time-skipping",
        "platform-compatibility",
        "download"
      ],
      "severity": "high",
      "userImpact": "Users on ARM64 Linux systems cannot run time-skipping tests, blocking their ability to test workflows on this increasingly common architecture.",
      "rootCause": "The time-skipping test server binary is not built or distributed for ARM64 Linux architecture. The download URL returns 404 because the binary does not exist for this platform.",
      "proposedFix": null,
      "workaround": "Users on ARM64 Linux can use an x64 Temporal server instead, but this does not provide time-skipping functionality.",
      "resolution": "wontfix",
      "resolutionDetails": "Acknowledged as a known limitation documented in the SDK README. Time-skipping test server does not work on ARM64 for Linux or Windows; only macOS can use x64 binary with Intel emulator.",
      "related": [
        379
      ],
      "keyQuote": "The time-skipping test environment does not work on ARM. The SDK will try to download the x64 binary on macOS for use with the Intel emulator, but for Linux or Windows ARM there is no proper time-skipping test server at this time.",
      "number": 716,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:44.774Z"
    },
    {
      "summary": "Tests for update operations are being skipped under the Java test server. The issue requests that either these tests should pass and stop being skipped, or the test server needs to be fixed to support them.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-server",
        "update-tests",
        "java-test-server"
      ],
      "concepts": [
        "testing",
        "test-server",
        "skip-conditions",
        "test-validation"
      ],
      "severity": "medium",
      "userImpact": "Update operation tests are not being properly validated against the Java test server, reducing test coverage and potentially hiding bugs.",
      "rootCause": "Update tests are being conditionally skipped under the Java test server, but it's unclear if this is due to test server limitations or test issues.",
      "proposedFix": "Either fix the update tests to pass with the Java test server, or fix the Java test server implementation to support update operations.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Tests were either fixed to pass with the Java test server or the test server was updated to support update operations.",
      "related": [],
      "keyQuote": "Either these pass, in which case they should not be skipped, or they don't, in which case the test server must be fixed.",
      "number": 708,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:44.528Z"
    },
    {
      "summary": "The Python SDK does not set the run ID in the UpdateHandle when receiving an update response, unlike other SDKs. This causes PollWorkflowUpdate requests to not be constrained to the specific run, potentially allowing polls to target different updates with the same ID in post-CAN or post-reset runs.",
      "category": "bug",
      "subcategory": "update-handle",
      "apis": [
        "UpdateHandle",
        "PollWorkflowUpdate"
      ],
      "components": [
        "client",
        "update-handle",
        "workflow-client"
      ],
      "concepts": [
        "run-id",
        "update-response",
        "poll-requests",
        "consistency",
        "sdk-alignment"
      ],
      "severity": "high",
      "userImpact": "Users experience incorrect update polling behavior where requests may target wrong runs, breaking update consistency guarantees.",
      "rootCause": "Python SDK client.py#L5339 does not set run ID in UpdateHandle when receiving update response, unlike TypeScript and Go SDKs.",
      "proposedFix": "Set the run ID in UpdateHandle similar to TypeScript (workflow-client.ts#L852-L853) and Go (internal_workflow_client.go#L2356) implementations.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Run ID is now set in UpdateHandle when receiving update response, constraining PollWorkflowUpdate requests to the correct run.",
      "related": [],
      "keyQuote": "PollWorkflowUpdate requests issued by the Python update handle do not contain a run ID (and so will land on any run)",
      "number": 704,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:42.295Z"
    },
    {
      "summary": "Activity cancellation in Python SDK requires heartbeats to propagate, doesn't interrupt long-running coroutines, and lacks a clean way to wait for cancellation completion. Users must catch broad `ActivityError` exceptions and work around cancellation issues with sleep calls or periodic heartbeats.",
      "category": "bug",
      "subcategory": "activity-cancellation",
      "apis": [
        "execute_activity",
        "handle.cancel",
        "activity.heartbeat"
      ],
      "components": [
        "activity-executor",
        "cancellation-handler",
        "asyncio-integration"
      ],
      "concepts": [
        "cancellation",
        "heartbeat",
        "asyncio-task",
        "activity-lifecycle",
        "error-handling",
        "interruption"
      ],
      "severity": "high",
      "userImpact": "Users cannot cleanly cancel activities and wait for completion, leading to workflows that hang or activities that complete despite cancellation requests.",
      "rootCause": "Activity cancellation is implemented as a request sent to the server rather than immediate task cancellation. The Python SDK only calls `task.cancel()` after an `activity.heartbeat()` call, which doesn't interrupt activities waiting on long-running coroutines like `asyncio.sleep()`.",
      "proposedFix": "Consider calling `task.cancel()` on the activity task when a cancellation request is detected, independent of heartbeat timing, to properly interrupt blocked asyncio operations.",
      "workaround": "Add periodic `asyncio.sleep(0.1)` calls or use `periodic_heartbeater` decorator to ensure cancellation requests are checked. Catch `ActivityError` and check `retry_state == RetryState.CANCEL_REQUESTED` to detect explicit cancellation.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        810
      ],
      "keyQuote": "if you try to cancel an activity that's currently awaiting a long-running coro, the activity won't receive a CancelledError until it sends a heartbeat, which it can't do until the coro finishes",
      "number": 700,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:28.608Z"
    },
    {
      "summary": "workflow.wait() with asyncio.FIRST_COMPLETED incorrectly blocks until all local activities complete instead of returning when the first one finishes. Non-local activities work correctly.",
      "category": "bug",
      "subcategory": "local-activities",
      "apis": [
        "workflow.wait"
      ],
      "components": [
        "workflow-executor",
        "local-activity-executor",
        "asyncio-integration"
      ],
      "concepts": [
        "asyncio",
        "task-coordination",
        "cancellation",
        "local-activities",
        "race-conditions",
        "waiting-strategies"
      ],
      "severity": "high",
      "userImpact": "Users cannot efficiently wait for the first completion among multiple local activities, breaking common patterns like competitive activity execution with selective cancellation.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Use non-local activities instead of local activities when needing FIRST_COMPLETED semantics.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If I start N local activities, then use `workflow.wait(tasks, return_when=asyncio.FIRST_COMPLETED)`, the `workflow.wait` coro blocks until all of the activities actually finish, instead of returning when the first one finishes.",
      "number": 699,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:30.135Z"
    },
    {
      "summary": "Python SDK fails to serialize exceptions when `e.__cause__` references itself (recursive exception), causing silent failures and timeout retries. The DefaultFailureConverter needs to handle this edge case and properly report serialization failures.",
      "category": "bug",
      "subcategory": "failure-converter",
      "apis": [],
      "components": [
        "DefaultFailureConverter",
        "failure-serialization",
        "exception-handling"
      ],
      "concepts": [
        "recursive-exception",
        "exception-serialization",
        "error-reporting",
        "cause-chain",
        "stack-overflow",
        "edge-case"
      ],
      "severity": "high",
      "userImpact": "Users encounter silent failures and startToClose timeouts when accidentally creating recursive exceptions with 'raise e from e' pattern.",
      "rootCause": "DefaultFailureConverter does not detect or handle recursive exception cause chains, and failures during serialization are not properly reported.",
      "proposedFix": "Add cycle detection in DefaultFailureConverter to handle cases where e.__cause__ == e or generally truncate exception chains at a maximum depth (e.g., 10), similar to how deep stacks are truncated during serialization.",
      "workaround": "Avoid using 'raise e from e' pattern; reraise exceptions directly without specifying from clause, or manually reset e.__cause__ to None if it points to itself.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        696,
        685
      ],
      "keyQuote": "The re-raised exception will have `e.__cause__` referencing itself resulting in failure to serialize error and python sdk won't report anything",
      "number": 697,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:29.931Z"
    },
    {
      "summary": "Feature request to add a 'passthrough_all_modules' option to SandboxRestrictions that disables import isolation for the sandbox, allowing all modules to pass through without requiring users to explicitly allow-list individual modules.",
      "category": "feature",
      "subcategory": "sandbox-configuration",
      "apis": [],
      "components": [
        "sandbox-runner",
        "sandbox-restrictions",
        "import-isolation"
      ],
      "concepts": [
        "module-isolation",
        "determinism",
        "state-isolation",
        "sandbox-configuration",
        "allow-listing"
      ],
      "severity": "low",
      "userImpact": "Users can now disable import isolation for the sandbox without needing to manually allow-list each deterministic module.",
      "rootCause": null,
      "proposedFix": "Add a `passthrough_all_modules: bool` field to `SandboxRestrictions` configuration.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to allow disabling import isolation while maintaining state isolation per workflow.",
      "related": [],
      "keyQuote": "This is helpful because it still allows the sandbox to provide state isolation for each workflow, but without requiring users to worry about allow-listing individual modules",
      "number": 691,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:11.349Z"
    },
    {
      "summary": "Add runtime validation to ensure update validator methods have the same parameter signature as the update methods they validate. This prevents runtime errors from mismatched signatures.",
      "category": "feature",
      "subcategory": "update-validation",
      "apis": [],
      "components": [
        "update-validator",
        "signature-validation",
        "worker"
      ],
      "concepts": [
        "parameter-signature",
        "validation",
        "runtime-check",
        "method-signature",
        "type-safety"
      ],
      "severity": "medium",
      "userImpact": "Users will receive early runtime errors if their update validators don't match the update method signatures they validate.",
      "rootCause": null,
      "proposedFix": "Add a runtime check that compares the parameter portion of the validator signature against the update method signature.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Runtime signature validation check was implemented for update validators.",
      "related": [],
      "keyQuote": "We need to make sure at runtime the parameter portion of the signature of the validator matches the parameter portion of the signature of the update method it is validating.",
      "number": 689,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:13.544Z"
    },
    {
      "summary": "Protobuf messages fail to deserialize in Kubernetes environments due to workflow sandbox restrictions preventing access to the google.protobuf module during JSON deserialization, even though the same code works locally.",
      "category": "bug",
      "subcategory": "workflow-sandbox",
      "apis": [],
      "components": [
        "workflow-sandbox",
        "converter",
        "protobuf-deserializer"
      ],
      "concepts": [
        "sandbox-isolation",
        "module-import",
        "protobuf-serialization",
        "kubernetes-deployment",
        "deserialization",
        "environment-specific"
      ],
      "severity": "high",
      "userImpact": "Users cannot execute workflows with Protobuf message parameters in Kubernetes environments, causing runtime failures despite working correctly locally.",
      "rootCause": "The workflow sandbox's import restriction mechanism blocks access to google.protobuf module during Protobuf JSON deserialization, preventing the json_format.Parse function from accessing message_type metadata.",
      "proposedFix": "Whitelist google.protobuf module in workflow sandbox imports to allow protobuf deserialization to access required metadata during message parsing.",
      "workaround": "Import google.protobuf explicitly within workflow.unsafe.imports_passed_through() context manager before executing workflows with Protobuf messages.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The only thing I can state is that if I add the following import, everything works: with workflow.unsafe.imports_passed_through(): import google.protobuf",
      "number": 688,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:19:13.830Z"
    },
    {
      "summary": "Feature request to support setting workflow execution timeouts at the workflow definition level or worker configuration, rather than only when directly executing workflows. Currently, timeouts can only be specified in client.execute_workflow() calls, making them inaccessible when workflows are triggered by schedules.",
      "category": "feature",
      "subcategory": "workflow-timeouts",
      "apis": [
        "execute_workflow",
        "create_schedule",
        "ScheduleActionStartWorkflow"
      ],
      "components": [
        "workflow-definition",
        "worker",
        "scheduler",
        "client-api"
      ],
      "concepts": [
        "timeout",
        "workflow-execution",
        "schedule",
        "configuration",
        "worker-definition"
      ],
      "severity": "medium",
      "userImpact": "Users cannot define workflow execution timeouts when workflows are triggered by schedules, requiring workarounds or manual configuration at the schedule level.",
      "rootCause": null,
      "proposedFix": "Add support for timeout parameters in workflow definition or worker configuration so timeouts are applied regardless of how workflows are triggered.",
      "workaround": "Define timeouts at the schedule level when creating schedules with ScheduleActionStartWorkflow, or use asyncio.timeout() inside workflow code for client-side timeout handling.",
      "resolution": "resolved",
      "resolutionDetails": "Issue was resolved through clarification that timeouts can be set at the schedule level using ScheduleActionStartWorkflow with execution_timeout parameter, and client-side timeouts can be implemented using asyncio.timeout() within workflow code.",
      "related": [
        2460
      ],
      "keyQuote": "Can you clarify the question a bit? The `execution_timeout` option is available both for create schedule in Python and for just starting a workflow directly in Python.",
      "number": 686,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:57.303Z"
    },
    {
      "summary": "Activity failure handling doesn't properly include application failure information when the failure can't be converted. The SDK builds a generic failure instead of a properly formatted one with application failure context.",
      "category": "bug",
      "subcategory": "activity-failure-handling",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "failure-conversion"
      ],
      "concepts": [
        "error-handling",
        "failure-conversion",
        "application-failure",
        "activity-execution"
      ],
      "severity": "medium",
      "userImpact": "Users lose important failure context and debugging information when activities fail due to conversion issues, making it harder to diagnose root causes.",
      "rootCause": "The SDK builds a general failure without properly populating application failure info when failure conversion fails, leaving out critical context.",
      "proposedFix": "Apply the same fix that was recently implemented in the .NET SDK (PR #353) to properly build activity failures with application failure information.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by implementing proper application failure information in the generic failure builder, aligned with the .NET SDK approach.",
      "related": [],
      "keyQuote": "When activity fails, but the failure can't be converted for whatever reason, we build a general failure... But we aren't properly building this by putting application failure info in it.",
      "number": 685,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:59.106Z"
    },
    {
      "summary": "Update requests in the Python SDK do not set the `first_execution_run_id` field, which other SDKs do set. Additionally, there's a question about whether the run ID from the response should be used when constructing the returned update handle.",
      "category": "bug",
      "subcategory": "update-requests",
      "apis": [
        "UpdateWorkflow"
      ],
      "components": [
        "client",
        "update-handler",
        "workflow-updates"
      ],
      "concepts": [
        "execution-run-id",
        "update-requests",
        "consistency",
        "sdk-parity"
      ],
      "severity": "medium",
      "userImpact": "Users may experience inconsistent behavior with update requests compared to other SDKs, potentially causing issues with workflow execution tracking and update handle construction.",
      "rootCause": "The Python SDK's update request implementation does not populate the `first_execution_run_id` field that should be sent, and possibly does not use the run ID from the response correctly.",
      "proposedFix": "Set `first_execution_run_id` in update requests to match other SDK implementations, and verify that the run ID from the response is correctly used in the returned update handle.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was resolved by updating the update request implementation to set the required field and align with other SDKs.",
      "related": [],
      "keyQuote": "Update requests do not send `first_execution_run_id`: ... However, they should, and other SDKs do.",
      "number": 682,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:57.651Z"
    },
    {
      "summary": "User requested support for running Temporal test server in air-gapped environments without internet access. The feature request seeks ergonomic ways to provide test server binaries via environment variables or alternative delivery methods since the default download from temporal.download is unavailable offline.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [
        "WorkflowEnvironment.start_time_skipping",
        "WorkflowEnvironment.start_local"
      ],
      "components": [
        "test-server",
        "ephemeral-server",
        "workflow-environment",
        "configuration"
      ],
      "concepts": [
        "air-gapped-environment",
        "offline-testing",
        "environment-variables",
        "binary-distribution",
        "test-server-deployment",
        "dependency-management"
      ],
      "severity": "medium",
      "userImpact": "Users in air-gapped environments cannot easily run workflow tests without manually downloading and configuring test server binaries outside the SDK.",
      "rootCause": "Test server binary is downloaded on-demand from temporal.download, which requires internet access and is not suitable for offline environments.",
      "proposedFix": "Provide test server path via environment variable, or distribute test server binaries through platform-specific PyPI packages as optional dependencies.",
      "workaround": "Manually download test server tarball, unpack it, and pass the binary path to test_server_existing_path parameter in WorkflowEnvironment.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers decided not to add multiple configuration methods beyond default download and the existing test_server_existing_path escape hatch. Users can handle custom downloads themselves. Documentation improvements were deferred.",
      "related": [],
      "keyQuote": "I am not sure we want to bake in so many ways to get a file beyond the default/no-option and the advanced option.",
      "number": 681,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:42.048Z"
    },
    {
      "summary": "User reported that non_retryable_error_types in RetryPolicy is not working - activities with non-retryable errors are still being retried instead of failing immediately. After investigation, the root cause was identified as a deserialization bug in a custom loguru logger patcher, not a bug in the SDK itself.",
      "category": "bug",
      "subcategory": "retry-policy",
      "apis": [
        "ExecuteActivity",
        "RetryPolicy",
        "ApplicationError"
      ],
      "components": [
        "activity-executor",
        "retry-handler",
        "error-handling"
      ],
      "concepts": [
        "retry",
        "non-retryable-errors",
        "error-type",
        "timeout",
        "activity-failure",
        "logging"
      ],
      "severity": "medium",
      "userImpact": "Users may experience unexpected activity retries when using non-retryable error types, causing tasks to fail with timeouts instead of immediately failing as expected.",
      "rootCause": "Custom loguru logger patcher with deserialization bug causing conflicts with the SDK's retry policy handling.",
      "proposedFix": null,
      "workaround": "Remove or fix the custom loguru logger patcher that causes deserialization issues; test in a clean Temporal environment to isolate SDK-specific behavior.",
      "resolution": "invalid",
      "resolutionDetails": "Issue was caused by user's custom logging implementation, not a SDK bug. Reporter confirmed the feature works correctly in a clean environment.",
      "related": [],
      "keyQuote": "I can confirm this retry not working issue is due to somehow a conflict of our custom logger built on loguru.",
      "number": 679,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:43.960Z"
    },
    {
      "summary": "The create_schedule function incorrectly sets trigger_immediately to true even when explicitly set to false by the user, but backfills are provided. This logic bug causes schedules to trigger immediately regardless of user intent.",
      "category": "bug",
      "subcategory": "schedule-backfill",
      "apis": [
        "create_schedule"
      ],
      "components": [
        "client",
        "schedule-creation",
        "backfill-handling"
      ],
      "concepts": [
        "schedule",
        "backfill",
        "trigger-immediately",
        "logic-bug",
        "schedule-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users who create schedules with backfills and explicitly set trigger_immediately=false will have their schedules trigger immediately, causing unexpected behavior.",
      "rootCause": "The code unconditionally sets trigger_immediately to true when backfills are provided, ignoring the user's explicit configuration.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Logic was corrected to respect user's trigger_immediately setting when backfills are provided, aligning with .NET SDK behavior.",
      "related": [],
      "keyQuote": "we are always setting trigger immediately even if user set it to false but provided backfills. .NET properly does not set this.",
      "number": 678,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:41.035Z"
    },
    {
      "summary": "User asks how to log uncaught exceptions from activities and workflows running in thread pool executors while still allowing Temporal to capture failures for retries and UI display. The standard sys.excepthook approach doesn't work for threads.",
      "category": "question",
      "subcategory": "exception-handling",
      "apis": [
        "ExecuteActivity",
        "Worker"
      ],
      "components": [
        "worker",
        "activity-executor",
        "interceptor"
      ],
      "concepts": [
        "exception-handling",
        "logging",
        "threading",
        "error-propagation",
        "retry-mechanism"
      ],
      "severity": "low",
      "userImpact": "Users running activities in thread pool executors cannot customize exception logging without losing error propagation to Temporal.",
      "rootCause": "sys.excepthook is not propagated to threads by Python, so standard exception handling patterns fail in worker thread pools.",
      "proposedFix": "Use activity and workflow interceptors to catch, log, and re-raise exceptions.",
      "workaround": "Redirect stderr to /dev/null in a context manager while using interceptors for logging.",
      "resolution": "fixed",
      "resolutionDetails": "User implemented an interceptor-based solution using structlog to log exceptions from both activities and workflows while maintaining error propagation.",
      "related": [],
      "keyQuote": "You should look into interceptors. For instance, this sample sends errors to Sentry. You can make an activity interceptor that logs and re-raises.",
      "number": 677,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:25.576Z"
    },
    {
      "summary": "JSONPlainPayloadConverter incorrectly deserializes `(str, Enum)` type hints (like `StrEnum`) to a list of characters instead of an Enum instance. Users need support for both `(str, Enum)` and `(int, Enum)` composite type hints in payload deserialization.",
      "category": "bug",
      "subcategory": "serialization",
      "apis": [],
      "components": [
        "JSONPlainPayloadConverter",
        "payload-converter",
        "deserialization"
      ],
      "concepts": [
        "enum",
        "type-hint",
        "serialization",
        "deserialization",
        "str-enum",
        "int-enum",
        "type-conversion"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably pass Enum instances (especially str-based Enums) as activity inputs, receiving lists of characters instead of proper Enum objects.",
      "rootCause": "The payload converter doesn't specifically handle composite type hints like `(str, Enum)` or `(int, Enum)`, treating them as plain types and deserializing the string value as an iterable.",
      "proposedFix": "Add checks for subclass combinations: `issubclass(hint, str) and issubclass(hint, Enum)` and `issubclass(hint, int) and issubclass(hint, Enum)`, then convert using `hint(value)` with appropriate type validation.",
      "workaround": "Use Pydantic data converter instead of JSONPlainPayloadConverter, which provides native support for Enum deserialization.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "JSONPlainPayloadConverter deserializes IntEnum, StrEnum correctly, but it is more reliable to convert payload to any Enum",
      "number": 676,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:28.070Z"
    },
    {
      "summary": "Feature request to auto-skip time when waiting on update results in testing environments, similar to the existing auto-skip behavior for workflow results.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-framework",
        "time-skipping",
        "update-handler"
      ],
      "concepts": [
        "time-skipping",
        "testing",
        "workflow-updates",
        "result-waiting"
      ],
      "severity": "low",
      "userImpact": "Users testing workflows with updates must manually manage time advancement instead of having it auto-skip like workflow result waits.",
      "rootCause": null,
      "proposedFix": "Implement auto-skip time behavior for update result waits, matching the existing pattern for workflow results.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        551
      ],
      "keyQuote": "We auto-skip time when waiting on workflow result, we need to do the same when waiting on update result.",
      "number": 675,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:26.839Z"
    },
    {
      "summary": "Workflow replays produce NondeterminismError when signals and updates overlap, causing events to be processed in a different order than the original execution. Activity completions are being processed after updates instead of before, leading to incorrect workflow behavior.",
      "category": "bug",
      "subcategory": "workflow-replay",
      "apis": [
        "execute_activity",
        "signal",
        "update"
      ],
      "components": [
        "replay-engine",
        "workflow-executor",
        "event-ordering"
      ],
      "concepts": [
        "nondeterminism",
        "event-ordering",
        "replay",
        "signal-update-interaction",
        "state-consistency"
      ],
      "severity": "high",
      "userImpact": "Workflows using both signals and updates can fail during replay with NondeterminismError, making workflows unreliable in production.",
      "rootCause": "The replay engine processes update events before completing activity completion events, violating the original event ordering from the execution history.",
      "proposedFix": "Fix in Core SDK to ensure activity completions are delivered before updates during replay, maintaining original event ordering.",
      "workaround": "Replace updates with signals instead of using overlapping signals and updates together.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed through a Core SDK upgrade; the fix will be available in the next Python SDK release that includes the Core update.",
      "related": [],
      "keyQuote": "The activity completion definitely should be delivered before update on replay if it was originally. We will likely treat this as a high priority.",
      "number": 673,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:11.379Z"
    },
    {
      "summary": "Feature request to drop Python 3.8 support and confirm 3.13 support. Issue discusses removing end-of-life Python 3.8 dependency while addressing compatibility issues with Python 3.13, including traceback problems and workflow sandbox access restrictions that are blocking users.",
      "category": "feature",
      "subcategory": "version-support",
      "apis": [],
      "components": [
        "workflow-sandbox",
        "worker",
        "asyncio-integration"
      ],
      "concepts": [
        "python-version-support",
        "compatibility",
        "dependency-management",
        "traceback-handling",
        "sandbox-restrictions"
      ],
      "severity": "high",
      "userImpact": "Users targeting Python 3.13 experience broken tracebacks and workflow sandbox access errors, while Python 3.8 support maintenance creates unnecessary burden.",
      "rootCause": "Python 3.13 changed traceback handling and introduced stricter sandbox restrictions that the current implementation doesn't account for; Python 3.8 is end-of-life and no longer warranted for support.",
      "proposedFix": "Follow the pattern established in #398 (PR #422) for dropping 3.7 and confirming 3.12 support; evaluate grpcio dependency compatibility with newer Python versions.",
      "workaround": "Revert to Python 3.12 to see actual error tracebacks instead of clobbered ones.",
      "resolution": "fixed",
      "resolutionDetails": "3.13 support was implemented and included in a subsequent release; Python 3.8 support was dropped.",
      "related": [
        398,
        422
      ],
      "keyQuote": "3.13 support is in the next release (should not be too long now, though no exact timelines)",
      "number": 672,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:12.779Z"
    },
    {
      "summary": "Security vulnerability identified in tonic Rust dependency affecting the Python SDK. Requires updating the package to version 0.12.3 to apply necessary security patches for GHSA-4jwc-w2hc-78qv and CVE-2024-47609.",
      "category": "bug",
      "subcategory": "security-dependency",
      "apis": [],
      "components": [
        "tonic",
        "rust-dependencies",
        "security-scanning"
      ],
      "concepts": [
        "security-vulnerability",
        "dependency-update",
        "version-upgrade",
        "rust-package",
        "security-patch",
        "advisory"
      ],
      "severity": "high",
      "userImpact": "Users of the Python SDK are exposed to a known security vulnerability in the tonic Rust dependency until the package is updated.",
      "rootCause": "Outdated version of tonic Rust package containing known security advisories GHSA-4jwc-w2hc-78qv and CVE-2024-47609.",
      "proposedFix": "Update tonic package to version 0.12.3 which contains the necessary security patches.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed as part of Core upgrade, completed in pull request #683.",
      "related": [
        683
      ],
      "keyQuote": "Security scans have identified a vulnerability in one of the Rust dependencies. The current version of the package is affected by the following advisories",
      "number": 671,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:18:10.212Z"
    },
    {
      "summary": "Add support for user metadata in the Python SDK by accepting optional summary/details parameters when starting workflows, creating activities and timers, and implementing the __temporal_workflow_metadata query handler.",
      "category": "feature",
      "subcategory": "user-metadata",
      "apis": [
        "StartWorkflow",
        "ExecuteActivity",
        "CreateTimer"
      ],
      "components": [
        "workflow-client",
        "activity-executor",
        "timer-executor",
        "metadata-handler"
      ],
      "concepts": [
        "user-metadata",
        "workflow-summary",
        "activity-summary",
        "metadata-query",
        "workflow-visibility"
      ],
      "severity": "medium",
      "userImpact": "Users can now provide custom metadata summaries for workflows, activities, and timers to improve visibility and debugging capabilities.",
      "rootCause": null,
      "proposedFix": "Accept optional static summary and static details parameters in workflow start methods, optional summary in timer and activity creation, and implement __temporal_workflow_metadata query handler.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented across workflow, activity, and timer APIs with metadata query support.",
      "related": [
        486,
        830,
        1597
      ],
      "keyQuote": "Accept optional \"static summary\" and \"static details\" on all ways to start a workflow (start, signal with start, schedule, and child workflow)",
      "number": 670,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:56.920Z"
    },
    {
      "summary": "OpenTelemetry logging and metrics integration doesn't work correctly with process-pool workers - trace IDs are reused across activities and the metric_meter() API is unavailable. User requests native support for OTel logging and metrics in process-pool workers similar to existing OTel tracing support.",
      "category": "feature",
      "subcategory": "observability-otel",
      "apis": [
        "activity.metric_meter()"
      ],
      "components": [
        "worker",
        "process-pool",
        "opentelemetry",
        "activity-executor",
        "metrics"
      ],
      "concepts": [
        "observability",
        "distributed-tracing",
        "logging",
        "metrics",
        "process-isolation",
        "trace-context",
        "otel-integration"
      ],
      "severity": "high",
      "userImpact": "Users cannot properly use OpenTelemetry logging and metrics with process-pool workers, making it difficult to instrument sync activities and correlate observability data.",
      "rootCause": "OpenTelemetry logging and metric providers have known limitations with process-forking models similar to TracingProvider, but unlike TracingProvider, no workaround or native support has been implemented in the Temporal SDK.",
      "proposedFix": "Implement native support for OpenTelemetry logging and metrics in process-pool workers, or provide guidance on initialization patterns per-process/activity similar to how TracingProvider was handled.",
      "workaround": "User mentions initializing MeterProvider for each process/activity may be simpler than the global logger approach, but hasn't been fully tested.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal managed to get that to work. Please provide guidance on setting up OTel logging and custom metrics in process-pool-based workers or support them natively",
      "number": 669,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:57.170Z"
    },
    {
      "summary": "User requests a way to manage cleanup of shared resources (like HTTP clients) used by worker activities. Currently, there's no clear lifespan mechanism to ensure proper cleanup of resources when the worker shuts down.",
      "category": "feature",
      "subcategory": "worker-lifecycle",
      "apis": [
        "Worker"
      ],
      "components": [
        "worker",
        "activity-executor",
        "shutdown"
      ],
      "concepts": [
        "resource-cleanup",
        "lifespan",
        "context-manager",
        "async-cleanup",
        "shutdown",
        "worker-lifecycle"
      ],
      "severity": "low",
      "userImpact": "Users cannot easily manage cleanup of shared resources (like HTTP clients) used across multiple activities without manually implementing complex cleanup logic.",
      "rootCause": null,
      "proposedFix": "Include cleanup callback mechanism or formalize worker shutdown cleanup pattern in documentation.",
      "workaround": "Add cleanup code after worker.run() completes or after exiting the async context manager.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer clarified that worker shutdown guarantees all activities are complete, making the current pattern (cleanup after worker.run() or context manager exit) sufficient.",
      "related": [],
      "keyQuote": "Worker shutdown should be sufficient. If you add your cleanup code after worker is shutdown (or after context manager is complete), that is enough to guarantee the worker isn't using things anymore.",
      "number": 666,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:54.416Z"
    },
    {
      "summary": "User requests activity-specific worker tuning to control concurrency per activity type rather than globally, allowing better rate limit management for activities calling external APIs. The current WorkerTuner only supports per-task-queue tuning, which requires deploying separate workers for each activity type.",
      "category": "feature",
      "subcategory": "worker-tuning",
      "apis": [
        "WorkerTuner"
      ],
      "components": [
        "worker",
        "activity-executor",
        "task-queue",
        "slot-supplier"
      ],
      "concepts": [
        "concurrency-control",
        "rate-limiting",
        "activity-tuning",
        "resource-management",
        "worker-scaling"
      ],
      "severity": "medium",
      "userImpact": "Users managing rate-limited external API activities must deploy separate workers for each activity type, increasing operational complexity and DevOps overhead.",
      "rootCause": "Temporal distributes work at the task queue level, not activity name level, so slots cannot be reserved per-activity. Concurrency control mechanisms (WorkerTuner, slots) operate at task queue granularity.",
      "proposedFix": "Implement activity-specific slot allocation or expand WorkerTuner to support per-activity tuning configuration.",
      "workaround": "Use separate task queues (separate workers) for activities needing independent concurrency limits, or implement a mutex workflow pattern to gate activity scheduling.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer explained that Temporal's architecture distributes work at task queue level, not activity level. Recommended using separate task queues or mutex workflows instead.",
      "related": [
        559
      ],
      "keyQuote": "A slot is a reserved spot to request work from a task queue from the server. The activity name is not known at slot reservation time.",
      "number": 663,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:40.390Z"
    },
    {
      "summary": "Child workflow execution gets stuck in WorkflowEnvironment when triggered from a workflow update. The test environment's automatic time-skipping doesn't work when waiting on an update result, preventing child workflows from progressing.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [
        "execute_child_workflow",
        "execute_update",
        "wait_condition"
      ],
      "components": [
        "WorkflowEnvironment",
        "time-skipping",
        "workflow-updates",
        "child-workflows"
      ],
      "concepts": [
        "automatic-time-skipping",
        "testing",
        "workflow-updates",
        "child-workflows",
        "deadlock",
        "event-progression"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably test parent workflows that use child workflow execution triggered by workflow updates in the test environment.",
      "rootCause": "Automatic time skipping in WorkflowEnvironment only advances time when waiting on workflow results, not when waiting on update results, causing child workflow events to never be processed.",
      "proposedFix": "Enable automatic time-skipping when waiting on an update result, similar to how it works for workflow results.",
      "workaround": "Use `asyncio.create_task(handle.result())` to wait for the workflow result in the background, or use `env.time_skipping_unlocked()` context manager to manually control time skipping.",
      "resolution": "fixed",
      "resolutionDetails": "Addressed by opening issue #675 to enable auto time-skipping when waiting on an update result.",
      "related": [
        675
      ],
      "keyQuote": "Automatic time skipping only works when you're waiting on the workflow result. If you start the workflow and do not wait on the result, the automatic time skipping will not occur.",
      "number": 661,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:42.897Z"
    },
    {
      "summary": "User requests a registry or lookup mechanism to retrieve workflow and activity classes/functions by their string names, to support CLI-based worker instantiation with text configuration.",
      "category": "feature",
      "subcategory": "worker-configuration",
      "apis": [
        "Worker"
      ],
      "components": [
        "worker",
        "workflow-registry",
        "activity-registry"
      ],
      "concepts": [
        "registry",
        "dynamic-loading",
        "configuration",
        "CLI",
        "string-lookup",
        "runtime-instantiation"
      ],
      "severity": "low",
      "userImpact": "Users deploying workers via CLI with text-based configuration cannot easily map string names to workflow/activity objects.",
      "rootCause": null,
      "proposedFix": "Maintain a dict of workflows/activities for lookup, potentially with custom decorator support; or add SDK-level registry mechanism.",
      "workaround": "Create a simple Python script that reads environment variables and builds sets of workflows/activities using dict lookup.",
      "resolution": "wontfix",
      "resolutionDetails": "SDK team declined to add global registry, recommending users maintain their own dict-based lookup instead for flexibility across multiple workers.",
      "related": [],
      "keyQuote": "We try not to assume global use of workflows/activities in the SDK. Many users create many workflows/activities and only start some of them in some workers and others in other workers",
      "number": 659,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:41.472Z"
    },
    {
      "summary": "When replacing a worker's client with a new one, validation fails if the original client didn't explicitly set a runtime (relying on the default instead). The runtime comparison doesn't work correctly for lazily-initialized default runtimes.",
      "category": "bug",
      "subcategory": "worker-configuration",
      "apis": [
        "Client.connect",
        "Worker"
      ],
      "components": [
        "worker",
        "client",
        "runtime",
        "service-bridge"
      ],
      "concepts": [
        "runtime-validation",
        "client-replacement",
        "lazy-initialization",
        "default-configuration",
        "type-checking"
      ],
      "severity": "medium",
      "userImpact": "Users cannot replace a worker's client with a new one when using the default runtime, forcing them to explicitly initialize the runtime as a workaround.",
      "rootCause": "The runtime comparison between the replaced client and worker doesn't account for lazily-initialized default runtimes on the client config - it only checks explicitly set runtime values.",
      "proposedFix": "Either eagerly set the runtime default on the client config for the service during initialization, or implement a different method to obtain the runtime from the bridge service client for comparison.",
      "workaround": "Explicitly pass `default=temporalio.runtime.Runtime.default()` when calling `Client.connect()` to ensure the runtime is set before comparison.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Our check that the runtime is the same on the replaced client as is on the worker does not work if the runtime is not set on the client (i.e. lazily uses the default).",
      "number": 657,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:23.426Z"
    },
    {
      "summary": "Sentry's catch_warnings context manager fails in Temporal workflows with KeyError: 'warnings' because the warnings module is restricted by the workflow sandbox for determinism safety. Users need guidance on disabling the sandbox for non-deterministic modules like Sentry.",
      "category": "bug",
      "subcategory": "sandbox-restrictions",
      "apis": [],
      "components": [
        "workflow-sandbox",
        "imports-system",
        "determinism-enforcement"
      ],
      "concepts": [
        "sandbox",
        "determinism",
        "module-restriction",
        "third-party-integration",
        "warnings-module",
        "sentry-integration"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to use Sentry or other libraries that depend on Python's warnings module in workflows encounter failures and need workarounds.",
      "rootCause": "The workflow sandbox restricts access to the warnings module because it is not determinism-safe, causing KeyError when libraries like Sentry try to reference sys.modules['warnings'].",
      "proposedFix": "Pass through imports and disable the sandbox for non-deterministic code sections using the documented sandbox avoidance techniques. The Sentry sample in samples-python demonstrates the correct approach.",
      "workaround": "Use pass-through imports and disable the sandbox for the specific code sections that require non-deterministic modules like Sentry, as documented in the SDK README and demonstrated in the Sentry sample.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "warnings is not a determinism-safe module... Using non-deterministic modules like Sentry require pass through and sandbox avoidance.",
      "number": 655,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:26.478Z"
    },
    {
      "summary": "MultiProcess executor reuses the same worker process when sequentially executing activities in a workflow, instead of distributing them across different processes as expected. The issue is resolved when activities run concurrently rather than sequentially.",
      "category": "bug",
      "subcategory": "multiprocess-executor",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "multiprocess-executor",
        "activity-executor",
        "worker-process-pool"
      ],
      "concepts": [
        "process-reuse",
        "sequential-execution",
        "concurrent-execution",
        "worker-distribution",
        "multiprocessing"
      ],
      "severity": "medium",
      "userImpact": "Users expecting multiprocess executor to distribute sequential activities across different worker processes find all activities running on the same process, defeating the purpose of multiprocessing.",
      "rootCause": "Multiprocess executor does not allocate new processes for sequential activity executions; it only distributes concurrent (parallel) activity executions across different processes.",
      "proposedFix": null,
      "workaround": "Run activities concurrently using asyncio.create_task() instead of sequentially awaiting them, which allows the multiprocess executor to distribute them across different worker processes.",
      "resolution": "invalid",
      "resolutionDetails": "This is not a bug but rather the expected behavior - multiprocess executor is designed to distribute concurrent activities across processes, not sequential ones. The user discovered the correct usage pattern through their own investigation.",
      "related": [
        1651
      ],
      "keyQuote": "I would have expected to see a different PID when the activity runs the second time!",
      "number": 654,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:24.928Z"
    },
    {
      "summary": "ScheduleOverlapPolicy enum has broken __eq__ method that incorrectly returns True when comparing different enum values. The issue is caused by ScheduleOverlapPolicy being both a dataclass and an IntEnum, causing comparison logic to malfunction.",
      "category": "bug",
      "subcategory": "schedule-management",
      "apis": [
        "ScheduleOverlapPolicy",
        "Schedule"
      ],
      "components": [
        "schedule",
        "enum",
        "comparison"
      ],
      "concepts": [
        "equality",
        "enum",
        "dataclass",
        "type-conflict",
        "comparison-logic",
        "schedule-validation"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably compare Schedule objects when they differ only in ScheduleOverlapPolicy, breaking deployment scripts that need to detect schedule changes.",
      "rootCause": "ScheduleOverlapPolicy is incorrectly defined as both a dataclass and an IntEnum simultaneously, causing the __eq__ method to use dataclass comparison logic instead of enum value comparison.",
      "proposedFix": "Remove the dataclass decorator from ScheduleOverlapPolicy, keeping only the IntEnum definition, as was done in PR #628.",
      "workaround": null,
      "related": [
        628
      ],
      "keyQuote": "ScheduleOverlapPolicy is both a dataclass and an IntEnum. Seems like the rare combination might have caused the behaviour here?",
      "resolution": null,
      "resolutionDetails": null,
      "number": 652,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:06.995Z"
    },
    {
      "summary": "Request to include update handler and ID information in logging context for better observability and debugging of update handlers in Python SDK.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "logging",
        "update-handler",
        "context"
      ],
      "concepts": [
        "observability",
        "debugging",
        "context-propagation",
        "handler-identification",
        "logging-context"
      ],
      "severity": "low",
      "userImpact": "Users have limited visibility into which update handler and ID is executing when reviewing logs.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented to include update handler and ID in logging context as requested in the features issue.",
      "related": [
        466
      ],
      "keyQuote": "Show update handler and ID in logging context",
      "number": 648,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:03.957Z"
    },
    {
      "summary": "Python SDK lacks support for OpenTelemetry metrics export via OTLP/HTTP protocol. Currently only OTLP/gRPC is supported, preventing integration with services like Pydantic Logfire that require HTTP endpoints.",
      "category": "feature",
      "subcategory": "opentelemetry-metrics",
      "apis": [],
      "components": [
        "opentelemetry-exporter",
        "metrics-pipeline"
      ],
      "concepts": [
        "opentelemetry",
        "otlp",
        "http-protocol",
        "grpc",
        "metrics-export",
        "protocol-support"
      ],
      "severity": "medium",
      "userImpact": "Users cannot export OpenTelemetry metrics to HTTP-based services, limiting integration options and requiring workarounds.",
      "rootCause": "Python SDK's OpenTelemetry metrics exporter only implements OTLP/gRPC transport, not OTLP/HTTP like some other SDKs.",
      "proposedFix": "Add a simple boolean `http` option to `OpenTelemetryConfig` to enable HTTP transport, consistent with Go SDK support.",
      "workaround": "Deploy an OpenTelemetry collector (e.g., Grafana Alloy) to receive gRPC metrics and forward them to HTTP endpoints.",
      "resolution": "fixed",
      "resolutionDetails": "Implementation in Rust Core layer (sdk-core #820) will provide HTTP support that can be applied to Python SDK.",
      "related": [
        820
      ],
      "keyQuote": "Support for exporting OTel Metrics to services via OLTP/HTTP. This would bring the Python SDK up to parity with the Go SDK",
      "number": 647,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:17:09.482Z"
    },
    {
      "summary": "Activity fails with payload conversion error when SQLModel objects with table=True are used as return values. The error references os.environ.get access from within workflow context, though this appears to be a side effect of SQLAlchemy mapper initialization rather than direct activity code.",
      "category": "bug",
      "subcategory": "payload-conversion",
      "apis": [],
      "components": [
        "converter",
        "payload-conversion",
        "sqlmodel-integration",
        "workflow-instance"
      ],
      "concepts": [
        "determinism",
        "environment-access",
        "orm-integration",
        "type-conversion",
        "mapper-initialization"
      ],
      "severity": "high",
      "userImpact": "Activities fail intermittently when returning SQLModel objects with table=True, preventing database operation results from being properly serialized back to workflows.",
      "rootCause": "SQLAlchemy mapper initialization triggered during payload deserialization accesses os.environ.get, which is blocked in workflow context. The issue occurs when SQLModel objects with table=True are instantiated during type conversion.",
      "proposedFix": "Use SQLModel with table=False (Pydantic BaseModel mode) for objects passed between workflows and activities, as these don't trigger ORM mapper initialization.",
      "workaround": "Define a base SQLModel class with table=False for cross-boundary communication instead of using ORM-backed models (table=True). ORM models should only be used within activity/worker context.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue is due to fundamental non-determinism in SQLAlchemy ORM initialization. Temporal SDK's environment restrictions are by design for deterministic workflow execution. Users should use non-ORM SQLModel variants for inter-process communication.",
      "related": [
        254
      ],
      "keyQuote": "I would not recommend using sqlalchemy models in workflows because they are likely non-deterministic.",
      "number": 646,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:52.852Z"
    },
    {
      "summary": "Request to expose update-with-start operations through the Python SDK API. This feature allows combining update and start workflow operations, similar to existing UpdateWithStart functionality in other Temporal SDKs.",
      "category": "feature",
      "subcategory": "workflow-operations",
      "apis": [
        "UpdateWithStart",
        "StartWorkflow"
      ],
      "components": [
        "sdk-api",
        "workflow-client",
        "update-handler"
      ],
      "concepts": [
        "update",
        "start-workflow",
        "idempotency",
        "workflow-state",
        "operation-composition"
      ],
      "severity": "medium",
      "userImpact": "Users cannot perform atomic update-with-start operations in Python, limiting their ability to safely update or create workflows in a single operation.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "UpdateWithStart API was implemented and exposed in the Python SDK",
      "related": [],
      "keyQuote": "Expose update-with-start operations via the SDK",
      "number": 643,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:50.141Z"
    },
    {
      "summary": "Security vulnerability in quinn-proto Rust dependency affecting the Python SDK. Update to version 0.11.7 to apply necessary security patches for advisories GHSA-vr26-jcq5-fjj8 and RUSTSEC-2024-0373.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "rust-dependencies",
        "quinn-proto",
        "build-system"
      ],
      "concepts": [
        "security-vulnerability",
        "dependency-management",
        "patch-update",
        "supply-chain"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to security vulnerabilities in the Python SDK's Rust dependencies until the package is updated.",
      "rootCause": "quinn-proto Rust package contains security advisories GHSA-vr26-jcq5-fjj8 and RUSTSEC-2024-0373 in current version.",
      "proposedFix": "Update quinn-proto Rust package to version 0.11.7 which contains the necessary security patches.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Dependency was updated to version 0.11.7 to address the identified security vulnerabilities.",
      "related": [],
      "keyQuote": "Update to version 0.11.7, which contains the necessary patches.",
      "number": 642,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:52.801Z"
    },
    {
      "summary": "When encode_common_attributes=True is set on the failure converter, querying an unregistered workflow query or the internal list query returns an \"Encoded failure\" error instead of a proper failure response. The UI's query listing feature fails because it relies on the internal query type which also encounters this encoding issue.",
      "category": "bug",
      "subcategory": "query-handling",
      "apis": [
        "Client.get_workflow_handle",
        "query"
      ],
      "components": [
        "query-handler",
        "failure-converter",
        "data-converter"
      ],
      "concepts": [
        "query-failure",
        "failure-encoding",
        "encode-common-attributes",
        "ui-integration",
        "internal-queries"
      ],
      "severity": "high",
      "userImpact": "Users cannot list workflow queries in the UI and cannot send unregistered queries with encrypted failure converters, both operations fail with uninformative \"Encoded failure\" errors.",
      "rootCause": "The server doesn't support Failure structure encoding for query failures, making encrypted failures incompatible with the UI's query listing mechanism which uses internal queries.",
      "proposedFix": "Server-side fix for query failure encoding (issue #6845), or SDK-side workaround to keep query failures in plain text temporarily.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        6845
      ],
      "keyQuote": "query failures use the Failure structure but the server doesn't support that for query failures",
      "number": 641,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:37.512Z"
    },
    {
      "summary": "User experiences intermittent 'operation was canceled' errors when calling start_workflow with multiple concurrent requests. Out of 10 workflows, only 6 succeed while 4 fail with RPC errors, with no consistent reproduction pattern.",
      "category": "bug",
      "subcategory": "workflow-start",
      "apis": [
        "start_workflow",
        "signal_workflow"
      ],
      "components": [
        "client",
        "rpc-layer",
        "workflow-starter"
      ],
      "concepts": [
        "concurrency",
        "network",
        "timeout",
        "retry",
        "connection",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably start multiple workflows due to intermittent cancellation errors, impacting workflow initialization reliability.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Add retry logic when calling start_workflow, though it only reduces the error frequency rather than eliminating it.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        807
      ],
      "keyQuote": "It happens when I called method `start_workflow`. Maybe it cant connect to create workflow on temporal and return `temporal_sdk_bridge.RPCError: (1, 'operation was canceled', b'')`",
      "number": 639,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:33.612Z"
    },
    {
      "summary": "Importing third-party packages like ruamel.yaml that patch stdlib modules (e.g., datetime) causes workflow validation to fail in the sandbox. The issue occurs because the SDK proxies datetime to enforce determinism, but library patches interact unexpectedly with these proxies, resulting in 'unhashable type' errors.",
      "category": "bug",
      "subcategory": "workflow-sandbox",
      "apis": [],
      "components": [
        "workflow-sandbox",
        "datetime-proxy",
        "validator"
      ],
      "concepts": [
        "sandbox",
        "determinism",
        "stdlib-patching",
        "third-party-compatibility",
        "import-side-effects",
        "proxying"
      ],
      "severity": "high",
      "userImpact": "Users cannot use common third-party packages that patch stdlib modules without either explicitly passing them through imports or disabling sandbox restrictions.",
      "rootCause": "ruamel.yaml patches datetime module at import time. The SDK proxies datetime for determinism checking, but when third-party code patches datetime, the proxy becomes unhashable, causing validation to fail when yaml's representer tries to register itself.",
      "proposedFix": "Disable datetime proxying for specific sandbox runners, or provide better error messages suggesting to use imports_passed_through or unsafe.sandbox_unrestricted().",
      "workaround": "Use imports_passed_through to pass ruamel.yaml through, or create a custom sandbox runner without datetime proxying, or wrap ruamel-using code with unsafe.sandbox_unrestricted().",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The main issue is that an innocuous change in another part of the codebase that doesn't touch anything temporal related can cause temporal sandbox failures.",
      "number": 638,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:35.968Z"
    },
    {
      "summary": "Windows builds for Python 3.12.5 fail during linking with 'cannot open input file python3.lib' error. The linker cannot find the required Python library file on Windows, preventing successful compilation.",
      "category": "bug",
      "subcategory": "build-windows",
      "apis": [],
      "components": [
        "build-system",
        "windows-build",
        "linking"
      ],
      "concepts": [
        "compilation",
        "linking",
        "windows-platform",
        "python-3.12",
        "build-failure",
        "library-resolution"
      ],
      "severity": "high",
      "userImpact": "Users on Windows with Python 3.12.5 cannot build or install the SDK due to linker failures.",
      "rootCause": "The linker cannot locate python3.lib, likely due to Python 3.12.5 Windows installation or build configuration issue.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "LINK : fatal error LNK1181: cannot open input file 'python3.lib'",
      "number": 637,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:21.315Z"
    },
    {
      "summary": "Request to add type-level testing capabilities to verify that mypy and pyright type checkers enforce the desired type constraints, and to prevent future regressions in type safety.",
      "category": "feature",
      "subcategory": "type-checking",
      "apis": [],
      "components": [
        "type-stubs",
        "type-checker-integration",
        "test-framework"
      ],
      "concepts": [
        "type-safety",
        "static-analysis",
        "mypy",
        "pyright",
        "type-enforcement",
        "regression-prevention"
      ],
      "severity": "medium",
      "userImpact": "Users need confidence that the SDK's type hints are correctly enforced by type checkers to catch errors at development time.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We'd like a good way to write type-level tests to confirm that mypy and pyright are enforcing what we want them to enforce and to prevent regressions.",
      "number": 636,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:17.597Z"
    },
    {
      "summary": "User encountered a 'limit on number of total updates has been reached (2000)' error when attempting to execute a workflow update from within a temporal activity. The issue was triggered when trying to send an update from a scheduled workflow's activity to a long-running workflow.",
      "category": "bug",
      "subcategory": "workflow-updates",
      "apis": [
        "execute_update",
        "start_workflow_update",
        "update_workflow_execution"
      ],
      "components": [
        "client",
        "activity-executor",
        "workflow-updates"
      ],
      "concepts": [
        "updates",
        "activities",
        "workflow-communication",
        "error-handling",
        "concurrency-limits",
        "cross-workflow-updates"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably execute workflow updates from activities without hitting server-side limits, preventing common patterns of inter-workflow communication.",
      "rootCause": "Server-side limit on total number of updates reached (2000), preventing new updates from being processed. User later identified a bug in their own code when sending invalid data.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was caused by a bug in the user's code when sending invalid data, not a bug in the SDK. The 'limit reached' error was a symptom of the underlying issue.",
      "related": [],
      "keyQuote": "There's a bug in my code when sending invalid data.",
      "number": 633,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:20.080Z"
    },
    {
      "summary": "OpenTelemetry export fails when using TLS connections. This is a feature request to update the Python SDK to match a fix implemented in sdk-core.",
      "category": "bug",
      "subcategory": "observability",
      "apis": [],
      "components": [
        "telemetry",
        "tls",
        "core-update"
      ],
      "concepts": [
        "opentelemetry",
        "export",
        "tls",
        "observability",
        "telemetry"
      ],
      "severity": "medium",
      "userImpact": "Users cannot export OpenTelemetry data when their Temporal connection uses TLS, limiting observability capabilities.",
      "rootCause": "OpenTelemetry export implementation does not properly handle TLS connections",
      "proposedFix": "Update Python SDK core to match the fix implemented in sdk-core v1.7.1",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed with the 1.7.1 release of the Python SDK",
      "related": [
        801
      ],
      "keyQuote": "This should now be fixed with the 1.7.1 release",
      "number": 632,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:02.063Z"
    },
    {
      "summary": "Worker hangs indefinitely when polling workflow task queue with incorrect authorization token. The worker receives 504 Gateway Timeout errors and PermissionDenied responses, eventually raising a RuntimeError without properly recovering or shutting down gracefully.",
      "category": "bug",
      "subcategory": "worker-authorization",
      "apis": [
        "Worker",
        "Client.connect"
      ],
      "components": [
        "worker",
        "grpc-client",
        "error-handling",
        "polling"
      ],
      "concepts": [
        "authorization",
        "authentication",
        "connection-timeout",
        "error-recovery",
        "graceful-shutdown",
        "retry-logic"
      ],
      "severity": "high",
      "userImpact": "Users with incorrect authorization tokens experience indefinite hangs and unclear error messages instead of immediate, actionable failures.",
      "rootCause": "Ingress gateway timeout configuration requires minimum 70-second polling intervals; worker does not properly handle authorization errors during long polls and fails to shut down gracefully.",
      "proposedFix": null,
      "workaround": "Reduce long polling interval via Temporal server dynamic config: `matching.longPollExpirationInterval: ['50s']`",
      "resolution": "wontfix",
      "resolutionDetails": "Closed as ingress configuration issue - Temporal requires ability to support 70-second calls. The root cause was determined to be infrastructure-related rather than SDK defect.",
      "related": [],
      "keyQuote": "The worker ends up raising a RuntimeError and not recovering from it.",
      "number": 631,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:06.734Z"
    },
    {
      "summary": "Calling workflow.upsert_search_attributes() with an empty list causes the SDK to crash with an unclear \"malformed workflow completion\" error instead of handling the empty input gracefully or providing a helpful validation error.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [
        "upsert_search_attributes"
      ],
      "components": [
        "workflow-engine",
        "completion-handler",
        "search-attributes"
      ],
      "concepts": [
        "validation",
        "error-handling",
        "input-validation",
        "workflow-completion",
        "search-attributes"
      ],
      "severity": "high",
      "userImpact": "Developers encounter cryptic runtime errors when accidentally passing empty lists to upsert_search_attributes, making it difficult to diagnose and fix the issue.",
      "rootCause": "The SDK does not validate empty input for upsert_search_attributes and sends a malformed workflow completion to the server, which rejects it with an opaque error message.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "At least one workflow command in the completion contained an empty variant",
      "number": 629,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:16:04.296Z"
    },
    {
      "summary": "Add support for serializing and deserializing `datetime` values in the default JSON converter using ISO 8601 format.",
      "category": "feature",
      "subcategory": "serialization",
      "apis": [],
      "components": [
        "json-converter",
        "serialization"
      ],
      "concepts": [
        "datetime",
        "iso8601",
        "json-serialization",
        "type-conversion",
        "data-encoding"
      ],
      "severity": "medium",
      "userImpact": "Users cannot directly serialize Python datetime objects in workflows, requiring manual conversion workarounds.",
      "rootCause": null,
      "proposedFix": "Implement datetime to/from ISO 8601 string conversion in the default JSON converter",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to support datetime serialization in the JSON converter",
      "related": [],
      "keyQuote": "datetime needs to be supported to/from string in default JSON converter (use iso8601)",
      "number": 627,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:48.723Z"
    },
    {
      "summary": "Fix pyright type-checking violations that were previously excluded from CI validation. This involves addressing type-checking issues to remove the exclusion list and achieve full type compliance.",
      "category": "other",
      "subcategory": "type-checking",
      "apis": [],
      "components": [
        "type-checker",
        "ci-configuration",
        "pyright-configuration"
      ],
      "concepts": [
        "type-safety",
        "static-analysis",
        "code-quality",
        "ci-validation",
        "type-compliance"
      ],
      "severity": "medium",
      "userImpact": "Improves code quality and developer experience by ensuring type safety across the Python SDK codebase.",
      "rootCause": "Pyright was added to CI with substantial exclusions rather than fixing underlying type violations.",
      "proposedFix": "Address all pyright type-checking violations and remove the exclusion list from CI configuration.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        420,
        795
      ],
      "keyQuote": "#420 started requiring pyright in CI, but with a substantial exclusion list. Fix violations and remove exclusion list.",
      "number": 625,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:50.590Z"
    },
    {
      "summary": "RPCStatusCode.ALREADY_EXISTS can be swallowed due to an unhandled execution branch in the client code, causing errors to be lost silently.",
      "category": "bug",
      "subcategory": "rpc-error-handling",
      "apis": [],
      "components": [
        "client",
        "rpc-status-handling",
        "error-handling"
      ],
      "concepts": [
        "error-swallowing",
        "rpc-status-codes",
        "unhandled-branches",
        "exception-handling",
        "client-communication"
      ],
      "severity": "high",
      "userImpact": "Users may experience silent failures when ALREADY_EXISTS errors occur, making it difficult to debug workflows that attempt duplicate operations.",
      "rootCause": "An unhandled branch of execution in temporalio/client.py at lines 5086-5093 can cause RPCStatusCode.ALREADY_EXISTS errors to be suppressed.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The unhandled execution branch was addressed to properly handle RPCStatusCode.ALREADY_EXISTS errors.",
      "related": [],
      "keyQuote": "RPCStatusCode.ALREADY_EXISTS can be swallowed due to an unhandled branch of execution",
      "number": 624,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:47.902Z"
    },
    {
      "summary": "Installation of temporalio package fails on Python 3.11 Alpine Linux due to missing protoc-wheel-0 dependency. The SDK maintainers do not provide pre-built Alpine/musl binaries, requiring users to build from source.",
      "category": "bug",
      "subcategory": "installation",
      "apis": [],
      "components": [
        "build-system",
        "dependency-resolution",
        "protoc-wheel"
      ],
      "concepts": [
        "alpine-linux",
        "musl-compatibility",
        "binary-wheels",
        "docker-base-image",
        "build-dependencies"
      ],
      "severity": "high",
      "userImpact": "Users cannot install temporalio on Alpine-based Python Docker images without building from source, limiting adoption of this popular lightweight base image.",
      "rootCause": "SDK does not distribute pre-built wheels for musl/Alpine Linux, and build process requires protoc-wheel-0 which is not available for Alpine environments.",
      "proposedFix": null,
      "workaround": "Build the SDK from source following the build instructions in the repository README.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We do not provide Alpine/musl builds in PyPI. You will have to build the SDK yourself.",
      "number": 622,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:35.819Z"
    },
    {
      "summary": "Create an official Pydantic contrib module for the Python SDK to provide seamless integration with Pydantic models, including JSON encoding payload converters, data converters, and datetime handling support.",
      "category": "feature",
      "subcategory": "pydantic-integration",
      "apis": [],
      "components": [
        "payload-converter",
        "data-converter",
        "worker",
        "contrib-module"
      ],
      "concepts": [
        "pydantic",
        "serialization",
        "json-encoding",
        "datetime-handling",
        "type-validation",
        "data-conversion"
      ],
      "severity": "medium",
      "userImpact": "Users can now officially use Pydantic models with Temporal workflows without friction or workarounds.",
      "rootCause": null,
      "proposedFix": "Create temporalio.contrib.pydantic module with JSON payload converter, default payload converter, default data converter, and Pydantic-safe worker runner for datetime handling.",
      "workaround": "Use the pydantic_converter sample from temporalio/samples-python, though it lacks Pydantic V2 support.",
      "resolution": "fixed",
      "resolutionDetails": "Official Pydantic v2 support was added in SDK version 1.10.0",
      "related": [
        97
      ],
      "keyQuote": "Pydantic is popular/common enough and people are struggling with [the sample], that we should just make a temporalio.contrib.pydantic module",
      "number": 621,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:34.806Z"
    },
    {
      "summary": "In Python SDK, workflow cancellation can be swallowed when an activity completes during the cancellation request, particularly with short-lived activities. The issue appears to be related to Python asyncio's behavior where cancellation can be lost if the awaited operation completes and swallows the cancellation signal.",
      "category": "bug",
      "subcategory": "workflow-cancellation",
      "apis": [
        "execute_activity",
        "cancel"
      ],
      "components": [
        "workflow-cancellation",
        "activity-executor",
        "asyncio-integration"
      ],
      "concepts": [
        "cancellation",
        "asyncio",
        "race-condition",
        "activity-completion",
        "signal-handling",
        "timeout"
      ],
      "severity": "high",
      "userImpact": "Workflows may fail to properly handle cancellation requests when activities complete during cancellation, leading to unexpected behavior and potential workflow hangs.",
      "rootCause": "Python asyncio behavior where cancellation can be swallowed if the awaited operation completes before the cancellation is processed, causing the CancelledError to be lost.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed and closed, likely through documentation or code fixes to ensure cancellation handling works correctly with both wait-cancel and try-cancel cancellation types.",
      "related": [],
      "keyQuote": "if the awaiting thing swallows cancellation it's like it never happened. This is probably just existing, unfortunate Python behavior we have to document.",
      "number": 620,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:32.873Z"
    },
    {
      "summary": "The wait_condition semantics in Python SDK cause unexpected behavior when multiple coroutines wait on the same condition. When the condition becomes true, all waiting coroutines wake up simultaneously, but the condition may change before all of them execute their subsequent code, causing some to see a different state than expected.",
      "category": "feature",
      "subcategory": "wait-condition",
      "apis": [
        "wait_condition"
      ],
      "components": [
        "workflow-runtime",
        "async-scheduler",
        "condition-variable"
      ],
      "concepts": [
        "concurrency",
        "condition-synchronization",
        "race-condition",
        "predicate-evaluation",
        "task-scheduling",
        "shared-state"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably use wait_condition with multiple concurrent tasks without accounting for the possibility that the condition may change before execution resumes, making concurrent workflow patterns error-prone.",
      "rootCause": "The wait_condition implementation wakes all waiting coroutines when the predicate is satisfied, but the condition may be modified by any coroutine before others execute, creating a time-of-check-time-of-use (TOCTOU) problem.",
      "proposedFix": null,
      "workaround": "Create tasks with small sleep delays between them to allow individual execution before batch operations, though this creates unnecessary temporal timers.",
      "related": [],
      "keyQuote": "the concern is that wait conditions _all_ wake up if the predicate is satisfied (as opposed to other SDKs that only wake up one)",
      "resolution": null,
      "resolutionDetails": null,
      "number": 618,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:18.998Z"
    },
    {
      "summary": "Feature request to investigate integrating Allure reporting framework into the Python SDK's GitHub Actions CI workflows. The reporting tool offers historical correlation of test results across runs, potentially improving test visibility and debugging.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "ci",
        "github-actions",
        "testing"
      ],
      "concepts": [
        "test-reporting",
        "historical-analysis",
        "continuous-integration",
        "test-correlation",
        "visualization"
      ],
      "severity": "low",
      "userImpact": "Better test visibility and historical trend analysis for developers working with the Python SDK, improving ability to identify flaky tests and regressions.",
      "rootCause": null,
      "proposedFix": "Integrate Allure pytest plugin into GitHub Actions workflows to generate and serve test reports with historical correlation capabilities.",
      "workaround": "Manual local setup: `brew install allure`, `pip install allure-pytest`, `pytest --alluredir`, `allure serve`",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The feature providing correlation of results across historical runs sounds particularly interesting, if it works well.",
      "number": 613,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:16.213Z"
    },
    {
      "summary": "The SDK should provide a helpful error message when users try to upsert a datetime search attribute with a timezone-naive datetime object, instead of letting the server fail the Workflow Task Completion.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [
        "upsert_search_attributes"
      ],
      "components": [
        "search-attributes",
        "validation",
        "error-handling"
      ],
      "concepts": [
        "datetime",
        "timezone",
        "validation",
        "error-message",
        "search-attributes",
        "user-experience"
      ],
      "severity": "medium",
      "userImpact": "Users get delayed, unclear errors from the server instead of immediate, helpful feedback when using invalid datetime objects in search attributes.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        572
      ],
      "keyQuote": "The SDK should raise an exception with a helpful error message if the datetime object is invalid for use as a datetime search attribute",
      "number": 611,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:14.088Z"
    },
    {
      "summary": "Add a `limit` parameter to `replay_workflows` and `workflow_replay_iterator` functions to support early stopping when working with async iterators, addressing the limitation that the API only supports page sizes without a LIMIT clause.",
      "category": "feature",
      "subcategory": "workflow-replay",
      "apis": [
        "replay_workflows",
        "workflow_replay_iterator"
      ],
      "components": [
        "workflow-replay",
        "async-iterator",
        "pagination"
      ],
      "concepts": [
        "limiting",
        "async-iteration",
        "pagination",
        "early-termination",
        "workflow-testing"
      ],
      "severity": "low",
      "userImpact": "Users cannot easily limit the number of workflows replayed without manually tracking iteration count, making it cumbersome to test or debug specific subsets of workflows.",
      "rootCause": null,
      "proposedFix": "Add a `limit` parameter to `replay_workflows` and `workflow_replay_iterator` functions to allow users to cap the number of results returned.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The limit parameter was implemented in the replay workflow functions.",
      "related": [],
      "keyQuote": "Add this to the fact that our API doesn't support a LIMIT clause, but only page sizes, we have no good way to stop these functions early.",
      "number": 610,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:01.694Z"
    },
    {
      "summary": "Feature request to improve error handling for payload conversion failures on workflow input by making the error type more discriminable and allowing it to be set on workflow_failure_exception_types, with consideration for reusing this exception across other payload conversion error locations.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "workflow",
        "payload-converter",
        "error-handling"
      ],
      "concepts": [
        "payload-conversion",
        "error-discrimination",
        "exception-types",
        "workflow-input",
        "error-classification"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily distinguish and handle payload conversion errors on workflow input, making it difficult to implement custom error handling logic.",
      "rootCause": null,
      "proposedFix": "Create a better error type for payload conversion failures on workflow input that can be set on workflow_failure_exception_types, and consider reusing this exception in other payload conversion error locations.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "On invalid workflow input, if a bad value is called, the task fails with a runtime error. This should be a better error type that can be set on `workflow_failure_exception_types`.",
      "number": 608,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:59.007Z"
    },
    {
      "summary": "Python worker unintentionally hot-reloads workflow definitions from disk on every workflow run or CAN instead of requiring process restart, creating production deployment risks where users expect file overwrites to only take effect after process restart.",
      "category": "bug",
      "subcategory": "worker-sandboxing",
      "apis": [],
      "components": [
        "worker",
        "sandboxing",
        "import-system",
        "workflow-loader"
      ],
      "concepts": [
        "hot-reload",
        "import-cache",
        "workflow-definition",
        "process-restart",
        "production-safety",
        "file-system"
      ],
      "severity": "high",
      "userImpact": "Production deployments are at risk because running workflow processes can unexpectedly hot-reload code changes from disk, breaking user expectations that file modifications only take effect after process restart.",
      "rootCause": "Worker sandboxing implementation disables Python import cache, causing the workflow definition to be re-read from disk on every workflow run and CAN instead of being cached.",
      "proposedFix": "Permit the re-read-from-disk to continue but have the worker copy relevant files to a private location and import workflows from there instead of directly from the original location.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "worker sandboxing involves disabling import cache which results in re-reading from disk... users are likely to expect that during deployments they can overwrite files without any modifications to running processes until process restart",
      "number": 607,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:15:01.332Z"
    },
    {
      "summary": "Align Python SDK's activation job application model with TypeScript SDK by applying all jobs in a single batch before running the event loop, instead of multiple batches. This aligns with the Core-based language pattern where state is updated synchronously before routines execute.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "worker",
        "workflow-instance",
        "event-loop",
        "job-application"
      ],
      "concepts": [
        "state-synchronization",
        "event-loop",
        "determinism",
        "job-batching",
        "task-execution"
      ],
      "severity": "low",
      "userImpact": "Potentially improves workflow execution consistency and intuitiveness across language SDKs without necessarily changing user-facing behavior.",
      "rootCause": null,
      "proposedFix": "Change Python's multi-batch job application (as shown in _workflow_instance.py#L325) to single-batch model followed by event loop execution, similar to PR #1488 in TypeScript SDK.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1488
      ],
      "keyQuote": "state is updated synchronously, then stuff runs until we're blocked",
      "number": 606,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:47.568Z"
    },
    {
      "summary": "Feature request to investigate and document how to use PyCharm debugger with Temporal workflows. PyCharm's asyncio debugging has compatibility issues with the SDK's custom asyncio implementation, requiring investigation and documentation of workarounds.",
      "category": "docs",
      "subcategory": "debugging-ide-integration",
      "apis": [
        "Replayer"
      ],
      "components": [
        "debugger-support",
        "asyncio-integration",
        "workflow-execution"
      ],
      "concepts": [
        "debugging",
        "ide-integration",
        "asyncio",
        "workflow-task-timeout",
        "sandbox",
        "breakpoints"
      ],
      "severity": "low",
      "userImpact": "Python developers using PyCharm cannot effectively debug workflows due to asyncio compatibility issues, requiring documentation of workarounds.",
      "rootCause": "PyCharm's asyncio debugger has patching/expectations around asyncio that don't hold true for the SDK's custom asyncio implementation.",
      "proposedFix": "Use Replayer with debugger attached to step through workflow code without Task timeouts interfering; toggle python.debug.asyncio.repl in PyCharm Registry.",
      "workaround": "Execute workflow code within a Replayer while debugger is attached; this bypasses Workflow Task timeouts (default 10s, max 120s) that would otherwise interfere with debugging.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        238
      ],
      "keyQuote": "You can get around this Task boundary by executing your code within a `Replayer`. This is done by passing Workflow Execution history into the Replayer while the debugger is attached to step through the code.",
      "number": 603,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:46.638Z"
    },
    {
      "summary": "User requests a workflow reset feature for the Python SDK similar to other Temporal SDKs. The maintainer clarifies that workflow reset is intentionally exposed only via raw gRPC calls through workflow_service for advanced users, not as a high-level API.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [
        "workflow_service.reset_workflow_execution"
      ],
      "components": [
        "workflow-service",
        "grpc-client",
        "api-exposure"
      ],
      "concepts": [
        "workflow-reset",
        "error-recovery",
        "state-management",
        "advanced-operations",
        "grpc-integration"
      ],
      "severity": "low",
      "userImpact": "Users needing to reset workflows for error recovery must use low-level gRPC APIs directly rather than a high-level SDK method.",
      "rootCause": null,
      "proposedFix": "User offered to contribute a PR with production-ready code for workflow reset functionality.",
      "workaround": "Use raw gRPC call via `await client.workflow_service.reset_workflow_execution(...)` for advanced users.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We intentionally don't expose workflow reset as high-level in most SDKs, but you can access the raw gRPC call for workflow reset via `workflow_service`.",
      "number": 601,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:44.230Z"
    },
    {
      "summary": "Add a workflow initialization method that runs before signals and updates are processed, allowing handlers to use guaranteed initialized values.",
      "category": "feature",
      "subcategory": "workflow-initialization",
      "apis": [],
      "components": [
        "workflow-runtime",
        "signal-handler",
        "update-handler"
      ],
      "concepts": [
        "initialization",
        "signal-handling",
        "update-handling",
        "lifecycle",
        "handler-ordering",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably initialize workflow state before receiving signals and updates, requiring defensive initialization patterns in handlers.",
      "rootCause": null,
      "proposedFix": "Implement a workflow init method that executes before any signals or updates are processed.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Workflow init support was implemented to allow guaranteed initialization before signal/update handlers execute.",
      "related": [],
      "keyQuote": "We should have a method that's guaranteed to initialize before signals and updates come in so handlers can use initialized values.",
      "number": 600,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:31.576Z"
    },
    {
      "summary": "pip install fails to build wheels for temporalio on Windows, showing compilation errors despite having Rust compiler installed. The user receives build failure messages when attempting to install the package.",
      "category": "bug",
      "subcategory": "installation",
      "apis": [],
      "components": [
        "build-system",
        "wheel-packaging",
        "pip-installer"
      ],
      "concepts": [
        "installation",
        "wheels",
        "compilation",
        "platform-compatibility",
        "windows"
      ],
      "severity": "high",
      "userImpact": "Users on Windows cannot install the temporalio package via pip, blocking them from using the SDK.",
      "rootCause": "pip is not using a prebuilt wheel, likely due to unsupported Windows architecture (ARM or 32-bit) or incompatible Python/platform tags.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "For some reason your `pip` install is not using a prebuilt wheel. Can you show the output of `pip debug --verbose`?",
      "number": 597,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:28.805Z"
    },
    {
      "summary": "Feature request to support schedule search attribute updates in the Python SDK, aligning with implementations in other SDKs like Go.",
      "category": "feature",
      "subcategory": "schedule-search-attributes",
      "apis": [],
      "components": [
        "schedule",
        "search-attributes"
      ],
      "concepts": [
        "scheduling",
        "search-attributes",
        "workflow-management",
        "feature-parity"
      ],
      "severity": "medium",
      "userImpact": "Users cannot update search attributes for scheduled workflows in Python SDK, limiting workflow management capabilities.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented to support schedule search attribute updates in Python SDK",
      "related": [
        512,
        1561
      ],
      "keyQuote": "Support schedule search attribute update",
      "number": 594,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:29.908Z"
    },
    {
      "summary": "Activity logger context is not populated when `activity_info_on_message` is False, even when `activity_info_on_extra` is True. The bug occurs because the code accesses `_logger_details` instead of `logger_details`, preventing lazy population of logger context.",
      "category": "bug",
      "subcategory": "activity-logging",
      "apis": [],
      "components": [
        "activity",
        "logger",
        "context"
      ],
      "concepts": [
        "logging",
        "context-propagation",
        "configuration",
        "lazy-initialization"
      ],
      "severity": "medium",
      "userImpact": "Users cannot include activity context information in log extra fields without also including it in the message string.",
      "rootCause": "Code accesses `_logger_details` private attribute directly instead of using `logger_details` property which performs lazy population",
      "proposedFix": "Change line 462 in temporalio/activity.py to access `logger_details` instead of `_logger_details`",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by accessing the logger_details property instead of the private _logger_details attribute, allowing lazy population regardless of activity_info_on_message setting",
      "related": [
        593
      ],
      "keyQuote": "We need to change to access `logger_details` instead of `_logger_details` since the former will lazily populate it",
      "number": 592,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:16.925Z"
    },
    {
      "summary": "Update the default schedule retention period in the Python SDK from its current value to 1 year to match the server-side default, with consideration for making it a server-side default instead of client-side.",
      "category": "feature",
      "subcategory": "schedule-configuration",
      "apis": [
        "Schedule"
      ],
      "components": [
        "client",
        "schedule",
        "configuration"
      ],
      "concepts": [
        "default-values",
        "retention-period",
        "server-defaults",
        "schedule-management",
        "api-compatibility"
      ],
      "severity": "low",
      "userImpact": "Users will have the correct default schedule retention period that aligns with server expectations, reducing configuration surprises.",
      "rootCause": "API PR #308 changed server-side default to 1 year, but Python SDK client-side default was not updated to match.",
      "proposedFix": "Change the default value in temporalio/client.py line 3579 to 1 year to match server default.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Default value was updated to match server-side default of 1 year.",
      "related": [
        308
      ],
      "keyQuote": "For now it was decided to just change this default value to match server default",
      "number": 590,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:15.255Z"
    },
    {
      "summary": "Feature request to allow workflow and activity parameters (timeouts, retry policies, task queues, etc.) to be specified at the definition/declaration site rather than requiring specification at every call site. The SDK would use call-site values first and fall back to declaration defaults.",
      "category": "feature",
      "subcategory": "activity-configuration",
      "apis": [
        "start_activity",
        "execute_activity"
      ],
      "components": [
        "activity-decorator",
        "activity-executor",
        "workflow-executor",
        "parameter-handling"
      ],
      "concepts": [
        "default-values",
        "configuration-inheritance",
        "retry-policy",
        "timeout-management",
        "activity-definition"
      ],
      "severity": "low",
      "userImpact": "Users currently must specify activity/workflow parameters at every call site, leading to code duplication and copy-pasting of configuration values.",
      "rootCause": "SDK architecture lacks support for worker-side default options that would allow the Temporal platform to enforce definition-site parameters, and the server has no mechanism to be aware of activities ahead of time.",
      "proposedFix": "Allow parameters like task_queue, start_to_close_timeout, heartbeat_timeout, and retry_policy to be specified in @activity.defn() decorator, with call-site values taking precedence over declaration defaults.",
      "workaround": "Users can create wrapper functions (e.g., 'start_my_activity') that encapsulate reusable parameters, or manage options in their own code layers.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers determined that without platform-level support for worker-side options, implementing this would create a false sense of what the options actually are, since callers can override them. Platform support for this feature is planned for future versions.",
      "related": [
        6339
      ],
      "keyQuote": "The SDK is unable to make these general assumptions about which definition site is the actual one desired because the call site may be a completely different set of code",
      "number": 588,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:17.261Z"
    },
    {
      "summary": "Feature request to add `id_conflict_policy` argument to `execute_child_workflow()` for Python SDK, similar to the parameter recently added for other workflow start methods. The feature would reduce boilerplate try/except error handling when starting child workflows.",
      "category": "feature",
      "subcategory": "child-workflows",
      "apis": [
        "execute_child_workflow",
        "start_child_workflow",
        "get_external_workflow_handle"
      ],
      "components": [
        "child-workflow-executor",
        "workflow-api",
        "error-handling"
      ],
      "concepts": [
        "id-conflict-policy",
        "child-workflows",
        "error-handling",
        "conflict-resolution",
        "api-consistency"
      ],
      "severity": "low",
      "userImpact": "Users must write manual try/except blocks with WorkflowAlreadyStartedError to handle ID conflicts in child workflows, adding complexity that could be simplified with the policy parameter.",
      "rootCause": "The `id_conflict_policy` parameter was added to the Temporal API but not yet exposed in the Python SDK's child workflow methods.",
      "proposedFix": "Add `id_conflict_policy` parameter to `execute_child_workflow()` and `start_child_workflow()` methods to match API capabilities.",
      "workaround": "Manually catch `WorkflowAlreadyStartedError` and use `get_external_workflow_handle()` to retrieve existing child workflow results, or store child handles on the workflow class.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers decided against adding this parameter, recommending explicit try/except handling as it's clearer for code readability and workflow history semantics.",
      "related": [
        359
      ],
      "keyQuote": "We recommend using `WorkflowAlreadyStartedError` and consider it a healthy use of try/except. You can wrap it in a helper if needed.",
      "number": 587,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:14:01.526Z"
    },
    {
      "summary": "User reported that `workflow.start_child_workflow()` throws `InvalidStateError` or blocks indefinitely in tests, while `workflow.execute_child_workflow()` works fine. The issue was resolved by clarifying that `ChildWorkflowHandle` is an `asyncio.Task` and should be awaited directly rather than calling `.result()`.",
      "category": "docs",
      "subcategory": "child-workflows",
      "apis": [
        "start_child_workflow",
        "execute_child_workflow",
        "ChildWorkflowHandle"
      ],
      "components": [
        "child-workflow-execution",
        "testing-framework",
        "workflow-handle"
      ],
      "concepts": [
        "async-task",
        "workflow-testing",
        "api-documentation",
        "child-workflows",
        "asyncio-integration"
      ],
      "severity": "low",
      "userImpact": "Users are confused about the correct API usage for `ChildWorkflowHandle` in tests, leading to runtime errors and incorrect code patterns.",
      "rootCause": "Insufficient documentation about the difference between `ChildWorkflowHandle` (which is an `asyncio.Task` to be awaited directly) and client workflow handles (which have a separate `.result()` method).",
      "proposedFix": "Update API documentation (docstrings) and other documentation sources to clarify that `ChildWorkflowHandle` extends `asyncio.Task` and should be awaited directly, not calling `.result()`.",
      "workaround": "Await the `ChildWorkflowHandle` directly (e.g., `await child`) instead of calling `await child.result()`.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Starting a child workflow returns a `ChildWorkflowHandle` which is an `asyncio.Task` that is already waiting on result and should be awaited itself to get the result",
      "number": 586,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:59.438Z"
    },
    {
      "summary": "Intermittent test failure on Python 3.8 during workflow import validation, suspected to be related to a known Python reimporting issue with sandbox imports.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "workflow-sandbox",
        "importer",
        "test-framework"
      ],
      "concepts": [
        "import-validation",
        "sandboxing",
        "workflow-preparation",
        "python-3.8",
        "flaky-tests"
      ],
      "severity": "medium",
      "userImpact": "Users experience intermittent test failures when running workflow validation tests on Python 3.8, reducing test reliability.",
      "rootCause": "Suspected to be related to cpython issue #91351 regarding reimporting in older Python versions, triggered during pytest assertion rewrite in sandbox context.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        91351
      ],
      "keyQuote": "I suspect this is the previously-known issue in older Python with reimporting",
      "number": 585,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:56.230Z"
    },
    {
      "summary": "PR #556 was incomplete - it failed to address dynamic handlers and didn't include tests verifying that warnings are emitted when continue-as-new is used. This issue tracks the need to complete this work.",
      "category": "bug",
      "subcategory": "handlers",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "handlers",
        "dynamic-handlers",
        "workflow-execution"
      ],
      "concepts": [
        "continue-as-new",
        "warning-emission",
        "handler-lifecycle",
        "testing",
        "incomplete-implementation"
      ],
      "severity": "medium",
      "userImpact": "Dynamic handlers may not work correctly with continue-as-new, and warnings about this behavior may not be properly emitted to users.",
      "rootCause": "PR #556 was incomplete and left dynamic handlers and warning tests unimplemented.",
      "proposedFix": "Complete PR #556 by adding support for dynamic handlers with continue-as-new and adding tests to verify warning emission.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The referenced PR #556 was completed to add dynamic handler support and continue-as-new warning tests.",
      "related": [
        556
      ],
      "keyQuote": "PR #556 omitted to address dynamic handlers and test that warnings are emitted on continue-as-new",
      "number": 584,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:43.569Z"
    },
    {
      "summary": "Workflow cancellation currently results in a generic 'workflow execution failed' error message. The author initially suggested changing it to 'workflow execution cancelled' but later clarified this isn't appropriate since the cancellation exception is contained as the cause of the outer exception.",
      "category": "other",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "workflow-execution",
        "error-handling"
      ],
      "concepts": [
        "cancellation",
        "error-messages",
        "exception-hierarchy",
        "cause-chain"
      ],
      "severity": "low",
      "userImpact": "Users receive a misleading error message when workflows are cancelled, making it harder to distinguish cancellation from actual failures.",
      "rootCause": "The outer WorkflowFailureError uses a generic message that doesn't distinguish between cancellation and actual failures.",
      "proposedFix": "Change the error message to 'workflow execution cancelled' instead of 'workflow execution failed'.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "The author determined the suggestion was inappropriate because the outer exception message should match its class name, with the cancellation exception properly contained as the cause.",
      "related": [],
      "keyQuote": "it makes sense that the outer message has a natural language string matching its class name; the cancellation exception is contained within the outer exception as the `__cause__`",
      "number": 583,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:42.050Z"
    },
    {
      "summary": "Activity returning dict instead of dataclass when called via string name. User was forced to manually convert the returned dict to the expected dataclass type. Issue was resolved by specifying the result_type parameter or using execute_activity_method.",
      "category": "bug",
      "subcategory": "activity-deserialization",
      "apis": [
        "execute_activity",
        "execute_activity_method"
      ],
      "components": [
        "activity-executor",
        "serialization",
        "type-hints"
      ],
      "concepts": [
        "type-deserialization",
        "return-type-inference",
        "dataclass-conversion",
        "json-defaults"
      ],
      "severity": "medium",
      "userImpact": "Users calling activities by string name receive dict objects instead of properly typed dataclass instances, requiring manual type conversion.",
      "rootCause": "When activities are called via string name instead of direct method reference, the SDK cannot extract the return type from the function signature and defaults to JSON deserialization (dict) instead of the intended dataclass type.",
      "proposedFix": "Specify result_type parameter when calling execute_activity with string name, or use execute_activity_method with direct method reference to allow type extraction from signature.",
      "workaround": "Manually check if returned object is a dataclass instance, and if not, create a new instance initialized with the dict contents.",
      "resolution": "fixed",
      "resolutionDetails": "User resolved by adding result_type parameter to execute_activity call, allowing proper deserialization to PostAbstractionDTO.",
      "related": [],
      "keyQuote": "If you call the activity via a string name, the code does not know the type to deserialize to, so it defaults to whatever JSON does (which is dict in this case).",
      "number": 578,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:41.374Z"
    },
    {
      "summary": "The cibuildwheel configuration is using an outdated manylinux2014 image (from March 2024) with an expired CentOS 7 repository mirror, causing build failures. Updating cibuildwheel to a newer version with updated repositories should resolve this.",
      "category": "bug",
      "subcategory": "ci-build-wheels",
      "apis": [],
      "components": [
        "ci-build-wheels",
        "cibuildwheel",
        "manylinux-images"
      ],
      "concepts": [
        "build-system",
        "docker-images",
        "repository-mirrors",
        "dependency-management"
      ],
      "severity": "high",
      "userImpact": "Users cannot build Python SDK wheel binaries due to expired repository mirrors in the CI environment.",
      "rootCause": "The manylinux2014 image pinned in cibuildwheel configuration uses outdated CentOS 7 repository mirrors that are no longer accessible.",
      "proposedFix": "Update cibuildwheel to use a newer version with current manylinux images and repository mirrors.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "cibuildwheel was updated to use current manylinux images with valid repository mirrors",
      "related": [],
      "keyQuote": "Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=container error was 14: curl#6 - \"Could not resolve host\"",
      "number": 577,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:26.882Z"
    },
    {
      "summary": "User requests better support for testing workflows that execute activities across multiple task queues. Current approach requires subclassing workflows in tests to override task queue names, and there's no straightforward way to run multiple workers in test scenarios.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [
        "Worker"
      ],
      "components": [
        "testing",
        "worker",
        "task-queue",
        "activity-executor"
      ],
      "concepts": [
        "multi-worker-testing",
        "task-queue-routing",
        "workflow-testing",
        "test-isolation",
        "activity-registration",
        "async-context-manager"
      ],
      "severity": "medium",
      "userImpact": "Users testing workflows with multi-queue activity routing must use workarounds like subclassing workflows or running workers in separate threads instead of using standard nested context managers.",
      "rootCause": "The SDK's async context manager design doesn't support nested Worker instances, and the testing environment doesn't provide mechanisms to override task queue routing for simplified multi-worker scenarios.",
      "proposedFix": "Suggested solutions include: adding WorkflowEnvironment.with_overridden_task_queue() to route all workflows to a single queue in tests, supporting nested Worker() context managers, or providing a WorkerGroup helper for managing multiple workers.",
      "workaround": "Use workflow input parameters to drive task queue names instead of subclassing, or run additional workers in separate threads rather than nested contexts.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer indicated running multiple workers in tests is the recommended approach and works the same as production. The limitation is not a bug but a design choice; users can implement their own WorkerGroup helper if needed.",
      "related": [],
      "keyQuote": "Running multiple workers is no different in tests or production. The library code is intentionally the same.",
      "number": 576,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:27.302Z"
    },
    {
      "summary": "Workflow cancellation does not cancel in-progress signal/update handler executions. Clients receive a generic 'workflow execution already completed' exception instead of a cancellation-specific error, making it unclear that the update was canceled due to workflow cancellation.",
      "category": "feature",
      "subcategory": "update-handlers",
      "apis": [
        "execute_update"
      ],
      "components": [
        "update-handler",
        "workflow-cancellation",
        "signal-handler"
      ],
      "concepts": [
        "cancellation",
        "handler-execution",
        "exception-handling",
        "workflow-lifecycle",
        "client-notification"
      ],
      "severity": "medium",
      "userImpact": "Clients cannot distinguish between update failures due to workflow completion versus workflow cancellation, making it harder to implement proper error handling and retry logic.",
      "rootCause": "Workflow cancellation logic does not propagate cancellation signals to in-progress handler executions, allowing handlers to continue or fail with generic completion errors.",
      "proposedFix": "Update handler executions should be canceled when the workflow is canceled, with clients receiving a cancellation-specific exception. For signal handlers, use an SDK workflow logic flag to maintain backwards compatibility.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation added cancellation handling for update handlers with cancellation-specific exceptions, and a workflow logic flag for signal handler cancellation compatibility.",
      "related": [],
      "keyQuote": "the client waiting on the update gets an exception, but the exception is `workflow execution already completed`. This ticket calls for that exception to inform the client that the update was canceled due to cancellation of the workflow.",
      "number": 575,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:24.076Z"
    },
    {
      "summary": "Request to add query support when listing schedules in the Python SDK, similar to the existing query functionality available for listing workflows.",
      "category": "feature",
      "subcategory": "schedule-management",
      "apis": [
        "ListSchedules"
      ],
      "components": [
        "schedule-client",
        "query-filter",
        "list-operations"
      ],
      "concepts": [
        "query-filtering",
        "schedule-listing",
        "api-parity",
        "workflow-schedules"
      ],
      "severity": "low",
      "userImpact": "Users cannot filter schedules when listing them, reducing the ability to manage and query large numbers of schedules programmatically.",
      "rootCause": null,
      "proposedFix": "Add query parameter support to the schedule listing API to match workflow listing functionality.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Need to support the query option when listing schedules same as we do when listing workflows.",
      "number": 573,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:03.027Z"
    },
    {
      "summary": "Upserting a datetime search attribute fails because the Python SDK doesn't format the datetime with a timezone. The server requires timezone information in the ISO 8601 format (with 'Z' suffix for UTC), but the SDK was sending the datetime without it, causing an InvalidArgument error.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [
        "upsert_search_attributes",
        "SearchAttributeKey.for_datetime"
      ],
      "components": [
        "search-attributes",
        "datetime-serialization",
        "workflow-sdk"
      ],
      "concepts": [
        "search-attributes",
        "datetime",
        "timezone",
        "serialization",
        "workflow-state"
      ],
      "severity": "high",
      "userImpact": "Users cannot upsert datetime search attributes without encountering an InvalidArgument error from the server.",
      "rootCause": "The Python SDK serializes datetime objects to ISO 8601 format without timezone information, but the Temporal server requires a timezone (typically 'Z' for UTC) to properly store and validate datetime search attributes.",
      "proposedFix": "The SDK should automatically append 'Z' to the ISO 8601 datetime string when serializing datetime search attributes, or require users to provide timezone-aware datetime objects.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was addressed by improving datetime serialization in the Python SDK to include timezone information and through a companion issue (#611) to add better error messages.",
      "related": [
        611
      ],
      "keyQuote": "the SDK requires you to specify a timezone -- our server needs to store user timestamps with a timezone",
      "number": 572,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:08.393Z"
    },
    {
      "summary": "Test suite contains multiple flaky tests that fail intermittently across different environments and test runs, including timeouts, pickling errors, and process failures.",
      "category": "bug",
      "subcategory": "test-reliability",
      "apis": [],
      "components": [
        "test-worker",
        "test-activity",
        "test-workflow",
        "process-management"
      ],
      "concepts": [
        "test-flakiness",
        "timeout",
        "serialization",
        "pickling",
        "environment-specific-failures"
      ],
      "severity": "high",
      "userImpact": "Unreliable test suite undermines confidence in SDK stability and makes development harder for contributors.",
      "rootCause": "Multiple underlying issues identified: pickling errors with lambda functions in heartbeat details, timeout issues in workflow handler tests, and process completion failures in CI.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed after collecting examples of flaky tests and implementing fixes.",
      "related": [
        609,
        739,
        566,
        567
      ],
      "keyQuote": "The test suite currently has multiple tests that sometimes fail. Let's collect examples in this ticket, and fix them.",
      "number": 568,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:13:05.640Z"
    },
    {
      "summary": "Feature request to replace BuildJet ARM runner with GitHub's native ARM runners in the Python SDK CI/CD pipeline, now that GitHub has made ARM runners available.",
      "category": "feature",
      "subcategory": "ci-cd",
      "apis": [],
      "components": [
        "ci-pipeline",
        "runner-infrastructure"
      ],
      "concepts": [
        "arm-architecture",
        "github-actions",
        "ci-cd",
        "infrastructure",
        "testing-environment"
      ],
      "severity": "low",
      "userImpact": "Users benefit from more reliable and cost-effective CI/CD infrastructure for the Python SDK.",
      "rootCause": null,
      "proposedFix": "Use GitHub's native ARM runners instead of BuildJet ARM runners.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the migration to GitHub ARM runners was completed.",
      "related": [],
      "keyQuote": "Now that GH has ARM runners, might as well",
      "number": 565,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:48.030Z"
    },
    {
      "summary": "Activity command creation occurs before parameter serialization, causing half-complete commands to be sent to Core if serialization fails and users treat the error as an application error rather than a task failure.",
      "category": "bug",
      "subcategory": "activity-scheduling",
      "apis": [
        "ScheduleActivity"
      ],
      "components": [
        "activity-executor",
        "command-builder",
        "serialization"
      ],
      "concepts": [
        "command-creation",
        "serialization",
        "error-handling",
        "parameter-validation",
        "protocol-state"
      ],
      "severity": "high",
      "userImpact": "Users who convert serialization errors to application errors may send incomplete command state to Core, causing unpredictable failures.",
      "rootCause": "Commands are created before attempting to serialize parameters; if serialization fails after command creation, the incomplete command remains in the activations list.",
      "proposedFix": "Either refactor to create commands only after all serialization succeeds, or ensure all potentially-failing operations complete before command creation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [
        540
      ],
      "keyQuote": "if it fails, the command is still in the activations commands list but incomplete",
      "number": 564,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:50.391Z"
    },
    {
      "summary": "Request to improve test workflow coverage by implementing automatic replay functionality and cache-less test runs to catch more issues during testing.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "testing",
        "workflow-replay",
        "test-cache"
      ],
      "concepts": [
        "replay",
        "test-coverage",
        "cache",
        "workflow-testing",
        "determinism",
        "issue-detection"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot thoroughly test workflows for replay compatibility and non-deterministic issues, potentially missing bugs in production.",
      "rootCause": null,
      "proposedFix": "Auto-replay workflows that complete in tests and provide option to run tests without cache to improve coverage.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We need to replay our test workflows more. Today we don't replay nor do we run without cache, both of which could catch issues.",
      "number": 563,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:51.338Z"
    },
    {
      "summary": "User reported that workflow task timeout is capped at 2 minutes and cannot be set higher, even though they tried to set it to 15 minutes. The maintainer clarified this is a server-enforced maximum timeout by design, not a Python SDK limitation.",
      "category": "question",
      "subcategory": "workflow-timeout",
      "apis": [
        "execute_workflow"
      ],
      "components": [
        "workflow-execution",
        "timeout-configuration"
      ],
      "concepts": [
        "task-timeout",
        "timeout-limit",
        "server-enforcement",
        "workflow-task",
        "configuration"
      ],
      "severity": "low",
      "userImpact": "Users may be confused about workflow task timeout limits and attempt to set values that the server will reject.",
      "rootCause": "Server-enforced maximum timeout for workflow tasks, as documented in Temporal architecture.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer clarified this is intentional server-side behavior. Recommended that users reconsider if they need to modify this value, as workflow tasks should complete in milliseconds.",
      "related": [],
      "keyQuote": "This is the maximum timeout of this value as enforced on the server. It is unrelated to Python... Workflow tasks should only take milliseconds.",
      "number": 561,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:34.913Z"
    },
    {
      "summary": "Rust dependency updates from PR #520 need to be released to pip to address security vulnerabilities. Users currently receive an outdated version of the Python SDK on pip that still contains the vulnerable dependencies.",
      "category": "feature",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "dependency-management",
        "rust-bindings",
        "pip-distribution"
      ],
      "concepts": [
        "security-vulnerability",
        "dependency-updates",
        "release-cycle",
        "distribution"
      ],
      "severity": "high",
      "userImpact": "Users installing the SDK from pip receive a version with known security vulnerabilities that have already been fixed in the codebase.",
      "rootCause": "Rust dependency updates from PR #520 have not yet been included in a release published to pip.",
      "proposedFix": "Include the Rust dependency updates from #520 in the next release to pip.",
      "workaround": "Build and install the SDK from source to get the latest Rust dependencies.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by releasing a new version of the SDK to pip with the updated Rust dependencies included.",
      "related": [
        520
      ],
      "keyQuote": "The changes in #520 have not yet propagated to `pip`, leading to the vulnerabilities previously identified to still exist in the latest version",
      "number": 560,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:36.630Z"
    },
    {
      "summary": "Add support for accepting search attributes in dev server startup configuration for the Python SDK. Users should be able to set search attributes through dev server options that translate to CLI --search-attribute calls.",
      "category": "feature",
      "subcategory": "dev-server",
      "apis": [],
      "components": [
        "dev-server",
        "configuration",
        "cli-integration"
      ],
      "concepts": [
        "search-attributes",
        "dev-server-setup",
        "configuration",
        "cli-integration"
      ],
      "severity": "low",
      "userImpact": "Users will be able to configure search attributes when starting the dev server, eliminating the need for manual CLI argument management.",
      "rootCause": null,
      "proposedFix": "Implement search attribute option in dev server startup that translates to --search-attribute CLI calls",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented following the deployment of CLI PR #593 and related features issue #494",
      "related": [
        494
      ],
      "keyQuote": "users should be allowed to set search attributes in dev server option that translate to `--search-attribute` calls",
      "number": 558,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:37.144Z"
    },
    {
      "summary": "Document and add tests confirming that asyncio.Lock and asyncio.Semaphore can be used safely in Python workflows to control interleaving of signal and update handlers with the main workflow coroutine.",
      "category": "docs",
      "subcategory": "concurrency-control",
      "apis": [],
      "components": [
        "asyncio",
        "signal-handlers",
        "update-handlers",
        "workflow-coroutine"
      ],
      "concepts": [
        "concurrency-control",
        "asyncio-lock",
        "asyncio-semaphore",
        "signal-handling",
        "update-handling",
        "interleaving",
        "synchronization"
      ],
      "severity": "medium",
      "userImpact": "Users need clear documentation and tested examples to safely use asyncio primitives for controlling concurrency in workflow code.",
      "rootCause": null,
      "proposedFix": "Add documentation and tests demonstrating safe usage of asyncio.Lock and asyncio.Semaphore in workflow code.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation and tests were added via samples-python PR #123 and documentation PR #2959.",
      "related": [],
      "keyQuote": "using `asyncio.Lock` in `sdk-python` workflow code is appropriate and recommended",
      "number": 554,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:22.945Z"
    },
    {
      "summary": "This is a placeholder testing issue with minimal content containing only the word 'testing'.",
      "category": "other",
      "subcategory": "testing",
      "apis": [],
      "components": [],
      "concepts": [
        "testing",
        "validation"
      ],
      "severity": "low",
      "userImpact": "No clear impact - this appears to be a test issue with insufficient information.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Closed as invalid due to insufficient content and context.",
      "related": [],
      "keyQuote": "testing",
      "number": 553,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:18.676Z"
    },
    {
      "number": 552,
      "repository": "temporalio-sdk-python",
      "summary": "Minimal issue with title 'Testing' and single word body 'testing'. Appears to be a placeholder or test issue with no substantive content describing a problem or request.",
      "category": "other",
      "subcategory": "placeholder",
      "apis": [],
      "components": [],
      "concepts": [
        "testing",
        "placeholder"
      ],
      "severity": "low",
      "userImpact": "No clear impact; insufficient information to determine relevance to users.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Closed as invalid - insufficient content to be actionable",
      "related": [],
      "keyQuote": "testing",
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:20.644Z"
    },
    {
      "summary": "Docker-based proto generation produces different whitespace in multi-paragraph documentation compared to manual generation outside the container, causing CI diff checks to fail.",
      "category": "bug",
      "subcategory": "proto-generation",
      "apis": [],
      "components": [
        "proto-generation",
        "docker-build",
        "ci-pipeline"
      ],
      "concepts": [
        "whitespace-handling",
        "proto-docs",
        "build-consistency",
        "ci-validation"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot merge changes due to CI failures caused by whitespace inconsistencies in generated proto documentation.",
      "rootCause": "Docker-based proto generation tool produces different whitespace formatting than manual generation, likely due to environment differences or tool version variations.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When using the docker-based gen proto method, for whatever reason it is slightly different than the manual way outside of the container which causes CI diff check to break",
      "number": 543,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:06.611Z"
    },
    {
      "summary": "Request to expose UpdateID in update handlers to allow users to generate unique idempotent primary keys for their update handler logic.",
      "category": "feature",
      "subcategory": "update-handler",
      "apis": [
        "UpdateID"
      ],
      "components": [
        "update-handler",
        "sdk-core"
      ],
      "concepts": [
        "idempotency",
        "update-handler",
        "unique-identifier",
        "primary-key"
      ],
      "severity": "low",
      "userImpact": "Users cannot currently access UpdateID within update handlers, limiting their ability to implement idempotent request handling.",
      "rootCause": null,
      "proposedFix": "Expose UpdateID as an accessible field within update handler context",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "UpdateID was exposed in the update handler implementation",
      "related": [],
      "keyQuote": "This will allow users to use it when they want a unique idempotent primary key for handlers",
      "number": 542,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:07.996Z"
    },
    {
      "summary": "When an exception occurs during data converter encoding in activity parameters, it doesn't propagate properly as a workflow failure. Instead, an incomplete activity command is sent to the server with a missing task queue name, causing a warning and allowing the workflow to timeout instead of failing immediately.",
      "category": "bug",
      "subcategory": "data-serialization",
      "apis": [
        "execute_activity",
        "start_workflow"
      ],
      "components": [
        "data-converter",
        "workflow-instance",
        "activity-scheduler"
      ],
      "concepts": [
        "serialization",
        "exception-handling",
        "workflow-failure",
        "payload-encoding",
        "error-propagation"
      ],
      "severity": "high",
      "userImpact": "Users cannot properly catch and handle serialization exceptions during activity scheduling, leading to confusing timeouts instead of explicit workflow failures.",
      "rootCause": "The command object is created before serializing activity parameters, so if serialization fails, an incomplete command with missing task queue is sent to the server.",
      "proposedFix": "Move payload serialization to the end of _apply_schedule_command before returning the command, so exceptions during serialization prevent command creation entirely.",
      "workaround": "Customize the converter or use failure_exception_types=[Exception] in @workflow.defn to handle exceptions differently.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was tracked separately as #564 for clearer problem statement and resolution.",
      "related": [
        564
      ],
      "keyQuote": "The issue is that we create the command before we try to serialize the contents. So if this does not fail the workflow task, the command is sent off incomplete.",
      "number": 540,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:12:06.053Z"
    },
    {
      "summary": "Users need methods to wait for all remaining handlers to complete when continuing as new or exiting a workflow. The SDK should provide explicit control over handler completion behavior.",
      "category": "feature",
      "subcategory": "workflow-handlers",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "workflow-runtime",
        "handler-management"
      ],
      "concepts": [
        "handler-lifecycle",
        "workflow-exit",
        "async-completion",
        "continue-as-new"
      ],
      "severity": "medium",
      "userImpact": "Users lack a standard mechanism to ensure all handlers complete before workflow continuation or exit, requiring workarounds.",
      "rootCause": null,
      "proposedFix": "Provide explicit methods for users to await or ignore all remaining handlers during workflow continuation or exit.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Handlers are now automatically awaited during workflow continuation.",
      "related": [],
      "keyQuote": "Users want to easily await (or ignore) all their remaining handlers when they continue as new or exit the workflow.",
      "number": 539,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:48.943Z"
    },
    {
      "summary": "Add warning or error when update handlers are left dangling across Continues-as-New (CAN) or workflow exit to prevent resource leaks and unexpected behavior. The issue recommends providing guidance on await-all-handlers or drop-all-handlers methods.",
      "category": "feature",
      "subcategory": "update-handlers",
      "apis": [],
      "components": [
        "update-handlers",
        "workflow-execution",
        "continues-as-new"
      ],
      "concepts": [
        "resource-cleanup",
        "handler-lifecycle",
        "workflow-exit",
        "dangling-handlers",
        "error-handling",
        "user-guidance"
      ],
      "severity": "medium",
      "userImpact": "Users may inadvertently leave update handlers running across workflow boundaries, leading to unexpected behavior or resource leaks without clear warning.",
      "rootCause": null,
      "proposedFix": "Implement warning or error when update handlers dangle across CAN or workflow exit; recommend await-all-handlers or drop-all-handlers methods to users.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We can recommend the await-all-handlers or drop-all-handlers methods. TBD whether error or warning.",
      "number": 538,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:50.954Z"
    },
    {
      "summary": "Improve error messages when workflow or activity return type decoding fails. Currently shows generic 'Failed decoding arguments' message; should specify what failed to decode (return value vs encoding) and which child workflow or activity was affected.",
      "category": "other",
      "subcategory": "error-messages",
      "apis": [],
      "components": [
        "child-workflow-executor",
        "activity-executor",
        "return-type-decoder"
      ],
      "concepts": [
        "error-handling",
        "type-validation",
        "debugging",
        "return-values",
        "encoding-decoding",
        "error-clarity"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily identify which child workflow or activity failed and whether the issue is with decoding the return value or encoding it, making debugging difficult.",
      "rootCause": null,
      "proposedFix": "Use more specific error messages: 'Failed to decode return value on <child workflow>' for workflows and similar messages for activities, distinguishing between decode and encode failures.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It should say 'Failed to decode return value on <child workflow>'. Same for activities (and make sure it's clear what child/activity failed to _encode_ if needed too).",
      "number": 536,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:52.329Z"
    },
    {
      "summary": "Convert non-deterministic asyncio call warnings into hard errors for asyncio.as_completed() and asyncio.wait() invoked in workflows, following a deprecation period after issue #533.",
      "category": "feature",
      "subcategory": "workflow-validation",
      "apis": [
        "asyncio.as_completed",
        "asyncio.wait"
      ],
      "components": [
        "workflow-runtime",
        "validation",
        "error-handling"
      ],
      "concepts": [
        "non-determinism",
        "asyncio",
        "workflow-safety",
        "deprecation",
        "validation",
        "error-raising"
      ],
      "severity": "medium",
      "userImpact": "Users will receive hard errors instead of warnings when using non-deterministic asyncio calls in workflows, preventing silent failures.",
      "rootCause": "asyncio.as_completed() and asyncio.wait() are non-deterministic and should not be used within workflow code.",
      "proposedFix": "After a deprecation period, convert the current warnings into errors similar to other invalid workflow calls.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        533
      ],
      "keyQuote": "After a release and a good period of time, we should fail when these are called the same way we do with other invalid workflow calls.",
      "number": 535,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:37.089Z"
    },
    {
      "summary": "Add validation on worker start/run using Core's validate function to ensure configuration is correct and provide early error feedback to users.",
      "category": "feature",
      "subcategory": "worker-validation",
      "apis": [],
      "components": [
        "worker",
        "core-integration"
      ],
      "concepts": [
        "validation",
        "configuration",
        "error-handling",
        "initialization",
        "core-sdk"
      ],
      "severity": "medium",
      "userImpact": "Users will receive immediate validation errors when starting a worker with invalid configuration instead of encountering issues later.",
      "rootCause": null,
      "proposedFix": "Call the validate function added in sdk-core PR #750 on worker start/run and propagate any validation errors to the user.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Validation call was implemented on worker initialization to catch configuration errors early.",
      "related": [
        750
      ],
      "keyQuote": "validate added in https://github.com/temporalio/sdk-core/pull/750, make sure to call it on worker start/run and if it errors, error to the user",
      "number": 532,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:33.424Z"
    },
    {
      "summary": "Add stack trace information to deadlock detection exceptions in the Python SDK. Currently, deadlock errors are raised without stack trace context, making debugging difficult.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "deadlock-detection",
        "error-reporting",
        "coroutine-tracking"
      ],
      "concepts": [
        "stack-trace",
        "debugging",
        "deadlock",
        "error-context",
        "coroutine-context",
        "diagnostics"
      ],
      "severity": "medium",
      "userImpact": "Users cannot debug deadlock issues effectively without stack trace information showing where the deadlock occurred.",
      "rootCause": null,
      "proposedFix": "Include stack trace information in deadlock detection exceptions, ideally per-task or per-coroutine context if possible.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Stack trace information was added to deadlock detection exceptions to improve debugging capabilities.",
      "related": [],
      "keyQuote": "We raise a deadlock detection error but we don't include any stack trace info. We should try to include this.",
      "number": 531,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:34.864Z"
    },
    {
      "summary": "Wrap GRPC CANCELED and DEADLINE_EXCEEDED exceptions in a SDK-specific Timeout exception for Update operations so users only need to catch one predictable exception type.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [
        "Update"
      ],
      "components": [
        "grpc-client",
        "exception-handling",
        "update-executor"
      ],
      "concepts": [
        "timeout",
        "exception-wrapping",
        "grpc-errors",
        "deadline",
        "error-handling",
        "user-experience"
      ],
      "severity": "medium",
      "userImpact": "Users must catch multiple exception types instead of a single unified SDK exception, making error handling more complex and error-prone.",
      "rootCause": "GRPC exceptions (CANCELED, DEADLINE_EXCEEDED) are exposed directly to users instead of being wrapped in SDK-specific exceptions.",
      "proposedFix": "Create a SDK Timeout exception wrapper that catches GRPC CANCELED and DEADLINE_EXCEEDED errors and re-raises them as the SDK exception type.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented as part of the features issue #483 referenced in the comments.",
      "related": [
        483
      ],
      "keyQuote": "So that users only have to catch one predictable exception",
      "number": 529,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:20.727Z"
    },
    {
      "summary": "Feature request to defer workflow completion until after all coroutines have settled in the task, ensuring proper cleanup and finalization of async operations before marking the workflow as complete.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "workflow-executor",
        "coroutine-manager",
        "task-completion"
      ],
      "concepts": [
        "async-cleanup",
        "coroutine-settlement",
        "workflow-lifecycle",
        "graceful-shutdown",
        "task-finalization"
      ],
      "severity": "medium",
      "userImpact": "Workflows may complete prematurely before all internal async operations are properly settled, potentially causing resource cleanup issues or incomplete processing.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation was completed to delay workflow completion until all coroutines have settled",
      "related": [
        481
      ],
      "keyQuote": "Do not set workflow completion until after all coroutines have settled in the task",
      "number": 528,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:16.567Z"
    },
    {
      "summary": "A non-halting workflow with concurrent wait_condition tasks causes a RuntimeError when the SDK tries to generate a stack trace, due to iterating over a set that changes size during iteration.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [
        "wait_condition"
      ],
      "components": [
        "workflow-instance",
        "workflow-sandbox",
        "stack-trace-generation"
      ],
      "concepts": [
        "concurrency",
        "set-modification",
        "workflow-state",
        "error-handling",
        "task-management"
      ],
      "severity": "high",
      "userImpact": "Workflows using concurrent wait_condition tasks can crash the SDK worker, preventing workflow execution.",
      "rootCause": "The _stack_trace() method iterates over self._tasks set without taking a snapshot, and the set can be modified by concurrent workflow execution during iteration, raising RuntimeError: Set changed size during iteration.",
      "proposedFix": "Take a copy of the set with list(self._tasks) before iterating in the _stack_trace() method at line 1774.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Set iteration issue fixed by creating snapshot of tasks before iteration.",
      "related": [],
      "keyQuote": "The fix may be to take a copy of this set with `list(self._tasks)`, but I haven't investigated yet",
      "number": 527,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:19.413Z"
    },
    {
      "summary": "A codec that returns the same payload object it was given on decode causes issues due to a bug in how the original payload is updated. This can lead to unexpected behavior when codecs attempt to reuse payload objects.",
      "category": "bug",
      "subcategory": "codec",
      "apis": [],
      "components": [
        "codec",
        "payload-handling"
      ],
      "concepts": [
        "payload-mutation",
        "object-reuse",
        "codec-decode",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Users implementing custom codecs may encounter unexpected behavior if their codec returns the same payload object instance that was passed to it.",
      "rootCause": "Bug in how the original payload is updated when a codec returns the same payload object it was given on decode",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If a codec returns the same payload object it was given on decode, it can cause issues due to a bug with how we update the original payload",
      "number": 525,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:59.575Z"
    },
    {
      "summary": "During workflow eviction, exceptions can be caught in finally/except blocks allowing side effects like logging when is_replaying should be True. Need to set is_replaying=True during eviction and throw WorkflowEvictingException to prevent user code from executing side effects.",
      "category": "bug",
      "subcategory": "workflow-eviction",
      "apis": [],
      "components": [
        "worker",
        "workflow_instance",
        "eviction"
      ],
      "concepts": [
        "is_replaying",
        "workflow-eviction",
        "exception-handling",
        "side-effects",
        "cache-eviction"
      ],
      "severity": "high",
      "userImpact": "Users experience log flooding and unexpected side effects during workflow eviction because is_replaying is not set to True and user exception handlers catch eviction exceptions.",
      "rootCause": "is_replaying flag is not set to True during eviction, and generic exceptions thrown during eviction can be caught by user code in finally/except blocks",
      "proposedFix": "Set is_replaying to True during eviction and create a new WorkflowEvictingException(BaseException) class to throw during eviction so user exception handlers cannot catch it",
      "workaround": "Users can ignore WorkflowEvictingException errors in their logs as they only occur during cache eviction",
      "resolution": "fixed",
      "resolutionDetails": "is_replaying set to True during eviction and WorkflowEvictingException implemented to prevent user code execution during eviction",
      "related": [],
      "keyQuote": "Ignoring add while deleting... logging should not occur during is_replaying",
      "number": 523,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:04.808Z"
    },
    {
      "summary": "Add a separate Cloud Operations API client for the Python SDK using cloud protos, marked as experimental. This feature requires sdk-core support and follows similar implementations in Go SDK.",
      "category": "feature",
      "subcategory": "cloud-client",
      "apis": [],
      "components": [
        "cloud-client",
        "proto-bindings"
      ],
      "concepts": [
        "cloud-operations",
        "experimental-api",
        "client-library",
        "cloud-integration"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to interact with Temporal Cloud operations programmatically through a dedicated Python client API.",
      "rootCause": null,
      "proposedFix": "Create a separate cloud client using cloud protos, following the pattern established in sdk-go#1426",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Cloud client was implemented as a separate module",
      "related": [
        440,
        1426,
        737
      ],
      "keyQuote": "We need a separate cloud client using the cloud protos (they are separate today). The client should be marked experimental.",
      "number": 522,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:11:01.604Z"
    },
    {
      "summary": "Update outdated Rust dependencies (rustls, mio, and h2) in the Python SDK to address security vulnerabilities flagged by security scans, even though the SDK is not directly affected.",
      "category": "feature",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "rust-dependencies",
        "build-system",
        "security-scanning"
      ],
      "concepts": [
        "security-vulnerability",
        "dependency-upgrade",
        "rustls",
        "mio",
        "h2",
        "supply-chain"
      ],
      "severity": "medium",
      "userImpact": "Users need updated dependencies to pass security scans and maintain compliance, even though there is no direct vulnerability impact.",
      "rootCause": "Outdated versions of rustls, mio, and h2 Rust dependencies are flagged by security scanning tools as having known vulnerabilities.",
      "proposedFix": "Update rustls, mio, h2, and any other required Rust dependencies to their latest secure versions.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Dependencies were updated in a subsequent Python SDK release.",
      "related": [],
      "keyQuote": "Even though we are unaffected by the vulnerabilities, security scans show our current rustls and mio versions as of this writing have vulnerabilities.",
      "number": 520,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:48.578Z"
    },
    {
      "summary": "Investigate non-deterministic behavior in asyncio.as_completed and asyncio.wait due to their use of set, either provide deterministic alternatives or document the risk and add warnings.",
      "category": "feature",
      "subcategory": "asyncio-determinism",
      "apis": [],
      "components": [
        "asyncio-integration",
        "task-coordination",
        "runtime"
      ],
      "concepts": [
        "determinism",
        "non-deterministic-behavior",
        "asyncio",
        "set-ordering",
        "task-scheduling",
        "concurrency"
      ],
      "severity": "high",
      "userImpact": "Users relying on asyncio.as_completed and asyncio.wait may experience non-deterministic workflow behavior that is difficult to debug and replay.",
      "rootCause": "asyncio.as_completed and asyncio.wait use Python's set data structure which has non-deterministic iteration order, potentially causing different task ordering across runs.",
      "proposedFix": "Provide deterministic alternatives to asyncio.as_completed and asyncio.wait, or at minimum warn users about the non-deterministic behavior.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed by implementing deterministic alternatives or adding warnings to the SDK documentation.",
      "related": [
        429
      ],
      "keyQuote": "asyncio.as_completed and asyncio.wait may be non-deterministic in their use of `set`. See if anything can be done and if not, document that they are dangerous and warn on their use.",
      "number": 518,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:47.849Z"
    },
    {
      "summary": "Feature request to make start_update users aware that it's synchronous with the worker. Related to broader SDK awareness issues documented in the features repository.",
      "category": "feature",
      "subcategory": "update-workflow",
      "apis": [
        "start_update"
      ],
      "components": [
        "update-handler",
        "worker",
        "api-documentation"
      ],
      "concepts": [
        "synchronous-execution",
        "worker-coordination",
        "user-awareness",
        "api-design"
      ],
      "severity": "low",
      "userImpact": "Users may be unaware that start_update is synchronous with the worker, potentially leading to misuse or unexpected behavior.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed through documentation or implementation changes to make the synchronous nature of start_update more apparent to users.",
      "related": [
        469
      ],
      "keyQuote": "Make start_update users aware that it's synchronous w/ worker",
      "number": 514,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:45.447Z"
    },
    {
      "summary": "Feature request to enable replacing the worker's client with a new one that may have different connection options, such as a new TLS certificate.",
      "category": "feature",
      "subcategory": "worker-configuration",
      "apis": [
        "Worker"
      ],
      "components": [
        "worker",
        "client",
        "connection"
      ],
      "concepts": [
        "client-replacement",
        "tls-certificate",
        "connection-options",
        "worker-lifecycle",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot update worker client credentials or connection settings without restarting the worker process.",
      "rootCause": null,
      "proposedFix": "Add capability to replace the worker client with a new client instance that may be configured with different options.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Need to be able to replace the worker client with a new client that may be connected with different options (e.g. a new TLS cert)",
      "number": 513,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:33.532Z"
    },
    {
      "summary": "User needs ability to reload short-lived client certificates without restarting the client. Solution implemented: users can replace the worker's client with a newly connected client containing updated TLS configuration.",
      "category": "feature",
      "subcategory": "tls-certificate-management",
      "apis": [
        "Worker"
      ],
      "components": [
        "client",
        "worker",
        "tls",
        "connection-pool"
      ],
      "concepts": [
        "certificate-rotation",
        "dynamic-configuration",
        "connection-management",
        "tls-lifecycle",
        "client-refresh"
      ],
      "severity": "medium",
      "userImpact": "Users with short-lived certificates can now update them without restarting their workers, improving operational flexibility in production environments.",
      "rootCause": "TLS configuration is set at client creation time with no mechanism to update certificates dynamically during runtime.",
      "proposedFix": "Allow users to replace the worker's client by setting the client property with a newly connected client that has refreshed TLS configuration.",
      "workaround": "Replace the worker's client property with a new client instance that has been connected with updated TLS configuration; the worker will use the new client for subsequent requests.",
      "resolution": "fixed",
      "resolutionDetails": "Implemented client replacement capability via worker.client property setter, allowing dynamic client updates without stopping the worker.",
      "related": [],
      "keyQuote": "You set the `client` property on the worker (which has a setter for `client` property) to replace the client with a newly connected client.",
      "number": 512,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:28.892Z"
    },
    {
      "summary": "Add a defaultConnection() method that dynamically switches between localhost and Temporal Cloud based on environment variables, enabling samples and documentation to easily demonstrate both local and cloud deployments.",
      "category": "feature",
      "subcategory": "connection-management",
      "apis": [],
      "components": [
        "client",
        "connection",
        "configuration"
      ],
      "concepts": [
        "connection-switching",
        "environment-configuration",
        "cloud-compatibility",
        "localhost-development",
        "deployment-flexibility"
      ],
      "severity": "low",
      "userImpact": "Users can easily run samples against both local and cloud Temporal servers by setting environment variables, reducing setup complexity for documentation and examples.",
      "rootCause": null,
      "proposedFix": "Expose a defaultConnection() method that reads environment variables to determine whether to connect to localhost or Temporal Cloud",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to provide environment-based connection switching for samples and demonstrations",
      "related": [
        454
      ],
      "keyQuote": "Expose a defaultConnection() method or similar that switches between localhost (default) and cloud based on some environment variable",
      "number": 509,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:29.478Z"
    },
    {
      "summary": "Request to add support for WorkflowIdConflictPolicy in the Python SDK, specifically for child workflows. This feature allows customization of how the system handles workflow ID conflicts when starting new workflows.",
      "category": "feature",
      "subcategory": "workflow-management",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-execution",
        "child-workflows"
      ],
      "concepts": [
        "workflow-id",
        "conflict-policy",
        "workflow-creation",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot control how the SDK handles workflow ID conflicts, limiting control over workflow creation behavior.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to support WorkflowIdConflictPolicy for child workflows in the Python SDK.",
      "related": [
        437
      ],
      "keyQuote": "Are there plans to support `WorkflowIdConflictPolicy` for child workflows?",
      "number": 504,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:13.586Z"
    },
    {
      "summary": "The logger mutates the `extra` dictionary passed to it without creating a copy first, which modifies the caller's original dictionary. This is a side effect that should be avoided.",
      "category": "bug",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "logger",
        "activity",
        "workflow"
      ],
      "concepts": [
        "mutation",
        "side-effects",
        "dictionary-handling",
        "data-integrity"
      ],
      "severity": "medium",
      "userImpact": "Users passing an `extra` dictionary to activity/workflow loggers will have their original dictionary unexpectedly modified, causing subtle bugs in their code.",
      "rootCause": "The logger implementation directly mutates the `extra` dictionary parameter instead of copying it first",
      "proposedFix": "Copy the `extra` dictionary before mutating it in the logger",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If there is an existing `extra` dictionary passed to the logger, copy it before mutating it",
      "number": 503,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:15.455Z"
    },
    {
      "summary": "Feature request to add extra context on task failure logs for workflows and activities in Python SDK, similar to what's provided in regular workflow/activity logger calls.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [
        "workflow.logger",
        "activity.logger"
      ],
      "components": [
        "logger",
        "workflow-executor",
        "activity-executor",
        "error-handling"
      ],
      "concepts": [
        "logging",
        "error-context",
        "task-failure",
        "debugging",
        "observability"
      ],
      "severity": "low",
      "userImpact": "Users have limited visibility into workflow and activity failures because extra context isn't provided in task failure logs.",
      "rootCause": null,
      "proposedFix": "Provide `extra` context on task failure logs with Temporal workflow/activity info, matching the behavior of regular workflow/activity logger calls.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "For workflow and activities, on task failure we need to provide `extra` with Temporal workflow/activity info like regular workflow/activity.logger calls do.",
      "number": 500,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:12.919Z"
    },
    {
      "summary": "Feature request to verify and support Protocol Buffers 5.x in the Python SDK. The issue was resolved by upgrading to protobuf 3.13, which only works with protobuf 5.x, and CI is now passing with this configuration.",
      "category": "feature",
      "subcategory": "dependency-upgrade",
      "apis": [],
      "components": [
        "protobuf",
        "ci",
        "dependency-management"
      ],
      "concepts": [
        "protobuf",
        "version-compatibility",
        "dependency-constraint",
        "ci-testing"
      ],
      "severity": "low",
      "userImpact": "Users can now use the Python SDK with Protobuf 5.x without version conflicts or compatibility issues.",
      "rootCause": "SDK was not tested against Protobuf 5.x and had unclear version support",
      "proposedFix": "Upgrade to protobuf 3.13 which requires 5.x, ensuring CI passes with the new version",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved as part of issue #694 by upgrading to protobuf 3.13, which only supports 5.x. Lock file now uses protobuf 5.x and CI is passing.",
      "related": [
        694
      ],
      "keyQuote": "This was done as part of #694 (the lock file now uses proto 5.x)",
      "number": 497,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:10:01.538Z"
    },
    {
      "summary": "The README incorrectly instructs users to mark Pydantic datetime fields as passthrough when they should be marked as unrestricted. The documentation needs correction and should include a code snippet example from the samples repository.",
      "category": "docs",
      "subcategory": "pydantic-datetime-serialization",
      "apis": [],
      "components": [
        "pydantic-converter",
        "readme-documentation"
      ],
      "concepts": [
        "datetime-serialization",
        "pydantic-configuration",
        "passthrough-vs-unrestricted",
        "type-conversion",
        "documentation-accuracy"
      ],
      "severity": "medium",
      "userImpact": "Users following the README are given incorrect instructions for configuring Pydantic datetime fields, leading to potential serialization issues in their workflows.",
      "rootCause": "README contains outdated or incorrect guidance on Pydantic datetime handling mechanism.",
      "proposedFix": "Update README to mark datetime as unrestricted instead of passthrough, and include code snippet from samples-python repository demonstrating correct configuration.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the right answer is to mark it as unrestricted. Fix this in README, and also consider providing snippet from samples-python",
      "number": 496,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:57.566Z"
    },
    {
      "summary": "Request to enable macOS M1/ARM runners for all Python SDK CI builds. This follows a feature request across the Temporal organization to support Apple Silicon architecture testing.",
      "category": "feature",
      "subcategory": "ci-infrastructure",
      "apis": [],
      "components": [
        "ci",
        "build-system",
        "test-infrastructure"
      ],
      "concepts": [
        "arm-architecture",
        "m1-support",
        "ci-runners",
        "platform-testing",
        "build-matrix"
      ],
      "severity": "medium",
      "userImpact": "Enables developers using macOS M1/ARM machines to have CI builds that accurately reflect their development environment, improving confidence in SDK compatibility.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was resolved by implementing M1/ARM runner support in the CI pipeline for the Python SDK, addressing the feature request.",
      "related": [
        396
      ],
      "keyQuote": "Use macOS M1/ARM runners for all SDK CI",
      "number": 495,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:59.223Z"
    },
    {
      "summary": "During garbage collection, finally clauses in Python workflows execute on a different event loop, causing commands to cross workflow contexts. This leads to activities from one workflow incorrectly executing in another workflow, resulting in non-determinism errors.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [
        "start_activity",
        "start_workflow"
      ],
      "components": [
        "workflow-runtime",
        "event-loop",
        "garbage-collector",
        "task-cache"
      ],
      "concepts": [
        "garbage-collection",
        "event-loop-context",
        "coroutine-finalization",
        "task-eviction",
        "thread-safety"
      ],
      "severity": "critical",
      "userImpact": "Activities from one workflow may execute in another workflow's context, causing data corruption and non-determinism errors that break workflow correctness guarantees.",
      "rootCause": "GeneratorExit during garbage collection of cached workflows can be interleaved on the same thread as another workflow's asyncio._set_running_loop, causing finally clauses to execute in the wrong event loop context.",
      "proposedFix": "Force tasks to complete before GC eviction similar to other SDKs, ensuring coroutines finish execution before cache eviction to prevent finally clauses from running on the wrong workflow's event loop.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved by implementing task completion before workflow cache eviction to prevent cross-context execution during garbage collection.",
      "related": [],
      "keyQuote": "during GC, finally clauses in Python will end up on a different workflow's even loop. This is very bad.",
      "number": 494,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:46.062Z"
    },
    {
      "summary": "Feature request to expose the `use_seconds_for_duration` option from sdk-core PR #706 for Prometheus metrics in the Python SDK, along with tests for duration values as seconds and ability to set duration values in custom metrics.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "prometheus",
        "custom-metrics"
      ],
      "concepts": [
        "duration",
        "seconds",
        "prometheus",
        "metrics-configuration",
        "time-unit"
      ],
      "severity": "medium",
      "userImpact": "Users need the ability to configure duration metrics to use seconds and set custom metric duration values in the Python SDK.",
      "rootCause": null,
      "proposedFix": "Expose `use_seconds_for_duration` option on Prometheus metrics and add ability to set duration values in custom metrics",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Partially addressed in #498; remaining work includes tests for duration values as seconds and ability to set duration values in custom metrics",
      "related": [
        498,
        706
      ],
      "keyQuote": "Need to at least expose `use_seconds_for_duration` option on Prometheus metrics from https://github.com/temporalio/sdk-core/pull/706",
      "number": 493,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:42.929Z"
    },
    {
      "summary": "Change the logging extra field to use dict-based temporal_activity and temporal_workflow instead of info data classes, improving compatibility with standard loggers.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "logger",
        "activity-executor",
        "workflow-executor"
      ],
      "concepts": [
        "logging",
        "structured-logging",
        "logger-extra",
        "info-objects",
        "dict-format"
      ],
      "severity": "low",
      "userImpact": "Users will have better logger compatibility and more standard logging output when using Temporal activities and workflows.",
      "rootCause": null,
      "proposedFix": "Stop setting activity_info and workflow_info as data classes in extra, instead set temporal_activity and temporal_workflow as dict entries in the logger details.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented change to use dict-based temporal_activity and temporal_workflow in logger extra instead of info data classes.",
      "related": [],
      "keyQuote": "Stop doing this by default, and start setting `temporal_activity` and `temporal_workflow` as the logger details dict",
      "number": 489,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:46.008Z"
    },
    {
      "summary": "A typo in the workflow sandbox restrictions file at line 602 uses a space instead of a comma, which needs to be corrected.",
      "category": "bug",
      "subcategory": "workflow-sandbox",
      "apis": [],
      "components": [
        "workflow-sandbox",
        "restrictions"
      ],
      "concepts": [
        "syntax-error",
        "code-quality",
        "sandbox-security"
      ],
      "severity": "low",
      "userImpact": "Users may encounter unexpected behavior or errors when the workflow sandbox restrictions are evaluated due to the syntax error.",
      "rootCause": "Typo in restrictions.py line 602 where a space is used instead of a comma in a restriction set definition.",
      "proposedFix": "Replace the space with a comma at the specified location in temporalio/worker/workflow_sandbox/_restrictions.py line 602.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "needs to be a comma instead of space",
      "number": 488,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:31.524Z"
    },
    {
      "summary": "Users with advanced logger handlers that don't work in sandbox mode need a way to opt-out of sandboxing for the workflow logger. The request is to add a helper function that allows wrapping logger process calls in unrestricted sandbox context.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "workflow-logger",
        "workflow-sandbox",
        "logging"
      ],
      "concepts": [
        "sandboxing",
        "logger-handlers",
        "unsafe-operations",
        "workflow-context",
        "advanced-logging"
      ],
      "severity": "medium",
      "userImpact": "Users with third-party logging libraries like structlog cannot use advanced handler features due to sandbox restrictions and must find workarounds.",
      "rootCause": "The workflow sandbox prevents certain logging library operations (like json serialization and threadlocal imports) that advanced handlers require.",
      "proposedFix": "Add a `temporalio.workflow.logger.unsafe_disable_sandbox()` helper that wraps logger process calls in `sandbox_unrestricted()` and `imports_passed_through()` contexts, or provide a base `SandboxDisabledLoggerAdapter` class.",
      "workaround": "Users can manually create a custom LoggerAdapter that wraps the process method in `with temporalio.workflow.unsafe.sandbox_unrestricted()` and `with temporalio.workflow.unsafe.imports_passed_through()` contexts.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Some users have advanced logger handlers that cannot work in sandbox mode. I think we can add a temporalio.workflow.logger.unsafe_disable_sandbox() call",
      "number": 487,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:31.181Z"
    },
    {
      "summary": "SDK should not return an update handle if the update has not reached the desired state. This feature request addresses inconsistent behavior where update handles are returned prematurely.",
      "category": "feature",
      "subcategory": "update-handling",
      "apis": [
        "UpdateHandle"
      ],
      "components": [
        "update-handler",
        "client-api",
        "state-management"
      ],
      "concepts": [
        "state-transition",
        "handle-lifecycle",
        "update-semantics",
        "desired-state",
        "async-operations"
      ],
      "severity": "medium",
      "userImpact": "Users may receive update handles before updates reach their desired state, leading to incorrect assumptions about operation status.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by implementing the desired behavior where update handles are only returned after reaching the desired state.",
      "related": [
        432
      ],
      "keyQuote": "SDK should not return an update handle if the update has not reached the desired state",
      "number": 485,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:30.552Z"
    },
    {
      "summary": "SDK clients should automatically set an UpdateID on update requests even when users don't explicitly specify one, ensuring consistent update handling across the SDK.",
      "category": "feature",
      "subcategory": "update-handling",
      "apis": [
        "UpdateID"
      ],
      "components": [
        "sdk-client",
        "update-request-handler"
      ],
      "concepts": [
        "update-id",
        "request-identification",
        "automatic-assignment",
        "api-consistency"
      ],
      "severity": "medium",
      "userImpact": "Users receive more robust update request handling without requiring explicit UpdateID specification in every update call.",
      "rootCause": null,
      "proposedFix": "Implement automatic UpdateID generation or assignment in SDK clients for any update request lacking an explicit UpdateID.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Enhancement implemented to automatically set UpdateID on update requests when not specified by users.",
      "related": [
        431
      ],
      "keyQuote": "SDK clients should set a UpdateID on any update request even if the user did not specify one",
      "number": 484,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:14.267Z"
    },
    {
      "summary": "Request to add API key client option to the Python SDK, referencing the features issue #426 for implementation details.",
      "category": "feature",
      "subcategory": "client-configuration",
      "apis": [],
      "components": [
        "client",
        "authentication",
        "connection-options"
      ],
      "concepts": [
        "api-key",
        "authentication",
        "client-configuration",
        "security",
        "credentials"
      ],
      "severity": "medium",
      "userImpact": "Users need a standardized way to configure API key authentication when connecting to Temporal clusters.",
      "rootCause": null,
      "proposedFix": "Implement API key client option as defined in features issue #426",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "API key authentication was implemented in the Python SDK client options",
      "related": [
        426
      ],
      "keyQuote": "See https://github.com/temporalio/features/issues/426",
      "number": 482,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:14.617Z"
    },
    {
      "summary": "Add reflection-based payload codec tests to Python SDK to ensure all non-search-attribute payloads are covered by codec conversion, similar to existing tests in the .NET SDK.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "codec",
        "payload",
        "worker",
        "test-framework"
      ],
      "concepts": [
        "payload-conversion",
        "codec",
        "reflection",
        "test-coverage",
        "search-attributes",
        "gRPC",
        "validation"
      ],
      "severity": "medium",
      "userImpact": "Ensures that codec changes are properly validated and prevents payload handling regressions in the Python SDK.",
      "rootCause": null,
      "proposedFix": "Implement reflection-based payload codec tests similar to the .NET SDK's WorkflowCodecHelperTests to validate all gRPC payloads are covered",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        477
      ],
      "keyQuote": "In .NET we have these tests which make sure any new payload added in gRPC is made part of codec conversion. But we don't have this for Python.",
      "number": 479,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:16.810Z"
    },
    {
      "summary": "Update handler inputs and responses are not being processed through payload converters and codecs, causing data serialization inconsistencies.",
      "category": "bug",
      "subcategory": "data-conversion",
      "apis": [
        "UpdateHandler"
      ],
      "components": [
        "update-handler",
        "data-converter",
        "codec"
      ],
      "concepts": [
        "serialization",
        "data-conversion",
        "payload-handling",
        "codec-pipeline",
        "input-processing"
      ],
      "severity": "high",
      "userImpact": "Users cannot use custom data converters or codecs with workflow updates, breaking data serialization consistency across update operations.",
      "rootCause": "Update input and response handling bypasses the standard payload converter/codec pipeline used for other workflow operations.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The input to and responses to updates are not currently being run through the payload converters / codecs.",
      "number": 477,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:58.256Z"
    },
    {
      "summary": "In heavily-loaded multiprocess workers, heartbeat queue polling times out when activities send heartbeats too frequently or with complex serialized details, causing the parent process to hang and stop processing further heartbeats, which fills the queue and blocks activity liveness.",
      "category": "bug",
      "subcategory": "activity-heartbeat",
      "apis": [
        "heartbeat"
      ],
      "components": [
        "worker",
        "activity-executor",
        "multiprocessing-manager",
        "heartbeat-processor"
      ],
      "concepts": [
        "timeout",
        "queue",
        "multiprocessing",
        "heartbeat",
        "serialization",
        "concurrency",
        "data-conversion"
      ],
      "severity": "high",
      "userImpact": "Users running heavily-loaded multiprocess workers experience worker hangs and inability to process tasks when activities send frequent heartbeats.",
      "rootCause": "The primary process cannot process the heartbeat queue fast enough when activities send heartbeats too frequently or with complex serialized details (especially Pydantic 2.x objects), causing Future.result(10) to timeout and the heartbeat processor to stop, leaving the queue blocked.",
      "proposedFix": "Throw an exception to shut down the entire worker when heartbeat processing fails, rather than silently returning and leaving the queue in a dysfunctional state.",
      "workaround": "Omit heartbeat details when sending heartbeats to reduce data conversion overhead; use standard threaded or async activities with multiprocessing invoked inside, rather than multiprocess activities.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I believe the worker is not processing heartbeats faster than you are sending them and at some point it's a timeout. We can't queue heartbeats indefinitely.",
      "number": 476,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:09:02.595Z"
    },
    {
      "summary": "Documentation enhancement to clarify that `poetry install --all-extras` is required for building the Python SDK, and potentially document that a newer version of pip is needed.",
      "category": "docs",
      "subcategory": "build-setup",
      "apis": [],
      "components": [
        "build-system",
        "documentation",
        "poetry-config"
      ],
      "concepts": [
        "build-setup",
        "dependencies",
        "installation",
        "documentation",
        "development-environment"
      ],
      "severity": "low",
      "userImpact": "Developers attempting to build the SDK may encounter build failures if they don't know to use the --all-extras flag with poetry install.",
      "rootCause": null,
      "proposedFix": "Update README to specify `poetry install --no-root --all-extras` and clarify pip version requirements",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "README and build documentation were updated to clarify the required installation flags and dependencies",
      "related": [],
      "keyQuote": "Need to change install command to `poetry install --no-root --all-extras`. Also may need to make clear newer version of `pip` required.",
      "number": 473,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:56.698Z"
    },
    {
      "summary": "Add clarification to SDK documentation that options on application failures are not mutated by the server. Specifically, non-retryable errors are client-side error options, not server-provided failures.",
      "category": "docs",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "error-handling",
        "options",
        "documentation"
      ],
      "concepts": [
        "error-options",
        "immutability",
        "server-client-contract",
        "non-retryable",
        "failure-handling",
        "documentation"
      ],
      "severity": "low",
      "userImpact": "Users will have clearer understanding of how error options work and won't be confused about whether the server mutates client-provided options.",
      "rootCause": null,
      "proposedFix": "Update documentation to clarify that options on application failures are not mutated by the server",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation clarification was provided to explain immutability of error options",
      "related": [],
      "keyQuote": "Clarify that options on application failures are not mutated by the server (e.g. non-retryable is an error option, not a server-provided failure)",
      "number": 470,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:42.823Z"
    },
    {
      "summary": "Add support for exposing the next retry delay field on ApplicationFailureInfo across all SDKs. The API now supports specifying retry delays on activity failures, and the Python SDK needs to expose this capability.",
      "category": "feature",
      "subcategory": "activity-retry",
      "apis": [
        "ApplicationFailureInfo"
      ],
      "components": [
        "activity",
        "failure-handling",
        "retry-logic"
      ],
      "concepts": [
        "retry",
        "delay",
        "failure",
        "activity",
        "backoff",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users can now programmatically specify custom retry delays when activities fail, enabling more granular control over retry behavior.",
      "rootCause": null,
      "proposedFix": "Expose the next retry delay field from ApplicationFailureInfo in the Python SDK, following the implementation in the API.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The field was exposed in the SDK to align with the API changes.",
      "related": [],
      "keyQuote": "We recently added the ability for an activity to specify the next retry delay on failure via a field on ApplicationFailureInfo.",
      "number": 468,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:44.629Z"
    },
    {
      "summary": "Feature request to support workflow metadata query in the Python SDK, similar to capabilities being developed across Temporal SDKs.",
      "category": "feature",
      "subcategory": "workflow-metadata",
      "apis": [],
      "components": [
        "workflow-runtime",
        "query-engine"
      ],
      "concepts": [
        "metadata-query",
        "workflow-introspection",
        "runtime-information",
        "state-inspection"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to query workflow metadata during execution, enabling better observability and dynamic workflow behavior.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        51
      ],
      "keyQuote": "Support workflow metadata query",
      "number": 467,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:40.340Z"
    },
    {
      "summary": "Feature request to support disabling TLS host verification in the Python SDK to enable testing with self-signed certificates. Users need this capability to connect to Temporal clusters with generic self-generated TLS certificates, similar to functionality available in other SDKs and the CLI.",
      "category": "feature",
      "subcategory": "tls-configuration",
      "apis": [
        "TlsConfig"
      ],
      "components": [
        "tls-client",
        "service-client",
        "bridge-client"
      ],
      "concepts": [
        "tls-verification",
        "self-signed-certificates",
        "host-validation",
        "certificate-validation",
        "security-configuration",
        "testing-infrastructure"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use self-signed certificates for testing against Temporal clusters without TLS host verification support, forcing workarounds or alternative deployment strategies.",
      "rootCause": "Python SDK's TlsConfig lacks a flag to disable hostname verification during certificate validation, unlike other Temporal SDKs which support this through environment variables or builder methods.",
      "proposedFix": "Add a hostname verification disable flag to the TlsConfig class that controls behavior in the rust SDK's client TLS configuration layer.",
      "workaround": "Set TlsConfig.domain to match the expected domain from the server's certificate instead of relying on the connection hostname.",
      "resolution": "fixed",
      "resolutionDetails": "Host verification disable option was implemented in TlsConfig",
      "related": [
        932
      ],
      "keyQuote": "You should be able to set `TlsConfig.domain` as an expected domain from the cert the server uses instead of it default to the given host",
      "number": 463,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:29.070Z"
    },
    {
      "summary": "Updates can be processed before the first workflow code executes if they arrive in the same task as the start workflow message. The SDK needs to buffer updates similar to how other SDKs handle this, or reorder activation job processing to prevent updates from executing before workflow initialization.",
      "category": "bug",
      "subcategory": "workflow-updates",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-executor",
        "update-handler",
        "activation-job-processor"
      ],
      "concepts": [
        "update-ordering",
        "buffer",
        "initialization",
        "activation-task",
        "signal-ordering"
      ],
      "severity": "high",
      "userImpact": "Users may encounter unexpected behavior where workflow update handlers execute before the workflow's initial code runs, breaking expected execution semantics.",
      "rootCause": "Core reorders updates to come before signals, but the Python SDK does not properly buffer or reorder activation jobs to ensure updates only execute after workflow initialization.",
      "proposedFix": "Buffer updates as implemented in other SDKs, or ensure activation job reordering matches Core's behavior while preventing updates from executing before workflow code starts.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        176
      ],
      "keyQuote": "it's currently possible to handle an update before running the first bit of workflow code if an update comes in same task as start workflow",
      "number": 462,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:27.419Z"
    },
    {
      "summary": "Worker process hangs indefinitely after encountering a polling error (PermissionDenied) during task retrieval from the server. The worker logs the error but fails to properly shut down, leaving the process in a hung state.",
      "category": "bug",
      "subcategory": "worker-shutdown",
      "apis": [
        "Worker",
        "Client.connect"
      ],
      "components": [
        "worker",
        "poll-task",
        "error-handling",
        "shutdown"
      ],
      "concepts": [
        "authorization",
        "polling",
        "error-recovery",
        "graceful-shutdown",
        "permission-denied",
        "hang-state",
        "task-polling"
      ],
      "severity": "high",
      "userImpact": "Users cannot recover from authentication or permission errors when starting a workerthe process hangs indefinitely and must be manually killed.",
      "rootCause": "Root cause identified in Rust Core layer (temporal/sdk-core#667). The core SDK fails to properly handle permission-denied errors during polling, preventing graceful worker shutdown.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in the Rust Core SDK (temporalio/sdk-core#667). The fix has been merged but was not yet released in a Python SDK version at the time of issue closure.",
      "related": [
        667
      ],
      "keyQuote": "Worker hangs after an error during polling. The authorization token is incorrect, so the worker cannot retrieve any tasks from the server.",
      "number": 459,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:26.172Z"
    },
    {
      "summary": "Feature request to automatically create tracing spans for schedule creation in the Python SDK. When the tracing interceptor is enabled, schedules should create spans and set them on workflow headers when starting workflows, with an opt-out option.",
      "category": "feature",
      "subcategory": "tracing-spans",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "tracing-interceptor",
        "schedule-creation",
        "workflow-header"
      ],
      "concepts": [
        "tracing",
        "spans",
        "observability",
        "parent-child-relationships",
        "distributed-tracing",
        "schedule"
      ],
      "severity": "medium",
      "userImpact": "Users cannot properly trace scheduled workflows because spans are created server-side (schedules) but client-side (workflows), breaking parent-child relationships across workers.",
      "rootCause": "Scheduled workflows are created server-side while spans are created client-side, preventing proper span hierarchy when workflows run on different workers.",
      "proposedFix": "Implement automatic span creation for schedules in the tracing interceptor with opt-out configuration option.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        794,
        394
      ],
      "keyQuote": "scheduled workflows are created server side and spans are created client side. Even if you create your own span when workflow starts client-side, that span will not remain the parent if the workflow runs on another worker.",
      "number": 454,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:09.757Z"
    },
    {
      "summary": "When importing a workflow from a nested package, parent package __init__.py files are implicitly imported via the sandbox, causing errors if those files contain non-deterministic code. Users expect only the specific workflow module to be imported.",
      "category": "bug",
      "subcategory": "workflow-import",
      "apis": [],
      "components": [
        "sandbox",
        "workflow-runner",
        "import-system"
      ],
      "concepts": [
        "determinism",
        "package-imports",
        "implicit-execution",
        "sandbox-validation",
        "module-loading"
      ],
      "severity": "medium",
      "userImpact": "Users with nested package structures cannot import workflows if parent packages contain non-deterministic code, forcing them to disable the sandbox or restructure their code.",
      "rootCause": "Python's import specification requires parent package __init__.py files to be executed when importing submodules. The SDK uses standard importlib behavior, which follows this specification.",
      "proposedFix": null,
      "workaround": "Disable the sandbox by passing sandboxed=False to the decorator or using UnsandboxedWorkflowRunner at worker construction.",
      "resolution": "wontfix",
      "resolutionDetails": "The maintainers determined this is intended behavior per Python's import specification. Parent __init__.py files must be executed when importing nested modules, and the SDK intentionally uses standard importlib behavior.",
      "related": [],
      "keyQuote": "This is the proper behavior according to Python specification... We use normal `importlib` behavior by intention.",
      "number": 453,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:11.839Z"
    },
    {
      "summary": "User requests that JSONPlainPayloadConverter handle protobuf-encoded JSON payloads when type hints are protobuf.Message subclasses, enabling workflows started via tctl with json/plain encoding to work with protobuf arguments.",
      "category": "feature",
      "subcategory": "payload-conversion",
      "apis": [],
      "components": [
        "JSONPlainPayloadConverter",
        "payload-converter",
        "protobuf-support"
      ],
      "concepts": [
        "payload-encoding",
        "type-conversion",
        "protobuf-serialization",
        "interoperability",
        "json-handling"
      ],
      "severity": "low",
      "userImpact": "Users who start workflows via tctl with json/plain encoding cannot use protobuf arguments, requiring them to use alternative tools or custom converters.",
      "rootCause": "JSONPlainPayloadConverter does not have special handling for proto.Message types like the JSON protobuf converter does.",
      "proposedFix": "Add protobuf support to JSONPlainPayloadConverter to decode json-encoded protobufs when type hints indicate a protobuf.Message subclass.",
      "workaround": "Users can implement a custom converter to handle json-encoded protobufs.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers declined to add proto support to the general json/plain converter, preferring proto support only in the dedicated proto converter. The underlying issue is with tctl's lack of encoding metadata support, which will be addressed in the new Temporal CLI.",
      "related": [
        185
      ],
      "keyQuote": "I hesitate to do this by default. We only want the JSON proto converter to have special rules about proto.",
      "number": 452,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:08:13.020Z"
    },
    {
      "summary": "The upsert_search_attribute method fails with StopIteration error when attempting to add new SearchAttribute that don't exist in the initial SearchAttributes set. This regression appeared in SDK 1.4.0.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [
        "upsert_search_attribute"
      ],
      "components": [
        "workflow-instance",
        "search-attributes"
      ],
      "concepts": [
        "search-attributes",
        "upsert",
        "state-management",
        "workflow-execution"
      ],
      "severity": "high",
      "userImpact": "Users cannot dynamically add new search attributes to workflows in SDK 1.4.0, blocking a core feature that worked in 1.3.0.",
      "rootCause": "The code attempts to find and update existing search attributes in a list, but throws StopIteration when the attribute key doesn't exist in the initial set.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in PR #440",
      "related": [
        440
      ],
      "keyQuote": "Adding new SearchAttribute using `workflow.upsert_search_attribute` fails with the `StopIteration` error since this list contains only previously set attributes.",
      "number": 451,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:50.938Z"
    },
    {
      "summary": "User encounters RestrictedWorkflowAccessError when using pathlib.Path.resolve() inside a workflow. The sandbox prevents access to pathlib methods, and the user seeks ways to disable restrictions while maintaining deterministic behavior.",
      "category": "question",
      "subcategory": "sandbox-restrictions",
      "apis": [
        "workflow.defn",
        "workflow.unsafe.imports_passed_through",
        "workflow.unsafe.sandbox_unrestricted"
      ],
      "components": [
        "sandbox",
        "workflow-execution",
        "import-restrictions"
      ],
      "concepts": [
        "determinism",
        "sandboxing",
        "pathlib",
        "workflow-safety",
        "import-passthrough",
        "disk-access"
      ],
      "severity": "low",
      "userImpact": "Users attempting to use pathlib or other restricted modules in workflows receive errors and need to understand workarounds or disable sandboxing.",
      "rootCause": "The workflow sandbox restricts access to pathlib.Path methods to enforce deterministic workflow execution, as disk-based operations are non-deterministic.",
      "proposedFix": null,
      "workaround": "Use `@workflow.defn(sandboxed=False)` to disable sandboxing entirely, or use `temporalio.workflow.unsafe.sandbox_unrestricted()` context manager for specific operations.",
      "resolution": "invalid",
      "resolutionDetails": "Closed as a question rather than a bug. The behavior is intentional - sandboxing prevents non-deterministic disk access. Workarounds and documentation references were provided.",
      "related": [],
      "keyQuote": "You can use temporalio.workflow.unsafe.sandbox_unrestricted() for specific operations, but disk-based calls from inside workflows are strongly discouraged.",
      "number": 450,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:56.546Z"
    },
    {
      "summary": "OpenTelemetry tracing is not generated or propagated for scheduled workflow executions in the Python SDK. When workflows are triggered by Temporal schedules, trace IDs are broken across activities, unlike manually invoked workflows which maintain consistent trace IDs.",
      "category": "bug",
      "subcategory": "observability-tracing",
      "apis": [],
      "components": [
        "tracing",
        "scheduler",
        "worker",
        "context-propagation"
      ],
      "concepts": [
        "opentelemetry",
        "tracing",
        "span-propagation",
        "trace-context",
        "scheduling",
        "workflow-execution"
      ],
      "severity": "high",
      "userImpact": "Users cannot effectively trace and observe scheduled workflow executions, making it difficult to debug and monitor workflows triggered by Temporal schedules.",
      "rootCause": "Trace context is not propagated from the scheduler to workflow executions; only user-controlled clients can create initial spans when starting workflows, and the SDK doesn't support creating orphaned spans server-side.",
      "proposedFix": "Support orphaned worker-side spans or implement a new mechanism to attach start-workflow headers on first task to propagate tracing context.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in a separate issue #454 with support for trace context propagation in scheduled workflows.",
      "related": [
        362,
        454
      ],
      "keyQuote": "We can't know what tracer or tracing framework a user is using to create the span for them when we start the workflow server side.",
      "number": 449,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:53.492Z"
    },
    {
      "summary": "User encountered deprecation warnings and breaking API changes in Python SDK 1.4.0, requiring migration of their codebase. They request a new release with improved documentation and a longer transition period for deprecated APIs.",
      "category": "bug",
      "subcategory": "release-management",
      "apis": [
        "typed_search_attributes"
      ],
      "components": [
        "python-sdk",
        "deprecation",
        "testing"
      ],
      "concepts": [
        "deprecation",
        "backward-compatibility",
        "migration",
        "release-cycle",
        "api-stability",
        "testing"
      ],
      "severity": "medium",
      "userImpact": "Users updating to new SDK versions face breaking changes and deprecation warnings that cause test failures, requiring codebase migration without adequate transition time or documentation.",
      "rootCause": "Typed search attributes API changes introduced deprecation warnings that fail pytest tests by default, with insufficient migration guidance and limited transition period before breaking changes.",
      "proposedFix": "Cut a new release with updated samples and provide a longer transition period before marking old APIs as deprecated.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Version 1.5.0 was released addressing the issue.",
      "related": [
        336
      ],
      "keyQuote": "Updating the python samples and a longer transition period before marking old apis as deprecated would be nice. Even just one release to massage any bugs.",
      "number": 448,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:36.055Z"
    },
    {
      "summary": "The workflow.logger incorrectly reports line numbers pointing to the LoggingAdapter.log method instead of the actual calling code. This makes it difficult to trace where log messages originate in workflow code.",
      "category": "bug",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "workflow.logger",
        "LoggingAdapter",
        "logging"
      ],
      "concepts": [
        "logging",
        "line-number-tracking",
        "stack-frames",
        "replay-prevention",
        "debugging"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot accurately identify which lines in their workflow code are generating log messages, making debugging and log analysis difficult.",
      "rootCause": "LogAdapter.log is overridden to prevent logging during replay, which breaks the standard Python logging mechanism's ability to correctly determine caller information.",
      "proposedFix": "Remove the log method override and replace it with an isEnabledFor method that checks both the log level and replay state.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was resolved by removing the LogAdapter.log override and implementing isEnabledFor to check replay state instead, preserving correct line number reporting.",
      "related": [],
      "keyQuote": "All the log records emitted by workflow.logger has the same file, path and lineno pointing to workflow.LoggingAdapter.log",
      "number": 447,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:37.373Z"
    },
    {
      "summary": "Add configurable workflow failure exception types to the Worker constructor and workflow decorators, allowing users to specify which exception types should be treated as workflow failures. Currently needed to support feature parity across SDKs.",
      "category": "feature",
      "subcategory": "worker-configuration",
      "apis": [
        "Worker",
        "workflow.defn"
      ],
      "components": [
        "worker",
        "workflow-definition",
        "failure-handling"
      ],
      "concepts": [
        "exception-handling",
        "workflow-failure",
        "configuration",
        "error-types",
        "worker-setup"
      ],
      "severity": "medium",
      "userImpact": "Users need control over which exception types trigger workflow failures, enabling more flexible error handling strategies.",
      "rootCause": null,
      "proposedFix": "Add `workflow_failure_exception_types: Set[Type[Exception]]` parameter to Worker constructor and `failure_exception_types: Set[Type[Exception]]` parameter to `@workflow.defn` decorator",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented with configurable exception types for both Worker and workflow definition levels",
      "related": [
        322,
        656
      ],
      "keyQuote": "need: workflow_failure_exception_types: Set[Type[Exception]] on Worker constructor, failure_exception_types on @workflow.defn",
      "number": 446,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:38.468Z"
    },
    {
      "summary": "The delete_workflow_execution RPC call was not properly available on the workflow_service in the Python SDK, throwing a ValueError when called. The issue was caused by the call being mistakenly placed on operator_service in the Rust side.",
      "category": "bug",
      "subcategory": "workflow-service",
      "apis": [
        "delete_workflow_execution"
      ],
      "components": [
        "workflow_service",
        "rpc_client",
        "operator_service"
      ],
      "concepts": [
        "workflow_deletion",
        "execution_cleanup",
        "rpc_availability",
        "service_routing"
      ],
      "severity": "low",
      "userImpact": "Users cannot delete workflow executions through the Python SDK, limiting their ability to clean up failed workflows for organizational purposes.",
      "rootCause": "The delete_workflow_execution RPC call was mistakenly placed on operator_service instead of workflow_service in the underlying service definition.",
      "proposedFix": "Move the delete_workflow_execution RPC call from operator_service to workflow_service in the service definition.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The Temporal team acknowledged the mistake and committed to fixing it by moving the RPC call to the correct service.",
      "related": [],
      "keyQuote": "It looks like mistakenly placed this on operator_service on our Rust side, we will fix.",
      "number": 445,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:21.819Z"
    },
    {
      "summary": "PyCharm IDE shows lint warnings when calling client.execute_workflow() with a parameterless workflow, even though the code is valid and passes other type checkers like Pyright.",
      "category": "question",
      "subcategory": "type-checking",
      "apis": [
        "execute_workflow"
      ],
      "components": [
        "client",
        "type-stubs"
      ],
      "concepts": [
        "type-checking",
        "IDE-integration",
        "linting",
        "coroutines",
        "type-hints"
      ],
      "severity": "low",
      "userImpact": "Developers using PyCharm CE see spurious type warnings that don't represent actual code errors, causing distraction and confusion.",
      "rootCause": "PyCharm's type checking appears to have different behavior than MyPy or Pyright when handling the execute_workflow method signature with parameterless workflows.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Issue determined to be a PyCharm-specific problem, not a Temporal SDK issue. The code works correctly with other type checkers (Pyright, MyPy).",
      "related": [],
      "keyQuote": "I tested my code with pyright, no issues reported, it should be a Pycharm issue.",
      "number": 444,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:23.263Z"
    },
    {
      "summary": "Upgrading Python SDK from 1.3.0 to 1.4.0 causes a 'coroutine raised StopIteration' error when triggering workflows that call workflow.upsert_search_attributes via CLI. The issue was fixed in PR #440 but not yet released at the time of reporting.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [
        "upsert_search_attributes"
      ],
      "components": [
        "worker",
        "workflow-instance",
        "cli-execution"
      ],
      "concepts": [
        "coroutine-handling",
        "search-attributes",
        "stopiteration-error",
        "deprecated-api",
        "workflow-execution"
      ],
      "severity": "high",
      "userImpact": "Users upgrading to SDK 1.4.0 experience workflow execution failures when using upsert_search_attributes, blocking workflow triggers from CLI.",
      "rootCause": "Coroutine handling change in SDK 1.4.0 related to upsert_search_attributes implementation causing StopIteration exception.",
      "proposedFix": "Use typed search attributes for upsert_search_attributes instead of the deprecated form (as noted in PR #440).",
      "workaround": "Downgrade to SDK 1.3.0 or use typed search attributes to avoid the deprecated upsert_search_attributes form.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in PR #440 which addressed the coroutine StopIteration issue, though recommendation is to migrate to typed search attributes.",
      "related": [
        440
      ],
      "keyQuote": "I believe this was just fixed in https://github.com/temporalio/sdk-python/pull/440 but not yet released. In the meantime, I recommend using typed search attributes",
      "number": 443,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:20.532Z"
    },
    {
      "summary": "GeneratorExit exceptions are causing context detach failures in OpenTelemetry integration when coroutines are garbage collected. The issue occurs when the garbage collector wakes up a coroutine on a different thread, causing token validation errors during context cleanup in the finally block.",
      "category": "bug",
      "subcategory": "opentelemetry-integration",
      "apis": [],
      "components": [
        "opentelemetry",
        "context-management",
        "workflow-execution"
      ],
      "concepts": [
        "generator-exit",
        "context-detach",
        "garbage-collection",
        "threading",
        "coroutine-lifecycle",
        "token-management"
      ],
      "severity": "high",
      "userImpact": "Users experience runtime errors and context cleanup failures when workflows are garbage collected, potentially leaving traces incomplete and causing context state corruption.",
      "rootCause": "GeneratorExit is triggered by garbage collection of coroutines/generators, potentially on different threads than where they started. When context detach is called in the finally block, the OpenTelemetry token was created in a different Context, causing ValueError.",
      "proposedFix": "Ensure all tasks complete gracefully rather than allowing coroutines/generators to be garbage collected without proper cancellation and cleanup.",
      "workaround": "Prevent reaching GeneratorExit by ensuring all coroutines are properly cancelled and awaited to completion rather than relying on GC. A specific workaround is mentioned in the linked OpenTelemetry issue.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by ensuring proper task completion and graceful shutdown to prevent GeneratorExit during garbage collection.",
      "related": [
        2606
      ],
      "keyQuote": "There's no way to disable this behavior, no way to intercept, and with threading (especially thread pools) no guarantee where it may wake up.",
      "number": 441,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:08.362Z"
    },
    {
      "summary": "Feature request to enable continue-as-new workflow capability from within update handlers in the Python SDK, allowing workflows to restart with new input parameters without losing context.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "update-handler",
        "workflow-executor",
        "api"
      ],
      "concepts": [
        "workflow-continuation",
        "state-management",
        "handler-capabilities",
        "workflow-restart"
      ],
      "severity": "medium",
      "userImpact": "Users cannot restart workflows with new parameters from update handlers, limiting workflow composition patterns and requiring workarounds.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Allow continue as new from update handler",
      "number": 439,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:07.044Z"
    },
    {
      "summary": "Feature request to allow RawValue objects to be used as search attribute values during upsert operations. This capability is needed for testing purposes and for workflows that need to transfer search attributes between different workflow instances.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [
        "UpsertSearchAttributes"
      ],
      "components": [
        "search-attributes",
        "type-system",
        "workflow-execution"
      ],
      "concepts": [
        "raw-value",
        "type-flexibility",
        "testing",
        "attribute-transfer",
        "serialization",
        "search-index"
      ],
      "severity": "low",
      "userImpact": "Users cannot currently use RawValue objects when upserting search attributes, limiting flexibility for advanced use cases like testing and cross-workflow attribute migration.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "There's no way at the moment to pass `RawValues` as a search attribute value. This is useful for ourselves for certain testing purposes, but could also be desirable for users shuttling SAs from one workflow to another, etc.",
      "number": 438,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:07:04.030Z"
    },
    {
      "summary": "A continue_as_new exception raised from an update handler may be swallowed as an uncaught exception in a task, rather than being properly handled as a task failure.",
      "category": "bug",
      "subcategory": "update-handler",
      "apis": [
        "continue_as_new"
      ],
      "components": [
        "update-handler",
        "task-execution",
        "exception-handling"
      ],
      "concepts": [
        "exception-handling",
        "task-failure",
        "continue-as-new",
        "update-handler",
        "error-propagation"
      ],
      "severity": "high",
      "userImpact": "Updates that invoke continue_as_new may not properly fail the task, leading to unexpected workflow behavior and silent failures.",
      "rootCause": "continue_as_new raised from an update handler is a base exception that may be re-raised and left uncaught in the task execution path.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "this may end up being swallowed (it's a base exception that I think is re-raised and uncaught in a task)",
      "number": 436,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:49.517Z"
    },
    {
      "summary": "MyPy type checking is not catching invalid single-parameter workflow calls where no parameter is provided. This is a regression in type safety that affects multiple areas of the SDK.",
      "category": "bug",
      "subcategory": "type-checking",
      "apis": [],
      "components": [
        "type-stubs",
        "mypy-plugin",
        "workflow-decorator"
      ],
      "concepts": [
        "type-checking",
        "overload",
        "mypy",
        "parameter-validation",
        "type-safety"
      ],
      "severity": "high",
      "userImpact": "Users can write invalid code that violates the workflow signature without MyPy catching the error, breaking type safety guarantees.",
      "rootCause": "Recent changes in MyPy version or SDK overload definitions are not properly enforcing parameter requirements for single-parameter workflows.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "you can execute a single-param workflow with no param and MyPy won't catch it",
      "number": 435,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:51.262Z"
    },
    {
      "summary": "Remove redundant activation job sorting from the Python SDK workflow instance since the core SDK now handles this. Add a test to verify that interleaved updates and signals maintain their order after the change.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "workflow-instance",
        "activation-jobs",
        "signal-handling"
      ],
      "concepts": [
        "job-ordering",
        "update-handling",
        "signal-interleaving",
        "core-integration",
        "synchronization"
      ],
      "severity": "medium",
      "userImpact": "Removes redundant code and ensures correct handling of interleaved workflow updates and signals.",
      "rootCause": "The Python SDK sorts activation jobs locally, but this is now handled by the core SDK after PR #639, creating duplication.",
      "proposedFix": "Remove the sorting logic from _workflow_instance.py and add a test confirming interleaved updates and signals remain interleaved.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "After https://github.com/temporalio/sdk-core/pull/639 is merged, remove that sorting, and write a test confirming that interleaved updates and signals remain interleaved.",
      "number": 433,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:52.924Z"
    },
    {
      "summary": "Raising a non-Temporal exception in a workflow causes indefinite workflow task retries instead of failing the workflow execution. The Python SDK treats all non-Temporal exceptions as retriable workflow task failures, contradicting the documented default behavior that workflows should not retry by default.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "workflow-instance",
        "worker",
        "failure-handling"
      ],
      "concepts": [
        "retry-policy",
        "workflow-failure",
        "exception-handling",
        "task-activation",
        "error-classification"
      ],
      "severity": "high",
      "userImpact": "Users cannot fail workflows by raising exceptions in workflow code, and tests hang indefinitely when workflows fail since automatic retries never stop.",
      "rootCause": "The Python SDK's workflow execution logic treats non-Temporal exceptions as retriable workflow task failures rather than workflow execution failures. Only Temporal errors (ApplicationError) are treated as non-retriable.",
      "proposedFix": "Clarify documentation or change behavior to allow non-Temporal exceptions to fail workflows, or provide configuration to control exception handling behavior.",
      "workaround": "Use ApplicationError instead of standard exceptions to fail workflows. For testing, set `workflow_failure_exception_types=[Exception]` on the Worker to treat exceptions as non-retriable.",
      "resolution": "wontfix",
      "resolutionDetails": "Closed as expected behavior. The Python SDK only treats Temporal errors as workflow-failing exceptions by design; other exceptions retry the workflow task. Users should use ApplicationError to fail workflows intentionally.",
      "related": [],
      "keyQuote": "This is expected, in all exception based SDKs (like python) only Temporal Errors fail a Workflow Execution by default, other exceptions will retry the Workflow Task.",
      "number": 432,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:38.085Z"
    },
    {
      "summary": "asyncio.wait() is non-deterministic when passed coroutines instead of tasks, causing activity results to be randomly reassigned across concurrent executions. This occurs because asyncio.wait() internally converts coroutines to a set, which has no ordering guarantees.",
      "category": "bug",
      "subcategory": "concurrency-determinism",
      "apis": [
        "execute_activity",
        "execute_activity"
      ],
      "components": [
        "workflow-execution",
        "async-orchestration",
        "activity-executor"
      ],
      "concepts": [
        "determinism",
        "asyncio-wait",
        "coroutines",
        "concurrency",
        "task-ordering",
        "non-deterministic-behavior"
      ],
      "severity": "high",
      "userImpact": "Users experience random mismatches in activity result assignments when using asyncio.wait() with coroutines, causing workflows to produce incorrect outputs and breaking deterministic execution guarantees.",
      "rootCause": "Python's asyncio.wait() non-deterministically converts coroutines to a set (unordered), causing them to start in unpredictable order. With async functions like execute_activity(), the coroutine isn't created until awaited, unlike sync functions that return awaitables.",
      "proposedFix": "Wrap coroutines in asyncio.create_task() before passing to asyncio.wait(), which ensures tasks are created deterministically and executed in order.",
      "workaround": "Use asyncio.create_task() to wrap execute_activity() calls, or use asyncio.gather() which uses an ordered dictionary internally for futures.",
      "resolution": "wontfix",
      "resolutionDetails": "Determined to be a Python asyncio limitation, not a Temporal SDK issue. Documented as a gotcha. Mitigated in Python 3.11+ which disallows passing coroutines to asyncio.wait().",
      "related": [],
      "keyQuote": "asyncio.wait is non-deterministically converts the input to a set (which unlike even dict, does not have any reliable ordering). This means the coroutines are not even started in the same order.",
      "number": 429,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:35.155Z"
    },
    {
      "summary": "Dataclass deserialization requires all fields to be present in JSON dict, including those with default values. This is overly strict and should allow omitting fields that have defaults.",
      "category": "bug",
      "subcategory": "dataclass-serialization",
      "apis": [],
      "components": [
        "dataclass",
        "serialization",
        "deserialization"
      ],
      "concepts": [
        "json-deserialization",
        "default-values",
        "dataclass-fields",
        "optional-fields"
      ],
      "severity": "medium",
      "userImpact": "Users cannot deserialize JSON objects that omit optional fields with defaults, making dataclass-based data models unnecessarily strict.",
      "rootCause": "Dataclass deserialization logic does not skip required validation for fields that have default values defined.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "a dataclass cannot deserialize a JSON dict that doesn't have _all_ fields including ones with defaults. This is a bug, we should not require all fields",
      "number": 427,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:33.552Z"
    },
    {
      "summary": "Feature request to implement `is_running` method on the Temporal Python workflow event loop to better conform to `AbstractEventLoop` interface. The method is needed for compatibility with libraries like aiokafka and aiohttp that expect this standard asyncio method to be implemented.",
      "category": "feature",
      "subcategory": "event-loop",
      "apis": [],
      "components": [
        "event-loop",
        "workflow-instance",
        "asyncio-compatibility"
      ],
      "concepts": [
        "abstract-event-loop",
        "interface-compliance",
        "asyncio-standards",
        "determinism",
        "compatibility",
        "external-libraries"
      ],
      "severity": "low",
      "userImpact": "Users attempting to use standard Python asyncio libraries (aiohttp, aiokafka) within workflows encounter NotImplementedError when those libraries call `is_running()` on the event loop.",
      "rootCause": "Temporal's `_WorkflowInstanceImpl` is not a complete implementation of `AbstractEventLoop` - it lacks the `is_running()` method that standard libraries expect to exist.",
      "proposedFix": "Add an `is_running` method to `_WorkflowInstanceImpl`, potentially returning `True` if a Runtime is defined or `not self._deleting`.",
      "workaround": "Move external service calls (like Kafka) to activities instead of workflows, ensuring deterministic workflow behavior.",
      "resolution": "wontfix",
      "resolutionDetails": "The maintainers decided that implementing `is_running()` would mask incorrect usage patterns where users try to use non-deterministic external I/O in workflows. The error provides beneficial detection of illegal event loop usage.",
      "related": [],
      "keyQuote": "I'm actually kind of glad this raised a not-implemented so it catches illegal things happening to the workflow's event loop, such as code based on whether it's \"running\" or not as if it was a local event loop.",
      "number": 426,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:21.039Z"
    },
    {
      "summary": "Add usedforsecurity=False parameter to MD5 hash function calls used for build ID generation in Python 3.9+. This addresses Python 3.9's requirement to explicitly mark non-security uses of cryptographic functions.",
      "category": "feature",
      "subcategory": "build-id",
      "apis": [],
      "components": [
        "build-id",
        "hashing",
        "python-runtime"
      ],
      "concepts": [
        "cryptographic-function",
        "security-flag",
        "build-identifier",
        "python-version-compatibility"
      ],
      "severity": "low",
      "userImpact": "Users on Python 3.9+ will avoid deprecation warnings or errors when the SDK generates build IDs using MD5 hashing.",
      "rootCause": "Python 3.9+ requires explicit usedforsecurity=False parameter for non-security cryptographic operations to reduce confusion about security vs. non-security use cases.",
      "proposedFix": "Add usedforsecurity=False to the md5() call used for build ID generation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by PR #472 which added the usedforsecurity=False parameter to MD5 calls for build ID.",
      "related": [
        472
      ],
      "keyQuote": "For Python 3.9+, add `usedforsecurity=False` to `md5` call for build ID",
      "number": 425,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:16.841Z"
    },
    {
      "summary": "Add ability to get an update handle from a workflow handle. Ensure update handles use the returned run ID when starting updates, and provide clear documentation about consequences of getting update handles on workflow handles without a run ID.",
      "category": "feature",
      "subcategory": "update-handle",
      "apis": [
        "update_handle",
        "workflow_handle",
        "start"
      ],
      "components": [
        "workflow-handle",
        "update-handle",
        "run-id"
      ],
      "concepts": [
        "update-handle",
        "workflow-handle",
        "run-id",
        "idempotency",
        "handle-management"
      ],
      "severity": "medium",
      "userImpact": "Users cannot retrieve update handles from workflow handles and lack clarity on run ID behavior, limiting their ability to manage workflow updates programmatically.",
      "rootCause": null,
      "proposedFix": "Implement getter method for update handles on workflow handles; ensure update handle uses returned run ID from start operations; add comprehensive documentation about run ID implications.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to provide update handle retrieval from workflow handles with proper run ID handling.",
      "related": [],
      "keyQuote": "Should be able to get an update handle from a workflow handle. Also since updates are always run-specific, when using \"start\", the update handle should use the returned run ID",
      "number": 424,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:19.757Z"
    },
    {
      "summary": "Migrate Python SDK from black/isort to Ruff for code formatting and import sorting. This tool consolidation was successfully tested in the samples-python repository and should be applied to the main SDK.",
      "category": "feature",
      "subcategory": "developer-tooling",
      "apis": [],
      "components": [
        "build-system",
        "linting",
        "code-formatting"
      ],
      "concepts": [
        "code-quality",
        "developer-experience",
        "tooling-consolidation",
        "import-management",
        "formatting"
      ],
      "severity": "low",
      "userImpact": "Simplifies the SDK development workflow by consolidating code formatting tools, making it easier for contributors to maintain code quality.",
      "rootCause": null,
      "proposedFix": "Replace black and isort dependencies with Ruff as the single tool for formatting and import sorting, similar to the successful migration in samples-python.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We did this at https://github.com/temporalio/samples-python/pull/91 with success, we should do it here too",
      "number": 421,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:02.421Z"
    },
    {
      "summary": "Pyright/VSCode type checker fails with multi-parameter update methods due to typing issues. Need to simplify type hints for start_update/execute_update and add Pyright to CI alongside MyPy.",
      "category": "feature",
      "subcategory": "type-checking",
      "apis": [
        "start_update",
        "execute_update"
      ],
      "components": [
        "type-hints",
        "ci",
        "pyright-integration"
      ],
      "concepts": [
        "type-inference",
        "type-checking",
        "overload-resolution",
        "vscode-integration"
      ],
      "severity": "medium",
      "userImpact": "Developers using VSCode with Pyright cannot properly use update methods due to type checking errors, reducing IDE support and developer experience.",
      "rootCause": "Multi-parameter update methods have overly specific typing that Pyright cannot resolve, unlike MyPy. Return types also not properly inferred in Pyright.",
      "proposedFix": "Remove specific typing for multi-param updates (use Sequence[Any] like signal/query) and add Pyright to CI pipeline alongside MyPy.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by PR #471 which fixed the overload issues. Pyright integration to CI was added as requested.",
      "related": [
        471
      ],
      "keyQuote": "Remove the typing for multi-param update (use Sequence[Any] like signal/query). Then we need to add pyright to CI alongside MyPy",
      "number": 420,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:05.415Z"
    },
    {
      "summary": "README contains multiple documentation issues: incorrect class initialization signature, missing poetry install flags for running tests, and unclear pip version requirements for development setup.",
      "category": "docs",
      "subcategory": "readme",
      "apis": [],
      "components": [
        "documentation",
        "readme",
        "development-setup"
      ],
      "concepts": [
        "documentation",
        "setup-instructions",
        "testing",
        "dependencies",
        "pip-requirements"
      ],
      "severity": "low",
      "userImpact": "Developers following the README may encounter confusion or errors when setting up the development environment or running tests.",
      "rootCause": "README documentation contains inaccurate or incomplete information regarding class initialization syntax, dependency installation, and version requirements.",
      "proposedFix": "Update README to: (1) fix class __init__() signature to include self, (2) add --all-extras flag to poetry install command for running tests, (3) clarify that latest pip is required for development.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation issues were corrected in the README.",
      "related": [],
      "keyQuote": "Class `__init__()` doesn't take `self` / Running tests should have `--all-extras` on `poetry install` / Clarify in dev section that you need latest pip",
      "number": 419,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:06:00.668Z"
    },
    {
      "summary": "Activity failure warning logs for not-found activities lack contextual workflow information because these activities never enter the activity context. This feature request seeks to ensure that activity failure warnings include relevant workflow information for better debugging and observability.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "activity-executor",
        "logging",
        "worker"
      ],
      "concepts": [
        "activity-not-found",
        "logging",
        "workflow-context",
        "error-reporting",
        "debugging"
      ],
      "severity": "medium",
      "userImpact": "Users have difficulty debugging activity failures because log messages lack workflow context when activities are not found.",
      "rootCause": "Not-found activities never enter activity context, preventing workflow information from being attached to failure logs.",
      "proposedFix": "Ensure activity failure warning logs include workflow information even for not-found activities.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Enhancement implemented to include contextual workflow information in activity failure warning logs.",
      "related": [],
      "keyQuote": "Today for not-found activities since they never enter activity context, they don't have workflow info. Make sure they get workflow info.",
      "number": 416,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:46.874Z"
    },
    {
      "summary": "Request to add an `in_workflow()` function to the Python SDK that allows code to detect if it's running within a workflow context, similar to Java's `isWorkflowThread()` and TypeScript's `inWorkflowContext()`. The implementer acknowledges this is not technically unsafe despite potentially being placed in an unsafe module.",
      "category": "feature",
      "subcategory": "workflow-context",
      "apis": [],
      "components": [
        "workflow",
        "context-detection"
      ],
      "concepts": [
        "workflow-context",
        "thread-detection",
        "runtime-detection",
        "unsafe-operations",
        "api-design"
      ],
      "severity": "low",
      "userImpact": "Users need a way to conditionally execute code based on whether they're running inside a workflow or in regular code.",
      "rootCause": null,
      "proposedFix": "Add `temporalio.workflow.in_workflow()` function to detect workflow context, potentially not requiring an unsafe module despite convention.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented and issue closed",
      "related": [],
      "keyQuote": "Similar to Java's isWorkflowThread() or TypeScript's inWorkflowContext, but discourage its use of course.",
      "number": 415,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:49.077Z"
    },
    {
      "summary": "The protoc-wheel-0 dev dependency is unavailable on PyPI for musl-based distributions like Alpine Linux, preventing installation of temporalio package. This affects users building on Alpine and other musl-based systems that lack pre-built wheels for protoc-wheel-0.",
      "category": "bug",
      "subcategory": "dependency-build",
      "apis": [],
      "components": [
        "build-system",
        "poetry-configuration",
        "dependencies",
        "protobuf-compilation"
      ],
      "concepts": [
        "build-dependency",
        "musl-compatibility",
        "alpine-linux",
        "wheel-availability",
        "protobuf-tooling",
        "installation-failure",
        "distribution-support"
      ],
      "severity": "high",
      "userImpact": "Users attempting to install temporalio on Alpine Linux or other musl-based distributions encounter installation failures due to unavailable protoc-wheel-0 package.",
      "rootCause": "protoc-wheel-0 dependency is locked at version 3.9 and has no distribution available on PyPI, particularly affecting musl-based platforms that lack pre-built wheels.",
      "proposedFix": "Replace protoc-wheel-0 dependency with protobuf package (version ^4.25.3) which has broader platform support.",
      "workaround": "Remove protoc-wheel-0 from pyproject.toml and lock files, then add protobuf ^4.25.3 as a replacement dependency.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by removing the protoc-wheel-0 dependency and replacing it with protobuf package, which has better cross-platform support including musl-based distributions.",
      "related": [
        622
      ],
      "keyQuote": "ERROR: Could not find a version that satisfies the requirement protoc-wheel-0 (from versions: none)",
      "number": 412,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:47.638Z"
    },
    {
      "summary": "Remove the OutboundInterceptor.poll_workflow_update method as it doesn't need to be interceptable, similar to how getting a workflow result is not interceptable.",
      "category": "feature",
      "subcategory": "interceptor-framework",
      "apis": [
        "OutboundInterceptor.poll_workflow_update"
      ],
      "components": [
        "interceptor",
        "workflow-update",
        "outbound-interceptor"
      ],
      "concepts": [
        "interceptor-pattern",
        "workflow-update",
        "api-simplification",
        "interception-scope"
      ],
      "severity": "low",
      "userImpact": "Users will have a simpler interceptor API with fewer methods to implement and override.",
      "rootCause": "poll_workflow_update does not require interception like getting a workflow result does not require interception.",
      "proposedFix": "Remove the poll_workflow_update method from OutboundInterceptor.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The method was removed from the OutboundInterceptor interface in the SDK.",
      "related": [],
      "keyQuote": "This does not need to be interceptable any more than getting a workflow result does",
      "number": 409,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:34.386Z"
    },
    {
      "summary": "Remove the timeout parameter from WorkflowUpdateHandle.result since users can achieve timeout behavior using asyncio.wait_for if needed, and the API should be consistent with WorkflowHandle.result which doesn't have a timeout parameter.",
      "category": "feature",
      "subcategory": "workflow-handle-api",
      "apis": [
        "WorkflowUpdateHandle.result",
        "WorkflowHandle.result"
      ],
      "components": [
        "workflow-handle",
        "update-handle",
        "api-design"
      ],
      "concepts": [
        "timeout",
        "api-consistency",
        "asyncio-integration",
        "optional-parameters",
        "handle-design"
      ],
      "severity": "low",
      "userImpact": "Users will have a simpler, more consistent API for workflow handles and can still apply timeouts using standard asyncio utilities if needed.",
      "rootCause": "Inconsistency between WorkflowUpdateHandle.result and WorkflowHandle.result APIs; timeout functionality can be achieved with asyncio.wait_for",
      "proposedFix": "Remove the timeout parameter from WorkflowUpdateHandle.result",
      "workaround": "Users can use asyncio.wait_for() wrapper to apply timeout behavior",
      "resolution": "fixed",
      "resolutionDetails": "The timeout parameter was removed from WorkflowUpdateHandle.result to maintain API consistency with WorkflowHandle.result",
      "related": [],
      "keyQuote": "Users can do this with `asyncio.wait_for` if they want, no reason to have this if we're not also gonna have it on `WorkflowHandle.result`",
      "number": 408,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:33.925Z"
    },
    {
      "summary": "Add a utility or sample code demonstrating how to integrate Temporal core logs into Python's standard logging framework using the new log forwarding feature introduced in #405.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "logging",
        "python-sdk",
        "log-forwarding"
      ],
      "concepts": [
        "logging",
        "log-integration",
        "forwarding",
        "utilities",
        "samples",
        "core-logs"
      ],
      "severity": "medium",
      "userImpact": "Users need guidance on how to incorporate Temporal's core logs into their Python logging infrastructure for better observability.",
      "rootCause": null,
      "proposedFix": "Create a utility function or provide a sample demonstrating log forwarding from Temporal core to Python loggers, with consideration for log ordering (researching TypeScript SDK's approach).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Folded into #311 as part of the MVP requirement for Python SDK logging integration.",
      "related": [
        405,
        311
      ],
      "keyQuote": "Need to demonstrate how to use the new log forwarding feature (#405) to push logs into Python loggers automatically.",
      "number": 407,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:31.214Z"
    },
    {
      "summary": "Feature request to expose StartDelay option in Workflow Options, allowing workflows to be scheduled with a delay before execution begins.",
      "category": "feature",
      "subcategory": "workflow-options",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-options",
        "workflow-execution"
      ],
      "concepts": [
        "scheduling",
        "delay",
        "workflow-start",
        "timing",
        "execution-control"
      ],
      "severity": "low",
      "userImpact": "Users cannot delay workflow execution, limiting scheduling flexibility for time-sensitive operations.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "StartDelay was added to Workflow Options in the Python SDK",
      "related": [],
      "keyQuote": "Expose StartDelay in Workflow Options",
      "number": 404,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:17.258Z"
    },
    {
      "summary": "User wants to trigger activity retry logic when a workflow receives a signal that indicates failure or timeout. They're looking for a Temporal-idiomatic way to retry failed activities based on signal conditions rather than manually implementing retry loops.",
      "category": "question",
      "subcategory": "activity-retry",
      "apis": [
        "execute_activity",
        "wait_condition"
      ],
      "components": [
        "activity-executor",
        "signal-handler",
        "workflow-engine"
      ],
      "concepts": [
        "retry-policy",
        "signal-handling",
        "timeout",
        "activity-failure",
        "workflow-pattern"
      ],
      "severity": "low",
      "userImpact": "Users need guidance on idiomatic Temporal patterns for retrying activities based on signal-driven conditions.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Use a for loop with manual retry logic instead of relying on Temporal's built-in retry policy.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer clarified that using a for loop is a valid Temporal pattern and closed as a question rather than a feature request.",
      "related": [],
      "keyQuote": "A for loop is very much a Temporal pattern",
      "number": 401,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:19.400Z"
    },
    {
      "summary": "Custom dataclass return types from activities fail to deserialize in Python 3.9 with a KeyError during sandbox import resolution, while the same code works in Python 3.11. The issue appears to be related to how the workflow sandbox importer handles automatic type resolution for older Python versions.",
      "category": "bug",
      "subcategory": "sandbox-imports",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "workflow-sandbox",
        "payload-converter",
        "importer"
      ],
      "concepts": [
        "dataclass",
        "deserialization",
        "type-hints",
        "sandbox-isolation",
        "python-compatibility",
        "auto-import"
      ],
      "severity": "high",
      "userImpact": "Users with Python 3.9 cannot use custom dataclasses as activity return types without manual imports in the workflow, limiting code portability and requiring version-specific workarounds.",
      "rootCause": "Python 3.9's type hint resolution in get_type_hints() attempts to access sys.modules through the sandbox importer's __getitem__, which raises KeyError for dataclasses defined in user modules that the sandbox hasn't explicitly imported.",
      "proposedFix": null,
      "workaround": "Import the custom dataclass explicitly in the workflow using `with workflow.unsafe.imports_passed_through(): from file import customDataClass` (not needed in Python 3.11)",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "So it appears in newer Python versions the sandbox importer can import your dataclass but in older Python versions you have to import it yourself",
      "number": 399,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:17.912Z"
    },
    {
      "summary": "Drop Python 3.7 support, update to minimum Python 3.8, and add Python 3.12 to CI. Also update build tooling versions (setuptools-rust, cibuildwheel) and re-investigate build system hacks.",
      "category": "feature",
      "subcategory": "build-system",
      "apis": [],
      "components": [
        "build-tooling",
        "CI-pipeline",
        "setuptools-rust",
        "cibuildwheel"
      ],
      "concepts": [
        "version-support",
        "python-compatibility",
        "build-automation",
        "dependency-management",
        "CI-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users will need to update to Python 3.8+ and benefit from Python 3.12 support and modernized build infrastructure.",
      "rootCause": null,
      "proposedFix": "Remove Python 3.7 support, update build tooling versions, add Python 3.12 to CI matrix, re-investigate and fix build system hacks",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the requested build improvements and Python version updates were completed",
      "related": [],
      "keyQuote": "Remove support for 3.7 (i.e. update everything to 3.8), Update build tooling versions of setuptools-rust, cibuildwheel, and others, Add 3.12 to CI",
      "number": 398,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:02.877Z"
    },
    {
      "summary": "Clients may not be auto-reconnecting when the server shuts down and restarts. Investigation and testing confirmed that clients do recover after server restart, though the issue was initially unconfirmed.",
      "category": "bug",
      "subcategory": "client-reconnection",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "client",
        "worker",
        "connection-management"
      ],
      "concepts": [
        "reconnection",
        "server-restart",
        "auto-recovery",
        "retry",
        "fault-tolerance"
      ],
      "severity": "medium",
      "userImpact": "Clients may experience temporary failures when servers restart, potentially requiring manual intervention or application-level error handling.",
      "rootCause": "Unclear - testing showed clients do eventually reconnect after server restart, but initial reports suggested potential reconnection issues.",
      "proposedFix": "Related to issue #396, which may address the reconnection behavior.",
      "workaround": "Existing retry logic handles reconnection; clients can trap exceptions during server downtime.",
      "resolution": "fixed",
      "resolutionDetails": "Investigation confirmed that clients automatically recover and reconnect after server restart. Worker and client instances remain usable without manual intervention.",
      "related": [
        396
      ],
      "keyQuote": "The worker automatically recovers. The same exact client instance is usable again for starting workflows.",
      "number": 397,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:01.730Z"
    },
    {
      "summary": "Request to expose client keep-alive options in the Python SDK. The functionality is already enabled by default in the SDK core, requiring only the addition of configuration options.",
      "category": "feature",
      "subcategory": "client-configuration",
      "apis": [],
      "components": [
        "client",
        "connection-management",
        "sdk-core-bindings"
      ],
      "concepts": [
        "keep-alive",
        "connection-management",
        "client-configuration",
        "heartbeat",
        "network-stability"
      ],
      "severity": "low",
      "userImpact": "Users cannot currently configure client keep-alive settings despite the feature being available in the underlying SDK core.",
      "rootCause": null,
      "proposedFix": "Expose the options from sdk-core issue #585 in the Python SDK API",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to expose client keep-alive options available in SDK core",
      "related": [
        585
      ],
      "keyQuote": "Already enabled by default in latest SDK core, just need to expose the options",
      "number": 395,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:05:03.985Z"
    },
    {
      "summary": "Cross-compilation for s390x and ppc64el architectures fails due to ring v0.16.20 not supporting these platforms. The issue is in a transitive dependency (rustls -> ring) and was resolved by updating rustls to v0.21.8.",
      "category": "bug",
      "subcategory": "cross-compilation",
      "apis": [],
      "components": [
        "bridge",
        "rustls",
        "ring"
      ],
      "concepts": [
        "cross-compilation",
        "architecture-support",
        "dependency-management",
        "build-failure",
        "platform-support"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to build the Python SDK for s390x or ppc64el architectures encounter build failures and cannot use the SDK on these platforms.",
      "rootCause": "ring v0.16.20 does not support s390x and ppc64el architectures, causing panic in build script. Issue exists in transitive dependency chain: rustls depends on ring.",
      "proposedFix": "Update rustls to v0.21.8 which includes the updated ring dependency with support for these architectures.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by updating rustls to v0.21.8 in temporalio/sdk-python PR #400, which includes ring with s390x and ppc64el support.",
      "related": [
        400
      ],
      "keyQuote": "I was able to cross-compile temporalio/bridge without any problems.",
      "number": 394,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:04:48.934Z"
    },
    {
      "summary": "User requests support for Python's ExceptionGroup traceback in workflows using asyncio.TaskGroup. When an ExceptionGroup is raised, its sub-exception tracebacks are missing from the stack_trace field, making debugging harder.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "converter",
        "failure-handling",
        "traceback-formatting"
      ],
      "concepts": [
        "exception-handling",
        "asyncio",
        "traceback",
        "sub-exceptions",
        "debugging",
        "failure-serialization"
      ],
      "severity": "low",
      "userImpact": "Users cannot see sub-exception tracebacks in the UI when ExceptionGroup is raised, making it harder to debug issues in parallel activities.",
      "rootCause": "The Temporal failure message protobuf only supports a single stack trace and cause, not multiple exceptions as needed for ExceptionGroup.",
      "proposedFix": "Discussed using traceback.format_exception instead of format_tb for ExceptionGroup specifically, but blocked by the failure message model limitation.",
      "workaround": "Users need to catch ExceptionGroup and re-raise as ApplicationError while logging the exception details manually.",
      "resolution": "wontfix",
      "resolutionDetails": "ExceptionGroup cannot be serialized to a Temporal failure (single stack trace, single cause model). Maintainers recommend handling via logs on the worker instead.",
      "related": [],
      "keyQuote": "Exception group is not serializable to a Temporal failure which is a single message w/ a single stack trace and a single cause.",
      "number": 393,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:04:48.441Z"
    },
    {
      "summary": "Design and implement a static analyzer for Python SDK to detect thread-blocking calls in async functions and illegal workflow calls. Discussion explores using mypy plugins, Pyright, or the emerging Ruff type-checker for implementation.",
      "category": "feature",
      "subcategory": "static-analysis",
      "apis": [],
      "components": [
        "type-checker",
        "lsp-integration",
        "call-graph-builder",
        "plugin-system"
      ],
      "concepts": [
        "static-analysis",
        "type-checking",
        "call-graph",
        "async-blocking",
        "workflow-constraints",
        "lsp-tooling",
        "code-extensibility"
      ],
      "severity": "medium",
      "userImpact": "Users need tooling to catch programming errors at development time rather than runtime, particularly thread-blocking calls in async functions and illegal workflow operations.",
      "rootCause": null,
      "proposedFix": "Implement static analyzer using mypy plugins as initial POC, with plans to evaluate Ruff type-checker once it provides extensibility. Focus on building proper call graphs to detect illegal operations.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        607
      ],
      "keyQuote": "there are two static analyzers needed IMO: General purpose Python static analyzer that catches people making thread-blocking calls in an async def function; Temporal-specific Python static analyzer",
      "number": 390,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:04:46.631Z"
    },
    {
      "summary": "Add support for Eager Workflow Start in the Python SDK, a performance optimization that allows clients to directly schedule the first workflow task to a local worker, reducing latency. This feature is already supported in Java and Go SDKs and depends on Core implementation.",
      "category": "feature",
      "subcategory": "workflow-start",
      "apis": [
        "start_workflow"
      ],
      "components": [
        "client",
        "workflow-executor",
        "task-scheduling"
      ],
      "concepts": [
        "performance",
        "latency-optimization",
        "eager-execution",
        "workflow-initialization",
        "client-scheduling"
      ],
      "severity": "medium",
      "userImpact": "Enables users to reduce workflow start latency by using eager workflow start when calling start_workflow with the new flag.",
      "rootCause": null,
      "proposedFix": "Add enable_eager_start flag to client.start_workflow method, defaulting to False until Core implementation is considered stable.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented with enable_eager_start flag added to start_workflow method.",
      "related": [
        242,
        606
      ],
      "keyQuote": "Support for Eager Workflow Start was added in the server in 1.20, and is currently supported in Java and Go SDKs. It reduces the latency to start a workflow by allowing the client to directly schedule the first workflow task to a local worker.",
      "number": 388,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:04:31.295Z"
    },
    {
      "summary": "Client hangs when connecting in a new process after a client was created in the current process. The issue occurs during process forking (e.g., in gunicorn), which is not supported by the SDK due to Rust runtime limitations.",
      "category": "bug",
      "subcategory": "client-connection",
      "apis": [
        "Client.connect"
      ],
      "components": [
        "client",
        "runtime",
        "bridge"
      ],
      "concepts": [
        "process-forking",
        "multiprocessing",
        "connection-pool",
        "runtime-lifecycle",
        "concurrency"
      ],
      "severity": "high",
      "userImpact": "Users attempting to use the Temporal SDK with forking frameworks like gunicorn experience client connection hangs, preventing multi-process deployments.",
      "rootCause": "The SDK does not support process forking because the Rust Tokio runtime cannot be safely shared or recreated across fork boundaries.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "SDK limitation by design - forking is not supported. Rust runtime cannot be safely shared across forks. Documentation should clarify this limitation.",
      "related": [
        11
      ],
      "keyQuote": "The SDK does not work with forking at this time. A runtime represents a Rust Tokio thread pool and runtime.",
      "number": 385,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:04:33.605Z"
    },
    {
      "summary": "Feature request to provide the last result and last failure from previous workflow executions to workflows, allowing access to retry state information similar to features implemented in other SDKs.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "workflow-execution",
        "retry-mechanism"
      ],
      "concepts": [
        "retry",
        "failure-handling",
        "workflow-state",
        "execution-history",
        "error-recovery"
      ],
      "severity": "medium",
      "userImpact": "Users cannot access previous execution results and failures in workflows, limiting retry logic capabilities and cross-attempt state passing.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to provide last result and last failure to workflows",
      "related": [
        635
      ],
      "keyQuote": "Provide last result and last failure to workflows",
      "number": 383,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:04:31.660Z"
    },
    {
      "summary": "Transitive dependency vulnerability in rustls-webpki-0.101.4 (CVSS 7.5) causing potential CPU denial of service in certificate path building. Issue was automatically closed after the vulnerable library was removed from the dependency tree.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "tonic",
        "tokio-rustls",
        "rustls",
        "bridge"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "denial-of-service",
        "certificate-verification",
        "transitive-dependency",
        "rust-crate"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to potential CPU denial of service attacks through the Python SDK's Rust bridge due to a vulnerability in the underlying TLS certificate verification library.",
      "rootCause": "CPU denial of service in rustls-webpki certificate path building logic when processing specially crafted X.509 certificates.",
      "proposedFix": "Upgrade rustls-webpki to version 0.100.2 or 0.101.4 (fix versions per advisory GHSA-fh2r-99q2-6mmg).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue automatically closed by Mend when the vulnerable library was removed from the dependency tree or marked as ignored.",
      "related": [],
      "keyQuote": "CPU denial of service in certificate path building - Availability Impact: High - CVSS Score: 7.5",
      "number": 382,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:04:19.249Z"
    },
    {
      "summary": "Feature request to implement typed search attributes for the Python SDK, allowing developers to use type-safe search attributes in workflows. This work builds on a proposal and was started in a related issue.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "search-attributes",
        "type-system",
        "sdk-core"
      ],
      "concepts": [
        "type-safety",
        "search-attributes",
        "workflow-queries",
        "filtering",
        "indexing"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to define and use search attributes with type safety, reducing errors and improving code maintainability.",
      "rootCause": null,
      "proposedFix": "Implement typed search attributes as specified in the SDK proposal document",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented and merged into the codebase",
      "related": [
        366
      ],
      "keyQuote": "Implementation of typed search attributes. Work started at #366.",
      "number": 381,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:04:14.765Z"
    },
    {
      "summary": "The test server's time skipping feature does not work on ARM architecture. Users need documentation about this limitation and workarounds such as using Intel/AMD-to-ARM translation on macOS or non-ARM Linux.",
      "category": "docs",
      "subcategory": "test-server",
      "apis": [],
      "components": [
        "test-server",
        "time-skipping"
      ],
      "concepts": [
        "ARM architecture",
        "test infrastructure",
        "platform compatibility",
        "time manipulation",
        "testing",
        "documentation"
      ],
      "severity": "low",
      "userImpact": "ARM users cannot use time skipping in tests and must use workarounds or alternative platforms.",
      "rootCause": "Time skipping test server implementation does not support ARM architecture",
      "proposedFix": "Document the ARM limitation and available workarounds",
      "workaround": "Use auto Intel/AMD-to-ARM translation with macOS or use non-ARM Linux",
      "resolution": "fixed",
      "resolutionDetails": "Documentation was added to inform users about the ARM limitation and available workarounds",
      "related": [
        1407
      ],
      "keyQuote": "This is a known issue. In the meantime, people have to use the auto intel/amd-to-arm translation with macOS or use non-ARM Linux.",
      "number": 379,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:04:17.161Z"
    },
    {
      "summary": "Feature request to add a warning when an activity method has `self` as its first parameter but is registered as a static activity, indicating the instance wasn't properly instantiated.",
      "category": "feature",
      "subcategory": "activity-registration",
      "apis": [],
      "components": [
        "worker",
        "activity-registration",
        "validation"
      ],
      "concepts": [
        "static-activity",
        "instance-method",
        "method-signature",
        "registration-validation",
        "developer-warning"
      ],
      "severity": "low",
      "userImpact": "Developers can miss configuration errors when registering activities, leading to runtime failures that could be caught earlier with a warning.",
      "rootCause": null,
      "proposedFix": "Add validation during worker creation to warn if an activity's first parameter is `self` but it's not an instance method.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If an activity's first parameter is `self` but it's not an instance method, warn when creating the worker.",
      "number": 377,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:57.681Z"
    },
    {
      "summary": "mTLS connections fail when using EC private keys instead of PKCS8 format due to an outdated tonic dependency in sdk-core. Users must manually convert their EC private keys to PKCS8 format as a workaround.",
      "category": "bug",
      "subcategory": "mTLS-certificate-support",
      "apis": [
        "TLSConfig"
      ],
      "components": [
        "client-connection",
        "tonic-transport",
        "certificate-parsing"
      ],
      "concepts": [
        "mTLS",
        "private-key-formats",
        "EC-keys",
        "PKCS8",
        "dependency-management",
        "certificate-validation"
      ],
      "severity": "high",
      "userImpact": "Users with EC private key certificates cannot establish mTLS connections to Temporal without manually converting their keys to PKCS8 format.",
      "rootCause": "python-sdk depends on an outdated version of tonic that cannot parse EC private key format; newer tonic versions support this format natively.",
      "proposedFix": "Upgrade sdk-core to use a newer version of tonic that includes support for non-PKCS8 private key formats (fixed in tonic PR #1145).",
      "workaround": "Convert EC private keys to PKCS8 format using: openssl pkcs8 -topk8 -nocrypt -in key.pem -out key-pkcs8.pem",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by upgrading tonic dependency as part of sdk-core updates before next release.",
      "related": [],
      "keyQuote": "python-sdk is pointing to an older commit in sdk-core which depends on an older version of tonic. The key issue has been fixed in this tonic PR.",
      "number": 371,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:04:02.389Z"
    },
    {
      "summary": "Advanced metrics support feature request for the Python SDK following sdk-core PR #544. Implements missing Prometheus/OpenTelemetry options, metric buffer forwarding, and metrics access from workflows and activities.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "prometheus",
        "opentelemetry",
        "worker",
        "activity-executor",
        "workflow-executor"
      ],
      "concepts": [
        "metrics",
        "observability",
        "monitoring",
        "prometheus",
        "opentelemetry",
        "forwarding",
        "tags"
      ],
      "severity": "medium",
      "userImpact": "Enables users to access and forward metrics from workflows and activities with proper tag support for better observability.",
      "rootCause": null,
      "proposedFix": "Implement metric buffer-based forwarding, add access to metrics from runtime/activities/workflows with appropriate default tags.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature request completed through multiple PRs (#380 for missing options, #384 for runtime/activity/workflow metrics access).",
      "related": [
        380,
        384,
        544
      ],
      "keyQuote": "Support user-accessible metric buffer-based forwarding",
      "number": 369,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:59.863Z"
    },
    {
      "summary": "Activity completions may be dropped when a worker shuts down, especially if the process exits immediately after shutdown returns. This causes activity failures to not be recorded before the worker closes.",
      "category": "bug",
      "subcategory": "worker-shutdown",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "worker",
        "activity-executor",
        "shutdown"
      ],
      "concepts": [
        "graceful-shutdown",
        "activity-completion",
        "signal-handling",
        "cancellation",
        "process-exit"
      ],
      "severity": "high",
      "userImpact": "Activity failures may not be recorded if the worker shuts down and the process exits immediately, leaving workflows in an inconsistent state.",
      "rootCause": "Running activity completions may not get recorded before the activity worker closes, particularly when sys.exit() is called immediately after shutdown returns.",
      "proposedFix": "Properly finish running activities and ensure all activity completions are recorded before closing the activity worker during shutdown.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The activity completion, often a cancel failure as result of worker shutdown, may not get recorded before run/shutdown returns.",
      "number": 368,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:41.957Z"
    },
    {
      "summary": "Activity fails on SIGTERM despite graceful_shutdown_timeout being set. User suspects the graceful shutdown mechanism isn't properly preventing activity failures during rolling restarts.",
      "category": "bug",
      "subcategory": "worker-shutdown",
      "apis": [
        "shutdown"
      ],
      "components": [
        "worker",
        "signal-handler",
        "activity-executor"
      ],
      "concepts": [
        "graceful-shutdown",
        "sigterm",
        "activity-failure",
        "retry",
        "timeout",
        "polling"
      ],
      "severity": "high",
      "userImpact": "Long-running activities fail and are retried unexpectedly during rolling restarts when using graceful shutdown.",
      "rootCause": "User implementation did not properly call shutdown() on the worker; graceful_shutdown_timeout is only honored when shutdown() is explicitly invoked.",
      "proposedFix": "Call worker.shutdown() in a SIGTERM signal handler to properly stop polling and honor the graceful shutdown timeout.",
      "workaround": "Implement a SIGTERM signal handler that calls worker.shutdown() with appropriate graceful timeout.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved through documentation clarification - the user discovered they needed to explicitly call shutdown() in their signal handler rather than relying on the timeout parameter alone.",
      "related": [],
      "keyQuote": "It stops polling, notifies all activities of worker shutdown, waits the graceful timeout, then forcefully cancels activities.",
      "number": 367,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:44.328Z"
    },
    {
      "summary": "Testing environment incorrectly overrides interceptors configuration due to a variable assignment error. The code should assign config_interceptors to config[\"interceptors\"] instead of the current incorrect assignment.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "testing",
        "workflow",
        "interceptors"
      ],
      "concepts": [
        "interceptors",
        "testing-environment",
        "configuration",
        "override"
      ],
      "severity": "medium",
      "userImpact": "Users testing workflows with custom interceptors find their interceptor configuration being incorrectly overridden by the testing environment.",
      "rootCause": "Incorrect variable assignment in temporalio/testing/_workflow.py line 565 where config[\"interceptors\"] is not properly set to config_interceptors.",
      "proposedFix": "Change line 565 to assign config[\"interceptors\"] = config_interceptors",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Bug fix applied to correct the interceptor configuration assignment in the testing framework.",
      "related": [],
      "keyQuote": "should be `config[\"interceptors\"] = config_interceptors`",
      "number": 363,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:45.372Z"
    },
    {
      "summary": "OpenTelemetry baggage is not propagating properly throughout activities in the Python SDK. A test and fix are needed to ensure baggage is properly serialized/rehydrated to and within activities.",
      "category": "bug",
      "subcategory": "tracing-otel",
      "apis": [],
      "components": [
        "_TracingActivityInboundInterceptor",
        "context-propagation",
        "otel-integration"
      ],
      "concepts": [
        "baggage",
        "context-propagation",
        "opentelemetry",
        "tracing",
        "activity-execution",
        "serialization"
      ],
      "severity": "medium",
      "userImpact": "Users relying on OpenTelemetry baggage for distributed tracing lose context information when executing activities.",
      "rootCause": "Known OpenTelemetry Python bug where start_as_current_span does not automatically attach the context, causing baggage to be lost during activity execution.",
      "proposedFix": "Use opentelemetry.context.attach/detach calls in _TracingActivityInboundInterceptor to manually attach the context before executing the activity.",
      "workaround": "Manually call opentelemetry.context.attach(ctx) and opentelemetry.context.detach(token) around the activity execution to re-attach the context.",
      "resolution": "fixed",
      "resolutionDetails": "Added context attach/detach calls to _TracingActivityInboundInterceptor and wrote tests to confirm baggage propagates correctly.",
      "related": [
        3350,
        2432
      ],
      "keyQuote": "I did a quick change to _TracingActivityInboundInterceptor to add attach/detach calls, and then it works",
      "number": 362,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:23.321Z"
    },
    {
      "summary": "Type hints are not used for workflow/activity parameters if the argument count doesn't match the input count, even when extra parameters have default values. This causes parameters to be treated as dicts instead of their intended types when using UI-based workflow starting with fewer arguments.",
      "category": "feature",
      "subcategory": "type-hints",
      "apis": [
        "@workflow.run",
        "StartWorkflow"
      ],
      "components": [
        "workflow-decorator",
        "type-hint-resolver",
        "parameter-binding"
      ],
      "concepts": [
        "type-hints",
        "default-parameters",
        "pydantic",
        "type-validation",
        "parameter-matching"
      ],
      "severity": "medium",
      "userImpact": "Users experience unexpected type conversions (dict instead of model objects) when starting workflows from UI with fewer arguments than defined, leading to confusion about data converter configuration.",
      "rootCause": "The type hint validation logic requires exact argument count match, ignoring parameters with default values that don't need to be provided at call time.",
      "proposedFix": "Either use type hints when required arg count matches input count, or remove the check altogether and use as many type hints as applicable.",
      "workaround": "Manually validate dicts to models using pydantic validation or wrap the workflow to handle type conversion.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the model becomes a `dict`. So maybe this limitation worth to be mentioned in README, somewhere around `@workflow.run` and Pydantic sections.",
      "number": 360,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:25.738Z"
    },
    {
      "summary": "Feature request to improve documentation and guidance around threaded activities in Python SDK. The proposal suggests documenting blocking behavior in async code, the debug mode warning, and encouraging use of threaded activities as the default pattern unless users are certain their calls are non-blocking.",
      "category": "docs",
      "subcategory": "activity-patterns",
      "apis": [],
      "components": [
        "threaded-activities",
        "async-executor",
        "documentation",
        "quickstart"
      ],
      "concepts": [
        "async-blocking",
        "thread-safety",
        "activity-execution",
        "performance-debugging",
        "best-practices"
      ],
      "severity": "medium",
      "userImpact": "Users may inadvertently block all async code by calling blocking operations in async activities, degrading application performance without understanding the root cause.",
      "rootCause": "Lack of clear documentation and guidance on when to use threaded activities versus async activities in the Python SDK.",
      "proposedFix": "Update quickstart and samples to use threaded activities by default, document the blocking behavior of async def calls, and document set_debug(True) which warns about tasks taking longer than 0.1 seconds.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation was updated and samples were modified to use threaded activities as the recommended pattern.",
      "related": [],
      "keyQuote": "Encourage more use of threaded activities unless the user is sure their calls are non-thread-blocking",
      "number": 358,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:21.184Z"
    },
    {
      "summary": "TracingInterceptor currently creates all spans with kind=INTERNAL, which doesn't provide good service graph visualization in APM tools. This feature request changes span kinds to use CLIENT/SERVER model for better service graph representation.",
      "category": "feature",
      "subcategory": "tracing-instrumentation",
      "apis": [],
      "components": [
        "TracingInterceptor",
        "span-creation",
        "tracing"
      ],
      "concepts": [
        "distributed-tracing",
        "span-kinds",
        "APM",
        "service-graphs",
        "observability",
        "instrumentation"
      ],
      "severity": "medium",
      "userImpact": "Users get better visibility into service graphs and distributed tracing in APM tools like Tempo with proper span kind classification.",
      "rootCause": "TracingInterceptor hardcodes span kind to INTERNAL instead of using CLIENT/SERVER model based on call direction.",
      "proposedFix": "Modify TracingInterceptor to set appropriate span kinds (CLIENT or SERVER) based on whether spans represent outbound or inbound calls.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "PR was merged to implement CLIENT/SERVER span kind model for TracingInterceptor.",
      "related": [
        356
      ],
      "keyQuote": "TracingInterceptor creates all spans with kind=INTERNAL... This PR changes this to CLIENT/SERVER model for convenient service graphs in APM",
      "number": 357,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:07.128Z"
    },
    {
      "summary": "Handle signals with incorrect argument counts gracefully by logging and dropping them instead of failing the workflow task. Should validate argument count matches the signal definition's parameter requirements.",
      "category": "feature",
      "subcategory": "signal-handling",
      "apis": [],
      "components": [
        "signal-dispatcher",
        "signal-definition",
        "workflow-task"
      ],
      "concepts": [
        "signal-validation",
        "argument-matching",
        "error-handling",
        "arity-checking",
        "workflow-reliability"
      ],
      "severity": "medium",
      "userImpact": "Prevents workflows from failing due to signal arity mismatches, improving robustness when signals are sent with incorrect argument counts.",
      "rootCause": null,
      "proposedFix": "Add min/max parameter count validation to _SignalDefinition and check signal arguments against these bounds before invoking the handler.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        349
      ],
      "keyQuote": "Try to make sure there is one argument for every required parameter and no more than accepted (varargs notwithstanding).",
      "number": 355,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:04.785Z"
    },
    {
      "summary": "Feature request to add metric forwarding support and enable custom metric recording in the Python SDK, similar to capabilities in TypeScript SDK and core.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "monitoring"
      ],
      "concepts": [
        "metric-forwarding",
        "custom-metrics",
        "observability",
        "monitoring"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently forward metrics or record custom metrics in the Python SDK, limiting observability and integration with monitoring systems.",
      "rootCause": null,
      "proposedFix": "Implement metric forwarding and custom metric support similar to TypeScript SDK and core implementations",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was handled/resolved in issue #369",
      "related": [
        369,
        1177,
        544
      ],
      "keyQuote": "Need a way to, independently, support metric forwarding and allow people to create/record custom metrics.",
      "number": 354,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:03:04.665Z"
    },
    {
      "summary": "Workflow cancellation may not be properly handled when a cancel request arrives between two awaitable operations (e.g., between activity completion and child workflow start). The asyncio task may not propagate the cancellation exception at the next await point if the cancel is received while not actively awaiting.",
      "category": "bug",
      "subcategory": "cancellation",
      "apis": [
        "execute_activity",
        "execute_child_workflow"
      ],
      "components": [
        "asyncio-executor",
        "workflow-engine",
        "task-cancellation",
        "event-processing"
      ],
      "concepts": [
        "cancellation",
        "race-condition",
        "event-ordering",
        "asyncio-task",
        "workflow-execution",
        "coroutine-propagation"
      ],
      "severity": "high",
      "userImpact": "Workflows may fail to respond to cancellation requests if the cancel event arrives between consecutive awaitable operations, potentially leaving workflows in an unresponsive state.",
      "rootCause": "Asyncio task cancellation may not automatically propagate CancelledError at the next await point when the cancel is received outside of an active await operation, possibly due to reliance on the GIL assumption that asyncio only yields when awaiting a coroutine.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When a task is cancelled, asyncio.CancelledError will be raised in the task at the next opportunity.",
      "number": 352,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:50.919Z"
    },
    {
      "summary": "Request to convert all exposed modules (temporalio.activity, temporalio.workflow, etc.) into packages to better organize code across multiple files and provide clearer control over `__all__` to improve intellisense accuracy by hiding imported items.",
      "category": "feature",
      "subcategory": "module-structure",
      "apis": [],
      "components": [
        "activity",
        "workflow",
        "module-exports"
      ],
      "concepts": [
        "package-structure",
        "code-organization",
        "api-visibility",
        "intellisense",
        "imports",
        "namespace-control"
      ],
      "severity": "low",
      "userImpact": "Improves developer experience through cleaner intellisense and better code organization without affecting runtime behavior.",
      "rootCause": null,
      "proposedFix": "Convert modules like temporalio.activity and temporalio.workflow into packages with explicit `__all__` definitions to control what is exposed.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "we can control `__all__` a bit more clearly so that users' intellisense doesn't display things we've imported",
      "number": 348,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:46.958Z"
    },
    {
      "summary": "Python SDK should drop signals that cannot be deserialized, matching behavior in Go and Java SDKs. This prevents errors when signal arguments cannot be properly deserialized.",
      "category": "bug",
      "subcategory": "signal-handling",
      "apis": [],
      "components": [
        "signal-dispatcher",
        "serialization",
        "worker"
      ],
      "concepts": [
        "deserialization",
        "signal-handling",
        "error-recovery",
        "compatibility",
        "argument-validation"
      ],
      "severity": "medium",
      "userImpact": "Users may encounter unhandled errors when signals with incompatible arguments are received, instead of having them gracefully dropped like in other SDKs.",
      "rootCause": "Python SDK lacks the signal deserialization error handling that Go and Java SDKs implement to silently drop problematic signals.",
      "proposedFix": "Implement signal dropping logic for deserialization failures, consistent with Go and Java SDK behavior.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented signal dropping for deserialization errors to match Go and Java SDK behavior",
      "related": [],
      "keyQuote": "In Go and Java we drop signals whose args can't be deserialized, we should here too.",
      "number": 347,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:48.717Z"
    },
    {
      "summary": "Feature request to expose in-workflow OpenTelemetry span creation to users. The issue was quickly resolved after the author discovered the feature already exists at `temporalio.contrib.opentelemetry.workflow.completed_span`.",
      "category": "feature",
      "subcategory": "opentelemetry-integration",
      "apis": [],
      "components": [
        "opentelemetry-interceptor",
        "workflow-execution"
      ],
      "concepts": [
        "observability",
        "tracing",
        "span-creation",
        "instrumentation"
      ],
      "severity": "low",
      "userImpact": "Users can now create OpenTelemetry spans during workflow execution for better observability and tracing.",
      "rootCause": null,
      "proposedFix": "Expose the existing completed_span functionality and document its caveats (original tracer usage, immediate completion, no replay behavior).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The feature was already implemented at temporalio.contrib.opentelemetry.workflow.completed_span; the issue was a discovery that the functionality existed.",
      "related": [],
      "keyQuote": "We should expose this to users but document the caveats (e.g. that it uses the original tracer, that it is marked completed immediately, that it is not done on replay, etc).",
      "number": 345,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:35.322Z"
    },
    {
      "summary": "Type checker Pyright incorrectly reports that activities imported via `workflow.unsafe.imports_passed_through()` should be strings rather than callable functions, breaking type checking for cross-file activity imports.",
      "category": "bug",
      "subcategory": "type-checking",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "workflow",
        "type-stubs",
        "imports"
      ],
      "concepts": [
        "type-inference",
        "imports",
        "sandbox",
        "type-checking",
        "cross-module"
      ],
      "severity": "medium",
      "userImpact": "Developers using Pyright get false type errors when importing activities through the sandbox import mechanism, reducing IDE support and type safety.",
      "rootCause": "Pyright's type inference fails to properly resolve the type of functions imported within a `workflow.unsafe.imports_passed_through()` context block, treating them as module types or unions instead of the actual function type.",
      "proposedFix": null,
      "workaround": "Move activity imports to the same file as the workflow to bypass the sandbox import mechanism.",
      "resolution": "wontfix",
      "resolutionDetails": "Determined to be a Pyright type checker limitation rather than an SDK bug. The SDK itself works correctly; the issue is in how Pyright handles type inference for imports within context managers.",
      "related": [],
      "keyQuote": "I think this may be a Pyright bug, because surrounding an import with something should not change its types from a static analysis perspective.",
      "number": 338,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:33.839Z"
    },
    {
      "summary": "Add support for workflow updates in the Python SDK, allowing workflows to be updated with new code while preserving their execution state. This feature is blocked on sdk-core implementation.",
      "category": "feature",
      "subcategory": "workflow-updates",
      "apis": [],
      "components": [
        "workflow-runtime",
        "python-sdk"
      ],
      "concepts": [
        "workflow-update",
        "code-versioning",
        "state-preservation",
        "feature-parity"
      ],
      "severity": "medium",
      "userImpact": "Python SDK users cannot update workflow definitions during execution, limiting operational flexibility compared to other SDKs.",
      "rootCause": "Waiting on core SDK implementation of workflow update support (sdk-core #582)",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved when sdk-core #582 was completed, enabling Python SDK to implement workflow update support",
      "related": [
        582
      ],
      "keyQuote": "This is waiting on https://github.com/temporalio/sdk-core/issues/582",
      "number": 336,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:33.526Z"
    },
    {
      "summary": "Request for worker versioning support in the Python SDK to enable rolling deployments and code version management.",
      "category": "feature",
      "subcategory": "worker-versioning",
      "apis": [],
      "components": [
        "worker",
        "deployment"
      ],
      "concepts": [
        "versioning",
        "rolling-deployment",
        "backward-compatibility",
        "code-version",
        "worker-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Users cannot manage multiple versions of worker code simultaneously, limiting their ability to perform safe rolling deployments.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Worker versioning support was implemented in the Python SDK",
      "related": [],
      "keyQuote": null,
      "number": 335,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:15.827Z"
    },
    {
      "summary": "BaseException subclasses raised in activities are silently swallowed instead of being properly caught and handled, causing a TypeError that masks the actual error. The SDK only catches Exception subclasses by design, but should handle BaseException more explicitly rather than silently ignoring it.",
      "category": "bug",
      "subcategory": "activity-exception-handling",
      "apis": [
        "activity.defn",
        "workflow.start_activity"
      ],
      "components": [
        "activity-executor",
        "exception-handling",
        "worker"
      ],
      "concepts": [
        "exception-catching",
        "BaseException",
        "error-handling",
        "exception-propagation",
        "activity-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Users raising non-Exception-derived exceptions in activities experience silent failures with confusing error messages instead of proper error handling.",
      "rootCause": "The SDK intentionally only catches Exception subclasses, not BaseException, which causes BaseException-derived exceptions to be silently swallowed rather than explicitly handled.",
      "proposedFix": null,
      "workaround": "Derive custom exceptions from Exception instead of BaseException, or wrap third-party BaseException-derived exceptions in a try-except block within the activity.",
      "resolution": "wontfix",
      "resolutionDetails": "The issue was left open but not fixed due to Python best practices discouraging BaseException subclassing. The workaround is to use Exception subclasses instead.",
      "related": [],
      "keyQuote": "we don't want to silently swallow it either (which in effect is what is happening today), so should we crash the entire worker for a base exception?",
      "number": 333,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:20.377Z"
    },
    {
      "summary": "Local activities without an explicit schedule_to_close_timeout fail immediately with a timeout error. The SDK was setting 0 seconds execution timeout instead of null, causing premature failure.",
      "category": "bug",
      "subcategory": "local-activity-timeout",
      "apis": [
        "execute_local_activity"
      ],
      "components": [
        "local-activity-executor",
        "timeout-handling",
        "activity-options"
      ],
      "concepts": [
        "timeout",
        "schedule-to-close",
        "local-activity",
        "execution-timeout",
        "default-values"
      ],
      "severity": "high",
      "userImpact": "Local activities fail immediately when schedule_to_close_timeout is not explicitly set, breaking workflows that rely on other timeout parameters.",
      "rootCause": "Go SDK client was setting 0 seconds execution timeout instead of null, causing immediate timeout",
      "proposedFix": "Upgrade sdk-core to include the fix from sdk-core#569 that properly handles null execution timeouts",
      "workaround": "Explicitly set schedule_to_close_timeout when calling execute_local_activity",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by upgrading sdk-core to include PR #569 which resolved the issue of 0 seconds timeout being set instead of null",
      "related": [
        569
      ],
      "keyQuote": "The issue is that the Go SDK client sets 0 seconds execution timeout instead of null which meant immediate timeout. This was solved by sdk-core#569.",
      "number": 331,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:20.828Z"
    },
    {
      "summary": "Remove outdated statement about Pydantic support from the README's no-generic section. The statement claiming limited similarity to Pydantic is inaccurate since Pydantic now has limited support in the SDK.",
      "category": "docs",
      "subcategory": "readme",
      "apis": [],
      "components": [
        "documentation",
        "readme"
      ],
      "concepts": [
        "pydantic",
        "generics",
        "type-hints",
        "documentation-accuracy"
      ],
      "severity": "low",
      "userImpact": "Users reading the README may be misled about Pydantic support capabilities in the Python SDK.",
      "rootCause": null,
      "proposedFix": "Remove or update the \"similar to Pydantic\" reference in the no-generic section of README to reflect current Pydantic support.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation was updated to remove or correct the outdated Pydantic reference.",
      "related": [],
      "keyQuote": "They do have limited support now",
      "number": 328,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:04.877Z"
    },
    {
      "summary": "Replace the custom development version of pydoctor with the stable version now that support for @overload has been released in the stable branch.",
      "category": "other",
      "subcategory": "documentation-tooling",
      "apis": [],
      "components": [
        "pydoctor",
        "documentation-generation"
      ],
      "concepts": [
        "documentation",
        "tooling",
        "stability",
        "version-management",
        "overload-support"
      ],
      "severity": "low",
      "userImpact": "Users get better documentation quality and stability by using the official stable pydoctor version.",
      "rootCause": null,
      "proposedFix": "Upgrade pydoctor to the stable version that includes @overload support.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Upgraded to stable pydoctor version after @overload support was released.",
      "related": [],
      "keyQuote": "Since support for `@overload` has now been released, the stable version should be used instead.",
      "number": 327,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:03.690Z"
    },
    {
      "summary": "Request for a helper function to check if an exception represents workflow cancellation. Currently users must check multiple exception types (asyncio.CancelledError, temporalio.exceptions.CancelledError, and nested cancellation in ActivityError/ChildWorkflowError) manually.",
      "category": "feature",
      "subcategory": "exception-handling",
      "apis": [],
      "components": [
        "exception-handling",
        "workflow-runtime",
        "activity-execution"
      ],
      "concepts": [
        "cancellation",
        "exception-type-checking",
        "error-handling",
        "asyncio-integration",
        "helper-function"
      ],
      "severity": "low",
      "userImpact": "Users must write verbose exception checking logic to detect cancellation scenarios, reducing code readability and increasing error-proneness.",
      "rootCause": null,
      "proposedFix": "Add a helper function that encapsulates the cancellation detection logic shown in the issue description.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "A helper function was implemented to check for cancellation exceptions across multiple types.",
      "related": [],
      "keyQuote": "is_cancelled = isinstance(err, asyncio.CancelledError) or isinstance(err, temporalio.exceptions.CancelledError) or (isinstance(err, temporalio.exceptions.ActivityError) or isinstance(err, temporalio.exceptions.ChildWorkflowError)) and isinstance(err.cause, temporalio.exceptions.CancelledError)",
      "number": 326,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:02:02.380Z"
    },
    {
      "summary": "Two critical issues with workflow execution: GeneratorExit exceptions are caught and not properly propagated when workflows are evicted with caching disabled, and timer IDs become mismatched during workflow re-execution after eviction.",
      "category": "bug",
      "subcategory": "workflow-eviction",
      "apis": [],
      "components": [
        "workflow-executor",
        "coroutine-management",
        "timer-management",
        "event-loop"
      ],
      "concepts": [
        "coroutine-cleanup",
        "generator-exit",
        "workflow-eviction",
        "cache-management",
        "timer-synchronization",
        "resource-cleanup"
      ],
      "severity": "high",
      "userImpact": "Users experience runtime errors and workflow task failures when using workflows with caching disabled or heavy timer/signal activity.",
      "rootCause": "When a coroutine is garbage collected after workflow eviction, Python's coroutine.close() raises GeneratorExit. If a finally block attempts to run async operations (like asyncio.sleep) during this cleanup without a running event loop, it fails silently and swallows the GeneratorExit, causing Python to raise a RuntimeError. This also causes timer IDs to become out of sync in the core.",
      "proposedFix": null,
      "workaround": "Ensure finally blocks in workflows don't attempt async operations without proper event loop handling, or avoid disabling workflow cache.",
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "RuntimeError: coroutine ignored GeneratorExit - the finally block tries to run on GeneratorExit but fails with 'no running event loop'",
      "number": 325,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:49.937Z"
    },
    {
      "summary": "Python SDK fails to deserialize generic dataclasses used as type hints in activity and workflow functions. When using Generic[T] containers, the type resolver cannot properly handle parameterized types like MyContainer[int], causing TypeError during payload conversion.",
      "category": "bug",
      "subcategory": "type-hints-generics",
      "apis": [
        "activity.defn",
        "workflow.defn",
        "execute_activity"
      ],
      "components": [
        "converter",
        "payload-converter",
        "type-resolver",
        "data-converter"
      ],
      "concepts": [
        "generics",
        "type-hints",
        "dataclass",
        "serialization",
        "type-resolution",
        "JSON-serialization"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use generic dataclasses as type hints for activity inputs and workflow parameters, limiting code reusability and forcing creation of specialized non-generic dataclasses.",
      "rootCause": "The SDK's type resolver does not support generic type resolution at runtime. Python's Generic[T] syntax creates parameterized types (e.g., MyContainer[int]) that the converter cannot deserialize because the actual type parameter is lost at runtime.",
      "proposedFix": "Add support for resolving simple generic types restricted to JSON-serializable primitives (int, float, str, bool), or implement generic type resolution similar to other SDK languages.",
      "workaround": "Create concrete non-generic dataclasses for each specific use case instead of reusing generic containers (e.g., IntegerGreeting(ComposeGreetingInput) with bonus: int).",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers decided generic support is not worth the implementation complexity and confusion cost. Users should create specialized dataclasses for specific use cases instead of attempting to reuse generic ones.",
      "related": [],
      "keyQuote": "It's not that we _couldn't_ do it, it's that there is limited benefit...more confusion (the next issue will be opened asking why we don't support other generic ways and so on ad infinitum).",
      "number": 322,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:49.050Z"
    },
    {
      "summary": "Feature request to allow Temporal exceptions from data converters and codecs to fail the entire workflow rather than just the individual task. Conversion failures have been addressed, but codec failure handling requires more implementation work and community consensus.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "converter",
        "codec",
        "workflow-execution"
      ],
      "concepts": [
        "error-handling",
        "exception-propagation",
        "workflow-failure",
        "data-serialization",
        "failure-semantics"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably handle serialization/codec failures at the workflow level, limiting error recovery options and requiring workarounds.",
      "rootCause": null,
      "proposedFix": "Extend workflow-level failure semantics to codec operations, similar to the conversion handling implemented in #329.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        329
      ],
      "keyQuote": "Need to let conversion and encoding/decoding fail workflow instead of just task.",
      "number": 321,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:49.577Z"
    },
    {
      "summary": "Remove the experimental warning from Schedules API in Python SDK, bringing it in line with the Go SDK which already had this warning removed.",
      "category": "feature",
      "subcategory": "schedules",
      "apis": [
        "Schedules"
      ],
      "components": [
        "schedules",
        "api",
        "python-sdk"
      ],
      "concepts": [
        "experimental-warning",
        "api-stability",
        "feature-maturity",
        "sdk-consistency"
      ],
      "severity": "low",
      "userImpact": "Users can use Schedules without seeing experimental warnings, indicating the feature is stable and production-ready.",
      "rootCause": null,
      "proposedFix": "Remove the experimental warning annotation/documentation from Schedules, following the pattern already implemented in Go SDK",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Experimental warning was removed from Schedules in Python SDK to match Go SDK changes",
      "related": [],
      "keyQuote": "Already removed in Go at https://github.com/temporalio/sdk-go/pull/1064.",
      "number": 320,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:33.665Z"
    },
    {
      "summary": "The `workflow.continue_as_new()` method fails when `task_queue` and `workflow` arguments are omitted, causing a 'Missing TaskQueue' error and workflow to hang. The docstring suggests these should default to the current workflow's values.",
      "category": "bug",
      "subcategory": "workflow-continuation",
      "apis": [
        "workflow.continue_as_new"
      ],
      "components": [
        "workflow-executor",
        "core-sdk",
        "test-framework"
      ],
      "concepts": [
        "continue-as-new",
        "task-queue",
        "workflow-defaults",
        "parameter-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use continue_as_new without explicitly passing task_queue and workflow parameters, limiting the convenience of the API.",
      "rootCause": "Known issue with the time-skipping test server (related to temporalio/sdk-java#1424)",
      "proposedFix": null,
      "workaround": "Explicitly pass task_queue and workflow parameters: `task_queue=workflow.info().task_queue, workflow=Workflow.run`",
      "resolution": "duplicate",
      "resolutionDetails": "Closed as duplicate of a known issue with the time-skipping test server reported in sdk-java#1424",
      "related": [
        1424
      ],
      "keyQuote": "When these arguments are not passed, I get a log error like: Missing TaskQueue and my workflow hangs",
      "number": 319,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:32.037Z"
    },
    {
      "summary": "When the `self` parameter is type-hinted (e.g., `self: Self`), it is not properly ignored during parameter deserialization, causing it to be incorrectly included in the deserialized arguments.",
      "category": "bug",
      "subcategory": "serialization",
      "apis": [],
      "components": [
        "common",
        "deserialization",
        "type-hints"
      ],
      "concepts": [
        "parameter-deserialization",
        "self-parameter",
        "type-hints",
        "type-annotation",
        "method-signature"
      ],
      "severity": "high",
      "userImpact": "Users with type-hinted self parameters in their workflow/activity methods will experience deserialization failures or incorrect parameter binding.",
      "rootCause": "The deserialization logic in temporalio/common.py#L235 does not account for type-hinted self parameters and only ignores untyped self.",
      "proposedFix": "Improve the logic to detect and ignore self parameters regardless of whether they have type hints.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We need to see if we can be smarter about ignoring `self`.",
      "number": 318,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:32.470Z"
    },
    {
      "summary": "Using await in async query functions causes the query to return immediately with an empty response error. Queries are read-only operations that cannot contain blocking code or asynchronous workflow calls like wait_condition or timers.",
      "category": "bug",
      "subcategory": "query-async",
      "apis": [
        "query",
        "wait_condition"
      ],
      "components": [
        "query-decorator",
        "async-runtime",
        "workflow-execution"
      ],
      "concepts": [
        "query-semantics",
        "async-functions",
        "blocking-operations",
        "workflow-history",
        "read-only-constraints"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use await or asynchronous workflow operations in query functions, leading to runtime errors when they attempt to do so.",
      "rootCause": "Queries must be immediate read-only operations. Asynchronous workflow calls like wait_condition and timers write events to the workflow history, which violates query semantics. The SDK allows async def for queries to support fast async Python library calls, but this creates confusion about what async operations are allowed.",
      "proposedFix": "Add more comprehensive documentation to clarify that async def queries can only use fast, non-blocking async calls. Consider adding a static analyzer or runtime warning to catch async workflow operations in queries.",
      "workaround": "Use synchronous query functions instead of async def, or refactor to avoid await calls within queries.",
      "resolution": "wontfix",
      "resolutionDetails": "Closed as expected behavior. Queries are designed to be read-only and immediate. The issue was user misunderstanding of what async def allows in queries.",
      "related": [],
      "keyQuote": "Queries have to be immediate and read-only. They cannot call asynchronous `workflow` module methods.",
      "number": 317,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:19.385Z"
    },
    {
      "summary": "The ScheduleState.limited_actions documentation incorrectly states it's not allowed on create, when it should be required for remaining_actions to take effect. The API needs validation to ensure limited_actions and remaining_actions fields are consistent on creation.",
      "category": "bug",
      "subcategory": "schedule-api",
      "apis": [
        "ScheduleState"
      ],
      "components": [
        "schedule",
        "api-validation",
        "documentation"
      ],
      "concepts": [
        "schedule-creation",
        "field-validation",
        "API-consistency",
        "remaining-actions",
        "limited-actions",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users receive incorrect documentation about schedule creation requirements and may face unexpected validation errors if field values are inconsistent.",
      "rootCause": "Documentation inaccuracy combined with missing validation logic to enforce consistency between limited_actions and remaining_actions fields.",
      "proposedFix": "Error on create if (limited_actions == True) != (remaining_actions > 0), while allowing mismatched fields on update.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation corrected and validation logic implemented to enforce field consistency on schedule creation.",
      "related": [],
      "keyQuote": "Error _on create_ if (limited_actions == True) != (remaining_actions > 0). Can allow the mismatched fields on update though just in case they only want to update one.",
      "number": 316,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:14.594Z"
    },
    {
      "summary": "Feature request to support custom options (especially ignoring unknown fields) when constructing JSONProtoPayloadConverter, matching the Go SDK's ProtoJSONPayloadConverterOptions API.",
      "category": "feature",
      "subcategory": "payload-converter",
      "apis": [],
      "components": [
        "JSONProtoPayloadConverter",
        "payload-converter"
      ],
      "concepts": [
        "protobuf",
        "json-serialization",
        "payload-conversion",
        "unknown-fields",
        "converter-options"
      ],
      "severity": "medium",
      "userImpact": "Users currently cannot configure JSONProtoPayloadConverter options like ignoring unknown fields without creating a custom converter.",
      "rootCause": null,
      "proposedFix": "Add support for passing google.protobuf.json_format.Parse options to JSONProtoPayloadConverter constructor, mirroring the Go SDK's ProtoJSONPayloadConverterOptions.",
      "workaround": "Users can create a custom payload converter instead of using the built-in JSONProtoPayloadConverter.",
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to allow passing custom parse options to JSONProtoPayloadConverter.",
      "related": [],
      "keyQuote": "Really, what we want is compatibility with the go.temporal.io/sdk/converter#ProtoJSONPayloadConverterOptions options.",
      "number": 315,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:16.181Z"
    },
    {
      "summary": "User reported that retry policy's max_interval parameter is not being respected when passed to start_workflow, with intervals doubling unexpectedly. The maintainer was unable to replicate the issue and closed it after the reporter didn't respond with additional details.",
      "category": "bug",
      "subcategory": "retry-policy",
      "apis": [
        "start_workflow",
        "RetryPolicy"
      ],
      "components": [
        "workflow-client",
        "retry-handler"
      ],
      "concepts": [
        "retry-policy",
        "backoff-coefficient",
        "maximum-interval",
        "exponential-backoff"
      ],
      "severity": "medium",
      "userImpact": "Users configuring retry policies with strict max_interval limits may experience unexpected retry behavior with doubling intervals.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Unable to replicate - maintainer tested the reported behavior and it worked as expected. Issue closed without fix as unreproducible and unresponded.",
      "related": [],
      "keyQuote": "This retries every second as expected, it does not double it. Make sure you are not observing an older workflow still running",
      "number": 313,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:01.640Z"
    },
    {
      "summary": "User requested runtime type validation in RetryPolicy and similar validate methods to catch cases where timedelta parameters are passed as integers, providing helpful error messages to guide developers.",
      "category": "feature",
      "subcategory": "validation",
      "apis": [
        "RetryPolicy"
      ],
      "components": [
        "common",
        "client",
        "validation"
      ],
      "concepts": [
        "type-checking",
        "error-messages",
        "developer-experience",
        "parameter-validation",
        "timedelta"
      ],
      "severity": "low",
      "userImpact": "Without runtime type validation, developers can pass incorrect types (e.g., integers instead of timedeltas) and receive cryptic errors instead of helpful guidance.",
      "rootCause": "RetryPolicy validation methods do not check parameter types at runtime, leading to unclear error messages when wrong types are passed.",
      "proposedFix": "Add type checks in validate methods to verify that timedelta parameters are actually timedelta objects, with clear error messages directing users to documentation.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers determined that runtime type checking for hundreds of parameters is unreasonable; Python type hints and static type checkers like MyPy are the intended solution.",
      "related": [],
      "keyQuote": "I think it's a bit unreasonable to check the types of the hundreds of parameters/fields we have at runtime. We offer type hints and strongly encourage the use of MyPy or similar.",
      "number": 312,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:00:59.924Z"
    },
    {
      "summary": "Python SDK lacks log forwarding support for Rust core logs, making it difficult to monitor and aggregate logs in unified logging systems. The Rust core logs use different formatting (with ANSI colors) compared to Python worker logs, and there's a need to forward these logs back to Python for consistent handling.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "runtime",
        "logging",
        "sdk-core-bridge"
      ],
      "concepts": [
        "log-forwarding",
        "structured-logging",
        "rust-core-integration",
        "log-formatting",
        "monitoring",
        "observability"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily integrate Rust core logs into their unified logging infrastructure or JSON-structured logging systems, making monitoring and debugging harder in containerized environments.",
      "rootCause": "Python SDK does not implement log forwarding from the Rust core (tokio-rs/tracing based) back to Python logging, unlike TypeScript SDK which has this capability.",
      "proposedFix": "Forward buffered logs from the Rust core to Python logging, similar to how the TypeScript SDK handles core logs, potentially leveraging log buffering and pushing mechanism from sdk-core.",
      "workaround": "Set environment variables like RUST_LOG_COLORS=never or TERM=dumb to disable ANSI colors in Rust logs, though this requires sdk-core runtime configuration changes.",
      "resolution": "wontfix",
      "resolutionDetails": "The resolution was deferred pending completion of sdk-core issue #618 to enable buffered log forwarding. The team decided to wait for the core infrastructure to support buffered, pushed logs sent to Python logging.",
      "related": [
        529,
        618
      ],
      "keyQuote": "The sdk could support passing back core logs to python like it does for typescript...but maybe worth clarifying why?",
      "number": 311,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:01:02.251Z"
    },
    {
      "summary": "temporalio cannot be installed in Alpine Linux containers because protoc-wheel-0 is not available as a pre-built wheel for musl libc. The package's build process requires protoc-wheel-0 as a build dependency, but pip cannot locate it even after manual installation in an Alpine environment.",
      "category": "bug",
      "subcategory": "installation",
      "apis": [],
      "components": [
        "build-system",
        "packaging",
        "protoc-wheel-0",
        "poetry"
      ],
      "concepts": [
        "alpine-linux",
        "musl-libc",
        "binary-wheels",
        "build-dependencies",
        "ci-pipeline",
        "docker-container"
      ],
      "severity": "high",
      "userImpact": "Users cannot install the temporalio SDK in Alpine Linux-based Docker containers, forcing them to use alternative Linux distributions or perform complex manual builds.",
      "rootCause": "protoc-wheel-0 is only distributed as a pre-built wheel for glibc systems. Alpine Linux uses musl libc, which is incompatible with glibc-based wheels. The poetry build process treats protoc-wheel-0 as a build dependency and searches PyPI in an isolated subprocess that cannot access manually installed wheels.",
      "proposedFix": "Build protoc-wheel-0 for musl/Alpine or provide an alternative installation method that doesn't require the pre-built wheel, allowing users to supply their own protoc binary via PATH.",
      "workaround": "Use a different Linux distribution instead of Alpine, or manually build the SDK with MUSL support and a custom protoc installation while disabling the protoc-wheel-0 dependency.",
      "resolution": "wontfix",
      "resolutionDetails": "The maintainers acknowledge the limitation and suggest users either switch distros or manually build with custom protoc setup, as supporting Alpine/musl is not a default use case.",
      "related": [],
      "keyQuote": "You can attempt to build the SDK yourself with MUSL for the Rust part and it will likely work fine. We just don't distribute one.",
      "number": 309,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:00:43.460Z"
    },
    {
      "summary": "Import of temporalio module fails with ModuleNotFoundError for 'temporalio.bridge.temporal_sdk_bridge'. The native bridge module is not built during standard installation, requiring explicit build steps.",
      "category": "bug",
      "subcategory": "build-system",
      "apis": [],
      "components": [
        "bridge",
        "build-system",
        "native-extension"
      ],
      "concepts": [
        "module-loading",
        "native-build",
        "installation",
        "development-setup",
        "cargo-build"
      ],
      "severity": "high",
      "userImpact": "Users cannot import temporalio without running manual build steps, blocking SDK usage entirely.",
      "rootCause": "Native Rust bridge module (temporal_sdk_bridge) is not built during standard poetry install; requires poe build-develop and recursive git clone.",
      "proposedFix": "Improve documentation to clarify that poe build-develop must be run after poetry install for SDK development, and that repo must be cloned recursively.",
      "workaround": "Run 'poe build-develop' and ensure repo is cloned with --recursive flag.",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved through documentation clarification and user error identification - users need to follow complete build instructions including recursive clone and poe build-develop.",
      "related": [],
      "keyQuote": "Did you run `poe build-develop` per https://github.com/temporalio/sdk-python?tab=readme-ov-file#local-sdk-development-environment?",
      "number": 306,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:00:38.633Z"
    },
    {
      "summary": "The README logo image URL needs to be fully qualified so it will display correctly on PyPI package pages instead of only working on GitHub.",
      "category": "feature",
      "subcategory": "documentation",
      "apis": [],
      "components": [
        "documentation",
        "package-metadata"
      ],
      "concepts": [
        "image-url",
        "pypi-display",
        "package-publishing",
        "readme-rendering"
      ],
      "severity": "low",
      "userImpact": "Users viewing the Python SDK on PyPI cannot see the project logo due to relative URL references.",
      "rootCause": "README uses relative image URL that works on GitHub but not when rendered on PyPI.",
      "proposedFix": "Use fully qualified URL for the logo image in README.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Logo was uploaded to a CDN to provide a fully qualified URL.",
      "related": [],
      "keyQuote": "@lorensr is working on uploading the logo to a CDN.",
      "number": 302,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:00:38.788Z"
    },
    {
      "summary": "Proxied classes in workflows are raising 'unhashable type' errors for datetime objects despite having `__hash__` defined, indicating a problem with the proxy implementation's hashability.",
      "category": "bug",
      "subcategory": "proxying",
      "apis": [],
      "components": [
        "proxy-system",
        "workflow-execution",
        "type-handling"
      ],
      "concepts": [
        "hashability",
        "object-identity",
        "proxy-pattern",
        "workflow-constraints",
        "type-wrapping",
        "serialization"
      ],
      "severity": "high",
      "userImpact": "Users encounter runtime errors when datetime or other objects are used in workflows due to improper proxy class hashing.",
      "rootCause": "Proxied classes do not properly implement or preserve the `__hash__` method, causing hashability failures even when defined.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Getting `unhashable type: '_RestrictedProxy'` possibly on a datetime in a workflow even though we define `__hash__`",
      "number": 301,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:00:20.030Z"
    },
    {
      "summary": "PyGILState_Release error occurs in short-lived client-only processes after execute_workflow completes, possibly related to PyO3's GIL handling during process shutdown or when the Tokio runtime attempts to call Python callbacks after the Python runtime is finalizing.",
      "category": "bug",
      "subcategory": "client-lifecycle",
      "apis": [
        "Client.connect",
        "start_workflow_execution"
      ],
      "components": [
        "client",
        "pyo3-bridge",
        "tokio-runtime"
      ],
      "concepts": [
        "GIL",
        "process-shutdown",
        "async-callbacks",
        "thread-safety",
        "runtime-finalization"
      ],
      "severity": "high",
      "userImpact": "Users experience fatal crashes with PyGILState_Release errors when client processes exit, particularly with short-lived workflows or background tasks.",
      "rootCause": "PyO3-asyncio calls Python::with_gil in callbacks during runtime finalization, but PyGILState_Ensure terminates threads when the Python runtime is finalizing, causing a fatal error.",
      "proposedFix": "Rewrite or vendor pyo3-asyncio to avoid calling Python::with_gil during callbacks, or check for runtime finalization before attempting GIL acquisition.",
      "workaround": "Avoid gracefully exiting the program while the client is in use; ensure all client operations complete before shutdown.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1274
      ],
      "keyQuote": "Calling this function from a thread when the runtime is finalizing will terminate the thread, even if the thread was not created by Python.",
      "number": 300,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:00:24.417Z"
    },
    {
      "summary": "Update Tokio dependency to version 1.25 or 1.26 to address a reported vulnerability in the Python SDK's Rust bridge. The fix can be applied via cargo update.",
      "category": "bug",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "python-bridge",
        "rust-bindings",
        "tokio-dependency"
      ],
      "concepts": [
        "vulnerability",
        "dependency-update",
        "security-patch",
        "cargo-management"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to a security vulnerability that requires updating the Tokio dependency to maintain a secure SDK.",
      "rootCause": "Outdated Tokio dependency version containing a reported vulnerability",
      "proposedFix": "Update Tokio to version 1.25 or 1.26 via cargo update in the Python bridge",
      "workaround": "Cargo update can be run to fix the dependency version",
      "resolution": "fixed",
      "resolutionDetails": "Fixed via cargo update in the Python bridge and merged in PR #307",
      "related": [
        244,
        307
      ],
      "keyQuote": "we should probably set a minimum Tokio version in this repo to 1.25 or 1.26",
      "number": 299,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:00:21.341Z"
    },
    {
      "summary": "Update bzip2 dependency to address CVE-2023-22895 vulnerability in the Rust bzip2 crate. The vulnerability is transitive through temporal-sdk-core -> zip -> bzip2 dependency chain.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "temporal-sdk-core",
        "dependency-management",
        "rust-bridge"
      ],
      "concepts": [
        "security-vulnerability",
        "dependency-update",
        "cve",
        "transitive-dependency",
        "cargo-management"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to CVE-2023-22895 security vulnerability through transitive dependencies in the SDK.",
      "rootCause": "bzip2-0.4.3.crate contains a known vulnerability (CVE-2023-22895); requires explicit update via cargo",
      "proposedFix": "Update bzip2 dependency via `cargo update` in Python bridge; PR #307 implements this fix",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by updating bzip2 dependency through cargo update in the Rust bridge, merged in PR #307",
      "related": [
        61
      ],
      "keyQuote": "Note, per https://github.com/zip-rs/zip/issues/335#issuecomment-1411543375 we will have to explicitly update bzip2 ourselves",
      "number": 298,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:00:07.032Z"
    },
    {
      "summary": "Update the remove_dir_all dependency to address a security vulnerability (GHSA-mc8h-8q98-g5hr) in the transitive dependency chain. The vulnerable library is remove_dir_all-0.5.3 which is required by tempfile through the Rust build chain.",
      "category": "bug",
      "subcategory": "dependency-vulnerability",
      "apis": [],
      "components": [
        "dependencies",
        "build-system",
        "rust-bridge"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "dependency-update",
        "transitive-dependency",
        "cargo"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to a security vulnerability through a transitive dependency that could be exploited if the vulnerable code path is triggered.",
      "rootCause": "Outdated remove_dir_all-0.5.3 dependency in the transitive dependency chain through tempfile-3.3.0, prost-build-0.11.1, and tonic-build-0.8.2.",
      "proposedFix": "Run `cargo update` in the Python bridge to update the vulnerable transitive dependencies to patched versions.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed via cargo update in Python bridge, merged in PR #307.",
      "related": [
        54
      ],
      "keyQuote": "This can be fixed via `cargo update` in Python bridge",
      "number": 297,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:00:04.563Z"
    },
    {
      "summary": "Update bumpalo dependency to fix a security vulnerability (RUSTSEC-2022-0078) in the dependency chain of temporal-sdk-core-api. The vulnerable bumpalo-3.11.0 is transitive dependency through multiple layers of Rust crates.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "temporal-sdk-core-api",
        "rust-dependencies",
        "bumpalo"
      ],
      "concepts": [
        "security-vulnerability",
        "dependency-management",
        "supply-chain",
        "rust-crates",
        "transitive-dependencies"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to a known security vulnerability through transitive dependencies in the Python SDK.",
      "rootCause": "Outdated bumpalo-3.11.0 crate in the dependency chain of temporal-sdk-core-api with known vulnerability RUSTSEC-2022-0078.",
      "proposedFix": "Run `cargo update` in the Python bridge to update bumpalo and fix the vulnerability.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed via PR #307 by running cargo update in the Python bridge.",
      "related": [
        54,
        307
      ],
      "keyQuote": "This can be fixed via `cargo update` in Python bridge",
      "number": 296,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T21:00:02.576Z"
    },
    {
      "summary": "Request for dependency injection support in Python SDK using pinject to manage scoped dependencies (global, customer-specific) for workflows and activities, similar to Spring Boot support in Java SDK.",
      "category": "feature",
      "subcategory": "dependency-injection",
      "apis": [],
      "components": [
        "workflow",
        "activity",
        "worker"
      ],
      "concepts": [
        "dependency-injection",
        "scoping",
        "configuration-management",
        "resource-management",
        "pinject"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily manage scoped dependencies like database connections and authentication data in their workflows and activities.",
      "rootCause": null,
      "proposedFix": "Add support for pinject dependency injection framework in Python SDK, similar to Spring Boot support in Java SDK (issue #745)",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed as duplicate; discussion moved to community forum at https://community.temporal.io/t/python-support-for-dependency-injection-pinject/7538",
      "related": [
        745
      ],
      "keyQuote": "we need the ability to inject dependencies such as grpc connections, db connections, kafka connections, auth data into temporal workflows and activities",
      "number": 295,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:49.582Z"
    },
    {
      "summary": "Request to add an API to count workflows in the Python SDK. This is a feature request related to a parent issue with more context.",
      "category": "feature",
      "subcategory": "workflow-query",
      "apis": [
        "CountWorkflows"
      ],
      "components": [
        "workflow-client",
        "query-api"
      ],
      "concepts": [
        "workflow-counting",
        "query",
        "visibility",
        "statistics"
      ],
      "severity": "low",
      "userImpact": "Users need a programmatic way to count workflows without listing all of them.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented in the Python SDK",
      "related": [],
      "keyQuote": "Add API to count workflows",
      "number": 294,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:45.628Z"
    },
    {
      "summary": "Add documentation clarification that activity_id parameter should not be set in most cases, requiring Temporal team confirmation before use. This aligns with Go SDK documentation practices.",
      "category": "docs",
      "subcategory": "activity-configuration",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "activity-executor",
        "documentation",
        "worker"
      ],
      "concepts": [
        "activity-id",
        "best-practices",
        "documentation",
        "configuration",
        "guidance"
      ],
      "severity": "low",
      "userImpact": "Users may incorrectly set activity_id without understanding it should rarely be used, potentially causing issues that could be avoided with clear documentation.",
      "rootCause": null,
      "proposedFix": "Add documentation similar to Go SDK clarifying that activity_id should not be set in most cases and users should contact Temporal team if they believe they need to set it.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation was updated to clarify activity_id parameter usage recommendations, aligning with Go SDK guidance.",
      "related": [],
      "keyQuote": "Go SDK docs say activity ID should not be set in most cases and to contact Temporal team to confirm you need to set this. We should do the same here.",
      "number": 293,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:47.774Z"
    },
    {
      "summary": "Request to support dynamic loading of activity and workflow implementations at runtime, similar to Java class loader mechanisms. User needs an extension point/factory interface to provide untyped implementations dynamically rather than registering them statically.",
      "category": "feature",
      "subcategory": "dynamic-loading",
      "apis": [],
      "components": [
        "worker",
        "activity-registry",
        "workflow-registry"
      ],
      "concepts": [
        "dynamic-loading",
        "factory-pattern",
        "runtime-registration",
        "extension-point",
        "plugin-system",
        "type-resolution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot dynamically load or register activity and workflow implementations at runtime, limiting flexibility in certain deployment scenarios.",
      "rootCause": null,
      "proposedFix": "Expose an extension point or factory interface that is called with activity or workflow type to provide an untyped implementation.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed as duplicate of issue #242",
      "related": [
        242,
        245
      ],
      "keyQuote": "Expose an extension point for dynamic activity loading. While the SDK might not provide the explicit loading mechanism it should provide a factory interface",
      "number": 292,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:32.643Z"
    },
    {
      "summary": "Add a disclaimer warning users that gevent's monkey patching is incompatible with asyncio and the SDK's custom event loop, until the underlying issue is resolved.",
      "category": "docs",
      "subcategory": "compatibility-warning",
      "apis": [],
      "components": [
        "event-loop",
        "asyncio",
        "gevent"
      ],
      "concepts": [
        "monkey-patching",
        "compatibility",
        "asyncio",
        "event-loop",
        "third-party-integration"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to use gevent with the Python SDK may encounter unexpected behavior or failures due to incompatible event loop modifications.",
      "rootCause": "Gevent's monkey patching interferes with asyncio and the SDK's custom event loop implementation",
      "proposedFix": "Add a disclaimer documenting the gevent incompatibility",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "A disclaimer was added to document the gevent incompatibility",
      "related": [
        59
      ],
      "keyQuote": "Until #59 is solved, we should call out gevent's monkey patching messes up asyncio and our custom event loop",
      "number": 291,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:33.986Z"
    },
    {
      "summary": "Feature request to support result_type parameter on child workflows and activities in the Python SDK, similar to the existing Client.start_workflow functionality. This would allow type-safe result handling across different workflow invocation patterns.",
      "category": "feature",
      "subcategory": "type-safety",
      "apis": [
        "start_workflow",
        "execute_activity",
        "execute_child_workflow"
      ],
      "components": [
        "workflow-client",
        "activity-executor",
        "child-workflow-invoker"
      ],
      "concepts": [
        "type-safety",
        "result-handling",
        "workflow-invocation",
        "activity-execution",
        "child-workflows"
      ],
      "severity": "medium",
      "userImpact": "Users cannot specify result types for activities and child workflows, reducing type safety and consistency across different Temporal API usage patterns.",
      "rootCause": null,
      "proposedFix": "Extend result_type parameter support from Client.start_workflow to child workflow execution and activity execution methods.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to support result_type on child workflows and activities, bringing parity with Client.start_workflow.",
      "related": [],
      "keyQuote": "Client.start_workflow accepts this for string-name-based workflows, we should do the same for activities and child workflows when invoked from workflows.",
      "number": 290,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:33.216Z"
    },
    {
      "summary": "JSONTypeConverter is not being passed through recursive calls in the value_to_type function, causing JSON type conversion to fail on nested data structures.",
      "category": "bug",
      "subcategory": "type-conversion",
      "apis": [],
      "components": [
        "value_to_type",
        "JSONTypeConverter",
        "type-conversion"
      ],
      "concepts": [
        "recursion",
        "type-conversion",
        "JSON-handling",
        "data-serialization",
        "nested-structures"
      ],
      "severity": "medium",
      "userImpact": "Users working with complex nested JSON data structures may experience incorrect type conversion when using custom JSONTypeConverter implementations.",
      "rootCause": "Recursive calls within value_to_type function do not pass the JSON type converters as parameters, causing nested values to bypass custom conversion logic.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The recursive calls in value_to_type were updated to properly pass JSON type converters through the call chain.",
      "related": [],
      "keyQuote": "Recursive calls in `value_to_type` are not passing the JSON type converters.",
      "number": 289,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:18.162Z"
    },
    {
      "summary": "Code crashes with AttributeError when trying to access HistoryEventFilterType.ValueType enum value. The issue occurs during workflow history event fetching in the multiprocess activity example. User resolved it by upgrading to protobuf 3.20.0+, which introduced the missing ValueType attribute.",
      "category": "bug",
      "subcategory": "protobuf-compatibility",
      "apis": [
        "execute_workflow"
      ],
      "components": [
        "client",
        "history-events",
        "protobuf-binding"
      ],
      "concepts": [
        "enum-handling",
        "dependency-versioning",
        "protobuf-version-compatibility",
        "workflow-result-retrieval"
      ],
      "severity": "medium",
      "userImpact": "Users running the multiprocess activity example with older protobuf versions encounter crashes when executing workflows and retrieving history.",
      "rootCause": "Protobuf enum ValueType attribute was added in protobuf 3.20.0. SDK code attempted to use this attribute without enforcing minimum protobuf version requirement.",
      "proposedFix": "Upgrade protobuf to version 3.20.0 or later",
      "workaround": "Manually upgrade protobuf dependency to 3.20.0+",
      "resolution": "fixed",
      "resolutionDetails": "User confirmed that upgrading to protobuf 3.20.0 resolved the issue. Root cause was identified as missing protobuf version requirement.",
      "related": [],
      "keyQuote": "That was it. I upgraded to version 3.20.0 and that resolved the issue.",
      "number": 288,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:16.910Z"
    },
    {
      "summary": "Worker's `_started` flag is not reset after shutdown, causing \"Already started\" error when attempting to run the worker again. However, workers are designed as single-use by intention, and users should create new worker instances instead of reusing them.",
      "category": "question",
      "subcategory": "worker-lifecycle",
      "apis": [
        "worker.shutdown()",
        "worker.run()"
      ],
      "components": [
        "worker",
        "worker-lifecycle"
      ],
      "concepts": [
        "worker-reuse",
        "state-reset",
        "lifecycle-management",
        "shutdown",
        "initialization"
      ],
      "severity": "low",
      "userImpact": "Users attempting to reuse workers after shutdown encounter errors; however, this is not supported by design and users should create new worker instances instead.",
      "rootCause": "The `_started` flag is set to True during worker startup but not reset during shutdown, preventing reuse of the same worker instance.",
      "proposedFix": null,
      "workaround": "Create a new worker instance with the same configuration instead of reusing the existing worker after shutdown.",
      "resolution": "wontfix",
      "resolutionDetails": "Closed as not a bug - workers are intentionally single-use to prevent confusion and misuse. Users should create new worker instances rather than attempting to restart existing ones.",
      "related": [],
      "keyQuote": "Workers are single use by intention to prevent confusion and misuse. You can create another worker with the same config and call `run` on it.",
      "number": 286,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:18.581Z"
    },
    {
      "summary": "The WorkflowReplayResult and WorkflowReplayResults classes are not publicly visible because they're defined in a private module and need to be re-exported from the main temporalio.worker module.",
      "category": "bug",
      "subcategory": "api-visibility",
      "apis": [
        "WorkflowReplayResult",
        "WorkflowReplayResults"
      ],
      "components": [
        "worker",
        "module-exports"
      ],
      "concepts": [
        "workflow-replay",
        "api-visibility",
        "module-structure",
        "re-export"
      ],
      "severity": "medium",
      "userImpact": "Users cannot access WorkflowReplayResult/WorkflowReplayResults classes because they are not publicly exposed from the worker module.",
      "rootCause": "Classes are defined in a private module without being re-exported from the public API surface.",
      "proposedFix": "Re-export WorkflowReplayResult and WorkflowReplayResults from the temporalio.worker module.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Classes were re-exported from the public API module to make them accessible to users.",
      "related": [],
      "keyQuote": "Need to re-export from private module",
      "number": 285,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:00.047Z"
    },
    {
      "summary": "Documentation should clarify that heartbeat timeout is a required configuration in addition to the heartbeat mechanism itself. This is a documentation enhancement to improve guidance for developers.",
      "category": "docs",
      "subcategory": "activity-heartbeat",
      "apis": [],
      "components": [
        "documentation",
        "activity-executor",
        "heartbeat-system"
      ],
      "concepts": [
        "heartbeat",
        "timeout",
        "configuration",
        "requirements",
        "activity-lifecycle"
      ],
      "severity": "low",
      "userImpact": "Developers may misunderstand heartbeat requirements and misconfigure activities without proper timeout settings.",
      "rootCause": null,
      "proposedFix": "Update documentation to explicitly state that heartbeat timeout is a required configuration alongside heartbeat.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation was updated to clarify heartbeat timeout requirements.",
      "related": [],
      "keyQuote": "Docs in addition to saying heartbeat is required need to say heartbeat timeout is required",
      "number": 282,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:59:01.531Z"
    },
    {
      "summary": "The types-protobuf dependency is pinned to 3.x while the SDK supports protobuf 4.x. Updating the version constraint from ~3.20 to >=3.20 would resolve typings conflicts with google-cloud-sdk and align with protobuf 4.x support.",
      "category": "bug",
      "subcategory": "dependencies",
      "apis": [],
      "components": [
        "protobuf-types",
        "dependency-management"
      ],
      "concepts": [
        "type-hints",
        "version-compatibility",
        "protobuf",
        "type-stubs"
      ],
      "severity": "medium",
      "userImpact": "Users cannot upgrade types-protobuf to 4.x due to the strict version constraint, causing typing conflicts when using google-cloud-sdk alongside the SDK.",
      "rootCause": "types-protobuf version constraint uses ~3.20 (tilde constraint) instead of >=3.20, which unnecessarily restricts to 3.x versions despite the SDK supporting protobuf 4.x.",
      "proposedFix": "Change types-protobuf constraint from ~3.20 to >=3.20 in the dependency specification.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The version constraint was updated to allow types-protobuf 4.x, resolving the typings conflict with google-cloud-sdk.",
      "related": [],
      "keyQuote": "using types-protobuf 3.x overrides google cloud SDK typings, so I want to upgrade",
      "number": 277,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:58:59.923Z"
    },
    {
      "summary": "The Python SDK's logging adapter doesn't properly merge extra attributes passed to log calls. When using `extra` parameters in workflow and activity logging, a KeyError is raised because the adapter doesn't preserve these attributes through the logging context.",
      "category": "bug",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "logging-adapter",
        "activity-logging",
        "workflow-logging"
      ],
      "concepts": [
        "structured-logging",
        "context-propagation",
        "log-formatting",
        "extra-attributes",
        "logger-adapter"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use Python's standard logging `extra` parameter with workflow and activity loggers without encountering KeyError exceptions.",
      "rootCause": "The LoggerAdapter's process() method was not properly merging extra attributes from both the adapter instance and the log call, causing extra data to be lost in the logging context.",
      "proposedFix": "Support a merge_extra parameter in the LoggerAdapter initializer to control whether to merge adapter-level extra attributes with call-level extra attributes, similar to Python 3.13's implementation. For older Python versions, document subclassing as the recommended approach.",
      "workaround": "Subclass the LoggerAdapter and override the process() method to manually merge extra attributes as needed.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by removing the super call in process() method (PR #280), then later addressed the merge_extra concern by documenting subclassing approach (issue #892).",
      "related": [
        280,
        892
      ],
      "keyQuote": "Logging a record with extra attribute \"foo\" here causes a \"KeyError\" exception because it can't find the attribute.",
      "number": 276,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:58:44.981Z"
    },
    {
      "summary": "User requested a way to access information about the invoking workflow from within an activity method. The issue was resolved by pointing to the existing temporalio.activity.info() function which provides workflow context.",
      "category": "question",
      "subcategory": "activity-context",
      "apis": [
        "activity.info",
        "activity.defn"
      ],
      "components": [
        "activity",
        "activity-executor",
        "context-propagation"
      ],
      "concepts": [
        "workflow-context",
        "activity-metadata",
        "invocation-info",
        "activity-information",
        "context-access"
      ],
      "severity": "low",
      "userImpact": "Users can access invoking workflow information in activities through the documented activity.info() function.",
      "rootCause": null,
      "proposedFix": "Use temporalio.activity.info() to retrieve activity context including workflow information",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was a support question rather than a bug or feature request. The requested functionality already exists via the activity.info() API.",
      "related": [],
      "keyQuote": "You can call temporalio.activity.info() which returns an Info class containing information about the activity including workflow information.",
      "number": 275,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:58:45.977Z"
    },
    {
      "summary": "User attempted to mutate workflow instance attributes from within executed activities, expecting reference-based behavior. This is not supportedactivities and workflows run in separate processes with serialized data exchange.",
      "category": "question",
      "subcategory": "activity-state-management",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "activity-executor",
        "workflow-state",
        "serialization"
      ],
      "concepts": [
        "state-mutation",
        "reference-semantics",
        "inter-process-communication",
        "data-serialization",
        "activity-workflow-isolation"
      ],
      "severity": "low",
      "userImpact": "Users cannot directly mutate workflow state from activities; they must use alternative patterns like return values or signals.",
      "rootCause": "Activities run in separate processes with serialized arguments, so mutations are not visible to the workflow.",
      "proposedFix": null,
      "workaround": "Use return values or signals to communicate state changes from activities back to workflows.",
      "resolution": "invalid",
      "resolutionDetails": "Closed as a question. The behavior is by designactivities and workflows are intentionally isolated with serialized data exchange.",
      "related": [],
      "keyQuote": "Activities may not even run on the same machine. Activities and workflows are intentionally completely separate things that run in separate place that serialize the data sent between them.",
      "number": 274,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:58:44.927Z"
    },
    {
      "summary": "Feature request to improve Python SDK workflow sandboxing by encouraging separate files for workflows, adding clearer exception messaging, improving SEO through README documentation, and updating samples to demonstrate best practices.",
      "category": "feature",
      "subcategory": "documentation",
      "apis": [],
      "components": [
        "workflow-definition",
        "documentation",
        "samples"
      ],
      "concepts": [
        "sandboxing",
        "workflow-isolation",
        "best-practices",
        "exception-handling",
        "code-organization"
      ],
      "severity": "low",
      "userImpact": "Helps developers properly structure their workflow code and understand exception messages through improved documentation and examples.",
      "rootCause": null,
      "proposedFix": "Update README with guidance on separating workflows from other code, improve exception message clarity, add SEO references, and update samples (except hello-world) to follow best practices.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature request was addressed through documentation improvements and sample updates.",
      "related": [],
      "keyQuote": "encourage putting workflows in separate files from all other code including activities",
      "number": 273,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:58:28.058Z"
    },
    {
      "summary": "Implement a Temporal CLI-based ephemeral server for Python SDK testing instead of Temporalite. This requires support for additional namespaces and depends on CLI features and SDK core updates.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-server",
        "cli-integration",
        "namespace-support"
      ],
      "concepts": [
        "ephemeral-server",
        "testing",
        "cli-integration",
        "namespaces",
        "temporalite-migration"
      ],
      "severity": "medium",
      "userImpact": "Users gain better testing capabilities with an ephemeral server solution that supports the full Temporal feature set including SDK metadata.",
      "rootCause": null,
      "proposedFix": "Integrate Temporal CLI as an ephemeral server for testing, replacing Temporalite while maintaining backward compatibility during transition.",
      "workaround": "Continue using Temporalite for testing, though it lacks support for SDK metadata and other features.",
      "resolution": "fixed",
      "resolutionDetails": "Implemented Temporal CLI-based ephemeral server support after required dependencies (sdk-core#485, temporal#4317) were completed.",
      "related": [
        485,
        4317
      ],
      "keyQuote": "Make sure to accept additional namespaces here beyond the primary one.",
      "number": 272,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:58:27.193Z"
    },
    {
      "summary": "Health proto package in the Python SDK conflicts with the gRPC health library's health.proto, causing a TypeError when both are imported. The SDK's health proto package needs to be renamed to avoid this namespace collision.",
      "category": "bug",
      "subcategory": "proto-package-conflict",
      "apis": [],
      "components": [
        "proto-definitions",
        "grpc-interop",
        "health-check"
      ],
      "concepts": [
        "package-namespace",
        "proto-conflict",
        "grpc-compatibility",
        "import-resolution",
        "library-clash"
      ],
      "severity": "high",
      "userImpact": "Users cannot use both the Temporal Python SDK and the grpc_health library together due to a protobuf package name conflict.",
      "rootCause": "The Python SDK's health proto uses package 'grpc.health.v1' which is identical to the official gRPC health library's package name, causing a protobuf registration conflict.",
      "proposedFix": "Rename the SDK's health proto package to avoid collision with gRPC's official health proto package.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Health proto package was renamed in the SDK to resolve the namespace conflict with grpc_health library.",
      "related": [],
      "keyQuote": "TypeError: Conflict register for file \"health/v1/health.proto\": grpc.health.v1.HealthCheckRequest is already defined",
      "number": 267,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:58:30.558Z"
    },
    {
      "summary": "Test and confirm that async activities returning Optional types (e.g., Optional[str]) can properly complete with null/None values. Currently, the code may not explicitly put a null value in the result.",
      "category": "feature",
      "subcategory": "activity-completion",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "activity-executor",
        "type-handling",
        "result-serialization"
      ],
      "concepts": [
        "optional-types",
        "null-handling",
        "type-safety",
        "activity-result",
        "async-completion"
      ],
      "severity": "low",
      "userImpact": "Users may encounter issues when activities are designed to return Optional types and need to return None values.",
      "rootCause": "Code may not explicitly handle putting null/None values in activity results for Optional return types",
      "proposedFix": "Create a test to confirm async activity completion works correctly with Optional return types and None values",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed as enhancement - test was added or implementation was verified to handle Optional return types correctly",
      "related": [],
      "keyQuote": "Make a test to confirm that is ok for a workflow expecting an activity response of `Optional[str]` or similar.",
      "number": 266,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:58:11.091Z"
    },
    {
      "summary": "Feature request to improve the query-handler-not-found error by displaying a list of available queries in a space-delimited format within brackets. This will help the UI show possible queries when a query handler is not found.",
      "category": "feature",
      "subcategory": "query-handler",
      "apis": [],
      "components": [
        "query-handler",
        "error-messaging",
        "ui"
      ],
      "concepts": [
        "error-handling",
        "query-discovery",
        "user-experience",
        "error-messages"
      ],
      "severity": "low",
      "userImpact": "Users cannot see available queries when a query handler is not found, limiting the UI's ability to display helpful information.",
      "rootCause": null,
      "proposedFix": "Put known queries in space-delimited form surrounded by brackets in the error message (e.g., 'Some query error, queries: [query1 query2]').",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The feature was implemented to display available queries in the error message format as requested.",
      "related": [
        51
      ],
      "keyQuote": "Need to put known queries in space-delimited form surrounded by brackets for use in the UI. E.g. \"Some query error, queries: [query1 query2]\".",
      "number": 265,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:58:14.136Z"
    },
    {
      "summary": "Users need a way to customize JSON type conversion for their own types. The feature request proposes adding a JSONTypeConverter ABC to make it easier to convert between user types and JSON-able types during payload conversion.",
      "category": "feature",
      "subcategory": "type-converters",
      "apis": [
        "value_to_type"
      ],
      "components": [
        "JSONPlainPayloadConverter",
        "payload-converter",
        "type-conversion"
      ],
      "concepts": [
        "serialization",
        "deserialization",
        "type-hints",
        "custom-types",
        "payload-encoding"
      ],
      "severity": "medium",
      "userImpact": "Users working with custom types like OpenAPI-generated models can now standardize their JSON conversion logic instead of inheriting and overriding converter methods.",
      "rootCause": null,
      "proposedFix": "Add a JSONTypeConverter ABC that converts to and from user types to JSON-able types, and update JSONPlainPayloadConverter to accept a collection of these converters during construction.",
      "workaround": "Users can inherit from JSONPlainPayloadConverter and override from_payload/to_payload methods, though this has limitations with optional types and collections.",
      "resolution": "fixed",
      "resolutionDetails": "PR was opened to implement JSON type converters and README updates to support the use case.",
      "related": [],
      "keyQuote": "Add a JSONTypeConverter ABC that converts to and from a user type to JSON-able type. Change JSONPlainPayloadConverter to accept a collection of these.",
      "number": 264,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:58:13.272Z"
    },
    {
      "summary": "Feature request to propagate contextvars when executing threaded activities using asyncio.run_in_executor. Currently contextvars are not passed to the executor, requiring manual context population as a workaround since to_thread() doesn't use custom executors.",
      "category": "feature",
      "subcategory": "activity-execution",
      "apis": [],
      "components": [
        "activity-executor",
        "threading",
        "contextvars"
      ],
      "concepts": [
        "contextvars",
        "thread-pool",
        "asyncio-executor",
        "context-propagation",
        "activity-threading"
      ],
      "severity": "medium",
      "userImpact": "Threaded activities cannot access contextvars from the parent async context, requiring workarounds or custom context management by users.",
      "rootCause": "asyncio.run_in_executor does not automatically propagate contextvars to the executor thread.",
      "proposedFix": "Manually populate the context in the executor call since run_in_executor doesn't support contextvars propagation and to_thread() doesn't use custom executors.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Since there is no way with `run_in_executor` and we don't want to use `to_thread` as suggested at https://github.com/python/cpython/issues/78195 because it doesn't use a custom executor, we need to just populate the context ourselves.",
      "number": 263,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:54.671Z"
    },
    {
      "summary": "User requests a code sample or guide for resetting workflows in the Python SDK, similar to tctl's reset functionality. The discussion reveals that while raw API access exists, there are no SDK samples for this advanced operation.",
      "category": "feature",
      "subcategory": "workflow-management",
      "apis": [
        "reset_workflow_execution",
        "workflow_service"
      ],
      "components": [
        "workflow-client",
        "api-service",
        "workflow-execution"
      ],
      "concepts": [
        "workflow-reset",
        "workflow-state-management",
        "history-reset",
        "continued-as-new",
        "advanced-operations"
      ],
      "severity": "low",
      "userImpact": "Advanced users need to know how to reset workflows programmatically without using tctl CLI, but this capability is not well-documented in SDK samples.",
      "rootCause": null,
      "proposedFix": "Provide code sample showing how to use the raw WorkflowService API for workflow resets via reset_workflow_execution()",
      "workaround": "Use tctl CLI command or call the raw WorkflowService API directly: `await client.workflow_service.reset_workflow_execution(ResetWorkflowExecutionRequest(...))`",
      "resolution": "answered",
      "resolutionDetails": "Answered by maintainer explaining that raw API access is available but resets are advanced operations meant for CLI, not typical SDK usage patterns",
      "related": [],
      "keyQuote": "you can call `await my_client.workflow_service.reset_workflow_execution(temporalio.api.workflowservice.v1.ResetWorkflowExecutionRequest(...` which would be calling the raw API",
      "number": 262,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:58.069Z"
    },
    {
      "summary": "User requested documentation and guidance on workflow versioning in the Python SDK, following the TypeScript implementation pattern. The issue was resolved by redirecting to existing documentation and a related documentation issue.",
      "category": "docs",
      "subcategory": "workflow-versioning",
      "apis": [
        "patched"
      ],
      "components": [
        "documentation",
        "workflow-engine"
      ],
      "concepts": [
        "versioning",
        "backward-compatibility",
        "workflow-updates",
        "patching"
      ],
      "severity": "low",
      "userImpact": "Users need clear documentation on how to implement workflow versioning in Python to safely evolve their workflows.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Use the patched() function as documented in the Python SDK, mirroring the TypeScript implementation approach.",
      "resolution": "duplicate",
      "resolutionDetails": "Resolved by pointing to existing patched() documentation and redirecting to documentation issue #1925.",
      "related": [
        1925
      ],
      "keyQuote": "Versioning is already supported in Python via the patched call in mostly the same way as TypeScript.",
      "number": 259,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:57.923Z"
    },
    {
      "summary": "StartWorkflow should throw WorkflowAlreadyStartedError when attempting to start a workflow that is already running, providing users with a clear, friendly error message instead of a generic failure.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-executor",
        "client",
        "error-handling"
      ],
      "concepts": [
        "error-handling",
        "workflow-lifecycle",
        "idempotency",
        "user-friendly-errors",
        "duplicate-detection"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to start an already-running workflow receive an unclear error message instead of a specific WorkflowAlreadyStartedError, making it difficult to debug workflow lifecycle issues.",
      "rootCause": "StartWorkflow does not properly validate and reject duplicate workflow start attempts with a specific error type.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation of WorkflowAlreadyStartedError exception for duplicate workflow start attempts.",
      "related": [],
      "keyQuote": "Needs to be friendly error returned in this case",
      "number": 258,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:39.460Z"
    },
    {
      "summary": "PEP 604 style optionals (using | for Union types) are not properly recognized in the converter module. The code needs to handle types.UnionType which is only available in Python 3.10+, and tests are needed to confirm proper support.",
      "category": "bug",
      "subcategory": "type-handling",
      "apis": [],
      "components": [
        "converter",
        "type-checking"
      ],
      "concepts": [
        "PEP 604",
        "union types",
        "optional types",
        "type hints",
        "Python 3.10 compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use modern PEP 604 style optionals (X | Y syntax) in their Temporal workflows, forcing them to use older Union[X, Y] syntax.",
      "rootCause": "The converter.py code only checks for Union type origin but not for types.UnionType introduced in Python 3.10 for the | operator syntax.",
      "proposedFix": "Update the isinstance check to handle both Union and types.UnionType, with version gating for Python 3.10+ compatibility.",
      "workaround": "Use Union[X, Y] syntax instead of X | Y for optional types.",
      "resolution": "fixed",
      "resolutionDetails": "The code was updated to handle both Union and types.UnionType with appropriate Python version checks, and tests were added.",
      "related": [],
      "keyQuote": "needs to change to: if origin is Union or isinstance(origin, types.UnionType):",
      "number": 255,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:42.438Z"
    },
    {
      "summary": "The with_child_unrestricted method in workflow sandbox restrictions has a bug where it fails to properly remove restrictions due to an incorrect condition check on child_path[0].",
      "category": "bug",
      "subcategory": "workflow-sandbox",
      "apis": [],
      "components": [
        "worker",
        "workflow_sandbox",
        "restrictions"
      ],
      "concepts": [
        "sandbox",
        "restrictions",
        "child_restriction",
        "path_matching",
        "condition_logic"
      ],
      "severity": "medium",
      "userImpact": "Users cannot properly unrestrict child workflows in sandbox environments, leading to workflows being incorrectly restricted.",
      "rootCause": "Incorrect conditional logic in with_child_unrestricted - checking membership against self.children instead of child_path[0]",
      "proposedFix": "Change the condition to check if child_path[0] is in self.children",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The condition was corrected to properly check child_path[0] membership in self.children",
      "related": [],
      "keyQuote": "Should be: if child_path[0] in self.children:",
      "number": 254,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:37.889Z"
    },
    {
      "summary": "Add validation warnings and errors for invalid query patterns in the Python SDK. Queries should warn if async, error if using workflow constructs like wait_condition, and error if creating commands.",
      "category": "feature",
      "subcategory": "query-validation",
      "apis": [
        "query"
      ],
      "components": [
        "query-executor",
        "validation",
        "runtime"
      ],
      "concepts": [
        "async-patterns",
        "workflow-constructs",
        "command-generation",
        "validation",
        "error-handling",
        "developer-experience"
      ],
      "severity": "medium",
      "userImpact": "Developers will receive early warnings and errors when using queries incorrectly, preventing silent failures or unexpected behavior.",
      "rootCause": null,
      "proposedFix": "Implement validation logic to: (1) warn on async def queries with override option, (2) error on workflow constructs usage, (3) error if queries create commands",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Query validation was implemented to detect and prevent invalid usage patterns",
      "related": [],
      "keyQuote": "If a query is an `async def` we should warn by default but allow overriding/silencing that warning",
      "number": 250,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:25.503Z"
    },
    {
      "summary": "The PayloadCodec.encode_failure and PayloadCodec.decode_failure methods are not applying codec transformations to the encoded_attributes field of Failure objects, which was missed in issue #185.",
      "category": "bug",
      "subcategory": "payload-codec",
      "apis": [
        "PayloadCodec.encode_failure",
        "PayloadCodec.decode_failure"
      ],
      "components": [
        "payload-codec",
        "failure-handling"
      ],
      "concepts": [
        "codec",
        "payload-encoding",
        "attribute-serialization",
        "data-transformation"
      ],
      "severity": "medium",
      "userImpact": "Users relying on codec customization for failure handling may experience inconsistent behavior where encoded_attributes bypass their configured codec.",
      "rootCause": "The encoded_attributes field was not included in the codec transformation logic during the implementation of issue #185.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [
        185
      ],
      "keyQuote": "Seems `PayloadCodec.encode_failure` and `PayloadCodec.decode_failure` missed this field as part of #185",
      "number": 247,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:22.376Z"
    },
    {
      "summary": "Local activities in the Python SDK are still experimental in core but lack documentation marking them as such in docstrings. This feature request asks to add experimental status documentation to local activities.",
      "category": "docs",
      "subcategory": "local-activities",
      "apis": [
        "LocalActivity"
      ],
      "components": [
        "local-activities",
        "documentation",
        "docstrings"
      ],
      "concepts": [
        "experimental",
        "documentation",
        "api-stability",
        "local-execution"
      ],
      "severity": "low",
      "userImpact": "Users are not aware that local activities are experimental, which could lead to adoption of unstable features.",
      "rootCause": null,
      "proposedFix": "Add experimental status markers to local activity docstrings to align with core SDK status",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Docstrings were updated to mark local activities as experimental",
      "related": [],
      "keyQuote": "Local activities are still experimental in core but we have not documented this as such in Python docstrings.",
      "number": 246,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:23.386Z"
    },
    {
      "summary": "When a worker process in ProcessPoolExecutor is terminated (e.g., segfault, OOMkill), the executor enters a BrokenProcessPool state that permanently disables activity execution. The worker cannot recover and continues failing all subsequent activities indefinitely.",
      "category": "bug",
      "subcategory": "activity-execution",
      "apis": [],
      "components": [
        "activity-executor",
        "process-pool",
        "worker"
      ],
      "concepts": [
        "process-pool",
        "fault-recovery",
        "executor-state",
        "activity-execution",
        "process-termination",
        "resilience"
      ],
      "severity": "high",
      "userImpact": "Users running synchronous activities with ProcessPoolExecutor cannot recover from worker process crashes, requiring manual worker restart and losing in-flight activities.",
      "rootCause": "ProcessPoolExecutor becomes permanently broken (BrokenProcessPool) when a worker process terminates, and the Temporal SDK does not handle this state by recovering the executor or failing the worker appropriately.",
      "proposedFix": "The Temporal Worker should fatally error and terminate when encountering a BrokenExecutor exception from run_in_executor, allowing orchestration systems (systemd, k8s) to restart the service.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Worker now fatally errors on BrokenExecutor exceptions, allowing proper recovery and restart.",
      "related": [],
      "keyQuote": "The temporal Worker could cope with a ProcessPoolExecutor that has entered a bad state",
      "number": 245,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:09.377Z"
    },
    {
      "summary": "Tokio dependency (version 1.21.2) contains two security vulnerabilities: WS-2023-0027 (CVSS 9.8) affecting Pin contract soundness, and CVE-2023-22466 (CVSS 5.4) affecting Windows named pipe security. Issue was auto-closed after vulnerability was removed from the repository.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "tokio-runtime",
        "security-scanning"
      ],
      "concepts": [
        "vulnerability",
        "dependency-upgrade",
        "security-patch",
        "CVE",
        "soundness"
      ],
      "severity": "critical",
      "userImpact": "Users of the Python SDK are exposed to critical security vulnerabilities in the tokio dependency that could allow remote code execution or unauthorized access.",
      "rootCause": "Tokio version 1.21.2 contains unpatched security vulnerabilities in Pin contract handling and Windows named pipe configuration.",
      "proposedFix": "Upgrade tokio to version 1.24.2 (for WS-2023-0027) or 1.23.1 (for CVE-2023-22466) to fix both vulnerabilities.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was auto-closed by Mend after the vulnerable library was removed from or updated in the repository inventory.",
      "related": [],
      "keyQuote": "A soundness issue was discovered in tokio. tokio::io::ReadHalf<T>::unsplit can violate the Pin contract.",
      "number": 244,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:10.739Z"
    },
    {
      "summary": "Add support for dynamic workflows, activities, signals, and queries that can catch all invocations with a given name as a parameter. This requires opt-in registration and access to the payload converter for raw payload handling.",
      "category": "feature",
      "subcategory": "dynamic-handlers",
      "apis": [],
      "components": [
        "workflow-registry",
        "activity-registry",
        "signal-handler",
        "query-handler",
        "payload-converter"
      ],
      "concepts": [
        "dynamic-dispatch",
        "catch-all-handler",
        "payload-conversion",
        "opt-in",
        "registration"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to implement flexible workflow/activity/signal/query handlers that accept any name dynamically instead of pre-registering each one.",
      "rootCause": null,
      "proposedFix": "Register a single workflow/activity as 'dynamic' with name passed as first arg and *args:Payload as second arg. Provide payload converter access. For signals/queries, modify existing dynamic=True to require payloads instead of decoded args.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to support dynamic workflows, activities, signals, and queries with proper payload handling.",
      "related": [],
      "keyQuote": "Should be able to register only one workflow as \"dynamic\" and it is given the name as a string as the first arg, then *args: Payload as second arg.",
      "number": 242,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:57:08.271Z"
    },
    {
      "summary": "Feature request to add an opt-in worker option `stack_trace_query_uses_converter` that allows the stack trace query response to be processed through custom data converters. The feature was later abandoned.",
      "category": "feature",
      "subcategory": "stack-trace-query",
      "apis": [],
      "components": [
        "worker",
        "data-converter",
        "query"
      ],
      "concepts": [
        "stack-trace",
        "query",
        "custom-converter",
        "data-serialization",
        "worker-configuration"
      ],
      "severity": "low",
      "userImpact": "Users who need custom serialization of stack trace query responses would have been able to opt-in to this behavior.",
      "rootCause": null,
      "proposedFix": "Add a new boolean worker option `stack_trace_query_uses_converter` to control whether stack trace query responses are converted/encoded with custom converters.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Feature was abandoned by maintainers",
      "related": [],
      "keyQuote": "Closing since the feature was abandoned",
      "number": 241,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:50.247Z"
    },
    {
      "summary": "VSCode debugger attachment fails when running workflows due to a KeyError in the workflow sandbox's import system. The debugpy module tries to access the 'warnings' module through the sandbox's restricted module loader, which doesn't provide it, causing workflow execution to crash.",
      "category": "bug",
      "subcategory": "debugging-sandbox",
      "apis": [
        "Worker"
      ],
      "components": [
        "workflow-sandbox",
        "importer",
        "worker"
      ],
      "concepts": [
        "debugging",
        "sandbox-restrictions",
        "module-access",
        "vscode-debugger",
        "import-system"
      ],
      "severity": "high",
      "userImpact": "Developers cannot debug workflows using VSCode debugger without manually disabling the sandbox, significantly impacting developer productivity during debugging sessions.",
      "rootCause": "The workflow sandbox's custom module loader (__getitem__ in _importer.py) doesn't provide access to standard library modules like 'warnings' that debugpy requires, causing KeyError when debugger tries to access sys.modules['warnings'].",
      "proposedFix": "Allow debugpy's _pydevd_bundle module to bypass sandbox restrictions or make standard library modules accessible through the sandbox's import system.",
      "workaround": "Disable the sandbox by using UnsandboxedWorkflowRunner() or SandboxedWorkflowRunner with explicit passthrough for _pydevd_bundle module, optionally setting debug_mode=True to avoid deadlock timeouts.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        213,
        254
      ],
      "keyQuote": "KeyError: 'warnings' when VSCode debugger tries to access sys.modules through workflow sandbox's custom import system",
      "number": 238,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:53.461Z"
    },
    {
      "summary": "The Python SDK cannot send a cancelled error to the server when cancellation is not explicitly requested, causing it to be converted to an application error. The SDK also needs to properly set the `ActivityExecutionResult.cancelled` flag when activities are cancelled.",
      "category": "bug",
      "subcategory": "activity-cancellation",
      "apis": [
        "ActivityExecutionResult"
      ],
      "components": [
        "activity-executor",
        "error-handling",
        "cancellation",
        "result-reporting"
      ],
      "concepts": [
        "cancellation",
        "error-handling",
        "activity-lifecycle",
        "thread-cancellation",
        "result-tracking",
        "server-communication"
      ],
      "severity": "medium",
      "userImpact": "Users experience incorrect error reporting when activities are cancelled without explicit cancellation requests, making it difficult to properly handle and debug cancellation scenarios.",
      "rootCause": "The SDK does not properly distinguish between requested and unrequested cancellations, and does not set the cancelled flag on the activity result.",
      "proposedFix": "Convert cancelled errors to application errors when not requested, and ensure ActivityExecutionResult.cancelled is set on cancellation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was fixed by implementing proper cancellation error handling and setting the cancelled flag on activity results.",
      "related": [],
      "keyQuote": "We cannot send a cancelled error to server if not requested, so convert to application error if not requested (such as thread cancel)",
      "number": 237,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:54.259Z"
    },
    {
      "summary": "The `inspect.isclass` check in temporalio/common.py doesn't properly handle `typing.Optional` (which is an alias for `typing.Union`), causing optional parameters with types like `Optional[X]` to not be handled correctly. The check needs to be removed and tests should validate that optional parameters work, including with Pydantic models.",
      "category": "bug",
      "subcategory": "type-handling",
      "apis": [],
      "components": [
        "type-validation",
        "parameter-handling",
        "common"
      ],
      "concepts": [
        "optional-types",
        "type-unions",
        "parameter-serialization",
        "type-checking",
        "pydantic-integration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use Optional[X] type hints for parameters, limiting expressiveness of workflow and activity signatures.",
      "rootCause": "The `inspect.isclass` check at temporalio/common.py#L247 returns False for typing.Optional (Union types), preventing proper type handling",
      "proposedFix": "Remove the `inspect.isclass` check to allow Union types like Optional to be processed correctly",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The check was removed to handle Optional/Union types properly",
      "related": [],
      "keyQuote": "Our check at line 247 that does `inspect.isclass` is `False` for `typing.Optional` (which is an alias for `typing.Union`). Remove that check.",
      "number": 236,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:39.086Z"
    },
    {
      "summary": "Feature request to raise clear exceptions when payload sizes exceed gRPC's 4MB maximum limit, instead of silently retrying and timing out. Currently, oversized payloads result in ResourceExhausted gRPC errors that are treated as transient and retried indefinitely, causing confusing timeouts without clear error messages.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "data-converter",
        "grpc-client",
        "retry-logic"
      ],
      "concepts": [
        "payload-size",
        "grpc-limits",
        "error-handling",
        "retry-strategy",
        "diagnostics",
        "resource-exhaustion"
      ],
      "severity": "medium",
      "userImpact": "Developers receive confusing timeout errors instead of clear exceptions when payloads exceed gRPC limits, making debugging significantly harder.",
      "rootCause": "ResourceExhausted gRPC status is treated as a transient error and retried indefinitely, even though oversized payloads will never succeed on retry.",
      "proposedFix": "Add payload size validation in the data converter to check against the 4MB limit and raise a clear exception, or handle ResourceExhausted status specially to fail immediately rather than retry.",
      "workaround": "Extend the JSONPlainPayloadConverter to check payload size before serialization and raise an exception if it exceeds MAX_PAYLOAD_SIZE (4194304 bytes).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        462
      ],
      "keyQuote": "ResourceExhausted is seen as a transient error even though it's not in this case... I think this is an issue in all SDKs today.",
      "number": 235,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:37.879Z"
    },
    {
      "summary": "Generic nested dataclasses are deserialized as plain dictionaries instead of proper dataclass instances when used as activity parameters. This causes AttributeError when trying to access nested dataclass attributes.",
      "category": "bug",
      "subcategory": "dataclass-serialization",
      "apis": [
        "activity.defn",
        "execute_activity"
      ],
      "components": [
        "data-converter",
        "activity-serialization",
        "type-conversion"
      ],
      "concepts": [
        "generics",
        "dataclass-deserialization",
        "type-hints",
        "nested-objects",
        "serialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot use generic dataclasses with nested types in activity parameters, forcing workarounds for pagination and parameterized API calls.",
      "rootCause": "The data converter does not resolve generic type parameters during deserialization of dataclass instances.",
      "proposedFix": null,
      "workaround": "Update documentation to warn users that data converters do not resolve dataclass generics for deserialization.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed with decision to document limitation rather than implement generic dataclass resolution in data converters.",
      "related": [],
      "keyQuote": "resolving dataclass generics for deserialization is not expected of temporal data converters",
      "number": 234,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:35.426Z"
    },
    {
      "summary": "Setuptools 65.3.0 contains a ReDoS vulnerability (CVE-2022-40897) in package_index.py that allows remote attackers to cause denial of service through malicious HTML in packages or custom package index pages. Fixed in setuptools 65.5.1.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "setuptools",
        "package-index"
      ],
      "concepts": [
        "security-vulnerability",
        "denial-of-service",
        "regular-expression",
        "redos",
        "dependency-upgrade"
      ],
      "severity": "medium",
      "userImpact": "Users with setuptools 65.3.0 are vulnerable to denial of service attacks via specially crafted packages or malicious package index pages.",
      "rootCause": "Regular Expression Denial of Service (ReDoS) in setuptools package_index.py",
      "proposedFix": "Upgrade setuptools to version 65.5.1 or later",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Automatically closed by Mend when the vulnerable library was removed from the Mend inventory or marked as ignored",
      "related": [],
      "keyQuote": "Python Packaging Authority (PyPA) setuptools before 65.5.1 allows remote attackers to cause a denial of service via HTML in a crafted package",
      "number": 233,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:23.085Z"
    },
    {
      "summary": "Python 3.11 raises TypeError when passing coroutines directly to asyncio.wait(). The code works in Python 3.10 but fails in 3.11 due to stricter async API requirements. The solution is to use asyncio.wait_for() with a timeout instead.",
      "category": "bug",
      "subcategory": "python-compatibility",
      "apis": [],
      "components": [
        "asyncio-integration",
        "event-loop"
      ],
      "concepts": [
        "coroutines",
        "async-await",
        "timeout",
        "python-version-compatibility",
        "asyncio-api-changes"
      ],
      "severity": "medium",
      "userImpact": "Users upgrading to Python 3.11 encounter runtime errors with idiomatic asyncio code patterns used in Temporal workflows.",
      "rootCause": "Python 3.11 forbids passing coroutine objects directly to asyncio.wait(); they must be wrapped as tasks explicitly.",
      "proposedFix": "Replace asyncio.wait() calls with asyncio.wait_for() for timeout-based waiting patterns.",
      "workaround": "Use asyncio.wait_for(self._is_reopened.wait(), timeout=...) wrapped in try/except TimeoutError instead of asyncio.wait() with coroutines.",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved by updating documentation and examples to use asyncio.wait_for() which is the correct pattern for Python 3.11+ compatibility.",
      "related": [],
      "keyQuote": "Passing coroutines is forbidden, use tasks explicitly.",
      "number": 232,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:21.082Z"
    },
    {
      "summary": "Move the stable user-facing APIs from the unstable temporalio.bridge.runtime package to a stable location, either at the top level or under temporalio.runtime or temporalio.common.",
      "category": "feature",
      "subcategory": "api-stability",
      "apis": [],
      "components": [
        "bridge",
        "runtime",
        "telemetry",
        "package-structure"
      ],
      "concepts": [
        "api-stability",
        "package-organization",
        "telemetry",
        "public-api",
        "backwards-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users currently need to import from an unstable package for telemetry and runtime features, creating concerns about API breaking changes in future versions.",
      "rootCause": null,
      "proposedFix": "Move user-facing parts to a stable package location: either temporalio top level, temporalio.runtime, or temporalio.common.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The request was implemented by moving the stable user-facing APIs to a stable package location.",
      "related": [],
      "keyQuote": "Move it somewhere stable, probably either temporalio top level or temporalio.runtime or temporalio.common.",
      "number": 230,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:21.256Z"
    },
    {
      "summary": "Verify that canceling sync activities running non-GIL code is safe in the Python SDK. This addresses a similar issue found in the Ruby SDK implementation.",
      "category": "feature",
      "subcategory": "activity-cancellation",
      "apis": [],
      "components": [
        "activity-executor",
        "threading",
        "sync-activities"
      ],
      "concepts": [
        "GIL",
        "thread-safety",
        "activity-cancellation",
        "non-blocking-code",
        "exception-handling"
      ],
      "severity": "low",
      "userImpact": "Users need assurance that canceling sync activities won't cause issues when those activities use non-GIL code paths.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Confirmed through practical testing that the behavior is safe enough; formal test cases deemed too difficult to implement.",
      "related": [],
      "keyQuote": "Appears good enough in practice so far, not easy enough to add test cases for",
      "number": 229,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:04.947Z"
    },
    {
      "summary": "Feature request to provide eager validation when registering an activity class instead of an instance, failing at worker registration time rather than at activity runtime.",
      "category": "feature",
      "subcategory": "activity-registration",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "registration"
      ],
      "concepts": [
        "validation",
        "error-handling",
        "registration",
        "class-vs-instance",
        "early-failure"
      ],
      "severity": "medium",
      "userImpact": "Users currently receive runtime errors for incorrect activity class registration when they should fail immediately during worker setup.",
      "rootCause": null,
      "proposedFix": "Add validation at worker registration time to detect when a callable activity class is registered instead of an instance.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation added eager validation to fail at worker registration time when activity class is registered instead of instance.",
      "related": [],
      "keyQuote": "Right now we get a failure at activity runtime. We should fail at worker registration time.",
      "number": 228,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:05.679Z"
    },
    {
      "summary": "Feature request to add support for dynamic activities and workflows in the Python SDK that can handle multiple different activities/workflows through a single handler function with variable arguments.",
      "category": "feature",
      "subcategory": "dynamic-activities-workflows",
      "apis": [
        "activity.defn",
        "workflow.defn",
        "workflow.run"
      ],
      "components": [
        "activity-registry",
        "workflow-registry",
        "worker",
        "decorator-system"
      ],
      "concepts": [
        "dynamic-dispatch",
        "activity-routing",
        "workflow-routing",
        "payload-handling",
        "function-signature"
      ],
      "severity": "medium",
      "userImpact": "Users can dynamically handle multiple activities or workflows through a single decorated function, reducing boilerplate and improving flexibility.",
      "rootCause": null,
      "proposedFix": "Add dynamic=True parameter to @activity.defn and @workflow.defn decorators, with single handler receiving name and *args, ensuring only one dynamic activity and one dynamic workflow per worker.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed in favor of duplicate issue #242",
      "related": [
        242
      ],
      "keyQuote": "Make sure worker can only have one dynamic activity and/or only one dynamic workflow",
      "number": 227,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:56:07.381Z"
    },
    {
      "summary": "HistoryEventFilterType.ValueType fails on protobuf 3.19 because the enum wrapper was not introduced until protobuf 3.20. The minimum protobuf dependency version needs to be updated.",
      "category": "bug",
      "subcategory": "protobuf-compatibility",
      "apis": [],
      "components": [
        "protobuf",
        "history-event-filter",
        "type-system"
      ],
      "concepts": [
        "protobuf-version",
        "dependency-management",
        "enum-wrapper",
        "compatibility"
      ],
      "severity": "high",
      "userImpact": "Users on protobuf 3.19 encounter runtime failures when accessing HistoryEventFilterType.ValueType.",
      "rootCause": "protobuf 3.20 introduced enum wrapper functionality that HistoryEventFilterType.ValueType depends on, but this feature is not available in protobuf 3.19.",
      "proposedFix": "Update the minimum required protobuf dependency version to 3.20 or later.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Minimum protobuf dependency was updated to require version 3.20 or later.",
      "related": [],
      "keyQuote": "HistoryEventFilterType.ValueType fails on protobuf 3.19 because the enum wrapper didn't come until 3.20",
      "number": 226,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:50.859Z"
    },
    {
      "summary": "The grpcio dependency is not being marked as an optional dependency in the Python SDK's package metadata. The dependency is defined as 'grpc' in pyproject.toml but referenced as 'grpcio' in the wheel metadata, causing the optional extra to not be properly declared.",
      "category": "bug",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "packaging",
        "pyproject.toml",
        "wheel-metadata"
      ],
      "concepts": [
        "optional-dependency",
        "grpc",
        "packaging",
        "extras",
        "metadata"
      ],
      "severity": "medium",
      "userImpact": "Users cannot properly install grpcio as an optional dependency, making it difficult to manage grpc support separately from the core SDK.",
      "rootCause": "The pyproject.toml references the grpcio dependency with an incorrect name 'grpc' instead of 'grpcio', preventing the packaging system from properly applying the optional extra marker.",
      "proposedFix": "Correct the dependency name in pyproject.toml from 'grpc' to 'grpcio' to match the actual package name and ensure the extra marker is properly applied.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The dependency name was corrected in pyproject.toml to properly reference 'grpcio' with the appropriate extra marker.",
      "related": [],
      "keyQuote": "Why does `grpcio` not have an extra listed? I think it's because it doesn't reference the `grpcio` dependency properly in `pyproject.toml` (calls it `grpc` instead).",
      "number": 224,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:47.813Z"
    },
    {
      "summary": "A deprecation warning appears when installing temporalio via pip due to grpcio being installed using the legacy setup.py method. This warning indicates that pip 23.1 will enforce stricter installation behavior.",
      "category": "bug",
      "subcategory": "installation",
      "apis": [],
      "components": [
        "pip-installer",
        "dependencies",
        "grpcio"
      ],
      "concepts": [
        "deprecation-warning",
        "installation",
        "dependency-management",
        "pip-compatibility",
        "package-distribution"
      ],
      "severity": "low",
      "userImpact": "Users installing temporalio receive a deprecation warning that may be confusing, though the installation still completes successfully.",
      "rootCause": "grpcio lacks a pyproject.toml and requires wheel package to be installed, causing pip to fall back to legacy setup.py installation method.",
      "proposedFix": null,
      "workaround": "Install the wheel package before installing temporalio.",
      "resolution": "fixed",
      "resolutionDetails": "grpcio is no longer installed by default as a dependency; it was made an optional dependency as noted in issue #224.",
      "related": [
        224
      ],
      "keyQuote": "grpcio is no longer installed by default",
      "number": 223,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:50.125Z"
    },
    {
      "summary": "Using loguru library with temporal-sdk causes import errors in the workflow sandbox due to loguru extending restricted classes like datetime. The sandbox restrictions prevent loguru from loading properly during workflow initialization.",
      "category": "bug",
      "subcategory": "workflow-sandbox",
      "apis": [],
      "components": [
        "workflow_sandbox",
        "importer",
        "sandbox_restrictions"
      ],
      "concepts": [
        "sandbox",
        "import_restrictions",
        "module_loading",
        "datetime",
        "third_party_libraries"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use the loguru logging library in their Temporal SDK code without manually configuring passthrough modules.",
      "rootCause": "Loguru imports datetime internally and extends restricted classes, which triggers sandbox restrictions that prevent module loading during workflow preparation.",
      "proposedFix": "Mark loguru as a passthrough module using SandboxRestrictions with custom passthrough_modules configuration including loguru in the access set.",
      "workaround": "Add loguru to passthrough_modules in SandboxRestrictions.default before creating the Worker, using SandboxMatcher(access={\"loguru\"}).",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved in a later release that made passthrough module configuration easier and more intuitive.",
      "related": [],
      "keyQuote": "This is due to a known issue with the sandbox when imports extend restricted classes like `datetime`. See the passthrough modules documentation.",
      "number": 220,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:32.823Z"
    },
    {
      "summary": "Feature request to remove the `namespace` option from child workflow calls in the Python SDK. Temporal plans to discontinue support for cross-namespace child workflow calls in favor of alternative approaches.",
      "category": "feature",
      "subcategory": "child-workflows",
      "apis": [
        "ChildWorkflowOptions"
      ],
      "components": [
        "child-workflow-executor",
        "workflow-api"
      ],
      "concepts": [
        "namespace",
        "cross-namespace-calls",
        "workflow-isolation",
        "api-deprecation"
      ],
      "severity": "medium",
      "userImpact": "Users will need to remove namespace parameters from child workflow calls as this feature will no longer be supported.",
      "rootCause": null,
      "proposedFix": "Remove the `namespace` option from child workflow call APIs",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature request to disable cross-namespace child workflow calls has been completed by removing the namespace option",
      "related": [],
      "keyQuote": "Remove `namespace` option from child workflow calls. Temporal does not want to support this approach for cross-namespace calling in the future",
      "number": 218,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:33.240Z"
    },
    {
      "summary": "Importing gevent.os causes sandbox errors due to gevent's internal use of dict.items() on a restricted proxy object, which Python's C code rejects.",
      "category": "bug",
      "subcategory": "sandbox-restrictions",
      "apis": [],
      "components": [
        "sandbox",
        "restricted-proxy",
        "gevent-compatibility"
      ],
      "concepts": [
        "module-introspection",
        "dict-methods",
        "proxy-wrapper",
        "sandbox-enforcement"
      ],
      "severity": "medium",
      "userImpact": "Users cannot import gevent.os in workflows due to sandbox violations, breaking gevent-based code.",
      "rootCause": "gevent internally calls dict.items() on the module's __dict__ through a restricted proxy wrapper, which Python's C code explicitly rejects for non-dict objects.",
      "proposedFix": null,
      "workaround": "Avoid importing gevent.os or use alternative approaches; workarounds to be suggested by maintainers.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue is in gevent's C code implementation and cannot be fixed by Temporal SDK; workarounds were to be provided.",
      "related": [],
      "keyQuote": "This is checked in C code and we can't fix it. Will suggest work arounds.",
      "number": 216,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:30.555Z"
    },
    {
      "summary": "Improve RPC error handling in the Python SDK by making RPC-based errors extend a common base class and properly exposing error details from the gRPC protocol. The solution involves making gRPC status information available with Any-typed details for developers to use when needed.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "service",
        "rpc",
        "grpc"
      ],
      "concepts": [
        "error-handling",
        "rpc-errors",
        "error-details",
        "grpc-status",
        "type-safety"
      ],
      "severity": "medium",
      "userImpact": "Developers will have better access to detailed RPC error information and more consistent error handling through a common base class.",
      "rootCause": null,
      "proposedFix": "Make gRPC status available with Any-typed details that developers can access when needed.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented by exposing gRPC status and Any-typed details for RPC errors.",
      "related": [],
      "keyQuote": "For now we'll just make the gRPC status available with the Any details they can use if they must",
      "number": 214,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:18.230Z"
    },
    {
      "summary": "User reported difficulty debugging Python SDK workflows while running in sandbox environment. Feature request to confirm and improve vscode debuggability within sandbox constraints.",
      "category": "feature",
      "subcategory": "debugging",
      "apis": [],
      "components": [
        "debugger",
        "sandbox",
        "vscode-integration"
      ],
      "concepts": [
        "debugging",
        "developer-experience",
        "sandbox-constraints",
        "vscode",
        "workflow-execution"
      ],
      "severity": "low",
      "userImpact": "Developers cannot effectively debug workflows running in sandboxed environments, limiting their ability to diagnose and fix issues.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "User reported issue debugging while in sandbox",
      "number": 213,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:14.530Z"
    },
    {
      "summary": "The Replayer README documentation is outdated and inaccurate regarding how to load workflow history from JSON. It needs to be updated with current client history fetching details.",
      "category": "docs",
      "subcategory": "replayer",
      "apis": [],
      "components": [
        "replayer",
        "documentation",
        "client"
      ],
      "concepts": [
        "workflow-replay",
        "history-loading",
        "json-serialization",
        "documentation-accuracy"
      ],
      "severity": "low",
      "userImpact": "Users following the Replayer README may use outdated or incorrect methods to load workflow history from JSON.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "README documentation was updated with accurate information about loading workflow history from JSON using the new client history fetching approach.",
      "related": [],
      "keyQuote": "README for replayer out of date about how to load from JSON. Update with new client history fetching details.",
      "number": 212,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:18.209Z"
    },
    {
      "summary": "User seeks guidance on integrating Temporal worker and client into an existing Django application, transitioning from Zappa async tasks to Temporal workflows and activities.",
      "category": "question",
      "subcategory": "django-integration",
      "apis": [],
      "components": [
        "worker",
        "client",
        "django-integration"
      ],
      "concepts": [
        "asyncio",
        "django-integration",
        "worker-startup",
        "async-tasks",
        "environment-configuration"
      ],
      "severity": "low",
      "userImpact": "Users migrating from other async task systems to Temporal need clear guidance on integrating workers and clients into Django applications.",
      "rootCause": null,
      "proposedFix": "Start worker on app startup using async def with asyncio.run(), run worker in separate process, or refer to Django async documentation",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Closed as this is a general integration question rather than a feature request. The maintainer indicated there is no one prescribed way to integrate Temporal into Django and directed the user to documentation and community channels.",
      "related": [],
      "keyQuote": "There is no one way to integrate Temporal into your existing Python project. Temporal uses asyncio. Integrate it like you would any asyncio software.",
      "number": 211,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:01.806Z"
    },
    {
      "summary": "Sandboxed classes cannot extend restricted classes in the Python SDK, a known limitation that prevents certain inheritance patterns from working correctly.",
      "category": "bug",
      "subcategory": "sandboxing",
      "apis": [],
      "components": [
        "sandboxing",
        "class-extension",
        "workflow-engine"
      ],
      "concepts": [
        "inheritance",
        "class-extension",
        "sandboxing",
        "restriction",
        "workaround"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use inheritance to extend restricted classes within sandboxed workflow code, limiting design patterns available for workflow implementation.",
      "rootCause": "The sandboxing mechanism does not properly support class extension when the parent class is restricted.",
      "proposedFix": null,
      "workaround": "Use class extension where it works, but avoid using it in the restricted context for now.",
      "resolution": "wontfix",
      "resolutionDetails": "Documented as a known limitation with a workaround; not scheduled for fix.",
      "related": [],
      "keyQuote": "We'll workaround where the class extension works, but using does not for now",
      "number": 210,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:04.030Z"
    },
    {
      "summary": "Sandbox importer fails to respect relative import levels when `__import__` is called with level > 0, causing issues during non-binary Pydantic installations. The importer needs to properly resolve module names for relative imports by calculating the fully qualified name based on the package context.",
      "category": "bug",
      "subcategory": "sandbox-importer",
      "apis": [],
      "components": [
        "sandbox-importer",
        "import-resolution",
        "module-loading"
      ],
      "concepts": [
        "relative-imports",
        "module-resolution",
        "import-levels",
        "package-context",
        "importlib"
      ],
      "severity": "medium",
      "userImpact": "Users installing Pydantic in non-binary mode experience import failures due to improper relative import handling in the sandbox environment.",
      "rootCause": "The sandbox importer does not handle relative imports with level > 0 correctly; it needs to calculate fully qualified module names based on package context using `__package__` and `__spec__` globals.",
      "proposedFix": "Implement `_resolve_module_name()` helper function that mimic's CPython's importlib logic to properly resolve relative imports by calculating package name and reconstructing fully qualified module names.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented relative import resolution logic matching CPython's importlib._bootstrap to properly handle __import__ calls with level > 0.",
      "related": [
        207
      ],
      "keyQuote": "Some relative imports are respected, but ones that call `__import__` with a level > 0 are not properly.",
      "number": 208,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:55:03.931Z"
    },
    {
      "summary": "Pydantic models with datetime fields are incorrectly instantiated with date objects instead when created within workflows using the default sandbox runner. The issue is caused by the sandbox's datetime restriction logic incorrectly handling Pydantic's field type validation.",
      "category": "bug",
      "subcategory": "workflow-sandbox",
      "apis": [
        "workflow.run",
        "workflow.now"
      ],
      "components": [
        "workflow-sandbox",
        "pydantic-integration",
        "field-type-validation"
      ],
      "concepts": [
        "datetime-handling",
        "type-coercion",
        "sandbox-restrictions",
        "pydantic-models",
        "serialization"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably use Pydantic models with datetime fields in workflows without incorrect type conversion or runtime errors.",
      "rootCause": "The sandbox's issubclass check for datetime types fails due to Pydantic's proxied objects and the sandbox's restriction logic incorrectly converting datetime to date during field validation.",
      "proposedFix": "Move the issubclass check for datetime before the issubclass check for date, or patch datetime.__subclasscheck__ to handle proxied objects correctly.",
      "workaround": "Use a custom payload converter for Pydantic models as shown in python-samples, disable sandboxing on the worker, or relax sandbox restrictions to allow datetime module access.",
      "resolution": "wontfix",
      "resolutionDetails": "Documented as a known limitation in the README. Users should use custom payload converters for Pydantic or relax sandbox restrictions. This is the supported approach rather than fixing the sandbox behavior.",
      "related": [
        219
      ],
      "keyQuote": "The default sandbox runner is resulting in `content` being converted to just a `date` despite being instantiated with a `datetime`.",
      "number": 207,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:54:49.609Z"
    },
    {
      "summary": "Feature request to simplify importing third-party modules in workflow sandboxes. Currently requires configuring the sandbox runner with unstable APIs; proposal suggests adding a context manager, AST comments, or worker-level configuration for easier passthrough module specification.",
      "category": "feature",
      "subcategory": "workflow-sandbox",
      "apis": [],
      "components": [
        "workflow-sandbox",
        "worker",
        "activity-sandbox"
      ],
      "concepts": [
        "module-imports",
        "sandbox-configuration",
        "security-isolation",
        "third-party-libraries",
        "passthrough-modules"
      ],
      "severity": "medium",
      "userImpact": "Users currently face friction when needing to use third-party libraries in workflows, requiring complex sandbox configuration.",
      "rootCause": null,
      "proposedFix": "Three options proposed: (1) context manager syntax with `workflow.unsafe.imports_passed_through()`, (2) AST/comment-based approach with `# temporal: pass-through` annotations, (3) worker-level configuration parameter `workflow_sandbox_passthrough_modules`",
      "workaround": "Configure the sandbox runner with unstable APIs",
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented and merged into the Python SDK",
      "related": [],
      "keyQuote": "Probably a simple `with`, so something like: with workflow.unsafe.imports_passed_through(): import requests",
      "number": 204,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:54:44.952Z"
    },
    {
      "summary": "README documentation uses incorrect argument name 'runner' instead of 'workflow_runner' in sandbox examples, causing copy-paste errors for users following the guide.",
      "category": "docs",
      "subcategory": "documentation",
      "apis": [],
      "components": [
        "readme",
        "examples",
        "sandbox"
      ],
      "concepts": [
        "documentation",
        "api-naming",
        "examples",
        "sandbox",
        "configuration"
      ],
      "severity": "low",
      "userImpact": "Users copying example code from README get incorrect parameter names that fail to execute.",
      "rootCause": "Documentation uses outdated or incorrect parameter name in code examples.",
      "proposedFix": "Update README examples to use 'workflow_runner' instead of 'runner'.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation corrected to use correct parameter name.",
      "related": [],
      "keyQuote": "Wrong arg name in sample",
      "number": 203,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:54:45.055Z"
    },
    {
      "summary": "Importing concurrent.futures.ProcessPoolExecutor or ThreadPoolExecutor in Python workflows causes workflow failure even when unused. The sandbox environment prevents these imports, requiring investigation into why these standard library imports are blocked.",
      "category": "bug",
      "subcategory": "sandbox-restrictions",
      "apis": [],
      "components": [
        "sandbox",
        "import-system",
        "workflow-execution"
      ],
      "concepts": [
        "sandbox-isolation",
        "concurrent-execution",
        "standard-library-imports",
        "process-pooling",
        "thread-pooling",
        "restriction"
      ],
      "severity": "high",
      "userImpact": "Users cannot import standard library concurrency utilities in workflows, limiting code reusability and forcing workarounds for common Python patterns.",
      "rootCause": "The sandbox environment has restrictions that block imports of concurrent.futures module components without clear reasoning.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The sandbox restrictions on concurrent.futures imports were addressed, allowing both ProcessPoolExecutor and ThreadPoolExecutor to be imported without causing workflow failure.",
      "related": [],
      "keyQuote": "from concurrent.futures import ProcessPoolExecutor causes workflow failure even if unused. Need to see why.",
      "number": 201,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:54:30.128Z"
    },
    {
      "summary": "Identity equality checks (`is`) on IntEnum values fail when passed as workflow arguments because type conversion uses the definition's type hint instead of the sandbox's type context. This causes type mismatch issues for IntEnum and potentially isinstance checks on dataclasses.",
      "category": "bug",
      "subcategory": "type-conversion",
      "apis": [],
      "components": [
        "argument-converter",
        "sandbox",
        "type-hints"
      ],
      "concepts": [
        "type-identity",
        "enum-handling",
        "sandboxing",
        "type-coercion",
        "context-isolation"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot reliably use identity checks on IntEnum workflow arguments, causing logic errors in workflows.",
      "rootCause": "Argument conversion uses type hints from the workflow definition context rather than re-referencing types within the sandbox context, causing type identity mismatches.",
      "proposedFix": "Re-reference the full type's qualified name during conversion instead of reusing the type hint itself, or use type hints from inside the sandbox.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by implementing proper type referencing in the sandbox context using qualified names.",
      "related": [],
      "keyQuote": "Maybe re-reference the full type's qualified name when converting instead of reusing the type hint itself?",
      "number": 200,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:54:29.794Z"
    },
    {
      "summary": "OpenTelemetry fails on first use inside workflow sandbox because it tries to load configuration files, which are not allowed in the sandbox. This occurs when workers are separated from clients and OpenTelemetry context hasn't been initialized outside the sandbox first.",
      "category": "bug",
      "subcategory": "opentelemetry-integration",
      "apis": [],
      "components": [
        "opentelemetry-contrib",
        "sandbox",
        "workflow-execution"
      ],
      "concepts": [
        "sandboxing",
        "lazy-loading",
        "initialization",
        "observability",
        "context-management"
      ],
      "severity": "medium",
      "userImpact": "Users get AttributeError when using OpenTelemetry instrumentation inside workflows without first initializing it in client code.",
      "rootCause": "OpenTelemetry uses setuptools.pkg_resources to lazily load global runtime context, which issues file open() calls. The sandbox prohibits open() operations, and this wasn't triggered in tests because context was initialized outside sandbox first.",
      "proposedFix": "Add opentelemetry.context.get_current() to the top of the opentelemetry contrib Python file to force lazy initialization outside the sandbox before workflow execution.",
      "workaround": "Ensure OpenTelemetry context is accessed in client code before workers execute workflows.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by initializing OpenTelemetry context outside sandbox to prevent file I/O errors during workflow execution.",
      "related": [],
      "keyQuote": "We don't allow open() in a sandbox. This wasn't being triggered in unit tests because the OpenTelemetry context was being used outside of the sandbox first.",
      "number": 199,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:54:33.061Z"
    },
    {
      "summary": "Type annotation error in metrics configuration where MetricsConfig type is incorrectly specified in the runtime bridge module.",
      "category": "bug",
      "subcategory": "metrics-configuration",
      "apis": [],
      "components": [
        "runtime",
        "bridge",
        "metrics"
      ],
      "concepts": [
        "type-annotation",
        "configuration",
        "metrics-setup"
      ],
      "severity": "low",
      "userImpact": "Developers using metrics configuration may encounter type checking errors or IDE warnings when configuring metrics.",
      "rootCause": "Incorrect type annotation for MetricsConfig parameter in runtime.py line 150",
      "proposedFix": "Change the type annotation to MetricsConfig",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Type annotation was corrected to use MetricsConfig",
      "related": [],
      "keyQuote": "should be `MetricsConfig`",
      "number": 198,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:54:14.064Z"
    },
    {
      "summary": "User requests support for intermediate results from long-running iterable activities in Python SDK. Currently, activities can only return final results after completion, not stream intermediate values during execution.",
      "category": "feature",
      "subcategory": "activity-results",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "activity-executor",
        "workflow-engine",
        "result-serialization"
      ],
      "concepts": [
        "streaming",
        "intermediate-results",
        "async-iteration",
        "long-running-tasks",
        "activity-lifecycle"
      ],
      "severity": "low",
      "userImpact": "Users cannot stream intermediate results from long-running activities, limiting use cases like batch processing with progress updates.",
      "rootCause": "Temporal architecture design: activities serialize results back to workflows as a single final value, with no intermediate result streaming mechanism.",
      "proposedFix": "Implement support for activities to yield intermediate results during execution, or provide examples of signaling patterns as an alternative.",
      "workaround": "Use signaling from the activity to the calling workflow via a Temporal client, combined with heartbeats on long-running activities.",
      "resolution": "wontfix",
      "resolutionDetails": "Core Temporal limitation by design - activities return single serialized results. Maintainer suggested workaround using workflow signaling instead.",
      "related": [
        175
      ],
      "keyQuote": "There is no concept of \"intermediate results\" in Temporal I am afraid. You'll have to rework your code if you need something different.",
      "number": 195,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:54:15.232Z"
    },
    {
      "summary": "Feature request to support patch search attributes in Python SDK, similar to Go's implementation using TemporalChangeVersion keys. Needs design decision on user-facing API for updating and retrieving individual search attributes.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "search-attributes",
        "workflow-task",
        "command-handling"
      ],
      "concepts": [
        "search-attributes",
        "versioning",
        "patch-updates",
        "command-buffering",
        "atomicity"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily update and retrieve individual search attributes in Python SDK, limiting their ability to track workflow changes and version information.",
      "rootCause": "Python SDK lacks support for patch-based search attribute updates that Go SDK implements with TemporalChangeVersion keys.",
      "proposedFix": "Buffer search attribute updates and emit a single upsert command at the end of activation, with proper ordering before workflow completion/failure commands.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented in core rather than language-specific layer, following the design decision documented in comments.",
      "related": [
        153,
        248
      ],
      "keyQuote": "This is being done in core not lang.",
      "number": 194,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:54:16.944Z"
    },
    {
      "summary": "Feature request to add support for upserting memo in the Python SDK. The issue was acknowledged as being on the near-term backlog but remains unresolved.",
      "category": "feature",
      "subcategory": "memo-operations",
      "apis": [],
      "components": [
        "workflow-execution",
        "memo-handler"
      ],
      "concepts": [
        "upsert",
        "memo",
        "workflow-state",
        "mutation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently update memo fields in workflows without explicit conditional logic.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It is on our relatively-near-term backlog, but no updates at this time",
      "number": 190,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:57.733Z"
    },
    {
      "summary": "Feature request to add a `Client.set_rpc_metadata` method to allow client headers to be mutated after the client is created, aligning with SDK Core's `set_headers` capability.",
      "category": "feature",
      "subcategory": "client-configuration",
      "apis": [
        "Client"
      ],
      "components": [
        "client",
        "rpc-metadata",
        "headers"
      ],
      "concepts": [
        "client-configuration",
        "headers",
        "metadata",
        "mutation",
        "rpc"
      ],
      "severity": "low",
      "userImpact": "Users cannot modify RPC metadata/headers after creating a client, limiting flexibility in dynamic environments.",
      "rootCause": null,
      "proposedFix": "Implement a `Client.set_rpc_metadata` method similar to SDK Core's `set_headers` functionality.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The feature was implemented to match SDK Core capabilities for header mutation.",
      "related": [],
      "keyQuote": "support a `Client.set_rpc_metadata` method",
      "number": 187,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:57.137Z"
    },
    {
      "summary": "Need a unit test demonstrating protobuf serialization for workflow inputs/outputs in sandbox mode, with documentation explaining that protobufs must be added as passthrough modules due to protobuf static C state reloading issues.",
      "category": "feature",
      "subcategory": "testing",
      "apis": [],
      "components": [
        "protobuf-serialization",
        "sandbox",
        "module-reloading",
        "test-framework"
      ],
      "concepts": [
        "protobuf",
        "serialization",
        "deserialization",
        "module-reloading",
        "static-state",
        "sandbox-execution",
        "passthrough-modules"
      ],
      "severity": "medium",
      "userImpact": "Users need to understand that protobufs must be added as passthrough modules to work correctly with workflow reloading in sandbox mode.",
      "rootCause": "Protobuf static C state is problematic when reloading import modules, as referenced in protocolbuffers/protobuf#10075 and #10143.",
      "proposedFix": "Add a unit test demonstrating proper protobuf usage as inputs/outputs of workflows in sandbox with documentation explaining the passthrough module requirement.",
      "workaround": "Add protobufs as passthrough modules until the upstream protobuf issue is fixed.",
      "resolution": "fixed",
      "resolutionDetails": "Merged with pull request #215.",
      "related": [
        181,
        215
      ],
      "keyQuote": "Explain that protobufs have to be added as passthrough modules until that is fix, and add a unit test that we can clean up when that's fixed.",
      "number": 182,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:54:00.833Z"
    },
    {
      "summary": "Support for protobuf 3.x alongside 4.x in the Python SDK to address library compatibility issues. Users are encountering problems due to mixed protobuf version dependencies across the ecosystem, requiring investigation of multi-version code generation strategies.",
      "category": "feature",
      "subcategory": "protobuf-compatibility",
      "apis": [],
      "components": [
        "protobuf-generation",
        "build-system",
        "dependencies",
        "proto-compilation",
        "grpc-tools"
      ],
      "concepts": [
        "dependency-compatibility",
        "version-management",
        "code-generation",
        "transitive-dependencies",
        "library-ecosystem"
      ],
      "severity": "medium",
      "userImpact": "Users with libraries depending on protobuf 3.x experience conflicts and cannot use the SDK until protobuf 3.x support is available.",
      "rootCause": "Mixed protobuf version dependencies across the ecosystem - some libraries upgraded to protobuf 4.x while others remain on 3.x, creating transitive dependency conflicts. Additionally, C static state sharing in protobuf 3.x can harm sandbox isolation.",
      "proposedFix": "Generate both proto 3.x and 4.x forms; modify initialization to dynamically import appropriate version based on installed protobuf; add CI testing for protobuf 3.x compatibility; investigate making 4.x default while maintaining 3.x compatibility.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed by supporting protobuf 3.x compatibility through code generation strategies and CI validation.",
      "related": [],
      "keyQuote": "Many libraries have not upgraded to 4.x while many have (including us). Depending on libraries that haven't is causing a problem for users.",
      "number": 181,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:44.073Z"
    },
    {
      "summary": "IDE auto-complete is cluttered with private implementation modules that should be hidden. Proposal to prefix private module filenames with underscores to make them truly private and improve IDE experience for users.",
      "category": "feature",
      "subcategory": "api-design",
      "apis": [],
      "components": [
        "module-organization",
        "public-api",
        "worker"
      ],
      "concepts": [
        "encapsulation",
        "api-visibility",
        "ide-experience",
        "python-conventions",
        "module-privacy"
      ],
      "severity": "low",
      "userImpact": "Users experience cleaner IDE auto-complete by having private implementation details hidden from public API suggestions.",
      "rootCause": null,
      "proposedFix": "Rename private module filenames to use underscore prefix (e.g., temporalio/_worker.py) while maintaining __all__ exports for the public API.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Private modules were renamed with underscore prefixes to improve module organization and hide implementation details from IDE auto-complete.",
      "related": [],
      "keyQuote": "Auto complete in IDE keeps bringing up things like temporalio.worker.activity even though we expose what we want via __all__",
      "number": 178,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:43.402Z"
    },
    {
      "summary": "String-based enums in Python SDK are not properly serialized when passed to activities, resulting in incorrect type information (list instead of str). The feature request is to add native support for string-based enums to properly handle their serialization.",
      "category": "bug",
      "subcategory": "serialization",
      "apis": [
        "execute_activity"
      ],
      "components": [
        "activity-executor",
        "serialization",
        "type-handling"
      ],
      "concepts": [
        "enum",
        "string-based-enum",
        "type-preservation",
        "serialization",
        "activity-parameters"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably use Python's string-based enums as activity parameters because the SDK incorrectly deserializes them, breaking type expectations and activity logic.",
      "rootCause": "The SDK's serialization logic does not properly handle string-based enums (class Status(str, Enum)), causing them to be deserialized as lists instead of strings or enum instances.",
      "proposedFix": "Add native support in the SDK to recognize and properly serialize/deserialize string-based enums, ensuring they maintain their type information during workflow execution.",
      "workaround": "Change activity signature to accept union types (Status | str) instead of just Status to work around the type mismatch.",
      "resolution": "fixed",
      "resolutionDetails": "The SDK was updated to properly handle string-based enum serialization and deserialization.",
      "related": [],
      "keyQuote": "This code prints `<class 'list'>` which can break your underlying activities.",
      "number": 176,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:45.791Z"
    },
    {
      "summary": "User inquiring about the estimated release date for Python SDK V1. The issue was closed as it's a discussion topic rather than a technical issue.",
      "category": "question",
      "subcategory": "release-planning",
      "apis": [],
      "components": [],
      "concepts": [
        "versioning",
        "release",
        "timeline"
      ],
      "severity": "low",
      "userImpact": "Users wanting to know when V1 of the Python SDK will be available for production use.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Closed by maintainer as not an issue - redirected to community channels (Slack/forum) for discussion.",
      "related": [],
      "keyQuote": "Closing this as it is not an issue. Feel free to come discuss via Slack or forum: https://temporal.io/community.",
      "number": 173,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:27.415Z"
    },
    {
      "summary": "Workflow completion fails with 'legacy query response along with other commands' error during dataclass typed workflow tests, particularly with Java test server. Investigation revealed this is caused by SDK-core issue #390.",
      "category": "bug",
      "subcategory": "workflow-completion",
      "apis": [],
      "components": [
        "workflow-executor",
        "query-handling",
        "completion-handler"
      ],
      "concepts": [
        "query-response",
        "workflow-completion",
        "malformed-message",
        "legacy-protocol",
        "test-reliability"
      ],
      "severity": "high",
      "userImpact": "Tests fail unexpectedly with cryptic errors when running workflow dataclass tests, particularly against Java test server.",
      "rootCause": "SDK-core issue #390 causes legacy query response to be sent alongside other commands in workflow completion",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Root cause identified as SDK-core issue #390",
      "related": [
        390
      ],
      "keyQuote": "Workflow completion had a legacy query response along with other commands. This is not allowed and constitutes an error in the lang SDK",
      "number": 171,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:31.059Z"
    },
    {
      "summary": "Timer presence check is too strict when a signal cancels a timer in the same job activation. The timer may not exist if it was already cancelled, but the current code requires it to be present when fired.",
      "category": "bug",
      "subcategory": "timer-execution",
      "apis": [],
      "components": [
        "timer-executor",
        "signal-handler",
        "activation-processor"
      ],
      "concepts": [
        "timer",
        "signal",
        "cancellation",
        "job-activation",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Workflows crash or behave incorrectly when signals cancel timers in the same activation.",
      "rootCause": "The timer presence validation is overly strict and doesn't account for timers being cancelled in the same job activation.",
      "proposedFix": "Remove or relax the timer presence requirement when handling fire-timer events, allowing graceful handling of already-cancelled timers.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Timer presence check was removed to handle cases where timers are cancelled before being fired in the same activation.",
      "related": [
        466
      ],
      "keyQuote": "if a timer is fired in the same job that a signal is received that cancels it, the timer won't be present when fired",
      "number": 167,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:29.237Z"
    },
    {
      "summary": "Add CI tests to confirm Python 3.11 support is working. This is a testing infrastructure enhancement to validate compatibility with Python 3.11.",
      "category": "feature",
      "subcategory": "testing",
      "apis": [],
      "components": [
        "ci",
        "test-framework",
        "python-runtime"
      ],
      "concepts": [
        "python-version",
        "compatibility",
        "ci-testing",
        "version-support",
        "regression-testing"
      ],
      "severity": "medium",
      "userImpact": "Ensures Python 3.11 compatibility is continuously verified, preventing regressions for users running the SDK on Python 3.11.",
      "rootCause": null,
      "proposedFix": "Add Python 3.11 to CI test matrix after issue #164 is resolved.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "CI tests for Python 3.11 support were added and implemented.",
      "related": [
        164
      ],
      "keyQuote": "Confirm in CI we support 3.11. Do not do this until after #164 lands.",
      "number": 165,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:16.528Z"
    },
    {
      "summary": "Query return type validation was not being enforced during decode operations. Issue was investigated and appears to be working correctly.",
      "category": "bug",
      "subcategory": "query-return-types",
      "apis": [
        "query"
      ],
      "components": [
        "query-handler",
        "type-validation",
        "deserialization"
      ],
      "concepts": [
        "type-validation",
        "return-type",
        "decode",
        "type-mismatch"
      ],
      "severity": "low",
      "userImpact": "Queries with mismatched return types may not fail as expected during deserialization.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Author determined the issue was not reproducible after investigation.",
      "related": [],
      "keyQuote": "Nevermind, looks ok",
      "number": 163,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:14.117Z"
    },
    {
      "summary": "Python SDK throws raw RPC errors for non-existent queries instead of wrapping them with user-friendly error messages like other SDKs do. This should be improved to provide clearer error context to users.",
      "category": "bug",
      "subcategory": "query-error-handling",
      "apis": [
        "query"
      ],
      "components": [
        "query-handler",
        "error-handling",
        "rpc-client"
      ],
      "concepts": [
        "error-wrapping",
        "user-experience",
        "error-messaging",
        "query-execution",
        "exception-handling"
      ],
      "severity": "medium",
      "userImpact": "Users receive unclear RPC errors when invoking non-existent queries instead of friendly, informative error messages available in other SDKs.",
      "rootCause": "Query RPC errors are not wrapped with appropriate error handling in the Python SDK.",
      "proposedFix": "Wrap query RPC errors with a more clear and friendly error message similar to implementations in other SDKs.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Query error handling was improved to wrap RPC errors with friendlier messages.",
      "related": [],
      "keyQuote": "In other SDKs, when a non-existent query is invoked a \"query exception\" in that lang is thrown. But in Python we just throw the RPC back out.",
      "number": 162,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:13.335Z"
    },
    {
      "summary": "Transitive dependency vulnerability in golang.org/x/net (CVE-2022-27664) causing potential HTTP/2 connection hangs during shutdown. High severity DoS risk from Go net package used in temporalite dependency.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "temporalite",
        "go-grpc-middleware",
        "golang-net"
      ],
      "concepts": [
        "vulnerability",
        "dependency",
        "security",
        "http2",
        "denial-of-service",
        "transitive-dependency",
        "cvss"
      ],
      "severity": "high",
      "userImpact": "Applications using the SDK are exposed to a high-severity HTTP/2 connection vulnerability that could cause denial of service during shutdown.",
      "rootCause": "golang.org/x/net v0.0.0-20220708220712-1185a9018129 contains CVE-2022-27664: HTTP/2 connections can hang during closing if shutdown is preempted by fatal error.",
      "proposedFix": "Update golang.org/x/net to version 1.18.6 or 1.19.1 and later where the vulnerability is patched.",
      "workaround": null,
      "resolution": "stale",
      "resolutionDetails": "Issue was automatically closed by Mend because the vulnerable library was either marked as ignored or no longer part of the Mend inventory, likely due to dependency updates.",
      "related": [],
      "keyQuote": "attackers can cause a denial of service because an HTTP/2 connection can hang during closing if shutdown were preempted by a fatal error",
      "number": 161,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:02.362Z"
    },
    {
      "summary": "OpenTelemetry interceptors in the Python SDK fail with a 'Failed to detach context' ValueError when the event loop is garbage collected during workflow execution, particularly on worker exit when context variables from different contexts collide.",
      "category": "bug",
      "subcategory": "opentelemetry-interceptor",
      "apis": [
        "TracingInterceptor"
      ],
      "components": [
        "opentelemetry-interceptor",
        "context-management",
        "workflow-execution"
      ],
      "concepts": [
        "context-propagation",
        "generator-exit",
        "context-vars",
        "async-cleanup",
        "garbage-collection"
      ],
      "severity": "medium",
      "userImpact": "Users instrumenting Temporal workflows with OpenTelemetry tracing experience errors on worker exit, causing log spam and potentially masking actual issues.",
      "rootCause": "Context token created in one async context is being reset in a different context during generator cleanup, particularly when the event loop is garbage collected. Similar to OpenTelemetry Python issue #2606.",
      "proposedFix": "Swallow detachment failures in the Temporal OpenTelemetry interceptor to gracefully handle context cleanup during generator exit.",
      "workaround": "Ensure workflows are in separate files and do not recreate global tracer provider instances - only set it once globally.",
      "related": [
        2606
      ],
      "keyQuote": "ValueError: <Token var=<ContextVar name='current_context'...> was created in a different Context",
      "resolution": null,
      "resolutionDetails": null,
      "number": 160,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:53:01.324Z"
    },
    {
      "summary": "Function type hint lookup uses `__qualname__` as cache key, which is not unique across same-named classes in different scopes. Additionally, `typing.Set` and similar typing wrappers are excluded from return type hints because they don't satisfy `inspect.isclass`.",
      "category": "bug",
      "subcategory": "type-hints",
      "apis": [],
      "components": [
        "type-hint-cache",
        "type-validation"
      ],
      "concepts": [
        "caching",
        "type-hints",
        "qualname",
        "typing-module",
        "class-definition"
      ],
      "severity": "medium",
      "userImpact": "Users defining same-named workflow classes in different scopes may encounter type hint lookup failures, and generic typing hints like Set are not properly handled in return types.",
      "rootCause": "The cache key relies on `__qualname__` which is only qualified to the class level, not accounting for module/scope differences. Additionally, return type validation uses `inspect.isclass` which excludes generic typing wrappers.",
      "proposedFix": "Either fully qualify the cache key to be unique across modules/scopes, remove the type hint cache entirely, or store type hints directly on definitions themselves. Additionally, update return type hint validation to support typing module generics.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by improving the cache key implementation or removing the cache dependency",
      "related": [],
      "keyQuote": "The name is only qualified to the class level, not higher. We can find a way to qualify this or just remove the type hint cache.",
      "number": 159,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:59.994Z"
    },
    {
      "summary": "Feature request to add an integration test confirming the OpenTelemetry gRPC URL format and TLS support for metrics collection in the Python SDK. After investigation, the implementation was deferred to the samples repository.",
      "category": "feature",
      "subcategory": "telemetry-integration",
      "apis": [
        "init_telemetry",
        "TelemetryConfig",
        "OtelCollectorConfig",
        "Client.connect",
        "Worker",
        "execute_workflow"
      ],
      "components": [
        "telemetry",
        "otel-integration",
        "grpc-collector",
        "test-framework"
      ],
      "concepts": [
        "observability",
        "metrics-collection",
        "integration-testing",
        "gRPC",
        "TLS",
        "OpenTelemetry",
        "localhost-endpoint"
      ],
      "severity": "low",
      "userImpact": "Users need confirmation that OpenTelemetry gRPC metrics collection works correctly with the SDK, including TLS support.",
      "rootCause": "Lack of integration test coverage for OpenTelemetry gRPC collector configuration in the SDK.",
      "proposedFix": "Add integration test to verify OtelCollectorConfig with gRPC URL format and TLS support.",
      "workaround": "Use samples repository for testing (referenced in PR temporalio/samples-python#18).",
      "resolution": "wontfix",
      "resolutionDetails": "Integration test moved to samples-python repository instead of primary SDK repo; issue author decided not to complete integration test in SDK.",
      "related": [],
      "keyQuote": "There's not a good way to put an integration test for the collector side in the primary SDK repo, so I added it to the PR at https://github.com/temporalio/samples-python/pull/18.",
      "number": 157,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:47.141Z"
    },
    {
      "summary": "Feature request to remove the custom Go server used for testing in the Python SDK, replacing it with Temporalite which now has search attribute cache disabling and proper TLS base config override capabilities.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-server",
        "custom-server",
        "temporalite-integration"
      ],
      "concepts": [
        "testing",
        "server-replacement",
        "tls-configuration",
        "search-attributes",
        "test-infrastructure"
      ],
      "severity": "low",
      "userImpact": "Simplifies SDK test infrastructure by removing custom server maintenance in favor of the community Temporalite project.",
      "rootCause": null,
      "proposedFix": "Replace custom Go server with Temporalite for testing, leveraging its search attribute cache disabling and TLS base config override features.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Custom Go server was removed in favor of Temporalite after it gained the required features (search attribute cache disabling and TLS base config override).",
      "related": [],
      "keyQuote": "Now that Temporalite is released w/ search attribute cache disabling and proper TLS base config override, we can just use it instead",
      "number": 156,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:46.875Z"
    },
    {
      "summary": "Request to add metric temporality configuration to telemetry settings, aligning with changes made in the core SDK. This is a duplicate of issue #141.",
      "category": "feature",
      "subcategory": "telemetry-config",
      "apis": [],
      "components": [
        "telemetry",
        "metrics",
        "configuration"
      ],
      "concepts": [
        "metric-temporality",
        "telemetry-configuration",
        "observability",
        "metrics-export",
        "sdk-alignment"
      ],
      "severity": "low",
      "userImpact": "Users cannot configure metric temporality options in the Python SDK telemetry configuration, limiting observability customization.",
      "rootCause": null,
      "proposedFix": "Implement metric temporality configuration similar to the core SDK (see PR #402 and #404 in sdk-core)",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #141",
      "related": [
        141
      ],
      "keyQuote": "Dupe of https://github.com/temporalio/sdk-python/issues/141",
      "number": 155,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:44.296Z"
    },
    {
      "summary": "Support replaying multiple workflows by adding lazy history fetching and modifying the replayer to accept an iterable of histories instead of a single history, enabling efficient streaming of workflow histories.",
      "category": "feature",
      "subcategory": "workflow-replay",
      "apis": [],
      "components": [
        "replayer",
        "history",
        "worker"
      ],
      "concepts": [
        "lazy-loading",
        "streaming",
        "workflow-replay",
        "history-management",
        "multi-workflow",
        "iteration"
      ],
      "severity": "medium",
      "userImpact": "Enables users to replay multiple workflows efficiently without loading all histories into memory at once.",
      "rootCause": null,
      "proposedFix": "Support lazy history fetching integrated with stream-map on lazy lists, and refactor replayer to accept iterable of histories using sdk-core PR #400 streaming capabilities.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented multi-workflow replayer support with lazy history fetching and iterable-based replayer API.",
      "related": [
        153,
        158,
        400
      ],
      "keyQuote": "Change replayer to accept iterable of histories instead of a single one and use streaming capabilities",
      "number": 154,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:33.239Z"
    },
    {
      "summary": "Rework search attributes to support individual, per-type attributes with collections only for keywords, and expose a workflow listing API on the client as a lazy iterator.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "client",
        "search-attributes",
        "workflow-listing"
      ],
      "concepts": [
        "search-attributes",
        "type-safety",
        "lazy-iteration",
        "api-design",
        "workflow-query"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to define type-safe search attributes and programmatically list workflows through the client API.",
      "rootCause": null,
      "proposedFix": "Implement per-type search attributes with keyword-only collections and add workflow listing lazy iterator to client",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #381 and #366",
      "related": [
        381,
        366
      ],
      "keyQuote": "Need to support individual, per-type search attributes and only allow collections for keywords",
      "number": 153,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:31.407Z"
    },
    {
      "summary": "Request for custom failure converters in the Python SDK, similar to implementations in TypeScript and Go SDKs. This feature allows developers to customize how exceptions are converted to Temporal failures.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "failure-converter",
        "exception-handling"
      ],
      "concepts": [
        "error-conversion",
        "exception-mapping",
        "failure-serialization",
        "customization",
        "sdk-parity"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot customize how exceptions are converted to Temporal failures, limiting their ability to handle domain-specific error scenarios.",
      "rootCause": null,
      "proposedFix": "Implement custom failure converters similar to the TypeScript PR #887 and Go PR #924.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #142, which addresses the same feature request.",
      "related": [
        142
      ],
      "keyQuote": "Dupe of https://github.com/temporalio/sdk-python/issues/142",
      "number": 152,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:30.976Z"
    },
    {
      "summary": "Telemetry initialization fails when using the replayer without a client, causing a panic exception about uninitialized metrics.",
      "category": "bug",
      "subcategory": "telemetry-replayer",
      "apis": [],
      "components": [
        "telemetry",
        "replayer",
        "metrics"
      ],
      "concepts": [
        "initialization",
        "telemetry",
        "replayer",
        "process-wide",
        "metrics"
      ],
      "severity": "high",
      "userImpact": "Users cannot use the replayer functionality without encountering a panic exception when telemetry is not properly initialized.",
      "rootCause": "Process-wide telemetry is not initialized when using replayer without a client.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved in issue #158",
      "related": [
        158
      ],
      "keyQuote": "Tried to use a metric but telemetry has not been initialized",
      "number": 151,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:18.126Z"
    },
    {
      "summary": "When one activity in a parallel asyncio.gather() fails, it causes the entire workflow to fail and cancels remaining activities, preventing their successful results from being saved to workflow history. Users expected successful activity completions to be persisted even when other parallel activities fail.",
      "category": "bug",
      "subcategory": "parallel-activities",
      "apis": [
        "execute_activity",
        "gather"
      ],
      "components": [
        "workflow-executor",
        "activity-orchestration",
        "error-handling"
      ],
      "concepts": [
        "parallel-execution",
        "exception-handling",
        "activity-cancellation",
        "workflow-state",
        "asyncio",
        "error-propagation"
      ],
      "severity": "medium",
      "userImpact": "Users lose successful activity results when any parallel activity fails, making it impossible to partially recover from failures in parallel workflows.",
      "rootCause": "asyncio.gather() with return_exceptions=False (default) propagates the first exception immediately, causing the workflow to complete and cancel all outstanding activities per Temporal server behavior.",
      "proposedFix": "Use asyncio.gather() with return_exceptions=True to handle exceptions individually and allow other activities to complete before the workflow fails.",
      "workaround": "Set return_exceptions=True in asyncio.gather() to allow activities to fail individually without cancelling the entire workflow.",
      "resolution": "invalid",
      "resolutionDetails": "Issue was determined to be user error - expected behavior clarified. Users should use return_exceptions=True in gather() to handle individual activity failures.",
      "related": [],
      "keyQuote": "If return_exceptions is False (default), the first raised exception is immediately propagated to the task that awaits on gather().",
      "number": 150,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:18.589Z"
    },
    {
      "summary": "Running two workers for the same task queue in a single Python process causes a \"History is out of order\" fatal error in the workflow engine. The user was attempting to achieve task priority by running multiple workers on the same queue, but this configuration triggers an ordering issue in the event history.",
      "category": "bug",
      "subcategory": "worker-concurrency",
      "apis": [
        "Worker"
      ],
      "components": [
        "worker",
        "workflow-engine",
        "history-processing"
      ],
      "concepts": [
        "concurrency",
        "event-ordering",
        "task-queue",
        "worker-priority",
        "history-management"
      ],
      "severity": "high",
      "userImpact": "Users cannot run multiple workers on the same task queue within a single process, preventing task prioritization strategies.",
      "rootCause": "Multiple workers processing history events for the same workflow concurrently causes events to be received out of order, violating the sequential ordering requirement of the workflow history.",
      "proposedFix": null,
      "workaround": "Run multiple workers on the same task queue in separate processes instead of within a single Python process.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was escalated to sdk-core repository (temporalio/sdk-core#428) for investigation and resolution.",
      "related": [
        428
      ],
      "keyQuote": "In the meantime, it is rarely ever necessary to run the multiple workers for the same task queue in the same process.",
      "number": 149,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:17.025Z"
    },
    {
      "summary": "Users reported that clients created in one asyncio event loop may not work properly if used in a different event loop. The issue requests clarification, documentation, and potentially error handling around this behavior.",
      "category": "docs",
      "subcategory": "asyncio-event-loop",
      "apis": [
        "Client"
      ],
      "components": [
        "client",
        "event-loop",
        "asyncio"
      ],
      "concepts": [
        "event-loop",
        "asyncio",
        "thread-safety",
        "client-lifecycle",
        "concurrent-execution"
      ],
      "severity": "medium",
      "userImpact": "Users may encounter unexpected behavior or failures when attempting to use a client across different asyncio event loops or threads.",
      "rootCause": "The Python client is not thread-safe and does not support usage across different event loops than the one it was created in.",
      "proposedFix": "Document the limitation that clients must be used within the same event loop they were created in, and potentially add runtime error handling to prevent misuse.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Confirmed the limitation exists and documented that clients are not thread-safe and should not be run in a separate event loop or thread.",
      "related": [],
      "keyQuote": "a client can run in a different event loop than it was created, but it is _not_ thread safe and therefore should not be run in a separate event loop",
      "number": 147,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:00.904Z"
    },
    {
      "summary": "Improve the API and mechanism for sync activity cancellation in Python SDK. Currently requires throwing CancelledError, but the team decided to implement PyThreadState_SetAsyncExc by default with options to disable or shield from thread exceptions.",
      "category": "feature",
      "subcategory": "activity-cancellation",
      "apis": [
        "@activity.defn"
      ],
      "components": [
        "activity-executor",
        "sync-activity-wrapper",
        "cancellation-handler"
      ],
      "concepts": [
        "cancellation",
        "thread-safety",
        "exception-handling",
        "sync-async-bridge",
        "user-experience"
      ],
      "severity": "medium",
      "userImpact": "Provides a cleaner, more intuitive API for handling activity cancellation without requiring users to manually throw exceptions.",
      "rootCause": "Sync activities lack a friendly mechanism to bubble out cancellation signals; requiring manual CancelledError throws is not user-friendly.",
      "proposedFix": "Use PyThreadState_SetAsyncExc via Rust FFI (pyo3) by default, with optional `no_thread_exception=True` in @activity.defn decorator and `activity.no_thread_exception()` context manager for shielding.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented PyThreadState_SetAsyncExc-based cancellation with configurable behavior through decorator parameters and context managers.",
      "related": [],
      "keyQuote": "Use https://docs.python.org/3/c-api/init.html#c.PyThreadState_SetAsyncExc by default with optional disable decorator and shielding context manager",
      "number": 146,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:04.748Z"
    },
    {
      "summary": "When a workflow is cancelled, the status returned by `wf_handle.describe().status` incorrectly shows `WorkflowExecutionStatus.FAILED` instead of `WorkflowExecutionStatus.CANCELED`. The SDK is using `fail_workflow_execution` instead of `cancel_workflow_execution`.",
      "category": "bug",
      "subcategory": "workflow-status",
      "apis": [
        "describe",
        "cancel"
      ],
      "components": [
        "workflow-instance",
        "workflow-executor",
        "status-handling"
      ],
      "concepts": [
        "workflow-cancellation",
        "execution-status",
        "error-handling",
        "status-reporting"
      ],
      "severity": "high",
      "userImpact": "Users cannot correctly determine if a workflow was cancelled vs failed, requiring workarounds to check for CancelledError.",
      "rootCause": "The SDK calls `fail_workflow_execution` instead of `cancel_workflow_execution` when handling workflow cancellation.",
      "proposedFix": "Change to `cancel_workflow_execution` instead of `fail_workflow_execution` and confirm in tests.",
      "workaround": "Check if status is FAILED and the error cause is CancelledError to determine if workflow was actually cancelled.",
      "resolution": "fixed",
      "resolutionDetails": "Implementation fix confirmed in workflow_instance.py to use correct cancellation execution method.",
      "related": [],
      "keyQuote": "Change to `cancel_workflow_execution` instead of `fail_workflow_execution` and confirm in tests",
      "number": 145,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:52:02.957Z"
    },
    {
      "summary": "The Python SDK's search attribute datetime parsing using `datetime.fromisoformat` fails to parse ISO 8601 strings with 'Z' timezone designator (RFC3339Nano format). This breaks workflow scheduling when the Temporal server sends timestamps like '2022-10-30T06:28:30Z'.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "converter",
        "search-attributes",
        "worker"
      ],
      "concepts": [
        "datetime-parsing",
        "ISO-8601",
        "RFC3339",
        "timezone-handling",
        "datetime-formatting",
        "search-attributes"
      ],
      "severity": "high",
      "userImpact": "Users experience workflow failures when using the Scheduler system because search attributes with RFC3339 timestamps cannot be parsed.",
      "rootCause": "Python's `datetime.fromisoformat()` does not support the 'Z' timezone designator used in RFC3339Nano format, only ISO 8601 basic format without timezone info.",
      "proposedFix": "Use `dateutil.parser.isoparse()` from the third-party dateutil package instead of `datetime.fromisoformat()` for full RFC3339 compliance.",
      "workaround": "Users can use the dateutil package with `parser.isoparse()` directly, though this requires adding a dependency.",
      "resolution": "fixed",
      "resolutionDetails": "Adopted dateutil dependency to replace datetime.fromisoformat for proper RFC3339Nano support.",
      "related": [],
      "keyQuote": "This does not support parsing arbitrary ISO 8601 strings - it is only intended as the inverse operation of datetime.isoformat()",
      "number": 144,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:51:45.060Z"
    },
    {
      "summary": "Pydantic models with nested non-JSON-serializable field types (like UUID lists) cannot be serialized because `dict()` doesn't produce JSON-friendly output. The SDK needs to handle custom types or provide workarounds for users.",
      "category": "bug",
      "subcategory": "pydantic-serialization",
      "apis": [],
      "components": [
        "payload-converter",
        "json-encoder",
        "pydantic-support"
      ],
      "concepts": [
        "serialization",
        "json-compatibility",
        "custom-types",
        "pydantic-models",
        "type-conversion",
        "nested-objects"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use Pydantic models with non-JSON-friendly types like UUID as workflow/activity parameters without custom workarounds.",
      "rootCause": "Pydantic's `dict()` method doesn't convert non-JSON-friendly types like UUID to JSON-serializable forms, and Pydantic lacks a built-in `json_dict()` method for nested serialization.",
      "proposedFix": "Add support for specific types like UUID through Pydantic's type system, and provide sample code showing how to create custom payload converters for full Pydantic model support using the slower JSON string approach.",
      "workaround": "Users can create a custom payload converter by replacing the default JSONPlainPayloadConverter with their own implementation, optionally using orjson for better performance.",
      "resolution": "fixed",
      "resolutionDetails": "Decided to add support for UUID and other specific types, provide documentation for custom converters, and wait for Pydantic to add JSON-able dict support rather than accepting the performance penalty of json() round-tripping.",
      "related": [],
      "keyQuote": "The best solution right now I think is to accept anything that has a callable `json` attribute and invoke it, then turn it _back_ to Python object form via `json.loads`.",
      "number": 143,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:51:48.649Z"
    },
    {
      "summary": "Feature request to implement FailureConverter concept in Python SDK, aligning with TypeScript SDK implementation (PR #887). This depends on completing work in issue #164.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "failure-converter",
        "error-handling"
      ],
      "concepts": [
        "error-conversion",
        "exception-handling",
        "sdk-alignment",
        "serialization"
      ],
      "severity": "medium",
      "userImpact": "Users need consistent error handling patterns across Temporal SDKs for converting failures between different representations.",
      "rootCause": null,
      "proposedFix": "Implement FailureConverter concept following TypeScript SDK PR #887 design patterns.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented as part of the error handling standardization work, dependent on completion of issue #164.",
      "related": [
        131,
        164
      ],
      "keyQuote": "See https://github.com/temporalio/sdk-typescript/pull/887. Also relates to #131.",
      "number": 142,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:51:45.069Z"
    },
    {
      "summary": "Feature request to expose OpenTelemetry temporality configuration from sdk-core to the Python SDK, allowing users to control how metrics are aggregated over time.",
      "category": "feature",
      "subcategory": "observability-otel",
      "apis": [],
      "components": [
        "otel-integration",
        "metrics-configuration"
      ],
      "concepts": [
        "observability",
        "opentelemetry",
        "temporality",
        "metrics",
        "configuration"
      ],
      "severity": "low",
      "userImpact": "Users cannot configure OpenTelemetry temporality settings, limiting control over how metrics are aggregated in their observability stack.",
      "rootCause": null,
      "proposedFix": "Expose temporality configuration from sdk-core pull request #402",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Temporality configuration was exposed in the Python SDK based on sdk-core PR #402",
      "related": [
        402
      ],
      "keyQuote": "Expose temporality configuration from https://github.com/temporalio/sdk-core/pull/402",
      "number": 141,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:51:27.374Z"
    },
    {
      "summary": "Feature request to improve the Python SDK documentation site (python.temporal.io) with a better page title and custom favicon for better browser tab visibility and consistency with other SDK documentation sites.",
      "category": "feature",
      "subcategory": "documentation-website",
      "apis": [],
      "components": [
        "documentation-site",
        "web-ui"
      ],
      "concepts": [
        "branding",
        "user-experience",
        "site-consistency",
        "favicon",
        "page-title"
      ],
      "severity": "low",
      "userImpact": "Users viewing Python SDK documentation in multiple browser tabs have difficulty distinguishing the Python docs tab from other SDK documentation.",
      "rootCause": null,
      "proposedFix": "Add custom favicon and improve page title to match other SDK documentation sites (TypeScript, Go, Java).",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer determined that adding custom favicon was acceptable but improving the page title was not worth the effort given the minimal benefit.",
      "related": [],
      "keyQuote": "Yes, Python is missing the suffix, but it's not worth the effort of a custom template.",
      "number": 140,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:51:29.731Z"
    },
    {
      "summary": "Request to expose the default sequence of payload converters so users can reference it when creating custom converters. Also requests making it easier to create a data converter with a different encoding payload converter prepended to the defaults.",
      "category": "feature",
      "subcategory": "payload-converter",
      "apis": [],
      "components": [
        "payload-converter",
        "data-converter",
        "converter-chain"
      ],
      "concepts": [
        "converter-composition",
        "custom-converters",
        "encoding",
        "extensibility",
        "default-configuration"
      ],
      "severity": "low",
      "userImpact": "Users cannot easily reference or extend the default payload converter configuration when building custom converters.",
      "rootCause": "The default sequence of payload converters is only defined in the default constructor and not exposed as a public constant or method.",
      "proposedFix": "Expose the default payload converter list and add a builder method like `temporalio.converter.default().with_encoding_payload_converter(...)` for easier customization.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The default converters were exposed and convenience methods were added for easier composition with custom encodings.",
      "related": [],
      "keyQuote": "Make the list available... Maybe something like `temporalio.converter.default().with_encoding_payload_converter(...)`",
      "number": 139,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:51:31.745Z"
    },
    {
      "summary": "User requested better documentation about exception handling in activities, specifically which exceptions cause failures, retries, and how to use ApplicationError in activities similar to workflows.",
      "category": "docs",
      "subcategory": "exception-handling",
      "apis": [
        "ApplicationError"
      ],
      "components": [
        "activity-executor",
        "error-handling",
        "retry-policy"
      ],
      "concepts": [
        "exception-handling",
        "retry-policy",
        "activity-failure",
        "non-retryable-errors",
        "timeout"
      ],
      "severity": "low",
      "userImpact": "Users lack clear documentation on how to properly raise and handle exceptions in activities, leading to confusion about retry behavior and failure handling.",
      "rootCause": null,
      "proposedFix": "Add exception handling documentation section to README and improve docs.temporal.io with Python-specific activity exception examples",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation improvements were communicated to the documentation team to properly document errors for both workflows and activities",
      "related": [
        679
      ],
      "keyQuote": "All are retryable by default except for 1) if you create ApplicationError with non_retryable=True, 2) the retry_policy set when you invoke the activity has a non_retryable_error_types that includes the unqualified name of the exception class",
      "number": 134,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:51:16.098Z"
    },
    {
      "summary": "Transitive dependency vulnerability (CVE-2022-3212) in axum-core 0.2.7 allows denial of service through unbounded request body handling. Detected in temporal-sdk-core-protos dependency chain.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "temporal-sdk-core-protos",
        "tonic",
        "axum",
        "dependency-management"
      ],
      "concepts": [
        "denial-of-service",
        "memory-exhaustion",
        "vulnerability",
        "transitive-dependency",
        "request-handling",
        "security"
      ],
      "severity": "high",
      "userImpact": "Malicious peers could send large request bodies causing the SDK server to run out of memory and crash.",
      "rootCause": "axum-core FromRequest extractor did not enforce limits on request body size, allowing memory exhaustion attacks.",
      "proposedFix": "Upgrade axum-core from 0.2.7 to 0.2.8",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Vulnerability was resolved by upgrading the dependency; issue auto-closed when vulnerable library was removed from inventory or marked as ignored.",
      "related": [],
      "keyQuote": "axum_core::extract::FromRequest would not, by default, set a limit for the size of the request body. That meant if a malicious peer would send a very large (or infinite) body your server might run out of memory and crash.",
      "number": 133,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:51:14.225Z"
    },
    {
      "summary": "The recently added Payload field in the Failure message needs to be processed through payload codecs, similar to other failure attributes. This is a follow-up to add codec support for encoded attributes.",
      "category": "feature",
      "subcategory": "codecs",
      "apis": [
        "Failure"
      ],
      "components": [
        "codecs",
        "failure-handling",
        "payload-processing"
      ],
      "concepts": [
        "payload-codecs",
        "encoding",
        "failure-attributes",
        "data-encoding"
      ],
      "severity": "medium",
      "userImpact": "Without codec support for Failure.encodedAttributes, custom payload codecs cannot properly encode/decode failure data, limiting codec functionality.",
      "rootCause": null,
      "proposedFix": "Apply the payload codec pipeline to the newly added Payload field in the Failure message.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented by applying codecs to Failure.encodedAttributes as part of the payload codec integration.",
      "related": [
        164
      ],
      "keyQuote": "Recently added Payload to the `Failure` message, needs to be run through the payload codecs.",
      "number": 131,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:51:11.959Z"
    },
    {
      "summary": "OutboundInterceptor does not support get_workflow_execution_history() method calls, preventing custom interceptors from running on workflow result retrieval operations. User needs interceptor support for runtime modifications like adding hostname-pid suffixes to workflow IDs in shared testing namespaces.",
      "category": "feature",
      "subcategory": "interceptor-workflow-result",
      "apis": [
        "get_workflow_handle",
        "result",
        "describe",
        "get_workflow_execution_history"
      ],
      "components": [
        "interceptor",
        "client",
        "workflow-handle"
      ],
      "concepts": [
        "interceptor-plugin",
        "cross-cutting-concerns",
        "outbound-requests",
        "runtime-modification",
        "workflow-execution"
      ],
      "severity": "medium",
      "userImpact": "Users building cross-cutting layers via interceptors cannot intercept workflow result retrieval, limiting ability to apply dynamic modifications to workflow execution queries.",
      "rootCause": null,
      "proposedFix": "Add interceptor support for get_workflow_execution_history() or provide a way to intercept getting a handle so it can be subclassed.",
      "workaround": "Extend the client and return a custom subclass of WorkflowHandle in an override of get_workflow_handle().",
      "resolution": "fixed",
      "resolutionDetails": "Team opened cross-SDK tracking issue (sdk-features#130) to implement interceptor support for result operations across all SDKs.",
      "related": [
        130
      ],
      "keyQuote": "there should be at least a way for OutboundInterceptor to run before get_workflow_execution_history()",
      "number": 130,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:58.659Z"
    },
    {
      "summary": "Conditions are incorrectly triggered when processing queries and patch notifications in Python SDK, causing unwanted commands to be generated. This issue was previously fixed in TypeScript SDK and needs the same fix applied to Python.",
      "category": "bug",
      "subcategory": "workflow-query",
      "apis": [
        "wait_condition"
      ],
      "components": [
        "query-processor",
        "patch-notification-handler",
        "workflow-execution"
      ],
      "concepts": [
        "condition-triggering",
        "query-processing",
        "patch-notifications",
        "command-generation",
        "event-handling"
      ],
      "severity": "medium",
      "userImpact": "Users may experience unexpected command generation when using patched statements within wait_condition functions, leading to incorrect workflow behavior.",
      "rootCause": "Python SDK does not properly filter out queries and patch notifications when processing conditions, unlike the TypeScript SDK which has this fix.",
      "proposedFix": "Apply the same fix that was implemented in TypeScript SDK to the Python SDK's condition handling logic.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the fix was applied to match TypeScript SDK behavior.",
      "related": [],
      "keyQuote": "We recently fixed this for TypeScript, Python should do the same.",
      "number": 129,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:55.229Z"
    },
    {
      "summary": "User requests support for Pydantic's SecretStr and SecretField types to protect sensitive data from appearing in Temporal logs and UI. The issue was resolved by directing users to use custom data converters for encryption rather than implementing Pydantic-specific support in the SDK.",
      "category": "feature",
      "subcategory": "serialization",
      "apis": [],
      "components": [
        "data-converter",
        "payload-converter",
        "serialization"
      ],
      "concepts": [
        "security",
        "sensitive-data",
        "encryption",
        "pydantic",
        "logging",
        "custom-converter"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily protect sensitive Pydantic fields from appearing in Temporal UI and logs without implementing custom converters.",
      "rootCause": "The SDK does not have Pydantic-specific serialization support for secret types; Temporal Server itself is language-agnostic and serializes to JSON without language-specific construct awareness.",
      "proposedFix": "Implement custom payload converters to handle encryption of sensitive data, as shown in samples-python issues #5 and #15.",
      "workaround": "Create a custom data converter using the data_converter parameter in the Client.connect() method to manually encrypt/decrypt sensitive fields.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed by reporter after understanding that Temporal's solution for sensitive data is end-to-end encryption via custom converters, not Python/Pydantic-specific field hiding.",
      "related": [
        5,
        15
      ],
      "keyQuote": "The Temporal server is not written in Python and does not know anything about Pydantic...you'll have to manually do so.",
      "number": 128,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:51:00.288Z"
    },
    {
      "summary": "Pydantic BaseModel objects fail to serialize to JSON in the Python SDK, causing workflows to crash with 'not JSON serializable' errors. The issue was resolved by implementing a Pydantic encoder in the SDK's converter.",
      "category": "bug",
      "subcategory": "serialization",
      "apis": [
        "execute_workflow",
        "execute_activity"
      ],
      "components": [
        "converter",
        "json-encoder",
        "payload-serialization"
      ],
      "concepts": [
        "serialization",
        "pydantic",
        "json-encoding",
        "type-compatibility",
        "data-passing"
      ],
      "severity": "high",
      "userImpact": "Users cannot pass Pydantic model instances to workflows or activities, preventing adoption of Pydantic for data validation.",
      "rootCause": "The JSON encoder in temporalio/converter.py does not recognize Pydantic BaseModel objects as JSON serializable types.",
      "proposedFix": "Implement a Pydantic-aware encoder in the converter that handles BaseModel serialization, completed in PR #102.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented in PR #102 which was released after the issue was filed.",
      "related": [
        102
      ],
      "keyQuote": "Object of type SignupCreate is not JSON serializable",
      "number": 126,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:42.573Z"
    },
    {
      "summary": "Mark global tracing/telemetry options as experimental to signal that the API may change. Currently, telemetry configuration is process-global through `temporalio.bridge.telemetry`, but the team wants to enable worker-specific settings in the future.",
      "category": "other",
      "subcategory": "telemetry",
      "apis": [],
      "components": [
        "telemetry",
        "bridge",
        "tracing"
      ],
      "concepts": [
        "global-configuration",
        "experimental-api",
        "worker-specific-settings",
        "telemetry",
        "process-wide-settings"
      ],
      "severity": "low",
      "userImpact": "Users will be informed that global telemetry configuration may change, allowing them to anticipate future API modifications.",
      "rootCause": null,
      "proposedFix": "Mark the top-level telemetry call in `temporalio.bridge.telemetry` as experimental to indicate the API may not last forever.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Global tracing options were marked as experimental to signal potential future changes and prevent the API from moving to public-facing packages prematurely.",
      "related": [
        384
      ],
      "keyQuote": "For now we need to make it clear on the API that this may not last forever.",
      "number": 125,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:44.076Z"
    },
    {
      "summary": "Child workflow invocation currently requires explicit workflow IDs. This feature request proposes making workflow IDs optional for in-workflow child workflow invocation, with automatic UUID generation as the default.",
      "category": "feature",
      "subcategory": "child-workflows",
      "apis": [
        "StartChildWorkflow"
      ],
      "components": [
        "workflow-engine",
        "child-workflow-executor"
      ],
      "concepts": [
        "workflow-id",
        "uuid-generation",
        "optional-parameters",
        "child-workflows",
        "default-values"
      ],
      "severity": "low",
      "userImpact": "Users would have simpler API for child workflow invocation without needing to manually generate and manage workflow IDs.",
      "rootCause": null,
      "proposedFix": "Make workflow ID optional in child workflow invocation API and default to auto-generated UUID.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "For in-workflow child workflow invocation, we should not require a workflow ID but instead make it optional and default to a UUID.",
      "number": 124,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:40.034Z"
    },
    {
      "summary": "Request to upgrade Python proto library from 3.x to support both 3.x and 4.x versions, similar to Pulumi's approach. Discussion includes considerations about gRPC version compatibility and memory leak concerns.",
      "category": "feature",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "proto-library",
        "grpc-dependency",
        "python-sdk"
      ],
      "concepts": [
        "dependency-upgrade",
        "version-compatibility",
        "grpc",
        "protobuf",
        "memory-leak"
      ],
      "severity": "medium",
      "userImpact": "Users are limited to proto 3.x and grpc 1.48.0+, preventing use of newer proto versions and potentially exposing them to known memory leaks in certain grpc versions.",
      "rootCause": "Current constraint to proto 3.x due to grpcio optional dependency requirements, and known memory leak in grpc versions >=1.43.0",
      "proposedFix": "Update dependency constraints to allow both proto 3.x and 4.x, potentially enabled by grpc PR 30382",
      "workaround": "Pin gRPC dependency to 1.41.0 which is known to be stable and free of memory leak issues",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by implementing support for proto 4.x alongside 3.x compatibility",
      "related": [
        30382
      ],
      "keyQuote": "Pulumi seems to allow both: https://github.com/pulumi/pulumi/blob/master/sdk/python/requirements.txt. See if we can too.",
      "number": 123,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:29.517Z"
    },
    {
      "summary": "Remove abbreviated option names in worker configuration to improve clarity. Options like `max_concurrent_wft_polls` should be renamed to `max_concurrent_workflow_task_polls` and `max_concurrent_at_polls` to `max_concurrent_activity_task_polls`.",
      "category": "other",
      "subcategory": "worker-options",
      "apis": [],
      "components": [
        "worker-options",
        "configuration"
      ],
      "concepts": [
        "naming-clarity",
        "api-design",
        "abbreviations",
        "developer-experience"
      ],
      "severity": "low",
      "userImpact": "Users working with worker options must remember cryptic abbreviations, reducing code readability and increasing learning curve.",
      "rootCause": null,
      "proposedFix": "Rename abbreviated option names to their full descriptive equivalents throughout the worker options API.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Option names were expanded to use full descriptive names instead of abbreviations for improved API clarity.",
      "related": [],
      "keyQuote": "In worker options, `max_concurrent_wft_polls` should be `max_concurrent_workflow_task_polls` and `max_concurrent_at_polls` should be `max_concurrent_activity_task_polls`.",
      "number": 122,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:29.660Z"
    },
    {
      "summary": "Refactor all Python SDK tests to use a test framework with multiple server forms instead of the current Go-based binary wrapper. This involves converting client/workflow tests to use injected test environments, updating activity tests, and running CI tests against both time-skipping and Temporalite test servers.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-framework",
        "client-tests",
        "workflow-tests",
        "activity-tests",
        "ci"
      ],
      "concepts": [
        "testing",
        "test-server",
        "temporalite",
        "time-skipping",
        "test-environment",
        "go-removal",
        "ci-automation"
      ],
      "severity": "medium",
      "userImpact": "Developers will have a more robust and maintainable test framework that works with multiple server forms, reducing Go binary dependencies.",
      "rootCause": null,
      "proposedFix": "Migrate tests to use injected test environments, convert activity tests to use local workflows or activity test environment, and update CI to run against multiple test servers.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Tests were successfully converted to use the test framework with support for multiple server forms.",
      "related": [
        81,
        121
      ],
      "keyQuote": "Change all client/workflow tests to inject a test environment instead of the Go-based one",
      "number": 120,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:27.763Z"
    },
    {
      "summary": "Remove the Callable shortcut from client interceptor type union. The interceptor type currently accepts both Interceptor objects and Callables that transform OutboundInterceptors, but this shortcut is unnecessary and should be removed.",
      "category": "feature",
      "subcategory": "interceptor-api",
      "apis": [
        "Interceptor",
        "OutboundInterceptor"
      ],
      "components": [
        "client",
        "interceptor",
        "type-system"
      ],
      "concepts": [
        "api-simplification",
        "type-union",
        "callable-removal",
        "client-configuration"
      ],
      "severity": "low",
      "userImpact": "Users will need to use Interceptor objects directly instead of Callable shortcuts, simplifying the API contract.",
      "rootCause": "The Callable shortcut in the type union is redundant and adds unnecessary complexity to the client interceptor interface.",
      "proposedFix": "Change the interceptor type from Union[temporalio.client.Interceptor, Callable[[temporalio.client.OutboundInterceptor], temporalio.client.OutboundInterceptor]] to just temporalio.client.Interceptor.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The Callable shortcut was removed from the accepted client interceptor types, simplifying the API to accept only Interceptor objects.",
      "related": [],
      "keyQuote": "Currently a client interceptor is Union[temporalio.client.Interceptor, Callable[[temporalio.client.OutboundInterceptor], temporalio.client.OutboundInterceptor]] but there's no need for the shortcut",
      "number": 119,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:16.298Z"
    },
    {
      "summary": "Avoid redundant JSON enum conversion in history processing when enums have already been converted. The SDK should detect and skip re-conversion to improve performance and prevent potential issues.",
      "category": "feature",
      "subcategory": "history-processing",
      "apis": [],
      "components": [
        "history-processor",
        "enum-conversion"
      ],
      "concepts": [
        "enum-conversion",
        "idempotency",
        "history-replay",
        "optimization",
        "data-integrity"
      ],
      "severity": "medium",
      "userImpact": "Users experience unnecessary overhead and potential data corruption when history containing pre-converted enums is processed multiple times.",
      "rootCause": "The history JSON conversion logic doesn't check if enums have already been converted before attempting conversion again.",
      "proposedFix": "Add detection logic to identify already-converted enums and skip re-conversion in the history processing pipeline.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented idempotent enum conversion that detects and skips already-converted enums in history processing.",
      "related": [
        121
      ],
      "keyQuote": "We need to not convert if already converted.",
      "number": 118,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:12.190Z"
    },
    {
      "summary": "The `get_workflow_handle_for` method does not properly extract and set the result type on the returned workflow handle. Additionally, the `result_type` parameter should be allowed on `get_workflow_handle` for users who know the type.",
      "category": "bug",
      "subcategory": "workflow-handle",
      "apis": [
        "get_workflow_handle_for",
        "get_workflow_handle"
      ],
      "components": [
        "workflow-client",
        "workflow-handle",
        "type-system"
      ],
      "concepts": [
        "type-inference",
        "result-type",
        "type-safety",
        "handle-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot properly type workflow results when using get_workflow_handle_for, requiring manual type specification workarounds.",
      "rootCause": "The get_workflow_handle_for method does not extract the result type from the workflow definition when creating the handle.",
      "proposedFix": "Allow result_type to be explicitly set on get_workflow_handle and automatically extract it in get_workflow_handle_for.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Result type is now properly extracted and set on workflow handles created via get_workflow_handle_for.",
      "related": [],
      "keyQuote": "When using `get_workflow_handle_for` with a workflow, the return type is not extracted and set on the handle.",
      "number": 117,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:13.923Z"
    },
    {
      "summary": "User encountered an error when setting search attributes with non-title-case keys in child workflows. After investigation, it was determined that search attributes must match the names registered on the server, not a format requirement.",
      "category": "question",
      "subcategory": "search-attributes",
      "apis": [
        "execute_child_workflow"
      ],
      "components": [
        "workflow",
        "search-attributes",
        "child-workflow"
      ],
      "concepts": [
        "search-attributes",
        "configuration",
        "validation",
        "server-registration",
        "naming-conventions"
      ],
      "severity": "low",
      "userImpact": "Users may be confused about search attribute key requirements and receive unclear error messages when keys don't match server-registered names.",
      "rootCause": "Search attribute keys must match names registered on the Temporal server; the error message was unclear about this requirement.",
      "proposedFix": null,
      "workaround": "Register search attributes on the server using tctl CLI tool with the exact key names you intend to use.",
      "resolution": "invalid",
      "resolutionDetails": "User misunderstood the feature; search attributes must be registered on the server first. Clarification was provided by maintainer.",
      "related": [],
      "keyQuote": "You can only set search attributes which have been registered/created on the server. See https://docs.temporal.io/tctl/admin/cluster/add-search-attributes for how to do this with the `tctl` CLI tool.",
      "number": 116,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:50:00.946Z"
    },
    {
      "summary": "Setting a memo parameter on `execute_child_workflow` throws a ValueError due to improper protobuf message assignment. The Python proto SDK requires lazy field access for submessages rather than direct assignment to map fields.",
      "category": "bug",
      "subcategory": "child-workflow",
      "apis": [
        "execute_child_workflow"
      ],
      "components": [
        "workflow_instance",
        "proto-handling",
        "child-workflow"
      ],
      "concepts": [
        "memo",
        "protobuf",
        "submessage",
        "map-field",
        "lazy-field-access"
      ],
      "severity": "high",
      "userImpact": "Users cannot set memo values when executing child workflows, blocking a core workflow feature.",
      "rootCause": "Improper proto library usage - direct assignment to map fields is not allowed in Python protobuf SDK; lazy field access is required for submessages.",
      "proposedFix": "Use lazy field access pattern for setting memo values on protobuf map fields instead of direct assignment.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was fixed by properly implementing proto library patterns for map field assignment.",
      "related": [],
      "keyQuote": "With the Python proto SDK, you can't set messages directly on maps/lists. You have to lazily set scalar fields only via lazy message field access.",
      "number": 115,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:57.467Z"
    },
    {
      "summary": "Add support for max task queue activities per second rate limiting in Python SDK, matching functionality already available in TypeScript SDK.",
      "category": "feature",
      "subcategory": "activity-throttling",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "task-queue"
      ],
      "concepts": [
        "rate-limiting",
        "throughput",
        "task-queue",
        "throttling",
        "performance"
      ],
      "severity": "medium",
      "userImpact": "Python SDK users cannot limit activity execution rate per task queue, causing potential resource exhaustion compared to TypeScript users.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to match TypeScript SDK capabilities for rate limiting activities per task queue.",
      "related": [],
      "keyQuote": "Max activities per second is also missing in Python.",
      "number": 114,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:58.244Z"
    },
    {
      "summary": "User requests native UUID support for activity arguments in Python SDK. Currently UUID raises a JSON serialization error, but many Python libraries treat UUID as JSON serializable via string representation.",
      "category": "feature",
      "subcategory": "serialization",
      "apis": [],
      "components": [
        "serialization",
        "activity-arguments",
        "type-conversion"
      ],
      "concepts": [
        "UUID",
        "JSON-serialization",
        "type-support",
        "cross-language-compatibility",
        "dataclass-encoding"
      ],
      "severity": "low",
      "userImpact": "Users cannot use Python UUID objects directly as activity arguments and must manually convert them to strings as a workaround.",
      "rootCause": "UUIDs lack nested encoding capability to JSON, causing issues with dataclasses.asdict() and similar nested encoding scenarios across the SDK.",
      "proposedFix": "Add native UUID support to Python SDK serialization, though nested UUID encoding presents challenges across SDKs.",
      "workaround": "Users can manually convert UUID objects to strings before passing as activity arguments.",
      "resolution": "wontfix",
      "resolutionDetails": "Closed as not supported due to inability to support encoding nested UUIDs to JSON in dataclass structures. Cross-SDK compatibility concerns also factored into the decision.",
      "related": [],
      "keyQuote": "UUIDs don't have nested abilities to auto-encode to JSON, it causes trouble with dataclasses.asdict and similar use cases.",
      "number": 113,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:45.664Z"
    },
    {
      "summary": "Simplify child workflow cancellation by using a single command instead of issuing different commands based on whether the child workflow has started. This was enabled by sdk-core PR #379.",
      "category": "other",
      "subcategory": "child-workflow-cancellation",
      "apis": [
        "CancelChildWorkflow"
      ],
      "components": [
        "child-workflow-executor",
        "command-handler",
        "workflow-engine"
      ],
      "concepts": [
        "cancellation",
        "command-optimization",
        "workflow-lifecycle",
        "state-management",
        "code-simplification"
      ],
      "severity": "low",
      "userImpact": "Users benefit from a simpler, more consistent API for cancelling child workflows regardless of their execution state.",
      "rootCause": "sdk-core previously required different cancel commands based on child workflow start state, now unified",
      "proposedFix": "Use the single cancel command provided by sdk-core PR #379",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented as part of issue #158, utilizing the unified command from sdk-core",
      "related": [
        158
      ],
      "keyQuote": "With https://github.com/temporalio/sdk-core/pull/379 we can now just use one command",
      "number": 111,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:41.492Z"
    },
    {
      "summary": "The SDK currently accepts iterables but may store them without consuming, which is problematic for single-use iterables. The request is to convert all accepted iterables into lists eagerly, or preferably accept Sequence types instead of Iterable everywhere to align with other Python libraries.",
      "category": "feature",
      "subcategory": "api-design",
      "apis": [],
      "components": [
        "type-system",
        "api-interface"
      ],
      "concepts": [
        "iterables",
        "sequences",
        "type-safety",
        "eager-evaluation",
        "api-consistency"
      ],
      "severity": "low",
      "userImpact": "Users passing single-use iterables may encounter unexpected behavior when the SDK stores rather than immediately consumes them.",
      "rootCause": "API accepts Iterable types which can only be iterated once, but implementation stores them for later use without consuming immediately.",
      "proposedFix": "Accept Sequence instead of Iterable everywhere, which is the standard pattern used by other Python libraries.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Replaced Iterable type hints with Sequence throughout the SDK to ensure accepted parameters support indexing and multiple iterations.",
      "related": [],
      "keyQuote": "Rather, just accept `Sequence` instead of `Iterable` everywhere. This is what other Python libraries seem to do.",
      "number": 110,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:43.833Z"
    },
    {
      "summary": "Request to accept timedelta, float, or int for all user-facing duration inputs across the Python SDK. Currently only wait_condition accepts floats; other APIs require timedeltas. The request was closed as won't fix due to API confusion concerns, but the author committed to accepting both types where floats are already supported for consistency.",
      "category": "feature",
      "subcategory": "duration-api",
      "apis": [
        "wait_condition",
        "RetryPolicy"
      ],
      "components": [
        "duration-handling",
        "api-design",
        "type-system"
      ],
      "concepts": [
        "duration",
        "type-flexibility",
        "python-idioms",
        "api-consistency",
        "user-experience"
      ],
      "severity": "low",
      "userImpact": "Developers must use timedeltas everywhere in the SDK, which is less intuitive than Python's native support for float seconds.",
      "rootCause": "Inconsistent duration input handling across the SDK API surface, with only wait_condition accepting float/int seconds.",
      "proposedFix": "Accept Union[timedelta, float, int] for all duration inputs, with timedeltas as the canonical return type.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Closed as won't fix due to concerns about API confusion and burden of conversion utilities. However, author committed to accepting both timedeltas and floats where floats are already supported.",
      "related": [],
      "keyQuote": "Everywhere that we accept duration input we should accept a timedelta or a float/int of seconds. But keep timedeltas as the way we show durations.",
      "number": 109,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:30.177Z"
    },
    {
      "summary": "Mend security scan detected 6 vulnerabilities in go.temporal.io/server-v1.17.1 transitive dependencies, including 4 high-severity CVEs (CVSS 7.5) affecting Apache Thrift, golang.org/x/net, and golang.org/x/text libraries. The issue was automatically closed by Mend as the vulnerable library was removed from inventory.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "dependency-management",
        "build-system"
      ],
      "concepts": [
        "vulnerability",
        "dependency-scanning",
        "transitive-dependencies",
        "denial-of-service",
        "supply-chain-security"
      ],
      "severity": "high",
      "userImpact": "Users running the Python SDK with Temporal Server dependencies could be exposed to remote denial-of-service attacks through vulnerable transitive dependencies.",
      "rootCause": "Transitive dependencies from go.temporal.io/server-v1.17.1 contain known vulnerabilities in Apache Thrift (0.10.0), golang.org/x/net, and golang.org/x/text that allow network-based denial of service attacks.",
      "proposedFix": "Upgrade vulnerable transitive dependencies: Apache Thrift to 0.13.0+, golang.org/x/net and golang.org/x/text to patched versions. Alternatively, upgrade go.temporal.io/server to a version with fixed dependencies.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Automatically closed by Mend bot as the vulnerable library was marked as ignored or removed from the Mend inventory, not indicating actual remediation.",
      "related": [],
      "keyQuote": "This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.",
      "number": 105,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:27.392Z"
    },
    {
      "summary": "Feature request to add `retry_policy` parameter to `continue_as_new()` so that retry policies can be carried over across workflow continuations. Currently, when a workflow continues as new, the server doesn't carry over retry policy settings, causing workflows to fail with `RetryPolicyNotSet` instead of respecting the original retry configuration.",
      "category": "feature",
      "subcategory": "continue-as-new",
      "apis": [
        "continue_as_new",
        "execute_activity",
        "start_workflow"
      ],
      "components": [
        "workflow-continuation",
        "retry-policy",
        "workflow-options"
      ],
      "concepts": [
        "retry",
        "continue-as-new",
        "workflow-state-management",
        "fault-tolerance",
        "idempotency",
        "parameter-propagation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot maintain consistent retry behavior across workflow continuations, leading to unexpected failures when workflows continue as new.",
      "rootCause": "The server doesn't carry over most parameters on continue_as_new requests; only explicitly specified parameters are used on the new workflow execution.",
      "proposedFix": "Add `retry_policy` parameter to the `continue_as_new()` method similar to how `search_attributes` is supported, with the long-term solution being for the temporal server to carry over workflow options by default.",
      "workaround": "Set retry_policy when calling start_workflow, but this only applies to the initial execution, not continuations.",
      "resolution": "fixed",
      "resolutionDetails": "Issue opened in sdk-core (referenced as #372) to implement in shared core code first, then implement in Python SDK.",
      "related": [
        1200,
        1201,
        372
      ],
      "keyQuote": "Server doesn't carry over most parameters. we could work around it by adding `retry_policy` to `continue_as_new`",
      "number": 103,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:26.497Z"
    },
    {
      "summary": "Feature request to enable per-call gRPC configuration for timeouts and headers in the Python SDK, allowing developers to customize gRPC behavior on a per-call basis rather than globally.",
      "category": "feature",
      "subcategory": "grpc-configuration",
      "apis": [],
      "components": [
        "grpc-client",
        "rpc-config",
        "metadata-handler"
      ],
      "concepts": [
        "timeout",
        "headers",
        "metadata",
        "grpc-options",
        "per-call-configuration",
        "request-customization"
      ],
      "severity": "medium",
      "userImpact": "Users need the ability to set different timeouts and headers for individual gRPC calls instead of being limited to global settings.",
      "rootCause": null,
      "proposedFix": "Implement a RpcConfig option with timeout: Optional[timedelta] and metadata: Mapping[str, str] fields for per-call gRPC customization.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Need to be able to set timeouts and headers per gRPC call",
      "number": 101,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:12.645Z"
    },
    {
      "summary": "Test and validate the Linux aarch64 wheel build, which is disabled in CI due to long build times. Resolve the issue that protoc-wheel-0 doesn't have a Linux aarch64 wheel, preventing protoc installation in this environment.",
      "category": "feature",
      "subcategory": "build-infrastructure",
      "apis": [],
      "components": [
        "build-system",
        "ci-pipeline",
        "wheel-builder",
        "protoc-installation"
      ],
      "concepts": [
        "aarch64-support",
        "arm-architecture",
        "cross-platform-builds",
        "ci-optimization",
        "protobuf-compilation",
        "dependency-resolution"
      ],
      "severity": "medium",
      "userImpact": "Users on Linux aarch64 systems need a working wheel distribution, and CI/build infrastructure must support ARM-based runners efficiently.",
      "rootCause": "protoc-wheel-0 lacks Linux aarch64 wheel, and aarch64 builds were disabled in CI due to excessive build time.",
      "proposedFix": "Use buildjet-4vcpu-ubuntu-2204-arm runner to enable aarch64 builds with adequate resources; conditionally skip protoc installation when not needed.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed by using a dedicated ARM runner (buildjet-4vcpu-ubuntu-2204-arm) to enable aarch64 wheel builds in CI.",
      "related": [],
      "keyQuote": "Can now set `runs-on: buildjet-4vcpu-ubuntu-2204-arm` since we now have a runner",
      "number": 100,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:10.905Z"
    },
    {
      "summary": "Request to expose the operator service raw gRPC API in the Python SDK, allowing direct access to operator service functionality.",
      "category": "feature",
      "subcategory": "operator-service",
      "apis": [],
      "components": [
        "operator-service",
        "grpc-client"
      ],
      "concepts": [
        "gRPC",
        "operator-service",
        "service-exposure",
        "raw-api",
        "SDK-expansion"
      ],
      "severity": "medium",
      "userImpact": "Users cannot directly access operator service functionality through the Python SDK without using workarounds.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Operator service raw gRPC API was exposed in the Python SDK.",
      "related": [],
      "keyQuote": null,
      "number": 93,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:49:10.835Z"
    },
    {
      "summary": "Feature request to implement lazy connectivity for the Python SDK client and health checks. The issue discusses design challenges with making client connection lazy when worker creation is synchronous, and proposes requiring lazy connectivity support in the core SDK.",
      "category": "feature",
      "subcategory": "client-connectivity",
      "apis": [
        "Client"
      ],
      "components": [
        "client",
        "worker",
        "connection-management",
        "health-check"
      ],
      "concepts": [
        "lazy-loading",
        "connectivity",
        "async-initialization",
        "worker-creation",
        "health-check",
        "synchronous-api"
      ],
      "severity": "medium",
      "userImpact": "Users cannot defer client connection until actually needed, forcing eager connection establishment which may impact startup time and resource usage.",
      "rootCause": "Worker creation is synchronous in Python (like Rust) but requires an already-connected client, making lazy client connectivity difficult to implement without changing the worker creation API to async.",
      "proposedFix": "Implement lazy connectivity support in the core SDK to allow deferring client connection. For Python workers, disallow lazy clients for now until async worker creation can be supported.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented with the constraint that lazy clients cannot be used with workers in Python until core SDK supports lazy connectivity and Python worker creation becomes async.",
      "related": [
        367,
        385
      ],
      "keyQuote": "For now I just disallow lazy clients to be used for workers. IMO we need to support lazy connectivity in core.",
      "number": 92,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:57.620Z"
    },
    {
      "summary": "When a workflow is started and immediately cancelled, the primary task gets cancelled before the workflow code runs, preventing the cancelled event from being caught. The fix requires deferring the task cancellation to the event loop so user code can execute and respond to cancellation.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "primary_task",
        "event_loop",
        "cancellation"
      ],
      "concepts": [
        "task_cancellation",
        "workflow_execution",
        "event_loop_ordering",
        "timing_issue",
        "race_condition"
      ],
      "severity": "high",
      "userImpact": "Workflows cannot respond to immediate cancellation requests because the cancellation occurs before the workflow code executes.",
      "rootCause": "Primary task is being cancelled synchronously before being added to the event loop, preventing workflow code from running.",
      "proposedFix": "Defer to self._primary_task.cancel() so cancellation is scheduled on the event loop after the workflow start, allowing user code to run first.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We need to defer to `self._primary_task.cancel()` so it is put on the event loop after the start so the user's code still runs and can react to cancellation.",
      "number": 91,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:57.821Z"
    },
    {
      "summary": "When a wait_condition times out and then later receives a successful bool result, attempting to set the result on the already-timed-out future raises an InvalidStateError.",
      "category": "bug",
      "subcategory": "workflow-conditions",
      "apis": [
        "wait_condition"
      ],
      "components": [
        "workflow-engine",
        "future-management",
        "timeout-handling"
      ],
      "concepts": [
        "timeout",
        "future-state",
        "race-condition",
        "callback-execution",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users encounter crashes when wait_condition timeouts coincide with callback success, blocking workflow execution.",
      "rootCause": "The timeout and callback success are both attempting to set the result on the same future, violating the single-assignment invariant of futures.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The wait_condition implementation was modified to prevent result-setting attempts after timeout or on already-resolved futures.",
      "related": [],
      "keyQuote": "When using `workflow.wait_condition`, if a wait condition times out _then_ gets a bool success, we attempt to set the future which causes an `InvalidStateError`.",
      "number": 89,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:55.123Z"
    },
    {
      "summary": "Mend security vulnerability report for Apache Thrift transitive dependency (v0.10.0) with 3 vulnerabilities including 2 high-severity issues (CVE-2019-0205, CVE-2019-0210) affecting availability and 1 medium-severity issue (CVE-2018-11798) affecting confidentiality. Issue was autoclosed as the vulnerable dependency is no longer part of the inventory.",
      "category": "bug",
      "subcategory": "dependency-security-vulnerability",
      "apis": [],
      "components": [
        "dependency-management",
        "thrift-client",
        "ringpop-go"
      ],
      "concepts": [
        "security-vulnerability",
        "transitive-dependency",
        "apache-thrift",
        "availability",
        "confidentiality",
        "cve"
      ],
      "severity": "high",
      "userImpact": "Users relying on temporalio/temporal v1.17.1 were exposed to high-severity denial-of-service vulnerabilities in Apache Thrift 0.10.0 through transitive dependencies.",
      "rootCause": "Apache Thrift 0.10.0 vulnerable to endless loops and panics when fed specific input data; path traversal vulnerability in static web server affecting Go bindings.",
      "proposedFix": "Upgrade Apache Thrift to version 0.13.0 or later",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Autoclosed by Mend as the vulnerable library is no longer part of the Mend inventory (likely due to upstream dependency updates)",
      "related": [],
      "keyQuote": "This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.",
      "number": 88,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:42.399Z"
    },
    {
      "summary": "Request for a high-level API to create and manage Temporal schedules in the Python SDK.",
      "category": "feature",
      "subcategory": "schedules",
      "apis": [
        "Schedule"
      ],
      "components": [
        "client",
        "schedule-manager",
        "api"
      ],
      "concepts": [
        "scheduling",
        "recurring-workflows",
        "time-based-execution",
        "workflow-automation"
      ],
      "severity": "medium",
      "userImpact": "Users lack a convenient high-level API for scheduling workflows, requiring lower-level implementation.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Schedule API was implemented in the Python SDK",
      "related": [],
      "keyQuote": null,
      "number": 87,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:38.856Z"
    },
    {
      "summary": "Python SDK required Rust installation for pip installation on unsupported platforms, complicating deployment. The issue was resolved by publishing pre-built wheels for common platforms (Windows x64, macOS x64/arm64, Linux x64) so Rust is only needed for rare environments.",
      "category": "feature",
      "subcategory": "build-deployment",
      "apis": [],
      "components": [
        "pip-installation",
        "wheel-distribution",
        "rust-bridge"
      ],
      "concepts": [
        "deployment",
        "platform-support",
        "binary-wheels",
        "build-requirements",
        "cross-platform"
      ],
      "severity": "medium",
      "userImpact": "Users without Rust installed cannot easily deploy the Python SDK without building from source.",
      "rootCause": "The Python SDK wraps a Rust core (temporalio/bridge) that requires compilation when pre-built wheels are unavailable for the target platform.",
      "proposedFix": "Publish pre-built wheels for common platforms (Windows x64, macOS x64/arm64, Linux x64 with libc >= 2.17) to eliminate Rust dependency for most users.",
      "workaround": "Use pre-built wheels from GitHub Actions artifacts or build from source if Rust is available.",
      "resolution": "fixed",
      "resolutionDetails": "Beta releases (0.1b1+) included expanded wheel coverage for major platforms, reducing Rust requirement to rare environments only.",
      "related": [
        100
      ],
      "keyQuote": "Our goal is to _not_ require Rust in any way to install. Our last alpha release only had a limited number of wheels pre-built",
      "number": 85,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:43.857Z"
    },
    {
      "summary": "M1 Mac users encounter import errors with grpcio when running Temporal Python workers. The issue was resolved by installing grpcio via conda instead of pip.",
      "category": "bug",
      "subcategory": "installation-m1-compatibility",
      "apis": [],
      "components": [
        "grpcio",
        "package-management",
        "m1-mac-support"
      ],
      "concepts": [
        "platform-compatibility",
        "arm64-architecture",
        "dependency-management",
        "environment-setup",
        "installation-issues"
      ],
      "severity": "medium",
      "userImpact": "M1 Mac users cannot run Temporal Python workers without following a specific workaround for grpcio installation.",
      "rootCause": "grpcio pip package lacks proper M1 Mac (ARM64) support, requiring native compilation or conda installation",
      "proposedFix": null,
      "workaround": "Uninstall grpcio from pip and install via conda instead",
      "resolution": "fixed",
      "resolutionDetails": "User found that installing grpcio via conda resolved the import error for M1 Mac",
      "related": [],
      "keyQuote": "FIXED: uninstall grpcio from pip and install via conda.",
      "number": 84,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:27.644Z"
    },
    {
      "summary": "Request to add README documentation explaining how to disable the deadlock detector in the Python SDK.",
      "category": "docs",
      "subcategory": "documentation",
      "apis": [],
      "components": [
        "deadlock-detector",
        "readme"
      ],
      "concepts": [
        "deadlock-detection",
        "configuration",
        "documentation",
        "runtime-behavior"
      ],
      "severity": "low",
      "userImpact": "Users need clearer documentation on how to configure or disable the deadlock detector feature.",
      "rootCause": null,
      "proposedFix": "Add information to the README explaining deadlock detector disabling options",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Moved to issue #82",
      "related": [
        82
      ],
      "keyQuote": "Moving to #82",
      "number": 83,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:24.643Z"
    },
    {
      "summary": "Implement a workflow replayer for the Python SDK to enable history replay and JSON parsing/fixing capabilities, which is a blocking feature for production use.",
      "category": "feature",
      "subcategory": "workflow-replay",
      "apis": [],
      "components": [
        "replayer",
        "history-parser",
        "workflow-execution"
      ],
      "concepts": [
        "workflow-replay",
        "history-analysis",
        "deadlock-detection",
        "testing",
        "production-readiness"
      ],
      "severity": "high",
      "userImpact": "Users cannot use the Python SDK in production without a replayer for analyzing and fixing workflow execution histories.",
      "rootCause": null,
      "proposedFix": "Implement workflow replayer with history JSON parser and fixer, with configuration to disable deadlock detection.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented through active development on a branch with PR pending.",
      "related": [],
      "keyQuote": "replayer is a blocking feature for production",
      "number": 82,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:25.751Z"
    },
    {
      "summary": "Request to create a test framework for the Python SDK that leverages the test server. This is a foundational testing infrastructure feature needed to support test development across the SDK.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-framework",
        "test-server",
        "testing-infrastructure"
      ],
      "concepts": [
        "testing",
        "test-server",
        "test-framework",
        "integration-testing",
        "sdk-testing"
      ],
      "severity": "high",
      "userImpact": "Users cannot effectively test their workflows without a proper test framework integrated with the test server, limiting the ability to validate Temporal SDK usage.",
      "rootCause": null,
      "proposedFix": "Leverage the test server to build a comprehensive test framework for the Python SDK",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Test framework was implemented leveraging the test server, with dependency on sdk-core issue #368",
      "related": [
        368
      ],
      "keyQuote": "Need test framework that leverages the test server",
      "number": 81,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:12.610Z"
    },
    {
      "summary": "Implement a workflow sandbox environment to run workflows with certain operations disabled, preventing non-deterministic behavior. Research on this capability has already begun.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "workflow-runner",
        "sandbox",
        "execution-environment"
      ],
      "concepts": [
        "determinism",
        "sandboxing",
        "non-deterministic-behavior",
        "workflow-isolation",
        "execution-constraints"
      ],
      "severity": "high",
      "userImpact": "Users need workflow sandboxing to ensure deterministic execution and prevent undefined behavior in their workflows.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Workflow sandbox was implemented to ensure deterministic workflow execution by restricting certain operations.",
      "related": [],
      "keyQuote": "We need to run workflows in a sandbox with some things disabled to prevent non-determinism.",
      "number": 80,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:09.089Z"
    },
    {
      "summary": "Python SDK should accept `host:port` format for client configuration instead of requiring full URLs, aligning with other SDKs. The tls_config parameter should be updated to handle both boolean and TLSConfig options.",
      "category": "feature",
      "subcategory": "client-configuration",
      "apis": [
        "Client"
      ],
      "components": [
        "client",
        "connection",
        "configuration"
      ],
      "concepts": [
        "host-port",
        "tls-configuration",
        "backwards-compatibility",
        "sdk-alignment",
        "client-setup",
        "url-handling"
      ],
      "severity": "medium",
      "userImpact": "Users can configure the client using consistent `host:port` notation across all Temporal SDKs, improving API consistency and developer experience.",
      "rootCause": "Python SDK differs from other SDKs by requiring full URLs instead of accepting host:port format.",
      "proposedFix": "Change tls_config parameter from Optional[TLSConfig] to Union[bool, TLSConfig] with default False. When no scheme is present, set 'http' if tls_config is false or 'https' otherwise. Consider renaming tls_config to tls for consistency.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation accepted the proposed solution with parameter changes and scheme handling logic.",
      "related": [],
      "keyQuote": "All other SDKs use `host:port`, we should too",
      "number": 79,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:48:12.624Z"
    },
    {
      "summary": "The Python SDK's dataclass type hinting support is too limited. Currently only supports basic object conversion via dacite, but lacks support for collections, mappings, unions, and optionals. The team decided to implement a custom JSON deserializer based on pyserde that respects type hints without requiring decorators.",
      "category": "feature",
      "subcategory": "dataclass-serialization",
      "apis": [],
      "components": [
        "dataclass-converter",
        "type-hinting",
        "json-deserialization"
      ],
      "concepts": [
        "type-hints",
        "serialization",
        "deserialization",
        "collections",
        "generics",
        "union-types",
        "optional-types"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily convert JSON to dataclass instances with complex types like List, Dict, Union, or Optional, limiting the flexibility of payload handling.",
      "rootCause": "The current dacite library is too primitive and doesn't support collections, mappings, unions, and optional types out of the box.",
      "proposedFix": "Write a custom JSON deserializer based on pyserde that respects type hints without requiring class decorators or mixins.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Custom JSON deserializer implemented based on pyserde pattern, supporting type hints without decorators.",
      "related": [
        243
      ],
      "keyQuote": "Ok, after internal discussion, we are going to write our own JSON deserializer. I am probably going to borrow from https://github.com/yukinarit/pyserde but remove the requirement to decorate the classes.",
      "number": 78,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:58.121Z"
    },
    {
      "summary": "Workflow cancellation is not properly reported when activity or child workflow cancellation errors bubble up. The Python SDK should convert these errors to workflow cancellation, similar to how TypeScript SDK handles it.",
      "category": "bug",
      "subcategory": "cancellation-handling",
      "apis": [
        "ExecuteActivity",
        "StartChildWorkflow"
      ],
      "components": [
        "workflow-executor",
        "error-handling",
        "cancellation-manager"
      ],
      "concepts": [
        "cancellation",
        "error-bubbling",
        "workflow-state",
        "activity-cancellation",
        "child-workflow-cancellation"
      ],
      "severity": "medium",
      "userImpact": "Users see workflows marked as failed instead of cancelled when activity or child workflow cancellations bubble up, leading to incorrect workflow status reporting.",
      "rootCause": "Python SDK does not properly convert activity and child workflow cancellation errors into workflow cancellation events when a cancellation is requested.",
      "proposedFix": "Implement error conversion logic similar to TypeScript SDK to treat activity/child cancellation errors as workflow cancellation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by implementing proper cancellation error handling to report workflow cancellation correctly.",
      "related": [],
      "keyQuote": "TypeScript properly converts activity error and child workflow error that are caused by cancellation into workflow cancellation when a cancellation is requested. We need to do the same.",
      "number": 70,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:57.109Z"
    },
    {
      "summary": "User requests a way to inject request/job-specific dependencies (like database sessions) into activity functions without creating circular import issues. The issue discusses various approaches including factory patterns and bootstrap functions, with uncertainty about best practices for async Python applications.",
      "category": "feature",
      "subcategory": "dependency-injection",
      "apis": [
        "Worker",
        "activity.defn",
        "Client.connect"
      ],
      "components": [
        "activity-registration",
        "worker-initialization",
        "dependency-management"
      ],
      "concepts": [
        "dependency-injection",
        "circular-dependencies",
        "factory-pattern",
        "hexagonal-architecture",
        "async-patterns"
      ],
      "severity": "medium",
      "userImpact": "Users struggle to properly pass shared resources like database sessions into activities while maintaining type safety and avoiding circular imports in their application architecture.",
      "rootCause": "Temporal Python SDK doesn't provide built-in dependency injection mechanism for activities, forcing users to choose between closures (losing type-safety) or repeated initialization (potential inefficiency).",
      "proposedFix": "Support registering methods on class instances or classes with `__call__` as activities, allowing external state to be stored on the class instance (referenced in #52).",
      "workaround": "Call bootstrap/initialization within each activity function to create dependencies per-activity-execution, or use string form of execute_activity to break circular imports.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed through work on #52 which added support for registering instance methods and callable classes as activities, enabling dependency injection patterns.",
      "related": [
        52,
        66
      ],
      "keyQuote": "We are actively working on #52 which will solve your problem here. You'll be able to more-easily register methods on a class instance or a class with `__call__` as an activity.",
      "number": 68,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:56.977Z"
    },
    {
      "summary": "User requested synchronous versions of workflow signal and handle APIs to call from non-async activities, but was clarified that activities use the client API instead, not the workflow package.",
      "category": "question",
      "subcategory": "activity-signaling",
      "apis": [
        "get_external_workflow_handle_for",
        "signal"
      ],
      "components": [
        "activity",
        "workflow-client",
        "signal-handling"
      ],
      "concepts": [
        "async",
        "non-async code",
        "generator",
        "workflow-communication",
        "threading",
        "activity-execution"
      ],
      "severity": "low",
      "userImpact": "Users may be confused about the correct API to use for signaling workflows from activities, leading to incorrect implementation attempts.",
      "rootCause": "API design confusion - users incorrectly attempted to use workflow package APIs from activities rather than the client API.",
      "proposedFix": "Improve documentation to clearly distinguish between workflow package APIs (for use inside workflows) and client APIs (for use from activities).",
      "workaround": "Use the client API to signal workflows from activities instead of workflow package APIs.",
      "resolution": "wontfix",
      "resolutionDetails": "Not a bug or missing feature - user was using the wrong API. Resolved by clarifying documentation approach.",
      "related": [
        52
      ],
      "keyQuote": "Everything in the workflow package is only for calling from inside the workflow. If you want to signal a workflow from an activity, you'd use a client.",
      "number": 64,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:42.806Z"
    },
    {
      "summary": "Activity info does not expose the retry_policy passed to execute_activity. User wants to access retry policy from within an activity to implement conditional error handling and observability (e.g., logging errors only on final retries).",
      "category": "bug",
      "subcategory": "activity-info",
      "apis": [
        "execute_activity",
        "activity.info()"
      ],
      "components": [
        "activity-executor",
        "activity-info",
        "retry-policy"
      ],
      "concepts": [
        "retry",
        "activity-info",
        "attempt-tracking",
        "error-handling",
        "observability",
        "conditional-logging"
      ],
      "severity": "low",
      "userImpact": "Activities cannot access the retry policy configuration to make intelligent decisions about error reporting and observability without manual parameter passing.",
      "rootCause": "The protobuf definition for activity_task does not expose the retry policy, and the SDK intentionally does not provide this in ActivityInfo to maintain consistency across other SDKs.",
      "proposedFix": "Pass the desired retry attempt threshold as an activity parameter rather than exposing retry_policy in ActivityInfo. Use activity.info().attempt to implement conditional error handling.",
      "workaround": "Create a dataclass to pass the desired error reporting threshold (e.g., send_error_after_attempt) as an activity parameter, or handle error logging/observability in the workflow instead of the activity.",
      "resolution": "wontfix",
      "resolutionDetails": "Design decision: retry_policy is intentionally not exposed in ActivityInfo. Other SDKs (Go, TypeScript, Java) also do not provide this. User should pass retry configuration as parameters to activities or handle observability at the workflow level.",
      "related": [],
      "keyQuote": "None of our SDKs provide the original retry policy via the activity info and Python shouldn't either.",
      "number": 62,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:42.998Z"
    },
    {
      "summary": "High-severity vulnerability (CVE-2023-22895, CVSS 7.5) in transitive dependency bzip2-0.4.3 used by temporal-sdk-core-0.1.0 allows denial of service through integer overflow. Enterprise customers unable to adopt SDK due to security policy restrictions.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "temporal-sdk-core",
        "dependency-management",
        "bzip2-compression"
      ],
      "concepts": [
        "vulnerability",
        "denial-of-service",
        "integer-overflow",
        "dependency-chain",
        "transitive-dependency",
        "cvss-score",
        "security-policy"
      ],
      "severity": "critical",
      "userImpact": "Enterprise customers cannot use the SDK due to high-severity security vulnerabilities blocking adoption.",
      "rootCause": "bzip2 crate version 0.4.3 contains integer overflow in mem.rs triggered by large files, causing DoS; transitive dependency via temporal-sdk-core  zip  bzip2.",
      "proposedFix": "Upgrade bzip2 dependency from 0.4.3 to 0.4.4 or higher where the vulnerability is fixed.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue automatically closed by Mend after vulnerable library was removed from inventory or marked as resolved in dependency chain.",
      "related": [],
      "keyQuote": "The sdk cannot be used by our enterprise customers because of this vulnerability. I believe this is probably the case for temporal customers too.",
      "number": 61,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:39.953Z"
    },
    {
      "summary": "Add support for headers on interceptors and create an OpenTelemetry module with an interceptor for proper span handling throughout the SDK, with testing across SDKs.",
      "category": "feature",
      "subcategory": "interceptors-observability",
      "apis": [],
      "components": [
        "interceptor",
        "opentelemetry",
        "instrumentation"
      ],
      "concepts": [
        "observability",
        "tracing",
        "spans",
        "headers",
        "instrumentation",
        "optional-dependency"
      ],
      "severity": "medium",
      "userImpact": "Users can add observability headers to interceptors and integrate OpenTelemetry for distributed tracing across their Temporal workflows.",
      "rootCause": null,
      "proposedFix": "Provide headers on all interceptors where applicable and create opentelemetry module with an interceptor for handling spans similar to the TypeScript implementation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented with headers support on interceptors and OpenTelemetry module added to the Python SDK.",
      "related": [],
      "keyQuote": "Provide headers on all interceptors where applicable and create opentelemetry module w/ optional dependency that has an interceptor for properly handling spans",
      "number": 60,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:27.700Z"
    },
    {
      "summary": "Users reported that calling gevent.monkey.patch_all() breaks workflow asyncio on gevent 1.5.0. The issue stems from gevent monkey-patching pure Python threading but not the C implementation, causing conflicts with the custom asyncio event loop management. The solution involved creating a separate executor for running the asyncio event loop in a native gevent thread.",
      "category": "bug",
      "subcategory": "gevent-integration",
      "apis": [
        "Worker"
      ],
      "components": [
        "worker",
        "event-loop",
        "task-executor",
        "threading"
      ],
      "concepts": [
        "gevent-compatibility",
        "asyncio-event-loop",
        "thread-local-storage",
        "monkey-patching",
        "executor",
        "threading-model"
      ],
      "severity": "high",
      "userImpact": "Users attempting to use Temporal Python SDK with gevent.monkey.patch_all() experience broken workflow execution on gevent 1.5.0.",
      "rootCause": "gevent monkey patches pure Python threading but not the C implementation of thread locals, causing conflicts when asyncio.run() or loop.run_until_complete() set the running loop on the C thread local, which gevent cannot override.",
      "proposedFix": "Use a separate executor for running the asyncio event loop in a native gevent thread, separate from the executor used for workflow/activity task execution.",
      "workaround": "Users can use a custom CFGThreadPool executor that wraps gevent.threadpool.ThreadPoolExecutor and uses loop.call_soon_threadsafe() to safely set futures from worker threads.",
      "resolution": "fixed",
      "resolutionDetails": "Implemented a solution combining gevent and Temporal by creating a separate executor for the asyncio event loop in a native gevent thread, demonstrated in samples-python PR #84.",
      "related": [],
      "keyQuote": "I basically took your approach but I created a _separate_ executor for running the asyncio event loop in a native gevent thread (separate from the executor that is used to execute workflow/activity tasks).",
      "number": 59,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:27.797Z"
    },
    {
      "summary": "Feature request to deserialize Python tracebacks from stack trace strings in Temporal failures. The SDK should parse stack trace strings back into traceback objects similar to how the Java SDK does, allowing better error context preservation and logging.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "failure-error",
        "exception-handling",
        "traceback"
      ],
      "concepts": [
        "stack-trace",
        "error-deserialization",
        "exception-chain",
        "logging",
        "traceback-preservation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily reconstruct and log stack traces from Temporal failures, requiring manual workarounds to append stack information to exception messages.",
      "rootCause": "Python SDK does not deserialize stack trace strings from Temporal failures back into traceback objects like the Java SDK does.",
      "proposedFix": "Parse stack trace strings using a library like python-tblib to reconstruct traceback objects, similar to Java's implementation.",
      "workaround": "Use a helper function to append stack traces to exception args when formatting logs, as demonstrated in comment #4.",
      "resolution": "wontfix",
      "resolutionDetails": "After investigation, the team decided not to deserialize tracebacks due to Python's traceback immutability constraints. Instead, users can opt-in to appending stack traces to exception messages using a custom helper function.",
      "related": [
        75
      ],
      "keyQuote": "I am closing this for now, but will refer back to it anytime anyone is curious why we can't deserialize tracebacks.",
      "number": 58,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:25.690Z"
    },
    {
      "summary": "Remove gRPC as a regular dependency since the Python SDK uses a Core client instead. Keep gRPC as a dev dependency for testing purposes only.",
      "category": "feature",
      "subcategory": "dependencies",
      "apis": [],
      "components": [
        "grpc",
        "core-client",
        "test-framework"
      ],
      "concepts": [
        "dependency-management",
        "build-optimization",
        "core-integration",
        "test-isolation",
        "pure-python"
      ],
      "severity": "low",
      "userImpact": "Reduces SDK package size and eliminates unnecessary gRPC dependency from production installations.",
      "rootCause": "Pure Python gRPC is generated unnecessarily when the SDK already uses Core client for communication.",
      "proposedFix": "Remove gRPC from regular dependencies and keep it only as a dev dependency for testing.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "gRPC dependency was removed from production dependencies and retained only for development/testing.",
      "related": [],
      "keyQuote": "We generate pure Python gRPC unnecessarily since we have Core client.",
      "number": 56,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:12.295Z"
    },
    {
      "summary": "Transitive dependency rustix-0.38.8 contains a medium severity vulnerability (CVSS 5.5) that can cause memory explosion in the Dir iterator with the linux_raw backend. The issue is in a deep dependency chain through temporal-sdk-core-api and affects the Python SDK build.",
      "category": "bug",
      "subcategory": "dependency-security-vulnerability",
      "apis": [],
      "components": [
        "cargo-dependencies",
        "bridge",
        "temporal-sdk-core-api"
      ],
      "concepts": [
        "security-vulnerability",
        "transitive-dependency",
        "memory-safety",
        "rust-bindings",
        "syscalls"
      ],
      "severity": "medium",
      "userImpact": "Users may be vulnerable to denial-of-service attacks through memory exhaustion when iterating over directories on Linux systems.",
      "rootCause": "rustix's Dir iterator with linux_raw backend has a memory explosion bug affecting versions before 0.35.15, 0.36.16, 0.37.25, and 0.38.19",
      "proposedFix": "Upgrade rustix to version 0.35.15, 0.36.16, 0.37.25, or 0.38.19 or later; potentially requires upgrading h2 from 0.3.16 to enable the rustix update",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Vulnerability was resolved by updating the transitive dependency; the issue was automatically closed and reopened multiple times by Mend monitoring as the vulnerable library status changed in the inventory",
      "related": [],
      "keyQuote": "I think this can be fixed potentially with a cargo update. We just need to get past h2 0.3.16 which we are currently at.",
      "number": 54,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:13.185Z"
    },
    {
      "summary": "Support for non-function callables as activities by using Python's inspect module instead of relying on __code__ attribute. This expands the types of callables that can be used as Temporal activities.",
      "category": "feature",
      "subcategory": "activity-definition",
      "apis": [
        "activity.defn"
      ],
      "components": [
        "activity-decorator",
        "callable-inspection",
        "signature-validation"
      ],
      "concepts": [
        "callable-types",
        "python-inspection",
        "decorator-validation",
        "method-support",
        "class-instances"
      ],
      "severity": "low",
      "userImpact": "Users can now use more types of callables (like class instances with __call__) as activities instead of being limited to functions.",
      "rootCause": "Current implementation checks for __code__ attribute which is not available on all callable types, particularly non-function callables.",
      "proposedFix": "Use Python's inspect module to analyze callable signatures instead of checking for __code__ attribute.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented support for non-function callables using inspect module for signature analysis.",
      "related": [],
      "keyQuote": "We can instead use the inspect module to analyze the callable's signature, allowing a broader set of non-function callables to be used as activities.",
      "number": 52,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:47:10.472Z"
    },
    {
      "summary": "The Python SDK should properly report its version. Currently references a hardcoded version location in the bridge client that may not reflect the actual SDK version.",
      "category": "other",
      "subcategory": "version-reporting",
      "apis": [],
      "components": [
        "bridge",
        "client",
        "version-management"
      ],
      "concepts": [
        "version-reporting",
        "package-metadata",
        "sdk-version",
        "version-consistency"
      ],
      "severity": "low",
      "userImpact": "Users cannot reliably determine which version of the Python SDK they are running.",
      "rootCause": "Version information is hardcoded in bridge/client.py and may become stale or inconsistent.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed as an enhancement, indicating the version reporting was implemented.",
      "related": [],
      "keyQuote": "Properly report version",
      "number": 47,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:57.150Z"
    },
    {
      "summary": "Stack traces for workflow queries are currently poor quality. The request is to improve them, ideally without significant performance impact.",
      "category": "feature",
      "subcategory": "debugging",
      "apis": [],
      "components": [
        "workflow",
        "query",
        "stack-trace"
      ],
      "concepts": [
        "stack-trace",
        "debugging",
        "observability",
        "query-result",
        "performance"
      ],
      "severity": "medium",
      "userImpact": "Developers have difficulty debugging workflows because stack traces from query results lack sufficient detail and clarity.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Make them better hopefully without unnecessary performance penalty",
      "number": 45,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:58.147Z"
    },
    {
      "summary": "Add more comprehensive docstrings at the module and package level to improve documentation and usage guidance for the Python SDK. The enhancement aims to provide clearer explanation of how to use each package, with optional examples.",
      "category": "docs",
      "subcategory": "docstrings",
      "apis": [],
      "components": [
        "documentation",
        "module-docs",
        "package-docs"
      ],
      "concepts": [
        "usability",
        "developer-experience",
        "documentation-coverage",
        "examples",
        "clarity"
      ],
      "severity": "low",
      "userImpact": "Users will have better understanding of how to use different modules and packages through improved inline documentation.",
      "rootCause": null,
      "proposedFix": "Add module/package level docstrings with usage explanations and potentially examples for each package.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Author decided to proceed with minor improvements to docstrings rather than comprehensive overhaul.",
      "related": [],
      "keyQuote": "After some thought, these won't be robust, just minor improvements.",
      "number": 40,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:55.902Z"
    },
    {
      "summary": "README lacks documentation on how to build the SDK from source as a dependency. Users need clear instructions on cloning recursively (including the sdk-core git submodule) and using the locally-built version.",
      "category": "docs",
      "subcategory": "build-documentation",
      "apis": [],
      "components": [
        "build-system",
        "sdk-core",
        "repository-structure"
      ],
      "concepts": [
        "build-from-source",
        "git-submodule",
        "dependency-management",
        "developer-experience",
        "documentation"
      ],
      "severity": "low",
      "userImpact": "Users cannot easily build and use the Python SDK from source without clear instructions, hindering local development and alternative installation workflows.",
      "rootCause": null,
      "proposedFix": "Add a section to README documenting how to build from source, including recursive clone requirement for the sdk-core submodule and example usage of locally-built version.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation was added to README with build-from-source instructions and submodule clone requirements.",
      "related": [],
      "keyQuote": "Add a section for building from source and even an example for using after built from source. Also make it clear that users must clone recursively because `temporalio/bridge/sdk-core` is a git submodule.",
      "number": 39,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:43.754Z"
    },
    {
      "summary": "Request to add public documentation links throughout Python SDK docstrings to help users discover available documentation. The author decided against this approach due to documentation headings changing frequently.",
      "category": "docs",
      "subcategory": "documentation-links",
      "apis": [],
      "components": [
        "docstrings",
        "documentation"
      ],
      "concepts": [
        "documentation",
        "discoverability",
        "api-reference",
        "user-guidance"
      ],
      "severity": "low",
      "userImpact": "Users would benefit from convenient links to relevant documentation directly in the Python SDK docstrings.",
      "rootCause": null,
      "proposedFix": "Sprinkle public documentation links throughout the docstrings.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "The author determined that documentation headings change too frequently to reliably maintain links in code.",
      "related": [],
      "keyQuote": "I am afraid these headings are changing to often to put into code.",
      "number": 37,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:42.189Z"
    },
    {
      "summary": "Request to publish the Python SDK to PyPI package registry. The team needs the package available for integration into their main repository rather than building from scratch.",
      "category": "feature",
      "subcategory": "distribution",
      "apis": [],
      "components": [
        "package-distribution",
        "pypi",
        "build-system"
      ],
      "concepts": [
        "packaging",
        "distribution",
        "pypi",
        "deployment",
        "dependency-management"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily install and integrate the SDK without building from source, blocking adoption and experimentation.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Build and install the package from source locally",
      "resolution": "fixed",
      "resolutionDetails": "The maintainer resolved the blocking issues and published the package to PyPI.",
      "related": [],
      "keyQuote": "This has been published",
      "number": 36,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:44.650Z"
    },
    {
      "summary": "Four security vulnerabilities detected in transitive dependencies of temporalio/temporal v1.14.4, including high-severity issues in Apache Thrift and Prometheus client library that could lead to denial of service attacks.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "thrift-rpc",
        "prometheus-monitoring"
      ],
      "concepts": [
        "security-vulnerability",
        "denial-of-service",
        "dependency-upgrade",
        "transitive-dependency",
        "CVSS-scoring"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to high-severity denial of service vulnerabilities through outdated transitive dependencies in the SDK.",
      "rootCause": "temporalio/temporal v1.14.4 depends on apache/thrift-0.10.0 (via ringpop-go) and prometheus/client_golang-v1.11.0 which contain known security vulnerabilities.",
      "proposedFix": "Upgrade temporalio/temporal to a version using thrift >= 0.13.0 and prometheus/client_golang >= 1.11.1",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was automatically closed by Mend as the vulnerable library was removed from the Mend inventory or marked as ignored in this branch.",
      "related": [],
      "keyQuote": "This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.",
      "number": 30,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:29.935Z"
    },
    {
      "summary": "Move from Sphinx to pydoctor for API documentation generation. Requires upstreaming support for @overload decorator to pydoctor, then making the existing gen-docs-pydoctor alternative the default and removing Sphinx.",
      "category": "other",
      "subcategory": "documentation",
      "apis": [],
      "components": [
        "documentation-generator",
        "pydoctor",
        "sphinx"
      ],
      "concepts": [
        "api-documentation",
        "documentation-tooling",
        "decorator-support",
        "build-system"
      ],
      "severity": "low",
      "userImpact": "Users will benefit from improved API documentation generation and maintenance through pydoctor instead of Sphinx.",
      "rootCause": null,
      "proposedFix": "Make gen-docs-pydoctor the default documentation tool and remove Sphinx integration after upstreaming @overload support to pydoctor.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The migration from Sphinx to pydoctor was completed successfully.",
      "related": [],
      "keyQuote": "We already have `gen-docs-pydoctor` alternative, we just need to make that the default and remove Sphinx.",
      "number": 29,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:31.098Z"
    },
    {
      "summary": "Request to add more comprehensive workflow tests to the Python SDK. The issue references TODO items in test files that outline additional test cases needed.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-suite",
        "workflow-testing"
      ],
      "concepts": [
        "test-coverage",
        "workflow-validation",
        "testing-framework",
        "code-quality"
      ],
      "severity": "low",
      "userImpact": "Incomplete test coverage makes it harder to ensure the Python SDK is reliable and stable for users.",
      "rootCause": null,
      "proposedFix": "Implement the TODO items listed in test files to expand test coverage",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Additional workflow tests were implemented to address the TODOs identified in test files",
      "related": [],
      "keyQuote": "See the list of TODO's at the bottom of test files",
      "number": 28,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:30.030Z"
    },
    {
      "summary": "Request to implement Python SDK patching functionality equivalent to the TypeScript SDK's patching feature documented at https://docs.temporal.io/typescript/patching",
      "category": "feature",
      "subcategory": "patching",
      "apis": [],
      "components": [
        "sdk-python",
        "workflow-engine"
      ],
      "concepts": [
        "versioning",
        "patching",
        "workflow-compatibility",
        "code-evolution"
      ],
      "severity": "medium",
      "userImpact": "Python SDK users cannot easily manage workflow versioning and patching compared to TypeScript SDK users.",
      "rootCause": null,
      "proposedFix": "Implement patching functionality matching the TypeScript SDK implementation",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Patching functionality was implemented in the Python SDK",
      "related": [],
      "keyQuote": "Basically https://docs.temporal.io/typescript/patching in Python",
      "number": 27,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:13.639Z"
    },
    {
      "summary": "Request for full search attribute support on the Python SDK client and within workflows, with proper type safety for search attributes.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "client",
        "workflow-runtime",
        "search"
      ],
      "concepts": [
        "search-attributes",
        "type-safety",
        "querying",
        "filtering",
        "metadata"
      ],
      "severity": "medium",
      "userImpact": "Users need robust, type-safe search attribute support to query and filter workflows effectively.",
      "rootCause": null,
      "proposedFix": "Implement full search attribute support on client and inside workflows with proper typing",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Search attributes support was implemented in the Python SDK with proper typing",
      "related": [],
      "keyQuote": "Full search attribute support on client and inside workflows. Make sure to type the search attributes properly.",
      "number": 26,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:15.544Z"
    },
    {
      "summary": "Fatal worker errors need to propagate out of run() method so developers can handle them, particularly for async context managers where error communication is otherwise limited.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [
        "run"
      ],
      "components": [
        "worker",
        "async-context-manager",
        "error-propagation"
      ],
      "concepts": [
        "fatal-error",
        "error-handling",
        "worker-lifecycle",
        "exception-propagation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably catch and handle fatal worker errors, especially when using async context managers.",
      "rootCause": "Fatal worker errors do not bubble up from run() method, and async context managers lack alternative error communication mechanisms.",
      "proposedFix": "Implement fatal error handler support for async with context managers and ensure errors propagate out of run().",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fatal worker errors now propagate out of run() method and fatal error handlers were added for async context managers.",
      "related": [
        325
      ],
      "keyQuote": "Make sure that fatal worker errors bubble out of `run()` and there is some way for people to add a \"fatal error handler\"",
      "number": 25,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:17.357Z"
    },
    {
      "summary": "Request to add async activity support to the Python SDK for both client-side activity execution and activity implementation. This enhancement would enable users to write async activities and execute them asynchronously.",
      "category": "feature",
      "subcategory": "async-activities",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "activity-client",
        "activity-executor",
        "worker"
      ],
      "concepts": [
        "async",
        "concurrency",
        "activity-execution",
        "non-blocking",
        "async-await"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently write or execute async activities in Python SDK, limiting concurrency patterns and integration with async Python libraries.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Async activity support was implemented in the Python SDK",
      "related": [],
      "keyQuote": "Need async activity support in client and activity itself",
      "number": 24,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:02.900Z"
    },
    {
      "summary": "Need repeatable, parameterized stress tests to understand default maximums and resource utilization under heavy workflow load. Tests should run in CI with lower parameters but be configurable for actual system stress testing.",
      "category": "feature",
      "subcategory": "testing",
      "apis": [],
      "components": [
        "worker",
        "test-framework"
      ],
      "concepts": [
        "stress-testing",
        "performance",
        "resource-utilization",
        "load-testing",
        "benchmarking",
        "defaults",
        "ci-integration"
      ],
      "severity": "medium",
      "userImpact": "Users lack guidance on reasonable resource limits and performance characteristics under heavy workflow load, making it difficult to properly configure and scale production systems.",
      "rootCause": null,
      "proposedFix": "Create repeatable, parameterized stress tests using xk6-temporal that can run in CI with conservative parameters and be configured for real system stress testing.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by implementing stress tests using xk6-temporal framework as suggested in final comment.",
      "related": [
        336
      ],
      "keyQuote": "We need to know reasonable default maximums and resource utilization of heavy workflow use",
      "number": 23,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:00.690Z"
    },
    {
      "summary": "User inquires about the Python SDK roadmap and timeline for production readiness, moving beyond alpha stage. The maintainer responds that development is split into three phases with Phase 1 complete and Phase 2 under active development, targeting beta status soon.",
      "category": "question",
      "subcategory": "roadmap",
      "apis": [],
      "components": [],
      "concepts": [
        "roadmap",
        "production-readiness",
        "alpha",
        "beta",
        "release-phases",
        "maturity",
        "stability"
      ],
      "severity": "low",
      "userImpact": "Users need clarity on SDK maturity and production readiness timeline to make adoption decisions.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "completed",
      "resolutionDetails": "Maintainer provided roadmap overview explaining three-phase approach with Phase 1 complete and Phase 2 in active development targeting beta, plus guidance on community channels.",
      "related": [],
      "keyQuote": "Phase 1 has already been implemented. Phase 2 is under active development and hopefully can get to beta shortly thereafter",
      "number": 19,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:46:02.229Z"
    },
    {
      "summary": "Security vulnerability in transitive dependency: CVE-2022-24713 in regex-1.5.4 crate (CVSS 7.5) causes denial of service through specially crafted regular expressions. This was automatically closed after the vulnerable library was removed from the project inventory.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "dependency-management",
        "regex-parser",
        "build-system"
      ],
      "concepts": [
        "security-vulnerability",
        "denial-of-service",
        "dependency-upgrade",
        "transitive-dependency",
        "cvss-score",
        "regex-parsing"
      ],
      "severity": "high",
      "userImpact": "Users could be affected by denial of service attacks if the vulnerable regex crate version was used to process untrusted regular expressions.",
      "rootCause": "Bug in regex-1.5.4 crate's mitigations designed to prevent denial of service attacks caused by untrusted regexes, allowing specially crafted regexes to bypass the mitigations.",
      "proposedFix": "Upgrade regex dependency from 1.5.4 to 1.5.5 or later",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The vulnerable library was removed from the project inventory or marked as ignored in Mend dependency tracking.",
      "related": [],
      "keyQuote": "It's possible to craft regexes that bypass such mitigations. This makes it possible to perform denial of service attacks by sending specially crafted regexes",
      "number": 18,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:45:48.349Z"
    },
    {
      "summary": "Security vulnerability report for go.temporal.io/server-v1.14.4 with 4 transitive dependency vulnerabilities (3 high, 1 medium severity) in Apache Thrift and Prometheus client. Issue was auto-closed as the vulnerable libraries are no longer part of the inventory.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "server-dependencies"
      ],
      "concepts": [
        "security-vulnerability",
        "denial-of-service",
        "transitive-dependency",
        "apache-thrift",
        "prometheus"
      ],
      "severity": "high",
      "userImpact": "Users of temporalio-sdk-python may be exposed to DoS and potential memory exhaustion attacks through vulnerable transitive dependencies in the Temporal server.",
      "rootCause": "Vulnerable versions of Apache Thrift (0.10.0) and Prometheus client_golang (1.11.0) included as transitive dependencies of go.temporal.io/server-v1.14.4.",
      "proposedFix": "Upgrade Apache Thrift to 0.13.0 or later and Prometheus client_golang to 1.11.1 or later.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Automatically closed by WhiteSource as the vulnerable library was marked as ignored or is no longer part of the inventory.",
      "related": [],
      "keyQuote": "This issue was automatically closed by WhiteSource because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the WhiteSource inventory.",
      "number": 17,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:45:48.852Z"
    },
    {
      "summary": "Multiprocess heartbeat and cancellation mechanism using multiprocessing.Manager() is flaky and confusing. The current implementation moves heartbeats through sync queues to async tasks, and fast heartbeating can block the event loop.",
      "category": "feature",
      "subcategory": "activity-heartbeat",
      "apis": [],
      "components": [
        "multiprocessing-heartbeat",
        "event-loop",
        "activity-executor",
        "queue-management"
      ],
      "concepts": [
        "heartbeat",
        "cancellation",
        "multiprocessing",
        "event-loop",
        "async-sync-bridge",
        "queuing"
      ],
      "severity": "medium",
      "userImpact": "Users writing multiprocess activities may experience flaky heartbeat/cancellation behavior and event loop blocking when heartbeating frequently.",
      "rootCause": "Using multiprocessing.Manager() queues and events with sync-to-async conversion; fast heartbeating holds up event loop due to scheduling overhead.",
      "proposedFix": "Streamline the threaded/multiprocess heartbeat queuing mechanism to be more reliable and efficient.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Try to streamline the threaded/multiprocess heartbeat queuing. Heartbeating too fast can hold up the event loop",
      "number": 12,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:45:45.119Z"
    },
    {
      "summary": "The Python SDK's behavior with fork() is undefined and fails unpredictably, particularly with gRPC client file descriptors that cause crashes during garbage collection in child processes. The request is to ensure fork() either works cleanly without leaks or fails explicitly when attempting to use forked resources.",
      "category": "bug",
      "subcategory": "fork-handling",
      "apis": [],
      "components": [
        "client",
        "core",
        "gRPC"
      ],
      "concepts": [
        "fork",
        "garbage-collection",
        "file-descriptors",
        "child-process",
        "resource-cleanup",
        "undefined-behavior"
      ],
      "severity": "high",
      "userImpact": "Users experience unpredictable failures when using fork() with Temporal Python SDK, particularly from implicit garbage collection crashes during child process cleanup.",
      "rootCause": "Rust file descriptors from gRPC client are copied across fork boundaries, causing crashes when the child process's version is garbage collected due to improper resource sharing across fork.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the client var, when copied across the fork, has Rust file descriptors copied for the gRPC stuff that fail during Rust drop when the child process's version of the var is GC'd",
      "number": 11,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:45:33.368Z"
    },
    {
      "summary": "Investigate support for more generic manylinux tags to build Python wheels compatible with a wider range of Linux distributions and glibc versions. Currently using manylinux_2_31 which limits compatibility.",
      "category": "feature",
      "subcategory": "build-distribution",
      "apis": [],
      "components": [
        "wheel-builder",
        "packaging",
        "linux-build"
      ],
      "concepts": [
        "linux-compatibility",
        "manylinux",
        "glibc-versions",
        "distribution-support",
        "cross-platform-build",
        "wheel-naming"
      ],
      "severity": "medium",
      "userImpact": "Users on Linux systems with older glibc versions cannot use the published wheels and must build from source.",
      "rootCause": null,
      "proposedFix": "Use manylinux container builds targeting older manylinux standards (e.g., manylinux2010 or manylinux2014) instead of manylinux_2_31.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Investigation completed and manylinux support was implemented to increase wheel compatibility across Linux distributions.",
      "related": [],
      "keyQuote": "We can use manylinux container to get Linux builds that work with a wider range of linuxes (e.g. `manylinux2010` or `manylinux2014`).",
      "number": 10,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:45:31.793Z"
    },
    {
      "summary": "Add support for macOS arm64 (Apple Silicon) by building and testing prebuilt wheels on this platform. This requires updating CI configuration and resolving pyo3 cross-compilation challenges.",
      "category": "feature",
      "subcategory": "build-infrastructure",
      "apis": [],
      "components": [
        "build-system",
        "ci-pipeline",
        "packaging"
      ],
      "concepts": [
        "cross-compilation",
        "platform-support",
        "macos-m1",
        "wheel-distribution",
        "ci-build"
      ],
      "severity": "medium",
      "userImpact": "macOS arm64 users can install the SDK without fallback to slower x86_64 emulation or building from source.",
      "rootCause": "pyo3 cross-compilation complexity and lack of dedicated macOS arm64 CI runner",
      "proposedFix": "Update CI build and test configuration to support macOS aarch64 platform, generate prebuilt wheels for this architecture",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "CI pipeline was updated to build and test on macOS arm64, enabling prebuilt wheel distribution for Apple Silicon",
      "related": [],
      "keyQuote": "Need a prebuilt macOS aarch64 wheel. Don't currently have it because cross-compilation is a bit of a pain with pyo3",
      "number": 9,
      "repo": "temporalio-sdk-python",
      "generatedAt": "2026-01-11T20:45:29.955Z"
    }
  ]
}