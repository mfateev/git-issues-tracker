{"assignees":[{"id":"MDQ6VXNlcjEzMTE2OTQ=","login":"vitarb","name":"Vitali","databaseId":0}],"author":{"id":"MDQ6VXNlcjU0NDk5MQ==","is_bot":false,"login":"kelnos","name":"Brian Tarricone"},"body":"## Expected Behavior\r\n\r\nWhen I re-deploy my Temporal cluster, I expect workers and workflow run initiators to not even notice that the topology of the cluster has changed, or that some hosts have gone away and others have reappeared.  Any connection failures due to these changes should be transparently \"fixed up\" by the SDK.\r\n\r\n## Actual Behavior\r\n\r\nDuring cluster deployments I see timeouts and periods of unavailability (with many log error messages with the GRPC `UNAVAILABLE` error in the activity poller).  Sometimes workflow run invocations fail as well with timeout errors.\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n  1. Build a temporal cluster with, say, 6 nodes. \r\n  1. Add some worker nodes implementing a simple workflow, set up to invoke the workflow fairly often, like once every 5 seconds.\r\n  1. Redeploy the entire Temporal cluster.  (In our case, we do a rolling deploy, where we put a new node in, wait for it to join the cluster [checking by polling the cluster metadata endpoint], wait an additional 2 minutes to allow things to settle, and then take the old node out.)\r\n  1. Watch metrics/logs on the worker nodes, and notice errors in the worker/activity pollers, as well as some failures to start workflows.\r\n\r\n## Specifications\r\n\r\n  - Version: 1.0.5\r\n  - Platform: Linux x86_64\r\n\r\n## More Information\r\n\r\nOur network topology uses a service mesh sidecar to communicate between hosts.  A Temporal worker (or workflow-run-invoker) is instructed to talk to Temporal by connecting to a port on localhost that is backed by [Envoy](https://www.envoyproxy.io) running in HTTP2 mode.  Envoy will then (over HTTP2+TLS) connect to Envoy running on the Temporal server, which will then connect to the Temporal server itself (again using HTTP2).\r\n\r\nIn order to accommodate low-traffic workers, initially server-to-server and client-to-Envoy timeouts were set very long, on the order of 10 minutes.  However, that caused up to 10 minute periods of unavailability during deployments.  More recently I have dropped the timeouts down to 65 seconds, and have instructed Envoy to retry transparently on GRPC `UNAVAILABLE` and `DEADLINE-EXCEEDED` errors.  However, we still see intermittent failures during Temporal cluster deployments.\r\n\r\nOn the workers, we have left all the default timeouts alone in the Temporal client (as they seem fairly sane and short).","closedAt":"2024-12-23T12:50:31Z","comments":[{"id":"MDEyOklzc3VlQ29tbWVudDc4NTM5MjI4MQ==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Some logs with stack traces:\r\n```\r\nio.grpc.StatusRuntimeException: UNAVAILABLE: transport is closing\r\n\tat io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)\r\n\tat io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)\r\n\tat io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)\r\n\tat io.temporal.api.workflowservice.v1.WorkflowServiceGrpc.pollWorkflowTaskQueue(WorkflowServiceGrpc.java:2658)\r\n\tat io.temporal.internal.worker.WorkflowPollTask.poll(WorkflowPollTask.java:77)\r\n\tat io.temporal.internal.worker.WorkflowPollTask.poll(WorkflowPollTask.java:37)\r\n\tat io.temporal.internal.worker.Poller.run(Poller.java:273)\r\n\tat io.temporal.internal.worker.Poller.run(Poller.java:242)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n```\r\nand\r\n```\r\nio.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason\r\n\tat io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)\r\n\tat io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)\r\n\tat io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)\r\n\tat io.temporal.api.workflowservice.v1.WorkflowServiceGrpc.pollWorkflowTaskQueue(WorkflowServiceGrpc.java:2658)\r\n\tat io.temporal.internal.worker.WorkflowPollTask.poll(WorkflowPollTask.java:77)\r\n\tat io.temporal.internal.worker.WorkflowPollTask.poll(WorkflowPollTask.java:37)\r\n\tat io.temporal.internal.worker.Poller.run(Poller.java:273)\r\n\tat io.temporal.internal.worker.Poller.run(Poller.java:242)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n```","createdAt":"2021-02-24T21:20:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-java/issues/362#issuecomment-785392281","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc4ODIwMDk3MQ==","author":{"login":"vitarb"},"authorAssociation":"CONTRIBUTOR","body":"There are two potential issues that can cause this:\r\n1. gRPC backoff keeps increasing if the underlying backend is unavailable and can go up to 2 minutes per retry. We've addressed this by periodically resetting gRPC backoff every 10 seconds.\r\n2. Connections are abruptly closed on the server side and client things that they are still active until TCP timeout occurs. For this issue, in the latest release (1.0.6), we've added simple connection management improvement in #351 which would re-establish connections to the server periodically. Also we've added a keep-alive checking on the client side, but it is currently disabled by default as server doesn't have keep-alive friendly configuration yet. Feel free to experiment with the configuration though (see #367 for the list of config parameters).\r\n\r\nWe know about these issues, and will be making more improvements in the future, including server-side connection management and enabling keep-alive check by default for all users.","createdAt":"2021-03-01T19:16:36Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-java/issues/362#issuecomment-788200971","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc4OTE2ODE0NQ==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Great, thank you @vitarb!  We'll be upgrading and doing some experiments over the next couple days.","createdAt":"2021-03-02T19:48:01Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-java/issues/362#issuecomment-789168145","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc5MjA5NDM5NA==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Ok, we've tried 1.0.6 and enabled the keepalive (with a 30 second interval), and things do definitely seem to be better.  We've been unable to repro the issue (so far) with new workflow run invocations failing, though we still do see a bit of log noise from the activity poller threads.  It does appear that all workflows and activities do end up getting run, though I don't have numbers yet on if there's any increase in latency.","createdAt":"2021-03-06T22:48:24Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-java/issues/362#issuecomment-792094394","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDgwOTc5ODQ1OA==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Things have been a lot better over the past few weeks, and we've gone through several Temporal cluster deployments without incident.  We've been using the keepalive mechanism with a 30-second ping.\r\n\r\nOne thing that has been a bit frustrating is that the [logging in Poller.java](https://github.com/temporalio/sdk-java/blob/v1.0.6/temporal-sdk/src/main/java/io/temporal/internal/worker/Poller.java#L70) is causing our pagers to go off, even though the poller reestablishes its connection and recovers on its own.  We end up with 15-20 instances of this error message[0] logged per worker host during the cluster deployment (some coming from `WorkflowPollTask` and some coming from `ActivityPollTask`).\r\n\r\nI've considered adding custom rules to avoid paging in this case, but anything I can come up with feels brittle (and also possibly not safe, and could mask real issues), and these custom rules would have to be manually applied to every new service that starts using Temporal.\r\n\r\nI think what would help us most is if the logging logic could log at `WARN` for an issue that could be transient, and then only escalate to `ERROR` if it's unable to re-establish a connection within a reasonable amount of time.  (In general we only page for some smallish number of occurrences of `ERROR` logs; some teams do page for `WARN`s but only in cases where there are a significantly large number of them.)\r\n\r\n[0] We see several different exception messages on this:\r\n* `UNAVAILABLE: upstream connect error or disconnect/reset before headers. reset reason: connection termination`\r\n* `UNAVAILABLE: transport is closing`\r\n* `DEADLINE_EXCEEDED: deadline exceeded after 69.999973290s. [remote_addr=127.0.0.1/127.0.0.1:17233]`\r\n* `UNAVAILABLE: Network closed for unknown reason`\r\n* `UNAVAILABLE: Subchannel shutdown invoked`","createdAt":"2021-03-29T23:46:37Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-java/issues/362#issuecomment-809798458","viewerDidAuthor":false}],"createdAt":"2021-02-24T00:53:15Z","labels":[{"id":"MDU6TGFiZWwxNjIyNzE5ODQx","name":"bug","description":"Something isn't working","color":"d73a4a"}],"milestone":null,"number":362,"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"state":"CLOSED","title":"Connection management issues with Java SDK","updatedAt":"2024-12-23T12:50:31Z","url":"https://github.com/temporalio/sdk-java/issues/362"}
