{"assignees":[{"id":"MDQ6VXNlcjM3NzA0Nzg=","login":"yux0","name":"Yu Xia","databaseId":0}],"author":{"id":"MDQ6VXNlcjM3MjE0MQ==","is_bot":false,"login":"skrul","name":"Steve Krulewitz"},"body":"**Is your feature request related to a problem? Please describe.**\r\n\r\nI run multiple separate Temporal clusters within a single k8s cluster. Each Temporal cluster has its own separate set of frontend, history, and matching services as well as persistence. Let's say I am running two Temporal clusters called \"A\" and \"B\" in a single k8s cluster. Note that in my setup, there are no networking restrictions on pods within the k8s cluster -- any pod may connect to any other pod if the IP address is known.\r\n\r\nI recently encountered a problem where it appeared that a frontend service from Temporal cluster A was talking to a matching service from Temporal cluster B. This happened during a time where the pods in both of the Temporal clusters were getting cycled a lot due to some AZ balancing automation. It also happens that this particular k8s cluster is configured in such a way that pod IP address reuse is more likely than usual.\r\n\r\nBoth Temporal cluster A and B are running 3 matching nodes each. However, I saw this log line on Temporal cluster A's frontend service:\r\n```\r\n{\"level\":\"info\",\"ts\":\"2021-01-27T00:34:15.414Z\",\"msg\":\"Current reachable members\",\"service\":\"frontend\",\"component\":\"service-resolver\",\"service\":\"matching\",\"addresses\":\"[100.123.207.80:7235 100.123.65.65:7235 100.123.120.28:7235 100.123.60.187:7235 100.123.17.255:7235 100.123.203.172:7235]\",\"logging-call-at\":\"rpServiceResolver.go:266\"}\r\n```\r\nThis is saying that Temporal cluster A's frontend service is seeing 6 matching nodes, three from A and three from B. Yikes.\r\n\r\nI believe what led to this is something like:\r\n1. A matching pod in cluster A gets replaced, releasing its IP address. This IP address remains in cluster A's `cluster_metadata` table.\r\n2. A matching pod is created in cluster B re-using this IP address.\r\n3. An event occurs that causes a frontend in cluster A to re-read the the node membership for its matching nodes. It finds the original matching node's IP address still in the table and it can still connect to it even though it is actually now a matching node in cluster B.\r\n4. Through this matching node in cluster B the the other cluster B matching nodes are located.\r\n\r\nMy fix for this is to make sure that each Temporal cluster has its own set of membership ports for each service. This would have prevented the discovery process in cluster A from seeing the pods in cluster B since it would be trying to connect on a different port.\r\n\r\n**Describe the solution you'd like**\r\n\r\nIt may be possible to prevent this by including a check that a given node is indeed part of the correct cluster before adding it to the ring.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nI don't believe our k8s environment has an easy way to prevent this using networking restrictions.\r\n","closedAt":"2022-12-01T18:20:27Z","comments":[{"id":"MDEyOklzc3VlQ29tbWVudDgyNDQ5MjQ0MA==","author":{"login":"robzienert"},"authorAssociation":"CONTRIBUTOR","body":"Also ran into this issue. My resolution was also to change the membership ports for the different clusters. Normally, I would not expect this to be an issue, but we had two deployments overlap for a time, which would've opened the door for IP reuse.\r\n\r\nAs requested, our deployment for Temporal using a \"rolling red black\", which deploys X percentage of the total fleet, then disables a matching X percentage, stepping up until 100% deployment is complete (as seen here).\r\n\r\n![Screen Shot 2021-04-21 at 7 23 05 PM](https://user-images.githubusercontent.com/108741/115646467-06c25700-a2d7-11eb-9288-950419b2eff4.png)\r\n\r\nNote: When Spinnaker disables a Temporal instance in service discovery, we have a signal that shuts down the Temporal process for that particular instance. So while the infrastructure is not destroyed, the Temporal process on that infrastructure is no longer running.","createdAt":"2021-04-22T02:27:36Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-824492440","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDgzODUwODY2Mg==","author":{"login":"vikasvb90"},"authorAssociation":"NONE","body":"We also experienced exactly the same problem. We have different temporal clusters setup in our lower environments and we use k8s namespaces to mimic different DCs and also, we don't have dedicated network pools in each namespace i.e. any pod can connect to any other pod belonging to a different namespace.\r\n\r\nWe observed a similar problem where namespace ids were unknown to the frontend service. We were getting logs of namespace id not found very frequently. This was because namespace id was unknown to the pod which which was present in different cluster. Since we had very limited IP range allocated for k8s pods, it increased the probability of getting a pod assigned with the same IP in different cluster.\r\n\r\nWith help from @wxing1292 , we resolved this issue by assigning different membership ports to each temporal cluster.\r\n\r\nIt seems like this issue was faced by others. We have temporarily adopted the different port approach but it would be great if we can get a more reliable fix in temporal. Probably attaching a cluster id to membership would help. During pod discovery, we can consider pod IP, port and cluster ID. Just a thought.","createdAt":"2021-05-11T13:49:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-838508662","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDg1OTg2NDM0Nw==","author":{"login":"robzienert"},"authorAssociation":"CONTRIBUTOR","body":"> During pod discovery, we can consider pod IP, port and cluster ID. Just a thought.\r\n\r\nIn such a solution, I'd expect the cluster ID to be an internal UUID or something, rather than use the cluster name from the configuration, as our environments re-use cluster names.","createdAt":"2021-06-11T20:15:03Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-859864347","viewerDidAuthor":false},{"id":"IC_kwDODNqesM415Avx","author":{"login":"aocole"},"authorAssociation":"NONE","body":"IMO this is a major issue. This brought down multiple production temporal clusters for me.","createdAt":"2021-08-23T21:14:35Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-904137713","viewerDidAuthor":false},{"id":"IC_kwDODNqesM42DC7H","author":{"login":"sagikazarmark"},"authorAssociation":"CONTRIBUTOR","body":"How does the service discovery work? In Cadence there were multiple strategies and DNS based discovery worked quite well in Kubernetes. This issue suggests that it now automatically discovers Temporal nodes on the network (?) which causes clusters to collapse, yes, but also rings all my security related bells as well.\r\n\r\nI wouldn't say using mTLS in a Kubernetes cluster between services is an absolute must, but it would be the only protection if this is the case.\r\n\r\nUnless I completely misunderstood what is happening.","createdAt":"2021-08-26T21:52:32Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-906768071","viewerDidAuthor":false},{"id":"IC_kwDODNqesM42DMN8","author":{"login":"nvtkaszpir"},"authorAssociation":"CONTRIBUTOR","body":"for now this might be mitigated by using network policies to limit connectivity between pods within the same k8s namespace\r\nhttps://kubernetes.io/docs/concepts/services-networking/network-policies/\r\nWith exposed services via ingress that may be a bit more problematic :D\r\n\r\nAnother option would be to use different certs/keys/ca per cluster and enforcing tls verification, this way it is possible to deny connection betwee pods from different clusters, because it would be aborted by non-matching certs","createdAt":"2021-08-26T23:18:14Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-906806140","viewerDidAuthor":false},{"id":"IC_kwDODNqesM426SJH","author":{"login":"chitreshd"},"authorAssociation":"NONE","body":"FWIW.. we ran into this issue twice in our clusters","createdAt":"2021-09-16T21:12:44Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-921248327","viewerDidAuthor":false},{"id":"IC_kwDODNqesM437Es5","author":{"login":"swyxio"},"authorAssociation":"CONTRIBUTOR","body":"while we wait for this experience to be improved at source, i've gone ahead and added some advice here https://github.com/temporalio/documentation/pull/662. Wholly out of my depth on this one, reviews and additions welcome as it seems to be a common pain point.","createdAt":"2021-10-07T23:50:06Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-938232633","viewerDidAuthor":false},{"id":"IC_kwDODNqesM44XiHW","author":{"login":"deltasquare4"},"authorAssociation":"NONE","body":"The documentation update is a welcome move. Thanks. We were hit by this hard a couple of days ago. We have had two Temporal instances (staging and production) running in a single kubernetes cluster for almost a year without any issues. An automated GKE upgrade changed something and triggered this issue. Pods from another environment were still listed in the matcher/frontend logs even after we changed the ports for some reason. NetworkPolicy wouldn't have made much sense for us because of how we interact with Temporal.","createdAt":"2021-10-18T12:01:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-945693142","viewerDidAuthor":false},{"id":"IC_kwDODNqesM44f4C5","author":{"login":"bhogatemadhura"},"authorAssociation":"NONE","body":"Has anybody added network policies as a stop-gap solution for time being ? I would appreciate if someone can post the policies they added?","createdAt":"2021-10-20T17:23:53Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-947880121","viewerDidAuthor":false},{"id":"IC_kwDODNqesM44f43A","author":{"login":"nvtkaszpir"},"authorAssociation":"CONTRIBUTOR","body":"this really depends on your setup, you need to understand network policies, interactions between services and write your own rules.","createdAt":"2021-10-20T17:27:53Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-947883456","viewerDidAuthor":false},{"id":"IC_kwDODNqesM44xFVm","author":{"login":"bhogatemadhura"},"authorAssociation":"NONE","body":"we have a scenario where there are multiple temporal namespaces within a cluster.\r\ne.g: `ns1` and `ns2` are the 2 namespaces\r\nI want to restrict the communication between the 2 namespaces so that temporal services don't match with each other instead of defining different membership ports within each namespace.\r\n\r\nThe policy for `ns1` will look something like - \r\n\r\n```\r\napiVersion: networking.k8s.io/v1\r\nkind: NetworkPolicy\r\nmetadata:\r\n  name: temporal-network-policy\r\n  namespace: ns1\r\nspec:\r\n  egress:\r\n  - to:\r\n    - namespaceSelector:\r\n        matchLabels:\r\n          name: ns1\r\n  ingress:\r\n  - from:\r\n    - namespaceSelector:\r\n        matchLabels:\r\n          name: ns1\r\n  podSelector:\r\n    matchLabels:\r\n      networpolicy: temporal\r\n  policyTypes:\r\n  - Ingress\r\n  - Egress\r\n  ``` \r\n  \r\n  here the `ns1` does have a label already defined\r\n  ```\r\napiVersion: v1\r\nkind: Namespace\r\n  labels:\r\n    name:namespace1\r\n  name: namespace1\r\n  ```\r\n  \r\n  and the temporal pods within `ns1` have the label `networpolicy= temporal`\r\n  \r\n  This policy should restricts the ingress/egress traffic for temporal pods within the namespace `ns1` itself. However, I still temporal pods in `ns1` matching with services in `ns2`.\r\n  \r\n  Is this not the way to go about resolving this issue ?\r\n  \r\n  Will appreciate your help.","createdAt":"2021-10-26T22:58:56Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-952391014","viewerDidAuthor":false},{"id":"IC_kwDODNqesM44x2v6","author":{"login":"nvtkaszpir"},"authorAssociation":"CONTRIBUTOR","body":"First, you need to set policy to drop all Ingress/egress per namespace and\nallow only specific traffic - this is safer but needs more work.\n\nOr you may add policy to drop traffic from specific pods from outside of\nthe given namespace.\n\nOn Wed, 27 Oct 2021, 00:59 Madhura Bhogate, ***@***.***>\nwrote:\n\n> we have a scenario where there are multiple temporal namespaces within a\n> cluster.\n> e.g: ns1 and ns2 are the 2 namespaces\n> I want to restrict the communication between the 2 namespaces so that\n> temporal services don't match with each other instead of defining different\n> membership ports within each namespace.\n>\n> The policy for ns1 will look something like -\n>\n> apiVersion: networking.k8s.io/v1\n> kind: NetworkPolicy\n> metadata:\n>   name: temporal-network-policy\n>   namespace: ns1\n> spec:\n>   egress:\n>   - to:\n>     - namespaceSelector:\n>         matchLabels:\n>           name: ns1\n>   ingress:\n>   - from:\n>     - namespaceSelector:\n>         matchLabels:\n>           name: ns1\n>   podSelector:\n>     matchLabels:\n>       networpolicy: temporal\n>   policyTypes:\n>   - Ingress\n>   - Egress\n>\n> here the ns1 does have a label already defined\n>\n> apiVersion: v1\n> kind: Namespace\n> labels:\n>   name:namespace1\n> name: namespace1\n>\n> and the temporal pods within ns1 have the label networpolicy= temporal\n>\n> This policy should restricts the ingress/egress traffic for temporal pods\n> within the namespace ns1 itself. However, I still temporal pods in ns1\n> matching with services in ns2.\n>\n> Is this not the way to go about resolving this issue ?\n>\n> Will appreciate your help.\n>\n> â€”\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/temporalio/temporal/issues/1234#issuecomment-952391014>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALJMPCKRYREHEYO6FJJKDDUI4XDZANCNFSM4WV3B2RQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n","createdAt":"2021-10-27T06:49:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-952593402","viewerDidAuthor":false},{"id":"IC_kwDODNqesM45b_CP","author":{"login":"jammerful"},"authorAssociation":"NONE","body":"I also hit this issue while running multiple temporal clusters in single k8s cluster, as noted here on the community forums: https://community.temporal.io/t/not-found-namespace-id-xxxxxx-xxxxx-xxxxx-xxxxx-xxxxxxxxxxxx-not-found/3320/7\r\n\r\nA documentation update would be highly appreciated for those that will try to adopt temporal after us, obviously would love the fix as well.","createdAt":"2021-11-08T22:35:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-963637391","viewerDidAuthor":false},{"id":"IC_kwDODNqesM46CQDY","author":{"login":"mathobbs"},"authorAssociation":"NONE","body":"@sw-yx re the added doc [\"FAQ: Multiple deployments on a single cluster\" documentation](https://docs.temporal.io/docs/server/production-deployment/#faq-multiple-deployments-on-a-single-cluster) the membership port change suggestion for Temporal2 uses port numbers from the default grpcPorts (7233 to 8233, etc.).  Clearer would be to base on the default membershipPorts (e.g. 6933 to 16933, etc).","createdAt":"2021-11-19T02:24:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-973668568","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5PhdPI","author":{"login":"yux0"},"authorAssociation":"MEMBER","body":"The cluster id is added into the membership app name. https://github.com/temporalio/temporal/pull/3415.","createdAt":"2022-12-01T18:20:27Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1234#issuecomment-1334170568","viewerDidAuthor":false}],"createdAt":"2021-01-27T19:58:49Z","labels":[{"id":"MDU6TGFiZWwxNjIxMDMwNzg3","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwyNTc2NTM1ODQ2","name":"difficulty: easy","description":"","color":"68d662"}],"milestone":null,"number":1234,"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":4}}],"state":"CLOSED","title":"Prevent incorrect service discovery with multiple Temporal clusters","updatedAt":"2022-12-01T18:20:27Z","url":"https://github.com/temporalio/temporal/issues/1234"}
