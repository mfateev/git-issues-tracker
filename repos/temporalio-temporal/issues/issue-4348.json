{"assignees":[{"id":"MDQ6VXNlcjM3NzA0Nzg=","login":"yux0","name":"Yu Xia","databaseId":0}],"author":{"id":"MDQ6VXNlcjEwNzQ5MzYx","is_bot":false,"login":"emmercm","name":"Christian Emmer"},"body":"_Carried over from this community thread: https://community.temporal.io/t/what-is-the-correct-way-to-disable-re-enable-multi-cluster-replication/8216?u=emmercm_\r\n\r\n## Expected Behavior\r\n\r\nWhen two clusters are replicating to each other, and one is taken offline for an extended period of time (longer than namespace retention windows), then when the cluster is brought back online it should catch up on replication.\r\n\r\n## Actual Behavior\r\n\r\nNo workflow history replication is occurring, including workflows newly started after the secondary cluster was brought back online.\r\n\r\nThe trio of error logs that I see constantly coming from the primary cluster's history service are, in order:\r\n\r\n- ```json\r\n  {\r\n    \"msg\": \"Persistent fetch operation Failure\",\r\n    \"wf-run-id\": \"...\",\r\n    \"store-operation\": \"get-wf-execution\",\r\n    \"shard-id\": 383,\r\n    \"address\": \"...:7234\",\r\n    \"wf-namespace-id\": \"...\",\r\n    \"stacktrace\": \"go.temporal.io/server/common/log.(*zapLogger).Error\\n\\t/home/builder/temporal/common/log/zap_logger.go:144\\ngo.temporal.io/server/service/history/workflow.getWorkflowExecution\\n\\t/home/builder/temporal/service/history/workflow/transaction_impl.go:423\\ngo.temporal.io/server/service/history/workflow.(*ContextImpl).LoadMutableState\\n\\t/home/builder/temporal/service/history/workflow/context.go:263\\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).processReplication\\n\\t/home/builder/temporal/service/history/replication/ack_manager.go:582\\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).generateHistoryReplicationTask\\n\\t/home/builder/temporal/service/history/replication/ack_manager.go:431\\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).toReplicationTask\\n\\t/home/builder/temporal/service/history/replication/ack_manager.go:356\\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).getTasks\\n\\t/home/builder/temporal/service/history/replication/ack_manager.go:280\\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).GetTasks\\n\\t/home/builder/temporal/service/history/replication/ack_manager.go:224\\ngo.temporal.io/server/service/history/api/replication.GetTasks\\n\\t/home/builder/temporal/service/history/api/replication/get_tasks.go:60\\ngo.temporal.io/server/service/history.(*historyEngineImpl).GetReplicationMessages\\n\\t/home/builder/temporal/service/history/historyEngine.go:750\\ngo.temporal.io/server/service/history.(*Handler).GetReplicationMessages.func1\\n\\t/home/builder/temporal/service/history/handler.go:1417\",\r\n    \"level\": \"error\",\r\n    \"wf-id\": \"...\",\r\n    \"error\": \"context canceled\",\r\n    \"logging-call-at\": \"transaction_impl.go:423\",\r\n    \"ts\": \"2023-05-16T17:39:45.746Z\"\r\n  }\r\n  ```\r\n- ```json\r\n  {\r\n    \"msg\": \"replication task reader encounter error, return earlier\",\r\n    \"component\": \"replicator-queue-processor\",\r\n    \"shard-id\": 383,\r\n    \"address\": \"...:7234\",\r\n    \"stacktrace\": \"go.temporal.io/server/common/log.(*zapLogger).Error\\n\\t/home/builder/temporal/common/log/zap_logger.go:144\\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).getTasks\\n\\t/home/builder/temporal/service/history/replication/ack_manager.go:281\\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).GetTasks\\n\\t/home/builder/temporal/service/history/replication/ack_manager.go:224\\ngo.temporal.io/server/service/history/api/replication.GetTasks\\n\\t/home/builder/temporal/service/history/api/replication/get_tasks.go:60\\ngo.temporal.io/server/service/history.(*historyEngineImpl).GetReplicationMessages\\n\\t/home/builder/temporal/service/history/historyEngine.go:750\\ngo.temporal.io/server/service/history.(*Handler).GetReplicationMessages.func1\\n\\t/home/builder/temporal/service/history/handler.go:1417\",\r\n    \"level\": \"error\",\r\n    \"error\": \"context canceled\",\r\n    \"logging-call-at\": \"ack_manager.go:281\",\r\n    \"ts\": \"2023-05-16T17:39:45.746Z\"\r\n  }\r\n  ```\r\n- ```json\r\n  {\r\n    \"msg\": \"Failed to retrieve replication messages.\",\r\n    \"shard-id\": 383,\r\n    \"address\": \"...:7234\",\r\n    \"stacktrace\": \"go.temporal.io/server/common/log.(*zapLogger).Error\\n\\t/home/builder/temporal/common/log/zap_logger.go:144\\ngo.temporal.io/server/service/history/api/replication.GetTasks\\n\\t/home/builder/temporal/service/history/api/replication/get_tasks.go:66\\ngo.temporal.io/server/service/history.(*historyEngineImpl).GetReplicationMessages\\n\\t/home/builder/temporal/service/history/historyEngine.go:750\\ngo.temporal.io/server/service/history.(*Handler).GetReplicationMessages.func1\\n\\t/home/builder/temporal/service/history/handler.go:1417\",\r\n    \"level\": \"error\",\r\n    \"error\": \"context canceled\",\r\n    \"logging-call-at\": \"get_tasks.go:66\",\r\n    \"ts\": \"2023-05-16T17:39:45.746Z\"\r\n  }\r\n  ```\r\n\r\nThe `Persistent fetch operation Failure` error seems to be the root problem. I would have expected `shard.Context.GetWorkflowExecution()` to return `serviceerror.NotFound` if the old workflows couldn't be found, though, so I'm confused by that.\r\n\r\nSome other metrics, carried over from the linked community thread:\r\n\r\n- The primary cluster's:\r\n  - Metric `persistence_error_with_type{operation=\"getreplicationtasks\"}` with `error_type=\"serviceerrorunavailable\"` is emitting at a fairly constant rate\r\n  - Metric `replication_tasks_fetched` is a flat zero\r\n  - Table `replication_tasks` is only being `INSERT`ed to, never `DELETE`d from. It has >2.3mil rows.\r\n  - DB has no obvious errors or timeouts.\r\n\r\nI'm happy to gather any other metrics that would help debug the issue.\r\n\r\nGiven the `ORDER BY` on `SELECT task_id, data, data_encoding FROM replication_tasks WHERE shard_id = ? AND task_id >= ? AND task_id < ? ORDER BY task_id LIMIT ?`, I don't think this will ever resolve on its own. \r\n\r\n## Steps to Reproduce the Problem\r\n\r\n1. Have two Temporal clusters:\r\n   1. With 512 history shards\r\n   2. In multi-cluster replication, and observe it is working as expected\r\n2. Have all namespaces with:\r\n   1. A default 72h retention period\r\n   2. The default 4 task queue partitions \r\n3. Have all namespaces active in the \"primary\" cluster, none active in the \"secondary cluster\"\r\n4. Scale the secondary cluster down to zero replicas\r\n5. Wait an extended period of time, e.g. 2 weeks\r\n   1. During this time, the primary cluster is still processing workflows, at a rate of ~240/hour for a total of ~140k completed while the secondary cluster is offline\r\n6. Scale the secondary cluster back above zero replicas\r\n7. Observe that no replication is occurring, based on the metrics above\r\n\r\n## Specifications\r\n\r\n- Version: Temporal server v1.19.1\r\n- Platform: Kubernetes & Docker `temporalio/server:1.19.1`, `docker.io/temporalio/server@sha256:c8a5cdb7c78d26c9d611ce19abb62733dfe5480e02d40a39968bd9b2ab8b45c2`\r\n- Persistence store: MySQL v8 via Vitess\r\n","closedAt":null,"comments":[{"id":"IC_kwDODNqesM5cr9ro","author":{"login":"emmercm"},"authorAssociation":"NONE","body":"I was able to reproduce this locally with Docker Compose. The steps were:\r\n\r\n1. Have two `temporalio/auto-setup:1.19.1` containers, both backed by MySQL\r\n2. Set up multi-cluster replication between the two\r\n3. Start a workflow worker application that is connected to the primary cluster only\r\n4. Start 100 workflows in the primary cluster, have the worker application process & complete them\r\n5. Observe the workflow history is replicated to the secondary cluster, via the secondary cluster's web UI\r\n6. Start triggering 60k workflows in the primary cluster\r\n7. Wait ~10sec, then observe that some of the workflows that have been started so far have been replicated to the secondary cluster\r\n8. Stop the secondary cluster's container\r\n9. Wait until every workflow has been completed or timed out, as observed in the primary cluster's web UI\r\n10. Stop the primary cluster's container (in order to purge any kind of in-memory caches)\r\n11. Observe that the primary cluster's MySQL has ~282k `replication_tasks` rows and ~39k `executions` rows (some of the workflows timed out)\r\n12. Delete every row in the primary cluster's `executions` and `current_executions` tables\r\n13. Re-start the primary cluster's container\r\n14. Re-start the secondary cluster's container\r\n15. After waiting >10min, continue to observe:\r\n  1. The primary cluster's MySQL still has ~282k `replication_tasks` rows\r\n  2. The primary cluster is emitting logs described in the original post\r\n\r\nThen, to see what would happen with newly started workflows with both clusters running, I did:\r\n\r\n1. Start another 100 workflows in the primary cluster\r\n2. Keep both cluster containers running\r\n3. Observe that the workflows were all processed & completed by the worker application that was never stopped\r\n4. Observe that the workflows were never replicated to the secondary cluster","createdAt":"2023-05-19T17:50:24Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/4348#issuecomment-1555028712","viewerDidAuthor":false}],"createdAt":"2023-05-16T18:58:35Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":4348,"reactionGroups":[],"state":"OPEN","title":"Replication tasks referencing archived workflow executions can't be processed, blocking all replication","updatedAt":"2023-05-19T21:56:57Z","url":"https://github.com/temporalio/temporal/issues/4348"}
