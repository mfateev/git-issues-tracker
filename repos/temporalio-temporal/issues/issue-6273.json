{"assignees":[],"author":{"id":"MDQ6VXNlcjg3NDcwODQ=","is_bot":false,"login":"yurkeen","name":"Yury Evtikhov"},"body":"We run schema update jobs during our deployment process. The command looks like the following\r\n```yaml\r\ncommand: [\"temporal-sql-tool\", \"--plugin\", \"postgres\", \"update-schema\", \"-d\", \"{{ $storeConfig.schemaPath }}\"]\r\n```\r\n\r\n## Expected Behavior\r\nIn the testing/staging environment where our Temporal instances don't have much workload and where the persistency and visibility databases are both relatively small, migrations run without any issues with the desired effect:\r\n\r\n```\r\n2024-07-11T13:38:03.670Z\tINFO\tUpdateSchemaTask started\t{\"config\": {\"DBName\":\"\",\"TargetVersion\":\"\",\"SchemaDir\":\"/etc/temporal/schema/postgresql/v12/visibility/versioned\",\"SchemaName\":\"\",\"IsDryRun\":false}, \"logging-call-at\": \"updatetask.go:104\"}\r\n2024-07-11T13:38:03.675Z\tDEBUG\tSchema Dirs: [v1.6]\t{\"logging-call-at\": \"updatetask.go:212\"}\r\n2024-07-11T13:38:03.675Z\tINFO\tProcessing schema file: v1.6/fix_root_workflow_info.sql\t{\"logging-call-at\": \"updatetask.go:256\"}\r\n2024-07-11T13:38:03.675Z\tDEBUG\trunning 1 updates for current version 1.5\t{\"logging-call-at\": \"updatetask.go:138\"}\r\n2024-07-11T13:38:03.675Z\tDEBUG\t---- Executing updates for version 1.6 ----\t{\"logging-call-at\": \"updatetask.go:157\"}\r\n2024-07-11T13:38:03.675Z\tDEBUG\tDROP INDEX by_root_workflow_id;\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.068Z\tDEBUG\tDROP INDEX by_root_run_id;\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.794Z\tDEBUG\tALTER TABLE executions_visibility DROP COLUMN root_workflow_id;\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.798Z\tDEBUG\tALTER TABLE executions_visibility DROP COLUMN root_run_id;\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.799Z\tDEBUG\tALTER TABLE executions_visibility ADD COLUMN root_workflow_id VARCHAR(255) NOT NULL DEFAULT '';\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.802Z\tDEBUG\tALTER TABLE executions_visibility ADD COLUMN root_run_id VARCHAR(255) NOT NULL DEFAULT '';\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.804Z\tDEBUG\tCREATE INDEX by_root_workflow_id ON executions_visibility (namespace_id, root_workflow_id, (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n...\r\n2024-07-11T13:38:49.703Z\tINFO\tUpdateSchemaTask done\t{\"logging-call-at\": \"updatetask.go:127\"}\r\n```\r\n## Actual Behavior\r\nFor our production instances, however, which are quite active and where databases are in constant use (~1TB for persistence and ~120 Gb visibility), index creation fails constantly.\r\nPart of the equation is that we have deadlock detection mechanism and our PostgreSQL instances would terminate certain blocking queries running over a configured time limit. This is often the case when manipulating indices referencing busy tables:\r\n\r\n```\r\n2024-07-11T13:38:03.670Z\tINFO\tUpdateSchemaTask started\t{\"config\": {\"DBName\":\"\",\"TargetVersion\":\"\",\"SchemaDir\":\"/etc/temporal/schema/postgresql/v12/visibility/versioned\",\"SchemaName\":\"\",\"IsDryRun\":false}, \"logging-call-at\": \"updatetask.go:104\"}\r\n2024-07-11T13:38:03.675Z\tDEBUG\tSchema Dirs: [v1.6]\t{\"logging-call-at\": \"updatetask.go:212\"}\r\n2024-07-11T13:38:03.675Z\tINFO\tProcessing schema file: v1.6/fix_root_workflow_info.sql\t{\"logging-call-at\": \"updatetask.go:256\"}\r\n2024-07-11T13:38:03.675Z\tDEBUG\trunning 1 updates for current version 1.5\t{\"logging-call-at\": \"updatetask.go:138\"}\r\n2024-07-11T13:38:03.675Z\tDEBUG\t---- Executing updates for version 1.6 ----\t{\"logging-call-at\": \"updatetask.go:157\"}\r\n2024-07-11T13:38:03.675Z\tDEBUG\tDROP INDEX by_root_workflow_id;\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.068Z\tDEBUG\tDROP INDEX by_root_run_id;\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.794Z\tDEBUG\tALTER TABLE executions_visibility DROP COLUMN root_workflow_id;\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.798Z\tDEBUG\tALTER TABLE executions_visibility DROP COLUMN root_run_id;\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.799Z\tDEBUG\tALTER TABLE executions_visibility ADD COLUMN root_workflow_id VARCHAR(255) NOT NULL DEFAULT '';\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.802Z\tDEBUG\tALTER TABLE executions_visibility ADD COLUMN root_run_id VARCHAR(255) NOT NULL DEFAULT '';\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:04.804Z\tDEBUG\tCREATE INDEX by_root_workflow_id ON executions_visibility (namespace_id, root_workflow_id, (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);\t{\"logging-call-at\": \"updatetask.go:159\"}\r\n2024-07-11T13:38:41.793Z\tERROR\tUnable to update SQL schema.\t{\"error\": \"error executing statement: driver: bad connection\", \"logging-call-at\": \"handler.go:78\"}\r\n```\r\nThe `error executing statement: driver: bad connection` error  is just an indication that query killer was engaged to terminate the query which blocks longer than the allowed limit.\r\n\r\n\r\n## Specifications\r\n\r\n  - Version: 1.24.2 (and earlier)\r\n  - Platform: N/A\r\n\r\n## Proposed Solution\r\n\r\nThe proposed solution for the issue should be creating the indices concurrently, so the `CREATE INDEX` requests are not blocking. For example `schema/postgresql/v12/visibility/versioned/v1.6/fix_root_workflow_info.sql` would have the following changes:\r\n\r\n```diff\r\n--- a/schema/postgresql/v12/visibility/versioned/v1.6/fix_root_workflow_info.sql\r\n+++ b/schema/postgresql/v12/visibility/versioned/v1.6/fix_root_workflow_info.sql\r\nALTER TABLE executions_visibility ADD COLUMN root_workflow_id VARCHAR(255) NOT NULL DEFAULT '';\r\nALTER TABLE executions_visibility ADD COLUMN root_run_id      VARCHAR(255) NOT NULL DEFAULT '';\r\n- CREATE INDEX by_root_workflow_id  ON executions_visibility (namespace_id, root_workflow_id, (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);\r\n- CREATE INDEX by_root_run_id       ON executions_visibility (namespace_id, root_run_id,      (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);\r\n+ CREATE INDEX CONCURRENTLY by_root_workflow_id  ON executions_visibility (namespace_id, root_workflow_id, (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);\r\n+ CREATE INDEX CONCURRENTLY by_root_run_id       ON executions_visibility (namespace_id, root_run_id,      (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);\r\n\r\n```\r\n","closedAt":null,"comments":[],"createdAt":"2024-07-11T15:36:54Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":6273,"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"state":"OPEN","title":"PostgreSQL schema update fails on busy Temporal instances","updatedAt":"2024-07-11T15:38:01Z","url":"https://github.com/temporalio/temporal/issues/6273"}
