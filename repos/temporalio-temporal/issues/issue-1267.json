{"assignees":[{"id":"MDQ6VXNlcjg3NjI4OTM=","login":"wxing1292","name":"Wenquan Xing","databaseId":0}],"author":{"id":"MDQ6VXNlcjM2NTYxMQ==","is_bot":false,"login":"shaunco","name":"Shaun"},"body":"## Expected Behavior\r\nIf `ExecuteWorkflow` is called with args that serialize to larger than the max history size, a proper error should be returned from ExecuteWorkflow and the workflow history should reflect that error.\r\n\r\n## Actual Behavior\r\nA workflow execution is created with no history. Subsequent attempts to retrieve the workflow via `tctl` or `temporal-web` get `Failed to get history on workflow` or `corrupted history event batch, eventID is not continuous`, and it appears that subsequent workflow executions in the same namespace (even for different workflows) get stuck behind the now corrupt workflow.\r\n\r\nWe haven't yet found a way to manually clear this execution with no history through Temporal provided tools and end up having to manually clear it from the database.\r\n\r\nSome `tctl` logs:\r\n```bash\r\n$ docker-compose run tctl --ns default wf list -op\r\nWORKFLOW TYPE |            WORKFLOW ID               |                RUN ID                |      TASK QUEUE       | START TIME | EXECUTION TIME\r\n    TS.Load   | ts-TS.Load-GTMTj7XPrsW6d8iVnohKGs6LZ | 2e30564c-8f4d-45e2-a5ed-d0af34c0a337 | TIMESERIES_TASK_QUEUE | 17:42:22   | 17:42:22\r\n\r\n$ docker-compose run tctl --ns default wf show -wid ts-TS.Load-GTMTj7XPrsW6d8iVnohKGs6LZ\r\nError: Failed to get history on workflow id: ts-TS.Load-GTMTj7XPrsW6d8iVnohKGs6LZ, run id: .\r\nError Details: context deadline exceeded\r\n('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)\r\n\r\n$ docker-compose run tctl --ns default admin wf desc -wid ts-TS.Load-GTMTj7XPrsW6d8iVnohKGs6LZ\r\nCache mutable state:\r\n{\r\n  \"executionInfo\": {\r\n    \"namespaceId\": \"67f5aeeb-0190-43d7-9ecf-1827acc18083\",\r\n    \"workflowId\": \"ts-TS.Load-GTMTj7XPrsW6d8iVnohKGs6LZ\",\r\n    \"taskQueue\": \"TIMESERIES_TASK_QUEUE\",\r\n    \"workflowTypeName\": \"TS.Load\",\r\n    \"workflowExecutionTimeout\": \"0s\",\r\n    \"workflowRunTimeout\": \"0s\",\r\n    \"defaultWorkflowTaskTimeout\": \"10s\",\r\n    \"lastEventTaskId\": \"1048613\",\r\n    \"lastFirstEventId\": \"4\",\r\n    \"startTime\": \"2021-02-07T17:42:22.593879300Z\",\r\n    \"lastUpdateTime\": \"2021-02-08T15:34:19.889Z\",\r\n    \"workflowTaskScheduleId\": \"5\",\r\n    \"workflowTaskTimeout\": \"10s\",\r\n    \"workflowTaskAttempt\": 7832,\r\n    \"workflowTaskScheduledTime\": \"2021-02-08T15:34:19.889413600Z\",\r\n    \"workflowTaskOriginalScheduledTime\": \"2021-02-08T15:34:19.889412300Z\",\r\n    \"workflowTaskRequestId\": \"emptyUuid\",\r\n    \"stickyScheduleToStartTimeout\": \"0s\",\r\n    \"attempt\": 1,\r\n    \"autoResetPoints\": {\r\n\r\n    },\r\n    \"versionHistories\": {\r\n      \"histories\": [\r\n        {\r\n          \"branchToken\": \"CiQyZTMwNTY0Yy04ZjRkLTQ1ZTItYTVlZC1kMGFmMzRjMGEzMzcSJDY1ZDlhMjlmLTc3NzQtNGFiYS1hYjZjLTdiOWFjYWNiMjM0Nw==\",\r\n          \"items\": [\r\n            {\r\n              \"eventId\": \"4\"\r\n            }\r\n          ]\r\n        }\r\n      ]\r\n    },\r\n    \"firstExecutionRunId\": \"2e30564c-8f4d-45e2-a5ed-d0af34c0a337\",\r\n    \"executionStats\": {\r\n      \"historySize\": \"53699\"\r\n    },\r\n    \"workflowRunExpirationTime\": \"0001-01-01T00:00:00Z\"\r\n  },\r\n  \"executionState\": {\r\n    \"createRequestId\": \"db985d3e-5659-4c1c-bc68-184858dcb9e7\",\r\n    \"runId\": \"2e30564c-8f4d-45e2-a5ed-d0af34c0a337\",\r\n    \"state\": \"Running\",\r\n    \"status\": \"Running\"\r\n  },\r\n  \"nextEventId\": \"5\"\r\n}\r\nDatabase mutable state:\r\n{\r\n  \"executionInfo\": {\r\n    \"namespaceId\": \"67f5aeeb-0190-43d7-9ecf-1827acc18083\",\r\n    \"workflowId\": \"ts-TS.Load-GTMTj7XPrsW6d8iVnohKGs6LZ\",\r\n    \"taskQueue\": \"TS_TASK_QUEUE\",\r\n    \"workflowTypeName\": \"TS.Load\",\r\n    \"workflowExecutionTimeout\": \"0s\",\r\n    \"workflowRunTimeout\": \"0s\",\r\n    \"defaultWorkflowTaskTimeout\": \"10s\",\r\n    \"lastEventTaskId\": \"1048613\",\r\n    \"lastFirstEventId\": \"4\",\r\n    \"startTime\": \"2021-02-07T17:42:22.593879300Z\",\r\n    \"lastUpdateTime\": \"2021-02-08T15:34:19.889Z\",\r\n    \"workflowTaskScheduleId\": \"5\",\r\n    \"workflowTaskTimeout\": \"10s\",\r\n    \"workflowTaskAttempt\": 7832,\r\n    \"workflowTaskScheduledTime\": \"2021-02-08T15:34:19.889413600Z\",\r\n    \"workflowTaskOriginalScheduledTime\": \"2021-02-08T15:34:19.889412300Z\",\r\n    \"workflowTaskRequestId\": \"emptyUuid\",\r\n    \"stickyScheduleToStartTimeout\": \"0s\",\r\n    \"attempt\": 1,\r\n    \"autoResetPoints\": {\r\n\r\n    },\r\n    \"versionHistories\": {\r\n      \"histories\": [\r\n        {\r\n          \"branchToken\": \"CiQyZTMwNTY0Yy04ZjRkLTQ1ZTItYTVlZC1kMGFmMzRjMGEzMzcSJDY1ZDlhMjlmLTc3NzQtNGFiYS1hYjZjLTdiOWFjYWNiMjM0Nw==\",\r\n          \"items\": [\r\n            {\r\n              \"eventId\": \"4\"\r\n            }\r\n          ]\r\n        }\r\n      ]\r\n    },\r\n    \"firstExecutionRunId\": \"2e30564c-8f4d-45e2-a5ed-d0af34c0a337\",\r\n    \"executionStats\": {\r\n      \"historySize\": \"53699\"\r\n    },\r\n    \"workflowRunExpirationTime\": \"0001-01-01T00:00:00Z\"\r\n  },\r\n  \"executionState\": {\r\n    \"createRequestId\": \"db985d3e-5659-4c1c-bc68-184858dcb9e7\",\r\n    \"runId\": \"2e30564c-8f4d-45e2-a5ed-d0af34c0a337\",\r\n    \"state\": \"Running\",\r\n    \"status\": \"Running\"\r\n  },\r\n  \"nextEventId\": \"5\"\r\n}\r\nCurrent branch token:\r\n{\r\n  \"tree_id\": \"2e30564c-8f4d-45e2-a5ed-d0af34c0a337\",\r\n  \"branch_id\": \"65d9a29f-7774-4aba-ab6c-7b9acacb2347\"\r\n}\r\nHistory service address: 172.22.0.3:7234\r\nShard Id: 1\r\n\r\n\r\n$ docker-compose run tctl --ns default admin wf show --db_address 127.0.0.1 --db_port 9042 --tree_id 2e30564c-8f4d-45e2-a5ed-d0af34c0a337\r\nError: ReadHistoryBranch err\r\nError Details: ReadHistoryBranch. Close operation failed. Error: invalid UUID \"\"\r\n('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)\r\n```\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n  1. Execute a workflow with args that serialize to larger than max history\r\n  1. Try to get any details about the \"Running\" execution\r\n\r\n## Specifications\r\n\r\n  - Version: 1.6.3\r\n  - Platform:\r\n","closedAt":"2021-06-11T23:13:17Z","comments":[{"id":"MDEyOklzc3VlQ29tbWVudDc3NTQzMTY3Nw==","author":{"login":"shaunco"},"authorAssociation":"NONE","body":"Digging deeper in to this, it was not actually `args` larger than max history, but a total payload larger than Scylla's `batch_size_fail_threshold` (default is 50kb):\r\n```\r\nBatchStatement - Batch of prepared statements for temporal.history_tree, temporal.history_node is of size 54566, exceeding specified FAIL threshold of 51200 by 3366.\r\n```\r\n\r\nWe can work around this for now by upping Scylla's `batch_size_fail_threshold`, but leaving this issue open as Temporal should fail gracefully and rollback/update the execution state when it gets a DB error, not end up in a broken state that requires manual DB modifications.","createdAt":"2021-02-08T20:29:51Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-775431677","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc3NTQ3MjQ5NQ==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"> when it gets a DB error\r\n\r\nis there any error returned by DB / logged by temporal?\r\nfrom the top of my head, if DB error is returned, business logic should return that error to caller.\r\n\r\na data point from MySQL: previously the schema was not set to allow more than 64K payload, and DB just silently truncate data: https://github.com/temporalio/temporal/pull/1056","createdAt":"2021-02-08T21:24:52Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-775472495","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc3NTQ3ODg1Mw==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"@shaunco btw, you can manually delete those corrupted workflow by `tctl admin workflow delete -h`","createdAt":"2021-02-08T21:35:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-775478853","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc3NTQ4Njc1Mw==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"seem that my local test setup is not working correctly, let me try using online cassandra\r\n\r\nUPDATE:\r\nseems that our test cluster setup (3 nodes) also unable to verify (probably due to cluster size == 3).\r\n\r\n<strike>\r\n@shaunco did try to repo the issue locally:\r\n1. confirm the issue is there\r\n2. seems that cass fails silently (no error logged / returned)\r\n</strike>","createdAt":"2021-02-08T21:50:27Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-775486753","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc3NTU0ODI2Ng==","author":{"login":"shaunco"},"authorAssociation":"NONE","body":"Maybe it is specific to Scylla? I get it with this `docker-compose.yml` and a 55kb argument.\r\n\r\n```yml\r\nversion: \"3.7\"\r\n\r\nservices:\r\n  # ScyllaDB\r\n  scylla:\r\n    image: scylladb/scylla:4.2.0\r\n    container_name: scylla\r\n    ports:\r\n      - 0.0.0.0:9042:9042\r\n    networks:\r\n      - scylla-test\r\n    volumes:\r\n      - scylla-data:/var/lib/scylla\r\n       \r\n  # Temporal, configured to use Scylla\r\n  temporal:\r\n    image: temporalio/auto-setup:${SERVER_TAG:-1.6.3}\r\n    container_name: temporal\r\n    ports:\r\n      - \"7233:7233\"\r\n    networks:\r\n      - scylla-test\r\n    volumes:\r\n      - ./config/dynamicconfig:/etc/temporal/config/dynamicconfig\r\n    environment:\r\n      CASSANDRA_SEEDS: \"scylla\"\r\n      CASSANDRA_PORT: \"9042\"\r\n      DYNAMIC_CONFIG_FILE_PATH: \"config/dynamicconfig/development.yaml\"\r\n    depends_on:\r\n      - scylla\r\n\r\n  # Temporal web portal - http://localhost:7230\r\n  temporal-web:\r\n    image: temporalio/web:${WEB_TAG:-1.6.1}\r\n    ports:\r\n      - \"7230:8088\"\r\n    networks:\r\n      - scylla-test\r\n    environment:\r\n      - \"TEMPORAL_GRPC_ENDPOINT=temporal:7233\"\r\n      - \"TEMPORAL_PERMIT_WRITE_API=true\"\r\n    depends_on:\r\n      - temporal\r\n\r\n  # Temporal CLI\r\n  tctl:\r\n    image: temporalio/tctl:${SERVER_TAG:-1.6.3}\r\n    networks:\r\n      - scylla-test\r\n    environment:\r\n      - \"TEMPORAL_CLI_ADDRESS=temporal:7233\"\r\n    depends_on:\r\n      - temporal      \r\n\r\nnetworks:\r\n  scylla-test:\r\n\r\nvolumes:\r\n  scylla-data:\r\n```","createdAt":"2021-02-09T00:03:59Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-775548266","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc3NTU1NDI5MQ==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"> Maybe it is specific to Scylla? I get it with this `docker-compose.yml` and a 55kb argument.\r\n> \r\n> ```yaml\r\n> version: \"3.7\"\r\n> \r\n> services:\r\n>   # ScyllaDB\r\n>   scylla:\r\n>     image: scylladb/scylla:4.2.0\r\n>     container_name: scylla\r\n>     ports:\r\n>       - 0.0.0.0:9042:9042\r\n>     networks:\r\n>       - scylla-test\r\n>     volumes:\r\n>       - scylla-data:/var/lib/scylla\r\n>        \r\n>   # Temporal, configured to use Scylla\r\n>   temporal:\r\n>     image: temporalio/auto-setup:${SERVER_TAG:-1.6.3}\r\n>     container_name: temporal\r\n>     ports:\r\n>       - \"7233:7233\"\r\n>     networks:\r\n>       - scylla-test\r\n>     volumes:\r\n>       - ./config/dynamicconfig:/etc/temporal/config/dynamicconfig\r\n>     environment:\r\n>       CASSANDRA_SEEDS: \"scylla\"\r\n>       CASSANDRA_PORT: \"9042\"\r\n>       DYNAMIC_CONFIG_FILE_PATH: \"config/dynamicconfig/development.yaml\"\r\n>     depends_on:\r\n>       - scylla\r\n> \r\n>   # Temporal web portal - http://localhost:7230\r\n>   temporal-web:\r\n>     image: temporalio/web:${WEB_TAG:-1.6.1}\r\n>     ports:\r\n>       - \"7230:8088\"\r\n>     networks:\r\n>       - scylla-test\r\n>     environment:\r\n>       - \"TEMPORAL_GRPC_ENDPOINT=temporal:7233\"\r\n>       - \"TEMPORAL_PERMIT_WRITE_API=true\"\r\n>     depends_on:\r\n>       - temporal\r\n> \r\n>   # Temporal CLI\r\n>   tctl:\r\n>     image: temporalio/tctl:${SERVER_TAG:-1.6.3}\r\n>     networks:\r\n>       - scylla-test\r\n>     environment:\r\n>       - \"TEMPORAL_CLI_ADDRESS=temporal:7233\"\r\n>     depends_on:\r\n>       - temporal      \r\n> \r\n> networks:\r\n>   scylla-test:\r\n> \r\n> volumes:\r\n>   scylla-data:\r\n> ```\r\n\r\nlet me take a try","createdAt":"2021-02-09T00:16:28Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-775554291","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc3NTU3NDYzNA==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"when i configure the test workflow to start a child workflow with 1MB payload, this is what is see (using the above docker compose)\r\n```\r\n\"AppendHistoryNodes operation failed. Error: Batch too large\"\r\n```\r\n\r\nscylla is reporting \r\n```\r\nscylla          | ERROR 2021-02-09 00:51:59,929 [shard 3] BatchStatement - Batch of prepared statements for temporal.history_tree, temporal.history_node is of size 1051978, exceeding specified FAIL threshold of 51200 by 1000778.\r\n```\r\n\r\n@shaunco can we sync on slack? i may need to have little bit more information (`Failed to get history on workflow or corrupted history event batch, eventID is not continuous`)\r\nOne more thing, we currently only officially support cassandra 3.11 / mysql 5.7 / postgresql 9.6\r\n","createdAt":"2021-02-09T01:03:31Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-775574634","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc3NTU4NjA4OA==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"tried my local cassandra setup as well as default cassandra docker (both 3.11), seems that either `batch_size_fail_threshold` carries a different meaning (cassandra vs scylla) or simply ignored\r\n\r\nlet us talk on slack.","createdAt":"2021-02-09T01:30:59Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-775586088","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDc3NjMxNzY3NQ==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"update: seems that cassandra / scylla are behaving differently (need more confirmation from both sides) .\r\n\r\nin cassandra the batch query in question is targeting at one partition (seem to be a guaranteed bahavior)\r\nin scylla the batch query in question is target at multiple partition (2)\r\n\r\n^ means `batch_size_fail_threshold` is not being evaluated in cassandra in this case","createdAt":"2021-02-09T23:35:35Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-776317675","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDg1OTk1MjA3MQ==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"since right now scylla DB is not officially supported, i will close this ticket.\r\n\r\nplz reopen this ticket if you have any comments / concern","createdAt":"2021-06-11T23:13:17Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-859952071","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5re4wA","author":{"login":"jeacott1"},"authorAssociation":"NONE","body":"Any chance of adding a feature like conductor has with a transparent payload storage?\r\nhttps://conductor.netflix.com/devguide/architecture/technicaldetails.html#external-payload-storage","createdAt":"2023-11-09T06:55:06Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1267#issuecomment-1803258880","viewerDidAuthor":false}],"createdAt":"2021-02-08T16:19:47Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":1267,"reactionGroups":[],"state":"CLOSED","title":"Corrupted history event when workflow args are too large","updatedAt":"2023-11-09T06:55:06Z","url":"https://github.com/temporalio/temporal/issues/1267"}
