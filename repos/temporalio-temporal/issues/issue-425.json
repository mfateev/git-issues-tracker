{"assignees":[{"id":"MDQ6VXNlcjg3NjI4OTM=","login":"wxing1292","name":"Wenquan Xing","databaseId":0}],"author":{"id":"MDQ6VXNlcjU0NDk5MQ==","is_bot":false,"login":"kelnos","name":"Brian Tarricone"},"body":"## Expected Behavior\r\n\r\nAfter adding a second node to a newly-bootstrapped one-node temporal cluster, logs should be generally quiet.\r\n\r\n## Actual Behavior\r\n\r\nInstead, after adding the second node, I see \"New timer generated is less than read level\" repeated once per second in the logs of the original node (logs of second node are quiet).  Full example log line:\r\n```\r\n{\"level\":\"warn\",\"ts\":\"2020-06-01T18:08:52.273-0700\",\"msg\":\"New timer generated is less than read level\",\"service\":\"history\",\"shard-id\":111,\"address\":\"[redacted]:7234\",\"wf-namespace-id\":\"892c1d1f-e202-49dc-bc77-97268a84c9ef\",\"wf-id\":\"eae6d2bc-89fe-4ac5-a544-a8087e3deede\",\"timestamp\":\"2020-06-01T17:14:41.689-0700\",\"cursor-timestamp\":\"2020-06-01T18:08:53.265-0700\",\"shard-update\":\"ShardAllocateTimerBeforeRead\",\"logging-call-at\":\"shardContext.go:1130\"}\r\n```\r\nNote that the IP address in the `address` field refers to the same node that is generating the log.\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n  1. Set up a fresh Cassandra keyspace and ES index.\r\n  1. Boot a single Temporal node and create a test namespace.\r\n  1. Start a workflow/activity worker and start a workflow execution to test that it works.\r\n  1. Boot a second Temporal node and wait for it to finish joining the cluster.\r\n  1. Repeated log message should start.\r\n\r\nI am not certain that step 3 is required, but this is what I did, so I'm detailing it here.\r\n\r\n## Specifications\r\n\r\n  - Version: 0.23.1\r\n  - Platform: Amazon Linux 2018.03 + Docker 18.09.9\r\n\r\n## More Information\r\n\r\nAfter running `tctl admin cluster describe` against both Temporal nodes, I can see that they both have the same view of the cluster, and that they both see all four services on both nodes.\r\n","closedAt":"2020-11-06T18:32:55Z","comments":[{"id":"MDEyOklzc3VlQ29tbWVudDYzNzIzNzYwNQ==","author":{"login":"samarabbas"},"authorAssociation":"MEMBER","body":"@kelnos this log usually is emitted when a certain protection kicks in which guarantees we don't generate timers less than read level resulting us skipping certain timers.  This does not indicate a problem with the system, but having this log being emitted so frequently is definitely we will look it.  ","createdAt":"2020-06-02T02:49:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-637237605","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDYzNzI2MzQyNA==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Thanks!  Just to follow up, it's been ~4 hours and it's still logging that, still once per second.  So it does appear that something is \"stuck\".","createdAt":"2020-06-02T04:24:51Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-637263424","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDY0MDkwMTAzNQ==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Samar & I had a zoom call, and the feedback is that I was setting a workflow execution timeout for a cron schedule workflow, which I shouldn't be doing.  What I was doing was this:\r\n```java\r\nfinal DemoCronWorkflow demoCronWorkflow = workflowClient.newWorkflowStub(\r\n        DemoCronWorkflow.class,\r\n        WorkflowOptions.newBuilder()\r\n                .setWorkflowIdReusePolicy(WorkflowIdReusePolicy.AllowDuplicate)\r\n                .setWorkflowExecutionTimeout(Duration.ofSeconds(10))\r\n                .setWorkflowTaskTimeout(Duration.ofSeconds(20))\r\n                .setTaskList(DemoCronWorkflow.TASK_LIST_NAME)\r\n                .setWorkflowId(WORKFLOW_ID)\r\n                .build()\r\n);\r\nWorkflowClient.start(demoCronWorkflow::logEveryMinute);\r\n```\r\nThe workflow itself is just a simple one-liner that writes to the log when it runs, which it's scheduled to do every minute (`@CronSchedule(\"* * * * *\")`).\r\n\r\nSample history for a single run (copy/paste destroyed formatting, so here's a screenshot):\r\n![history](https://user-images.githubusercontent.com/544991/84082694-24bfeb00-a995-11ea-8177-1f1316a7e0aa.png)\r\n\r\nI'm going to remove the execution timeout, cancel execution of the current workflow, and start it anew to see if that fixes things, and will report back.\r\n\r\n","createdAt":"2020-06-08T21:37:14Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-640901035","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDY0MDkwNzE5OA==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Good news: not setting the workflow execution timeout worked.  The repeated log message in the temporal log is gone, and the cron workflow is working correctly and running once per minute.","createdAt":"2020-06-08T21:52:56Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-640907198","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcwODAxMzY5MA==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"We've been running into this again (with Temporal 1.0.0), and it's with a regularly-started workflow, not a cron workflow.  The service in question is just doing:\r\n```java\r\nfinal Workflow workflow = workflowClient.newWorkflowStub(\r\n            Workflow.class,\r\n            WorkflowOptions.newBuilder()\r\n                .setWorkflowId(event.getId())\r\n                .setWorkflowIdReusePolicy(WorkflowIdReusePolicy.WORKFLOW_ID_REUSE_POLICY_REJECT_DUPLICATE)\r\n                .setWorkflowExecutionTimeout(Duration.ofMinutes(10))\r\n                .setWorkflowTaskTimeout(Duration.ofMinutes(1))\r\n                .setRetryOptions(RetryOptions.newBuilder()\r\n                    .setInitialInterval(Duration.ofSeconds(1))\r\n                    .setBackoffCoefficient(2)\r\n                    .setMaximumInterval(Duration.ofSeconds(20))\r\n                    .setMaximumAttempts(3)\r\n                    .build()\r\n                )\r\n                .setTaskQueue(RecyclingEventWorkflow.TASK_QUEUE_NAME)\r\n                .build()\r\n        );\r\nWorkflowClient.start(workflow::processEvent, event);\r\n```\r\n\r\nWe're also seeing an issue where (sometimes, but not always, after a flood of these log messages), the temporal-server process pegs a CPU at 100% and stops responding to requests.  Not sure if that's related to this, but either way it's very concerning.","createdAt":"2020-10-13T21:16:19Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-708013690","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcwODA4NTkzMA==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Some more info: the workflow runs appear to complete successfully, but I see this warning in the log quite often (upwards of 15 times per minute).  At most there seem to be one or two workflow runs per minute, but usually only one, and the run takes only a few seconds at most.  Also just noticed that I see it with other workflows, not just the one above.","createdAt":"2020-10-14T00:42:51Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-708085930","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcwODEyMDA5Mg==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Just updated to the new 1.1.0 release and I'm still seeing the warning.","createdAt":"2020-10-14T02:43:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-708120092","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcwODE1NjU5NQ==","author":{"login":"samarabbas"},"authorAssociation":"MEMBER","body":"@kelnos this log does not represent an issue with the system.  This usually happens if a timer is created with a very short interval and while the transaction is outstanding readlevel for timer queue moves beyond the visibility time for the new timer.  \r\n@wxing1292 can you convert this to a metric instead as this log is continuously causing confusion to users?  At the minimum we should change the log level to info instead of warning.\r\n\r\n@kelnos your 100% cpu is more concerning.  Can you provide more information about the setup?\r\n1) Cluster size?\r\n2) Are you deploying each role as separate instances?\r\n3) Which role goes into 100% cpu\r\n4) what kind of workload you are running through the cluster. ","createdAt":"2020-10-14T04:57:28Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-708156595","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcwODUzMjcxMg==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"@kelnos can you plz take a CPU & mem profile?\r\n\r\nCPU:\r\n```zsh\r\ngo tool pprof -pdf http://localhost:7936/debug/pprof/profile\\?seconds\\=30\r\n```\r\n\r\nMem:\r\n```zsh\r\ngo tool pprof -pdf http://localhost:7936/debug/pprof/heap\r\n```","createdAt":"2020-10-14T16:59:22Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-708532712","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcwODYyNTIyMw==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"@samarabbas thanks for the clarification!\r\n\r\n>     1. Cluster size?\r\n\r\nTwo nodes\r\n\r\n>     2. Are you deploying each role as separate instances?\r\n\r\nNo, all roles on the same instance.\r\n\r\n>     3. Which role goes into 100% cpu\r\n\r\nUnknown.\r\n\r\n>     4. what kind of workload you are running through the cluster.\r\n\r\nVery light workload, currently single-digit concurrent workflow invocations.  All invocations are relatively short lived and either complete within a few seconds or few tens of seconds.\r\n\r\n@wxing1292 I assume the CPU/mem profiles will only be useful to capture while the system is in the bad state?  It seems to be happening once per day or two, so I should be able to get that for you soon.","createdAt":"2020-10-14T19:53:26Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-708625223","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcwODc5NjE2OQ==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"> No, all roles on the same instance.\r\n\r\nI would really recommend putting different services (frontend / matching / history) on different nodes / hosts if you are running production traffic.","createdAt":"2020-10-15T00:56:30Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-708796169","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMTg1OTQ1OA==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Unfortunately when the server gets into this state, the pprof endpoint does not respond.  I get something similar to:\r\n```\r\nFetching profile over HTTP from http://localhost:7936/debug/pprof/heap\r\nhttp://localhost:7936/debug/pprof/heap: Get http://localhost:7936/debug/pprof/heap: net/http: timeout awaiting response headers\r\nfailed to fetch any source profiles\r\n```\r\nVerified that pprof is indeed working when the server is working normally.\r\n\r\nI did manage to get a goroutine dump by sending a `SIGQUIT`: [lockup.log](https://github.com/temporalio/temporal/files/5400336/lockup.log)\r\n","createdAt":"2020-10-19T08:41:38Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-711859458","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzc4NDY1Mg==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"Any thoughts on how to proceed here?  This is a huge blocker for us, as I'm seeing node failures in our development environment nearly daily (with all 1.0.0+ releases).  We're still running 0.30.0 in prod with no failures.","createdAt":"2020-10-21T18:35:59Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-713784652","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzgyODA2Ng==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"@kelnos \r\nwhat DB are you using?\r\nany configuration that you can share?\r\n\r\none important thing, can you plz try deploy the service as one service per host, rather than all in one?","createdAt":"2020-10-21T19:35:35Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-713828066","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzgyOTAxOQ==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"btw i will be on slack: https://temporalio.slack.com/join/shared_invite/zt-c1e99p8g-beF7~ZZW2HP6gGStXD8Nuw#/","createdAt":"2020-10-21T19:37:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-713829019","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcxMzg0NzAwNg==","author":{"login":"kelnos"},"authorAssociation":"CONTRIBUTOR","body":"@wxing1292 I'm using Cassandra, and our configuration is using `docker/config_template.yaml` with some minor changes.  Rendered, it looks like this on one of my nodes:\r\n```yaml\r\nlog:\r\n    stdout: true\r\n    level: info\r\n\r\npersistence:\r\n    numHistoryShards: 512\r\n    defaultStore: default\r\n    visibilityStore: visibility\r\n    advancedVisibilityStore: es-visibility\r\n    datastores:\r\n        default:\r\n            cassandra:\r\n                hosts: 127.0.0.1:10002\r\n                keyspace: temporal\r\n        visibility:\r\n            cassandra:\r\n                hosts: 127.0.0.1:10002\r\n                keyspace: temporal_visibility\r\n        es-visibility:\r\n            elasticsearch:\r\n                url:\r\n                    scheme: \"http\"\r\n                    host: \"localhost:8422\"\r\n                indices:\r\n                    visibility: temporal-visibility\r\n\r\nglobal:\r\n    membership:\r\n        maxJoinDuration: 240s\r\n        broadcastAddress: \r\n    tls:\r\n        internode:\r\n            server:\r\n                certFile: \r\n                keyFile: \r\n                requireClientAuth: false\r\n                clientCaFiles:\r\n                    - \r\n            client:\r\n                rootCaFiles:\r\n                    - \r\n        frontend:\r\n            server:\r\n                certFile: \r\n                keyFile: \r\n                requireClientAuth: false\r\n                clientCaFiles:\r\n                    - \r\n                    - \r\n            client:\r\n                rootCaFiles:\r\n                    - \r\n\r\nservices:\r\n    frontend:\r\n        rpc:\r\n            grpcPort: 7233\r\n            membershipPort: 6933\r\n            bindOnIP: 172.22.199.254\r\n        metrics:\r\n            statsd:\r\n                hostPort: localhost:8127\r\n                prefix: ops.temporal.frontend\r\n\r\n    matching:\r\n        rpc:\r\n            grpcPort: 7235\r\n            membershipPort: 6935\r\n            bindOnIP: 172.22.199.254\r\n        metrics:\r\n            statsd:\r\n                hostPort: localhost:8127\r\n                prefix: ops.temporal.matching\r\n\r\n    history:\r\n        rpc:\r\n            grpcPort: 7234\r\n            membershipPort: 6934\r\n            bindOnIP: 172.22.199.254\r\n        metrics:\r\n            statsd:\r\n                hostPort: localhost:8127\r\n                prefix: ops.temporal.history\r\n\r\n    worker:\r\n        rpc:\r\n            grpcPort: 7239\r\n            membershipPort: 6939\r\n            bindOnIP: 172.22.199.254\r\n        metrics:\r\n            statsd:\r\n                hostPort: localhost:8127\r\n                prefix: ops.temporal.worker\r\n\r\nclusterMetadata:\r\n    enableGlobalNamespace: false\r\n    failoverVersionIncrement: 10\r\n    masterClusterName: \"active\"\r\n    currentClusterName: \"active\"\r\n    clusterInformation:\r\n        active:\r\n            enabled: true\r\n            initialFailoverVersion: 1\r\n            rpcName: \"frontend\"\r\n            rpcAddress: \"127.0.0.1:7233\"\r\n\r\ndcRedirectionPolicy:\r\n    policy: \"noop\"\r\n    toDC: \"\"\r\n\r\narchival:\r\n  history:\r\n    state: \"enabled\"\r\n    enableRead: true\r\n    provider:\r\n      filestore:\r\n        fileMode: \"0666\"\r\n        dirMode: \"0766\"\r\n  visibility:\r\n    state: \"enabled\"\r\n    enableRead: true\r\n    provider:\r\n      filestore:\r\n        fileMode: \"0666\"\r\n        dirMode: \"0766\"\r\n\r\nnamespaceDefaults:\r\n  archival:\r\n    history:\r\n      state: \"disabled\"\r\n      URI: \"file:///tmp/temporal_archival/development\"\r\n    visibility:\r\n      state: \"disabled\"\r\n      URI: \"file:///tmp/temporal_vis_archival/development\"\r\n\r\nkafka:\r\n    tls:\r\n        enabled: false\r\n    clusters:\r\n        test:\r\n            brokers:\r\n                - 127.0.0.1:10003\r\n    topics:\r\n        Architecture.TemporalVisibility:\r\n            cluster: test\r\n        Architecture.TemporalVisibilityDlq:\r\n            cluster: test\r\n    applications:\r\n        visibility:\r\n            topic: Architecture.TemporalVisibility\r\n            dlq-topic: Architecture.TemporalVisibilityDlq\r\n\r\npublicClient:\r\n    hostPort: 172.22.199.254:7233\r\n\r\ndynamicConfigClient:\r\n    filepath: /etc/temporal/config/dynamicconfig/twilio.yaml\r\n    pollInterval: \"60s\"\r\n```\r\nThe dynamic config looks like:\r\n```yaml\r\nhistory.EnableConsistentQueryByNamespace:\r\n  - value: true\r\n```\r\n\r\nJust a note that Cassandra and ES are not running on the local node; the \"localhost\" there is just the entry point into our service mesh.\r\n\r\nI can try doing one service per node, but that might take me a little bit of time to set up.\r\n\r\nI'm also on your slack, as `@btarricone`.  If you think it'll be easier to continue discussion there, I'm happy to chat there instead.","createdAt":"2020-10-21T20:13:41Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-713847006","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcyMzIzNDYyMw==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"Issue got resolved once services are deployed into dedicated docker / pods.\r\n","createdAt":"2020-11-06T18:32:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-723234623","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDcyMzIzNjUzNQ==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"the issue maybe related to #915 (this PR make sure only start one metrics client per deployment)","createdAt":"2020-11-06T18:37:21Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/425#issuecomment-723236535","viewerDidAuthor":false}],"createdAt":"2020-06-02T01:17:52Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":425,"reactionGroups":[],"state":"CLOSED","title":"Repeated \"New timer generated is less than read level\" in logs after adding node to cluster","updatedAt":"2020-11-06T18:37:21Z","url":"https://github.com/temporalio/temporal/issues/425"}
