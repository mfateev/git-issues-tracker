{"assignees":[],"author":{"id":"MDQ6VXNlcjEwMDA1Mjk=","is_bot":false,"login":"justinrixx","name":"Justin Ricks"},"body":"**Is your feature request related to a problem? Please describe.**\n\nMy (self-hosted) cluster's persistence DB gets overwhelmed (100% cpu, high latency for ~20 mins at a time) on a cron schedule like clockwork, every 12 hours. I believe this is due to the scavenger jobs to clean up the DB. Unfortunately the only configs I have to tune this seems to be the various `persistenceMaxQPS` settings, which is too blunt of a tool.\n\nThe various scavengers also have inconsistent dynamic configuration available: [the executions scanner has a per-shard qps setting](https://github.com/temporalio/temporal/blob/v1.27.2/service/worker/scanner/workflow.go#L194-L195), but [the history scanner has no such configuration](https://github.com/temporalio/temporal/blob/v1.27.2/service/worker/scanner/history/scavenger.go#L106); its `rps` [comes from persistenceMaxQPS](https://github.com/temporalio/temporal/blob/v1.27.2/service/worker/scanner/workflow.go#L137)\n\n**Describe the solution you'd like**\n\nI'd like to add a new dynamic configuration field for `historyScannerRPS` to tune this job. This will allow me to set a (low) limit for these heavy requests that won't overwhelm the underlying persistence DB, while keeping a higher limit for other persistence requests (so that the cluster's throughput isn't artificially limited just for these cleanup windows).\n\nAnother note: it may be useful to make the cron schedule configurable as well, defaulting to its current value. I don't _need_ the ability to change this as the current cleanups happen mostly outside regular hours, but if i were to deploy to a different region where this cleanup schedule coincided with peak traffic, I'd have no options for mitigation.\n\n**Describe alternatives you've considered**\n\nThe main alternative I've considered is lowering the `persistenceMaxQPS` setting. I've been careful about lowering this though as I don't want the cluster's throughput to suffer outside of the cleanup time. I've had to take this setting pretty low to see any difference in the load pattern on my DB.\n\n**Additional context**\n\nI've never made a contribution to this project, but would be willing to submit a PR if the project is inclined to accept one. I'm also open to alternative ideas; maybe there's something I'm overlooking and I could solve this in some other way.","closedAt":null,"comments":[{"id":"IC_kwDODNqesM6poX5b","author":{"login":"bergundy"},"authorAssociation":"MEMBER","body":"@justinrixx we do not plan on working on this in the immediate future but contributions are welcome.","createdAt":"2025-05-01T22:44:23Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/7625#issuecomment-2845933147","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6s-ZBo","author":{"login":"justinrixx"},"authorAssociation":"NONE","body":"@bergundy thanks\n\ni'm waiting on some internal stuff at my company to sign off on the CLA so haven't gotten around to this yet. it looks like somebody else opened a PR as well. if that falls through by the time i get CLA approval i'll take a crack at this.","createdAt":"2025-05-22T17:31:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/7625#issuecomment-2902036584","viewerDidAuthor":false}],"createdAt":"2025-04-17T17:37:40Z","labels":[{"id":"MDU6TGFiZWwxNjIxMDMwNzg3","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwyNTc2NTM1ODQ2","name":"difficulty: easy","description":"","color":"68d662"},{"id":"MDU6TGFiZWwzMTM4ODI4MTE3","name":"up-for-grabs","description":"Issues to consider for external contribution","color":"69CD90"}],"milestone":null,"number":7625,"reactionGroups":[],"state":"OPEN","title":"Additional tuning configs for scavenger jobs","updatedAt":"2025-05-22T17:31:41Z","url":"https://github.com/temporalio/temporal/issues/7625"}
