{"assignees":[],"author":{"id":"MDQ6VXNlcjExMjAzMjg=","is_bot":false,"login":"smcgivern","name":"Sean McGivern"},"body":"We run Temporal 1.20.3 on Kubernetes. It has very light load. We have noticed that over time, the worker process consumes more and more CPU time and memory, eventually getting 'stuck' and requiring a restart. Here's a chart from GCP showing CPU and memory over a one-week period in July (blue line is usage, green is requests, red is limits). You can see that we restarted this manually at the end.\r\n\r\n<img width=\"796\" alt=\"image\" src=\"https://github.com/temporalio/temporal/assets/1120328/4dd3149f-0d36-477f-97e6-d4af2a1f8944\">\r\n\r\nAdditionally, a full metrics scrape against one of these pods (using `kubectl port-forward`) takes 7 seconds, even in the idle state:\r\n\r\n```shell\r\n$ time curl -I localhost:9090/metrics\r\nHTTP/1.1 200 OK\r\nContent-Type: text/plain; version=0.0.4; charset=utf-8\r\nDate: Wed, 02 Aug 2023 10:30:32 GMT\r\n\r\n\r\nreal\t0m7.580s\r\nuser\t0m0.006s\r\nsys\t0m0.011s\r\n```\r\n\r\n## Profiles\r\n\r\nThese profiles were taken when CPU usage and memory usage are very high. \r\n\r\n* [pprof.temporal-server.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz](https://github.com/temporalio/temporal/files/12239105/pprof.temporal-server.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz)\r\n* [pprof.temporal-server.samples.cpu.001.pb.gz](https://github.com/temporalio/temporal/files/12239107/pprof.temporal-server.samples.cpu.001.pb.gz)\r\n\r\nThe memory profile shows - make sure to change to `alloc_space` sampling to include all allocations, not just live memory - that the majority of the allocations were in `metrics.tagsToAttributes`:\r\n\r\n<img width=\"1348\" alt=\"image\" src=\"https://github.com/temporalio/temporal/assets/1120328/9635ad82-0445-4a0a-ab40-5b8d1bc12651\">\r\n\r\nThat also shows in the CPU profile, but not as dramatically. We can see a lot of GC time:\r\n\r\n<img width=\"1348\" alt=\"image\" src=\"https://github.com/temporalio/temporal/assets/1120328/be44b603-cc98-4a2b-9712-e15aa306af93\">\r\n\r\n## Config\r\n\r\nThis is our metrics config:\r\n\r\n```yaml\r\nmetrics:\r\n  tags:\r\n    type: \"worker\"\r\n  prometheus:\r\n    framework: \"opentelemetry\"\r\n    timerType: \"histogram\"\r\n    listenAddress: \"0.0.0.0:9090\"\r\n```\r\n\r\nAnd we give it these resources:\r\n\r\n```yaml\r\nresources:\r\n  limits:\r\n    cpu: 800m\r\n    memory: 2000Mi\r\n    ephemeral-storage: \"1Gi\"\r\n  requests:\r\n    cpu: 500m\r\n    memory: 500Mi\r\n    ephemeral-storage: \"500Mi\"\r\n```\r\n\r\n## Mitigation\r\n\r\nWe changed our scrape interval from 15 seconds to 60 seconds and it now takes much longer to get in this state, as expected. It still ends up there eventually, though.\r\n\r\n## Side note\r\n\r\nIt seems like the dynamic metric tags come from two places:\r\n\r\n1. Config - this doesn't change during runtime.\r\n2. Call sites - this doesn't change after compilation.\r\n\r\nSo recalculating this every time does seem inefficient, but I'm not sure why it's not a problem for other people and it is for us.\r\n\r\n## Related issues and PRs\r\n\r\n* https://github.com/temporalio/temporal/pull/3578 looks like it improved this but it was closed. I'm not clear on why - @wxing1292, would you mind elaborating?\r\n* https://github.com/temporalio/temporal/issues/3015 is about `tagsToMap` which is for Tally metrics and seems to do a similar thing","closedAt":"2023-09-15T12:45:34Z","comments":[{"id":"IC_kwDODNqesM5jEJux","author":{"login":"smcgivern"},"authorAssociation":"NONE","body":"I was able to reproduce some very slow responses with these changes to https://github.com/tsurdilo/my-temporal-dockercompose:\r\n\r\n```diff\r\ndiff --git a/compose-postgres.yml b/compose-postgres.yml\r\nindex 400491a..5b65488 100644\r\n--- a/compose-postgres.yml\r\n+++ b/compose-postgres.yml\r\n@@ -7,7 +7,7 @@ services:\r\n       POSTGRES_USER: temporal\r\n     image: postgres:13\r\n     ports:\r\n-      - 5432:5432\r\n+      - 5444:5432\r\n   postgres-exporter:\r\n     container_name: postgres-exporter\r\n     image: wrouesnel/postgres_exporter:v0.8.0\r\ndiff --git a/compose-services.yml b/compose-services.yml\r\nindex 44cdbc9..6539585 100644\r\n--- a/compose-services.yml\r\n+++ b/compose-services.yml\r\n@@ -219,6 +219,7 @@ services:\r\n       - DYNAMIC_CONFIG_FILE_PATH=config/dynamicconfig/development.yaml\r\n       - SERVICES=worker\r\n       - PROMETHEUS_ENDPOINT=0.0.0.0:8003\r\n+      - PPROF_PORT=8989\r\n       # set to loadbalancing\r\n       - USE_INTERNAL_FRONTEND=true\r\n     image: temporalio/server:${TEMPORAL_SERVER_IMG}\r\n@@ -227,6 +228,7 @@ services:\r\n         target: 7232\r\n       - published: 8003\r\n         target: 8003\r\n+      - 8989:8989\r\n     restart: on-failure\r\n     volumes:\r\n       - ./dynamicconfig:/etc/temporal/config/dynamicconfig\r\ndiff --git a/deployment/prometheus/config.yml b/deployment/prometheus/config.yml\r\nindex bcbd173..47acae1 100644\r\n--- a/deployment/prometheus/config.yml\r\n+++ b/deployment/prometheus/config.yml\r\n@@ -4,6 +4,7 @@ scrape_configs:\r\n   - job_name: 'temporalmetrics'\r\n     metrics_path: /metrics\r\n     scheme: http\r\n+    scrape_interval: 1s\r\n     static_configs:\r\n       # Server metrics target\r\n       - targets:\r\ndiff --git a/template/my_config_template.yaml b/template/my_config_template.yaml\r\nindex 399bb96..18c6b72 100644\r\n--- a/template/my_config_template.yaml\r\n+++ b/template/my_config_template.yaml\r\n@@ -279,6 +279,8 @@ global:\r\n       prefix: \"temporal\"\r\n   {{- else if .Env.PROMETHEUS_ENDPOINT }}\r\n   metrics:\r\n+    tags:\r\n+      some_label: value\r\n     prometheus:\r\n       timerType: {{ default .Env.PROMETHEUS_TIMER_TYPE \"histogram\" }}\r\n       listenAddress: \"{{ .Env.PROMETHEUS_ENDPOINT }}\"\r\n```\r\n\r\nFor instance, not long after starting:\r\n\r\n```shell\r\n$ time curl -I localhost:8003/metrics\r\nHTTP/1.1 200 OK\r\nContent-Type: text/plain; version=0.0.4; charset=utf-8\r\nDate: Wed, 02 Aug 2023 10:41:18 GMT\r\n\r\n\r\nreal\t0m0.022s\r\nuser\t0m0.005s\r\nsys\t0m0.009s\r\n```\r\n\r\nAnd then later:\r\n\r\n```shell\r\n$ time curl -I localhost:8003/metrics\r\nHTTP/1.1 200 OK\r\nContent-Type: text/plain; version=0.0.4; charset=utf-8\r\nDate: Wed, 02 Aug 2023 11:22:35 GMT\r\n\r\n\r\nreal\t0m58.154s\r\nuser\t0m0.003s\r\nsys\t0m0.010s","createdAt":"2023-08-02T11:24:59Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/4716#issuecomment-1662032817","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5j9Jnp","author":{"login":"smcgivern"},"authorAssociation":"NONE","body":"@dnr looks like https://github.com/temporalio/temporal/pull/4722 was merged :tada:\r\n\r\nDoes that close this issue?","createdAt":"2023-08-14T09:14:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/4716#issuecomment-1676974569","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5kBIy9","author":{"login":"dnr"},"authorAssociation":"MEMBER","body":"> Does that close this issue?\r\n\r\nI hope so! We can keep it open until it's confirmed though. That PR will be released with upcoming server version 1.22","createdAt":"2023-08-14T20:33:45Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/4716#issuecomment-1678019773","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5mIBpm","author":{"login":"smcgivern"},"authorAssociation":"NONE","body":"We're trying out 1.22 (as of Friday) and it looks very promising! I'll report back in on Friday - that was long enough for us to see issues with the previous versions.","createdAt":"2023-09-11T08:08:43Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/4716#issuecomment-1713379942","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5ml79O","author":{"login":"smcgivern"},"authorAssociation":"NONE","body":"This is what memory usage looked like for the past two weeks in our deployment. I think it's very clear where we switched to 1.22 - thanks for your help here, @dnr!\r\n\r\n<img width=\"1478\" alt=\"image\" src=\"https://github.com/temporalio/temporal/assets/1120328/4a5807dc-2951-485b-beae-336aa8d4d426\">\r\n","createdAt":"2023-09-15T12:45:34Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/4716#issuecomment-1721220942","viewerDidAuthor":false}],"createdAt":"2023-08-02T10:37:41Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":4716,"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"state":"CLOSED","title":"Worker processes end up spending all their time in GC due to metric tag allocations","updatedAt":"2023-09-15T12:45:35Z","url":"https://github.com/temporalio/temporal/issues/4716"}
