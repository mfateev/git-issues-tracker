{"assignees":[{"id":"MDQ6VXNlcjE1OTg1Mg==","login":"stephanos","name":"Stephan Behnke","databaseId":0},{"id":"MDQ6VXNlcjQ2NjcyMw==","login":"dnr","name":"David Reiss","databaseId":0},{"id":"MDQ6VXNlcjE5MjM4MjI=","login":"bchav","name":"Brandon Chavis","databaseId":0},{"id":"MDQ6VXNlcjMwOTM1NDU=","login":"bechols","name":"Ben Echols","databaseId":0}],"author":{"id":"MDQ6VXNlcjY5MDg1Ng==","is_bot":false,"login":"oskwazir","name":"Omer Wazir"},"body":"**Is your feature request related to a problem? Please describe.**\r\nCurrently there is no way to assign priority to a task and also ensure fairness in workers executing tasks. \r\n\r\nQuote from [sjansen ](https://community.temporal.io/t/rate-limiting-based-on-metadata/385/4) on community.temporal.io\r\n>Imagine, for example, a parent workflow that starts an expensive child workflow for each user in the account. If multiple accounts start the parent workflow near the same time, an account with a much larger number of users could monopolize all available workers by simply being first to queue up a large number of activities associated with each user. When there’s no contention, it’s desirable for an account to be able to use 100% of available workers but as soon as there’s contention between accounts it’s desirable to attempt fair scheduling in order to keep latency proportional to account or workflow size.\r\n\r\n> Obviously it would be possible to partition capacity by creating separate queues for each account, but the result is either potentially significant idle capacity or latency waiting for workers to scale out.\r\n\r\n**Describe the solution you'd like**\r\nBe able to assign priority to a task, which ensures that the task queue is always ordered based on the highest priority. Fair scheduling of tasks is important too, so that high priority work doesn't consume all workers leaving lower priority work queued. \r\n\r\n**Describe alternatives you've considered**\r\n Multiple task queues which don't necessarily solve this problem because it allows for idle workers or latency in scaling up workers.\r\n\r\n**Additional context**\r\nThis feature request stems from a question & discussion on the community website: \r\nhttps://community.temporal.io/t/rate-limiting-based-on-metadata/385/\r\n","closedAt":null,"comments":[{"id":"MDEyOklzc3VlQ29tbWVudDg0MDc3ODg3OA==","author":{"login":"wxing1292"},"authorAssociation":"CONTRIBUTOR","body":"BTW, first the there must be consensus on the design \r\n\r\nthe feature request will require the following:\r\n\r\nSDK: allow workflow logic providing priority of workflow / activity\r\n\r\nServer:\r\n* propagate the above priority within history service logic\r\n* pass the above priority to matching service for dispatch\r\n* new priority queue impl\r\n* refactoring matching service for the new priority queue impl\r\n* refactoring matching service's DB schema & impl adding support of priority\r\n* forward / backward compatibility","createdAt":"2021-05-13T19:22:37Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-840778878","viewerDidAuthor":false},{"id":"IC_kwDODNqesM49h-Mn","author":{"login":"rainfd"},"authorAssociation":"NONE","body":"Are there any plans to support priority taskqueue or workflow?","createdAt":"2022-02-08T08:01:16Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":6}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1032315687","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5ODElJ","author":{"login":"PenguinToast"},"authorAssociation":"CONTRIBUTOR","body":"Roughly how much work would this be to implement? We're pretty interested in having this sort of functionality and would be happy to help out with implementation here.","createdAt":"2022-11-09T21:55:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"EYES","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1309428041","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5OHZgD","author":{"login":"medihack"},"authorAssociation":"NONE","body":"Also super interested. We would like to replace a Celery setup (esprecially [Canvas workflows](https://docs.celeryq.dev/en/stable/userguide/canvas.html#canvas-designing-work-flows)) with Temporal, but we make heavy use of [RabbitMQ Message Priorities](https://docs.celeryq.dev/en/stable/userguide/routing.html#rabbitmq-message-priorities) which is not available in Temporal.","createdAt":"2022-11-10T16:30:02Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":5}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1310562307","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5PWzFG","author":{"login":"amajedi-a"},"authorAssociation":"NONE","body":"I'm exploring a similar concept, something I'm curious about is how being able to assign task priority would unlock fairness in execution?","createdAt":"2022-11-29T22:01:02Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1331376454","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5PfPSN","author":{"login":"dnr"},"authorAssociation":"MEMBER","body":"Priorities and fairness are definitely features we're looking at as we evolve the matching portion of Temporal. I think it's fair to say that it will be a lot of work to develop and productionize, and it's going to involve the internals in the matching service, so it might be difficult for outside developers to work on. At this point, the most helpful thing might be to describe your requirements in some more detail, so we can keep them in mind as we do the design (since \"priority\" and \"fairness\" sometimes mean slightly different things to different people.) E.g. how do you want to specify priorities, with what granularity, do you need reserved capacity for different priorities?\r\n\r\nA few more notes:\r\n- From my point of view, priority doesn't \"unlock\" fairness, but they are two features that affect the order that tasks are dispatched in, and they are often desired to be used together. So it makes some sense to work on both at once, or at least design them together.\r\n- One thing that we're unlikely to ever offer is _guaranteed_ priorities or ordering, because of task queue partitions that need to work independently for scalability.","createdAt":"2022-12-01T11:03:32Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"EYES","users":{"totalCount":2}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1333589133","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5PfxND","author":{"login":"medihack"},"authorAssociation":"NONE","body":"In [our case](https://github.com/radexperts/adit), we are looking for a replacement for [Celery](https://docs.celeryq.dev/en/stable/userguide/routing.html#rabbitmq-message-priorities) which itself uses [RabbitMQ queue priorities](https://www.rabbitmq.com/priority.html). If there is for example only one worker it will always take the task with the highest priority (even if that task is later in the queue than others with lower priority).\r\n","createdAt":"2022-12-01T12:59:42Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1333728067","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5TPXH4","author":{"login":"zedongh"},"authorAssociation":"NONE","body":"Is there any work in progress?","createdAt":"2023-01-19T07:12:54Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1396535800","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5Wrszn","author":{"login":"medihack"},"authorAssociation":"NONE","body":"Is it completed? Where can I find more information?","createdAt":"2023-03-04T00:25:15Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1454296295","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5W2HP4","author":{"login":"dnr"},"authorAssociation":"MEMBER","body":"Sorry, the close/open was a github issues mishap. This is still planned but no ETA yet.","createdAt":"2023-03-06T21:26:18Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"CONFUSED","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1457026040","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5nWY6o","author":{"login":"tarampampam"},"authorAssociation":"NONE","body":"Any update on this? Lack of prioritization is not infrequently a reason for abandoning Temporal in my practice...","createdAt":"2023-09-25T15:12:35Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1733922472","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5oLgYi","author":{"login":"charlesmelby"},"authorAssociation":"NONE","body":"Dealing with large work spikes from individual clients is common in our systems.\r\nThis feature would greatly simplify management and help make a stronger case for adopting Temporal.","createdAt":"2023-10-05T00:34:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":3}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1747846690","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5oMxfg","author":{"login":"Multiply"},"authorAssociation":"NONE","body":"> Dealing with large work spikes from individual clients is common in our systems. This feature would greatly simplify management and help make a stronger case for adopting Temporal.\r\n\r\nCan't you run multiple task queues, and manage it by swapping which ones you want to handle?","createdAt":"2023-10-05T06:36:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-1748178912","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6A8Ol6","author":{"login":"gmintoco"},"authorAssociation":"NONE","body":"this would be very useful for us too :) primarily the priority queue idea","createdAt":"2024-06-12T15:03:01Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-2163272058","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6R_5tX","author":{"login":"darkms"},"authorAssociation":"NONE","body":"Having priorities and fairness would definitely help to increase Temporal adoption in our company.\r\n\r\nOur use case for priority is that we have the same operations used for real-time use cases, such as when user click a button in UI and expect immediate answer, and reused for asynchronous batch processing, but with much larger scale (can make all workers fully utilized for minutes-hours per each batch).\r\nIdeally we'd want to reuse all the same infrastructure, avoid over-provisioning & idle resources, and just have Temporal workers execute items with higher priority before going to lower priority ones (ie have worker activity queue be sorted by priority).\r\nCurrently we work-around this by slightly over-provisioning our infra and reserving some capacity for real-time use cases while limiting the activity tasks concurrency for workers. Sadly it is not always enough to process sudden influx of real-time requests, but also when there aren't any real-time requests that leaves the reserved capacity idle, leading to inefficient utilization.\r\n\r\nOur use case for fairness is to allocate available service capacity equally for each group (user/tenant/other grouping dimension), but at the same time also we are OK with allowing even a single group to consume all of available capacity if nobody else is using the service at the moment.\r\nSo far we haven't found a way to achieve it with Temporal, we're still using our bespoke external scheduler component with centralized database that schedules items 1by1 up to pre-configured amount of concurrently running jobs, I don't really like it much because it's so hard to reuse across services.\r\nWould absolutely love if Temporal offered a similar feature out of the box, maybe it could be in form of job scheduler configuration where we could define dynamic priority formula with ability to lookup # of workflow/activities with same workflow/activity attributes already being executed (at the time of scheduling).","createdAt":"2024-10-31T09:49:36Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":12}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-2449447767","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6SzVgu","author":{"login":"atihkin"},"authorAssociation":"NONE","body":"Thanks @darkms that's really good input. \r\n\r\nThe team is actively looking into this feature so we welcome feedback on use cases. ","createdAt":"2024-11-07T18:19:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":3}},{"content":"HEART","users":{"totalCount":4}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-2462930990","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6glCOc","author":{"login":"Hubro"},"authorAssociation":"NONE","body":"I'm using Temporal at my job to process large batches of data that can take multiple days to complete. This creates hundreds or thousands of workflows in Temporal that all \"start\" at the same time. This works fine, but one big issue is that it can take hours before the first workflow finishes, even though a single workflow only takes a couple of minutes to finish. This is because Temporal spreads the limited workers around to progress on all workflows concurrently. I want Temporal to prioritize *finishing* workflows higher than progressing other workflows.\n\nIn my case, there is almost no value in having 1000 workflows with the first 2 activities completed, it would be far more valuable to have 200 workflows completed and 800 unstarted. This could be achieved by prioritizing later activities in a workflow higher than earlier activities, which is why I'm very interested in this feature.","createdAt":"2025-03-03T11:27:26Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":11}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-2694063004","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6hFmtd","author":{"login":"millerick"},"authorAssociation":"NONE","body":"> The team is actively looking into this feature so we welcome feedback on use cases.\n\nIs there any guidance on how far in the future priority queues are?  Asking to know how much time/effort/energy we should invest in crafting our own solution that is robust vs. waiting for one to be provided.","createdAt":"2025-03-06T02:23:25Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-2702601053","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6hKTAS","author":{"login":"atihkin"},"authorAssociation":"NONE","body":"Thank you everyone for your patience! Yes, we (@dnr, @stephanos, myself, several others) are actively working on features for task queue level priority and fairness. At a high-level, here's where we're heading directionally. \n\nWe plan to ship features in the order described here starting from \"least complex\" to the \"most complex\". Timing-wise, we will start to roll this out mid-year 2025 and incrementally add more features as we go. \n\n### Key features\n\n**Simple priority**: This is an integer in the range [1, 5], with default 3. Lower numbers are a higher priority. Tasks are dispatched strictly in order i.e. all “1” tasks come before all “4” tasks in a matching backlog.\n\nThe intended semantics of priority are to differentiate e.g. “interactive”/”real-time” vs “batch”/”idle” work. Also, it can be used as an “emergency priority” mechanism for a user to say “run this right now, ignoring everything else”.\n\nThese fixed priorities are mostly exact (exact within one task queue partition). You will also be able to define rate limits per priority-level. \n\n**Fairness by key and weight**: Fairness is controlled by two values: a short string key (up to 64 bytes) and a positive float weight in [1e-6, 1e6]. The default is an empty string for the key and 1.0 for the weight. \n\nFairness is intended for a few different purposes (note that this is a non-exhaustive list and can be used in conjunction):\n- In multi-tenant situations, giving each tenant an approximately equal (or weighted unequal) share of resources.\n- Defining priority tiers with weighted capacity, e.g. “high”/”medium”/”low” with weights 70/20/10, to allow spending more resources on more important work without starving less important. If one class is not present, the other classes can expand to use all capacity.\n- Activity type could be used as a fairness key to avoid starving specific activities on a shared task queue.\n\nYou will be able to define rate limits per fairness-level. \n\nTasks are dispatched such that the ratio of dispatch rates between two keys is approximately equal to the ratio of their weights. i.e. for two keys with the same weight, their tasks should get dispatched at approximately the same rate, no matter how many tasks each of them has in the backlog. This prevents “heavy” keys from disrupting “light” keys.\n\nThere is no limit on cardinality of fairness keys, but if more than a certain number are used, the accuracy may degrade.\n\nFairness is approximate across both time and space: a) it may take some time for the fairness mechanism to react to a change in distribution of fairness keys, and b) not all decisions would be made the same as if we could use infinite resources for tracking past and future tasks. \n\n**Ordering i.e. arbitrary priorities**: The ordering key is given by an arbitrary positive integer (64-bit) with no limit on cardinality with default of 1. We attempt to dispatch tasks in order of ordering key (small to large), after taking fairness into account.\n\nOrdering is intended for situations like:\n\n-  Using a continuous value like workflow start time, to order activities of older workflows before activities of newer workflows. This reduces the negative effects of starting many workflows that compete for resources, which could otherwise starve the older ones.\n- Similarly, using original activity start time to prioritize activity retries over new activities, or deadline to act as a deadline scheduler.\n- Users who want precise control over activity execution order and want to supply an arbitrary scheduling ordering.\n\n_Levels apply in order from priority to fairness to ordering: Priority first, then fairness applies within a priority level, then ordering applies within a fairness key. Priority and ordering are exact (at least within partitions), while fairness may be approximate._\n\n**Please note that these features are in active development, so everything described here is subject to change as we build and iterate. To stay in touch with the latest developments or to provide feedback / ask questions, please join the channel #priority-fairness in Temporal's community [Slack](https://join.slack.com/share/enQtODU0NTI4OTE4NDU2Ny1hZDEyMjBhZTk5MWNkNzFmNTYwNDI1ZGRjNGRhYWZmYWViMWU1NmNlOGUzYTc1MWQ1ZjBjMzIzZjk1OGU0YTZj).**","createdAt":"2025-03-06T13:18:12Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}},{"content":"HOORAY","users":{"totalCount":32}},{"content":"HEART","users":{"totalCount":35}},{"content":"ROCKET","users":{"totalCount":21}},{"content":"EYES","users":{"totalCount":2}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-2703831058","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6jKm0D","author":{"login":"JobaDiniz"},"authorAssociation":"NONE","body":"> I want Temporal to prioritize _finishing_ workflows higher than progressing other workflows.\n\nThat's **exactly** our use case!\n\nWe don't even need to \"start\" all workflow **at the same time**. By starting batches of workflows within a timeframe, we want the first ones to finish earlier than the last ones without much contention. Imagine the batches keep coming, eventually we want the later batches to not even start (not the workflow, not the activities) until earlier batches completely finish.","createdAt":"2025-03-19T17:21:46Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":7}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-2737466627","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6lgvD7","author":{"login":"kshitij-g"},"authorAssociation":"NONE","body":"Eagerly waiting for this feature. We have Celery and RabbitMQ implementation which we want to migrate to temporal. Priorities are a must for us otherwise some workflows will never complete. E.g. if activity A within a workflow has returned, then next activity B needs to be finished within 5 mins, otherwise the results of activity A are invalid and activity A itself needs to be retried.","createdAt":"2025-04-03T20:15:29Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-2776822011","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6-_sVm","author":{"login":"NikhilVerma"},"authorAssociation":"NONE","body":"We have a heap based slot supplier, i am considering to try and tweak it to adjust slots based on priority.\n\n```ts\nimport v8 from \"node:v8\";\nimport { logger } from \"@nonfx/logger\";\nimport {\n\ttype CustomSlotSupplier,\n\ttype SlotInfo,\n\ttype SlotReserveContext,\n\ttype SlotMarkUsedContext,\n\ttype SlotReleaseContext,\n\ttype SlotPermit\n} from \"@temporalio/worker\";\nimport { uuid4 } from \"@temporalio/workflow\";\n\n// Enable detailed logging for debugging\nconst DETAILED_LOGGING = false;\n\n/**\n * A simple heap-based slot supplier that uses a threshold approach.\n * When heap usage exceeds the threshold, it triggers garbage collection\n * and blocks new slot reservations until memory usage drops.\n */\nexport class HeapBasedSlotSupplier<SI extends SlotInfo> implements CustomSlotSupplier<SI> {\n\tpublic readonly type = \"custom\" as const;\n\n\tprivate readonly options: HeapBasedSlotSupplierOptions;\n\tprivate readonly activeSlots = new Set<SlotPermit>();\n\tprivate lastHeapCheck = 0;\n\tprivate isOverThreshold = false;\n\n\tconstructor(options: Partial<HeapBasedSlotSupplierOptions> = {}) {\n\t\tthis.options = {\n\t\t\theapThreshold: 80,\n\t\t\tcheckInterval: 1000,\n\t\t\tenableGC: true,\n\t\t\tminWaitBeforeSlotReservation: 50,\n\t\t\t...options\n\t\t};\n\n\t\tlogger.info(\"HeapBasedSlotSupplier initialized\", {\n\t\t\theapThreshold: this.options.heapThreshold,\n\t\t\tcheckInterval: this.options.checkInterval,\n\t\t\tenableGC: this.options.enableGC,\n\t\t\tminWaitBeforeSlotReservation: this.options.minWaitBeforeSlotReservation\n\t\t});\n\t}\n\n\t/**\n\t * Get current heap usage percentage\n\t */\n\tprivate getHeapUsagePercent(): number {\n\t\tconst heapStats = v8.getHeapStatistics();\n\t\treturn (heapStats.used_heap_size / heapStats.heap_size_limit) * 100;\n\t}\n\n\t/**\n\t * Check if heap usage is above threshold and trigger GC if needed\n\t * @param force - Force check regardless of checkInterval\n\t */\n\tprivate checkHeapUsage(force = false): void {\n\t\tconst now = Date.now();\n\n\t\t// Only check periodically to avoid excessive overhead, unless forced\n\t\tif (!force && now - this.lastHeapCheck < this.options.checkInterval) {\n\t\t\treturn;\n\t\t}\n\n\t\tthis.lastHeapCheck = now;\n\t\tconst heapUsagePercent = this.getHeapUsagePercent();\n\n\t\tif (heapUsagePercent > this.options.heapThreshold) {\n\t\t\tif (!this.isOverThreshold) {\n\t\t\t\tthis.isOverThreshold = true;\n\t\t\t\tlogger.warn(\"Heap usage exceeded threshold\", {\n\t\t\t\t\theapUsagePercent: heapUsagePercent.toFixed(2),\n\t\t\t\t\tthreshold: this.options.heapThreshold,\n\t\t\t\t\tactiveSlots: this.activeSlots.size\n\t\t\t\t});\n\t\t\t}\n\n\t\t\t// Trigger garbage collection if enabled and available\n\t\t\tif (this.options.enableGC && global.gc) {\n\t\t\t\ttry {\n\t\t\t\t\tglobal.gc({\n\t\t\t\t\t\texecution: \"sync\"\n\t\t\t\t\t});\n\t\t\t\t\tlogger.debug(\"Garbage collection triggered\");\n\t\t\t\t} catch (error) {\n\t\t\t\t\tlogger.error(\"Failed to trigger garbage collection\", { error });\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tif (this.isOverThreshold) {\n\t\t\t\tthis.isOverThreshold = false;\n\t\t\t\tlogger.info(\"Heap usage back below threshold\", {\n\t\t\t\t\theapUsagePercent: heapUsagePercent.toFixed(2),\n\t\t\t\t\tthreshold: this.options.heapThreshold\n\t\t\t\t});\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Wait for heap usage to drop below threshold\n\t */\n\tprivate waitForHeapRecovery(abortSignal: AbortSignal): Promise<void> {\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tconst checkInterval = setInterval(() => {\n\t\t\t\tif (abortSignal.aborted) {\n\t\t\t\t\tclearInterval(checkInterval);\n\t\t\t\t\treject(abortSignal.reason ?? new Error(\"Slot reservation aborted\"));\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tthis.checkHeapUsage();\n\n\t\t\t\tif (!this.isOverThreshold) {\n\t\t\t\t\tclearInterval(checkInterval);\n\t\t\t\t\tresolve();\n\t\t\t\t}\n\t\t\t}, 100); // Check every 100ms when waiting\n\t\t});\n\t}\n\n\t/**\n\t * Wait a minimum time before slot reservation to allow existing slots to consume memory\n\t */\n\tprivate waitBeforeSlotReservation(abortSignal: AbortSignal): Promise<void> {\n\t\tif (this.options.minWaitBeforeSlotReservation <= 0) {\n\t\t\treturn Promise.resolve();\n\t\t}\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tconst timeout = setTimeout(() => {\n\t\t\t\tresolve();\n\t\t\t}, this.options.minWaitBeforeSlotReservation);\n\n\t\t\t// Handle abort signal\n\t\t\tconst handleAbort = () => {\n\t\t\t\tclearTimeout(timeout);\n\t\t\t\treject(abortSignal.reason ?? new Error(\"Slot reservation aborted\"));\n\t\t\t};\n\n\t\t\tif (abortSignal.aborted) {\n\t\t\t\tclearTimeout(timeout);\n\t\t\t\treject(abortSignal.reason ?? new Error(\"Slot reservation aborted\"));\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tabortSignal.addEventListener(\"abort\", handleAbort, { once: true });\n\t\t});\n\t}\n\n\tasync reserveSlot(ctx: SlotReserveContext, abortSignal: AbortSignal): Promise<SlotPermit> {\n\t\t// Check if already aborted\n\t\tabortSignal.throwIfAborted();\n\n\t\t// Wait minimum time before attempting reservation to let existing slots consume memory\n\t\tawait this.waitBeforeSlotReservation(abortSignal);\n\n\t\t// Block until we have capacity and heap is below threshold\n\n\t\twhile (true) {\n\t\t\tabortSignal.throwIfAborted();\n\n\t\t\t// Always check current heap usage before proceeding (force check)\n\t\t\tthis.checkHeapUsage(true);\n\n\t\t\t// Wait for heap recovery if needed\n\t\t\tif (this.isOverThreshold) {\n\t\t\t\tawait this.waitForHeapRecovery(abortSignal);\n\t\t\t\tcontinue; // Re-check everything after heap recovery\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\n\t\t// Final abort check before creating permit\n\t\tabortSignal.throwIfAborted();\n\n\t\t// Create and track the permit\n\t\tconst permit: SlotPermit = uuid4();\n\n\t\tthis.activeSlots.add(permit);\n\n\t\tif (DETAILED_LOGGING) {\n\t\t\tlogger.debug(\"Slot reserved\", {\n\t\t\t\tslotType: ctx.slotType,\n\t\t\t\ttaskQueue: ctx.taskQueue,\n\t\t\t\tactiveSlots: this.activeSlots.size,\n\t\t\t\theapUsagePercent: this.getHeapUsagePercent().toFixed(2)\n\t\t\t});\n\t\t}\n\n\t\treturn permit;\n\t}\n\n\ttryReserveSlot(ctx: SlotReserveContext): SlotPermit | null {\n\t\t// For tryReserveSlot, we can't wait async, but we can check if we should be more conservative\n\t\t// by considering how recently we handed out slots\n\t\tconst timeSinceLastCheck = Date.now() - this.lastHeapCheck;\n\t\tif (timeSinceLastCheck < this.options.minWaitBeforeSlotReservation) {\n\t\t\t// Be more conservative and reject if we haven't waited long enough\n\t\t\treturn null;\n\t\t}\n\n\t\t// Always check current heap usage before proceeding (force check)\n\t\tthis.checkHeapUsage(true);\n\n\t\t// If heap usage is above threshold, reject reservation\n\t\tif (this.isOverThreshold) {\n\t\t\treturn null;\n\t\t}\n\n\t\t// Create and track the permit\n\t\tconst permit: SlotPermit = uuid4();\n\n\t\tthis.activeSlots.add(permit);\n\n\t\tif (DETAILED_LOGGING) {\n\t\t\tlogger.debug(\"Slot eagerly reserved\", {\n\t\t\t\tslotType: ctx.slotType,\n\t\t\t\ttaskQueue: ctx.taskQueue,\n\t\t\t\tactiveSlots: this.activeSlots.size,\n\t\t\t\theapUsagePercent: this.getHeapUsagePercent().toFixed(2)\n\t\t\t});\n\t\t}\n\n\t\treturn permit;\n\t}\n\n\tmarkSlotUsed(ctx: SlotMarkUsedContext<SI>): void {\n\t\tif (DETAILED_LOGGING) {\n\t\t\tlogger.debug(\"Slot marked as used\", {\n\t\t\t\tslotInfo: ctx.slotInfo,\n\t\t\t\tactiveSlots: this.activeSlots.size\n\t\t\t});\n\t\t}\n\t}\n\n\treleaseSlot(ctx: SlotReleaseContext<SI>): void {\n\t\tif (this.activeSlots.has(ctx.permit)) {\n\t\t\tthis.activeSlots.delete(ctx.permit);\n\n\t\t\tif (DETAILED_LOGGING) {\n\t\t\t\tlogger.debug(\"Slot released\", {\n\t\t\t\t\tslotInfo: ctx.slotInfo,\n\t\t\t\t\tactiveSlots: this.activeSlots.size,\n\t\t\t\t\theapUsagePercent: this.getHeapUsagePercent().toFixed(2)\n\t\t\t\t});\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Get current statistics\n\t */\n\tgetStats() {\n\t\treturn {\n\t\t\tactiveSlots: this.activeSlots.size,\n\t\t\theapUsagePercent: this.getHeapUsagePercent(),\n\t\t\tisOverThreshold: this.isOverThreshold,\n\t\t\toptions: this.options\n\t\t};\n\t}\n}\n\nexport type HeapBasedSlotSupplierOptions = {\n\t/**\n\t * Heap usage threshold as a percentage (0-100).\n\t * When heap usage exceeds this threshold, garbage collection will be triggered.\n\t * Default: 80\n\t */\n\theapThreshold: number;\n\n\t/**\n\t * How often to check heap usage (in milliseconds).\n\t * Default: 1000\n\t */\n\tcheckInterval: number;\n\n\t/**\n\t * Whether to enable garbage collection when threshold is exceeded.\n\t * Default: true\n\t */\n\tenableGC: boolean;\n\n\t/**\n\t * Minimum wait time before slot reservation (in milliseconds).\n\t * This allows existing slots to consume memory before handing out more slots.\n\t * Set to 0 to disable. Default: 50\n\t */\n\tminWaitBeforeSlotReservation: number;\n};\n```","createdAt":"2025-08-20T06:28:34Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-3204367718","viewerDidAuthor":false},{"id":"IC_kwDODNqesM7AAjPX","author":{"login":"liran"},"authorAssociation":"NONE","body":"Simple priority for task queues - pre-release https://github.com/temporalio/temporal/releases/tag/v1.28.0","createdAt":"2025-08-25T18:49:00Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"ROCKET","users":{"totalCount":3}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-3221369815","viewerDidAuthor":false},{"id":"IC_kwDODNqesM7AMt6U","author":{"login":"JobaDiniz"},"authorAssociation":"NONE","body":"> Simple priority for task queues - pre-release https://github.com/temporalio/temporal/releases/tag/v1.28.0\n\n### Use case clarification: prioritizing active workflows over starting new ones\n\nWe run a backlog-driven scenario with a limited pool of workers (and limited slots) processing a large number of workflows. All workflows are of the same type, and they all tend to “start” around the same time, quickly filling every worker.\n\n**The result**: workflows that began early stagnate because their subsequent tasks or activities are queued behind many others, preventing completion. For example, Workflow A may already be partway done, but once workers are saturated, its remaining tasks get stuck in the same backlog as entirely new workflows. Temporal continues pulling in new workflows, leaving us with hundreds or thousands that are all partially completed rather than a smaller set that are fully done.\n\n**Desired behavior**: Once a workflow has started (executed some activities), it should be prioritized to finish before beginning new workflows from the backlog. In practice, it’s far better to end up with 200 completed workflows and 800 untouched than 1000 half-finished.\n\n### Does v1.28.0 address this?\nUnfortunately, no. The new **Task Queue Priority** mechanism allows assigning priority values within a single queue for activities, but since all our workflows are of the same type, there’s no meaningful way to differentiate them by priority. The mechanism doesn’t ensure that in-progress workflows get worker time over brand-new ones.\n\n### Roadmap\nIs there anything planned to support prioritizing _already-started_ workflows over new ones when worker capacity is limited?","createdAt":"2025-08-26T15:03:29Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":3}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-3224559252","viewerDidAuthor":false},{"id":"IC_kwDODNqesM7CEXcm","author":{"login":"fabianoengler"},"authorAssociation":"NONE","body":"> **The result**: workflows that began early stagnate because their subsequent tasks or activities are queued behind many others, preventing completion. (...)\n> \n> **Desired behavior**: Once a workflow has started (executed some activities), it should be prioritized to finish before beginning new workflows from the backlog. In practice, it’s far better to end up with 200 completed workflows and 800 untouched than 1000 half-finished.\n> ### Does v1.28.0 address this?\n> \n> Unfortunately, no. \n\nWe have a similar use case. For now we are testing a concept of throttling the start of new workflows by having our own queue and only starting workflows on temporal if there is capacity, if there is not we keep waiting for workflows to finish, some sort of sliding window, but of course this a somewhat poor workaround, it's hard to measure capacity correctly because even thought the workflows and activities are of the same type, the data they process varies a lot in size and on the load the impose.","createdAt":"2025-09-04T22:02:47Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-3255924518","viewerDidAuthor":false},{"id":"IC_kwDODNqesM7C0mB2","author":{"login":"hckhanh"},"authorAssociation":"NONE","body":"FYI: https://youtu.be/Cf6_PBoyxbk?si=H43wXaZUd1qX6_lt","createdAt":"2025-09-09T01:43:42Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}},{"content":"HEART","users":{"totalCount":3}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-3268567158","viewerDidAuthor":false},{"id":"IC_kwDODNqesM7HjGee","author":{"login":"testingmodels43-beep"},"authorAssociation":"NONE","body":"is fairness supported in OSS version 1.28.1? \nI am using the official compose setup for temporal and even with the dynamic config etc enabled its not working like the demo","createdAt":"2025-09-29T16:11:45Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/1507#issuecomment-3347867550","viewerDidAuthor":false}],"createdAt":"2021-04-29T18:58:01Z","labels":[{"id":"MDU6TGFiZWwxNjIxMDMwNzg3","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwyMDE5NzgzNzUw","name":"API","description":"Issues/features involving the API","color":"3d910f"},{"id":"MDU6TGFiZWwyMDE5OTIzMjY4","name":"architecture","description":"","color":"ad9fe0"}],"milestone":null,"number":1507,"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":50}},{"content":"EYES","users":{"totalCount":3}}],"state":"OPEN","title":"Provide priority task queues","updatedAt":"2025-12-01T22:31:09Z","url":"https://github.com/temporalio/temporal/issues/1507"}
