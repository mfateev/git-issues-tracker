{"assignees":[{"id":"MDQ6VXNlcjQ0MDg3MjQ=","login":"atihkin","name":"Nikitha Suryadevara","databaseId":0}],"author":{"id":"MDQ6VXNlcjE0NjM2MjI=","is_bot":false,"login":"mfateev","name":"Maxim Fateev"},"body":"https://github.com/kedacore/keda","closedAt":"2025-04-10T16:02:01Z","comments":[{"id":"IC_kwDODNqesM5NPhDa","author":{"login":"yiminc"},"authorAssociation":"MEMBER","body":"This sounds like a feature request to SDK?","createdAt":"2022-10-29T17:37:58Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-1295913178","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5NYxs0","author":{"login":"aakarim"},"authorAssociation":"NONE","body":"Just casting some light on this - we use KEDA with Postgres queries right now to scale our workers when there is more than a predefined number of tasks that our base node pool can handle. \r\n\r\nAt the moment it's 2 workers base and when there are more than 10 concurrent Workflows we spin up more servers, 5 concurrent Workflows per node. It's working well, but we're always a little worried that the schema will change and break everything. It would be great to abstract this out to a native integration with KEDA.","createdAt":"2022-11-01T10:44:26Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-1298340660","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5eTuMp","author":{"login":"ross-p-smith"},"authorAssociation":"NONE","body":"I have this working within KEDA using the temporalClient's ListOpenWorkflow method and would love to chat to someone from the temporal maintainers about the best way to get this into the various repos and whether things make sense","createdAt":"2023-06-08T09:37:47Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-1582228265","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5jeJYr","author":{"login":"febinct"},"authorAssociation":"NONE","body":"PR for the same https://github.com/kedacore/keda/pull/4863/files ","createdAt":"2023-08-08T03:20:53Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}},{"content":"ROCKET","users":{"totalCount":3}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-1668847147","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5sH91q","author":{"login":"RonaldGalea"},"authorAssociation":"NONE","body":"+1 would be a great feature to have. It's common to have specific workers (services) listening on specific task queues. Exposing the size of a given task queue would be a very precise autoscaling metric for those workers.","createdAt":"2023-11-16T08:54:32Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"ROCKET","users":{"totalCount":3}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-1814027626","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5_TecH","author":{"login":"sinkr"},"authorAssociation":"NONE","body":"My organization would like to see this, too.","createdAt":"2024-05-28T17:50:32Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2135811847","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5_TfkD","author":{"login":"henrytseng"},"authorAssociation":"NONE","body":"+1 This would be great to have","createdAt":"2024-05-28T17:53:08Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2135816451","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5_T_SC","author":{"login":"justinfx"},"authorAssociation":"NONE","body":"My studio has just started testing Temporal and it would be great to have this feature. ","createdAt":"2024-05-28T19:18:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2135946370","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NSlkb","author":{"login":"jhecking"},"authorAssociation":"NONE","body":"There were two previous attempts to implement a Temporal scaler for Keda, but both got closed. Ref. https://github.com/kedacore/keda/pull/4721 and https://github.com/kedacore/keda/pull/4863.\r\n\r\n@cretz, since you were directly involved in https://github.com/kedacore/keda/pull/4863, do you think the new Task Queue Statistics added in [v1.25](https://github.com/temporalio/temporal/releases/tag/v1.25.0) would be the right way to implement a Keda scaler for Temporal? If so, any thoughts on whether the Temporal team might consider to implement this or whether support would have to come from the community?\r\n\r\n\r\n","createdAt":"2024-09-24T07:50:56Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2370459931","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NS9gU","author":{"login":"jhecking"},"authorAssociation":"NONE","body":"To provide some more context: In particular, we are interested in using Keda's ability to scale down Temporal workers to zero if there are no pending tasks on the worker's task queue(s) for some period of time. This is not possible using the (previously?) recommended way of scaling Temporal workers based on the `temporal_worker_task_slots_available` metric generated by the workers themselves.","createdAt":"2024-09-24T08:04:47Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2370557972","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NWARW","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"> do you think the new Task Queue Statistics added in [v1.25](https://github.com/temporalio/temporal/releases/tag/v1.25.0) would be the right way to implement a Keda scaler for Temporal?\r\n\r\nAbsolutely, and this is on our roadmap to build. We demo'd this at our Replay conference. Stay tuned for more info.","createdAt":"2024-09-24T13:56:22Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":3}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2371355734","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NbbIm","author":{"login":"jhecking"},"authorAssociation":"NONE","body":"> this is on our roadmap to build\r\n\r\n@cretz Any further information you can share on this, i.e. possibly a rough timeline? This would help us decide whether we can wait for an official version or whether we need to build something in-house for our own use first.","createdAt":"2024-09-25T02:43:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2372776486","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NbdeT","author":{"login":"febinct"},"authorAssociation":"NONE","body":"you could autoscale using https://keda.sh/docs/2.15/scalers/prometheus/ @jhecking \r\neg: \"histogram_quantile(0.95, sum(rate(task_schedule_to_start_latency_bucket{exported_namespace=\"namespace\",task_type=\\\"Activity\\\", taskqueue=\"queuename\"}[5m])) by (taskqueue, task_type, le))\"","createdAt":"2024-09-25T02:52:37Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2372786067","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NbhCD","author":{"login":"jhecking"},"authorAssociation":"NONE","body":"Thanks, @febinct. That's what we currently do for 1->n scaling. What we are looking for is a solution that can handle 0->1 / 1->0 scaling as well. ","createdAt":"2024-09-25T02:56:24Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2372800643","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NbnGB","author":{"login":"febinct"},"authorAssociation":"NONE","body":"@jhecking Scaling from zero to one or vice versa isnâ€™t currently feasible with Temporal because it relies on workers continuously polling task queues. If at least one worker isnâ€™t running, weâ€™re unable to submit jobs and execute it as metrics are exported from sdk. This setup doesnâ€™t support Lambda or keda scaled job like  use-cases at the moment. I discussed this with Maxim (CTO of Temporal) during the last Temporal meetup, and he mentioned that itâ€™s on the roadmap didnt get any ETA thou :) \r\n\r\n\r\nOur team raised [this PR](https://github.com/kedacore/keda/pull/4863), and weâ€™re actively exploring Task Queue Statistics-based autoscaling. We plan to raise a PR in KEDA within the next 2-4 weeks.\r\n\r\n\r\nas a hack to avoid running larger machines and to save cost what we did was run a very small pod from that temporal workflow we triggered an sqs event and using sqs we triggered https://keda.sh/docs/1.4/concepts/scaling-jobs/ and then waited for signal to come back from that sqs processor which is executing the expensive job ","createdAt":"2024-09-25T03:19:45Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2372825473","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NfbX7","author":{"login":"jhecking"},"authorAssociation":"NONE","body":"> Scaling from zero to one or vice versa isnâ€™t currently feasible with Temporal because it relies on workers continuously polling task queues. If at least one worker isnâ€™t running, weâ€™re unable to submit jobs and execute it as metrics are exported from sdk. \r\n\r\n@febinct I don't think this is correct with regards to the new task queue statistics. I was able to spin up a v1.25 dev server using the Temporal CLI. Then I used the hello-world examples to start several workflows on tasks queues that had no active workers, as well as some additional workflows that ran activities on task queues that had no active workers. When I use the Temporal CLI to query the DescribeTaskQueue API, I get the expected stats, i.e.\r\n\r\n```\r\nâ¯ temporal task-queue describe --task-queue hello-world-workflow-on-task-queue-without-workers\r\nTask Queue Statistics:\r\n    BuildID    TaskQueueType  ApproximateBacklogCount  ApproximateBacklogAge  BacklogIncreaseRate  TasksAddRate  TasksDispatchRate\r\n  UNVERSIONED  workflow                             2  1m 58.619811s                  0.028910344   0.028910344                  0\r\n  UNVERSIONED  activity                             0  0s                                       0             0                  0\r\nPollers:\r\n  BuildID  TaskQueueType  Identity  LastAccessTime  RatePerSecond\r\n  \r\nâ¯ temporal task-queue describe --task-queue hello-world-activity-on-task-queue-without-workers\r\nTask Queue Statistics:\r\n    BuildID    TaskQueueType  ApproximateBacklogCount  ApproximateBacklogAge  BacklogIncreaseRate  TasksAddRate  TasksDispatchRate\r\n  UNVERSIONED  workflow                             0  0s                                       0             0                  0\r\n  UNVERSIONED  activity                             3  13.814805s                     0.090869784   0.090869784                  0\r\nPollers:\r\n  BuildID  TaskQueueType  Identity  LastAccessTime  RatePerSecond\r\n```\r\n\r\nSo I think it should be possible to implement a Keda scaler that queries the DescribeTaskQueue API and uses the ApproximateBacklogCount metric to make 0->1 and 1->0 scaling decisions.\r\n\r\n@cretz please correct me if I got any of this wrong.","createdAt":"2024-09-25T11:31:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2373826043","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NgDFp","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"> Any further information you can share on this, i.e. possibly a rough timeline?\r\n\r\nI am afraid there is no specific timeline at this time.\r\n\r\n> So I think it should be possible to implement a Keda scaler that queries the DescribeTaskQueue API and uses the ApproximateBacklogCount metric to make 0->1 and 1->0 scaling decisions.\r\n\r\nYes, unlike schedule to start latency (which is worker side so requires a worker), backlog count can be used for scale-to-zero use cases and was one of the primary motivators behind this API.\r\n\r\nFeel free to come discuss scaling in our community slack or our community forums.","createdAt":"2024-09-25T12:46:24Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2373988713","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NgUuc","author":{"login":"jhecking"},"authorAssociation":"NONE","body":"> Yes, unlike schedule to start latency (which is worker side so requires a worker), backlog count can be used for scale-to-zero use cases and was one of the primary motivators behind this API.\r\n\r\nGreat! Thanks for the confirmation.","createdAt":"2024-09-25T13:16:23Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2374060956","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NlhqJ","author":{"login":"atihkin"},"authorAssociation":"NONE","body":"Hi all ðŸ‘‹ðŸ½ I'm Nikitha, a PM here at Temporal and I wanted to acknowledge all the great feedback and discussion in this thread. \r\n\r\nI'm excited to share that we do have imminent plans to build and contribute a KEDA scaler upstream (yes scale to zero will work as @cretz confirmed). I don't have an ETA for you just yet, but it's actively in the works and we will share more soon! ","createdAt":"2024-09-25T23:06:57Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":9}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2375424649","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NowfH","author":{"login":"febinct"},"authorAssociation":"NONE","body":"https://github.com/kedacore/keda/pull/6191/files pr for the same please review @cretz @jhecking ","createdAt":"2024-09-26T08:19:58Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"HEART","users":{"totalCount":3}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2376271815","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NpqZ8","author":{"login":"jhecking"},"authorAssociation":"NONE","body":"> https://github.com/kedacore/keda/pull/6191/files pr for the same please review @cretz @jhecking\r\n\r\nThank you! Will take a look.","createdAt":"2024-09-26T10:03:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2376509052","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6Nrr0L","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"@febinct - from @atihkin above, \"we do have imminent plans to build and contribute a KEDA scaler upstream\", but this seems to preempt us from being able to build this by adding an externally created one. Our algorithm may differ slightly from the one in the PR (for instance, combined task queue stats probably not the way to go unless opted in, build-id-specific stats may be better). I will get with the engineers on the scaling project and review the submission. We should hold off on merging this PR until Temporal takes a look and/or submits a similar alternative.","createdAt":"2024-09-26T13:51:23Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2377039115","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NsAOC","author":{"login":"febinct"},"authorAssociation":"NONE","body":"If it makes sense please give review comments happy to collaborate as an extended team and collaborate on the growth of the temporal community. ","createdAt":"2024-09-26T14:22:04Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2377122690","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NsdRu","author":{"login":"jhecking"},"authorAssociation":"NONE","body":"I, for one, am very grateful to @febinct and team for having put their own implementation out there. ðŸ™ I have reviewed the PR and I think it will meet our needs. We are planning to go ahead and run some tests with it to get a feel for how well the 0->1 / 1->0 scaling works for our workloads. Though we would probably wait for the official implementation of the Temporal team before using it in prod.","createdAt":"2024-09-26T15:06:36Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2377241710","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6O701E","author":{"login":"atihkin"},"authorAssociation":"NONE","body":"To update folks on this thread - the Temporal team has taken a look and we've decide to go ahead with @febinct's proposal (thank you for your contribution and also @jhecking for your review!). @robholland has left a few comments in https://github.com/kedacore/keda/pull/6191/files but we do hope to be able to merge this PR soon. ","createdAt":"2024-10-07T22:39:07Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2398047556","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6O9JXC","author":{"login":"febinct"},"authorAssociation":"NONE","body":"All the credit goes to https://github.com/Prajithp from our team. We are actively working on closing the comments and then get merged. Will close soon.\n\nThanks @atihkin ","createdAt":"2024-10-08T01:18:09Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2398393794","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6O94Jf","author":{"login":"jhecking"},"authorAssociation":"NONE","body":"Thank you @Prajithp and  @febinct for pushing this forward! ðŸ™ \r\n\r\nBut I do want to point out that from our perspective the new Keda Temporal scaler is not yet production ready, as we are still faced with the issue of [Keda using up 100% of the allocated CPU](https://github.com/kedacore/keda/pull/6191#issuecomment-2393615091) as soon as we enable the new scaler. I'm continuing to debug the issue but have yet to find a solution.","createdAt":"2024-10-08T02:44:14Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2398585439","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6O-ojL","author":{"login":"febinct"},"authorAssociation":"NONE","body":"we are also checking same @jhecking as of now suspecting creating new gRPC connections too frequently. The MinConnectTimeout of 5 seconds might be causing rapid reconnections if the connection does not succeed within that time frame could be another potential reason. can you also try bypassing Consul temporarily to see if the CPU load decreases as well? as we dont have Consul setup","createdAt":"2024-10-08T04:07:45Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2398783691","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6O-tpk","author":{"login":"jhecking"},"authorAssociation":"NONE","body":"> can you also try bypassing Consul temporarily to see if the CPU load decreases as well? as we dont have Consul setup\r\n\r\nIn our case, the Temporal workers are often running in a different cluster from the Temporal server and Consul is required for the workers and Keda to connect to the Temporal server. So far, none of our Temporal workers (using the Typescript, Java and Python SDKs) have shown any similar issues. But I'll try to replicate this in a different cluster where Consul is not required.","createdAt":"2024-10-08T04:30:26Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2398804580","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6Qx3wV","author":{"login":"raanand-dig"},"authorAssociation":"NONE","body":"any update\r\n","createdAt":"2024-10-22T11:11:34Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2428992533","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6RDwdO","author":{"login":"robholland"},"authorAssociation":"MEMBER","body":"@raanand-dig please follow progress in https://github.com/kedacore/keda/pull/6191/files","createdAt":"2024-10-23T22:55:22Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":2}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2433681230","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6dmjCf","author":{"login":"webchick"},"authorAssociation":"MEMBER","body":"OMG it got merged! ðŸ¤©  Does that mean we can close this ticket?","createdAt":"2025-02-07T21:06:03Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"HOORAY","users":{"totalCount":5}}],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2644127903","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6mYqKH","author":{"login":"webchick"},"authorAssociation":"MEMBER","body":"OMG it got **released**! https://github.com/kedacore/keda/releases/tag/v2.17.0\n\nI think that means we can _definitely_ close this ticket! :D ","createdAt":"2025-04-10T03:53:18Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2791481991","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6mjpD7","author":{"login":"atihkin"},"authorAssociation":"NONE","body":"https://github.com/temporal-community/temporal-scaling-demo -> sample app highlighting how this works! \n\nBut yes with the release of https://github.com/kedacore/keda/releases/tag/v2.17.0 this issue is officially closed.","createdAt":"2025-04-10T16:02:01Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/33#issuecomment-2794361083","viewerDidAuthor":false}],"createdAt":"2019-11-26T22:07:09Z","labels":[{"id":"MDU6TGFiZWwxNjIxMDMwNzg3","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwyMDE5ODA4MDcy","name":"product-integration","description":"New integration with the product","color":"db6db4"},{"id":"MDU6TGFiZWwyMDE5ODkxMzc4","name":"packaging","description":"","color":"baf702"},{"id":"MDU6TGFiZWwzMTM4ODI4MTE3","name":"up-for-grabs","description":"Issues to consider for external contribution","color":"69CD90"}],"milestone":null,"number":33,"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":51}}],"state":"CLOSED","title":"Add workers autoscaling through KEDA","updatedAt":"2025-04-10T16:02:02Z","url":"https://github.com/temporalio/temporal/issues/33"}
