{"assignees":[{"id":"MDQ6VXNlcjM3NzA0Nzg=","login":"yux0","name":"Yu Xia","databaseId":0}],"author":{"id":"MDQ6VXNlcjE0MzYxNDg4","is_bot":false,"login":"muralisrini","name":""},"body":"## Expected Behavior\r\nSuccessful workflow creation . We did not see this problem in 1.18.x. \r\n\r\nNote that there was **no** data migration involved (new deployment including database)\r\n\r\n## Actual Behavior\r\nErrors like\r\n```\r\n2023-01-19T13:46:39.099356181Z {\"level\":\"error\",\"ts\":\"2023-01-19T13:46:39.099Z\",\"msg\":\"error starting temporal-sys-history-scanner-workflow workflow\",\"service\":\"worker\",\"error\":\"context deadline exceeded\",\"logging-call-at\":\"scanner.go:232\",\"stacktrace\":\"go.temporal.io/server/common/log.(*zapLogger).Error\\n\\t/home/builder/temporal/common/log/zap_logger.go:144\\ngo.temporal.io/server/service/worker/scanner.(*Scanner).startWorkflow\\n\\t/home/builder/temporal/service/worker/scanner/scanner.go:232\\ngo.temporal.io/server/service/worker/scanner.(*Scanner).startWorkflowWithRetry.func1\\n\\t/home/builder/temporal/service/worker/scanner/scanner.go:209\\ngo.temporal.io/server/common/backoff.ThrottleRetry.func1\\n\\t/home/builder/temporal/common/backoff/retry.go:170\\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\\n\\t/home/builder/temporal/com  1.\r\n  1.\r\n  1.mon/backoff/retry.go:194\\ngo.temporal.io/server/common/backoff.ThrottleRetry\\n\\t/home/builder/temporal/common/backoff/retry.go:171\\ngo.temporal.io/server/service/worker/scanner.(*Scanner).startWorkflowWithRetry\\n\\t/home/builder/temporal/service/worker/scanner/scanner.go:208\"}\r\n```\r\n\r\n## Steps to Reproduce the Problem\r\n\r\nDoes not happen every time but when it does, seems to persist.\r\n\r\nWe do not have a simple way to recreate this but hoping this might be a known issue given we saw it immediately after going to 1.19.0. We can upgrade to 1.19.1 if this is a know issue fixed in that version.\r\n\r\nAttaching temporal logs...\r\n\r\n[kubectl-logs-temporal-frontend-c84645c8-wj8fr-temporal-frontend-current.txt](https://github.com/temporalio/temporal/files/10475407/kubectl-logs-temporal-frontend-c84645c8-wj8fr-temporal-frontend-current.txt)\r\n[kubectl-logs-temporal-web-557c4795f9-46n76-temporal-web-current.txt](https://github.com/temporalio/temporal/files/10475408/kubectl-logs-temporal-web-557c4795f9-46n76-temporal-web-current.txt)\r\n[kubectl-logs-temporal-worker-77456bfc96-979hl-temporal-worker-current.txt](https://github.com/temporalio/temporal/files/10475409/kubectl-logs-temporal-worker-77456bfc96-979hl-temporal-worker-current.txt)\r\n\r\n\r\n## Specifications\r\n\r\n  - Version: 1.19.0\r\n  - Platform: Ubuntu\r\n","closedAt":"2023-02-16T23:18:17Z","comments":[{"id":"IC_kwDODNqesM5T3378","author":{"login":"yiminc"},"authorAssociation":"MEMBER","body":"Is this a new namespace just created?\r\n\"wf-namespace\":\"manetu\",\"error\":\"Namespace 4ad12a22-a459-4715-836b-b35aacb55d2e is not found.\"\r\n\r\nDid the problem persist after a few minutes?","createdAt":"2023-01-27T22:38:10Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1407155964","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5T36iy","author":{"login":"muralisrini"},"authorAssociation":"NONE","body":"> Is this a new namespace just created? \"wf-namespace\":\"manetu\",\"error\":\"Namespace 4ad12a22-a459-4715-836b-b35aacb55d2e is not found.\"\r\n\r\nYes, this is a new namespace just created.\r\n\r\n> Did the problem persist after a few minutes?\r\n\r\nIt did.\r\n","createdAt":"2023-01-27T22:55:21Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1407166642","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5T4BpH","author":{"login":"dnr"},"authorAssociation":"MEMBER","body":"It does succeed eventually (after ~19 minutes) but that does seem weird.\r\n\r\nIt looks like something odd is going on with history and ringpop (service discovery): note that the worker locates frontend, matching, and itself right at startup, but doesn't locate history for another 12 seconds. (The worker doesn't contact history directly to start a workflow but it suggests that the frontend might also have a problem.) Then at 2023-01-19T13:46:27.123Z it seems to lose history completely, and find it again 25 seconds later, and at the next retry the scanner workflow is created successfully.\r\n\r\nLooking at the frontend log itself, it either see no history pods, or has trouble reaching the one it can see, until it succeeds.\r\n\r\nSo I'd suspect something about the connectivity of the history pod.","createdAt":"2023-01-27T23:44:48Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1407195719","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5UFw91","author":{"login":"Gibstick"},"authorAssociation":"NONE","body":"We're seeing this issue too, with the same characteristics:\r\n\r\n- suspicious log messages of `Current reachable members` that seem to indicate services appearing and disappearing for no reason\r\n- it doesn't always happen, but when it does, the temporal cluster just stays in a broken state for a long time (not sure if it ever recovers)\r\n- we did not see the issue in 1.18.x\r\n- after downgrading to 1.18.5, we aren't able to reproduce the issue\r\n\r\nSome differences that don't seem to be substantial are that\r\n\r\n- it happens in 1.19.1\r\n- we're seeing it after a migration, so it's not limited to just fresh deploys","createdAt":"2023-01-31T17:36:39Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1410797429","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5UF_aY","author":{"login":"yux0"},"authorAssociation":"MEMBER","body":"Can you provide the logs from history role? Looking at the frontend and worker logs, the worker failed to start because of the error in frontend and the frontend errors are related to the history role.","createdAt":"2023-01-31T18:13:32Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1410856600","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5UGfWX","author":{"login":"Gibstick"},"authorAssociation":"NONE","body":"Here are all logs for completeness. Using the timestamps in the logs, I started everything at pretty much the same time at 19:55, and then waited a few minutes for the `Current reachable members` messages to show up in the history component.\r\n\r\n[temporal-frontend-logs.txt](https://github.com/temporalio/temporal/files/10550373/temporal-frontend-logs.txt)\r\n[temporal-history-logs.txt](https://github.com/temporalio/temporal/files/10550374/temporal-history-logs.txt)\r\n[temporal-matching-logs.txt](https://github.com/temporalio/temporal/files/10550375/temporal-matching-logs.txt)\r\n[temporal-worker-logs.txt](https://github.com/temporalio/temporal/files/10550376/temporal-worker-logs.txt)\r\n","createdAt":"2023-01-31T20:01:56Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1410987415","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5UG9sV","author":{"login":"dnr"},"authorAssociation":"MEMBER","body":"I don't have any specific theories yet, just listing a few things I'm noticing in those logs:\r\n- The worker started about 4m after the others. Not sure if that was intentional.\r\n- The frontend says `unable to bootstrap ringpop` after 30s, and then succeeds 8s later. But not reliably: services seem to blink in and out of view after that.\r\n- All services are using port 7233 for their main grpc port. That's unusual but I don't immediately see why it would cause problems since of course the IPs are different. You could try giving them different ports like the standard configs.\r\n- They _are_ using distinct membership ports.\r\n- There are some extra entries in the cluster membership table (e.g. 172.17.0.16:6935,172.17.0.23:6933,172.17.0.20:6939). I'm assuming those are old? I.e. they were running within 20s of starting this set of services but are no longer running? That shouldn't be a problem as long as those are unreachable. If they _are_ still running, then you're pointing two temporal clusters at the same database, which won't work. Assuming they're not still running, can you check if trying to connect to those ports fails immediately or hangs for a long time? I would hope that ringpop can still bootstrap even if those connections just hang, as long as it can make others, but it might be something to look into.\r\n- Can you see if you can reproduce this if you wait at least 20s between all previous services being stopped and starting any new ones? ","createdAt":"2023-01-31T21:46:08Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1411111701","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5UKxqd","author":{"login":"Gibstick"},"authorAssociation":"NONE","body":"> worker started about 4m after the others\r\n\r\nMy bad, that's just the worker crash looping because the rest of the cluster is broken. But every run looks about the same.\r\n\r\n> All services are using port 7233\r\n\r\nThat was a bit of laziness from when I created this helm chart for deploying temporal on k8s. I've tried it again with unique ports but the issue still manifests in the same way.\r\n\r\n> extra entries in the cluster membership table \r\n\r\nThese are old, but this is a case I run into often because I'm running this locally and I frequently bring down all of my pods and bring them up quickly again for local development. However, those stale ones are unreachable with a connection refused error (I just poked those host:ports with netcat from a different pod).\r\n\r\n> wait 20s\r\n\r\n**WORKAROUND**: I can't reproduce this if I wait 20s. And it seems like in the rare cases where I don't wait 20s before restarting but things still work, all of the pods came back with the same IPs as before. So it seems like something changed about service discovery/ringpop that broke my use case.\r\n\r\nIs there anything I can tweak to get the old behaviour back? Or is this something that can easily be tweaked from your end in the service discovery code? As far as I can tell, that `cluster_membership` table is still being written to in the same way, so it seems like something changed about how it's being used.\r\n","createdAt":"2023-02-01T14:03:44Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1412111005","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5UnHiy","author":{"login":"roded"},"authorAssociation":"NONE","body":"I think that we're experiencing the same issue after upgrading to 1.19.1:\r\nhttps://temporalio.slack.com/archives/CTRCR8RBP/p1675433773709559","createdAt":"2023-02-06T18:15:34Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1419540658","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5UnQD_","author":{"login":"dnr"},"authorAssociation":"MEMBER","body":"> I think that we're experiencing the same issue after upgrading to 1.19.1: [temporalio.slack.com/archives/CTRCR8RBP/p1675433773709559](https://temporalio.slack.com/archives/CTRCR8RBP/p1675433773709559)\r\n\r\nInteresting. Can you reproduce the problem if you wait at least 20s between all services being stopped and starting them again?","createdAt":"2023-02-06T18:45:33Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1419575551","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5Un51r","author":{"login":"roded"},"authorAssociation":"NONE","body":"> Interesting. Can you reproduce the problem if you wait at least 20s between all services being stopped and starting them again?\r\n\r\nYeah, I did manage to reproduce the issue after killing the temporal process (I have just the one) and restarting it in the same container.\r\n\r\nEdit: I did wait 20 seconds before restarting.","createdAt":"2023-02-06T21:01:36Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1419746667","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5UuZVb","author":{"login":"yux0"},"authorAssociation":"MEMBER","body":"I tried to repro this issue by:\r\n\r\n1. Start four service roles, frontend, history matching, worker\r\n2. Kill all processes\r\n3. Start only frontend service role.\r\n\r\nThe issue I found is the startup timeout is too short. I added a fix https://github.com/temporalio/temporal/pull/3911. Please let me know if this fixes your issue.\r\n","createdAt":"2023-02-07T21:04:30Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1421448539","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5UupmM","author":{"login":"Gibstick"},"authorAssociation":"NONE","body":"Some more info requested from slack:\r\n> Just want to confirm, after you cleaned the cluster_membership table. The issue cannot be repro? was this issue repro on an upgrade path(1.18 -> 1.19). or it is repro on 1.19 with a brand new cluster?\r\n\r\nThe first time I saw this issue was from upgrading. But I've since gone back and reproduced it with a brand-new cluster in 1.19.1.\r\n\r\nMy earlier messages about cleaning the cluster_membership table are just me trying to work around the issue. Basically, if I restart my services enough and hit this issue where they are all stuck, I can bring them all down, clear `cluster_membership`, and then bring them up again, then the immediate problem will be solved and my cluster will come back again healthy. But it's not a permanent fix, because the issue will come back if I restart quickly again a few times.","createdAt":"2023-02-07T21:59:00Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1421515148","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5UvLln","author":{"login":"Gibstick"},"authorAssociation":"NONE","body":"I build the server image from https://github.com/temporalio/docker-builds and I tested it. The easiest way I have of reproducing it is just running `kubectl delete pod -l...` and using a label that selects all temporal pods. Since they are in a deployment, they come back pretty much right away.\r\n\r\nWith the fix from @yux0, I haven't been able to reproduce the issue. In the 20 times I tried killing the pods and checking the cluster, it came back up successfully every time (verified with `tctl wf list`). Thanks!","createdAt":"2023-02-07T23:41:17Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/3826#issuecomment-1421654375","viewerDidAuthor":false}],"createdAt":"2023-01-22T20:17:09Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":3826,"reactionGroups":[],"state":"CLOSED","title":"context deadline exceeded in worker log when starting a workflow after upgrading to 1.19.0","updatedAt":"2023-02-16T23:18:17Z","url":"https://github.com/temporalio/temporal/issues/3826"}
