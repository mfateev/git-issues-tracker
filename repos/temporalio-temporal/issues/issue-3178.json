{"assignees":[{"id":"MDQ6VXNlcjc3NTQxMjA=","login":"yycptt","name":"Yichao Yang","databaseId":0}],"author":{"id":"MDQ6VXNlcjg3NjI4OTM=","is_bot":false,"login":"wxing1292","name":"Wenquan Xing"},"body":"**Is your feature request related to a problem? Please describe.**\r\nHistory shard will override the context to ensure minimal timeout, however, this [checking / overriding logic](https://github.com/temporalio/temporal/blob/v1.17.2/service/history/shard/context_impl.go#L1967) does not prevent caller from cancelling the call, potentially causing shard instability (assert shard whenever see non shard ownership lost error)\r\n\r\nthanks @nagl-temporal for finding this potential issue","closedAt":"2022-08-12T21:50:08Z","comments":[{"id":"IC_kwDODNqesM5HyFm6","author":{"login":"nagl-temporal"},"authorAssociation":"CONTRIBUTOR","body":"To repro: Insert a short (I used 10ms) sleep in UpdateWorkflowExecution to increase the chances that context cancellation occurs during the persistence operation. start a reasonably chatty (parallelism=4) maru workload against temporal running locally. Ctrl-c your bench-go worker - you should see rangeID updates in the log from reliable-shard. You may have to do this a few times to get unlucky (or increase the sleep).\r\n\r\nIt's weird that a badly behaved client can trigger rangeID updates.","createdAt":"2022-08-03T18:13:08Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3178#issuecomment-1204312506","viewerDidAuthor":false}],"createdAt":"2022-08-03T17:51:26Z","labels":[{"id":"MDU6TGFiZWwxNjIxMDMwNzg3","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":3178,"reactionGroups":[],"state":"CLOSED","title":"History service: use a separate context when IO","updatedAt":"2022-08-12T21:50:08Z","url":"https://github.com/temporalio/temporal/issues/3178"}
