{"assignees":[{"id":"MDQ6VXNlcjQ5NTI1MzU=","login":"prathyushpv","name":"Prathyush PV","databaseId":0}],"author":{"id":"MDQ6VXNlcjM4MjU3MTY=","is_bot":false,"login":"mindaugasrukas","name":"Mind.R."},"body":"Yesterday I launched the single process for development: `temporal server start-dev`. Usually, I keep that running for a couple of days without any issues.\nBut today, I got this HTTP 503 response on the web UI:\n\n\n```\n{\"statusCode\":503,\"statusText\":\"Service Unavailable\",\"response\":{},\"message\":\"GetClusterMetadata operation failed. Error: SQL logic error: no such table: cluster_metadata_info (1)\"}\n```\n\nSo I had to restart the process.\nI'm still trying to figure out how to reproduce or if this is a real issue, so I'm leaving this here for a record in case that repeats or we can better understand the problem.\n\nSome log snippets:\n\n\n```\n{\"level\":\"error\",\"ts\":\"2023-01-06T10:01:33.589-0800\",\"msg\":\"Operation failed with internal error.\",\"error\":\"GetMetadata operation failed. Error: SQL logic error: no such table: namespace_metadata (1)\",\"metric-scope\":55,\"logging-call-at\":\"persistenceMetricClients.go:1579\",\"stacktrace\":\"go.temporal.io/server/common/log.(*zapLogger).Error\\n\\tgo.temporal.io/server@v1.18.5/common/log/zap_logger.go:143\\ngo.temporal.io/server/common/persistence.(*metricEmitter).updateErrorMetric\\n\\tgo.temporal.io/server@v1.18.5/common/persistence/persistenceMetricClients.go:1579\\ngo.temporal.io/server/common/persistence.(*metadataPersistenceClient).GetMetadata\\n\\tgo.temporal.io/server@v1.18.5/common/persistence/persistenceMetricClients.go:908\\ngo.temporal.io/server/common/persistence.(*metadataRetryablePersistenceClient).GetMetadata.func1\\n\\tgo.temporal.io/server@v1.18.5/common/persistence/persistenceRetryableClients.go:901\\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\\n\\tgo.temporal.io/server@v1.18.5/common/backoff/retry.go:194\\ngo.temporal.io/server/common/persistence.(*metadataRetryablePersistenceClient).GetMetadata\\n\\tgo.temporal.io/server@v1.18.5/common/persistence/persistenceRetryableClients.go:905\\ngo.temporal.io/server/common/namespace.(*registry).refreshNamespaces\\n\\tgo.temporal.io/server@v1.18.5/common/namespace/registry.go:426\\ngo.temporal.io/server/common/namespace.(*registry).refreshLoop\\n\\tgo.temporal.io/server@v1.18.5/common/namespace/registry.go:403\\ngo.temporal.io/server/internal/goro.(*Handle).Go.func1\\n\\tgo.temporal.io/server@v1.18.5/internal/goro/goro.go:64\"}\n```\n\n```\n{\"level\":\"error\",\"ts\":\"2023-01-06T10:01:33.589-0800\",\"msg\":\"Operation failed with internal error.\",\"error\":\"GetTaskQueue operation failed. Failed to check if task queue default-worker-tq of type Workflow existed. Error: SQL logic error: no such table: task_queues (1)\",\"metric-scope\":41,\"logging-call-at\":\"persistenceMetricClients.go:1579\",\"stacktrace\":\"go.temporal.io/server/common/log.(*zapLogger).Error\\n\\tgo.temporal.io/server@v1.18.5/common/log/zap_logger.go:143\\ngo.temporal.io/server/common/persistence.(*metricEmitter).updateErrorMetric\\n\\tgo.temporal.io/server@v1.18.5/common/persistence/persistenceMetricClients.go:1579\\ngo.temporal.io/server/common/persistence.(*taskPersistenceClient).GetTaskQueue\\n\\tgo.temporal.io/server@v1.18.5/common/persistence/persistenceMetricClients.go:724\\ngo.temporal.io/server/service/matching.(*taskQueueDB).takeOverTaskQueueLocked\\n\\tgo.temporal.io/server@v1.18.5/service/matching/db.go:123\\ngo.temporal.io/server/service/matching.(*taskQueueDB).RenewLease\\n\\tgo.temporal.io/server@v1.18.5/service/matching/db.go:109\\ngo.temporal.io/server/service/matching.(*taskWriter).renewLeaseWithRetry.func1\\n\\tgo.temporal.io/server@v1.18.5/service/matching/taskWriter.go:302\\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\\n\\tgo.temporal.io/server@v1.18.5/common/backoff/retry.go:194\\ngo.temporal.io/server/service/matching.(*taskWriter).renewLeaseWithRetry\\n\\tgo.temporal.io/server@v1.18.5/service/matching/taskWriter.go:306\\ngo.temporal.io/server/service/matching.(*taskWriter).initReadWriteState\\n\\tgo.temporal.io/server@v1.18.5/service/matching/taskWriter.go:131\\ngo.temporal.io/server/service/matching.(*taskWriter).taskWriterLoop\\n\\tgo.temporal.io/server@v1.18.5/service/matching/taskWriter.go:221\\ngo.temporal.io/server/internal/goro.(*Handle).Go.func1\\n\\tgo.temporal.io/server@v1.18.5/internal/goro/goro.go:64\"}\n```\n\n## Expected Behavior\nNo issues.\n\n## Actual Behavior\nA single process failed due to a missing DB table.\n\n## Steps to Reproduce the Problem\nUnknown. I was not able to construct reproducible steps.\n\nWhat I did initially:\n\n\n* `% temporal server start-dev`\n* do something for a day\n* next day, it fails\n\n## Specifications\n* Version:\n\n```\n% temporal -v              \ntemporal version 0.2.0 (server 1.18.5) (ui 2.9.0)\n```\n\n* Platform:\n\n```\n% uname -mrs   \nDarwin 21.6.0 arm64\n```\n\n","closedAt":"2024-12-05T18:55:54Z","comments":[{"id":"IC_kwDODNqesM5WPQrS","author":{"login":"mindaugasrukas"},"authorAssociation":"CONTRIBUTOR","body":"The same issues have been reported for version `temporal version 0.5.0 (server 1.20.0) (ui 2.10.3)`:\r\n\r\n<code>{\"level\":\"error\",\"ts\":\"2023-02-22T08:56:41.887-0800\",\"msg\":\"Operation failed with internal error.\",\"error\":\"ListNamespaces operation failed. Failed to get namespace rows. Error: SQL logic error: no such table: namespaces (1)\",\"operation\":\"ListNamespaces\",\"logging-call-at\":\"persistenceMetricClients.go:1171\",\"stacktrace\":\"go.temporal.io/server/common/log.(*zapLogger).Error\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/log/zap_logger.go:150\\ngo.temporal.io/server/common/persistence.updateErrorMetric\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/persistence/persistenceMetricClients.go:1171\\ngo.temporal.io/server/common/persistence.(*metricEmitter).recordRequestMetrics\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/persistence/persistenceMetricClients.go:1148\\ngo.temporal.io/server/common/persistence.(*metadataPersistenceClient).ListNamespaces.func1\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/persistence/persistenceMetricClients.go:683\\ngo.temporal.io/server/common/persistence.(*metadataPersistenceClient).ListNamespaces\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/persistence/persistenceMetricClients.go:685\\ngo.temporal.io/server/common/persistence.(*metadataRetryablePersistenceClient).ListNamespaces.func1\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/persistence/persistenceRetryableClients.go:887\\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/backoff/retry.go:199\\ngo.temporal.io/server/common/persistence.(*metadataRetryablePersistenceClient).ListNamespaces\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/persistence/persistenceRetryableClients.go:891\\ngo.temporal.io/server/common/namespace.(*registry).refreshNamespaces\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/namespace/registry.go:386\\ngo.temporal.io/server/common/namespace.(*registry).refreshLoop\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/namespace/registry.go:357\\ngo.temporal.io/server/internal/goro.(*Handle).Go.func1\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/internal/goro/goro.go:64\"}\r\n{\"level\":\"error\",\"ts\":\"2023-02-22T08:56:41.892-0800\",\"msg\":\"Operation failed with internal error.\",\"error\":\"GetTaskQueue operation failed. Failed to check if task queue default-worker-tq of type Workflow existed. Error: SQL logic error: no such table: task_queues (1)\",\"operation\":\"GetTaskQueue\",\"logging-call-at\":\"persistenceMetricClients.go:1171\",\"stacktrace\":\"go.temporal.io/server/common/log.(*zapLogger).Error\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/log/zap_logger.go:150\\ngo.temporal.io/server/common/persistence.updateErrorMetric\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/persistence/persistenceMetricClients.go:1171\\ngo.temporal.io/server/common/persistence.(*metricEmitter).recordRequestMetrics\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/persistence/persistenceMetricClients.go:1148\\ngo.temporal.io/server/common/persistence.(*taskPersistenceClient).GetTaskQueue.func1\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/persistence/persistenceMetricClients.go:567\\ngo.temporal.io/server/common/persistence.(*taskPersistenceClient).GetTaskQueue\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/persistence/persistenceMetricClients.go:569\\ngo.temporal.io/server/service/matching.(*taskQueueDB).takeOverTaskQueueLocked\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/service/matching/db.go:123\\ngo.temporal.io/server/service/matching.(*taskQueueDB).RenewLease\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/service/matching/db.go:109\\ngo.temporal.io/server/service/matching.(*taskWriter).renewLeaseWithRetry.func1\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/service/matching/taskWriter.go:302\\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/common/backoff/retry.go:199\\ngo.temporal.io/server/service/matching.(*taskWriter).renewLeaseWithRetry\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/service/matching/taskWriter.go:306\\ngo.temporal.io/server/service/matching.(*taskWriter).initReadWriteState\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/service/matching/taskWriter.go:131\\ngo.temporal.io/server/service/matching.(*taskWriter).taskWriterLoop\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/service/matching/taskWriter.go:221\\ngo.temporal.io/server/internal/goro.(*Handle).Go.func1\\n\\tgo.temporal.io/server@v1.18.1-0.20230207023301-52c3a9eefb06/internal/goro/goro.go:64\"}\r\n</code>","createdAt":"2023-02-27T18:25:47Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3784#issuecomment-1446841042","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5WPk95","author":{"login":"bergundy"},"authorAssociation":"MEMBER","body":"Posting some context from @yiminc:\r\n\r\nNote that if the last database connection in the pool closes, the in-memory database is deleted. Make sure the [max idle connection limit](https://golang.org/pkg/database/sql/#DB.SetMaxIdleConns) is > 0, and the [connection lifetime](https://golang.org/pkg/database/sql/#DB.SetConnMaxLifetime) is infinite.","createdAt":"2023-02-27T19:22:15Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3784#issuecomment-1446924153","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5YE1F4","author":{"login":"ThePlenkov"},"authorAssociation":"NONE","body":"I have a similar issue:\r\n```\r\nerror while fetching cluster metadata: operation GetClusterMetadata encountered table cluster_metadata_info does not exist\r\n```","createdAt":"2023-03-21T11:18:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3784#issuecomment-1477661048","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5hXW-s","author":{"login":"mindaugasrukas"},"authorAssociation":"CONTRIBUTOR","body":"Linking for visibility: https://github.com/temporalio/cli/issues/124","createdAt":"2023-07-13T04:05:17Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3784#issuecomment-1633513388","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6Eke6i","author":{"login":"mjameswh"},"authorAssociation":"MEMBER","body":"I've been observing multiple flakes of this error message in TS SDK's integration tests recently. To be exact, 11 times in the last 3 weeks, vs none before that (as far as I can see in GHA logs).\r\n\r\nIn the context of those CI jobs, it only happens with the CLI Dev Server started at the GHA job level (i.e. not with Dev Server instances started using the SDK's built-in `TestWorkflowEnvironment`), using CLI 0.12.0 and 0.13.2. Interestingly, 9 times out of 11, the \"error\" started at almost the same place during the tests, in \"Worker Lifecycle\" tests.\r\n\r\nI have modified the CI workflow to retain the server's logs on failure. Hopefully, I may be able to provide more data on this soon.","createdAt":"2024-07-12T00:13:51Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3784#issuecomment-2224156322","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6WRhBb","author":{"login":"prathyushpv"},"authorAssociation":"MEMBER","body":"We have pushed a fix to the SQLite driver: https://gitlab.com/cznic/sqlite/-/merge_requests/74.\r\nThe next temporal release(v1.26) will have this fix: https://github.com/temporalio/temporal/pull/6836","createdAt":"2024-12-05T18:55:36Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3784#issuecomment-2521174107","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6xbjg_","author":{"login":"eldondevat"},"authorAssociation":"NONE","body":"@prathyushpv We are still experiencing this regularly on 1.27. We deploy small testing environments that run temporal containers in docker-compose in developer mode (so using local sqlite databases). In these environments we regularly get spammed with log messages saying `Operation failed with internal error.\" error=\"ListNamespaces operation failed. Failed to get namespace rows. Error: SQL logic error: no such table: namespaces (1)`","createdAt":"2025-06-16T14:03:03Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3784#issuecomment-2976790591","viewerDidAuthor":false}],"createdAt":"2023-01-06T19:42:20Z","labels":[{"id":"MDU6TGFiZWwxNjIxMDMwNzg0","name":"bug","description":"Something isn't working","color":"d73a4a"}],"milestone":null,"number":3784,"reactionGroups":[],"state":"CLOSED","title":"SQLite failed due to missing DB table","updatedAt":"2025-06-16T14:03:03Z","url":"https://github.com/temporalio/temporal/issues/3784"}
