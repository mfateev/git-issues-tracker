{"assignees":[],"author":{"id":"MDQ6VXNlcjM4MzEzMzI3","is_bot":false,"login":"CameronWard301","name":""},"body":"### What are you really trying to do?\r\n\r\nI'm trying to benchmark different AWS RDS database types to choose the best one for my workflows based on cost and efficiency.\r\n\r\n### Describe the bug\r\n\r\nWhen running the example basic-spike test scenario, the first hundred or so are completed successfully but part way through the test the rest of the workflows fail. The worker produces the error: `Unable to complete Workflow context deadline exceeded`.\r\n\r\nI completed and ran the basic-test scenario with no issues. However, once receiving the \"workflow context deadline exceeded\" error after the spike test, temporal can no longer run any workflows. The only fix I have is to destroy the database and rebuild it again.\r\n\r\nThe worker produces the following error:\r\n`2022-07-21T16:22:51.873Z        ERROR   bench/driver_activity.go:44     failed to start workflow        {\"Namespace\": \"default\", \"TaskQueue\": \"temporal-bench\", \"WorkerID\": \"17118@ip-10-1-0-243@\", \"ActivityID\":\"18\", \"ActivityType\": \"bench-DriverActivity\", \"Attempt\": 1, \"WorkflowType\": \"bench-workflow\", \"WorkflowID\": \"3\", \"RunID\": \"832874bc-8eb1-4cde-b592-04f621767bc6\", \"Error\": \"context deadline exceeded\", \"ID\": \"basic-workflow-3-1-7-76\"}\r\n[github.com/temporalio/maru/bench.(*Activities).DriverActivity](http://github.com/temporalio/maru/bench.(*Activities).DriverActivity)\r\n        /home/ec2-user/maru/worker/bench/driver_activity.go:44\r\nreflect.Value.call\r\n        /usr/local/go/src/reflect/value.go:556\r\nreflect.Value.Call\r\n        /usr/local/go/src/reflect/value.go:339\r\n[go.temporal.io/sdk/internal.(*activityExecutor).Execute](http://go.temporal.io/sdk/internal.(*activityExecutor).Execute)\r\n        /home/ec2-user/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/internal_worker.go:777\r\n[go.temporal.io/sdk/internal.(*activityTaskHandlerImpl).Execute](http://go.temporal.io/sdk/internal.(*activityTaskHandlerImpl).Execute)\r\n        /home/ec2-user/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/internal_task_handlers.go:1816\r\n[go.temporal.io/sdk/internal.(*activityTaskPoller).ProcessTask](http://go.temporal.io/sdk/internal.(*activityTaskPoller).ProcessTask)\r\n        /home/ec2-user/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/internal_task_pollers.go:875\r\n[go.temporal.io/sdk/internal.(*baseWorker).processTask](http://go.temporal.io/sdk/internal.(*baseWorker).processTask)\r\n        /home/ec2-user/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/internal_worker_base.go:343`\r\n\r\nThe worker pod produces:\r\n`error starting temporal-sys-tq-scanner-workflow workflow\",\"service\":\"worker\",\"error\":\"context deadline exceeded`\r\n\r\nThe frontend pod log has this message:\r\n`history client encountered error\",\"service\":\"frontend\",\"error\":\"Not enough hosts to serve the request\"`\r\n\r\nThe history pod has these messages:\r\n`Operation failed with internal error.\",\"error\":\"GetWorkflowExecution: failed to get activity info. Error: Failed to get activity info. Error: context deadline exceeded\"`  \r\n`{\"level\":\"error\",\"ts\":\"2022-07-21T14:30:42.102Z\",\"msg\":\"Persistent fetch operation Failure\",\"shard-id\":110,\"address\":\"10.1.1.61:7234\",\"wf-namespace-id\":\"88830a6d-8ab6-4a0a-984c-e0065eb5b491\",\"wf-id\":\"basic-workflow-3-1-6-47\",\"wf-run-id\":\"dde13770-a4c5-45f3-af29-9317be21f78a\",\"store-operation\":\"get-wf-execution\",\"error\":\"context deadline exceeded\",\"logging-call-at\":\"transaction_impl.go:489\",\"stacktrace\":\"[go.temporal.io/server/common/log.(*zapLogger).Error](http://go.temporal.io/server/common/log.(*zapLogger).Error)\\n\\t/home/builder/temporal/common/log/zap_logger.go:142\\[ngo.temporal.io/server/service/history/workflow.getWorkflowExecutionWithRetry](http://ngo.temporal.io/server/service/history/workflow.getWorkflowExecutionWithRetry)\\n\\t/home/builder/temporal/service/history/workflow/transaction_impl.go:489\\[ngo.temporal.io/server/service/history/workflow.(*ContextImpl).LoadWorkflowExecution](http://ngo.temporal.io/server/service/history/workflow.(*ContextImpl).LoadWorkflowExecution)\\n\\t/home/builder/temporal/service/history/workflow/context.go:274\\[ngo.temporal.io/server/service/history.LoadMutableStateForTask](http://ngo.temporal.io/server/service/history.LoadMutableStateForTask)\\n\\t/home/builder/temporal/service/history/nDCTaskUtil.go:142\\[ngo.temporal.io/server/service/history.loadMutableStateForTransferTask](http://ngo.temporal.io/server/service/history.loadMutableStateForTransferTask)\\n\\t/home/builder/temporal/service/history/nDCTaskUtil.go:79\\[ngo.temporal.io/server/service/history.(*transferQueueActiveTaskExecutor).processWorkflowTask](http://ngo.temporal.io/server/service/history.(*transferQueueActiveTaskExecutor).processWorkflowTask)\\n\\t/home/builder/temporal/service/history/transferQueueActiveTaskExecutor.go:199\\[ngo.temporal.io/server/service/history.(*transferQueueActiveTaskExecutor).Execute](http://ngo.temporal.io/server/service/history.(*transferQueueActiveTaskExecutor).Execute)\\n\\t/home/builder/temporal/service/history/transferQueueActiveTaskExecutor.go:129\\[ngo.temporal.io/server/service/history/queues.(*executorWrapper).Execute](http://ngo.temporal.io/server/service/history/queues.(*executorWrapper).Execute)\\n\\t/home/builder/temporal/service/history/queues/executor_wrapper.go:67\\[ngo.temporal.io/server/service/history/queues.(*executableImpl).Execute](http://ngo.temporal.io/server/service/history/queues.(*executableImpl).Execute)\\n\\t/home/builder/temporal/service/history/queues/executable.go:161\\[ngo.temporal.io/server/common/tasks.(*ParallelProcessor).executeTask.func1](http://ngo.temporal.io/server/common/tasks.(*ParallelProcessor).executeTask.func1)\\n\\t/home/builder/temporal/common/tasks/parallel_processor.go:207\\[ngo.temporal.io/server/common/backoff.ThrottleRetry.func1](http://ngo.temporal.io/server/common/backoff.ThrottleRetry.func1)\\n\\t/home/builder/temporal/common/backoff/retry.go:166\\[ngo.temporal.io/server/common/backoff.ThrottleRetryContext](http://ngo.temporal.io/server/common/backoff.ThrottleRetryContext)\\n\\t/home/builder/temporal/common/backoff/retry.go:190\\[ngo.temporal.io/server/common/backoff.ThrottleRetry](http://ngo.temporal.io/server/common/backoff.ThrottleRetry)\\n\\t/home/builder/temporal/common/backoff/retry.go:167\\[ngo.temporal.io/server/common/tasks.(*ParallelProcessor).executeTask](http://ngo.temporal.io/server/common/tasks.(*ParallelProcessor).executeTask)\\n\\t/home/builder/temporal/common/tasks/parallel_processor.go:217\\[ngo.temporal.io/server/common/tasks.(*ParallelProcessor).processTask](http://ngo.temporal.io/server/common/tasks.(*ParallelProcessor).processTask)\\n\\t/home/builder/temporal/common/tasks/parallel_processor.go:195\"}\r\n{\"level\":\"error\",\"ts\":\"2022-07-21T14:30:42.102Z\",\"msg\":\"Fail to process task\",\"shard-id\":110,\"address\":\"10.1.1.61:7234\",\"component\":\"transfer-queue-processor\",\"cluster-name\":\"active\",\"wf-namespace-id\":\"88830a6d-8ab6-4a0a-984c-e0065eb5b491\",\"wf-id\":\"basic-workflow-3-1-6-47\",\"wf-run-id\":\"dde13770-a4c5-45f3-af29-9317be21f78a\",\"queue-task-id\":1048588,\"queue-task-visibility-timestamp\":\"2022-07-21T11:38:25.376Z\",\"queue-task-type\":\"TransferWorkflowTask\",\"queue-task\":{\"NamespaceID\":\"88830a6d-8ab6-4a0a-984c-e0065eb5b491\",\"WorkflowID\":\"basic-workflow-3-1-6-47\",\"RunID\":\"dde13770-a4c5-45f3-af29-9317be21f78a\",\"VisibilityTimestamp\":\"2022-07-21T11:38:25.376757699Z\",\"TaskID\":1048588,\"TaskQueue\":\"temporal-basic\",\"ScheduledEventID\":2,\"Version\":0},\"wf-history-event-id\":2,\"error\":\"context deadline exceeded\",\"lifecycle\":\"ProcessingFailed\",\"logging-call-at\":\"lazy_logger.go:68\",\"stacktrace\":\"[go.temporal.io/server/common/log.(*zapLogger).Error](http://go.temporal.io/server/common/log.(*zapLogger).Error)\\n\\t/home/builder/temporal/common/log/zap_logger.go:142\\[ngo.temporal.io/server/common/log.(*lazyLogger).Error](http://ngo.temporal.io/server/common/log.(*lazyLogger).Error)\\n\\t/home/builder/temporal/common/log/lazy_logger.go:68\\[ngo.temporal.io/server/service/history/queues.(*executableImpl).HandleErr](http://ngo.temporal.io/server/service/history/queues.(*executableImpl).HandleErr)\\n\\t/home/builder/temporal/service/history/queues/executable.go:231\\[ngo.temporal.io/server/common/tasks.(*ParallelProcessor).executeTask.func1](http://ngo.temporal.io/server/common/tasks.(*ParallelProcessor).executeTask.func1)\\n\\t/home/builder/temporal/common/tasks/parallel_processor.go:208\\[ngo.temporal.io/server/common/backoff.ThrottleRetry.func1](http://ngo.temporal.io/server/common/backoff.ThrottleRetry.func1)\\n\\t/home/builder/temporal/common/backoff/retry.go:166\\[ngo.temporal.io/server/common/backoff.ThrottleRetryContext](http://ngo.temporal.io/server/common/backoff.ThrottleRetryContext)\\n\\t/home/builder/temporal/common/backoff/retry.go:190\\[ngo.temporal.io/server/common/backoff.ThrottleRetry](http://ngo.temporal.io/server/common/backoff.ThrottleRetry)\\n\\t/home/builder/temporal/common/backoff/retry.go:167\\[ngo.temporal.io/server/common/tasks.(*ParallelProcessor).executeTask](http://ngo.temporal.io/server/common/tasks.(*ParallelProcessor).executeTask)\\n\\t/home/builder/temporal/common/tasks/parallel_processor.go:217\\[ngo.temporal.io/server/common/tasks.(*ParallelProcessor).processTask](http://ngo.temporal.io/server/common/tasks.(*ParallelProcessor).processTask)\\n\\t/home/builder/temporal/common/tasks/parallel_processor.go:195\"}`\r\n\r\nThe RDS log is here:\r\n`2022-07-21 16:07:51 UTC::@:[9698]:LOG: received SIGHUP, reloading configuration files\r\n2022-07-21 16:07:51 UTC::@:[9698]:LOG: skipping missing configuration file \"/rdsdbdata/config/recovery.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9698]:LOG: skipping missing configuration file \"/rdsdbdata/db/postgresql.auto.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9698]:LOG: parameter \"unix_socket_permissions\" cannot be changed without restarting the server\r\n2022-07-21 16:07:51 UTC::@:[9698]:LOG: configuration file \"/rdsdbdata/config/postgresql.conf\" contains errors; unaffected changes were applied\r\n2022-07-21 16:07:51 UTC::@:[9708]:LOG: skipping missing configuration file \"/rdsdbdata/config/recovery.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9708]:LOG: skipping missing configuration file \"/rdsdbdata/db/postgresql.auto.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9754]:LOG: skipping missing configuration file \"/rdsdbdata/config/recovery.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9754]:LOG: skipping missing configuration file \"/rdsdbdata/db/postgresql.auto.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9759]:LOG: skipping missing configuration file \"/rdsdbdata/config/recovery.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9759]:LOG: skipping missing configuration file \"/rdsdbdata/db/postgresql.auto.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9756]:LOG: skipping missing configuration file \"/rdsdbdata/config/recovery.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9756]:LOG: skipping missing configuration file \"/rdsdbdata/db/postgresql.auto.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9755]:LOG: skipping missing configuration file \"/rdsdbdata/config/recovery.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9755]:LOG: skipping missing configuration file \"/rdsdbdata/db/postgresql.auto.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9758]:LOG: skipping missing configuration file \"/rdsdbdata/config/recovery.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9758]:LOG: skipping missing configuration file \"/rdsdbdata/db/postgresql.auto.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9699]:LOG: skipping missing configuration file \"/rdsdbdata/config/recovery.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9699]:LOG: skipping missing configuration file \"/rdsdbdata/db/postgresql.auto.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9757]:LOG: skipping missing configuration file \"/rdsdbdata/config/recovery.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9757]:LOG: skipping missing configuration file \"/rdsdbdata/db/postgresql.auto.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9760]:LOG: skipping missing configuration file \"/rdsdbdata/config/recovery.conf\"\r\n2022-07-21 16:07:51 UTC::@:[9760]:LOG: skipping missing configuration file \"/rdsdbdata/db/postgresql.auto.conf\"\r\n2022-07-21 16:08:32 UTC:postgres@temporal:[1842]:ERROR: duplicate key value violates unique constraint \"namespaces_pkey\"\r\n2022-07-21 16:08:32 UTC:postgres@temporal:[1842]:DETAIL: Key (partition_id, id)=(54321, \\x32049b68787240948e63d0dd59896a83) already exists.\r\n2022-07-21 16:08:32 UTC:postgres@temporal:[1842]:STATEMENT: INSERT INTO\r\nnamespaces (partition_id, id, name, is_global, data, data_encoding, notification_version)\r\nVALUES($1, $2, $3, $4, $5, $6, $7)\r\n2022-07-21 16:08:53 UTC:postgres@temporal:[2929]:ERROR: duplicate key value violates unique constraint \"namespaces_pkey\"\r\n2022-07-21 16:08:53 UTC:postgres@temporal:[2929]:DETAIL: Key (partition_id, id)=(54321, \\x32049b68787240948e63d0dd59896a83) already exists.\r\n2022-07-21 16:08:53 UTC:postgres@temporal:[2929]:STATEMENT: INSERT INTO\r\nnamespaces (partition_id, id, name, is_global, data, data_encoding, notification_version)\r\nVALUES($1, $2, $3, $4, $5, $6, $7)\r\n2022-07-21 16:09:24 UTC:postgres@temporal:[4183]:ERROR: duplicate key value violates unique constraint \"namespaces_pkey\"\r\n2022-07-21 16:09:24 UTC:postgres@temporal:[4183]:DETAIL: Key (partition_id, id)=(54321, \\x32049b68787240948e63d0dd59896a83) already exists.\r\n2022-07-21 16:09:24 UTC:postgres@temporal:[4183]:STATEMENT: INSERT INTO\r\nnamespaces (partition_id, id, name, is_global, data, data_encoding, notification_version)\r\nVALUES($1, $2, $3, $4, $5, $6, $7)\r\n2022-07-21 16:22:13 UTC::[unknown]@[unknown]:[5084]:LOG: PID 4778 in cancel request did not match any process\r\n2022-07-21 16:30:44 UTC:[unknown]@[unknown]:[26778]:LOG: PID 26721 in cancel request did not match any process\r\n2022-07-21 16:32:14 UTC:postgres@temporal:[4598]:LOG: unexpected EOF on client connection with an open transaction\r\n2022-07-21 16:32:14 UTC:postgres@temporal:[26725]:LOG: unexpected EOF on client connection with an open transaction`\r\n\r\nNote the incident ran at 2022-07-21T16:22:51\r\n\r\n### Minimal Reproduction\r\n\r\nIn an EC2 instance, run the maru worker. In another session start the spike scenario with default values for the workflow type and count: `tctl wf start --tq temporal-bench --wt bench-workflow --wtt 5 --et 1800 --if ./scenarios/basic-test.json --wid 1`\r\n\r\n### Environment/Versions\r\n\r\n- Temporal Version: 1.17.1\r\n- Using AWS private EKS cluster (version 1.22) to deploy Temporal using the helm chart: `helm install -f values/values.postgresql.yaml temporal --set elasticsearch.enabled=false . --timeout 900s`  \r\nThe cluster has 5 t3.large nodes and can scale to 10\r\n- RDS Postgres version 13.4\r\n\r\nHappy to provide any more information if needed :)\r\n","closedAt":"2022-07-26T14:05:33Z","comments":[{"id":"IC_kwDODNqesM5HEnxi","author":{"login":"robholland"},"authorAssociation":"MEMBER","body":"Which RDS instance type are you using?","createdAt":"2022-07-22T09:51:31Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3131#issuecomment-1192393826","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5HEqBc","author":{"login":"robholland"},"authorAssociation":"MEMBER","body":"The RDS log shows no sign of corruption, the namespace insertions are before the spike. I will try and replicate this.","createdAt":"2022-07-22T10:01:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/3131#issuecomment-1192403036","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5HEq09","author":{"login":"CameronWard301"},"authorAssociation":"NONE","body":"@robholland Instance class is: db.r6g.large","createdAt":"2022-07-22T10:04:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3131#issuecomment-1192406333","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5HFn-P","author":{"login":"robholland"},"authorAssociation":"MEMBER","body":"As discussed on Slack I don't believe this is a database corruption issue but a problem with Kubernetes setup. @CameronWard301 is testing a new cluster configuration.","createdAt":"2022-07-22T14:54:15Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3131#issuecomment-1192656783","viewerDidAuthor":false},{"id":"IC_kwDODNqesM5HQllY","author":{"login":"CameronWard301"},"authorAssociation":"NONE","body":"After removing the EKS Fargate profile, CNI plugin and changing the node capacity type to 'on-demand', the issue is now resolved! I also had to adjust the security profile of the node group to allow all traffic between them.  Thanks, @robholland for all the help. Before, Fargate seemed to be tainting the pods and putting them all onto one node which was running out of resources and my nodes could not talk to each other correctly.","createdAt":"2022-07-26T14:05:33Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/3131#issuecomment-1195530584","viewerDidAuthor":false}],"createdAt":"2022-07-22T09:09:06Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":3131,"reactionGroups":[],"state":"CLOSED","title":"[Bug] Maru Spike Test Corrupts AWS RDS Postgresql DB","updatedAt":"2022-07-26T14:08:30Z","url":"https://github.com/temporalio/temporal/issues/3131"}
