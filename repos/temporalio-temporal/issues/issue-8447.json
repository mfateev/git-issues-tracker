{"assignees":[],"author":{"id":"MDQ6VXNlcjU4OTIwNjE=","is_bot":false,"login":"jtamagnan","name":"Jules Tamagnan"},"body":"## Problem\n\nI've gone ahead and enabled the much anticipated (for me) poller autoscaling functionality but have not experienced as impressive scale downs as I would have hoped. It seems like it is much easier to scale up pollers than scale them down. My understanding is that\n\nScale down occurs when:\n - Any poller has waited idle for at least `PollerScalingWaitTime` (1s default) without matching a task\n\nScale up occurs when:\n - There's a non-zero backlog and the approximate backlog age exceeds `PollerScalingBacklogAgeScaleUp` (200ms default)\n\nThis seems to mean that N pollers would never scale down if there were at least N events per second. This feels like a bottleneck because presumably a single poller can handle more than 1 request per second. We are stuck at a higher number of pollers when a lower number could suffice.\n\n## Potential solutions:\n\nThere are a few potential solutions to better scaling down the number of pollers.\n1. Always give tasks to the mostly recently connected poller instead of using a [fifo queue](https://github.com/temporalio/temporal/blob/main/service/matching/matcher_data.go#L44) for the pollers. That would ensure that old pollers would be more likely to time out appropriately. This would ensure that we always have the lowest number of possible pollers.\n2. Scale down if the backlog is less than the `PollerScalingBacklogAgeScaleUp`. This feels a bit more finicky in that it could lead to the number of pollers bouncing up and down.\n\n## Additional Context\n\nI've noticed strange performance characteristics when there are too many pollers on a single task queue / partition. Instead of actively lowering the number of pollers I'd like for this to be handled more automatically to reduce the maintenance and tuning burden.\n\nI'd be happy to help contribute a fix if we agree that this scale down could be better optimized\n\n---\n\nSolution 1, which would ensure that the oldest pollers languish the longest and eventually get scaled down seems pretty simple though I'm not sure what other issues might come up from this.\n<details>\n<summary> DIff </summary>\n```diff\ndiff --git a/service/matching/matcher_data.go b/service/matching/matcher_data.go\nindex 77fca6197..e64026b58 100644\n--- a/service/matching/matcher_data.go\n+++ b/service/matching/matcher_data.go\n@@ -41,7 +41,7 @@ func (p *pollerPQ) Less(i int, j int) bool {\n \t} else if (a.isTaskForwarder || a.isTaskValidator) && !(b.isTaskForwarder || b.isTaskValidator) {\n \t\treturn false\n \t}\n-\treturn a.startTime.Before(b.startTime)\n+\treturn a.startTime.After(b.startTime)\n }\n \n func (p *pollerPQ) Add(poller *waitingPoller) {\n```\n</details>","closedAt":"2025-11-13T23:26:53Z","comments":[{"id":"IC_kwDODNqesM7KOEv_","author":{"login":"ShahabT"},"authorAssociation":"MEMBER","body":"@jtamagnan thanks for the feedback and the proposed solutions. Unfortunately, the proposed solutions are not ideal either IMHO:\n\n1. Could lead to hot workers because the workers that came first could remain idle while all the work is done by the workers who came later.\n2. Reasons you mentioned.\n\nSome users want to have have enough headroom in poller count to avoid backlog (and latency hit) when spike happens in traffic.\n\nIf you do not need the headroom, maybe try reducing the dynamic config from 1s to like 200ms or so? Let me know how that works for you. We can definitely improve this feature.","createdAt":"2025-10-11T00:48:46Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/8447#issuecomment-3392687103","viewerDidAuthor":false},{"id":"IC_kwDODNqesM7KkjC6","author":{"login":"jtamagnan"},"authorAssociation":"CONTRIBUTOR","body":"> @jtamagnan thanks for the feedback and the proposed solutions. Unfortunately, the proposed solutions are not ideal either IMHO:\n\nThank you for reading through the post, the suggestions and the feedback! I wonder if there are changes or behaviors that we can apply across the board that are more likely to make everyone happy, or if more customization would help. The default behavior of scaling pollers down only if there is low throughput feels limiting when a single poller can typically handle more than 1 task per second. \n\n> Could lead to hot workers because the workers that came first could remain idle while all the work is done by the workers who came later.\n\nYeah that's definitely a draw back. Would it be possible to mitigate this by having folks running into this issue do additional slot tunin using a ResourceBasedTuner?\n\nIts definitely interesting that there are so many dimensions across which to tune.\n\n> If you do not need the headroom, maybe try reducing the dynamic config from 1s to like 200ms or so? Let me know how that works for you. We can definitely improve this feature.\n\nI've tried to lower the value to 500ms which implies that N Pollers will only start to scale down if there are less than 2N tasks per second. I'll bring that down to 200ms to see if that helps.\n\nI can probably get a better idea of the correct values by looking at the actual task throughput. Or maybe some combination of poller reconnection latency and throughput. In my current mental model the number of required pollers is based on connection latency and task throughput.\n\nIn a way I was hoping that autoscaling pollers would free me from the need to do as much tuning.","createdAt":"2025-10-13T18:14:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/8447#issuecomment-3398578362","viewerDidAuthor":false},{"id":"IC_kwDODNqesM7LK72Y","author":{"login":"ShahabT"},"authorAssociation":"MEMBER","body":"Pollers are cheap from worker side and as long as you are not hitting the server max concurrent polls limit (configurable on the server) you don't need to put a ton of effort minimizing the number of poller IMHO. Usually, the problem is that users don't have enough pollers.","createdAt":"2025-10-15T23:36:58Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/8447#issuecomment-3408641432","viewerDidAuthor":false},{"id":"IC_kwDODNqesM7Safz7","author":{"login":"gow"},"authorAssociation":"MEMBER","body":"@jtamagnan going to close this task for now. Please reopen if needed.","createdAt":"2025-11-13T23:26:53Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/8447#issuecomment-3530161403","viewerDidAuthor":false}],"createdAt":"2025-10-07T22:48:06Z","labels":[{"id":"MDU6TGFiZWwxNjIxMDMwNzg3","name":"enhancement","description":"New feature or request","color":"a2eeef"}],"milestone":null,"number":8447,"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"state":"CLOSED","title":"Poller autoscaling scale down feels rigid","updatedAt":"2025-11-13T23:26:53Z","url":"https://github.com/temporalio/temporal/issues/8447"}
