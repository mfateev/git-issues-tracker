{"assignees":[],"author":{"id":"MDQ6VXNlcjE5MTIwMjg=","is_bot":false,"login":"spacether","name":"Justin Black"},"body":"## Expected Behavior\r\nI am invoking workflow.continue_as_new in a SignalReceiverWorkflow and I expect it to create a new instance of itself and shutdown the original instance.\r\n\r\n## Actual Behavior\r\nLogging shows that no new instance is created, `__init__` + run methods are not invoked on a new instance after workflow.continue_as_new s called.\r\n\r\n\r\n## Steps to Reproduce the Problem\r\n\r\nIn a virtual env with Python 3.11.9\r\n```\r\npip install temporalio==1.8.0\r\npip install pydantic==1.10.19\r\n```\r\nturn on the temporal dev server: `temporal server start-dev`\r\nSave a file named `continue_as_new.py` with this code:\r\n```\r\nimport asyncio\r\nimport collections\r\nimport datetime\r\nimport dataclasses\r\nimport enum\r\nimport logging\r\nimport typing\r\nimport uuid\r\nimport re\r\nimport json\r\n\r\nimport pydantic\r\n\r\nfrom temporalio import workflow, activity, worker, client, converter\r\nfrom temporalio.worker import workflow_sandbox\r\nfrom temporalio.api.common import v1\r\nfrom pydantic.json import pydantic_encoder\r\n\r\nlogger = logging.getLogger()\r\nlogging.basicConfig(level=logging.INFO)\r\nlogger.setLevel(logging.INFO)\r\n\r\n\r\nclass PydanticJSONPayloadConverter(converter.JSONPlainPayloadConverter):\r\n    \"\"\"Pydantic JSON payload converter.\r\n\r\n    This extends the :py:class:`JSONPlainPayloadConverter` to override\r\n    :py:meth:`to_payload` using the Pydantic encoder.\r\n    \"\"\"\r\n\r\n    def to_payload(self, value: typing.Any) -> typing.Optional[v1.Payload]:\r\n        \"\"\"Convert all values with Pydantic encoder or fail.\r\n\r\n        Like the base class, we fail if we cannot convert. This payload\r\n        converter is expected to be the last in the chain, so it can fail if\r\n        unable to convert.\r\n        \"\"\"\r\n        cls_name = f\"{value.__class__.__module__}.{value.__class__.__name__}\"\r\n        if cls_name.startswith(\"__temporal_main__.MachineStatusUpdateEvent\"):\r\n            return None\r\n        # We let JSON conversion errors be thrown to caller\r\n        return v1.Payload(\r\n            metadata={\"encoding\": self.encoding.encode()},\r\n            data=json.dumps(\r\n                value, separators=(\",\", \":\"), sort_keys=True, default=pydantic_encoder\r\n            ).encode(),\r\n        )\r\n\r\n\r\ndef pascal_to_snake(pascal_name: str) -> str:\r\n    \"\"\"Converts a Pascal case string to snake case.\"\"\"\r\n    return re.sub(r\"(?<!^)([A-Z])\", r\"_\\1\", pascal_name).lower()\r\n\r\n\r\ndef str_to_class(full_class_name: str) -> typing.Type:\r\n    if (\r\n        full_class_name\r\n        == \"machine_status_update_event_data.MachineStatusUpdateEventData\"\r\n    ):\r\n        return MachineStatusUpdateEventData\r\n\r\n\r\ndef __pascal_to_camel(name: str) -> str:\r\n    \"\"\"Converts a PascalCase string to camelCase.\"\"\"\r\n    if not name:\r\n        return \"\"\r\n    return name[0].lower() + name[1:]\r\n\r\n\r\ndef __remove_none_values(d: dict) -> dict:\r\n    return {k: v for k, v in d.items() if v is not None}\r\n\r\n\r\nclass MachineStatusUpdateEvent:\r\n    def __init__(self, data):\r\n        self.payloadMetadataType = \"LineContext\"\r\n        self.payloadDataType = \"MachineStatusUpdateEventData\"\r\n        self.schemaFormat = \"URL\"\r\n        self.schema = \"http://bm/all-asyncapi.yml\"\r\n        self.schemaVersion = \"0.213.0\"\r\n        self.payload = Payload(data)\r\n\r\n\r\ndef serialialize(\r\n    event: typing.Union[MachineStatusUpdateEvent,]\r\n) -> typing.Dict[str, typing.Any]:\r\n    serialized_object = dict(event.__dict__)\r\n    serialized_object[\"payload\"] = {\r\n        \"data\": {\r\n            __pascal_to_camel(event.payloadDataType): __remove_none_values(\r\n                event.payload.data.__dict__\r\n            )\r\n        },\r\n    }\r\n    return serialized_object\r\n\r\n\r\nclass ModelinaPayloadConverter(converter.EncodingPayloadConverter):\r\n    \"\"\"\r\n    A converter for old-style\r\n    Modelina classes\r\n    \"\"\"\r\n\r\n    @property\r\n    def encoding(self) -> str:\r\n        return \"json/modelina\"\r\n\r\n    def to_payload(self, value: typing.Any) -> typing.Optional[v1.Payload]:\r\n        cls_name = f\"{value.__class__.__module__}.{value.__class__.__name__}\"\r\n        if cls_name.startswith(\"__temporal_main__.MachineStatusUpdateEvent\"):\r\n            return v1.Payload(\r\n                metadata={\"encoding\": self.encoding.encode()},\r\n                data=json.dumps(serialialize(value)).encode(),\r\n            )\r\n        else:\r\n            return None\r\n\r\n    def from_payload(\r\n        self, payload: v1.Payload, type_hint: typing.Optional[typing.Type] = None\r\n    ) -> typing.Any:\r\n        if type_hint not in {\r\n            MachineStatusUpdateEvent,\r\n        }:\r\n            raise RuntimeError(\"Unsupported payload type\")\r\n        data_str = payload.data.decode()\r\n        data_dict = json.loads(data_str)\r\n        # data class deserialization\r\n        payload_data_key = list(data_dict[\"payload\"][\"data\"].keys())[0]\r\n        payload_data_dict = data_dict[\"payload\"][\"data\"][payload_data_key]\r\n        # covert dict to actual class instance\r\n        payload_data_type = data_dict[\"payloadDataType\"]\r\n        # sometimes the first character is lowercase when it should be uppercase for class name logic below\r\n        payload_data_type = payload_data_type[0].upper() + payload_data_type[1:]\r\n        full_class_name = f\"{pascal_to_snake(payload_data_type)}.{payload_data_type}\"\r\n        # MachineStatusUpdateEventData -> machine_status_update_event_data.MachineStatusUpdateEventData\r\n        data_cls = str_to_class(full_class_name)\r\n        if data_cls not in {\r\n            MachineStatusUpdateEventData,\r\n        }:\r\n            raise RuntimeError(\"Unsupported payload.payload type\")\r\n        data_cls_inst = data_cls()\r\n        for key, value in payload_data_dict.items():\r\n            setattr(data_cls_inst, key, value)\r\n        event = type_hint(data_cls_inst)\r\n        return event\r\n\r\n\r\nclass CustomPayloadConverter(converter.CompositePayloadConverter):\r\n    \"\"\"Payload converter that replaces Temporal JSON conversion with Pydantic\r\n    JSON conversion.\r\n    \"\"\"\r\n\r\n    def __init__(self) -> None:\r\n        converters = [\r\n            ModelinaPayloadConverter(),\r\n            PydanticJSONPayloadConverter(),\r\n        ]\r\n        super().__init__(*converters)\r\n\r\n\r\ndata_converter = converter.DataConverter(payload_converter_class=CustomPayloadConverter)\r\n\r\n\r\nclass MachineStatus(enum.StrEnum):\r\n    READY = \"Ready\"\r\n    BUSY = \"Busy\"\r\n    RUNNING = \"Running\"\r\n    ACTION_REQUIRED = \"ActionRequired\"\r\n    STARVING = \"Starving\"\r\n    ERROR = \"Error\"\r\n    SAFETY = \"Safety\"\r\n    INIT_REQUIRED = \"InitRequired\"\r\n\r\n\r\nclass MachineStatusEventData(pydantic.BaseModel):\r\n    status: MachineStatus\r\n    orchestrator_status: str\r\n    delta_time_seconds: typing.Union[int, float]\r\n    required_action: typing.List[str]\r\n    prev_status: typing.Optional[str] = None\r\n\r\n\r\nclass MachineStatusUpdateEventData:\r\n    def __init__(self):\r\n        self.status = None\r\n        self.prevStatus = None\r\n        self.orchestratorStatus = None\r\n        self.deltaTimeSeconds = None\r\n        self.requiredAction = None\r\n\r\n\r\nclass Payload:\r\n    def __init__(self, data):\r\n        self.metadata = None\r\n        self.data = data\r\n        self.responseInfo = None\r\n\r\n\r\ndef get_machine_status_update_event(\r\n    machine_status_event_data: MachineStatusEventData,\r\n) -> MachineStatusUpdateEvent:\r\n    data = MachineStatusUpdateEventData()\r\n    data.status = machine_status_event_data.status.value\r\n    data.orchestratorStatus = machine_status_event_data.orchestrator_status\r\n    data.deltaTimeSeconds = machine_status_event_data.delta_time_seconds\r\n    data.requiredAction = machine_status_event_data.required_action\r\n    if machine_status_event_data.prev_status:\r\n        data.prevStatus = machine_status_event_data.prev_status\r\n\r\n    event = MachineStatusUpdateEvent(data=data)\r\n    return event\r\n\r\n\r\nclass Activities:\r\n    @activity.defn\r\n    async def run_activity(self, event: MachineStatusUpdateEvent) -> bool:\r\n        logger.info(f\"Running activity payloadDataType={event.payloadDataType}\")\r\n        return False\r\n\r\n\r\nT = typing.TypeVar(\"T\")\r\n\r\n\r\n@dataclasses.dataclass\r\nclass BatchEventsData(typing.Generic[T]):\r\n    batch_events: typing.List[T]\r\n    queue: collections.deque[T]\r\n    activity_method: typing.Callable\r\n\r\n\r\n@workflow.defn\r\nclass SignalReceiverWorkflow:\r\n    def __init__(\r\n        self, sleep_duration_secs: int = 5, batch_events_qty: int = 10\r\n    ) -> None:\r\n        logger.info(f\"SignalReceiverWorkflow __init__\")\r\n        self.iteration = 0\r\n        self.events = collections.deque()\r\n        self.sleep_duration_secs = sleep_duration_secs\r\n        self.handled_events_qty_for_restart = 50\r\n        self.batch_events_qty = batch_events_qty\r\n        self.start_to_close_timeout = datetime.timedelta(seconds=4)\r\n\r\n    def get_batch_events_data(\r\n        self, event_queue: collections.deque[T], activity_method: typing.Callable\r\n    ) -> BatchEventsData[T]:\r\n        batch_events = []\r\n        for _ in range(self.batch_events_qty):\r\n            while event_queue:\r\n                batch_events.append(event_queue.popleft())\r\n        return BatchEventsData(\r\n            batch_events=batch_events,\r\n            queue=event_queue,\r\n            activity_method=activity_method,\r\n        )\r\n\r\n    async def send_batch_events(self, batch_events_data: BatchEventsData[T]):\r\n        for event in batch_events_data.batch_events:\r\n            logger.info(\"sending batch event activity_method\")\r\n            request_succeeded = await workflow.execute_activity_method(\r\n                batch_events_data.activity_method,\r\n                event,\r\n                start_to_close_timeout=self.start_to_close_timeout,\r\n            )\r\n            if not request_succeeded:\r\n                batch_events_data.queue.append(event)\r\n\r\n    async def send_pending_events(self):\r\n        if self.events:\r\n            batch_events_data = self.get_batch_events_data(\r\n                self.events,\r\n                Activities.run_activity,\r\n            )\r\n            await self.send_batch_events(batch_events_data)\r\n\r\n    @workflow.run\r\n    async def run(\r\n        self,\r\n        iteration: int,\r\n        events: typing.Optional[collections.deque[MachineStatusUpdateEvent]] = None,\r\n    ):\r\n        self.iteration = iteration\r\n        if events:\r\n            self.events = events\r\n        else:\r\n            self.events = collections.deque()\r\n        logger.info(\r\n            f\"running SignalReceiverWorkflow iteration={self.iteration}, events={len(self.events)}\"\r\n        )\r\n        while True:\r\n            await asyncio.sleep(self.sleep_duration_secs)\r\n            if self.events:\r\n                logger.info(f\"sending pending events, ...\")\r\n                await self.send_pending_events()\r\n\r\n            actual_events = workflow.info().get_current_history_length()\r\n            logger.info(\r\n                f\"checking to see if workflow should be continued, actual_events={actual_events}\"\r\n            )\r\n            if actual_events >= self.handled_events_qty_for_restart:\r\n                break\r\n        logger.info(\"invoking continue_as_new ***\")\r\n        # max input size is 2 Mb\r\n        # https://docs.temporal.io/cloud/limits#:~:text=Blob%20size%20limit%20for%20Payloads,History%20transaction%20is%204%20MB.\r\n        workflow.continue_as_new(\r\n            args=(self.iteration + 1, self.events),\r\n        )\r\n\r\n    @workflow.signal\r\n    async def invoke_run_activity(\r\n        self,\r\n        machine_status_data: MachineStatusEventData,\r\n    ):\r\n        event = get_machine_status_update_event(\r\n            machine_status_data\r\n        )\r\n        result = await workflow.execute_activity_method(\r\n            Activities.run_activity,\r\n            event,\r\n            start_to_close_timeout=self.start_to_close_timeout,\r\n        )\r\n        if not result:\r\n            self.events.append(event)\r\n\r\n\r\nequipment_id = \"BRC_ID_XXX\"\r\nline_id = uuid.UUID(\"3ca3607e-b0db-478d-817a-54cedfb7d145\")\r\n\r\n\r\n@workflow.defn\r\nclass SignalSenderWorkflow:\r\n\r\n    @workflow.run\r\n    async def run(self, signal_receiver_workflow_id: str) -> str:\r\n        logger.info(\"running SignalSenderWorkflow\")\r\n\r\n        for _ in range(65):\r\n            workflow_handle = workflow.get_external_workflow_handle_for(\r\n                SignalReceiverWorkflow.run, signal_receiver_workflow_id\r\n            )\r\n            machine_status_event_data = MachineStatusEventData(\r\n                status=MachineStatus.READY,\r\n                orchestrator_status=\"\",\r\n                delta_time_seconds=1,\r\n                required_action=[],\r\n            )\r\n\r\n            logger.info(\"sending invoke_run_activity\")\r\n            await workflow_handle.signal(\r\n                SignalReceiverWorkflow.invoke_run_activity,\r\n                args=(machine_status_event_data,),\r\n            )\r\n            await asyncio.sleep(1)\r\n        return \"SignalSenderWorkflow ran\"\r\n\r\n\r\ndef __new_sandbox_runner() -> workflow_sandbox.SandboxedWorkflowRunner:\r\n    \"\"\"\r\n    Fixes issue where datetimes become dates\r\n    https://github.com/temporalio/sdk-python/issues/207\r\n    TODO(cretz): Use with_child_unrestricted when https://github.com/temporalio/sdk-python/issues/254\r\n    is fixed and released\r\n    \"\"\"\r\n    invalid_module_member_children = dict(\r\n        workflow_sandbox.SandboxRestrictions.invalid_module_members_default.children\r\n    )\r\n    del invalid_module_member_children[\"datetime\"]\r\n    return workflow_sandbox.SandboxedWorkflowRunner(\r\n        restrictions=dataclasses.replace(\r\n            workflow_sandbox.SandboxRestrictions.default,\r\n            invalid_module_members=dataclasses.replace(\r\n                workflow_sandbox.SandboxRestrictions.invalid_module_members_default,\r\n                children=invalid_module_member_children,\r\n            ),\r\n        )\r\n    )\r\n\r\n\r\nasync def main():\r\n\r\n    # Start client\r\n    client_inst = await client.Client.connect(\r\n        \"localhost:7233\",\r\n        data_converter=data_converter,\r\n    )\r\n    signal_receiver_workflow_id = \"signal_receiver_workflow_id\"\r\n    activity_instance = Activities()\r\n\r\n    # Run a worker for the workflow\r\n    task_queue = \"some-task-queue\"\r\n    async with worker.Worker(\r\n        client_inst,\r\n        task_queue=task_queue,\r\n        workflows=[SignalSenderWorkflow, SignalReceiverWorkflow],\r\n        activities=[\r\n            activity_instance.run_activity,\r\n        ],\r\n        workflow_runner=__new_sandbox_runner(),\r\n    ) as _worker:\r\n        signal_receiver_workflow_handler = await client_inst.start_workflow(\r\n            SignalReceiverWorkflow.run,\r\n            0,\r\n            id=signal_receiver_workflow_id,\r\n            task_queue=task_queue,\r\n        )\r\n        _result = await client_inst.execute_workflow(\r\n            SignalSenderWorkflow.run,\r\n            signal_receiver_workflow_id,\r\n            id=\"signal-sender-workflow-id\",\r\n            task_queue=task_queue,\r\n        )\r\n    await signal_receiver_workflow_handler.terminate()\r\n\r\nif __name__ == \"__main__\":\r\n    asyncio.run(main())\r\n```\r\nRun the file\r\n`python continue_as_new.py`\r\n\r\nView terminal results:\r\n```\r\nINFO:root:Running activity payloadDataType=MachineStatusUpdateEventData\r\nINFO:root:checking to see if workflow should be continued, actual_events=88\r\nINFO:root:invoking continue_as_new ***\r\nINFO:root:sending invoke_run_activity\r\nINFO:root:Running activity payloadDataType=MachineStatusUpdateEventData\r\n```\r\nOne does not see the log lines `SignalReceiverWorkflow __init__` or `running SignalReceiverWorkflow iteration=...` after invoking continue_as_new why?\r\n\r\n## Specifications\r\n\r\n  - Version: Python 3.11.9 (main, Dec  4 2024, 11:12:58) [Clang 16.0.0 (clang-1600.0.26.4)] on darwin\r\n  - Platform: MacOs Sequoia Version 15.1 (24B2083\r\n","closedAt":"2024-12-12T19:09:14Z","comments":[{"id":"IC_kwDODNqesM6XYmfq","author":{"login":"spacether"},"authorAssociation":"NONE","body":"This is not a real bug. What was happening was that the PydanticJSONPayloadConverter was handling the  `events: typing.Optional[collections.deque[MachineStatusUpdateEvent]] = None,` argument and it was probably not converting the Modelina MachineStatusUpdateEvent correctly. Once I changed my interfaces to:\r\n- only use pydantic classes or single instances of Modelina classes\r\n\r\nThe continue_as_new worked.\r\nIt is odd that no exceptions were thrown though, I would have expected a serialization/deserialization error to be thrown.","createdAt":"2024-12-12T19:09:14Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6969#issuecomment-2539808746","viewerDidAuthor":false}],"createdAt":"2024-12-11T20:33:44Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":6969,"reactionGroups":[],"state":"CLOSED","title":"workflow.continue_as_new does not create a new workflow","updatedAt":"2024-12-12T19:15:22Z","url":"https://github.com/temporalio/temporal/issues/6969"}
