{"assignees":[],"author":{"id":"U_kgDOCc43_Q","is_bot":false,"login":"abdel-0","name":""},"body":"\n### Problem\n\nToday, Temporal workers enforce concurrency with static limits (`max_concurrent_activity_task_executions`, `max_concurrent_workflow_task_executions`, etc.). These limits assume that workloads are uniform, but in practice workflows and activities have very different profiles (short IO-bound tasks vs. long CPU-bound tasks vs. memory-heavy ones).\n\nThis creates a false trade-off:\n\n* **Set limits too low →** workers are underutilized, leaving CPU/RAM idle and wasting money.\n* **Set limits too high →** workers oversubscribe, pods risk OOM/CPU thrash, and robustness is undermined.\n\nKubernetes autoscaling works on CPU/memory thresholds, but Temporal workers don’t adapt to that. The two systems don’t “speak the same language,” leading to either inefficiency or instability.\n\n---\n\n### Proposed Direction\n\nIntroduce **resource-aware concurrency controls** in workers. For example:\n\n* Workers should stop polling when pod CPU/memory reaches configurable thresholds (e.g. 70–80%).\n* Concurrency could scale dynamically based on resource availability, rather than fixed magic numbers.\n* Optionally, expose hooks so operators can plug in their own resource metrics or scaling policies.\n\nThis would allow Temporal’s built-in backpressure to align with Kubernetes autoscaling, ensuring:\n\n* Pods never overload themselves.\n* Idle capacity is minimized.\n* Scaling is smooth and cost-efficient.\n\n---\n\n### Why This Matters\n\nTemporal’s core value proposition is **reliability at scale**. Without resource-aware workers, teams are forced to choose between:\n\n* **Wasted resources** (cost inefficiency).\n* **Risky oversubscription** (reliability gaps).\n\nA resource-adaptive model would make Temporal more robust, cloud-native, and cost-efficient out of the box.\n\n","closedAt":null,"comments":[{"id":"IC_kwDODNqesM7W_5YP","author":{"login":"baptistejouin"},"authorAssociation":"NONE","body":"Hey, I'm also searching for this type of feature, and I've just found this page on the Temporal documentation:\n\n> Temporal offers three types of slot suppliers: fixed assignment, resource-based, and custom. Here’s how to choose the best approach based on your system requirements and workload characteristics.\n\n> The following use cases are particularly well suited to resource-based auto-tuning slot suppliers:\nFluctuating workloads with low per-Task consumption [...]\nProtection from out-of-memory & over-subscription in the face of unpredictable per-task consumption [...]\n\n\nhttps://docs.temporal.io/develop/worker-performance#worker-performance-tuning\n\nI haven't tested it yet, but it seems promising.\n","createdAt":"2025-12-03T14:15:05Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/8356#issuecomment-3607074319","viewerDidAuthor":false}],"createdAt":"2025-09-20T12:41:45Z","labels":[{"id":"MDU6TGFiZWwxNjIxMDMwNzg3","name":"enhancement","description":"New feature or request","color":"a2eeef"}],"milestone":null,"number":8356,"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":6}}],"state":"OPEN","title":"Resource-Aware Worker Concurrency","updatedAt":"2025-12-03T14:15:05Z","url":"https://github.com/temporalio/temporal/issues/8356"}
