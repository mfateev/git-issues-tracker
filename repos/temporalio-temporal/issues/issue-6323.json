{"assignees":[{"id":"MDQ6VXNlcjE5MjM5OTU=","login":"gow","name":"Chetan Gowda","databaseId":0}],"author":{"id":"U_kgDOBg93Ag","is_bot":false,"login":"uritig","name":"guriti"},"body":"## Expected Behavior\r\nThere should be no memory leak resulting from objects not being properly garbage collected.\r\n\r\n## Actual Behavior\r\nNumber of objects on Heap keeps growing. This seems to be result in slow increase of cpu & memory usage eventually resulting in outage.\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n- Start server\r\n\r\n` temporal server start-dev --port 7233 --ui-port 8233 --metrics-port 9233`\r\n\r\n- Do not start workflows or make any grpc calls, use the sdks or the web ui. aka keep the usage to a minimum.\r\n\r\n- Periodically check pprof & metrics. This shows that the goroutinue counts, objects on heap (aka memory allocations for objects) & memory allocations keeps growing perpetually.\r\n\r\n![image](https://github.com/user-attachments/assets/5eb28209-2bf2-411c-939c-fa45bb0d52e2)\r\n\r\n- runtime.MemStats from from http://localhost:<pprof-port>/debug/pprof/heap?debug=1\r\n```\r\n# runtime.MemStats\r\n# Alloc = 65091800\r\n# TotalAlloc = 318383929504\r\n# Sys = 119521560\r\n# Lookups = 0\r\n# Mallocs = 1641446565\r\n# Frees = 1641167975\r\n# HeapAlloc = 65091800\r\n# HeapSys = 82378752\r\n# HeapIdle = 11141120\r\n# HeapInuse = 71237632\r\n# HeapReleased = 4464640\r\n# HeapObjects = 278590\r\n# Stack = 18284544 / 18284544\r\n# MSpan = 825408 / 960048\r\n# MCache = 19200 / 31200\r\n# BuckHashSys = 2638467\r\n# GCSys = 12561120\r\n# OtherSys = 2667429\r\n# NextGC = 69589984\r\n# LastGC = 1721688648441657922\r\n...\r\n# NumGC = 10489\r\n# NumForcedGC = 0\r\n# GCCPUFraction = 4.835076125703521e-05\r\n# DebugGC = false\r\n# MaxRSS = 178909184\r\n\r\n```\r\n\r\n- Flame graph\r\n\r\n![flame_graph](https://github.com/user-attachments/assets/44044c3b-2fae-4160-8b87-28f2e6b80407)\r\n\r\n- This was observed across local & shared & production environments. Please see the prometheus chart in a production environment where the num_goroutine count kept increasing until a restart. Notice that the \"leap\" appears to be isolated to the frontend service. The rest seem fine. The CPU usage & memory usage charts followed the same pattern.\r\n \r\n![image](https://github.com/user-attachments/assets/034b8b5e-96b2-4879-85ac-18def393131e)\r\n\r\n\r\n## Specifications\r\nThis was observed across multiple versions\r\n  - Server Version: 1.22.5, 1.22.7, 1.23.1, etc.\r\n  - Platform: Linux\r\n  - MTLS enabled\r\n  - Auth disabled\r\n\r\n## Links\r\nThis issue is potentially related to https://community.temporal.io/t/high-cpu-usage-memory-leakage-on-frontend-service/4246/1 \r\n","closedAt":null,"comments":[{"id":"IC_kwDODNqesM6F4F65","author":{"login":"clayzermk1"},"authorAssociation":"NONE","body":"I am also experiencing this issue.","createdAt":"2024-07-23T19:08:49Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2246074041","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6GPjhW","author":{"login":"TheHiddenLayer"},"authorAssociation":"NONE","body":"ditto","createdAt":"2024-07-26T08:21:30Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2252224598","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6HTdf-","author":{"login":"gow"},"authorAssociation":"MEMBER","body":"Hello, I took a brief look into this and was not able to reproduce it locally. I left the server running for over 2 days and didn't observe go-routines growing. Plus in the screenshot you provided (from http://localhost/debug/pprof/heap?debug=1) I don't find anything abnormal. Even an idle server has periodic internal activity and it uses about 2800 goroutines. The `heap` and `allocs` count you see are total allocations (count and size) that may have been freed as well. It doesn't represent currently \"in-use\" allocated objects and their size.\r\n\r\nHowever the last graph you linked showing `num_goroutines` from a production environment is a bit concerning. Since `num_goroutines` are reported [as a guage](https://prometheus.io/docs/tutorials/understanding_metric_types/#gauge), I initially thought applying sum operation on it will result in such graphs. But seeing this happen only in frontend and not in other services is a bit confusing. So we need to dig a little deeper. As a first step, could you please apply the `avg` or `avg_over_time` function on `num_goroutines` and plot the same graph? If you still see the same behavior, then we need to connect to one of the server and obtain the go-routine profile to see what is holding up all those goroutines. You can do this by port-forwarding to one of those frontend hosts and visiting the http://localhost:your_port/debug/pprof. Then click on \"goroutines\" link. Please let us know what you find out.","createdAt":"2024-08-05T22:27:54Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2270025726","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6Htbzs","author":{"login":"uritig"},"authorAssociation":"NONE","body":"@gow thank you for the response.\r\nPlease note that this issue still persists in production & (fortunately) in our sandbox environment as well.\r\nI'm about to enable pprof & leave it running for a couple of weeks. But first, captured:\r\n![image](https://github.com/user-attachments/assets/f2ce41b5-5963-45ba-9f79-d7f760ae89be)\r\n![image](https://github.com/user-attachments/assets/36618caa-6721-4a0c-8b1c-0c713a811275)\r\n\r\nWill post back with the same 2 charts and results of the \"goroutines\" link in a couple of weeks.","createdAt":"2024-08-08T23:02:30Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2276834540","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6Htm4-","author":{"login":"gow"},"authorAssociation":"MEMBER","body":"Thanks for posting the avg(num_goroutine) graphs @guritinvda \r\nTo enable pprof, do you have to restart the service? If not, could you please connect one of the frontend hosts in your sandbox environment and visit `/debug/pprof` and get the goroutine dump in its current state?","createdAt":"2024-08-08T23:32:10Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2276879934","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6HtpmZ","author":{"login":"uritig"},"authorAssociation":"NONE","body":">To enable pprof, do you have to restart the service?\r\n\r\nUnfortunately, yes.","createdAt":"2024-08-08T23:46:37Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2276891033","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6IvNQp","author":{"login":"gow"},"authorAssociation":"MEMBER","body":"@guritinvda any new findings?","createdAt":"2024-08-16T19:31:05Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2294076457","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6I6efN","author":{"login":"uritig"},"authorAssociation":"NONE","body":"I was able to repro the issue in our sandbox environment. Blue arrow in the screenshot is when I enabled pprof and \"restarted\" the frontend service.\r\n![image](https://github.com/user-attachments/assets/c3c2e077-b0c2-4b55-b14c-2771e435772e)\r\n\r\n![image](https://github.com/user-attachments/assets/76755eef-4147-4cde-ad99-e536cb69a8ce)\r\n\r\nHere's the goroutine dump (2 debug levels):\r\n[goroutine-1.txt](https://github.com/user-attachments/files/16663260/goroutine-1.txt)\r\n[goroutine-2.txt](https://github.com/user-attachments/files/16663261/goroutine-2.txt)\r\n\r\n","createdAt":"2024-08-19T17:02:17Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2297030605","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6JDrik","author":{"login":"abhishekhugetech"},"authorAssociation":"NONE","body":"Hey which version of temporal server or temporal SDK version are you using?","createdAt":"2024-08-20T18:05:22Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2299443364","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6KDlVk","author":{"login":"uritig"},"authorAssociation":"NONE","body":"We observed this issue across multiple temporal server versions - 1.22.5, 1.22.7, 1.23.1, etc.\r\ngo-SDK version - v1.24.0 (majority of the workers and clients are here).\r\npython-SDK version - 1.5.1","createdAt":"2024-08-28T20:29:11Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2316195172","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6KM_5l","author":{"login":"gow"},"authorAssociation":"MEMBER","body":"@uritig thanks for posting the data. Looking at the hung Goroutines, it seems to be coming from improper `grpc.ClientConn` cleanup.\r\n```\r\ngoroutine profile: total 4204\r\n3444 @ 0x43e5ce 0x44ea05 0x971c75 0x471661\r\n#\t0x971c74\tgoogle.golang.org/grpc/internal/grpcsync.(*CallbackSerializer).run+0x114\r\n/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/internal/grpcsync/callback_serializer.go:76\r\n```\r\nI found a similar issue posted here on `grpc-go` repo that explains not closing `grpc.ClientConn` as the cause - https://github.com/grpc/grpc-go/issues/6413#issuecomment-1610047629\r\n\r\nThere is a PR (https://github.com/temporalio/temporal/pull/6441) in temporal to cache these connections instead of creating them frequently on demand. This issue might be fixed with that change.","createdAt":"2024-08-29T19:08:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2318663269","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6KNAFw","author":{"login":"gow"},"authorAssociation":"MEMBER","body":"I'll followup once that PR lands.","createdAt":"2024-08-29T19:08:42Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2318664048","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6KOdBk","author":{"login":"dnr"},"authorAssociation":"MEMBER","body":"I've seen this before on the worker service. I'm not sure why we would recreate grpc.ClientConns since we have a stub-level cache that should avoid creating new ones. The new cache should only make a difference if we connect to multiple services on the same endpoint, which we generally don't do. So there might be an issue with the stub cache also.","createdAt":"2024-08-29T21:27:28Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2319044708","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6kBfxM","author":{"login":"gtejasvi"},"authorAssociation":"NONE","body":"seeing a similar behaviour with the history service where we are seeing that after a day of contineous usage the history service uses 100% of cpu available and comes back to normal for 2-3 hours after a pod restart. The pod continues to use 100% cpu till it is restarted even if the no of active workflows are less than 100.","createdAt":"2025-03-25T16:32:10Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-2751855692","viewerDidAuthor":false},{"id":"IC_kwDODNqesM7ZRTIr","author":{"login":"vikas-rampp"},"authorAssociation":"NONE","body":"Hi Team, any update on the fix for this as we are also observing continuously increasing memory consumption by history service. Is there any configuration we need to pass to control this as reading this conversation does not landed me any specific solution.","createdAt":"2025-12-12T07:01:24Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6323#issuecomment-3645190699","viewerDidAuthor":false}],"createdAt":"2024-07-22T23:49:06Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":6323,"reactionGroups":[],"state":"OPEN","title":"Frontend Service - goroutine (CPU & Memory) Leak","updatedAt":"2025-12-12T07:01:24Z","url":"https://github.com/temporalio/temporal/issues/6323"}
