{"assignees":[{"id":"MDQ6VXNlcjU1MTU2Nzg=","login":"shawnhathaway","name":"Shawn Hathaway","databaseId":0}],"author":{"id":"MDQ6VXNlcjEwOTY1MjU4","is_bot":false,"login":"kaushiksriram100","name":"ks"},"body":"## XDC replication should work normally.\r\n\r\n\r\n## Seeing 2 types of errors in \"replication worker\" nodes. One is shardID not set and another is a timeout to workernode:7233. But worker node listens on 7239. Not sure why it is checking 7233. \r\n\r\nThe shard ID not set error seems to happen for all offsets. \r\n\r\n```\r\n{\"level\":\"error\",\"ts\":\"2020-05-15T10:27:07.333-0700\",\"msg\":\"Failed to deserialize replication task\",\"service\":\"worker\",\"component\":\"replicator\",\"component\":\"replication-task-processor\",\"xdc-source-cluster\":\"temporal-dark-knight\",\"kafka-consumer-name\":\"temporal-begins_consumer_for_temporal-dark-knight\",\"kafka-partition\":0,\"kafka-offset\":396,\"attempt-start\":\"2020-05-15T10:27:06.145-0700\",\"error\":\"ShardId not set on request.\",\"attempt-end\":\"2020-05-15T10:27:07.333-0700\",\"logging-call-at\":\"processor.go:269\",\"stacktrace\":\"github.com/temporalio/temporal/common/log/loggerimpl.(*loggerImpl).Error\\n\\t/var/tmp/temporal/common/log/loggerimpl/logger.go:134\\ngithub.com/temporalio/temporal/service/worker/replicator.(*replicationTaskProcessor).nackMsg\\n\\t/var/tmp/temporal/service/worker/replicator/processor.go:269\\ngithub.com/temporalio/temporal/service/worker/replicator.(*replicationTaskProcessor).decodeMsgAndSubmit\\n\\t/var/tmp/temporal/service/worker/replicator/processor.go:249\\ngithub.com/temporalio/temporal/service/worker/replicator.(*replicationTaskProcessor).messageProcessLoop\\n\\t/var/tmp/temporal/service/worker/replicator/processor.go:196\"}\r\n{\"level\":\"error\",\"ts\":\"2020-05-15T10:27:35.224-0700\",\"msg\":\"error starting temporal-sys-tl-scanner-workflow workflow\",\"service\":\"worker\",\"error\":\"last connection error: connection error: desc = \\\"transport: Error while dialing dial tcp 172.17.0.10:7233: connect: connection refused\\\"\",\"logging-call-at\":\"scanner.go:197\",\"stacktrace\":\"github.com/temporalio/temporal/common/log/loggerimpl.(*loggerImpl).Error\\n\\t/var/tmp/temporal/common/log/loggerimpl/logger.go:134\\ngithub.com/temporalio/temporal/service/worker/scanner.(*Scanner).startWorkflow\\n\\t/var/tmp/temporal/service/worker/scanner/scanner.go:197\\ngithub.com/temporalio/temporal/service/worker/scanner.(*Scanner).startWorkflowWithRetry.func1\\n\\t/var/tmp/temporal/service/worker/scanner/scanner.go:174\\ngithub.com/temporalio/temporal/common/backoff.Retry\\n\\t/var/tmp/temporal/common/backoff/retry.go:99\\ngithub.com/temporalio/temporal/service/worker/scanner.(*Scanner).startWorkflowWithRetry\\n\\t/var/tmp/temporal/service/worker/scanner/scanner.go:173\"}\r\n\r\n```\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n  1.Start worker nodes, enableGlobalNamespace=true. \r\n  2. I am able to failover the namespace by updating the namespace config. \r\n  3. clients are able to see the correct active/inactive namespaces. \r\n  4. However, the workflow doesn't continue in the failed over cluster. \r\n  5. I add a workflow.Sleep(60 sec) between activities and failover namespace during that Sleep time. I am expecting the subsequent activity to continue in the failed over cluster. \r\n\r\n## Specifications\r\n\r\n  - Version: Temporal Alpha - 0.21.1\r\n  - Platform:CentOS. \r\n\r\n## Describe namespace: \r\n./tctl --ad 192.168.99.102:17233 --namespace domain_gd_sre3 namespace describe                                  \r\nName: domain_gd_sre3\r\nId: d7420e75-6583-476f-b065-6f68a82ca8de\r\nDescription: \r\nOwnerEmail: \r\nNamespaceData: map[string]string(nil)\r\nStatus: Registered\r\nRetentionInDays: 3\r\nEmitMetrics: false\r\nActiveClusterName: temporal-begins\r\nClusters: temporal-dark-knight, temporal-begins\r\nHistoryArchivalStatus: Enabled\r\nHistoryArchivalURI: file:///tmp/temporal_archival/development\r\nVisibilityArchivalStatus: Disabled\r\nBad binaries to reset:\r\n+-----------------+----------+------------+--------+\r\n| BINARY CHECKSUM | OPERATOR | START TIME | REASON |\r\n+-----------------+----------+------------+--------+\r\n+-----------------+----------+------------+--------+\r\n\r\n## Cluster layout: \r\n```\r\n//temporal-begins is the master cluster & temporal-dark-knight is the Secondary cluster. \r\nmbp-008814:deployment-config skaushik$ kubectl get pods --namespace=temporal-dev\r\nNAME                                             READY     STATUS    RESTARTS   AGE\r\ntemporal-begins-admin-1624787586-d8h59           1/1       Running   0          58m\r\ntemporal-begins-frontend-2147766010-r0p47        1/1       Running   0          58m\r\ntemporal-begins-history-2514570768-dmj11         1/1       Running   2          58m\r\ntemporal-begins-matching-921325243-wmm0v         1/1       Running   0          58m\r\ntemporal-begins-worker-547048616-nj2t4           1/1       Running   0          58m\r\ntemporal-dark-knight-admin-602034094-g7f7f       1/1       Running   0          58m\r\ntemporal-dark-knight-frontend-1492145190-pkff5   1/1       Running   0          58m\r\ntemporal-dark-knight-history-1658671932-1fjw6    1/1       Running   1          58m\r\ntemporal-dark-knight-matching-251941863-lml5l    1/1       Running   0          58m\r\ntemporal-dark-knight-worker-3757330900-mxl45     1/1       Running   0          58m\r\nmbp-008814:deployment-config skaushik$\r\n```\r\n","closedAt":"2020-05-15T23:55:16Z","comments":[{"id":"MDEyOklzc3VlQ29tbWVudDYyOTU1NDQwNg==","author":{"login":"kaushiksriram100"},"authorAssociation":"NONE","body":"@samarabbas  - what is the risk of the other error -  connection timeout to <worker:7233>? The worker doesn't even listen in that port. That still happens. ","createdAt":"2020-05-16T00:02:00Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/379#issuecomment-629554406","viewerDidAuthor":false}],"createdAt":"2020-05-15T17:45:48Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":379,"reactionGroups":[],"state":"CLOSED","title":"XDC replication not working","updatedAt":"2020-05-16T00:02:01Z","url":"https://github.com/temporalio/temporal/issues/379"}
