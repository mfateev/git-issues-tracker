{"assignees":[{"id":"MDQ6VXNlcjQwOTIyNg==","login":"tdeebswihart","name":"Tim Deeb-Swihart","databaseId":0}],"author":{"id":"U_kgDOBu8DQg","is_bot":false,"login":"aloknikhil","name":"Alok"},"body":"## Expected Behavior\r\nThe Temporal server processes workflows normally\r\n\r\n## Actual Behavior\r\nWorkflows are stuck and the server logs indicate the history service is stuck in a loop trying to connect to a Postgres 14.12 DB with `Error: no usable database connection found`\r\n\r\n## Steps to Reproduce the Problem\r\nUnfortunately, we don't have a way to reproduce this\r\n\r\n## Specifications\r\n\r\n  - Version: 1.25.0\r\n  - Platform: Linux ARM64\r\n  - DB: RDS Postgres v14.12\r\n\r\n## Logs\r\n```bash\r\n{\"level\":\"error\",\"ts\":1726181026.2843156,\"logger\":\"temporal-server\",\"caller\":\"log/zap_logger.go:155\",\"msg\":\"Error range completing queue task\",\"app\":\"temporal-backend\",\"version\":\"v1.0.0\",\"shard-id\":130,\"address\":\"20.0.150.47:7234\",\"component\":\"transfer-queue-processor\",\"error\":\"RangeCompleteTransferTask operation failed. Error: no usable database connection found\",\"logging-call-at\":\"/go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/queue_base.go:386\",\"stacktrace\":\"go.temporal.io/server/common/log.(*zapLogger).Error\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/log/zap_logger.go:155\\ngo.temporal.io/server/service/history/queues.(*queueBase).rangeCompleteTasks\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/queue_base.go:386\\ngo.temporal.io/server/service/history/queues.(*queueBase).checkpoint\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/queue_base.go:346\\ngo.temporal.io/server/service/history/queues.(*immediateQueue).processEventLoop\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/queue_immediate.go:173\"}\r\n\r\n{\"level\":\"warn\",\"ts\":1726181026.2003603,\"logger\":\"temporal-server\",\"caller\":\"log/zap_logger.go:147\",\"msg\":\"sql handle: did not refresh database connection pool because the last refresh was too close\",\"app\":\"temporal-backend\",\"version\":\"v1.0.0\",\"min_refresh_interval_seconds\":1,\"logging-call-at\":\"/go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/sql/sqlplugin/db_handle.go:116\"}\r\n\r\n{\"level\":\"error\",\"ts\":1726181026.2853682,\"logger\":\"temporal-server\",\"caller\":\"log/zap_logger.go:155\",\"msg\":\"Operation failed with internal error.\",\"app\":\"temporal-backend\",\"version\":\"v1.0.0\",\"error\":\"GetTransferTasks operation failed. Select failed. Error: no usable database connection found\",\"error-type\":\"serviceerror.Unavailable\",\"operation\":\"GetTransferTasks\",\"logging-call-at\":\"/go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_metric_clients.go:1315\",\"stacktrace\":\"go.temporal.io/server/common/log.(*zapLogger).Error\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/log/zap_logger.go:155\\ngo.temporal.io/server/common/persistence.updateErrorMetric\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_metric_clients.go:1315\\ngo.temporal.io/server/common/persistence.(*metricEmitter).recordRequestMetrics\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_metric_clients.go:1291\\ngo.temporal.io/server/common/persistence.(*executionPersistenceClient).GetHistoryTasks.func1\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_metric_clients.go:396\\ngo.temporal.io/server/common/persistence.(*executionPersistenceClient).GetHistoryTasks\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_metric_clients.go:398\\ngo.temporal.io/server/common/persistence.(*executionRetryablePersistenceClient).GetHistoryTasks.func1\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_retryable_clients.go:377\\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/backoff/retry.go:90\\ngo.temporal.io/server/common/persistence.(*executionRetryablePersistenceClient).GetHistoryTasks\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_retryable_clients.go:381\\ngo.temporal.io/server/service/history/queues.NewImmediateQueue.func1.1\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/queue_immediate.go:79\\ngo.temporal.io/server/common/collection.(*PagingIteratorImpl[...]).getNextPage\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/collection/paging_iterator.go:116\\ngo.temporal.io/server/common/collection.NewPagingIterator[...]\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/common/collection/paging_iterator.go:52\\ngo.temporal.io/server/service/history/queues.(*IteratorImpl).HasNext\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/iterator.go:71\\ngo.temporal.io/server/service/history/queues.(*SliceImpl).SelectTasks\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/slice.go:378\\ngo.temporal.io/server/service/history/queues.(*ReaderImpl).loadAndSubmitTasks\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/reader.go:471\\ngo.temporal.io/server/service/history/queues.(*ReaderImpl).eventLoop\\n\\t/go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/reader.go:439\"}\r\n```\r\n\r\n## Mitigations attempted\r\n- Tried restarting the temporal server\r\n- Tried restarting the database\r\n- Tried restarting the workers\r\n- **Revert back to v1.24.2 - Fixed the issue**","closedAt":"2024-09-20T19:31:07Z","comments":[{"id":"IC_kwDODNqesM6MN-J8","author":{"login":"tarampampam"},"authorAssociation":"NONE","body":"I met the same behavior, unfortunately","createdAt":"2024-09-16T09:55:48Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"CONFUSED","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2352472700","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6Mkl8I","author":{"login":"tdeebswihart"},"authorAssociation":"MEMBER","body":"@aloknikhil in 1.25.x we introduced logic to reconnect to Postgres when connections fail (https://github.com/temporalio/temporal/pull/5926).\r\n\r\nIf you can provide additional logs that'd help us figure out what went wrong; all this log shows is that the failure happened recently so it wont reconnect immediately (this is to prevent it from overwhelming the DB server).\r\n\r\nYou should have a log containing the string `database connection lost` and probably some logs containing `unable to refresh database connection pool`, could you provide those?\r\n\r\n@tarampampam and @jetexe if you have logs like that I'd appreciate it if you could include them, that'll help me diagnose what went wrong","createdAt":"2024-09-18T12:58:26Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2358402824","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6MwKmw","author":{"login":"aloknnikhil"},"authorAssociation":"NONE","body":"Hope this helps\r\n\r\n```\r\nDeleteWorkflowExecution: failed to execute DeleteAllFromSignalInfoMaps: database connection lost: driver: bad connection\r\n\r\ngo.temporal.io/server/common/log.(*zapLogger).Error\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/log/zap_logger.go:155\r\ngo.temporal.io/server/common/persistence.updateErrorMetric\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_metric_clients.go:1315\r\ngo.temporal.io/server/common/persistence.(*metricEmitter).recordRequestMetrics\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_metric_clients.go:1291\r\ngo.temporal.io/server/common/persistence.(*executionPersistenceClient).DeleteWorkflowExecution.func1\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_metric_clients.go:313\r\ngo.temporal.io/server/common/persistence.(*executionPersistenceClient).DeleteWorkflowExecution\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_metric_clients.go:315\r\ngo.temporal.io/server/common/persistence.(*executionRetryablePersistenceClient).DeleteWorkflowExecution.func1\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_retryable_clients.go:312\r\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/backoff/retry.go:90\r\ngo.temporal.io/server/common/persistence.(*executionRetryablePersistenceClient).DeleteWorkflowExecution\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/persistence/persistence_retryable_clients.go:315\r\ngo.temporal.io/server/service/history/shard.(*ContextImpl).DeleteWorkflowExecution.func2\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/shard/context_impl.go:1085\r\ngo.temporal.io/server/service/history/shard.(*ContextImpl).DeleteWorkflowExecution\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/shard/context_impl.go:1092\r\ngo.temporal.io/server/service/history/deletemanager.(*DeleteManagerImpl).deleteWorkflowExecutionInternal\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/deletemanager/delete_manager.go:174\r\ngo.temporal.io/server/service/history/deletemanager.(*DeleteManagerImpl).DeleteWorkflowExecutionByRetention\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/deletemanager/delete_manager.go:154\r\ngo.temporal.io/server/service/history.(*timerQueueTaskExecutorBase).executeDeleteHistoryEventTask\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/timer_queue_task_executor_base.go:153\r\ngo.temporal.io/server/service.history.(*timerQueueActiveTaskExecutor).Execute\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/service.history/timer_queue_active_task_executor.go:137\r\ngo.temporal.io/server/service/history/queues.(*activeStandbyExecutor).Execute\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/active_standby_executor.go:67\r\ngo.temporal.io/server/service/history/queues.(*executableImpl).Execute\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/service/history/queues/executable.go:330\r\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).executeTask.func1\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/tasks/fifo_scheduler.go:223\r\ngo.temporal.io/server/common/backoff.ThrottleRetry.func1\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/backoff/retry.go:63\r\ngo.temporal.io/server/common.backoff.ThrottleRetryContext\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/backoff/retry.go:90\r\ngo.temporal.io/server.common.backoff.ThrottleRetry\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/backoff/retry.go:64\r\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).executeTask\r\n    /go/pkg/mod/go.temporal.io/server@v1.25.0/common/tasks/fifo_scheduler.go:233\r\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).processTask\r\n    /go/pkg/mod/go.temporal.io.server@v1.25.0/common/tasks/fifo_scheduler.go:211\r\n```\r\n\r\nAs mentioned in the original mitigation log, we tried restarting the service as well as the DB and it couldn't get out of this loop. Downgrading to 1.24.2 is the only thing that fixed it","createdAt":"2024-09-19T16:08:39Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2361436592","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6Mwuil","author":{"login":"tdeebswihart"},"authorAssociation":"MEMBER","body":"```\r\ndriver: bad connection\r\n```\r\nWell that's cryptic... I'll see what I can do, thanks for the details","createdAt":"2024-09-19T16:46:51Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2361583781","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6Mw5lW","author":{"login":"tdeebswihart"},"authorAssociation":"MEMBER","body":"According to Go's docs:\r\n\r\n> ErrBadConn should be returned by a driver to signal to the [database/sql](https://pkg.go.dev/database/sql) package that a driver.[Conn](dfile:///Users/timods/Library/Application%20Support/Dash/DocSets/Go/Go.docset/Contents/Resources/Documents/pkg.go.dev/database/sql/driver@go1.23.html#Conn) is in a bad state (such as the server having earlier closed the connection) and the [database/sql](https://pkg.go.dev/database/sql) package should retry on a new connection.\r\n\r\n@aloknnikhil what database driver are you using? `postgres12`? `postgres12_pgx`?","createdAt":"2024-09-19T16:53:51Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2361629014","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6Mw_aN","author":{"login":"tdeebswihart"},"authorAssociation":"MEMBER","body":"Additionally, can I have some details about your setup? I'd like to reproduce this if at all possible so I can validate changes I make","createdAt":"2024-09-19T16:57:31Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2361652877","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6MyqxG","author":{"login":"aloknnikhil"},"authorAssociation":"NONE","body":"- We are using the postgres12 driver\r\n- I am using Postgres 14.12 hosted on RDS on a t4g.small\r\n- I have two Temporal servers running in HA mode (hosted per zone)\r\n- I have about 200 workflows / sec running at peak\r\n- The issue seems to be if for whatever reason a DB connection goes bad, it never recovers from that state","createdAt":"2024-09-19T20:13:44Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2362092614","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6M5B8O","author":{"login":"tdeebswihart"},"authorAssociation":"MEMBER","body":"Thanks for this, that'll help. We're taking a look at this","createdAt":"2024-09-20T13:38:14Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2363760398","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6NJY1P","author":{"login":"tdeebswihart"},"authorAssociation":"MEMBER","body":"@aloknnikhil @tarampampam We'll have this out with our next patch release. This should hopefully fix your issues!","createdAt":"2024-09-23T12:17:49Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2368048463","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6Piw5t","author":{"login":"aloknnikhil"},"authorAssociation":"NONE","body":"Thank you for the quick turnaround! I see the new release is out. We will test this over the weekend and provide feedback.","createdAt":"2024-10-12T00:08:59Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2408255085","viewerDidAuthor":false},{"id":"IC_kwDODNqesM6QkJPX","author":{"login":"aloknnikhil"},"authorAssociation":"NONE","body":"So far things have been stable and no issues. I think we are good here. Thanks again @tdeebswihart!","createdAt":"2024-10-21T01:50:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/temporal/issues/6514#issuecomment-2425394135","viewerDidAuthor":false}],"createdAt":"2024-09-12T22:54:34Z","labels":[{"id":"MDU6TGFiZWwyMDE5ODE3MzQ2","name":"potential-bug","description":"","color":"66b9cc"}],"milestone":null,"number":6514,"reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":3}}],"state":"CLOSED","title":"Stuck Temporal Server","updatedAt":"2024-10-21T01:50:56Z","url":"https://github.com/temporalio/temporal/issues/6514"}
