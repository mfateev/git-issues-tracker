{
  "generatedAt": "2026-01-22T20:31:12.368Z",
  "totalCards": 1491,
  "cards": [
    {
      "summary": "Feature request to separate read and write QPS limits for persistence operations. Currently, a single combined QPS limit causes issues where read-heavy operations like Schedule API queries and UI browsing fail due to persistence throttling, even though reads are less costly than writes.",
      "category": "feature",
      "subcategory": "persistence-qps",
      "apis": [
        "DescribeSchedule"
      ],
      "components": [
        "persistence",
        "history-service",
        "namespace-quotas",
        "schedule-api"
      ],
      "concepts": [
        "qps-limit",
        "throttling",
        "read-write-separation",
        "persistence-overhead",
        "namespace-quotas",
        "burst-traffic",
        "database-load"
      ],
      "severity": "high",
      "userImpact": "Users cannot access schedules via UI when persistence QPS limits are hit by read-heavy operations, limiting the usability of schedule management features.",
      "rootCause": "A single DescribeSchedule request generates 5-20+ persistence read calls in burst (GetCurrentExecution, GetWorkflowExecution, ReadHistoryBranch, etc.), which are treated the same as write operations under a combined QPS limit, causing read-heavy workloads to exceed the limit.",
      "proposedFix": "Introduce separate QPS configuration parameters: history.persistenceNamespaceReadMaxQPS and history.persistenceNamespaceWriteMaxQPS to allow independent rate limiting based on operation type.",
      "workaround": "Increase the namespace QPS quota to accommodate read-heavy schedule operations.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A single DescribeSchedule request can generate 5-20+ persistence calls (GetCurrentExecution, GetWorkflowExecution, ReadHistoryBranch, etc.) in a burst.",
      "number": 9108,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-22T20:31:12.107Z"
    },
    {
      "summary": "Remove obsolete version check and 9-second delay that was added in v1.27 release. The check was meant to be temporary during upgrade transitions but is no longer needed as multiple versions have been released since then.",
      "category": "other",
      "subcategory": "technical-debt",
      "apis": [],
      "components": [
        "worker",
        "deletenamespace",
        "reclaimresources"
      ],
      "concepts": [
        "version-check",
        "technical-debt",
        "cleanup",
        "release-cycle",
        "deprecated-code"
      ],
      "severity": "low",
      "userImpact": "Reduces unnecessary complexity and conditional logic in the codebase without affecting user-facing functionality.",
      "rootCause": "Temporary version check and delay were added during v1.27 release transition period and are now obsolete after subsequent releases.",
      "proposedFix": "Remove the version check and 9-second delay constant from the workflow.go file.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        7385
      ],
      "keyQuote": "v1.27.0 was released 11 months ago... v1.28.0 was released in June... v1.29.0 was released in October",
      "number": 9060,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-22T20:31:09.818Z"
    },
    {
      "summary": "Comprehensive architectural guide documenting design patterns for building background code execution agents, including queue systems, job orchestration, state management, reliability patterns, security considerations, and implementation recommendations.",
      "category": "docs",
      "subcategory": "architecture-guide",
      "apis": [],
      "components": [
        "queue-systems",
        "job-orchestration",
        "state-management",
        "worker-pool"
      ],
      "concepts": [
        "message-queues",
        "workflow-orchestration",
        "event-sourcing",
        "distributed-systems",
        "scalability",
        "reliability",
        "observability"
      ],
      "severity": "low",
      "userImpact": "Provides architectural guidance for developers building robust background job execution systems with Temporal.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Choose patterns based on your specific requirements for consistency, availability, partition tolerance, and perf",
      "number": 9057,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-22T20:31:07.557Z"
    },
    {
      "summary": "When starting workflows, activities, Nexus operations, or creating schedules with null search attribute values, those attributes should be excluded from the started item rather than included. Currently this is mostly prevented by SDK implementations, but the server should handle it consistently.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [
        "StartWorkflow",
        "StartActivity",
        "NexusStart",
        "CreateSchedule",
        "ContinueAsNew"
      ],
      "components": [
        "workflow-start",
        "activity-start",
        "nexus-operations",
        "schedule-creation",
        "search-attributes"
      ],
      "concepts": [
        "search-attributes",
        "null-handling",
        "data-validation",
        "workflow-initialization",
        "consistency"
      ],
      "severity": "medium",
      "userImpact": "Users may encounter inconsistent behavior when null search attributes are included in workflow/activity starts, potentially causing unexpected filtering or query results.",
      "rootCause": "Server does not filter out null search attribute values during workflow/activity/Nexus/schedule initialization.",
      "proposedFix": "Filter out search attributes with null values when processing workflow start, activity start, Nexus start, schedule create, or continue as new operations.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2150
      ],
      "keyQuote": "If a search attribute's value is null on workflow start, activity start, Nexus start, schedule create, or continue as new, it should not be included on the started item.",
      "number": 9056,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-22T20:30:53.865Z"
    },
    {
      "summary": "History scavenger is prematurely deleting workflow execution history for active workflows due to error handling in the namespace registry that incorrectly converts persistence errors to NamespaceNotFound, bypassing the mutable state check safeguard.",
      "category": "bug",
      "subcategory": "history-scavenger",
      "apis": [],
      "components": [
        "history-scavenger",
        "namespace-registry",
        "execution-data-cleaner"
      ],
      "concepts": [
        "data-loss",
        "garbage-collection",
        "mutable-state",
        "safeguards",
        "error-handling",
        "namespace-resolution"
      ],
      "severity": "critical",
      "userImpact": "Users experience complete loss of workflow execution history while mutable state still exists, causing inaccessible workflows and data integrity violations.",
      "rootCause": "The namespace registry error handler at line 559 swallows all persistence errors and converts them to NamespaceNotFound, causing the scavenger's safeguard check to incorrectly pass and allow history deletion for active workflows.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "it swallow any error returned from persistence and convert it to a NamespaceNotFound error, make safeguard #2 to be satisfied also.",
      "number": 9021,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-22T20:30:55.748Z"
    },
    {
      "summary": "Request to add custom headers support to the \"Start Workflow\" form in the Temporal Web UI, enabling users to specify context propagation headers (like x-tenant-id) when manually starting workflows without needing CLI or custom scripts.",
      "category": "feature",
      "subcategory": "web-ui-workflow-management",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "web-ui",
        "workflow-start-form",
        "context-propagation"
      ],
      "concepts": [
        "custom-headers",
        "context-propagation",
        "multi-tenant",
        "workflow-execution",
        "request-metadata",
        "user-experience"
      ],
      "severity": "medium",
      "userImpact": "Users must currently use CLI or custom scripts to start workflows with custom headers, limiting the Web UI's usefulness for testing and operational scenarios requiring header-based context.",
      "rootCause": null,
      "proposedFix": "Add a \"Headers\" section to the \"Start Workflow\" form accepting a JSON object with header names and string values, similar to existing Memo and Search Attributes sections.",
      "workaround": "Use Temporal CLI with --header flag or create wrapper API endpoints to handle header-based context propagation.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Supporting custom headers in the Web UI would align the Web UI capabilities with what's possible via the SDK and make it a complete tool for workflow management.",
      "number": 9012,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-22T20:30:54.227Z"
    },
    {
      "summary": "Users request the ability to filter workflows in the UI by substring matching. Currently, the UI only supports filtering by exact match or 'starts with' pattern, limiting discoverability and search functionality.",
      "category": "feature",
      "subcategory": "workflow-filtering",
      "apis": [],
      "components": [
        "ui",
        "workflow-search",
        "filtering"
      ],
      "concepts": [
        "substring-matching",
        "search",
        "filtering",
        "workflow-discovery",
        "usability"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently search and filter workflows by partial names, reducing productivity when managing large numbers of workflows.",
      "rootCause": null,
      "proposedFix": "Implement substring filtering capability in the workflow filter UI alongside existing exact match and 'starts with' options.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A major frustration we have is that there is no way to filter workflows in UI by a substring. The only way to filter now is \"starts with\" or \"equal\"",
      "number": 9005,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:30:19.958Z"
    },
    {
      "summary": "Client-set RPC timeouts/deadlines are ignored by the server for long poll GetWorkflowExecutionHistory calls starting in v1.29.0. The server waits the full long poll interval (~20s) instead of respecting the client's deadline, though it does respond approximately 1s before the deadline is reached.",
      "category": "bug",
      "subcategory": "rpc-deadline",
      "apis": [
        "GetWorkflowExecutionHistory"
      ],
      "components": [
        "server",
        "long-poll",
        "rpc-handler"
      ],
      "concepts": [
        "rpc-deadline",
        "timeout",
        "client-server-communication",
        "long-polling",
        "deadline-handling"
      ],
      "severity": "high",
      "userImpact": "Users cannot enforce client-side RPC timeouts on GetWorkflowExecutionHistory calls, causing requests to hang longer than intended and breaking timeout-based control flow.",
      "rootCause": "Server-side change introduced in v1.29.0 that ignores client-set deadlines for long poll GetWorkflowExecutionHistory operations",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed indicating resolution, likely through server-side deadline handling fix",
      "related": [],
      "keyQuote": "Client-set RPC timeouts/deadlines are ignored by the server for long poll `GetWorkflowExecutionHistory` calls starting in v1.29.0.",
      "number": 8970,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:30:18.708Z"
    },
    {
      "summary": "User asks whether different fairness weights for the same fairness key on different workflows will cause issues, and which weight would take precedence. The maintainer clarified that semantically fairness weight should be a property of the key, and using different weights for the same key is semantically incorrect though technically allowed.",
      "category": "question",
      "subcategory": "fairness-scheduling",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-scheduling",
        "priority-queue",
        "fairness-algorithm"
      ],
      "concepts": [
        "fairness-key",
        "fairness-weight",
        "workflow-priority",
        "scheduling",
        "weighting"
      ],
      "severity": "low",
      "userImpact": "Users may be confused about fairness behavior when using the same fairness key with different weights across workflows.",
      "rootCause": "API design allows flexible external weight storage but doesn't enforce weight consistency per key, leading to potential confusion.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was closed by maintainer indicating it's a general question better suited for community forums, not a bug report. Clarification was provided that different weights for the same key behavior is undefined semantically.",
      "related": [],
      "keyQuote": "the effective behavior will be something like an average of the weights",
      "number": 8968,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:30:19.851Z"
    },
    {
      "summary": "User inquiring about the release timeline for version 1.30.0 of Temporal Server.",
      "category": "question",
      "subcategory": "release-timeline",
      "apis": [],
      "components": [],
      "concepts": [
        "release",
        "versioning",
        "timeline"
      ],
      "severity": "low",
      "userImpact": "Users need visibility into when the next major release will be available to plan their upgrades.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When can I expect 1.30.0 to be released?",
      "number": 8967,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:30:03.425Z"
    },
    {
      "summary": "Feature request to create an MCP (Model Context Protocol) server for Temporal that enables AI-powered workflow orchestration. The server would provide tools for workflow management (start, signal, query), activity execution, and namespace management.",
      "category": "feature",
      "subcategory": "mcp-integration",
      "apis": [
        "StartWorkflow",
        "SignalWorkflow",
        "QueryWorkflow"
      ],
      "components": [
        "workflow-engine",
        "activity-executor",
        "namespace-manager",
        "grpc-api"
      ],
      "concepts": [
        "ai-integration",
        "tool-standard",
        "workflow-automation",
        "model-context-protocol",
        "orchestration",
        "interoperability"
      ],
      "severity": "low",
      "userImpact": "Enables AI systems to natively orchestrate and manage Temporal workflows through standardized MCP protocol.",
      "rootCause": null,
      "proposedFix": "Implement MCP server exposing Temporal workflow operations as tools compatible with AI models.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Model Context Protocol - AI tool standard (Anthropic, OpenAI).",
      "number": 8955,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:30:08.254Z"
    },
    {
      "summary": "User reports that a scheduled workflow configured to run every 28 days shows incorrect upcoming run information after immediate execution, causing unexpected error behavior.",
      "category": "bug",
      "subcategory": "scheduling",
      "apis": [],
      "components": [
        "scheduler",
        "schedule-execution"
      ],
      "concepts": [
        "scheduling",
        "recurring-execution",
        "time-calculation",
        "interval"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably schedule workflows with multi-day intervals due to incorrect upcoming run time calculations.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I created a schedule and executed it immediately, setting it to run every 28 days. Why am I getting the \"upcoming run\" error?",
      "number": 8953,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:30:05.292Z"
    },
    {
      "summary": "Missing admin-tools Docker image tagged as version 1.29.2. The release notes pointed to an incorrect Docker Hub URL (base-admin-tools instead of admin-tools), and there was no matching admin-tools image for this Temporal server version.",
      "category": "docs",
      "subcategory": "docker-images",
      "apis": [],
      "components": [
        "docker-images",
        "release-notes",
        "documentation"
      ],
      "concepts": [
        "docker",
        "versioning",
        "release-management",
        "documentation-accuracy",
        "image-tagging"
      ],
      "severity": "low",
      "userImpact": "Users deploying Temporal 1.29.2 were confused about which admin-tools Docker image version to use due to incorrect documentation links.",
      "rootCause": "Release notes contained an incorrect Docker Hub URL pointing to base-admin-tools repository instead of admin-tools, and no matching 1.29.2 tag existed for admin-tools.",
      "proposedFix": "Use the 'latest' admin-tools image or 1.29 tag, which correspond to server version 1.29.2. Corrected the release notes URL.",
      "workaround": "Use 'latest' or '1.29' tag for admin-tools when deploying with Temporal 1.29.2 server.",
      "resolution": "fixed",
      "resolutionDetails": "The release notes URL was corrected to point to the correct admin-tools repository. Users were advised that the 'latest' or '1.29' admin-tools tags are compatible with server version 1.29.2.",
      "related": [],
      "keyQuote": "the link provided in the release notes was not correct. It should point to https://hub.docker.com/r/temporalio/admin-tools/",
      "number": 8943,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:52.205Z"
    },
    {
      "summary": "Temporal Server v1.29.1 logs deprecation warnings as errors when using Elasticsearch 8.19.2 due to the olivere/elastic/v7 client sending deprecated parameters like include_upper. The deprecated field warnings generate excessive error-level log noise in production environments.",
      "category": "bug",
      "subcategory": "elasticsearch-compatibility",
      "apis": [
        "CountWorkflowExecutions"
      ],
      "components": [
        "elasticsearch-client",
        "visibility-store",
        "logger"
      ],
      "concepts": [
        "deprecation",
        "elasticsearch-compatibility",
        "log-levels",
        "legacy-api",
        "version-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal with Elasticsearch 8.x experience excessive error logs cluttering production monitoring and alerting systems, making it difficult to identify genuine issues.",
      "rootCause": "The olivere/elastic/v7 client library continues to send deprecated Elasticsearch parameters (include_upper) that trigger deprecation warnings in Elasticsearch 8.x, which are logged at error level instead of being suppressed or downgraded.",
      "proposedFix": "Update Temporal to use the latest Elasticsearch APIs compatible with version 8.x, or filter/downgrade deprecation warning log levels from Elasticsearch to avoid error-level noise.",
      "workaround": "Configure Elasticsearch cluster settings to set logger.org.elasticsearch.deprecation to ERROR level to suppress deprecation warnings.",
      "resolution": "invalid",
      "resolutionDetails": "User resolved the issue on their end by configuring Elasticsearch to suppress deprecation warnings, but the underlying problem of using deprecated APIs remains unresolved by the Temporal team.",
      "related": [],
      "keyQuote": "I hope that in the future, Temporal will be patched to use the latest APIs instead of deprecated ones, depending on the Elasticsearch version.",
      "number": 8909,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:53.978Z"
    },
    {
      "summary": "History service memory usage grows unbounded in Kubernetes deployments under cyclical workloads, causing OOM kills even during idle periods. Cache configuration issues prevent stable memory management despite various tuning attempts.",
      "category": "bug",
      "subcategory": "history-service-memory",
      "apis": [],
      "components": [
        "history-service",
        "cache-manager",
        "event-cache",
        "persistence"
      ],
      "concepts": [
        "memory-leak",
        "cache-eviction",
        "garbage-collection",
        "kubernetes",
        "oom",
        "load-pattern",
        "persistence-layer"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably run Temporal Server on Kubernetes without experiencing OOM crashes on the history service, blocking production deployments.",
      "rootCause": "Mutable cache capacity-based configuration causes memory usage to significantly exceed configured limits and GOMEMLIMIT, suggesting improper cache accounting or eviction logic.",
      "proposedFix": null,
      "workaround": "Configure mutable cache in count units instead of capacity units; apply GOMEMLIMIT setting. However, this is only a partial workaround as switching back to capacity units still causes OOM.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "as soon as I set the mutable cache in capacity units, the history pod significantly exceeds the configured cache capacity, even surpasses the GOMEMLIMIT",
      "number": 8902,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:53.588Z"
    },
    {
      "summary": "Feature request to skip re-executing completed activities when a workflow is retried due to failure. Currently, Temporal replays all activities from the beginning on workflow retry, causing duplicate side effects in external systems for activities with non-reversible effects.",
      "category": "feature",
      "subcategory": "workflow-retry",
      "apis": [
        "StartWorkflowOptions",
        "RetryPolicy",
        "ResetWorkflowExecution"
      ],
      "components": [
        "workflow-retry-policy",
        "event-history-replay",
        "activity-execution"
      ],
      "concepts": [
        "replay",
        "idempotency",
        "side-effects",
        "retry-policy",
        "event-history",
        "workflow-recovery"
      ],
      "severity": "high",
      "userImpact": "Users with non-reversible external side effects cannot safely retry workflows without manual intervention to clean up duplicate records.",
      "rootCause": "Workflow retry mechanism replays entire event history including completed activities, without option to skip already-completed steps.",
      "proposedFix": "Add ReplayOnRetry flag to RetryPolicy that skips re-execution of already-completed activities and resumes from the point of last failure.",
      "workaround": "Manual deletion of external records before retry, or making all activities idempotent to handle duplicate execution.",
      "resolution": "wontfix",
      "resolutionDetails": "Closed with recommendation to implement idempotent activities as per Temporal best practices, rather than adding workflow-level replay skip feature.",
      "related": [],
      "keyQuote": "Temporal strongly recommends that Activities be idempotent. This activity may be retried for many reasons other than workflow retry policy.",
      "number": 8901,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:41.266Z"
    },
    {
      "summary": "Hikaflow, an AI-powered engineering assistant, is offered as a trial to help Temporal team review PRs, analyze impact on workflows and components, detect issues, and improve release confidence through automated analysis and documentation generation.",
      "category": "other",
      "subcategory": "tool-partnership",
      "apis": [],
      "components": [
        "core-service",
        "sdks",
        "persistence-layer",
        "workflow-routing",
        "history-management"
      ],
      "concepts": [
        "code-review",
        "impact-analysis",
        "determinism",
        "workflow-execution",
        "regression-detection",
        "release-confidence",
        "documentation-generation"
      ],
      "severity": "low",
      "userImpact": "Developers could benefit from automated PR review insights, risk analysis, and documentation if Hikaflow trial is accepted and integrated.",
      "rootCause": null,
      "proposedFix": "Trial integration of Hikaflow as a GitHub app to provide PR analysis, impact mapping, issue detection, and documentation automation without requiring workflow changes.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I'd love to offer a two-week trial of Hikaflow on Temporal to see if these features can help streamline PR reviews, surface regressions early, and improve release confidence.",
      "number": 8889,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:39.470Z"
    },
    {
      "summary": "Address a golang crypto/x509 vulnerability (CVE-2025-61729) affecting Docker images for both Temporal Server and admin tools.",
      "category": "bug",
      "subcategory": "security-vulnerability",
      "apis": [],
      "components": [
        "docker-image",
        "server",
        "admin-tools"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "dependency",
        "golang",
        "cryptography",
        "docker"
      ],
      "severity": "high",
      "userImpact": "Users running vulnerable Docker images are exposed to potential security exploits through compromised x509 certificate handling.",
      "rootCause": "Outdated golang crypto/x509 library version containing CVE-2025-61729",
      "proposedFix": "Update golang:crypto/x509 dependency to a patched version in Docker images",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We see that golang:crypto/x509 has vulnerability CVE-2025-61729. Please help us with a docker image for both server and admin tools by fixing these vulnerabilities.",
      "number": 8866,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:40.551Z"
    },
    {
      "summary": "Address CVE-2025-61727 security vulnerability in golang crypto/x509 package affecting Docker images for Temporal server and admin tools.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "docker",
        "server",
        "admin-tools",
        "crypto/x509"
      ],
      "concepts": [
        "security-vulnerability",
        "CVE",
        "docker-image",
        "dependency-management",
        "golang"
      ],
      "severity": "high",
      "userImpact": "Temporal server and admin tool Docker images are vulnerable to CVE-2025-61727, exposing deployments to potential cryptographic attacks.",
      "rootCause": "golang crypto/x509 library contains an unpatched security vulnerability (CVE-2025-61727).",
      "proposedFix": "Update golang crypto/x509 dependency and rebuild Docker images for server and admin tools.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "golang:crypto/x509 has vulnerability CVE-2025-61727. Please help us with a docker image for both server and admin tools by fixing these vulnerabilities.",
      "number": 8865,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:27.919Z"
    },
    {
      "summary": "Workflow execution fails with 'Unable to query workflow due to Workflow Task in failed state' error when using dedicated cloud PostgreSQL databases. The issue occurs consistently after setting up Temporal with remote PostgreSQL, and also manifests with MySQL after some time.",
      "category": "bug",
      "subcategory": "database-persistence",
      "apis": [],
      "components": [
        "persistence-layer",
        "workflow-task-executor",
        "scheduler",
        "database-connector"
      ],
      "concepts": [
        "workflow-state",
        "database-connection",
        "schema-setup",
        "permissions",
        "task-failure"
      ],
      "severity": "high",
      "userImpact": "Users cannot run workflows or schedules when Temporal is deployed with external PostgreSQL or MySQL databases, blocking production deployments.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Unable to query workflow due to Workflow Task in failed state.",
      "number": 8864,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:28.523Z"
    },
    {
      "summary": "Scheduler's `describe().info.runningActions` does not include workflows triggered with `triggerImmediately` option, even though they are created correctly. Manual triggers via `handler.trigger()` properly track running actions.",
      "category": "bug",
      "subcategory": "scheduler-tracking",
      "apis": [
        "schedule.create",
        "schedule.getHandle",
        "describe"
      ],
      "components": [
        "scheduler",
        "workflow-execution-tracking",
        "state-management"
      ],
      "concepts": [
        "immediate-trigger",
        "running-actions",
        "overlap-policy",
        "schedule-state",
        "workflow-linking"
      ],
      "severity": "medium",
      "userImpact": "Users cannot track workflows triggered immediately via scheduler options, causing confusion about execution status despite workflows being created successfully.",
      "rootCause": "triggerImmediately was being treated differently and filtered out as an overlapping run, not properly tracked in runningActions regardless of schedule's overlap policy.",
      "proposedFix": "Fix #8932 ensures these runs are always tracked in runningActions regardless of the schedule's overlap policy.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        8932
      ],
      "keyQuote": "triggerImmediately was being treated differently inside the system.... it was often filtered out as an overlapping run.",
      "number": 8833,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:27.025Z"
    },
    {
      "summary": "Empty test ticket with no details provided. The issue contains only template structure with no actual problem description, expected behavior, actual behavior, reproduction steps, or specifications filled in.",
      "category": "other",
      "subcategory": "test-ticket",
      "apis": [],
      "components": [],
      "concepts": [
        "testing",
        "placeholder",
        "template"
      ],
      "severity": "low",
      "userImpact": "No impact - this is a placeholder test ticket with no substantive content.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Closed as invalid - empty template ticket with no actual issue content",
      "related": [],
      "keyQuote": null,
      "number": 8806,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:13.410Z"
    },
    {
      "summary": "The history_node table continues to grow despite setting a 3-day retention period, while the workflow count appears to respect retention. User reports 19.4M rows in history_node table when workflow count is only 7,274.",
      "category": "bug",
      "subcategory": "data-retention",
      "apis": [],
      "components": [
        "history-scanner",
        "database",
        "retention-policy"
      ],
      "concepts": [
        "data-retention",
        "table-growth",
        "history-cleanup",
        "worker-configuration",
        "database-size"
      ],
      "severity": "high",
      "userImpact": "Database storage continues to grow uncontrollably despite retention policies being configured, potentially leading to disk space issues and performance degradation.",
      "rootCause": "History scanner may not be properly enabled or configured; possible failure in cleanup process leaves first event batch in DB until scanner processes it.",
      "proposedFix": "Verify worker.historyScannerEnabled flag is enabled (should be default); check that internal scanner workflows (temporal-sys-history-scanner-workflow) are running; consider reducing worker.historyScannerDataMinAge to trigger cleanup sooner.",
      "workaround": "Reduce worker.historyScannerDataMinAge from default 60 days to a lower value to allow history scanner to clean up rows more aggressively.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "history_node table keep growing, now has 19402165 count",
      "number": 8790,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:15.939Z"
    },
    {
      "summary": "User requests the ability to list workflows and activities registered in the Temporal system using the Python SDK. Currently, the server does not maintain a registry of workflow and activity types, and the discussion explores potential solutions including querying workers directly.",
      "category": "feature",
      "subcategory": "registry-discovery",
      "apis": [],
      "components": [
        "worker",
        "server",
        "registry"
      ],
      "concepts": [
        "service-discovery",
        "workflow-registration",
        "activity-registration",
        "worker-communication",
        "registry-management"
      ],
      "severity": "low",
      "userImpact": "Users cannot programmatically discover available workflows and activities in the system, forcing them to maintain separate internal registries.",
      "rootCause": "Server does not maintain a built-in registry of workflow and activity types.",
      "proposedFix": "Either implement server-side registry of workflow/activity types, or provide APIs to query workers for registered types (requires running workers and task queue information).",
      "workaround": "Build and maintain an internal registry of workflows and activities.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "As of now server does not keep registry of Workflow and Activity Types. But this is something that maybe we want to consider in the future.",
      "number": 8764,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:16.358Z"
    },
    {
      "summary": "Request for plugin-based authentication support for PostgreSQL backend to enable Azure Entra ID, AWS IAM, and Google Cloud IAM token-based authentication without embedding cloud-specific SDKs into Temporal core.",
      "category": "feature",
      "subcategory": "database-authentication",
      "apis": [],
      "components": [
        "postgresql-plugin",
        "sql-tool",
        "server-config"
      ],
      "concepts": [
        "authentication",
        "cloud-identity",
        "token-based-auth",
        "plugin-architecture",
        "secret-management",
        "entra-id",
        "managed-identity"
      ],
      "severity": "medium",
      "userImpact": "Users self-hosting Temporal on Azure, AWS, or Google Cloud can use cloud-native identity and access management for database authentication instead of managing static credentials.",
      "rootCause": "Temporal currently lacks a plugin-based mechanism for dynamic token-based database authentication, forcing organizations to maintain custom forks for cloud-provider-specific auth schemes.",
      "proposedFix": "Implement a plugin-based approach where cloud-specific token providers (CLI binary or Go plugin) can be invoked to fetch and manage JWT tokens for database connections, with support for token expiry handling through existing connection lifecycle management.",
      "workaround": "Organizations can maintain custom patches and forks that embed cloud SDKs directly into Temporal binaries, though this is not ideal for maintenance and distribution.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I would propose a more generic plugin-based approach can be taken here to make it possible to use any cloud provider that follows a similar JWT auth pattern.",
      "number": 8724,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:04.197Z"
    },
    {
      "summary": "Temporal servers retain stale internode gRPC connections to terminated Kubernetes pods, resulting in repeated dial timeout errors to non-existent pod IPs. This occurs when using Deployments and happens during pod churn events like rolling updates or scale-downs.",
      "category": "bug",
      "subcategory": "internode-communication",
      "apis": [],
      "components": [
        "grpc-client",
        "membership-ring",
        "connection-pool",
        "kubernetes-integration"
      ],
      "concepts": [
        "connection-cleanup",
        "pod-termination",
        "stale-connections",
        "kubernetes-deployments",
        "dial-timeout",
        "membership-management"
      ],
      "severity": "high",
      "userImpact": "Kubernetes deployments experience continuous gRPC timeout errors and resource waste from repeated connection attempts to terminated pods, impacting cluster stability and observability.",
      "rootCause": "Temporal does not properly drop internode gRPC connections when pods are removed from the membership ring during Kubernetes pod lifecycle events.",
      "proposedFix": null,
      "workaround": "Consider switching from Deployment to StatefulSet for more stable pod identities, though this requires diverging from the official Helm chart.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        8586
      ],
      "keyQuote": "Temporal servers keep stale internode gRPC connections after Kubernetes pods are terminated (during scale-down, rolling updates, or node drains).",
      "number": 8719,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:02.568Z"
    },
    {
      "summary": "Review and patch multiple CVEs affecting Go standard libraries (versions up to 1.25.0) that impact URL parsing, certificate validation, cookie handling, and TLS handshakes. Upgrade to Go 1.25.3 is recommended to address security vulnerabilities.",
      "category": "bug",
      "subcategory": "security-vulnerability",
      "apis": [],
      "components": [
        "dependencies",
        "tls",
        "networking",
        "url-parsing"
      ],
      "concepts": [
        "security",
        "cve",
        "memory-exhaustion",
        "performance",
        "certificate-validation",
        "url-parsing",
        "tls-handshake"
      ],
      "severity": "high",
      "userImpact": "Temporal deployments using Go 1.25.0 or earlier may be vulnerable to multiple security CVEs affecting URL parsing, certificate validation, and TLS, requiring immediate patching to Go 1.25.3.",
      "rootCause": "Go standard library vulnerabilities in URL parsing (RFC 3986 violation), certificate validation (name constraints, DSA casting), cookie parsing (unlimited memory), and TLS handshakes (ALPN leakage) and HTTP response parsing (string concatenation).",
      "proposedFix": "Upgrade Go runtime from version 1.25.0 to version 1.25.3 to address all identified CVEs.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The installed version is 1.25.0, and as per the patch advisory, it should be upgraded to 1.25.3 to address the issue.",
      "number": 8698,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:29:01.670Z"
    },
    {
      "summary": "The Java SDK's temporal-serviceclient dependency includes a vulnerable version of grpc-netty-shaded (<1.75.0) that triggers a High-severity CVE-2025-55163 (Netty/HTTP2 DDoS vulnerability), blocking builds for teams using corporate security scanners.",
      "category": "bug",
      "subcategory": "dependency-vulnerability",
      "apis": [],
      "components": [
        "temporal-serviceclient",
        "grpc-netty-shaded",
        "dependency-management"
      ],
      "concepts": [
        "vulnerability",
        "CVE",
        "DDoS",
        "security-scanning",
        "grpc",
        "netty",
        "dependency-upgrade"
      ],
      "severity": "high",
      "userImpact": "Users of the Java SDK are unable to build and deploy applications due to high-severity CVE detected by corporate security scanners.",
      "rootCause": "temporal-serviceclient depends on grpc-netty-shaded version 1.58.1, which is vulnerable to CVE-2025-55163 (Netty/HTTP2 MadeYouReset DDoS vulnerability)",
      "proposedFix": "Upgrade grpc-netty-shaded to version 1.75.0 or higher",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Upgraded grpc-netty-shaded dependency to version 1.75.0",
      "related": [],
      "keyQuote": "CVE: CVE-2025-55163, Severity: High, Description: Netty/HTTP2 \"MadeYouReset\" DDoS vulnerability, Fix version: 1.75.0",
      "number": 8663,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:48.361Z"
    },
    {
      "summary": "Multiple CVEs found in temporalio/admin-tools:1.29.1 Docker image, including critical sqlite vulnerability and high-severity golang.org/x/net and OpenTelemetry vulnerabilities. Requires updating Go dependencies and Alpine base image to address security risks in production deployments.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "admin-tools",
        "docker-image",
        "dependencies",
        "base-image"
      ],
      "concepts": [
        "vulnerability",
        "cve",
        "dependency-update",
        "container-security",
        "alpine-image",
        "golang-dependencies"
      ],
      "severity": "critical",
      "userImpact": "Production deployments using the admin-tools container are exposed to critical and high-severity security vulnerabilities that could lead to security breaches.",
      "rootCause": "Outdated Go dependencies (golang.org/x/net v0.39.0, otelgrpc v0.59.0) and vulnerable Alpine base image (3.22 with sqlite 3.49.2-r0 containing CVSS 9.8 vulnerability)",
      "proposedFix": "Update Go dependencies in go.mod to latest secure versions, update Alpine base image from 3.22 to 3.23+, run dependency audit and vulnerability scan, and update go.sum",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The CRITICAL sqlite vulnerability affects the admin-tools container which is used for database migrations and administrative operations",
      "number": 8655,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:50.367Z"
    },
    {
      "summary": "Workflows are not visible in the Temporal UI or through `tctl workflow list`, despite being present in the MySQL database and indexed in Elasticsearch. Individual workflows can be described via `tctl workflow show` but cannot be listed.",
      "category": "bug",
      "subcategory": "visibility-ui",
      "apis": [],
      "components": [
        "web-ui",
        "tctl",
        "visibility-engine",
        "elasticsearch"
      ],
      "concepts": [
        "workflow-listing",
        "visibility",
        "ui-rendering",
        "elasticsearch-indexing",
        "workflow-discovery"
      ],
      "severity": "high",
      "userImpact": "Users cannot view their workflows in the UI or list them via CLI, preventing workflow management and monitoring despite data being present in the database.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Workflows are not visible on temporal UI. Through tctl I'm able to describe the workflow using tctl workflow show --workflow_id <workflow-id> but get an empty output through tctl workflow list",
      "number": 8654,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:48.789Z"
    },
    {
      "summary": "Request to document ScyllaDB compatibility with Temporal's Cassandra backend. Currently unclear whether ScyllaDB is recommended, unsupported, or unverified despite being presented as a drop-in Cassandra alternative.",
      "category": "docs",
      "subcategory": "database-compatibility",
      "apis": [],
      "components": [
        "cassandra-backend",
        "storage-persistence"
      ],
      "concepts": [
        "database-compatibility",
        "scylladb",
        "backend-support",
        "documentation",
        "production-readiness",
        "testing-validation"
      ],
      "severity": "medium",
      "userImpact": "Users evaluating storage options for Temporal lack clear guidance on whether ScyllaDB is a viable production alternative to Cassandra.",
      "rootCause": null,
      "proposedFix": "Add documentation clarifying ScyllaDB support status and guidance for users, such as: expected to work but not officially tested, or compatibility may be revisited if there is interest.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2683,
        3511,
        1267
      ],
      "keyQuote": "If ScyllaDB claims itself to be compatible, then it should work out of the box. For production usage, there's obviously a risk though as we don't really do any release testing/validation on top of ScyllaDB.",
      "number": 8652,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:37.359Z"
    },
    {
      "summary": "Request to add ExecutionStatus as a visibility column for archived executions so users can search and filter archived workflows by execution status without needing to examine event history.",
      "category": "feature",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "archival",
        "visibility-columns",
        "search-attributes"
      ],
      "concepts": [
        "archived-executions",
        "execution-status",
        "visibility",
        "search",
        "workflow-history"
      ],
      "severity": "medium",
      "userImpact": "Users cannot search archived workflows by execution status and must examine event history as a workaround.",
      "rootCause": null,
      "proposedFix": "Add ExecutionStatus as an exported search attribute for archival providers",
      "workaround": "Users can examine event history to determine execution status of archived workflows",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add ExecutionStatus as one of the exported search attributes for archival",
      "number": 8648,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:34.897Z"
    },
    {
      "summary": "Request for a plugin architecture to support custom authentication in Temporal Server without requiring recompilation. User wants to deploy a pre-built server image from registry with pluggable auth instead of compiling the server with custom authentication code.",
      "category": "feature",
      "subcategory": "auth-plugin-architecture",
      "apis": [],
      "components": [
        "authentication",
        "server-configuration",
        "plugin-system",
        "deployment"
      ],
      "concepts": [
        "plugin-architecture",
        "authentication-customization",
        "runtime-extensibility",
        "deployment-convenience",
        "performance-trade-offs",
        "code-compilation"
      ],
      "severity": "medium",
      "userImpact": "Users currently must recompile Temporal Server to use custom authentication, making it difficult to deploy pre-built server images from registries.",
      "rootCause": "Current authentication plugin interface requires compile-time implementation, preventing runtime customization without recompilation.",
      "proposedFix": "Implement a plugin architecture using one of three approaches: Go's plugin package, HashiCorp's go-plugin framework, or WebAssembly (wazero). User suggests wasm might be feasible with upcoming CGO performance improvements in Go 1.26.",
      "workaround": "Compile temporal-server with custom auth code included, though this is operationally inconvenient for container deployments.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Curious to hear what the community thinks. Either way, would like to get away from compiling temporal and simply pull an image from the registry.",
      "number": 8636,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:37.328Z"
    },
    {
      "summary": "Creating a search attribute that already exists returns success instead of an error. The server logs a warning but returns nil, preventing the CLI from informing users that the operation failed due to duplication.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "frontend",
        "operator-handler",
        "search-attributes"
      ],
      "concepts": [
        "idempotency",
        "error-handling",
        "deduplication",
        "dual-visibility"
      ],
      "severity": "medium",
      "userImpact": "Users cannot detect when they attempt to create duplicate search attributes, making it unclear whether operations succeeded or failed.",
      "rootCause": "The operator handler logs a warning when a search attribute already exists but returns nil instead of a service error, preventing proper error propagation to clients.",
      "proposedFix": "Return a serviceError when attempting to create a search attribute that already exists, rather than silently returning nil.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "In this loop server logs the warn message with errSearchAttributeAlreadyExistsMessage but because it does not add anything to map it just returns nil",
      "number": 8631,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:24.930Z"
    },
    {
      "summary": "Request to allow workflows to specify benign error exhaustion thresholds for activities, enabling suppression of error logs and metrics for the first N retry attempts while treating subsequent failures as non-benign. This allows activities that poll services to avoid noisy error reporting during expected retry periods.",
      "category": "feature",
      "subcategory": "activity-retry-policy",
      "apis": [
        "ActivityOptions"
      ],
      "components": [
        "activity-executor",
        "error-handling",
        "retry-logic",
        "interceptor"
      ],
      "concepts": [
        "retry",
        "benign-errors",
        "activity-polling",
        "error-suppression",
        "metrics",
        "thresholds"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot currently suppress error logs and metrics for expected activity retries when polling services, requiring workarounds with context propagators and interceptors.",
      "rootCause": "Activity benign error classification is controlled at the activity level, not at the activity invocation site in the workflow, preventing flexible per-call configuration of error handling behavior.",
      "proposedFix": "Add benign exhaustion threshold configuration to ActivityOptions alongside retry policy, with server-side or interceptor-based logic to classify exceptions as BENIGN until thresholds are breached.",
      "workaround": "Use ContextPropagator and WorkflowThreadLocal to pass benign retry policy from workflow to activity, with activity-side interceptor transforming exceptions to BENIGN when under thresholds.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        704
      ],
      "keyQuote": "A workflow will call an activity that polls a service. We don't want error logs and metrics for the first, say, N activity retry attempts, but we want it thereafter.",
      "number": 8611,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:24.374Z"
    },
    {
      "summary": "Add opt-in ability to retrieve workflow input and output (if closed) directly from describe workflow API without requiring history crawl. This feature is being added for Nexus operations and standalone activities and should be extended to workflows.",
      "category": "feature",
      "subcategory": "workflow-describe-api",
      "apis": [
        "DescribeWorkflow"
      ],
      "components": [
        "workflow-service",
        "api-server",
        "describe-workflow"
      ],
      "concepts": [
        "input-output-retrieval",
        "api-optimization",
        "workflow-metadata",
        "describe-operation",
        "nexus-parity"
      ],
      "severity": "medium",
      "userImpact": "Users can now retrieve workflow input and output directly from describe workflow without needing to crawl through workflow history, improving performance and API usability.",
      "rootCause": null,
      "proposedFix": "Add opt-in fields to describe workflow response to include workflow input and output when requested.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Some users need workflow input/output without crawling history. This is something that is being added for Nexus operations and standalone activities.",
      "number": 8608,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:25.475Z"
    },
    {
      "summary": "Request to allow TriggerImmediatelyRequest to specify custom arguments when triggering a schedule, enabling one-off runs with different payloads than the start action defines.",
      "category": "feature",
      "subcategory": "schedule-execution",
      "apis": [
        "TriggerImmediatelyRequest"
      ],
      "components": [
        "schedule-service",
        "schedule-trigger",
        "api-protos"
      ],
      "concepts": [
        "schedule-triggers",
        "custom-arguments",
        "payload-override",
        "one-off-execution",
        "schedule-flexibility"
      ],
      "severity": "low",
      "userImpact": "Users cannot trigger scheduled workflows with different arguments than those defined in the schedule's start action, limiting flexibility in one-off executions.",
      "rootCause": null,
      "proposedFix": "Extend TriggerImmediatelyRequest to include an optional arguments field that allows specifying custom payload for the triggered execution.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Currently not possible to trigger schedule with a different payload than what is specified within the start action.",
      "number": 8606,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:11.733Z"
    },
    {
      "summary": "Users request the ability to update Schedule memos to better reconcile their Schedules. The team is planning to implement this enhancement in early 2026.",
      "category": "feature",
      "subcategory": "scheduled-actions",
      "apis": [
        "UpdateSchedule"
      ],
      "components": [
        "schedule-manager",
        "schedule-service"
      ],
      "concepts": [
        "schedule",
        "memo",
        "reconciliation",
        "metadata",
        "update"
      ],
      "severity": "medium",
      "userImpact": "Users can better manage and reconcile their Schedules by updating associated metadata.",
      "rootCause": null,
      "proposedFix": "Enable updating of the Schedule memo field through the Schedule API",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Enable updating of the Schedule memo.",
      "number": 8581,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:12.612Z"
    },
    {
      "summary": "Multiple CVEs detected in Temporal server 1.29 Docker image, including high-severity vulnerabilities in golang.org/x/net and OpenTelemetry instrumentation packages. Security patches are available for several dependencies.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "docker-image",
        "dependencies",
        "grpc-instrumentation",
        "http-proxy"
      ],
      "concepts": [
        "vulnerability",
        "cve",
        "security-patch",
        "dependency-management",
        "container-security",
        "supply-chain"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal server 1.29 in production are exposed to known security vulnerabilities that could be exploited by attackers.",
      "rootCause": "Outdated dependencies with known CVEs: golang.org/x/net v0.34.0 (should be 0.36.0), OpenTelemetry contrib v0.36.4 (should be 0.46.0), and other packages not updated to patched versions.",
      "proposedFix": "Update vulnerable dependencies to fixed versions: golang.org/x/net to 0.36.0, go.opentelemetry.io/contrib to 0.46.0, curl to 8.14.1-r2, and openssl to 3.5.4-r0.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "There are some CVEs found from the latest Temporal image: temporalio/server:1.29",
      "number": 8579,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:13.380Z"
    },
    {
      "summary": "Add a new `chasm.NoValue` type that is `*struct{}` to properly represent methods that don't have a return value in the Chasm language/framework.",
      "category": "feature",
      "subcategory": "chasm-types",
      "apis": [],
      "components": [
        "chasm",
        "type-system"
      ],
      "concepts": [
        "return-values",
        "type-representation",
        "void-types",
        "nil-semantics"
      ],
      "severity": "low",
      "userImpact": "Enables developers to properly express and handle methods without return values in Chasm code.",
      "rootCause": null,
      "proposedFix": "Create a `chasm.NoValue` type as `*struct{}` that returns `nil` from methods without return values.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Closed by PR #8568, which implemented the NoValue type.",
      "related": [
        8499,
        8568
      ],
      "keyQuote": "we need to add a `chasm.NoValue` type that is `*struct{}` and just return `nil` from methods that don't have a return value.",
      "number": 8566,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:00.968Z"
    },
    {
      "summary": "temporal-sql-tool requires client certificates to establish TLS connections, even when the server doesn't require them. The tool fails with 'must provide tls certs to use' error when TLS is enabled but client certs are not provided.",
      "category": "bug",
      "subcategory": "cli-tools",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "tls-configuration",
        "client-connection"
      ],
      "concepts": [
        "tls",
        "client-certificates",
        "database-connection",
        "configuration",
        "ssl-verification"
      ],
      "severity": "medium",
      "userImpact": "Users cannot run temporal-sql-tool with TLS-enabled databases that don't require client certificates, blocking schema setup operations.",
      "rootCause": "temporal-sql-tool incorrectly requires client certificates for all TLS connections instead of making them optional when the server doesn't mandate them.",
      "proposedFix": null,
      "workaround": "Upgrade to temporalio/admin-tools:1.29.0-tctl-1.18.4-cli-1.4.2 or later, where this issue is resolved.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was fixed in a later version (admin-tools:1.29.0-tctl-1.18.4-cli-1.4.2). User confirmed the problem no longer exists in the latest tag.",
      "related": [],
      "keyQuote": "Fails without a client cert, despite my server not needing or desiring a client cert, but it does require TLS.",
      "number": 8546,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:28:00.610Z"
    },
    {
      "summary": "Request to add a user_identity field to HistoryEvent that captures the identity of users who initiated events like signals and cancellation requests. This field should be set by the server based on context and work consistently across OSS and cloud deployments.",
      "category": "feature",
      "subcategory": "history-events",
      "apis": [
        "HistoryEvent"
      ],
      "components": [
        "history",
        "server",
        "events",
        "authorization"
      ],
      "concepts": [
        "user-identity",
        "signals",
        "cancellation",
        "history-events",
        "server-context",
        "authorization"
      ],
      "severity": "medium",
      "userImpact": "Users will be able to track which users initiated events like signals and cancellation requests, enabling better auditing and authorization decisions.",
      "rootCause": null,
      "proposedFix": "Add a user_identity field to HistoryEvent that is set by the server for user-initiated events (signals, cancellation requests) based on context.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Need something like `HistoryEvent.user_identity` that is set for any events that are initiated by users (so signals and cancellation requests and such, but not timer firing).",
      "number": 8538,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:58.971Z"
    },
    {
      "summary": "SDK needs first_execution_run_id field in StartWorkflowExecutionResponse and WorkflowExecutionAlreadyStartedFailure to properly handle workflow conflicts. Currently when a workflow start conflicts with an existing workflow using use-existing policy, the SDK lacks the first execution run ID needed for subsequent operations like cancel and signal.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "StartWorkflowExecution",
        "StartWorkflowExecutionResponse",
        "WorkflowExecutionAlreadyStartedFailure"
      ],
      "components": [
        "api",
        "workflow-execution",
        "conflict-handling",
        "persistence"
      ],
      "concepts": [
        "conflict-resolution",
        "workflow-identity",
        "first-execution",
        "run-id",
        "use-existing-policy",
        "workflow-binding"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably track and manage workflows that encounter start conflicts with use-existing policy, limiting their ability to cancel or signal such workflows.",
      "rootCause": "The API responses for conflict scenarios (started=false or WorkflowExecutionAlreadyStartedFailure) do not include the first_execution_run_id, requiring persistence layer changes to track this information.",
      "proposedFix": "Add first_execution_run_id field to StartWorkflowExecutionResponse and WorkflowExecutionAlreadyStartedFailure responses; store first execution run ID in persistence record at request handling time.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "SDK needs `StartWorkflowExecutionResponse.first_execution_run_id` for on-conflict-use-existing situations, and `WorkflowExecutionAlreadyStartedFailure.first_execution_run_id` for other already-started situations.",
      "number": 8537,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:49.072Z"
    },
    {
      "summary": "Multiple CVEs discovered in the temporalio/admin-tools:1.29 Docker image, including critical and high-severity vulnerabilities in sqlite, golang.org/x/net, and OpenTelemetry dependencies that require package updates.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "docker-image",
        "admin-tools",
        "dependency-management"
      ],
      "concepts": [
        "security-vulnerability",
        "cve",
        "dependency-update",
        "docker-image",
        "vulnerability-scanning",
        "package-management"
      ],
      "severity": "critical",
      "userImpact": "Users running the admin-tools Docker image are exposed to critical and high-severity security vulnerabilities that could be exploited.",
      "rootCause": "Outdated dependencies with known CVEs in the Docker image build, including sqlite 3.49.2-r0, golang.org/x/net v0.35.0, and go.opentelemetry.io/contrib v0.36.4.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Team indicated they are reworking Docker images to address these vulnerabilities soon.",
      "related": [],
      "keyQuote": "We are reworking our docker images and many of these issues will be addressed soon.",
      "number": 8535,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:48.975Z"
    },
    {
      "summary": "User attempting to run Temporal Server 1.29.0 on MySQL 5.7 encounters SQL schema initialization failures. The issue is that MySQL 5.7 is no longer supported and was deprecated since Server version 1.20.",
      "category": "question",
      "subcategory": "database-compatibility",
      "apis": [],
      "components": [
        "database",
        "schema-initialization",
        "mysql"
      ],
      "concepts": [
        "version-compatibility",
        "deprecation",
        "database-requirements",
        "schema-migration",
        "minimum-version"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to run Temporal Server on unsupported MySQL versions will encounter schema initialization failures.",
      "rootCause": "MySQL 5.7 is deprecated and no longer supported; Server 1.20+ requires a newer MySQL version.",
      "proposedFix": null,
      "workaround": "Upgrade MySQL to a supported version per the official documentation.",
      "resolution": "invalid",
      "resolutionDetails": "User was directed to upgrade MySQL to a supported version as per official requirements documented since Server 1.20.",
      "related": [],
      "keyQuote": "we require a minimum version of MySql. V5.7 has been deprecated since OSS version V1.20.",
      "number": 8527,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:47.525Z"
    },
    {
      "summary": "Test issue used for verifying GitHub syncing functionality.",
      "category": "other",
      "subcategory": "testing",
      "apis": [],
      "components": [
        "github-sync",
        "issue-tracking"
      ],
      "concepts": [
        "testing",
        "syncing",
        "verification",
        "github-integration"
      ],
      "severity": "low",
      "userImpact": "This is a test issue with no real user impact.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Test issue created for verification purposes",
      "related": [],
      "keyQuote": "Test issue.",
      "number": 8514,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:33.030Z"
    },
    {
      "summary": "When a scheduled action fails and sets ContinuedFailure payload, a subsequent scheduled action that succeeds with a null payload fails to clear the ContinuedFailure state, causing it to continue propagating to future scheduled actions.",
      "category": "bug",
      "subcategory": "schedules",
      "apis": [],
      "components": [
        "scheduler",
        "workflow",
        "activities",
        "scheduled-actions"
      ],
      "concepts": [
        "failure-handling",
        "state-propagation",
        "scheduled-execution",
        "null-payload",
        "error-recovery"
      ],
      "severity": "medium",
      "userImpact": "Users experience unexpected ContinuedFailure errors in subsequent scheduled actions after a successful execution without a payload, breaking error recovery expectations.",
      "rootCause": "Scheduled action workflow doesn't properly clear ContinuedFailure payload when a subsequent action completes successfully with a null payload; should check completed workflow's execution status to determine which fields to propagate.",
      "proposedFix": "Check the completed workflow's execution status to determine which fields to propagate, rather than relying on payload presence.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If a scheduled action fails, setting ContinuedFailure's payload, and then a subsequent scheduled action succeeds without returning a payload (a null payload), ContinuedFailure will continue to propagate",
      "number": 8490,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:36.202Z"
    },
    {
      "summary": "MySQL INSERT ... ON DUPLICATE KEY UPDATE queries on the custom_search_attributes table exhibit extremely high lock times and cause database crashes when the table grows beyond 30 million records. User reports that migrating to Elasticsearch resolved the issue, suggesting MySQL may have fundamental scalability limitations for this operation.",
      "category": "bug",
      "subcategory": "visibility-store-mysql",
      "apis": [],
      "components": [
        "visibility-store",
        "mysql-persistence",
        "custom-search-attributes"
      ],
      "concepts": [
        "database-locking",
        "scalability",
        "query-performance",
        "insert-on-duplicate-key",
        "mysql-limits"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal with MySQL visibility store experience database crashes and performance degradation when custom search attributes table exceeds 30 million records.",
      "rootCause": "INSERT ... ON DUPLICATE KEY UPDATE statement exhibits poor performance characteristics on large MySQL tables (30M+ rows), causing excessive locking and semaphore timeouts in InnoDB.",
      "proposedFix": null,
      "workaround": "Migrate visibility store to Elasticsearch, which does not exhibit the same locking behavior with large datasets.",
      "resolution": "wontfix",
      "resolutionDetails": "User confirmed the issue is a MySQL scalability limitation rather than a Temporal bug. The recommended solution is to use Elasticsearch for visibility stores at scale, not to optimize the MySQL query.",
      "related": [],
      "keyQuote": "Maybe insert ... on duplicate key update is not good for large table",
      "number": 8475,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:36.105Z"
    },
    {
      "summary": "AWS SDK Go dependency (github.com/aws/aws-sdk-go v1.55.6) contains known vulnerabilities including in-band key negotiation and CBC padding oracle issues in the S3 Crypto SDK. An open PR bumps the version to 1.55.8 which includes fixes.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "aws-sdk"
      ],
      "concepts": [
        "security-vulnerability",
        "cryptography",
        "s3-encryption",
        "dependency-upgrade"
      ],
      "severity": "high",
      "userImpact": "Applications using Temporal with AWS S3 functionality are exposed to cryptographic vulnerabilities that could compromise data confidentiality and integrity.",
      "rootCause": "AWS SDK Go library contains unpatched security vulnerabilities in the S3 Crypto SDK (GO-2022-0635 and GO-2022-0646)",
      "proposedFix": "Upgrade AWS SDK to version 1.55.8 or later which includes fixes for the identified vulnerabilities",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "PR #7884 bumps AWS SDK to 1.55.8 which includes the security fixes, merging in progress",
      "related": [
        7884
      ],
      "keyQuote": "GO-2022-0635: In-band key negotiation issue in AWS S3 Crypto SDK for golang in github.com/aws/aws-sdk-go",
      "number": 8465,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:18.688Z"
    },
    {
      "summary": "Request to add documentation for deploying self-hosted Temporal on cloud environments (AWS EKS and Azure AKS) with specific guidance on known configuration issues and gotchas.",
      "category": "docs",
      "subcategory": "cloud-deployment",
      "apis": [],
      "components": [
        "deployment",
        "archival",
        "visibility-store",
        "database",
        "postgres"
      ],
      "concepts": [
        "self-hosting",
        "cloud-deployment",
        "EKS",
        "AKS",
        "TLS",
        "archival-storage",
        "database-configuration",
        "postgres-extensions"
      ],
      "severity": "medium",
      "userImpact": "Users deploying self-hosted Temporal on AWS EKS and Azure AKS encounter undocumented configuration gotchas that require workarounds, increasing deployment complexity and time-to-value.",
      "rootCause": "Lack of documented guidance for cloud-specific deployment requirements and constraints (Azure TLS enforcement, missing database extensions, archival support limitations).",
      "proposedFix": "Add comprehensive documentation covering AWS EKS and Azure AKS deployment with specific sections on TLS configuration, database extension requirements (BTREE_GIN for PostgreSQL), and archival support limitations.",
      "workaround": "For Azure TLS requirement: either enable TLS with Azure-managed certificates or disable TLS via Postgres server parameters (require_secure_transport). For visibility store: manually enable BTREE_GIN extension in PostgreSQL.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Lack of archival support for Azure with storage containers. AKS + Azure flexible postgres server fails with lots of tiny errors",
      "number": 8461,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:19.772Z"
    },
    {
      "summary": "Poller autoscaling scale-down behavior is too rigid, requiring sustained low throughput (below 1 task/second per poller) to trigger scaling down. Users want more aggressive scale-down behavior to reduce unnecessary pollers and avoid operational complexity.",
      "category": "feature",
      "subcategory": "poller-autoscaling",
      "apis": [],
      "components": [
        "poller",
        "matching",
        "autoscaling",
        "task-queue"
      ],
      "concepts": [
        "scaling",
        "backlog",
        "throughput",
        "resource-optimization",
        "configuration-tuning",
        "latency"
      ],
      "severity": "medium",
      "userImpact": "Users are unable to efficiently reduce poller counts even when a single poller can handle multiple tasks per second, forcing manual tuning or acceptance of resource waste.",
      "rootCause": "Scale-down only triggers when a poller is idle for PollerScalingWaitTime (1s default) without matching tasks, which means N pollers won't scale down if there are N+ events per second, regardless of individual poller capacity.",
      "proposedFix": "Two approaches suggested: (1) Use LIFO queue instead of FIFO for pollers so newest pollers get tasks first, letting old pollers timeout naturally, or (2) Scale down when backlog is less than PollerScalingBacklogAgeScaleUp threshold.",
      "workaround": "Reduce PollerScalingWaitTime dynamic config from 1s to 200-500ms to allow faster scale-down with higher throughput, or use ResourceBasedTuner for additional slot tuning.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers closed the issue noting pollers are cheap and the main concern is usually having enough pollers, not minimizing them. Users can adjust configuration values or resource-based tuning if needed.",
      "related": [],
      "keyQuote": "This seems to mean that N pollers would never scale down if there were at least N events per second. This feels like a bottleneck because presumably a single poller can handle more than 1 request per second.",
      "number": 8447,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:23.547Z"
    },
    {
      "summary": "User attempted to create a one-time schedule using only `start_at` without an interval or calendar spec, expecting it to trigger once. The schedule was created successfully but the workflow never executed. The root cause was a misunderstanding of the API`start_at` and `end_at` only set bounds, not the actual trigger times.",
      "category": "question",
      "subcategory": "schedule-execution",
      "apis": [
        "create_schedule",
        "Schedule",
        "ScheduleSpec",
        "ScheduleActionStartWorkflow",
        "ScheduleState"
      ],
      "components": [
        "schedule-executor",
        "temporal-client",
        "workflow-scheduling"
      ],
      "concepts": [
        "scheduling",
        "one-time execution",
        "calendar spec",
        "interval spec",
        "start time bounds",
        "schedule matching"
      ],
      "severity": "low",
      "userImpact": "Users may create schedules that appear valid but never execute, leading to confusion about API behavior.",
      "rootCause": "User misunderstood that `start_at`/`end_at` only set temporal bounds; an actual interval or calendar spec is required to define matching times.",
      "proposedFix": "Use a calendar spec that matches a single timestamp for one-time schedules, or use the delayed start feature on workflows directly.",
      "workaround": "For one-time schedules, use the delayed start feature on workflows instead of the scheduler, which is more efficient.",
      "resolution": "wontfix",
      "resolutionDetails": "Not a buguser misunderstood the API. The schedule requires either an interval or calendar spec to define trigger times; start_at/end_at only bound when matching can occur.",
      "related": [],
      "keyQuote": "Start time and end time only set bounds on allowed matching times. You still need either an interval or calendar spec to actually set the time(s) to match.",
      "number": 8439,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:07.113Z"
    },
    {
      "summary": "PostgreSQL connection configuration does not support sslmode=verify-ca because the code unconditionally sets sslmode to require/disable/verify-full based on TLS settings, causing a duplicate connection attribute panic when users try to override it.",
      "category": "bug",
      "subcategory": "postgresql-ssl-configuration",
      "apis": [],
      "components": [
        "postgresql-session",
        "sql-persistence",
        "tls-configuration"
      ],
      "concepts": [
        "ssl-mode",
        "connection-attributes",
        "certificate-verification",
        "postgresql-driver",
        "configuration-override"
      ],
      "severity": "high",
      "userImpact": "Users cannot use PostgreSQL's verify-ca SSL mode, which provides certificate verification without hostname verification, forcing them to use less secure or overly strict alternatives.",
      "rootCause": "The buildDSNAttr function in postgresql/session/session.go unconditionally sets sslmode based on cfg.TLS settings, then iterates over cfg.ConnectAttributes and panics if any key already exists in the parameters, making it impossible to override or use alternate SSL modes.",
      "proposedFix": "Add an EnableCaVerification field to TLS config and modify the SSL mode logic to set sslMode to verify-ca when this flag is enabled, or allow ConnectAttributes to override the auto-set sslmode instead of panicking.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was resolved by implementing support for verify-ca SSL mode in the PostgreSQL connection configuration.",
      "related": [],
      "keyQuote": "duplicate connection attr: sslmode:require, sslmode:verify-ca when cfg.TLS.Enabled == true",
      "number": 8436,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:07.345Z"
    },
    {
      "summary": "User requests changing the log level of dynamic config update messages from info to warn in file_based_client so they can see important config changes without enabling verbose info-level logging.",
      "category": "feature",
      "subcategory": "dynamic-config",
      "apis": [],
      "components": [
        "file_based_client",
        "dynamicconfig",
        "logging"
      ],
      "concepts": [
        "logging",
        "dynamic-configuration",
        "observability",
        "configuration-management"
      ],
      "severity": "low",
      "userImpact": "Users must enable info-level logging to see important dynamic config change notifications, resulting in excessive log volume.",
      "rootCause": null,
      "proposedFix": "Change log level of dynamic config updated message from info to warn in file_based_client.go",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Message can be pretty important to have in logs to know that dynamic config changes have been applied (and when according to timestamp)",
      "number": 8432,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:27:06.404Z"
    },
    {
      "summary": "Docker Compose deployment fails because TEMPORAL_ADMINTOOLS_VERSION is set to 1.29.0 in the .env file, but the temporalio/admin-tools Docker image only has a 1.29 tag available. The issue is actually a configuration error in the docker-compose repository, not in the temporal repository.",
      "category": "bug",
      "subcategory": "docker-deployment",
      "apis": [],
      "components": [
        "docker-image",
        "admin-tools",
        "docker-compose"
      ],
      "concepts": [
        "version-mismatch",
        "docker-tagging",
        "deployment-configuration",
        "manifest-resolution"
      ],
      "severity": "medium",
      "userImpact": "Users following the Docker Compose deployment instructions encounter a manifest error and cannot start the admin-tools container.",
      "rootCause": "The .env file in temporalio/docker-compose specifies TEMPORAL_ADMINTOOLS_VERSION=1.29.0, but the Docker image uses major.minor format (1.29) without patch versions.",
      "proposedFix": "Update the TEMPORAL_ADMINTOOLS_VERSION in the docker-compose repository's .env file from 1.29.0 to 1.29, or change the Docker tagging strategy to include patch versions.",
      "workaround": "Manually edit the .env file to change TEMPORAL_ADMINTOOLS_VERSION from 1.29.0 to 1.29 before running docker-compose.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The TEMPORAL_ADMINTOOLS_VERSION=1.29.0 you're referring to is in the temporalio/docker-compose repository's .env file, not in this repository",
      "number": 8431,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:42.614Z"
    },
    {
      "summary": "Activity heartbeats sent very close to activity failure can be lost, with the final heartbeat details not being recorded in the workflow description. The issue involves a race condition where heartbeat details may be cleared or not properly flushed when an activity completes or fails near the time a heartbeat is sent.",
      "category": "bug",
      "subcategory": "activity-heartbeat",
      "apis": [
        "RecordActivityTaskHeartbeat",
        "RespondActivityTaskFailedResponse"
      ],
      "components": [
        "activity-heartbeat-manager",
        "worker",
        "activity-executor"
      ],
      "concepts": [
        "heartbeat",
        "race-condition",
        "activity-lifecycle",
        "state-management",
        "timing"
      ],
      "severity": "high",
      "userImpact": "Users may lose critical heartbeat progress information when activities complete or fail at the same time heartbeats are being sent.",
      "rootCause": "A race condition where heartbeat details are not properly maintained in mutable state when checked immediately before activity failure, causing the final heartbeat to not be flushed or recorded.",
      "proposedFix": "Use the `last_heartbeat_details` field in `RespondActivityTaskFailedResponse` to deliver the last heartbeat details, and add a capability check for this RPC method.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1019
      ],
      "keyQuote": "the describe workflow call never reflects that final detail... the final heartbeat you are completing the RPC because I get the response back",
      "number": 8376,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:54.760Z"
    },
    {
      "summary": "Task queue validator intermittently rejects valid root task queue partitions on poll requests with an error claiming the name starts with reserved prefix /_sys/. While workers retry and eventually succeed, the error appears in logs and causes concern.",
      "category": "bug",
      "subcategory": "task-queue-validation",
      "apis": [
        "PollWorkflowTaskQueue",
        "PollActivityTaskQueue"
      ],
      "components": [
        "task-queue-validator",
        "worker-poller",
        "request-handler"
      ],
      "concepts": [
        "validation",
        "task-queue-names",
        "reserved-prefixes",
        "partition-routing",
        "intermittent-failure"
      ],
      "severity": "medium",
      "userImpact": "Users see alarming error messages in logs and worker failures occur intermittently, though the system eventually recovers with retries.",
      "rootCause": "Task queue validator incorrectly identifies valid root task queue partitions as having reserved /_sys/ prefix under certain conditions.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in server version 1.28.0",
      "related": [],
      "keyQuote": "task queue name cannot start with reserved prefix /_sys/ ... Once this error happens, the worker goes into the Failed state and stop processing activities/workflows.",
      "number": 8363,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:43.987Z"
    },
    {
      "summary": "Workers currently use static concurrency limits that don't account for varying workload profiles (CPU-bound vs IO-bound vs memory-heavy tasks), creating a false trade-off between resource utilization and stability. The proposal is to introduce resource-aware concurrency controls that dynamically adjust task polling based on pod CPU/memory thresholds, aligning with Kubernetes autoscaling.",
      "category": "feature",
      "subcategory": "worker-concurrency",
      "apis": [],
      "components": [
        "worker",
        "concurrency-control",
        "task-poller",
        "slot-supplier"
      ],
      "concepts": [
        "resource-awareness",
        "dynamic-scaling",
        "backpressure",
        "pod-utilization",
        "kubernetes-integration",
        "cost-efficiency",
        "autoscaling"
      ],
      "severity": "medium",
      "userImpact": "Users must choose between wasting resources (setting limits too low) or risking pod overload and OOM errors (setting limits too high), compromising both cost-efficiency and reliability.",
      "rootCause": "Static concurrency limits assume uniform workloads but don't adapt to actual pod resource availability or Kubernetes autoscaling signals.",
      "proposedFix": "Introduce resource-aware concurrency controls: (1) stop polling when pod CPU/memory reaches configurable thresholds (70-80%), (2) scale concurrency dynamically based on available resources, (3) expose hooks for custom resource metrics or scaling policies.",
      "workaround": "Use resource-based slot suppliers as documented in Temporal docs, which offer auto-tuning capabilities for fluctuating workloads and protection from OOM.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Pods never overload themselves. Idle capacity is minimized. Scaling is smooth and cost-efficient.",
      "number": 8356,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:31.057Z"
    },
    {
      "summary": "Support relative time syntax in visibility store queries to allow searching workflows by relative time periods (e.g., '< 5m', '>= 1 day') instead of requiring absolute timestamps.",
      "category": "feature",
      "subcategory": "visibility-store",
      "apis": [],
      "components": [
        "visibility-store",
        "query-engine",
        "timestamp-processing"
      ],
      "concepts": [
        "relative-time",
        "timestamp",
        "query-syntax",
        "search",
        "workflow-filtering",
        "duration"
      ],
      "severity": "medium",
      "userImpact": "Users must currently calculate and provide absolute timestamps for visibility queries instead of using intuitive relative time expressions.",
      "rootCause": "Visibility store currently only supports absolute timestamp values in search queries, lacking Duration-based comparison support.",
      "proposedFix": "Implement signed Duration value support to compare with current timestamp at query execution time for DateTime search attributes.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        8498
      ],
      "keyQuote": "I want to be able to query with < '5m' or >= '1 day' where it's a relative time and the absolute timestamp is calculated at the time of query execution.",
      "number": 8349,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:29.472Z"
    },
    {
      "summary": "Temporal Server fails to start when MySQL is configured with BINLOG_FORMAT=STATEMENT and READ_COMMITTED isolation level due to a MySQL constraint that requires REPEATABLE-READ isolation with STATEMENT binlog format.",
      "category": "bug",
      "subcategory": "database-configuration",
      "apis": [],
      "components": [
        "mysql-persistence",
        "database-initialization",
        "cluster-metadata"
      ],
      "concepts": [
        "binlog-format",
        "transaction-isolation",
        "mysql-configuration",
        "session-settings",
        "database-compatibility"
      ],
      "severity": "high",
      "userImpact": "Users cannot start Temporal Server with MySQL when BINLOG_FORMAT=STATEMENT is set, resulting in startup failure with cryptic error message about binary logging constraints.",
      "rootCause": "Temporal sets default MySQL session isolation to READ_COMMITTED, which is incompatible with BINLOG_FORMAT=STATEMENT. MySQL requires REPEATABLE-READ (or more conservative) isolation level when using STATEMENT binlog format.",
      "proposedFix": "Users can override session isolation via connectAttributes in the persistence configuration to set transaction_isolation to 'REPEATABLE-READ'.",
      "workaround": "Configure connectAttributes in mysql-default and mysql-visibility datastores to override transaction_isolation to 'REPEATABLE-READ', or switch to ROW binlog format.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed with proposed solution. STATEMENT binlog is noted as experimental in Temporal, with recommendation to use ROW binlog with READ-COMMITTED instead. This is a user configuration issue rather than a product bug.",
      "related": [],
      "keyQuote": "Using binlog_format=STATEMENT would require the isolation to be REPEATABLE-READ (or more conservative) else it breaks transactional guarantees.",
      "number": 8323,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:30.862Z"
    },
    {
      "summary": "Temporal Server logs a WARN level message every time a task runs without an explicit task queue kind, despite kind having a well-documented default of 'Normal'. This creates excessive log noise, especially for SDKs like Ruby that don't explicitly set the kind parameter.",
      "category": "bug",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "frontend",
        "task-queue",
        "logging"
      ],
      "concepts": [
        "logging-verbosity",
        "task-queue-kind",
        "default-behavior",
        "log-noise",
        "warning-suppression"
      ],
      "severity": "medium",
      "userImpact": "Users experience excessive warning logs in server output when running tasks without explicit task queue kind, making it difficult to identify actual issues in logs.",
      "rootCause": "Server logs a warning for unspecified task queue kind even though the field has a documented default value of 'Normal', treating a normal use case as a warning condition.",
      "proposedFix": "Either don't emit the log line by default, or move it to a verbose/debug logging option.",
      "workaround": "Explicitly set task queue kind to 'Normal' in the Ruby SDK or patch it to do so automatically.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        5903
      ],
      "keyQuote": "kind is well-documented to default to 'Normal', so it seems odd to warn about this behavior.",
      "number": 8322,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:17.712Z"
    },
    {
      "summary": "User asks how Temporal can track and control versions of activity code and worker implementations, enable rollback capabilities, and provide visibility into code hashes and environment details for reproducibility and disaster recovery.",
      "category": "question",
      "subcategory": "worker-versioning",
      "apis": [],
      "components": [
        "worker",
        "activity"
      ],
      "concepts": [
        "versioning",
        "deployment",
        "rollback",
        "code-tracking",
        "reproducibility",
        "environment-management"
      ],
      "severity": "low",
      "userImpact": "Users need clarity on how to version control their activity code and workers, track what code is running, and perform safe rollbacks.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Closing as invalid - the user's question was already answered with a reference to the worker versioning and deployment APIs documentation. No response from original poster after clarification.",
      "related": [],
      "keyQuote": "How can we control each individual code components to be frozen to a fixed version, or at least be notified about its environments and code hashes",
      "number": 8316,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:16.025Z"
    },
    {
      "summary": "User reports discrepancy between Temporal.io documentation and SDK inline docs regarding WorkflowIdReusePolicy.TERMINATE_IF_RUNNING - docs show it as valid option but code suggests it's deprecated in favor of WorkflowIdReusePolicy + WorkflowIdConflictPolicy combination. User seeks clarification on whether this policy can be used for child workflows that need to reset with their parent.",
      "category": "question",
      "subcategory": "workflow-reuse-policy",
      "apis": [
        "WorkflowIdReusePolicy",
        "WorkflowIdConflictPolicy",
        "StartWorkflow"
      ],
      "components": [
        "workflow-options",
        "child-workflow",
        "workflow-id-management"
      ],
      "concepts": [
        "workflow-reuse-policy",
        "conflict-policy",
        "child-workflow",
        "workflow-reset",
        "documentation-consistency",
        "API-deprecation"
      ],
      "severity": "medium",
      "userImpact": "Users are confused about which WorkflowIdReusePolicy settings are valid for child workflows, potentially leading to incorrect workflow configuration during resets.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "User currently relies on TERMINATE_IF_RUNNING for the intended behavior with child workflows, though clarification on alternatives is needed.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue appears to be closed as the user's specific use case (child workflows) has documented limitations on server side regarding conflict policies support.",
      "related": [
        1628
      ],
      "keyQuote": "Currently, it looks like the only way I can configure this behaviour is with `WorkflowIdReusePolicy.TERMINATE_IF_RUNNING`",
      "number": 8314,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:16.166Z"
    },
    {
      "summary": "Container built from Temporal 1.28.1 fails to start when running on a host system with a proxy due to internal frontend connection failures. The issue is caused by gRPC library version changes that broke NO_PROXY environment variable handling.",
      "category": "bug",
      "subcategory": "grpc-proxy-handling",
      "apis": [],
      "components": [
        "grpc-client",
        "internal-frontend",
        "container-deployment"
      ],
      "concepts": [
        "proxy-configuration",
        "environment-variables",
        "grpc-connection",
        "version-upgrade",
        "container-networking"
      ],
      "severity": "high",
      "userImpact": "Users upgrading to 1.28.1 in containerized environments with proxy configurations experience service startup failures due to broken internal service connections.",
      "rootCause": "gRPC library upgrade from v1.70.0 (1.27.1) to v1.71.0 (1.27.2/1.28.1) introduced bugs where NO_PROXY environment variable is no longer respected on custom resolver results (grpc-go#8327) and grpc.Dial fails with https_proxy set (grpc-go#8207).",
      "proposedFix": "Upgrade gRPC library to v1.72.2 or later which contains fixes for both grpc-go#8327 and grpc-go#8207.",
      "workaround": "Apply a git patch to the gRPC library prior to build to restore NO_PROXY environment variable handling.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in Temporal 1.29 release which includes gRPC v1.72.2 with the necessary patches.",
      "related": [],
      "keyQuote": "Temporal 1.27.1 uses google.golang.org/grpc v1.70.0, which works Temporal 1.27.2/v1.27.3 uses google.golang.org/grpc v1.71.0, which doesn't work",
      "number": 8305,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:04.421Z"
    },
    {
      "summary": "User reports that setting InitialInterval to 10 minutes in the retry policy does not trigger retries as expected. The retry strategy appears ineffective when the initial interval is set to a longer duration.",
      "category": "bug",
      "subcategory": "retry-policy",
      "apis": [
        "ExecuteActivity",
        "WithActivityOptions"
      ],
      "components": [
        "activity-executor",
        "retry-mechanism",
        "workflow-context"
      ],
      "concepts": [
        "retry",
        "timeout",
        "InitialInterval",
        "backoff",
        "activity-execution",
        "scheduling"
      ],
      "severity": "high",
      "userImpact": "Users cannot configure longer retry intervals for activities, breaking retry logic for workflows that require delayed retry attempts.",
      "rootCause": "The retry policy's InitialInterval parameter is not being respected by the activity execution engine when set to values longer than the StartToCloseTimeout.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved, likely through validation of retry policy parameters or engine fix to respect InitialInterval configuration.",
      "related": [],
      "keyQuote": "InitialInterval is set to 10 minutes, Such setting will not trigger a retry",
      "number": 8304,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:02.636Z"
    },
    {
      "summary": "Feature request for a CLI command to display Temporal services, their metadata (such as IP addresses), and applied dynamic configuration. This would help operators manage service-level settings at scale without needing to dig through configuration files.",
      "category": "feature",
      "subcategory": "cli-commands",
      "apis": [],
      "components": [
        "cli",
        "service-discovery",
        "dynamic-configuration"
      ],
      "concepts": [
        "service-metadata",
        "operational-visibility",
        "configuration-management",
        "multi-instance-deployment",
        "dynamic-settings"
      ],
      "severity": "medium",
      "userImpact": "Operators gain direct visibility into service configurations and metadata, reducing the need to access configuration files or environment variables and improving error-prone management at scale.",
      "rootCause": null,
      "proposedFix": "Add a service details command to the Temporal CLI that returns service information including IPs and dynamic configuration in JSON format, potentially addressed by PR #8214.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #421 and addressed by PR #8214.",
      "related": [
        421,
        8214
      ],
      "keyQuote": "This deserves to be a part of the CLI because it is information that operators would benefit from having up-front rather than needing to dig into from configuration files",
      "number": 8303,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:26:01.665Z"
    },
    {
      "summary": "Enable Nexus to work seamlessly without configuration by allowing temporal://system URLs as callbacks and automatically dispatching them internally. This involves adding a callback token to the Nexus specification and SDKs, then making temporal://system the default callback URL behind a dynamic config.",
      "category": "feature",
      "subcategory": "nexus-callbacks",
      "apis": [],
      "components": [
        "nexus-operations",
        "callbacks",
        "server-config"
      ],
      "concepts": [
        "callback-token",
        "zero-configuration",
        "system-urls",
        "service-discovery"
      ],
      "severity": "high",
      "userImpact": "Users will be able to use Nexus operations without manually configuring callback URLs, reducing setup complexity and making the feature more accessible.",
      "rootCause": null,
      "proposedFix": "Implement temporal://system URL support as default callback, add CallbackToken field to Nexus SDKs, update SPEC.md with callback token requirements, and provide dynamic config for gradual migration.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        8308
      ],
      "keyQuote": "Allow `temporal://system` URLs always to be attached as callbacks. Dispatch `temporal://system` callback URLs internally, automatically.",
      "number": 8298,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:50.366Z"
    },
    {
      "summary": "Task backlog metrics in the matching service persist incorrectly after task queue partitions move between pods during rebalancing. The matching service should reset gauge metrics to zero when queues are unloaded.",
      "category": "bug",
      "subcategory": "metrics-matching-service",
      "apis": [],
      "components": [
        "matching-service",
        "task-queue",
        "metrics",
        "rebalancing"
      ],
      "concepts": [
        "backlog-metrics",
        "gauge-reset",
        "partition-movement",
        "unloading",
        "pod-rebalancing"
      ],
      "severity": "medium",
      "userImpact": "Incorrect task backlog metrics can mislead monitoring and alerting systems about queue health during task queue rebalancing.",
      "rootCause": "Gauge metrics are not reset to zero when task queue partitions are unloaded during rebalancing between pods.",
      "proposedFix": "Reset gauge metrics to zero when queues are unloaded during rebalancing.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #8375, though local replication was challenging so the fix is being actively monitored in production.",
      "related": [
        8375
      ],
      "keyQuote": "The matching service should reset gauge metrics to zero when queues are unloaded during rebalancing.",
      "number": 8297,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:49.329Z"
    },
    {
      "summary": "User asked whether Python activities can be used within Java workflows. Marked as closed with a solution reference to the temporal-polyglot sample, indicating this is already supported.",
      "category": "question",
      "subcategory": "polyglot-activities",
      "apis": [],
      "components": [
        "activity-executor",
        "worker",
        "polyglot-support"
      ],
      "concepts": [
        "cross-language",
        "activity-implementation",
        "language-interoperability",
        "worker-configuration"
      ],
      "severity": "low",
      "userImpact": "Users can now understand how to mix activity implementations across different languages in their Temporal workflows.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Community member bergundy confirmed this is fully supported and referenced the temporal-polyglot sample as documentation.",
      "related": [],
      "keyQuote": "It is 100% supported and often the reason that users pick Temporal. See this sample for reference: https://github.com/temporalio/temporal-polyglot",
      "number": 8295,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:50.837Z"
    },
    {
      "summary": "TestFixedAddressTranslator test panics with index out of range error on Go 1.24.6+ and 1.25.0. The test assumes DNS resolution of temporal.io domain will succeed, but when DNS resolution fails or returns an empty result, accessing the first element of an empty array causes a panic.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "cassandra-translator",
        "nosql-plugin",
        "persistence",
        "test-suite"
      ],
      "concepts": [
        "DNS-resolution",
        "unit-test",
        "index-bounds",
        "panic",
        "test-isolation"
      ],
      "severity": "medium",
      "userImpact": "Build and test suite fails for users building Temporal 1.28.1+ with Go 1.24.6+, preventing compilation and deployment.",
      "rootCause": "Test assumes DNS resolution of temporal.io will always return at least one address, but when DNS resolution fails or returns empty results (network issues, DNS configuration, or environment constraints), the code attempts to access index [0] of an empty slice, causing a runtime panic.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer acknowledged the issue is caused by DNS resolution dependency in unit test and noted this shouldn't be done in a unit test, but stated there's no motivation to change it. Invited community contribution for alternative implementation.",
      "related": [],
      "keyQuote": "Looks like you have an issue with DNS resolution on your machine where you cannot resolve the temporal.io domain. Arguably this isn't something that should be done in a unit test but there's not much motivation to change that.",
      "number": 8294,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:39.465Z"
    },
    {
      "summary": "PostgreSQL archival service causes significant performance degradation at scale due to inefficient row-by-row DELETE operations. User proposes a partition-aware archival mode that detaches, archives, and drops entire table partitions instead of deleting rows.",
      "category": "feature",
      "subcategory": "archival-performance",
      "apis": [],
      "components": [
        "archival-service",
        "postgres-backend",
        "blob-store"
      ],
      "concepts": [
        "partitioning",
        "table-lifecycle",
        "storage-reclamation",
        "database-maintenance",
        "operational-efficiency",
        "iops-optimization"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal at scale with PostgreSQL experience severe database performance degradation, table bloat, and resource exhaustion due to the archival service's inefficient deletion pattern.",
      "rootCause": "The current archival process relies on row-by-row DELETE operations which do not scale efficiently on massive tables, causing excessive IOPS, CPU consumption, and table bloat.",
      "proposedFix": "Implement partition-aware archival mode for PostgreSQL that: (1) detaches partitions older than retention period, (2) archives entire partition contents to blob store, (3) drops detached partition for instant storage reclamation.",
      "workaround": "Tune dynamic configs: history.archivalBackendMaxRPS and history.archivalTaskBatchSize to throttle the archival process to avoid impacting live workloads.",
      "resolution": "wontfix",
      "resolutionDetails": "Closed as not planned because data has varying lifespans across workflows and namespaces with different retention policies, making time-based partition dropping impractical. Server deduplication requirements also complicate the approach.",
      "related": [],
      "keyQuote": "Data that we write to the DB may have varying lifespans...There is no clear cut to just dropping a data partition by time.",
      "number": 8287,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:38.508Z"
    },
    {
      "summary": "When a workflow is created via SignalWithStart and has a retry policy, the signal sent during creation is not carried over to retried executions. Retried executions only include the original workflow inputs, losing the signal that was part of the initial creation request.",
      "category": "bug",
      "subcategory": "workflow-retry",
      "apis": [
        "SignalWithStart",
        "RetryOptions"
      ],
      "components": [
        "workflow-engine",
        "signal-processing",
        "retry-handler"
      ],
      "concepts": [
        "retry",
        "signal",
        "workflow-execution",
        "state-preservation",
        "SignalWithStart"
      ],
      "severity": "medium",
      "userImpact": "Users who use SignalWithStart to create workflows with retry policies lose the initialization signal when the workflow retries, potentially causing incorrect behavior or missed initialization data.",
      "rootCause": "The retry mechanism does not include signals from the initial workflow creation attempt, only preserving the original workflow inputs.",
      "proposedFix": "Could add an option to RetryOptions to copy all signals from previous workflow attempts, though maintainers questioned if this is a good pattern.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If original execution created fails, the retry does not include the signal thats also sent via SignalWithStart exec creation.",
      "number": 8284,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:36.854Z"
    },
    {
      "summary": "Reduce the log level of failed Nexus requests originating from workers to avoid polluting server logs. Currently every failed request is logged as an error, inconsistent with how activity and workflow task failures are handled, but we need to maintain visibility into internal errors for debugging.",
      "category": "other",
      "subcategory": "nexus-logging",
      "apis": [],
      "components": [
        "nexus-operations",
        "executors",
        "logging"
      ],
      "concepts": [
        "log-level",
        "error-handling",
        "visibility",
        "debugging",
        "worker-requests"
      ],
      "severity": "low",
      "userImpact": "Server logs are polluted with error messages for failed Nexus requests, making it harder to identify critical issues.",
      "rootCause": "Failed Nexus requests from workers are logged at error level for every request, inconsistent with activity and workflow task failure handling.",
      "proposedFix": "Adjust log level for failed Nexus requests originating from workers while maintaining internal error visibility for debugging.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the log level adjustment for failed Nexus requests was implemented.",
      "related": [],
      "keyQuote": "We don't want to completely lose visibility into these errors since some of the errors can originate internally and may be tricky to debug.",
      "number": 8269,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:26.299Z"
    },
    {
      "summary": "Cron-scheduled workflows using WorkflowOptions do not reschedule after termination when reaching retry limits. Users expect the next scheduled run to execute, but terminated workflows prevent any future executions.",
      "category": "bug",
      "subcategory": "cron-scheduling",
      "apis": [
        "WorkflowOptions"
      ],
      "components": [
        "workflow-scheduler",
        "cron-engine",
        "retry-mechanism"
      ],
      "concepts": [
        "cron-scheduling",
        "retry-limit",
        "workflow-termination",
        "reschedule",
        "workflow-lifecycle"
      ],
      "severity": "high",
      "userImpact": "Production cron jobs fail to reschedule after reaching retry limits, requiring manual intervention to restore scheduled execution.",
      "rootCause": "The system terminates workflows that exhaust retry limits, and the cron scheduling mechanism does not create new scheduled runs after termination.",
      "proposedFix": "Users are encouraged to migrate to the newer Schedules feature which is more robust and decoupled from workflow lifetime.",
      "workaround": "Use the newer Schedules API instead of cron schedule via WorkflowOptions for more reliable rescheduling behavior.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue reporter was directed to use the newer Schedules feature as the recommended solution. The older cron scheduling via WorkflowOptions has this limitation by design.",
      "related": [],
      "keyQuote": "I would also strongly encourage you to use the newer schedules feature, which is more robust and decoupled from the lifetime of a workflow.",
      "number": 8266,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:26.474Z"
    },
    {
      "summary": "Users need a bulk pause feature in the Temporal Web UI to pause multiple schedules at once, rather than pausing each individually. This would improve usability for teams managing dozens or hundreds of schedules, with support for filtering, confirmation prompts, and bulk resume functionality.",
      "category": "feature",
      "subcategory": "ui-schedule-management",
      "apis": [],
      "components": [
        "web-ui",
        "schedule-service",
        "schedule-manager"
      ],
      "concepts": [
        "bulk-operations",
        "schedule-pause",
        "user-efficiency",
        "maintenance-operations",
        "ui-selection-mechanism"
      ],
      "severity": "medium",
      "userImpact": "Users managing large numbers of schedules must pause each individually, creating manual overhead and scaling inefficiency during maintenance or debugging operations.",
      "rootCause": null,
      "proposedFix": "Implement a checkbox selection mechanism on the Schedules list page with a 'Pause Selected' or 'Pause All' button, including confirmation prompts, logging, filtering capabilities (e.g., by task queue), and corresponding bulk resume functionality.",
      "workaround": "Use temporal CLI (temporal schedule pause) in a scripted loop for batch operations, though this requires leaving the UI and handling authentication/error-checking manually.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This feature would greatly improve usability for teams managing dozens or hundreds of schedules.",
      "number": 8247,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:25.085Z"
    },
    {
      "summary": "TestWorkflowEnvironment.sleep() incorrectly uses real time instead of virtual time when executed after a test that uses child workflows. This causes tests that should complete instantly to hang for the specified duration.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [
        "newChildWorkflowStub",
        "setStartDelay",
        "getWorkflowExecution"
      ],
      "components": [
        "TestWorkflowEnvironment",
        "child-workflow",
        "virtual-time-clock"
      ],
      "concepts": [
        "virtual-time",
        "test-isolation",
        "timing",
        "workflow-execution",
        "state-management",
        "test-environment"
      ],
      "severity": "high",
      "userImpact": "Developers cannot reliably test workflows with time-dependent behavior in test suites, as tests become flaky and slow down unpredictably depending on execution order.",
      "rootCause": "Child workflow execution in preceding tests appears to switch TestWorkflowEnvironment from virtual time mode to real time mode, affecting subsequent tests.",
      "proposedFix": null,
      "workaround": "Run affected test in isolation or ensure tests using sleep() execute before tests with child workflows.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Looks like something in the child workflow test 'switches' the TestWorkflowEnvironment from virtual time to real time.",
      "number": 8241,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:14.372Z"
    },
    {
      "summary": "GetWorkflow API fails to retrieve archived workflow executions when both workflow ID and run ID are provided, returning a 'not found' error instead of accessing the archived execution.",
      "category": "bug",
      "subcategory": "archival-retrieval",
      "apis": [
        "GetWorkflow",
        "GetWorkflowHistory",
        "ListArchivedWorkflowExecutions"
      ],
      "components": [
        "persistence-layer",
        "sql-execution",
        "archival-storage",
        "workflow-retrieval"
      ],
      "concepts": [
        "archival",
        "workflow-execution",
        "persistence",
        "data-retrieval",
        "archived-workflows",
        "execution-history"
      ],
      "severity": "high",
      "userImpact": "Users cannot retrieve completed and archived workflow executions using the standard GetWorkflow API, forcing them to use workarounds with ListArchivedWorkflowExecutions instead.",
      "rootCause": "The GetWorkflow API does not fall back to the archival store when an execution is not found in the primary persistence layer, causing failures for archived workflows even when the correct workflow ID and run ID are provided.",
      "proposedFix": null,
      "workaround": "Use ListArchivedWorkflowExecutions with a WorkflowId query filter to find archived workflows, then manually retrieve history events using GetWorkflowHistory.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "GetWorkflow doesn't find archived one... Workflow executionsRow not found when passing both workflow and run ID",
      "number": 8235,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:13.646Z"
    },
    {
      "summary": "Handover workflow can timeout between namespace state updates, leaving namespaces in an unavailable state. The issue requires validation at workflow start to ensure timeout is sufficient (>>1hr) and backout logic before unavailable state transition.",
      "category": "bug",
      "subcategory": "namespace-migration-handover",
      "apis": [],
      "components": [
        "handover-workflow",
        "namespace-manager",
        "migration-service"
      ],
      "concepts": [
        "timeout",
        "state-transition",
        "namespace-availability",
        "distributed-coordination",
        "workflow-execution",
        "compensation"
      ],
      "severity": "high",
      "userImpact": "Users performing namespace handover/migration can experience stuck unavailable namespaces if workflow execution timeout is insufficient.",
      "rootCause": "Handover workflow can timeout between the two namespace state updates, leaving the namespace in an inconsistent unavailable state without proper cleanup or compensation.",
      "proposedFix": "Add validation at workflow start to fail immediately if execution timeout is too small (requires >>1hr). Implement backout/compensation logic before transitioning namespace to unavailable state.",
      "workaround": "Always invoke handover workflow with timeout >> 1hr (the max timeout for catchup activity)",
      "resolution": "fixed",
      "resolutionDetails": "Fixed via PRs #8231 and #8258 to prevent insufficient timeout and add detach context for compensation logic",
      "related": [
        8231,
        8258
      ],
      "keyQuote": "Add validations in the beginning of the workflow to fail it immediately if the workflow execution timeout is too small",
      "number": 8229,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:12.465Z"
    },
    {
      "summary": "Security vulnerabilities detected in Temporal server v1.28.1 Docker image, including CVEs in golang.org/x/net, OpenTelemetry gRPC instrumentation, and other dependencies. Most vulnerabilities are in deprecated tools (tctl) or planned-to-be-removed image components that will be addressed in future releases.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "docker-image",
        "tctl-binary",
        "dockerize",
        "curl-dependency"
      ],
      "concepts": [
        "security-vulnerabilities",
        "cve",
        "dependency-management",
        "image-hardening",
        "deprecation"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal server v1.28.1 in Docker are exposed to multiple CVEs, some with high severity ratings, though most are in deprecated or planned-to-be-removed components.",
      "rootCause": "Outdated or vulnerable versions of dependencies including golang.org/x/net, OpenTelemetry gRPC instrumentation, and markdown parser bundled in the Docker image.",
      "proposedFix": "Upgrade golang.org/x/net to v0.36.0, upgrade OpenTelemetry gRPC to v0.46.0, upgrade OpenSSL to 3.5.1-r0, and remove deprecated tctl binary and dockerize from future image releases.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We will be taking action on all of these.",
      "number": 8220,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:59.414Z"
    },
    {
      "summary": "Multiple CVEs identified in the latest temporalio/admin-tools Docker image (1.28.1-tctl-1.18.4-cli-1.4.1), including high-severity vulnerabilities in golang.org/x/net and OpenTelemetry instrumentation. The team is aware and planning an initiative to address image cleanup.",
      "category": "bug",
      "subcategory": "security-vulnerabilities",
      "apis": [],
      "components": [
        "docker-image",
        "admin-tools",
        "dependencies"
      ],
      "concepts": [
        "security",
        "CVE",
        "vulnerability-scanning",
        "dependency-management",
        "container-security",
        "version-patching"
      ],
      "severity": "high",
      "userImpact": "Users running the Docker image are exposed to known security vulnerabilities that could be exploited by attackers.",
      "rootCause": "Outdated dependencies in the Docker image including golang.org/x/net v0.35.0, OpenTelemetry instrumentation v0.36.4, and other packages with published CVEs.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We are aware, we have an initiative to clean up these images that we are trying to start.",
      "number": 8219,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:00.332Z"
    },
    {
      "summary": "The default JWT claim mapper hardcodes the 'sub' claim name, blocking users whose identity providers use different subject claim names like 'client_id' or 'user_id'. The user requests a configurable subjectClaimName parameter similar to the existing permissionsClaimName option.",
      "category": "feature",
      "subcategory": "jwt-authorization",
      "apis": [],
      "components": [
        "authorization",
        "jwt-claim-mapper",
        "default-jwt-claim-mapper"
      ],
      "concepts": [
        "jwt",
        "authentication",
        "authorization",
        "claim-mapping",
        "configuration",
        "oidc",
        "identity-provider"
      ],
      "severity": "medium",
      "userImpact": "Users with non-standard JWT tokens (using claim names other than 'sub' for subject) cannot use Temporal's default JWT authorizer and must implement a custom claim mapper.",
      "rootCause": "The defaultJWTClaimMapper hardcodes 'sub' as the subject claim name in common/authorization/default_jwt_claim_mapper.go, with no configuration option to override it.",
      "proposedFix": "Add a subjectClaimName configuration field to the Authorization struct, defaulting to 'sub' for backward compatibility, following the existing pattern of permissionsClaimName.",
      "workaround": "Implement a custom ClaimMapper by importing go.temporal.io/server/temporal and providing a custom implementation that maps the desired claim to 'sub'.",
      "resolution": "wontfix",
      "resolutionDetails": "Closed due to lack of follow-up from the original poster. The issue was framed as non-standard JWT usage, with the recommended solution being either to configure the IdP to provide 'sub' claim or to use a custom ClaimMapper.",
      "related": [],
      "keyQuote": "The default JWT claim mapper hardcodes the subject claim name as \"sub\" in common/authorization/default_jwt_claim_mapper.go, blocking users whose identity providers use different subject claim names",
      "number": 8218,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:25:01.549Z"
    },
    {
      "summary": "Temporal Schedules with local timestamps don't handle Daylight Saving Time transitions properly, potentially causing cron jobs to run zero, one, or two times during DST shifts. The issue requests configurable DST policies (similar to Airflow) or sensible defaults to deduplicate/shift missing runs.",
      "category": "feature",
      "subcategory": "schedules-dst",
      "apis": [],
      "components": [
        "schedules",
        "cron-engine",
        "timezone-handling"
      ],
      "concepts": [
        "daylight-saving-time",
        "timezone-transitions",
        "cron-scheduling",
        "time-handling",
        "schedule-policy"
      ],
      "severity": "medium",
      "userImpact": "Users scheduling local time cron jobs during DST transitions may experience unexpected behavior with jobs running zero, one, or multiple times, affecting reliability of time-sensitive operations.",
      "rootCause": "The underlying cron library used by Temporal does not implement special handling for DST transitions, leaving the behavior undefined during DST boundaries.",
      "proposedFix": "Implement configurable DST transition policies (duplicate_dst: RUN_FIRST/RUN_LAST/RUN_NONE; missing_dst: SHIFT/SKIP) or adopt Airflow's default behavior of deduplicating and shifting missing runs.",
      "workaround": "Schedule cron jobs in UTC timezone or avoid scheduling during known DST transition periods (e.g., 2:00-3:00 AM in Europe).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If not configurable, maybe make the default the most intuitive one, i.e. de-duplicate and shift missing run. It's the default behavior of Airflow.",
      "number": 8205,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:48.486Z"
    },
    {
      "summary": "Temporal server crashes with CrashLoopBackOff and cannot recover when GKE nodes are replaced during cluster updates due to unhandled database connection errors. The service requires manual redeployment to re-establish database connectivity.",
      "category": "bug",
      "subcategory": "database-connection",
      "apis": [],
      "components": [
        "persistence",
        "sql-plugin",
        "database-driver",
        "connection-pool"
      ],
      "concepts": [
        "connection-refresh",
        "error-handling",
        "node-replacement",
        "kubernetes-deployment",
        "database-resilience",
        "recovery"
      ],
      "severity": "high",
      "userImpact": "Users experience service unavailability and require manual intervention to restore Temporal during infrastructure updates, breaking automated deployment workflows.",
      "rootCause": "SQL driver returns unrecognized error types during node replacement that don't trigger the connection refresh logic, preventing automatic reconnection attempts.",
      "proposedFix": "Extend connection refresh logic to handle additional error types returned by the database driver in network disruption scenarios.",
      "workaround": "Disable GKE auto-upgrade to prevent node replacement, or manually redeploy Temporal with Helm after node updates.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "unable to read DB schema version keyspace/database: temporal error: no usable database connection found",
      "number": 8202,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:47.241Z"
    },
    {
      "summary": "NexusOperationCancelRequestCompleted event fails to trigger a workflow task when not buffered, inconsistently handling cancellation requests in Nexus operations depending on event buffering state.",
      "category": "bug",
      "subcategory": "nexus-operations",
      "apis": [],
      "components": [
        "nexus-operation",
        "workflow-task",
        "event-buffering"
      ],
      "concepts": [
        "cancellation",
        "event-buffering",
        "workflow-task-trigger",
        "nexus-operations",
        "event-handling"
      ],
      "severity": "high",
      "userImpact": "Users experience inconsistent behavior when canceling Nexus operations, where cancellation completion events may fail to trigger workflow task execution unless buffered.",
      "rootCause": "The event handling logic does not properly trigger a workflow task (WFT) when NexusOperationCancelRequestCompleted is not buffered, suggesting conditional WFT triggering based on buffer state.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved through code changes addressing the event buffering behavior for NexusOperationCancelRequestCompleted.",
      "related": [],
      "keyQuote": "NexusOperationCancelRequestCompleted was buffered and triggered a WFT, whereas in the run on the right it was not and did not.",
      "number": 8175,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:45.898Z"
    },
    {
      "summary": "Docker images for Temporal server and admin tools need to address CVE-2025-22871 vulnerability in golang:net/http/internal dependency. User requests updated images with the security vulnerability fixed.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "docker-image",
        "server",
        "admin-tools"
      ],
      "concepts": [
        "security-vulnerability",
        "cve",
        "dependency-management",
        "golang",
        "docker"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal server and admin tools in Docker are exposed to security vulnerabilities that could be exploited until patched images are released.",
      "rootCause": "golang:net/http/internal dependency contains CVE-2025-22871 vulnerability",
      "proposedFix": "Release updated Docker images with patched golang:net/http/internal dependency",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "golang:net/http/internal has vulnerability CVE-2025-22871. Please help us with a docker image for both server and admin tools by fixing these vulnerabilities.",
      "number": 8153,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:32.879Z"
    },
    {
      "summary": "The signals_requested_sets table grows unbounded with orphaned rows referencing workflow executions that no longer exist, causing database bloat. User investigation confirmed hundreds of thousands of orphaned entries per shard with no corresponding current or historical executions.",
      "category": "bug",
      "subcategory": "database-leak",
      "apis": [],
      "components": [
        "database",
        "signals",
        "garbage-collection"
      ],
      "concepts": [
        "orphaned-data",
        "memory-leak",
        "database-bloat",
        "cleanup",
        "retention",
        "data-consistency"
      ],
      "severity": "high",
      "userImpact": "Database storage grows unbounded over time due to orphaned signal request records, requiring manual intervention or workarounds to reclaim disk space.",
      "rootCause": "Signals requested in the signals_requested_sets table are not being properly cleaned up when workflow executions complete or are garbage collected, leaving orphaned references.",
      "proposedFix": null,
      "workaround": "Setting worker.historyScannerDataMinAge to a value past retention limit (e.g., 4 days for 3-day retention) may trigger cleanup of orphaned history data, though this did not fully resolve the issue for the reporter.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "for each shard we have about 300k orphans and only 3k alive entries... table contains rows referencing workflow runs that do not exist anymore",
      "number": 8146,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:35.573Z"
    },
    {
      "summary": "When a worker restarts without setting MaxActivityTasksPerSecond, the server retains the previous dispatch rate limit instead of resetting to the default value. The field is not included in PollActivityTaskQueueRequest when unset, but the server doesn't treat this absence as a reset signal.",
      "category": "bug",
      "subcategory": "activity-task-dispatch",
      "apis": [
        "PollActivityTaskQueueRequest"
      ],
      "components": [
        "matching",
        "task-queue-partition-manager",
        "activity-dispatcher"
      ],
      "concepts": [
        "rate-limiting",
        "task-dispatch",
        "configuration-reset",
        "worker-lifecycle",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Workers cannot reduce or remove activity dispatch rate limits after setting them, forcing server restarts or workarounds to reset the configuration.",
      "rootCause": "The task_queue_partition_manager does not treat an absent max_tasks_per_second field in PollActivityTaskQueueRequest as a reset signal; instead it preserves the previously stored value.",
      "proposedFix": null,
      "workaround": "Use the UpdateTaskQueueConfig request API to explicitly unset the limit.",
      "resolution": "wontfix",
      "resolutionDetails": "Resolved as expected behavior with workaround. Users should use UpdateTaskQueueConfig API instead of relying on absent field to reset configuration.",
      "related": [],
      "keyQuote": "if the worker restarts without setting MaxActivityTasksPerSecond, the server does not reset the dispatch rate to its default value, it retains the previous value instead",
      "number": 8137,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:34.887Z"
    },
    {
      "summary": "User reports that activity retry configuration with maximumAttempts:1 causes the worker to stop, while retry values >1 work but cause duplicate activity executions and database duplicate records due to activity replay behavior.",
      "category": "question",
      "subcategory": "activity-retry",
      "apis": [],
      "components": [
        "activity",
        "worker",
        "retry"
      ],
      "concepts": [
        "retry",
        "idempotency",
        "activity-timeout",
        "duplicate-execution",
        "worker-state"
      ],
      "severity": "medium",
      "userImpact": "Users encounter worker crashes with retry:1 and duplicate database records with retry>1, making the activity execution unreliable for critical business operations.",
      "rootCause": "Activity timeouts cause Temporal to retry activities; users are not properly implementing idempotent activities and need to account for activity start-to-close timeouts.",
      "proposedFix": "Activities must be coded as idempotent to handle retries correctly, and developers should pay attention to activity start-to-close timeout configuration.",
      "workaround": "Implement idempotent activity logic to prevent duplicate side effects when retries occur.",
      "resolution": "invalid",
      "resolutionDetails": "Closed as not a bug; issue stems from user misunderstanding of expected behavior and need for idempotent activity implementations.",
      "related": [],
      "keyQuote": "You should code the activities to be idempotent and prevent this situation from happening and pay attention to the activity start-to-close timeouts.",
      "number": 8129,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:19.737Z"
    },
    {
      "summary": "Workflow deletion batch jobs report successful completion but fail to actually delete workflows due to visibility store timeouts. The batch operation incorrectly marks deletions as successful even when context deadline exceeded errors occur during persistence operations.",
      "category": "bug",
      "subcategory": "workflow-deletion",
      "apis": [],
      "components": [
        "visibility-queue-task-executor",
        "visibility-manager",
        "batch-delete",
        "persistence-layer"
      ],
      "concepts": [
        "timeout",
        "batch-operations",
        "async-operations",
        "context-deadline",
        "workflow-deletion",
        "visibility-store"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably delete workflows as batch deletion reports success while workflows remain in the system, creating confusion and data management issues.",
      "rootCause": "Context deadline exceeded errors in visibility store operations during deletion, combined with incorrect error handling that reports timeouts as successful completions rather than failures.",
      "proposedFix": "Implement proper error handling to report batch deletion failures when visibility store operations timeout, and consider adding a synchronous deletion API as an alternative.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Issue was closed because workflow deletion is async by design and works as expected. Timeouts in persistence layer prevent deletions, not the deletion API itself. Suggestion to add synchronous deletion API noted but not implemented.",
      "related": [
        6995
      ],
      "keyQuote": "Workflow deletion is an async operation, it's actually just requesting that the workflow be deleted and not waiting for actual deletion.",
      "number": 8125,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:21.945Z"
    },
    {
      "summary": "Worker Versioning with PINNED behavior in versionOverrides fails in Temporal Server 1.28.0+ due to the TypeScript SDK partially filling both v0.31 and v0.32 versioning override fields, causing the server validation to fail. The issue was traced to SDK implementation and fixed in the TypeScript SDK.",
      "category": "bug",
      "subcategory": "worker-versioning",
      "apis": [
        "WorkflowClient.start"
      ],
      "components": [
        "versioning-override",
        "worker-deployment",
        "workflow-client"
      ],
      "concepts": [
        "worker-versioning",
        "version-pinning",
        "deployment",
        "protocol-compatibility",
        "field-validation"
      ],
      "severity": "high",
      "userImpact": "Users cannot start workflows with pinned worker versions in Temporal 1.28.0+, blocking deployment workflows that rely on version pinning.",
      "rootCause": "TypeScript SDK was partially filling both v0.31 and v0.32 versioning override fields simultaneously, but the server expects either one complete set or the other, not a mix of both.",
      "proposedFix": "Update TypeScript SDK to properly construct versioning overrides using only one API version (either complete v0.31 or complete v0.32 fields, not mixed).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was identified as a TypeScript SDK bug. The fix was implemented and merged in sdk-typescript PR #1754.",
      "related": [],
      "keyQuote": "When the server expects either... the TS SDK will need to change to only send one complete set of versioning override fields.",
      "number": 8114,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:19.065Z"
    },
    {
      "summary": "Request to add Activity versioning support via `versioningOverride` in `ActivityOptions`, allowing Workflows to specify which version of an Activity Worker to use, similar to existing Workflow versioning capabilities.",
      "category": "feature",
      "subcategory": "worker-versioning",
      "apis": [
        "ActivityOptions",
        "WorkerDeploymentOptions"
      ],
      "components": [
        "activity-worker",
        "worker-versioning",
        "task-queue",
        "deployment"
      ],
      "concepts": [
        "versioning",
        "deployment-lifecycle",
        "backwards-compatibility",
        "task-routing",
        "worker-interface"
      ],
      "severity": "medium",
      "userImpact": "Teams with independent Activity and Workflow release cycles cannot specify Activity versions from Workflows, forcing workarounds like encoding versions in task queue names.",
      "rootCause": "Worker Versioning feature was designed for Workflows but not extended to Activities, leaving Activities without versioning support despite their independent deployment cycles.",
      "proposedFix": "Add `versioningOverride` to `ActivityOptions` and leverage existing Workflow Versioning infrastructure and `WorkerDeploymentOptions` to resolve Activities to specific Worker Deployments.",
      "workaround": "Bake the Activity version into the task queue name to pin Activity calls to specific versions.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Generally, during the design of this feature we've been trying to not involve the caller in worker versioning... But I don't think there is any big reason preventing us to add this feature if there are compelling use cases for it.",
      "number": 8113,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:07.090Z"
    },
    {
      "summary": "Docker image for server v1.28.0 contains multiple security vulnerabilities across alpine base image, Go dependencies, and third-party libraries. User requests remediation and timeline for a patched image.",
      "category": "bug",
      "subcategory": "security-vulnerability",
      "apis": [],
      "components": [
        "docker-image",
        "alpine-base",
        "go-runtime",
        "grpc",
        "opentelemetry"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "cve",
        "dependency",
        "docker",
        "image",
        "remediation"
      ],
      "severity": "high",
      "userImpact": "Users running server v1.28.0 in production are exposed to multiple known security vulnerabilities that could be exploited by attackers.",
      "rootCause": "Outdated versions of alpine base image, Go runtime, and multiple Go dependencies (gRPC, OpenTelemetry, crypto, oauth2, JWT) contain known CVEs.",
      "proposedFix": null,
      "workaround": null,
      "related": [],
      "keyQuote": "We see the below components of the server:v1.28.0 image have some vulnerabilities.",
      "resolution": null,
      "number": 8110,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:04.413Z"
    },
    {
      "summary": "Feature request to support custom, human-readable names for timers in Temporal workflows instead of system-generated UUIDs. This would improve debugging, monitoring, and observability by making timers easily identifiable in the Web UI and logs.",
      "category": "feature",
      "subcategory": "timer-management",
      "apis": [
        "newTimer"
      ],
      "components": [
        "timer-executor",
        "workflow-engine",
        "web-ui"
      ],
      "concepts": [
        "observability",
        "debugging",
        "monitoring",
        "human-readable-identifiers",
        "workflow-visibility",
        "naming"
      ],
      "severity": "medium",
      "userImpact": "Developers struggle to identify and debug multiple timers in workflows due to generic UUID identifiers in the Web UI and logs.",
      "rootCause": null,
      "proposedFix": "Add an option to assign custom, descriptive names to timers when creating them (e.g., Workflow.newTimer(Duration, String timerName)) that are displayed in the Web UI and logs.",
      "workaround": "Using custom search attributes, workflow metadata tagging, or logging messages with timer names; naming workflows or task queues meaningfully as partial compensation.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would be very helpful to have an option to assign custom, descriptive names or labels to timers when creating them via the SDK",
      "number": 8109,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:24:05.341Z"
    },
    {
      "summary": "RecordActivityStarted implementation holds workflow lock while loading ActivityScheduledEvent from history, causing unnecessary lock contention when many activities are scheduled simultaneously. The proposal is to read the event outside the lock since history is immutable.",
      "category": "feature",
      "subcategory": "activity-scheduling",
      "apis": [],
      "components": [
        "workflow-lock",
        "history-service",
        "activity-scheduler",
        "mutable-state",
        "event-cache"
      ],
      "concepts": [
        "lock-contention",
        "concurrency",
        "performance",
        "history-immutability",
        "caching",
        "workflow-scheduling"
      ],
      "severity": "medium",
      "userImpact": "Users experience task failures and retries when scheduling large numbers of activities simultaneously due to workflow lock contention.",
      "rootCause": "RecordActivityStarted loads ActivityScheduledEvent while holding workflow lock, and event is not cached, causing lock contention during high-volume activity scheduling.",
      "proposedFix": "Read ActivityScheduledEvent outside workflow lock and ensure ActivityScheduledEvent is cached during activity scheduling.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "loading the ActivityScheduledEvent while holding the workflow lock...increases the time workflow is locked unnecessarily (because history is immutable and can be accessed without workflow being locked)",
      "number": 8101,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:52.703Z"
    },
    {
      "summary": "Remove unnecessary waits before fetching pages in batch activities to allow free executors to pick up work immediately rather than waiting for page fetches to complete.",
      "category": "feature",
      "subcategory": "batch-activities",
      "apis": [],
      "components": [
        "batcher",
        "activities",
        "worker",
        "executor"
      ],
      "concepts": [
        "concurrency",
        "executor-utilization",
        "pagination",
        "performance",
        "work-scheduling"
      ],
      "severity": "medium",
      "userImpact": "Improves throughput and latency of batch activity operations by allowing executors to process work in parallel rather than sequentially waiting for page fetches.",
      "rootCause": "Batch activity code waits before fetching next page even when executors are available to pick up work.",
      "proposedFix": "Remove the wait before fetching the next page in the batch activities implementation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, likely addressed in PR #8081 or related changes.",
      "related": [
        8081
      ],
      "keyQuote": "there's no reason to wait before fetching the next page if we have executors that are free to pick up work",
      "number": 8098,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:51.973Z"
    },
    {
      "summary": "Currently, incoming signals are rejected when a workflow attempts continueAsNew but fails due to buffered signals, preventing infinite loops. The feature request proposes flushing buffered signals to the new run instead of the current run, allowing continueAsNew to succeed without rejecting signals.",
      "category": "feature",
      "subcategory": "signal-handling",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "workflow-engine",
        "signal-buffer",
        "task-processor"
      ],
      "concepts": [
        "signal-buffering",
        "workflow-continuation",
        "signal-carryover",
        "continueAsNew",
        "task-completion"
      ],
      "severity": "medium",
      "userImpact": "Workflows using continueAsNew with buffered signals currently fail and require retry, but could succeed by carrying signals to the new run.",
      "rootCause": "Current implementation rejects incoming signals during continueAsNew to prevent infinite loops, but this applies the same logic as complete/fail operations.",
      "proposedFix": "Flush buffered signals to the new run during continueAsNew instead of rejecting them, allowing the operation to succeed without task failure.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Upon continueAsNew, we can simply flush buffered signals to the new run instead of current run, and we don't have to fail & retry the workflow task",
      "number": 8097,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:53.563Z"
    },
    {
      "summary": "Multiple security vulnerabilities (CVEs) were found in the Temporalio/admin-tools:1.28.0 Docker image, including high-severity issues in JWT, networking, OAuth2, and OpenTelemetry packages. Some vulnerabilities were resolved in version 1.28.1.",
      "category": "bug",
      "subcategory": "security-vulnerability",
      "apis": [],
      "components": [
        "docker-image",
        "admin-tools",
        "dependencies"
      ],
      "concepts": [
        "security",
        "CVE",
        "vulnerability",
        "dependency-management",
        "container-image",
        "version-upgrade"
      ],
      "severity": "high",
      "userImpact": "Users running the admin-tools Docker image are exposed to multiple high-severity security vulnerabilities that could be exploited to compromise their systems.",
      "rootCause": "Outdated dependencies in the admin-tools Docker image including github.com/golang-jwt/jwt, golang.org/x/net, golang.org/x/oauth2, and go.opentelemetry.io packages with known CVEs.",
      "proposedFix": "Upgrade vulnerable dependencies to patched versions: jwt/v4 to 4.5.2, httpproxy to 0.36.0, oauth2 to 0.27.0, and otelgrpc to 0.46.0.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved in version 1.28.1 with dependency updates. Additional CVEs reported separately in issue #8219.",
      "related": [
        8219
      ],
      "keyQuote": "some CVEs are resolved in latest version v1.28.1. The new reported CVEs are informed in https://github.com/temporalio/temporal/issues/8219",
      "number": 8092,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:40.772Z"
    },
    {
      "summary": "Multiple CVEs (CVE-2025-30204, CVE-2025-22870, CVE-2023-47108, CVE-2024-44337, CVE-2024-2689, CVE-2025-4575) detected in temporalio/server:1.28.0 Docker image with high and medium severity ratings. Requires dependency updates to address security vulnerabilities.",
      "category": "bug",
      "subcategory": "security-vulnerabilities",
      "apis": [],
      "components": [
        "docker-image",
        "dependencies",
        "jwt",
        "http-proxy",
        "grpc-instrumentation",
        "openssl"
      ],
      "concepts": [
        "security",
        "cve",
        "vulnerability",
        "dependency-management",
        "docker",
        "version-upgrade"
      ],
      "severity": "high",
      "userImpact": "Users running temporalio/server:1.28.0 are exposed to multiple security vulnerabilities that could potentially be exploited.",
      "rootCause": "Outdated dependencies with known CVEs: golang-jwt/jwt v4.5.1, golang.org/x/net v0.34.0, otelgrpc v0.36.4, gomarkdown/markdown parser, openssl 3.5.0-r0, and temporal server library versions",
      "proposedFix": "Update dependencies: golang-jwt/jwt to 4.5.2+, golang.org/x/net to 0.36.0+, otelgrpc to 0.46.0+, temporal server to 1.20.5/1.21.6/1.22.7+, openssl to 3.5.1-r0+",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Issue was closed in favor of #8220 which addresses the same security concerns in a later patch release",
      "related": [
        8220
      ],
      "keyQuote": "Closing in favor of #8220 which is the latest patch release",
      "number": 8091,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:38.980Z"
    },
    {
      "summary": "The no_poller_tasks metric disappeared from Prometheus after upgrading Temporal Server from v1.27.1.0 to v1.27.2.0 or later, breaking monitoring dashboards and alerts that depend on this metric for tracking task queue poller availability.",
      "category": "bug",
      "subcategory": "metrics-prometheus",
      "apis": [],
      "components": [
        "metrics",
        "prometheus",
        "task-queue",
        "poller"
      ],
      "concepts": [
        "monitoring",
        "metrics-export",
        "prometheus-scraping",
        "task-queue-visibility",
        "upgrade-compatibility"
      ],
      "severity": "high",
      "userImpact": "Users cannot monitor task queue poller availability after upgrades, breaking existing monitoring dashboards and alerts.",
      "rootCause": "Metric tags may have been modified in the new version; alternatively, metrics system may not be reading new nodes' metrics correctly after rolling restarts.",
      "proposedFix": "Verify metric tags have not changed; ensure metrics system reads metrics from all nodes after upgrades/restarts.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was likely user-side; support suggested checking metric tags without filters and verifying all service pods/hosts are present in metrics collection.",
      "related": [],
      "keyQuote": "the metric is not removed but it could be that some tags are modified in the new version",
      "number": 8088,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:41.289Z"
    },
    {
      "summary": "Request for a new metric to track when scheduled runs are skipped due to overlap policy, missed catchup windows, or buffer overruns. Customers need to alarm on and identify schedules with missing or skipped runs.",
      "category": "feature",
      "subcategory": "schedules-metrics",
      "apis": [],
      "components": [
        "schedules",
        "metrics",
        "monitoring"
      ],
      "concepts": [
        "scheduled-actions",
        "skipped-runs",
        "overlap-policy",
        "catchup-window",
        "buffer-overrun",
        "alerting",
        "observability"
      ],
      "severity": "medium",
      "userImpact": "Customers cannot currently monitor or alarm when scheduled runs are skipped, making it difficult to detect missed executions.",
      "rootCause": null,
      "proposedFix": "Introduce a new metric to track skipped scheduled runs with visibility into the reason (overlap policy, missed catchup window, or buffer overrun).",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Customers would like to alarm on when a scheduled run was missed or skipped.",
      "number": 8087,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:25.303Z"
    },
    {
      "summary": "Request to combine reset and signal operations in a single command, allowing workflows to reset to a specific point and simultaneously apply a signal with additional context to the reset workflow task.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [
        "ResetWorkflowExecution",
        "SignalWorkflow"
      ],
      "components": [
        "workflow-executor",
        "reset-handler",
        "signal-processor"
      ],
      "concepts": [
        "reset",
        "signal",
        "workflow-task",
        "state-management",
        "replay",
        "context-propagation"
      ],
      "severity": "medium",
      "userImpact": "Users gain the ability to atomically reset workflow execution and provide additional context through signals, enabling more flexible recovery and debugging workflows.",
      "rootCause": null,
      "proposedFix": "Implement a ResetWithSignal operation that combines workflow reset with signal application to the post-reset task.",
      "workaround": "Currently users must perform reset and signal as separate operations, losing atomicity and requiring manual coordination.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Request is to be able to do reset + signal which would reset execution at specific reset point and also apply a signal to the workflow task after the one that is failed by reset command.",
      "number": 8074,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:27.620Z"
    },
    {
      "summary": "Request to expose more SQL connection metrics from Temporal to provide better visibility when troubleshooting database connection issues and configuration tuning, addressing concerns about excessive connections to PostgreSQL.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "database",
        "metrics",
        "connection-pool",
        "postgres"
      ],
      "concepts": [
        "connection-pooling",
        "metrics-exposure",
        "troubleshooting",
        "database-monitoring",
        "configuration-tuning",
        "observability"
      ],
      "severity": "medium",
      "userImpact": "Users lack visibility into SQL connection behavior, making it difficult to diagnose and tune database connection issues in production.",
      "rootCause": null,
      "proposedFix": "Expose additional SQL connection metrics from the Temporal server",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        8071
      ],
      "keyQuote": "Expose more SQL connection metrics from temporal, provide more insight when troubleshooting and config tunning",
      "number": 8072,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:26.264Z"
    },
    {
      "summary": "The auto-setup Docker image fails when deploying Temporal with Elasticsearch 8 because it tries to load missing v8-specific schema files (cluster_settings_v8.json and index_template_v8.json) that don't exist in the image.",
      "category": "bug",
      "subcategory": "elasticsearch-schema",
      "apis": [],
      "components": [
        "auto-setup",
        "elasticsearch",
        "schema"
      ],
      "concepts": [
        "elasticsearch-version",
        "schema-compatibility",
        "kubernetes-deployment",
        "docker-image",
        "configuration"
      ],
      "severity": "high",
      "userImpact": "Users attempting to deploy Temporal with Elasticsearch 8 using auto-setup Docker image encounter startup failures due to missing schema files.",
      "rootCause": "The auto-setup script attempts to load ES version-specific schema files (v8) that don't exist, when the v7 schema is actually compatible with ES8.",
      "proposedFix": "Remove the ES_VERSION environment variable from the auto-setup script since v7 schema works with both ES7 and ES8.",
      "workaround": "Set ES_VERSION=v7 even when running Elasticsearch 8, as the v7 schema is compatible with v8.",
      "resolution": "fixed",
      "resolutionDetails": "ES_VERSION environment variable was removed from the auto-setup script by the maintainers, allowing v7 schema to be used universally.",
      "related": [],
      "keyQuote": "You can simply start ES8, and run Temporal with ES_VERSION=v7, and it should work.",
      "number": 8063,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:14.324Z"
    },
    {
      "summary": "Request to expose last activity failure information through PollActivityTaskQueueResponse, allowing users to access failure context within activity code for informed decision-making based on previous execution history.",
      "category": "feature",
      "subcategory": "activity-execution",
      "apis": [
        "PollActivityTaskQueueResponse"
      ],
      "components": [
        "activity-executor",
        "worker",
        "task-queue",
        "api-response"
      ],
      "concepts": [
        "failure-handling",
        "activity-retry",
        "execution-history",
        "error-context",
        "decision-making"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently access last activity failure information within activity code, limiting their ability to make context-aware decisions based on previous execution failures.",
      "rootCause": null,
      "proposedFix": "Expose last activity failure info via PollActivityTaskQueueResponse in the temporal/api workflowservice/v1/request_response.proto",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Would allow users to check last failure info within their activity code and able to make decisions based on it.",
      "number": 8039,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:12.439Z"
    },
    {
      "summary": "Worker service experiences abnormal restart due to a null pointer exception in the execution scanner task when processing execution records. The issue occurs when the logic doesn't return on errors and continues to validate an empty record, causing an NPE.",
      "category": "bug",
      "subcategory": "execution-scanner",
      "apis": [],
      "components": [
        "worker",
        "execution-scanner",
        "persistence"
      ],
      "concepts": [
        "null-pointer-exception",
        "error-handling",
        "database-compatibility",
        "service-restart"
      ],
      "severity": "high",
      "userImpact": "Worker service crashes and restarts abnormally, disrupting workflow execution and stability in production environments.",
      "rootCause": "The execution scanner task logic doesn't return on errors and continues to validate an empty record, causing a null pointer exception. Additionally, the execution scanner is enabled for SQL databases which don't support the ListConcreteExecutions persistence API.",
      "proposedFix": "Add error handling logic to return when ListConcreteExecutions fails. Add a check to only start the execution scanner when the underlying DB supports the required persistence API (not SQL).",
      "workaround": "Turn off the execution scanner feature flag (worker.executionsscannerenabled) and terminate any workflow in the temporal-system namespace with workflow type 'temporal-sys-executions-scanner-workflow'.",
      "resolution": "fixed",
      "resolutionDetails": "The root cause was identified in the executions/task.go file where error handling was missing. A fix was planned to add proper error handling and database compatibility checks.",
      "related": [],
      "keyQuote": "the logic didn't return on errors and continued to validate an empty record causing the NPE",
      "number": 8037,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:15.273Z"
    },
    {
      "summary": "Dev server experiencing persistent timeout and context cancellation errors after ~1 hour of idle operation, with logs showing SQLite database operation failures and deadline exceeded errors. Issue appears to be transient timeout-related rather than missing database tables despite the title.",
      "category": "bug",
      "subcategory": "server-persistence",
      "apis": [],
      "components": [
        "matching-engine",
        "transfer-queue-processor",
        "outbound-queue-processor",
        "sqlite-persistence"
      ],
      "concepts": [
        "timeout",
        "context-cancellation",
        "database-operations",
        "task-queue",
        "persistence"
      ],
      "severity": "medium",
      "userImpact": "Dev server becomes unstable and returns 500 errors after approximately one hour of idle operation, requiring restart.",
      "rootCause": "Transient timeout errors in SQLite database operations (GetHistoryTasks, GetTransferTasks, update-task-queue operations) causing context deadlines to be exceeded.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "stale",
      "resolutionDetails": "Closed without followup after maintainer suggested testing with version 1.28.1 and noted no actual 'missing table' errors in logs, indicating the issue was likely version-specific or environment-dependent.",
      "related": [
        3784
      ],
      "keyQuote": "I don't see errors like \"no such table\" or \"missing table\" in the logs you pasted. Looks like they are all transient timeout errors.",
      "number": 8017,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:01.182Z"
    },
    {
      "summary": "ListWorkflows with custom search attributes sometimes returns incorrect workflow results when multiple workflows are created simultaneously with different attribute values. The wrong workflow is returned instead of the correct one or no results, though the index eventually becomes consistent.",
      "category": "bug",
      "subcategory": "visibility-search",
      "apis": [
        "ListWorkflowsAsync",
        "StartWorkflowAsync"
      ],
      "components": [
        "visibility-store",
        "search-attributes",
        "indexing"
      ],
      "concepts": [
        "eventual-consistency",
        "search-accuracy",
        "concurrent-creation",
        "custom-attributes",
        "visibility-index"
      ],
      "severity": "high",
      "userImpact": "Users may receive incorrect workflow results when querying by custom search attributes on workflows created in rapid succession, causing data inconsistency until the visibility index eventually reconciles.",
      "rootCause": "Race condition in visibility store indexing when multiple workflows with different custom search attributes are created within the same second, causing the wrong workflow to be returned before index consistency is achieved.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "In some cases, the method returns a workflow that does not contain the requested search attribute. This workflow was created at the same time (within the same second) as the expected one.",
      "number": 8013,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:02.530Z"
    },
    {
      "summary": "User requests the ability to customize activity labels in the Event History diagram to better distinguish between multiple calls to the same activity function. Currently, the diagram only shows the activity function name, making it difficult to identify which specific call an activity in the history corresponds to.",
      "category": "feature",
      "subcategory": "event-history-visualization",
      "apis": [],
      "components": [
        "event-history-ui",
        "activity-display",
        "visualization"
      ],
      "concepts": [
        "activity-labeling",
        "event-history",
        "customization",
        "workflow-visibility",
        "debugging"
      ],
      "severity": "low",
      "userImpact": "Users cannot easily distinguish between multiple calls to the same activity in event history visualizations, making debugging and understanding workflow execution more difficult.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I want each activity to have a custom name so that when I look at the event history diagram, I can quickly identify what call it corresponds to.",
      "number": 8006,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:23:00.872Z"
    },
    {
      "summary": "The temporal-sql-tool and temporal-cassandra-tool have hardcoded versions (0.0.1) that cannot be updated during the build process using standard Go build flags. The issue raises questions about whether these tools should be versioned together with temporal-server or independently.",
      "category": "bug",
      "subcategory": "versioning",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "temporal-cassandra-tool",
        "build-system"
      ],
      "concepts": [
        "versioning",
        "build-flags",
        "cli-tools",
        "version-management",
        "binary-distribution"
      ],
      "severity": "low",
      "userImpact": "Users cannot determine the exact version of temporal-sql-tool and temporal-cassandra-tool they are running, making it difficult to track which code version is deployed.",
      "rootCause": "Version is hardcoded in main.go and cannot be set via Go build flags (-ldflags) because the variable is scoped within a function.",
      "proposedFix": "Implement proper version setting mechanism during build process, potentially folding these tools into tdbg sub commands instead of maintaining separate binaries.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Plan to fold temporal-sql-tool and temporal-cassandra-tool into tdbg sub commands to resolve versioning complexity.",
      "related": [],
      "keyQuote": "We will likely fold these tools into `tdbg` sub commands instead of maintaining separate binaries.",
      "number": 7992,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:49.176Z"
    },
    {
      "summary": "CVEs reported in admin-tools Docker image (version 1.28.0-tctl-1.18.2-cli-1.3.0) affecting multiple binaries. Issue was closed in favor of a newer related issue.",
      "category": "bug",
      "subcategory": "security-vulnerabilities",
      "apis": [],
      "components": [
        "admin-tools",
        "docker-image",
        "container"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "cve",
        "container-image",
        "dependency",
        "image-deprecation"
      ],
      "severity": "critical",
      "userImpact": "Users running the admin-tools image are exposed to critical CVEs that could compromise security.",
      "rootCause": "Dependencies in admin-tools image contain known vulnerabilities (CVE-2025-22868, CVE-2025-22869, CVE-2025-22870, CVE-2025-22871, CVE-2025-22872).",
      "proposedFix": "Deprecate the admin-tools image and only package the minimal set of tools needed in images.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed in favor of issue #8220 which is the newer, related issue.",
      "related": [
        8220
      ],
      "keyQuote": "Closing, since #8220 is newer.",
      "number": 7986,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:47.785Z"
    },
    {
      "summary": "Cron jobs scheduled with CRON_TZ incorrectly shift by one hour after DST transitions when the server uses an out-of-date tzdata database that doesn't include recent timezone rule changes. After Cairo's 2023 DST transition, a job scheduled for 09:59 AM executed at 10:59 AM instead.",
      "category": "bug",
      "subcategory": "cron-scheduling",
      "apis": [],
      "components": [
        "cron-scheduler",
        "timezone-handler",
        "server"
      ],
      "concepts": [
        "DST",
        "timezone",
        "scheduling",
        "tzdata",
        "local-time",
        "wall-clock"
      ],
      "severity": "medium",
      "userImpact": "Users with outdated tzdata may see their cron jobs execute at incorrect local times after DST transitions, causing missed or misaligned scheduled workflows.",
      "rootCause": "Server uses out-of-date tzdata database (2022f) that doesn't include 2023 DST rule updates for affected timezones like Africa/Cairo.",
      "proposedFix": "Update tzdata on server pods to a version that includes recent timezone rule changes (2023 or later).",
      "workaround": "Update the tzdata package on your Temporal server deployment to a version that includes the necessary timezone rule updates.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by updating tzdata on the server pods to include recent DST rule changes.",
      "related": [],
      "keyQuote": "If you update tzdata on your pods that should resolve the issue.",
      "number": 7983,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:50.150Z"
    },
    {
      "summary": "User experiences unexpected latency spikes when scheduling multiple activities with limited concurrency, where workflow completion time occasionally increases from 500-600ms to 2+ seconds despite activities being properly queued.",
      "category": "bug",
      "subcategory": "activity-scheduling",
      "apis": [
        "ExecuteActivity",
        "StartWorkflow"
      ],
      "components": [
        "activity-scheduler",
        "worker",
        "activity-executor"
      ],
      "concepts": [
        "latency",
        "activity-scheduling",
        "concurrency",
        "scheduling-delay",
        "queueing",
        "performance-variance"
      ],
      "severity": "medium",
      "userImpact": "Unpredictable latency spikes in activity execution create performance inconsistencies that are difficult to debug and affect workflow completion times.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue closed as discussion moved to Slack; no definitive resolution documented",
      "related": [],
      "keyQuote": "I don't understand why there are such long idle time between activity scheduled and activity started",
      "number": 7982,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:36.221Z"
    },
    {
      "summary": "User unable to add search attributes when using GCS archiver with GCP application-default service account. Error occurs during search attribute creation despite successful archival operations with the same configuration.",
      "category": "bug",
      "subcategory": "archiver-gcs",
      "apis": [],
      "components": [
        "archiver-provider",
        "gcs-connector",
        "visibility-store",
        "search-attributes"
      ],
      "concepts": [
        "credentials",
        "gcp-authentication",
        "archival",
        "visibility",
        "search-attributes",
        "bootstrap"
      ],
      "severity": "medium",
      "userImpact": "Users configuring Temporal with GCS archival using application-default service accounts cannot create search attributes for visibility stores.",
      "rootCause": "Missing bootstrap container initialization when using GCS provider with application-default credentials, fixed in v1.28.0 by PR #7766.",
      "proposedFix": "Upgrade to v1.28.0 or later where the bootstrap container initialization issue was resolved.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved in v1.28.0 by PR #7766 which fixed the 'unable to find bootstrap container' error.",
      "related": [
        7766
      ],
      "keyQuote": "unable to find bootstrap container for the given service name",
      "number": 7981,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:37.349Z"
    },
    {
      "summary": "Users need an easier way to discover available dynamic configuration keys. Currently, they must manually check the source code at a specific git SHA, requiring knowledge of the running version and corresponding commit hash.",
      "category": "feature",
      "subcategory": "cli-webui",
      "apis": [],
      "components": [
        "cli",
        "webui",
        "dynamic-config"
      ],
      "concepts": [
        "configuration",
        "discoverability",
        "documentation",
        "version-management",
        "server-setup"
      ],
      "severity": "low",
      "userImpact": "Users must perform cumbersome manual cross-referencing between release notes and source code to find available dynamic configuration options.",
      "rootCause": null,
      "proposedFix": "Add a CLI or Web UI option to list all dynamic configuration key-value pairs available in the running server version.",
      "workaround": "Check the source code at the appropriate git SHA for the running version, cross-reference against release notes.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A CLI/Web UI option that prints out all dynamic configs KV pairs would make things much easier.",
      "number": 7947,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:35.362Z"
    },
    {
      "summary": "Request to allow authentication headers to be sent over HTTP for localhost development, particularly for codec servers. Currently, auth headers are only allowed over HTTPS, requiring developers to use workarounds like ngrok to test locally.",
      "category": "feature",
      "subcategory": "codec-server",
      "apis": [],
      "components": [
        "codec-server",
        "http-client",
        "authentication"
      ],
      "concepts": [
        "localhost",
        "development",
        "auth-headers",
        "http-tls",
        "oauth",
        "certificate-validation"
      ],
      "severity": "low",
      "userImpact": "Developers must use tunneling services like ngrok instead of testing codec servers directly on localhost during development.",
      "rootCause": "Strict HTTPS requirement for sending authentication headers prevents localhost HTTP usage despite OAuth providers allowing it.",
      "proposedFix": "Allow http://localhost as an exception to the HTTPS requirement for auth headers",
      "workaround": "Use ngrok or similar tunneling service to provide HTTPS endpoint for localhost development",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When developing a codec server, it's always frustrating to have to run my localhost version through ngrok to get an HTTPS address.",
      "number": 7940,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:24.830Z"
    },
    {
      "summary": "Error message when PostgreSQL misconfiguration occurs is too generic ('no usable database connection found') and doesn't help users debug the actual underlying issue. The error message regression was introduced in PR #5926 where connection errors are logged but not returned in the reconnect method.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "postgres",
        "database-connection",
        "schema-initialization",
        "error-handling"
      ],
      "concepts": [
        "error-message",
        "database-connection",
        "postgres-misconfiguration",
        "debugging",
        "logging",
        "schema-version-check"
      ],
      "severity": "high",
      "userImpact": "Users cannot effectively debug PostgreSQL configuration issues because generic error messages hide the actual connection or configuration problems.",
      "rootCause": "In PR #5926, connection errors are only logged but not returned from the reconnect method, causing the schema version compatibility check to report a generic 'no usable database connection found' error instead of the underlying cause.",
      "proposedFix": "Return the connection error from the reconnect method instead of just logging it, so the actual error is propagated to the user.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The connection error is now returned from the reconnect method to provide the actual underlying error instead of a generic message.",
      "related": [
        5926
      ],
      "keyQuote": "the connection error only got logged but not returned...Shouldn't be hard to just return the connection error from the `reconnect` method.",
      "number": 7939,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:24.730Z"
    },
    {
      "summary": "Request to add the ability to set workflow run/execution timeout values when resetting a running or completed execution. This would allow overriding existing timeouts or setting them if none were previously defined.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [
        "ResetWorkflowExecution"
      ],
      "components": [
        "workflow-reset",
        "execution-management",
        "timeout-handling"
      ],
      "concepts": [
        "timeout",
        "workflow-reset",
        "execution-lifecycle",
        "configuration",
        "override"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently configure workflow timeouts during reset operations, limiting flexibility in managing long-running or re-executed workflows.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add ability to set workflow run/execution timeout when resetting a running / completed execution.",
      "number": 7933,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:21.622Z"
    },
    {
      "summary": "Replace the deprecated olivere/elastic v7 client with the official elastic/go-elasticsearch client. The current dependency on olivere/elastic is deprecated and also transitively depends on AWS SDK v1 which has reached end-of-life.",
      "category": "feature",
      "subcategory": "dependency-upgrade",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "search-persistence"
      ],
      "concepts": [
        "dependency-management",
        "client-migration",
        "opensearch-compatibility",
        "aws-sdk-eol",
        "authentication"
      ],
      "severity": "high",
      "userImpact": "Users are blocked from using the latest versions of AWS SDK and Elasticsearch client libraries, and may face security or compatibility issues.",
      "rootCause": "olivere/elastic is deprecated and depends on AWS SDK v1 which has reached end-of-life",
      "proposedFix": "Migrate from github.com/olivere/elastic/v7 to the official github.com/elastic/go-elasticsearch client",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4499,
        5680
      ],
      "keyQuote": "we are also interested in this migration...because it depends on the AWS SDK v1 which has reached EOL",
      "number": 7930,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:10.461Z"
    },
    {
      "summary": "Workflow history disappears when restarting Temporal dev server with SQLite persistence. Starting the server with an existing SQLite database using --db-filename results in event history being cleared from the history_tree table.",
      "category": "bug",
      "subcategory": "persistence-sqlite",
      "apis": [],
      "components": [
        "dev-server",
        "sqlite-persistence",
        "workflow-history"
      ],
      "concepts": [
        "database-persistence",
        "server-restart",
        "state-retention",
        "data-loss",
        "sqlite"
      ],
      "severity": "high",
      "userImpact": "Users lose all workflow history and data when restarting the dev server with an existing SQLite database, making local testing with persistent data impossible.",
      "rootCause": "Potential retention timers triggering on workflows stored in the database upon server startup, causing automatic cleanup of old events.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Could not be reproduced by maintainers. Theory suggests retention timers were triggered on stored workflows. Issue closed due to inability to provide reliable reproduction, though user recommends using workflow list command or UI instead of inspecting internal DB state.",
      "related": [],
      "keyQuote": "The current theory is that retention timers were triggered on the workflows stored in the DB.",
      "number": 7919,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:12.016Z"
    },
    {
      "summary": "Request to allow workers to listen on multiple task queues with independent rate limits, enabling fine-grained throttling of specific activities to prevent overloading downstream systems.",
      "category": "feature",
      "subcategory": "task-queue-routing",
      "apis": [
        "ExecuteActivity",
        "WithActivityOptions"
      ],
      "components": [
        "worker",
        "task-queue",
        "activity-executor",
        "rate-limiter"
      ],
      "concepts": [
        "throttling",
        "rate-limiting",
        "task-queue",
        "activity-routing",
        "load-management",
        "downstream-protection"
      ],
      "severity": "medium",
      "userImpact": "Users managing downstream API calls need external workarounds or multiple worker instances to implement per-activity throttling, increasing complexity and resource overhead.",
      "rootCause": "Current worker architecture supports only a single task queue per worker instance, forcing users to run multiple workers for different rate limits.",
      "proposedFix": "Allow worker.New() to accept multiple task queues with independent QueueOptions including TaskQueueActivitiesPerSecond, and enable activity execution to specify target queue via ActivityOptions.",
      "workaround": "Run multiple workers in the same process, each listening on different task queues with different rate limits, though this requires manual resource sharing management.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the general direction we're going in is to make a single task queue more powerful and configurable instead of requiring the use of multiple task queues",
      "number": 7916,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:22:10.486Z"
    },
    {
      "summary": "Worker process fails to pick up tasks when Temporal server on-prem Kubernetes cluster connects to AWS RDS Postgres database, with persistent fetch operation failures and context cancellation errors during workflow execution retrieval.",
      "category": "bug",
      "subcategory": "database-connectivity",
      "apis": [],
      "components": [
        "history-service",
        "workflow-execution",
        "persistence-layer"
      ],
      "concepts": [
        "database-timeout",
        "context-cancellation",
        "network-latency",
        "signal-retrieval",
        "persistence-qps"
      ],
      "severity": "high",
      "userImpact": "Workers cannot process tasks when using AWS RDS as the database backend, causing complete workflow processing failures in production deployments.",
      "rootCause": "Database operation timeouts and delays when Temporal pods interact with remote AWS RDS, particularly during signal info retrieval operations.",
      "proposedFix": "Tune dynamic configurations like history.persistenceGlobalMaxQPS to rate limit database requests and prevent overload from persistence operations.",
      "workaround": "Adjust persistence configuration settings to rate limit requests sent to the database and maintain database stability.",
      "resolution": "invalid",
      "resolutionDetails": "Closed as the issue was determined to be a configuration/deployment problem rather than a code bug. Community support recommended for self-hosting issues.",
      "related": [],
      "keyQuote": "Temporal pod is highly sensitive to delays and timeouts when interacting with the database. However, I couldn't find any environment configuration that could mitigate this issue.",
      "number": 7900,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:59.654Z"
    },
    {
      "summary": "Users cannot load archived workflows on the UI despite having 10,000+ workflows archived in S3/GCS, with the API returning 504 errors and timeouts when listing archived workflows.",
      "category": "bug",
      "subcategory": "archived-workflows",
      "apis": [],
      "components": [
        "ui",
        "archived-workflows-api",
        "visibility-store",
        "storage-backend"
      ],
      "concepts": [
        "timeout",
        "performance",
        "scalability",
        "archived-workflows",
        "visibility",
        "storage-access",
        "api-errors"
      ],
      "severity": "high",
      "userImpact": "Users with large numbers of archived workflows cannot access them through the UI or CLI, blocking visibility into archived workflow history.",
      "rootCause": "Timeout issues when retrieving large numbers of archived workflows from S3/GCS storage backends, likely due to performance degradation with high object counts (2.2GB+ buckets with 56k+ workflows).",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The bucket has around 2.2GB of objects... context deadline exceeded",
      "number": 7894,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:56.928Z"
    },
    {
      "summary": "Request to support Workflow Id Conflict Policy selection for child workflows, specifically the 'Use Existing' reuse policy, to enable child workflow reuse when parent workflows are reset. Currently unavailable across all SDKs.",
      "category": "feature",
      "subcategory": "child-workflows",
      "apis": [
        "StartChildWorkflow"
      ],
      "components": [
        "child-workflow-executor",
        "workflow-id-management",
        "reuse-policy"
      ],
      "concepts": [
        "child-workflows",
        "workflow-id-conflict",
        "reuse-policy",
        "parent-child-orchestration",
        "workflow-reset",
        "state-isolation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot configure 'Use Existing' reuse policy for child workflows, limiting workflow reset patterns and forcing suboptimal alternatives like terminating running children.",
      "rootCause": null,
      "proposedFix": "Add support for selecting Workflow Id Conflict Policy (specifically 'Use Existing') when starting child workflows in all SDKs.",
      "workaround": "Either redesign workflow pattern to avoid child workflows, or use 'Terminate if Running' reuse policy despite child workflows potentially being healthy.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Support for selecting the \"Workflow Id Conflict Policy\" when starting a child workflow (in Python).",
      "number": 7882,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:58.349Z"
    },
    {
      "summary": "Temporal deployment on Kubernetes using Helm chart fails to connect to AWS RDS PostgreSQL database with TLS enabled. The issue was resolved by adding explicit TLS configuration to the database connection settings and importing the AWS CA certificate.",
      "category": "bug",
      "subcategory": "database-connection",
      "apis": [],
      "components": [
        "persistence",
        "sql-driver",
        "database-connection",
        "helm-chart"
      ],
      "concepts": [
        "TLS",
        "RDS",
        "PostgreSQL",
        "Kubernetes",
        "database-configuration",
        "SSL-certificate"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal on Kubernetes with RDS struggle to establish database connections when TLS is enforced, preventing cluster initialization.",
      "rootCause": "Missing or incomplete TLS configuration in the Helm chart database persistence settings when connecting to RDS with SSL/TLS enforcement enabled.",
      "proposedFix": "Add explicit TLS configuration block with enabled: true and enableHostVerification: false to the SQL datastores configuration, and import the AWS CA certificate.",
      "workaround": "Disable RDS SSL enforcement (rds.force_ssl=0) or use Docker-compose with SQL_TLS_ENABLED and SQL_HOST_VERIFICATION environment variables set appropriately.",
      "resolution": "fixed",
      "resolutionDetails": "User confirmed that adding TLS configuration with disabled host verification and importing AWS CA certificate resolves the connection issue.",
      "related": [],
      "keyQuote": "Try adding the following TLS configuration: ... tls: enabled: true enableHostVerification: false ... For more details: https://docs.temporal.io/references/configuration#tls-1",
      "number": 7873,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:47.024Z"
    },
    {
      "summary": "Temporal frontend deployment with Elasticsearch visibility store fails with a Cassandra port requirement error even though Cassandra is disabled and not being used.",
      "category": "bug",
      "subcategory": "elasticsearch-visibility",
      "apis": [],
      "components": [
        "helm-chart",
        "server-deployment",
        "visibility-store",
        "elasticsearch"
      ],
      "concepts": [
        "visibility-store",
        "elasticsearch",
        "cassandra",
        "configuration",
        "helm-deployment",
        "persistence"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy Temporal with Elasticsearch as the visibility store due to an incorrect validation requirement for Cassandra configuration.",
      "rootCause": "Helm chart template has a validation check that requires Cassandra port to be specified for visibility store, even when Cassandra is not enabled and Elasticsearch is configured as the visibility store.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Please specify cassandra port for visibility store.",
      "number": 7869,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:44.326Z"
    },
    {
      "summary": "Request to upgrade Go version from current to 1.23.5 to address multiple reported security vulnerabilities (CVEs) affecting the open source version.",
      "category": "bug",
      "subcategory": "dependency-update",
      "apis": [],
      "components": [
        "build-system",
        "golang-runtime",
        "ci-cd"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "dependency-management",
        "version-upgrade",
        "cve-patching"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to known security vulnerabilities in the Go runtime that could be exploited in deployed Temporal systems.",
      "rootCause": "Current Go version contains unpatched security vulnerabilities (CVE-2024-45341, CVE-2025-22866, CVE-2024-45336, CVE-2024-9355).",
      "proposedFix": "Upgrade Go version to 1.23.5 which includes patches for the reported CVEs.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Please upgrade the golang version to 1.23.5 as we have bugs reported for the open source version",
      "number": 7867,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:44.027Z"
    },
    {
      "summary": "User requests AWS STS support for S3 archival in self-hosted Temporal. Currently requires actual AWS access keys/secrets, but infrastructure only supports STS authentication. Works with actual credentials but STS is needed due to infrastructure restrictions.",
      "category": "feature",
      "subcategory": "archival-s3-authentication",
      "apis": [],
      "components": [
        "archival",
        "s3-provider",
        "authentication"
      ],
      "concepts": [
        "AWS STS",
        "S3 archival",
        "authentication",
        "IAM roles",
        "credentials",
        "infrastructure constraints"
      ],
      "severity": "medium",
      "userImpact": "Users with STS-only infrastructure cannot use Temporal archival to S3 without violating security policies requiring actual AWS credentials.",
      "rootCause": "Temporal server's S3 archival provider does not support AWS STS role assumption, only static access key and secret authentication.",
      "proposedFix": "Implement AWS STS support in S3 archival provider to allow role assumption via AWS_ROLE_ARN environment variable, similar to AWS SDK standard patterns.",
      "workaround": "Set AWS_ROLE_ARN environment variable and use role-based authentication (requires EC2/ECS/EKS setup with proper role configuration).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We are unable to access AWS S3 from the Temporal service because it does not support AWS STS. The Temporal server requires actual access_key and secret, but due to infrastructure restrictions, we cannot use actual AWS credentials.",
      "number": 7866,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:33.932Z"
    },
    {
      "summary": "System workers fail to authenticate with the frontend service when JWT authorization is enabled on a Temporal cluster, resulting in \"Request unauthorized\" errors. The issue can be resolved by using the internal-frontend deployment option or mTLS configuration.",
      "category": "bug",
      "subcategory": "authentication-authorization",
      "apis": [],
      "components": [
        "worker",
        "frontend",
        "authorization",
        "jwt"
      ],
      "concepts": [
        "authentication",
        "jwt",
        "authorization",
        "system-workers",
        "internal-frontend",
        "credentials"
      ],
      "severity": "high",
      "userImpact": "System workers cannot perform essential operations like history scanning and workflow cleanup when JWT authorization is enabled, causing cluster functionality to degrade.",
      "rootCause": "System workers lack proper authentication credentials when JWT authorization is enforced; they need either internal-frontend bypass or mTLS configuration to authenticate.",
      "proposedFix": "Enable internal-frontend deployment using `internalFrontend: enabled: true` configuration, or configure mTLS with proper claim mapper integration.",
      "workaround": "Use internal-frontend deployment or implement mTLS authentication for system workers as documented in samples-server.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue resolved as working as intended - users must use internal-frontend or mTLS for system worker authentication when JWT is enabled.",
      "related": [],
      "keyQuote": "You'll want to use internal-frontend for the easiest integration. It was introduced exactly to support this purpose.",
      "number": 7857,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:32.729Z"
    },
    {
      "summary": "User is asking whether the SSL/TLS configuration properties shown in the config_template.yml are sufficient for enabling SSL in the Temporal engine. They've modified default values and added environment variables but are seeking confirmation of the correct setup.",
      "category": "question",
      "subcategory": "ssl-tls-configuration",
      "apis": [],
      "components": [
        "config-template",
        "docker-compose",
        "tls-configuration",
        "database-connection"
      ],
      "concepts": [
        "ssl",
        "tls",
        "encryption",
        "host-verification",
        "certificate",
        "configuration",
        "security"
      ],
      "severity": "low",
      "userImpact": "Users need clear guidance on proper SSL/TLS configuration to securely connect to databases in Temporal deployments.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Before both default .Env.SQL_TLS_ENABLED and .Env.SQL_HOST_VERIFICATION are false and i changed it to true",
      "number": 7845,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:31.016Z"
    },
    {
      "summary": "The 'temporal workflow list' command requires different filter syntax for archived workflows versus hot workflows. Using 'WorkflowType' fails with archived workflows but succeeds with hot workflows, while 'WorkflowTypeName' works with archived workflows, creating an inconsistent query interface.",
      "category": "bug",
      "subcategory": "workflow-list-query",
      "apis": [],
      "components": [
        "workflow-list",
        "query-filter",
        "archival-backend"
      ],
      "concepts": [
        "query-syntax",
        "archived-workflows",
        "filter-translation",
        "backend-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users must use different query filter names for archived and hot workflows, preventing uniform query syntax across workflow states.",
      "rootCause": "Server limitation and query capability differences between hot and archival backends; filter name translation not supported across backends.",
      "proposedFix": "Implement filter name translation/mapping to normalize query syntax between hot and archived workflows.",
      "workaround": "Use 'WorkflowTypeName' instead of 'WorkflowType' when querying archived workflows.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We should be able to fix/translate the filter name. But the query capability on different backends are very different",
      "number": 7821,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:20.996Z"
    },
    {
      "summary": "User unable to create namespace using CLI when TLS is configured on Kubernetes deployment. Connection fails with \"error reading server preface: EOF\" when attempting to reach the server.",
      "category": "bug",
      "subcategory": "namespace-management",
      "apis": [],
      "components": [
        "namespace-operator",
        "cli",
        "tls-configuration"
      ],
      "concepts": [
        "namespace-creation",
        "tls",
        "kubernetes",
        "connection-error",
        "operator-cli"
      ],
      "severity": "high",
      "userImpact": "Users cannot create namespaces on Kubernetes deployments with TLS enabled, blocking namespace setup operations.",
      "rootCause": "Server connection failure when TLS is configured; possible TLS handshake or certificate validation issue with the operator CLI connecting to the temporal server.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was closed by reporter without explanation; likely invalid or already resolved on their end.",
      "related": [],
      "keyQuote": "failed reaching server: last connection error: connection error: desc = \"error reading server preface: EOF\"",
      "number": 7801,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:21.593Z"
    },
    {
      "summary": "System worker fails to authenticate with \"Request unauthorized\" error when SSO with JWT authorization is enabled. The scanner workflow cannot access the frontend, preventing the temporal-sys-history-scanner-workflow from starting.",
      "category": "bug",
      "subcategory": "authentication-sso",
      "apis": [],
      "components": [
        "worker",
        "scanner",
        "authentication",
        "frontend"
      ],
      "concepts": [
        "SSO",
        "JWT",
        "authorization",
        "authentication",
        "system-worker",
        "security",
        "access-control"
      ],
      "severity": "high",
      "userImpact": "Users enabling SSO with JWT authorization in self-hosted Kubernetes deployments cannot run Temporal server because the system worker fails to authenticate with the frontend.",
      "rootCause": "System worker does not use the internal-frontend or does not have proper credentials configured to authenticate against JWT-protected frontend when SSO is enabled.",
      "proposedFix": "Use internal-frontend configuration as suggested in release notes (v1.20.0), which provides internal access without JWT authentication requirements.",
      "workaround": "Configure internal-frontend to bypass JWT authentication for internal system worker communication.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "if i enable SSO with JWT authorization the system worker cannot access the frontend",
      "number": 7800,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:20.682Z"
    },
    {
      "summary": "GetNextTime function enters an infinite loop when processing certain bad schedule specifications, causing high CPU usage on the Temporal frontend process. A nonsensical schedule that tries to run on Sundays while also skipping Sundays triggers this issue despite a previous fix attempt in PR #7389.",
      "category": "bug",
      "subcategory": "scheduling",
      "apis": [],
      "components": [
        "GetNextTime",
        "schedule-processor",
        "frontend"
      ],
      "concepts": [
        "infinite-loop",
        "CPU-usage",
        "schedule-validation",
        "bad-input-handling",
        "performance-degradation"
      ],
      "severity": "high",
      "userImpact": "Users can accidentally create schedules that cause their Temporal frontend process to consume all available CPU and become unresponsive.",
      "rootCause": "The schedule validation logic does not properly handle contradictory schedule specifications (e.g., intervals that skip all possible execution days), causing GetNextTime to loop infinitely.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was identified and a fix was implemented, though the initial fix in PR #7389 was incomplete and the issue persisted in version 1.27.2.",
      "related": [
        7389
      ],
      "keyQuote": "This is a nonsensical spec, as it's trying to run on Sundays and also skip Sundays, but I think Temporal should handle that in some way",
      "number": 7793,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:09.549Z"
    },
    {
      "summary": "Request to support HTTPS metrics endpoint on OSS Temporal with optional custom certificates, allowing secure exposure of metrics APIs in Kubernetes environments where only secure endpoints are permitted.",
      "category": "feature",
      "subcategory": "metrics-endpoint",
      "apis": [],
      "components": [
        "metrics-endpoint",
        "tls-configuration",
        "certificate-management"
      ],
      "concepts": [
        "https",
        "metrics",
        "tls",
        "certificate",
        "kubernetes",
        "prometheus",
        "security"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal in Kubernetes clusters with security policies requiring HTTPS for all endpoints cannot expose metrics securely without workarounds.",
      "rootCause": null,
      "proposedFix": "Add support for loading a dedicated certificate different from inter-node and frontend certificates for the metrics endpoint, allowing integration with service mesh certificates.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A feature that allows to load a dedicated certificate different from the Temporal's inter-node and frontend certificate for the metrics endpoint.",
      "number": 7780,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:09.121Z"
    },
    {
      "summary": "Workflow task completions that exceed gRPC's 4MB message size limit cause server failures. A solution is needed to handle large tasks, potentially through splitting responses into multiple calls or using alternative approaches.",
      "category": "feature",
      "subcategory": "grpc-limits",
      "apis": [],
      "components": [
        "server",
        "grpc",
        "task-handler"
      ],
      "concepts": [
        "message-size-limits",
        "grpc-protocol",
        "task-response",
        "scaling",
        "protocol-design"
      ],
      "severity": "high",
      "userImpact": "Users cannot complete workflows with large task responses, causing failures in production systems handling substantial data.",
      "rootCause": "gRPC enforces a maximum message size of 4MB, and current implementation does not handle task responses exceeding this limit.",
      "proposedFix": "Implement a solution to split large task responses into multiple calls or tasks, with the exact approach to be determined.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        624
      ],
      "keyQuote": "Today when a > 4MB gRPC request is sent, server fails.",
      "number": 7777,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:21:09.546Z"
    },
    {
      "summary": "User requests TLS support for metrics endpoint in Temporal Server. Currently metrics are served over HTTP even when TLS is enabled for all other components in a Helm-deployed cluster.",
      "category": "feature",
      "subcategory": "metrics-tls",
      "apis": [],
      "components": [
        "metrics",
        "server",
        "tls"
      ],
      "concepts": [
        "tls",
        "security",
        "metrics",
        "encryption",
        "helm",
        "deployment"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal with TLS security cannot secure metrics endpoints, creating a potential security gap in encrypted deployments.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "unknown",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "enabled TLS for all components but metrics continue to be served with http",
      "number": 7742,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:55.740Z"
    },
    {
      "summary": "Workflow updates sent to cached workflows trigger transient WFT failures with 'Premature end of stream' errors. While subsequent WFT fetches recover by replaying full history, these transient failures are causing unnecessary alerts at workers.",
      "category": "bug",
      "subcategory": "workflow-update",
      "apis": [],
      "components": [
        "workflow-task-processor",
        "stream-handler",
        "history-fetcher"
      ],
      "concepts": [
        "workflow-update",
        "premature-end-of-stream",
        "transient-error",
        "history-replay",
        "stream-protocol",
        "cached-workflow"
      ],
      "severity": "medium",
      "userImpact": "Users receive false alerts about workflow task failures when workflow updates are applied to cached workflows, causing operational noise in monitoring systems.",
      "rootCause": "Stream protocol handling issue when workflow updates are applied to workflows in cache, causing premature stream termination that is recovered on subsequent fetches.",
      "proposedFix": null,
      "workaround": "Monitor and ignore transient WFT failures as they are benign; subsequent WFT fetches will automatically recover by replaying entire history.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        7732,
        7878
      ],
      "keyQuote": "We know this is a transient and benign error, subsequent WFT fetches entire history and resumes well",
      "number": 7741,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:57.204Z"
    },
    {
      "summary": "User reports that running temporal dev server prevents their application from accessing remote PostgreSQL databases (e.g., Neon), with connections timing out. The issue was resolved through refactoring shared database management code used across activities.",
      "category": "bug",
      "subcategory": "dev-server-networking",
      "apis": [],
      "components": [
        "dev-server",
        "networking",
        "database-connections"
      ],
      "concepts": [
        "network-isolation",
        "database-access",
        "remote-connections",
        "port-conflicts",
        "activity-execution",
        "shared-state"
      ],
      "severity": "medium",
      "userImpact": "Developers using temporal dev server cannot access remote databases from their application code, blocking development and testing workflows.",
      "rootCause": "Shared database management code across activities was breaking when temporal dev server was running, causing connection timeouts.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "User refactored shared database management code that was being used across activities, which resolved the connectivity issue.",
      "related": [],
      "keyQuote": "the issue was around shared database management code that was being shared across activities and breaking everything",
      "number": 7731,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:58.066Z"
    },
    {
      "summary": "Need to expose the API version to API users at runtime and via the API itself. Requires research on how to obtain the API version in code and how to expose it through the API (e.g., headers, DescribeNamespace) without impacting all responses.",
      "category": "feature",
      "subcategory": "api-versioning",
      "apis": [],
      "components": [
        "api-go",
        "system-info",
        "describe-namespace"
      ],
      "concepts": [
        "versioning",
        "api-version",
        "backward-compatibility",
        "runtime-discovery",
        "namespace-metadata",
        "build-info"
      ],
      "severity": "medium",
      "userImpact": "Users, especially companies with proxies, need to discover which API version they're using to handle incompatible changes in non-GA APIs.",
      "rootCause": null,
      "proposedFix": "Research two aspects: 1) obtaining API version in code via manual const, build-flag, or debug.ReadBuildInfo, 2) exposing version via response header, DescribeNamespace, or alternative mechanism",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We need a way to expose the version of https://github.com/temporalio/api currently in use to users. This is helpful for companies with proxies",
      "number": 7721,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:43.155Z"
    },
    {
      "summary": "Developer experience issue with manual restart of workflow and worker processes during development. User requests automatic restart on code changes and improved connection reliability during dev mode restarts, similar to hot-reload in npm/uv.",
      "category": "feature",
      "subcategory": "dev-mode",
      "apis": [],
      "components": [
        "workflow",
        "worker",
        "dev-mode-ui"
      ],
      "concepts": [
        "hot-reload",
        "automatic-restart",
        "connection-reliability",
        "developer-experience",
        "process-management",
        "graceful-shutdown"
      ],
      "severity": "medium",
      "userImpact": "Developers waste time manually restarting multiple terminals and troubleshooting connection issues during the development cycle.",
      "rootCause": "Dev mode lacks automatic process restart on code changes and proper connection sequencing when processes restart in different orders.",
      "proposedFix": "Implement a kill switch in dev mode UI and automatic restart of dev-mode-specific processes with proper connection ordering.",
      "workaround": "Manually kill all terminals and restart them in the correct order.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I do not like restarting 3 terminals in dev mode. Either there should be an auto workflow/worker restart feature whenever a change is made",
      "number": 7708,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:44.411Z"
    },
    {
      "summary": "Feature request for a built-in concurrency limit (semaphore) per Workflow type to prevent too many instances of the same workflow from running simultaneously in a namespace or cluster. Users currently lack a native way to enforce this limit without custom dispatcher workflows or external distributed locks.",
      "category": "feature",
      "subcategory": "workflow-concurrency-control",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-dispatcher",
        "concurrency-control",
        "workflow-execution"
      ],
      "concepts": [
        "concurrency-limit",
        "semaphore",
        "throttling",
        "workflow-type",
        "resource-protection",
        "batching"
      ],
      "severity": "high",
      "userImpact": "Users must implement complex workarounds with custom dispatchers or external distributed locks to enforce per-workflow-type concurrency limits, increasing operational complexity and maintenance burden.",
      "rootCause": null,
      "proposedFix": "Add built-in semaphore/concurrency limit per Workflow type, configurable via API or workflow start options, so Temporal natively restricts simultaneously running workflows of a given type.",
      "workaround": "Custom dispatcher workflows or external distributed locks can be used, though these solutions are complex, fragile, and operationally heavy.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A built-in semaphore would be a general and powerful solution for throttling, batching, and resource protection for a wide range of use cases.",
      "number": 7666,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:45.713Z"
    },
    {
      "summary": "A Series A startup asks for comparisons and experiences with Temporal versus competing orchestrators (Prefect and Flyte) for ML pipeline orchestration. A maintainer redirects the discussion to community channels.",
      "category": "question",
      "subcategory": "community-discussion",
      "apis": [],
      "components": [],
      "concepts": [
        "orchestration",
        "ml-pipelines",
        "tool-comparison",
        "use-case-evaluation"
      ],
      "severity": "low",
      "userImpact": "User seeks guidance on choosing between workflow orchestrators for their specific use case.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer redirected the discussion to appropriate community channels (community.temporal.io or Slack) as this type of question is outside the scope of the issue tracker.",
      "related": [],
      "keyQuote": "Could you please move this discussion to https://community.temporal.io/ or https://temporal.io/slack?",
      "number": 7659,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:32.255Z"
    },
    {
      "summary": "Request to add configurable prefixes for Temporal Server table names to prevent naming conflicts when multiple deployments share a single database schema. Users could optionally prefix all tables (e.g., 'dev_executions', 'dev_schema_version') while maintaining full compatibility with existing operations.",
      "category": "feature",
      "subcategory": "database-schema",
      "apis": [],
      "components": [
        "schema-manager",
        "database-layer",
        "migration-system"
      ],
      "concepts": [
        "table-naming",
        "multi-tenancy",
        "schema-isolation",
        "configuration",
        "database-compatibility"
      ],
      "severity": "low",
      "userImpact": "Users sharing a single database schema across multiple Temporal deployments would gain flexibility to avoid naming conflicts and manage environments more easily.",
      "rootCause": null,
      "proposedFix": "Add an optional configuration setting during schema initialization to specify a custom prefix that applies to all Temporal tables, with examples like 'dev_' resulting in 'dev_executions' and 'dev_schema_version'.",
      "workaround": "Use separate database schemas for different Temporal deployments.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "most users can use separate schemas for their different deployments. Happy to take a contribution if this is important to you.",
      "number": 7658,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:33.309Z"
    },
    {
      "summary": "Temporal and Visibility schemas cannot coexist in the same database because both use the same `schema_version` table name, causing versioning conflicts. The proposal is to rename the Visibility schema's versioning table to avoid this naming conflict.",
      "category": "feature",
      "subcategory": "schema-versioning",
      "apis": [],
      "components": [
        "visibility-schema",
        "schema-versioning",
        "database-schema"
      ],
      "concepts": [
        "schema-conflict",
        "table-naming",
        "database-coexistence",
        "versioning",
        "schema-upgrade",
        "migration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot run Temporal and Visibility schemas in the same database, limiting deployment flexibility and requiring separate database instances.",
      "rootCause": "Both Temporal and Visibility schemas use identical table name `schema_version` for tracking schema versions, preventing them from coexisting.",
      "proposedFix": "Rename the Visibility schema's versioning table from `schema_version` to `visibility_version` or another distinct name.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        7658
      ],
      "keyQuote": "Temporal and Temporal Visibility schemas currently can't coexist in the same database because both utilize the same table name `schema_version`.",
      "number": 7657,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:31.502Z"
    },
    {
      "summary": "The temporal operator cluster upsert command fails to properly handle gRPC addresses with the passthrough:/// URI prefix. The CreateRemoteFrontendGRPCConnection implementation uses net.SplitHostPort which expects basic host:port format and cannot parse URI-scheme prefixed addresses, causing the passthrough resolver to be ignored.",
      "category": "bug",
      "subcategory": "cluster-replication",
      "apis": [],
      "components": [
        "CreateRemoteFrontendGRPCConnection",
        "rpc",
        "operator-cluster",
        "gRPC-client"
      ],
      "concepts": [
        "multi-cluster-replication",
        "gRPC-resolver",
        "URI-parsing",
        "passthrough-resolver",
        "address-resolution",
        "cluster-metadata"
      ],
      "severity": "medium",
      "userImpact": "Users setting up multi-cluster replication cannot explicitly use the passthrough:/// resolver prefix for remote cluster addresses, limiting their control over gRPC connection resolution behavior.",
      "rootCause": "The net.SplitHostPort function in CreateRemoteFrontendGRPCConnection expects basic host:port format and fails to parse addresses with URI-scheme prefixes like passthrough:///. The health command works because it doesn't split the URI.",
      "proposedFix": "Modify CreateRemoteFrontendGRPCConnection to properly parse and preserve gRPC URI schemes (like passthrough:///) before extracting hostname and port, ensuring the gRPC resolver behavior is respected.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        7744
      ],
      "keyQuote": "net.SplitHostPort function expects a basic host:port format and fails when the address includes a URI-style scheme like passthrough:///.",
      "number": 7640,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:20.443Z"
    },
    {
      "summary": "Request for a dedicated metric to identify corrupted workflows in Temporal. Currently, workflow corruption can only be detected through logs or expensive database scans, making it difficult to identify problematic workflows in large-scale deployments.",
      "category": "feature",
      "subcategory": "observability-metrics",
      "apis": [],
      "components": [
        "persistence-layer",
        "metrics",
        "execution-manager",
        "workflow-state"
      ],
      "concepts": [
        "workflow-corruption",
        "data-loss",
        "metrics-cardinality",
        "database-integrity",
        "detection",
        "monitoring"
      ],
      "severity": "medium",
      "userImpact": "Large-scale deployments struggle to identify and track corrupted workflows without expensive database scans or log parsing.",
      "rootCause": "Workflow corruption can occur at the database layer due to ConditionFailedError during execution updates or DataLoss errors, but current detection requires log parsing or full database scans.",
      "proposedFix": "Add a dedicated workflow_corrupted metric or implement a system workflow for automated corruption scanning, similar to existing internal scanner workflows.",
      "workaround": "Parse error logs to identify corruption, or manually run database scans using admin tools.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "corrupted workflows are either: Logged & counted in namespace-level metric or Logged only without workflow info",
      "number": 7637,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:21.647Z"
    },
    {
      "summary": "Feature request to build Temporal Server binaries with FIPS 140-3 mode enabled to meet compliance requirements. Users need FIPS-compliant builds for environments with strict security standards.",
      "category": "feature",
      "subcategory": "build-configuration",
      "apis": [],
      "components": [
        "build-system",
        "binaries",
        "server"
      ],
      "concepts": [
        "FIPS-compliance",
        "security-standards",
        "cryptography",
        "certification",
        "build-configuration",
        "deployment"
      ],
      "severity": "medium",
      "userImpact": "Users in regulated environments cannot use standard Temporal Server binaries due to FIPS compliance requirements and must build from source instead.",
      "rootCause": null,
      "proposedFix": "Build server binaries with FIPS mode enabled using Go 1.24+ FIPS flag or GOEXPERIMENT boringcrypto for older versions.",
      "workaround": "Users can build the server from source with FIPS flags enabled manually.",
      "resolution": "wontfix",
      "resolutionDetails": "Team decided not to release additional FIPS-specific binaries, recommending users build from source with these flags instead.",
      "related": [],
      "keyQuote": "We currently do not have plans on releasing more binaries. I would encourage building the server from source with these flags for now.",
      "number": 7635,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:18.936Z"
    },
    {
      "summary": "Namespace creation fails with S3 archival enabled when internal-frontend service is running. The archiver provider was being called with FrontendService instead of InternalFrontendService, causing bootstrap container lookup failures.",
      "category": "bug",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "internal-frontend",
        "archiver-provider",
        "workflow-handler"
      ],
      "concepts": [
        "namespace-registration",
        "archival",
        "s3-storage",
        "service-bootstrap",
        "internal-frontend"
      ],
      "severity": "high",
      "userImpact": "Users cannot create namespaces with S3 archival enabled when using the internal-frontend service with authentication enabled.",
      "rootCause": "GetHistoryArchiver and GetVisibilityArchiver were being called with primitives.FrontendService parameter even when the request came through internal-frontend, causing bootstrap container lookup to fail.",
      "proposedFix": "Check both FrontendService and InternalFrontendService when calling GetHistoryArchiver and GetVisibilityArchiver, applying the fallback pattern suggested in comments.",
      "workaround": "Disable internal-frontend service or disable authentication on the frontend service.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #7766 which updated archiver calls to handle both FrontendService and InternalFrontendService appropriately.",
      "related": [
        7748,
        7766
      ],
      "keyQuote": "In case of internal-frontend there must be primitives.InternalFrontendService",
      "number": 7631,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:08.836Z"
    },
    {
      "summary": "User's self-hosted Temporal cluster experiences database performance degradation every 12 hours due to scavenger cleanup jobs. They request per-job RPS configuration (specifically historyScannerRPS) and potentially configurable cron schedules to allow fine-grained tuning of cleanup operations without impacting overall cluster throughput.",
      "category": "feature",
      "subcategory": "scavenger-tuning",
      "apis": [],
      "components": [
        "scavenger",
        "persistence",
        "database",
        "worker-scanner"
      ],
      "concepts": [
        "rate-limiting",
        "database-performance",
        "cleanup-jobs",
        "dynamic-configuration",
        "resource-management",
        "scheduling"
      ],
      "severity": "medium",
      "userImpact": "Operators cannot fine-tune database cleanup operations, causing periodic performance degradation and forcing them to globally reduce persistence throughput.",
      "rootCause": "Scavenger jobs lack granular per-job RPS configuration; history scanner relies only on persistenceMaxQPS which affects all persistence operations, and cron schedule is hardcoded.",
      "proposedFix": "Add dynamic configuration field for historyScannerRPS to allow per-job RPS tuning; optionally make the cron schedule configurable with current value as default.",
      "workaround": "Lower the global persistenceMaxQPS setting, though this reduces cluster throughput outside cleanup windows.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I'd like to add a new dynamic configuration field for historyScannerRPS to tune this job without overwhelm the underlying persistence DB",
      "number": 7625,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:07.765Z"
    },
    {
      "summary": "Request to add support for delayed scheduling of activity tasks to avoid the overhead of using workflow.Sleep. Currently, delaying activity execution requires extra workflow actions.",
      "category": "feature",
      "subcategory": "activity-scheduling",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "activity-executor",
        "scheduler",
        "retry-handler"
      ],
      "concepts": [
        "delay",
        "scheduling",
        "activity-execution",
        "retry-timer",
        "timing"
      ],
      "severity": "low",
      "userImpact": "Users currently must use workflow.Sleep as a workaround to delay activity execution, adding unnecessary workflow actions and complexity.",
      "rootCause": null,
      "proposedFix": "Reuse the existing activity retry timer mechanism for the first attempt if a delay is specified.",
      "workaround": "Use workflow.Sleep to add delay before executing activity task.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Ability to define a delay in scheduling activity task. Currently alternatives like using workflow.Sleep add extra actions",
      "number": 7623,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:20:06.100Z"
    },
    {
      "summary": "GCP Objectstore archival fails because the `storage.buckets.get` IAM permission is required by Temporal's archiver implementation to verify bucket existence, but it's not included in the standard Objectstore binding which only provides object-level permissions.",
      "category": "bug",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "archiver",
        "gcloud-archiver",
        "gcp-integration"
      ],
      "concepts": [
        "gcp-permissions",
        "iam-roles",
        "bucket-verification",
        "service-account",
        "archival-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot set up archival with GCP Objectstore using only object-level permissions because the archiver requires bucket-level access verification.",
      "rootCause": "The GCP archiver implementation at common/archiver/gcloud/history_archiver.go calls bucket.Exists() both during archival and namespace configuration, which requires the storage.buckets.get permission not included in standard Objectstore bindings.",
      "proposedFix": null,
      "workaround": "Add the storage.buckets.get IAM permission to the service account.",
      "resolution": "wontfix",
      "resolutionDetails": "The issue was closed due to lack of priority and interest. The maintainers indicated this is working as designed and suggested adding the required permission is the expected solution.",
      "related": [],
      "keyQuote": "This is not a priority for us now unfortunately. Happy to take a contribution if this is important and adding permission is not an option.",
      "number": 7608,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:53.685Z"
    },
    {
      "summary": "Multiple CVEs reported in temporalio/admin-tools:1.27.2 Docker image including high-severity vulnerabilities in JWT libraries, Go net packages, and other dependencies. The issue tracks security vulnerabilities found during container scanning with multiple fixes available.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "docker-image",
        "admin-tools",
        "dependencies"
      ],
      "concepts": [
        "CVE",
        "vulnerability",
        "security",
        "dependency-management",
        "container-security",
        "patching"
      ],
      "severity": "high",
      "userImpact": "Users deploying the admin-tools:1.27.2 Docker image are exposed to multiple known security vulnerabilities that could be exploited if not patched.",
      "rootCause": "Outdated and vulnerable versions of dependencies including golang-jwt/jwt, golang.org/x/net, golang.org/x/oauth2, sqlite, and other packages bundled in the Docker image.",
      "proposedFix": "Update all vulnerable dependencies to their fixed versions: golang-jwt/jwt to 4.5.2+, golang.org/x/net to 0.33.0+, golang.org/x/oauth2 to 0.27.0+, sqlite to 3.48.0-r1+, postgresql17 to 17.4-r0+, and others as specified in CVE reports.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the vulnerabilities were addressed through dependency updates in the admin-tools image.",
      "related": [],
      "keyQuote": "There are some CVEs found from the latest Temporal image: temporalio/admin-tools:1.27.2",
      "number": 7602,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:55.609Z"
    },
    {
      "summary": "Multiple CVEs identified in Temporal server v1.27.2 including high-severity vulnerabilities in JWT libraries, OAuth2 dependencies, and other Go packages. Requires dependency updates to address security risks.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "dependencies",
        "server-release",
        "docker-image",
        "jwt-handling"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "CVE",
        "dependency-management",
        "container-image",
        "compliance"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal server v1.27.2 are exposed to known security vulnerabilities that could be exploited, requiring urgent dependency updates to secure their deployments.",
      "rootCause": "Outdated versions of transitive dependencies including golang-jwt/jwt, golang.org/x/net, golang.org/x/oauth2, and go-jose libraries in the server release.",
      "proposedFix": "Upgrade affected dependencies: golang-jwt/jwt to v4.5.2+, golang.org/x/net/http/httpproxy to v0.36.0+, golang.org/x/oauth2/jws to v0.27.0+, github.com/go-jose/go-jose/v4 to v4.0.5+, and update go-jose from v2 to v4.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed after CVE fixes were addressed in subsequent releases by upgrading vulnerable dependencies.",
      "related": [],
      "keyQuote": "Is an upgrade planned for temporal to address these CVEs? Mend in particular is showing go-jose - which is on v2, should be on v4, as v2 is no longer supported.",
      "number": 7601,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:56.060Z"
    },
    {
      "summary": "Starting a workflow with TERMINATE_EXISTING policy fails with a duplicate key constraint error on the current_executions table, suggesting a DB inconsistency where a previous workflow wasn't properly cleaned up. The issue occurs intermittently under heavy load and can only be resolved by manually deleting the conflicting row.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [
        "start_workflow_execution"
      ],
      "components": [
        "history-shard",
        "current-executions-table",
        "transaction-rollback"
      ],
      "concepts": [
        "duplicate-key-error",
        "database-inconsistency",
        "conflict-policy",
        "transaction-rollback",
        "data-cleanup"
      ],
      "severity": "high",
      "userImpact": "Workflows fail to start despite correct conflict policies, requiring manual database intervention to recover.",
      "rootCause": "Likely a failed rollback in a previous start_workflow request that left an orphaned record in current_executions table, or an issue in the retention timer logic that fails to properly delete the record.",
      "proposedFix": null,
      "workaround": "Manually delete the conflicting row from the current_executions table using SQL DELETE statement.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A previous start workflow request failed and also failed to rollback the inserted current record.",
      "number": 7600,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:42.748Z"
    },
    {
      "summary": "Importing uvicorn in a Temporal Python SDK project causes a RestrictedWorkflowAccessError because uvicorn's initialization calls gettext.gettext.__call__, which is restricted in the workflow sandbox.",
      "category": "bug",
      "subcategory": "workflow-sandbox",
      "apis": [
        "Worker",
        "Client",
        "execute_activity",
        "start_workflow"
      ],
      "components": [
        "workflow-sandbox",
        "importer",
        "restrictions"
      ],
      "concepts": [
        "import-restriction",
        "sandbox-enforcement",
        "third-party-libraries",
        "workflow-initialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot import uvicorn in projects using Temporal Python SDK, blocking common web framework use cases.",
      "rootCause": "Workflow sandbox restricts access to gettext.gettext.__call__ during module import; uvicorn's initialization code triggers this restriction when imported at module level.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was closed and moved to sdk-python repository for proper tracking and resolution.",
      "related": [],
      "keyQuote": "Cannot access gettext.gettext.__call__ from inside a workflow. If this is code from a module not used in a workflow or known to only be used deterministically from a workflow, mark the import as pass through.",
      "number": 7598,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:43.206Z"
    },
    {
      "summary": "Nexus service APIs (DispatchByEndpoint and others) are not listed in the metadata.go file, causing custom authorizers to fail when enforcing authorization rules. This breaks workflows targeting Nexus endpoints in self-hosted clusters.",
      "category": "bug",
      "subcategory": "nexus-api-metadata",
      "apis": [
        "DispatchByEndpoint",
        "DispatchNexusTask",
        "PollNexusTaskQueue"
      ],
      "components": [
        "nexus-service",
        "api-metadata",
        "authorizer",
        "endpoint-dispatcher"
      ],
      "concepts": [
        "API discovery",
        "authorization",
        "metadata registration",
        "nexus-endpoints",
        "method-routing"
      ],
      "severity": "high",
      "userImpact": "Users with custom authorizers cannot properly enforce authorization rules for Nexus APIs, preventing workflows from starting in target namespaces.",
      "rootCause": "Nexus service APIs are missing from the metadata.go file that defines method metadata used by custom authorizers.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fix was being worked on in PR #7630 to add the missing Nexus APIs to the metadata file.",
      "related": [
        7630
      ],
      "keyQuote": "DispatchByEndpoint seems crucial as it dispatches the request to the Endpoint I believe... shouldn't they be part of the metadata file",
      "number": 7596,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:43.015Z"
    },
    {
      "summary": "Activities are not being executed in workers despite being scheduled, leading to startToClose timeouts. The issue occurs sporadically and resolves after restarting the worker.",
      "category": "bug",
      "subcategory": "activity-execution",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "task-queue"
      ],
      "concepts": [
        "activity-scheduling",
        "task-polling",
        "worker-connection",
        "timeout",
        "sporadic-failure"
      ],
      "severity": "high",
      "userImpact": "Activities scheduled in workflows fail to execute on workers, causing workflow timeouts and requiring manual worker restarts to resolve.",
      "rootCause": "Likely related to worker starting before the temporal cluster is available, causing a stale or corrupted connection state in the task queue polling mechanism.",
      "proposedFix": null,
      "workaround": "Restart the worker to recover from the issue.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer determined issue is not related to server bug and will be troubleshot in Slack thread with user.",
      "related": [],
      "keyQuote": "activity is not actually executed in the worker, which eventually leads to the startToClose timeout. When I try to restart the worker, it works fine",
      "number": 7593,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:29.534Z"
    },
    {
      "summary": "Multiple ChildWorkflowStarted events are being written for the same child workflow when a parent workflow attempts ContinueAsNew and the server fails with UnhandledCommand. This appears to be a buffering/resurrection issue in event history, potentially related to Cassandra 5 compatibility.",
      "category": "bug",
      "subcategory": "child-workflow-execution",
      "apis": [
        "StartChildWorkflow",
        "ContinueAsNew"
      ],
      "components": [
        "event-history",
        "child-workflow-executor",
        "workflow-task-processing"
      ],
      "concepts": [
        "event-duplication",
        "buffering",
        "continue-as-new",
        "unhandled-command",
        "cassandra"
      ],
      "severity": "high",
      "userImpact": "Users experience duplicate ChildWorkflowStarted events in their workflow history, causing incorrect event ordering and potential replay inconsistencies.",
      "rootCause": "Server records buffered/resurrected ChildWorkflowExecutionStarted events to event history after workflow task failure when ContinueAsNew is attempted with UnhandledCommand error, potentially related to Cassandra 5 support issues.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "server also seems to record a second ChildWorkflowExecutionStarted (seems buffered / resurrected) event to event history after workflow task failure",
      "number": 7591,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:31.280Z"
    },
    {
      "summary": "Temporal server needs to implement retry logic for transient 5xx errors from the visibility datastore, particularly during temporary database connection failures. The issue is related to database connection pool exhaustion and request timeouts similar to problems reported in TiDB.",
      "category": "feature",
      "subcategory": "persistence-connectivity",
      "apis": [],
      "components": [
        "persistence",
        "visibility-datastore",
        "database-connection-pool"
      ],
      "concepts": [
        "retry",
        "connectivity",
        "transient-errors",
        "database-connections",
        "connection-timeout",
        "resilience"
      ],
      "severity": "high",
      "userImpact": "Users experiencing temporary database connectivity issues face service failures instead of automatic recovery, requiring manual intervention or resource scaling.",
      "rootCause": "Temporal server does not retry transient 5xx errors from visibility datastore when database connections are temporarily unavailable.",
      "proposedFix": "Implement retry logic for transient 5xx errors from visibility datastore, or expose connectAttributes and connect_timeout configuration options.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        52653
      ],
      "keyQuote": "Temporal server needs to retry transient 5xx errors from visibility datastore especially when it fails to acquire database connections temporarily.",
      "number": 7577,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:29.972Z"
    },
    {
      "summary": "Multiple critical and high-severity security vulnerabilities in Temporal 1.27.2 are blocking production deployment. Issues include CVE-2024-6197, CVE-2025-30204, CVE-2024-24790, CVE-2024-45338 affecting container images (UI, server, admin-tools) with CVSS scores up to 9.8.",
      "category": "bug",
      "subcategory": "security-vulnerabilities",
      "apis": [],
      "components": [
        "container-images",
        "ui",
        "server",
        "admin-tools",
        "dependencies"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "cve",
        "cvss",
        "dependency-management",
        "container-security"
      ],
      "severity": "critical",
      "userImpact": "Production deployment is blocked due to critical security vulnerabilities in Temporal container images that require immediate patching.",
      "rootCause": "Outdated dependencies in container images including curl 8.5.0-r0, golang-jwt v3.2.2, net/netip 1.22.1, and golang.org/x/net/html v0.31.0.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "there are few security vulnarablites which are critical/severe in nature and this is stopping us from rolling temporal to prod",
      "number": 7576,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:19.260Z"
    },
    {
      "summary": "After ShutdownWorker is called, workflow tasks scheduled when no workers are available may fail with a timeout on the shutdown sticky queue instead of waiting indefinitely on the normal queue. This causes spurious WFT failures in the workflow history despite graceful worker shutdown.",
      "category": "bug",
      "subcategory": "worker-shutdown",
      "apis": [
        "ShutdownWorker"
      ],
      "components": [
        "worker",
        "sticky-queue",
        "workflow-task-scheduler"
      ],
      "concepts": [
        "graceful-shutdown",
        "task-scheduling",
        "sticky-queue",
        "timeout",
        "worker-availability",
        "queue-routing"
      ],
      "severity": "medium",
      "userImpact": "Users observe spurious workflow task failures in their history after gracefully shutting down workers, creating confusion and potentially complicating workflow debugging.",
      "rootCause": "When a workflow task is scheduled after ShutdownWorker is called and no workers are available, the task is routed to the shutdown sticky queue where it times out instead of being routed to the normal queue to wait for workers.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A WFT failure should not be observable in a workflow's history if all workers are shut down gracefully.",
      "number": 7566,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:19.300Z"
    },
    {
      "summary": "Request to add a configurable regular expression pattern to parse JWT claim permissions in the default JWT claim mapper, allowing flexible extraction of namespace and role without requiring a custom authorization plugin.",
      "category": "feature",
      "subcategory": "jwt-authorization",
      "apis": [],
      "components": [
        "jwt-claim-mapper",
        "authorization-config",
        "auth-plugin"
      ],
      "concepts": [
        "jwt-claims",
        "permissions-parsing",
        "regular-expressions",
        "authorization",
        "configuration",
        "namespaces",
        "roles"
      ],
      "severity": "medium",
      "userImpact": "Users can configure JWT permission parsing with regex patterns instead of maintaining custom authorization plugins, simplifying self-hosted Temporal deployments.",
      "rootCause": "Default JWT claim mapper only supports hardcoded namespace:role format, requiring custom plugins when issuers use different claim structures.",
      "proposedFix": "Add PermissionsPattern configuration to Authorization Config that uses regex with named groups (namespace, role) to extract permissions, with fallback to original behavior when not configured.",
      "workaround": "Maintain a custom authorization plugin, though this requires custom build, additional CI/CD steps, and Docker image management.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        7574
      ],
      "keyQuote": "Add `PermissionsPattern` into `Authorization` Config...with named groups would cover our use case and avoid overhead related to maintaining custom auth plugin",
      "number": 7560,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:18.442Z"
    },
    {
      "summary": "Feature request to provide an atomic API for resetting a workflow to a previous point and immediately applying an update request without a race condition where the workflow continues with the old state.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [
        "ResetWithUpdate"
      ],
      "components": [
        "workflow-execution",
        "reset-handler",
        "update-handler"
      ],
      "concepts": [
        "atomicity",
        "race-condition",
        "workflow-state",
        "reset",
        "update",
        "synchronization"
      ],
      "severity": "medium",
      "userImpact": "Users cannot safely reset a workflow and immediately update its state, risking data inconsistency when updates are delayed relative to reset operations.",
      "rootCause": "Reset and update operations are not atomic, allowing the workflow to execute with stale state between the reset and update application.",
      "proposedFix": "Implement a ResetWithUpdate(...) API that atomically applies both operations together.",
      "workaround": "Using workflow.Await with extra flags to control logic flow, though this adds complexity.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "There could be a race that when the code rely on the updated state after reset immediately, but the update request is not guaranteed to be applied atomically with reset",
      "number": 7551,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:07.685Z"
    },
    {
      "summary": "Activity retry intervals are not being respected consistently. Some retry attempts occur with less than 1 second between them instead of the configured 1-minute interval, causing the backoff policy to be violated.",
      "category": "bug",
      "subcategory": "activity-retry",
      "apis": [],
      "components": [
        "activity-executor",
        "retry-handler",
        "worker"
      ],
      "concepts": [
        "retry-policy",
        "backoff-interval",
        "timing-accuracy",
        "activity-execution",
        "error-handling"
      ],
      "severity": "high",
      "userImpact": "Users cannot rely on retry intervals to space out activity attempts, breaking load management and timing assumptions in their workflows.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Some attempts have less than 1 minute between them. In the logs below, attempts 6, 17, and 53 occur less than 1 second after the log for the preceding event.",
      "number": 7515,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:06.739Z"
    },
    {
      "summary": "When starting a child workflow with a WorkflowID longer than 255 characters on SQL persistence, the workflow gets stuck instead of failing with a clear error. The issue stems from SQL column size limits and a mismatch between dynamic configuration and actual database constraints.",
      "category": "bug",
      "subcategory": "child-workflow-management",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "persistence-layer",
        "workflow-execution",
        "child-workflow-manager",
        "current-executions"
      ],
      "concepts": [
        "workflow-id-validation",
        "persistence-constraints",
        "error-handling",
        "configuration-management",
        "sql-limits"
      ],
      "severity": "high",
      "userImpact": "Users experience stuck workflows with unclear errors when attempting to start child workflows with IDs exceeding 255 characters on SQL persistence, leading to poor debugging experience.",
      "rootCause": "The maxIDLength is a dynamic config that doesn't enforce SQL column size limits (255 chars for character varying columns), causing insert failures into current_executions table when IDs exceed the database constraint.",
      "proposedFix": "Either hardcode maxIDLength based on the persistence implementation used, or enforce a max limit based on the persistence layer to prevent exceeding database constraints.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If a workflow try to start a child with workflow ID longer than 255, the workflow would see a child initiated, and then it would stuck.",
      "number": 7506,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:19:07.145Z"
    },
    {
      "summary": "User asks whether Temporal server supports database sharding to handle 20 million workflow executions per day across a distributed database setup. Temporal maintainer clarifies that multiple database support is not currently available.",
      "category": "question",
      "subcategory": "deployment-scaling",
      "apis": [],
      "components": [
        "server",
        "database",
        "persistence"
      ],
      "concepts": [
        "sharding",
        "scaling",
        "high-throughput",
        "database-distribution",
        "multi-tenancy"
      ],
      "severity": "medium",
      "userImpact": "Users unable to deploy Temporal at scale across sharded databases for high-volume workflow execution scenarios.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Direct users to community forum or Slack for deployment recommendations and workarounds.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal server doesn't support multiple databases at this time.",
      "number": 7503,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:52.756Z"
    },
    {
      "summary": "User's self-hosted Temporal server periodically makes outbound connections to Amazon IP addresses on port 443 and wants to know the cause and how to disable it. The feature turns out to be a version check mechanism that reports minimal server info to check for new versions.",
      "category": "question",
      "subcategory": "server-configuration",
      "apis": [],
      "components": [
        "frontend",
        "version-checker",
        "server"
      ],
      "concepts": [
        "outbound-connections",
        "version-check",
        "server-info",
        "aws-s3",
        "configuration",
        "security"
      ],
      "severity": "low",
      "userImpact": "Users running self-hosted Temporal may be concerned about unexpected outbound connections to external services and need to know how to disable them.",
      "rootCause": "The Temporal server includes a feature that periodically checks for new server versions available and reports minimal server info, implemented in the frontend version_checker component.",
      "proposedFix": "Disable the version check feature by setting `frontend.enableServerVersionCheck` to false or setting the `TEMPORAL_VERSION_CHECK_DISABLED` environment variable to true or 1.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was answered by providing the explanation of the version check feature and instructions to disable it via configuration or environment variable.",
      "related": [],
      "keyQuote": "It's a feature for checking if any new server version is available (UI will display new version info if there's one) and reporting minimal server info.",
      "number": 7492,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:55.375Z"
    },
    {
      "summary": "The gRPC Status response for query errors contains malformed details field that doesn't follow Google's AIP-193 guidance for status details encoding. The binary-packed failure response isn't properly structured as an ErrorInfo followed by a QueryFailedFailure Any, making it difficult for non-Go SDKs to parse.",
      "category": "bug",
      "subcategory": "query-error-handling",
      "apis": [],
      "components": [
        "query-service",
        "error-details",
        "grpc-status"
      ],
      "concepts": [
        "grpc-status-details",
        "protobuf-encoding",
        "error-response",
        "api-compatibility",
        "cross-sdk-support"
      ],
      "severity": "medium",
      "userImpact": "Non-Go SDKs cannot properly parse query error details from gRPC Status responses, limiting debugging and error handling capabilities.",
      "rootCause": "The Status details field is directly serializing QueryFailedFailure instead of following AIP-193 guidance with ErrorInfo followed by Any-wrapped QueryFailedFailure.",
      "proposedFix": "Restructure the encoded failure in Status details to adhere to Google AIP-193 guidance, wrapping ErrorInfo and QueryFailedFailure in the proper Any format.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        867
      ],
      "keyQuote": "it's just directly serializing a QueryFailedFailure somehow, which maybe is OK for api-go users, but isn't for anyone else.",
      "number": 7487,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:54.546Z"
    },
    {
      "summary": "The admin-tools Docker image (temporalio/admin-tools:1.27.1) contains multiple CVEs including high-severity vulnerabilities in golang.org/x/net/html and golang.org/x/net/http/httpproxy, along with medium and low-severity issues in dependencies like MariaDB, PostgreSQL, and expat.",
      "category": "bug",
      "subcategory": "dependency-vulnerabilities",
      "apis": [],
      "components": [
        "docker-image",
        "admin-tools",
        "dependency-management"
      ],
      "concepts": [
        "security-vulnerability",
        "CVE",
        "dependency-patching",
        "container-image",
        "version-upgrade"
      ],
      "severity": "high",
      "userImpact": "Users running the vulnerable admin-tools container image are exposed to security exploits that could lead to denial of service attacks and other security compromises.",
      "rootCause": "Outdated dependencies in the admin-tools Docker image, including golang.org/x/net/html (v0.31.0), golang.org/x/net/http/httpproxy (v0.35.0), and database packages.",
      "proposedFix": "Update vulnerable dependencies to fixed versions: golang.org/x/net/html to 0.33.0, golang.org/x/net/http/httpproxy to 0.36.0, MariaDB to 11.4.5-r0, PostgreSQL to 17.4-r0, and expat to 2.7.0-r0.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "There are several CVEs found from the latest Temporal image: temporalio/admin-tools:1.27.1-tctl-1.18.2-cli-1.3.0",
      "number": 7481,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:43.128Z"
    },
    {
      "summary": "Multiple security vulnerabilities (CVEs) detected in Temporal server v1.27.1 release, including high-severity issues in golang.org/x/net and golang.org/x/oauth2 dependencies. Requires dependency updates to address exposed security risks.",
      "category": "bug",
      "subcategory": "security-vulnerabilities",
      "apis": [],
      "components": [
        "server",
        "dependencies",
        "container-image"
      ],
      "concepts": [
        "security",
        "CVE",
        "vulnerability",
        "dependency-management",
        "docker-image",
        "patching"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal server v1.27.1 are exposed to known security vulnerabilities that could be exploited by attackers.",
      "rootCause": "Outdated dependencies in the server release, including golang.org/x/net/http/httpproxy (v0.34.0), golang.org/x/oauth2/jws (v0.25.0), and other packages with known CVEs.",
      "proposedFix": "Update vulnerable dependencies: golang.org/x/net to v0.36.0, golang.org/x/oauth2 to v0.27.0, go-jose/v4 to v4.0.5, and other affected packages to their fixed versions.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "There are some CVEs found from the latest Temporal image: temporalio/server:1.27.1",
      "number": 7480,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:43.673Z"
    },
    {
      "summary": "The SQL schema update tool fails on Windows due to filepath.Join() producing backslash-separated paths that are incompatible with fs.ReadFile(). The issue was resolved by using path.Join() instead, which uses forward slashes consistently across platforms.",
      "category": "bug",
      "subcategory": "sql-schema-tool",
      "apis": [],
      "components": [
        "sql-tool",
        "schema-handler",
        "manifest-reader"
      ],
      "concepts": [
        "cross-platform-compatibility",
        "file-paths",
        "windows-compatibility",
        "schema-versioning",
        "sql-scripts"
      ],
      "severity": "high",
      "userImpact": "Windows users cannot use the update-schema command, blocking schema migrations on Windows environments.",
      "rootCause": "filepath.Join() returns backslash-separated paths on Windows (e.g., v1.0\\manifest.json) which fs.ReadFile() cannot process; the fs package expects forward slashes regardless of OS.",
      "proposedFix": "Replace filepath.Join() with path.Join() in updatetask.go to ensure forward-slash paths that work with fs.ReadFile() across all platforms.",
      "workaround": "Manually edit paths or use workaround by specifying paths with forward slashes, though not practical for Windows users.",
      "resolution": "fixed",
      "resolutionDetails": "The maintainer confirmed the fix using path.Join() resolves the issue by aligning with Go's embed stdlib behavior, which does not consider OS-specific paths.",
      "related": [],
      "keyQuote": "On windows, filepath.Join() returns a path with \"\\\\\" as the separator and paths like this don't work with fs.ReadFile()",
      "number": 7454,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:43.271Z"
    },
    {
      "summary": "Temporal must migrate from AWS SDK for Go v1 (ending support July 31, 2025) to AWS SDK for Go v2. This affects S3 archiver and Elasticsearch client components.",
      "category": "feature",
      "subcategory": "aws-sdk-migration",
      "apis": [],
      "components": [
        "s3store",
        "elasticsearch-client",
        "archiver",
        "persistence"
      ],
      "concepts": [
        "dependency-upgrade",
        "migration",
        "sdk-compatibility",
        "end-of-support",
        "maintenance-mode",
        "aws-services"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal on AWS will be at risk when AWS SDK v1 reaches end-of-support, potentially causing service disruptions if not migrated beforehand.",
      "rootCause": "AWS SDK for Go v1 entered maintenance mode July 31, 2024 and will reach end-of-support July 31, 2025, requiring migration to v2.",
      "proposedFix": "Migrate to AWS SDK for Go v2 using the official migration guide; update /common/archiver/s3store and /common/persistence/visibility/store/elasticsearch/client packages.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4499,
        7930,
        7985,
        8173
      ],
      "keyQuote": "This SDK entered maintenance mode on July 31, 2024 and will reach end-of-support on July 31, 2025.",
      "number": 7421,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:30.520Z"
    },
    {
      "summary": "User reports workflow tasks not being executed by worker for several hours despite setting activity timeout to 3600 seconds. Most tasks complete normally, but some have significantly delayed execution.",
      "category": "question",
      "subcategory": "workflow-task-timeout",
      "apis": [],
      "components": [
        "worker",
        "workflow-task-processor",
        "activity-executor"
      ],
      "concepts": [
        "timeout",
        "retry",
        "polling",
        "task-queue",
        "worker-load",
        "history-events"
      ],
      "severity": "medium",
      "userImpact": "Workflow tasks experience unexplained delays of several hours, impacting application reliability and user experience.",
      "rootCause": "Workflow task failing or timing out repeatedly, with worker potentially overloaded or slow processing. Retries not recorded in history to prevent explosion of history events.",
      "proposedFix": null,
      "workaround": "Check worker logs for failures, monitor worker load to detect overload conditions.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "That typically means the workflow task is keep failing (or timing out) and retrying. We do not record all those retries as history from exploding",
      "number": 7405,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:31.556Z"
    },
    {
      "summary": "Request to integrate Needle's RAG API as a native Temporal workflow component, allowing users to create collections and perform semantic searches within workflows without leaving the Temporal environment.",
      "category": "feature",
      "subcategory": "workflow-components",
      "apis": [],
      "components": [
        "workflow",
        "activity",
        "integration"
      ],
      "concepts": [
        "RAG",
        "semantic-search",
        "collections",
        "external-integration",
        "native-component"
      ],
      "severity": "low",
      "userImpact": "Users could perform AI-powered semantic searches and work with retrieval-augmented generation directly within Temporal workflows without custom HTTP activity boilerplate.",
      "rootCause": null,
      "proposedFix": "Integrate Needle RAG API as a native Temporal workflow component with built-in collection management and semantic search capabilities.",
      "workaround": "Custom HTTP activities or third-party middleware (both with trade-offs)",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Enable users to create collections, perform semantic searches, and pass results to downstream workflow steps without leaving the Temporal environment.",
      "number": 7390,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:29.255Z"
    },
    {
      "summary": "Internal frontend HTTP port (7246) for Nexus operations is not listening despite being configured, causing connection refused errors when attempting cross-namespace Nexus communication. The service only listens on gRPC port 7236 instead of the configured HTTP port.",
      "category": "bug",
      "subcategory": "nexus-operations",
      "apis": [],
      "components": [
        "internal-frontend",
        "nexus-operations",
        "http-server"
      ],
      "concepts": [
        "nexus",
        "cross-namespace-communication",
        "port-binding",
        "configuration",
        "http-endpoint",
        "network-connectivity"
      ],
      "severity": "high",
      "userImpact": "Users cannot use Nexus endpoints for cross-namespace communication as the required HTTP port on internal frontend is not accessible.",
      "rootCause": "Internal frontend HTTP port (7246) is not being bound/listened on despite being configured, only gRPC port 7236 is active",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was acknowledged and team indicated they are investigating",
      "related": [],
      "keyQuote": "dial tcp 10.1.251.74:7246: connect: connection refused",
      "number": 7386,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:19.786Z"
    },
    {
      "summary": "JPA queries are intermittently returning null even though records exist in the database, but only when executed inside Temporal worker activities. The issue occurs with default retry options and a 10-minute startToCloseTimeout.",
      "category": "bug",
      "subcategory": "activity-database-access",
      "apis": [
        "Activity"
      ],
      "components": [
        "worker",
        "activity-executor",
        "jpa-integration"
      ],
      "concepts": [
        "database-query",
        "null-return",
        "activity-execution",
        "retry",
        "timeout",
        "race-condition"
      ],
      "severity": "high",
      "userImpact": "Activities using JPA queries fail intermittently in production, causing workflow failures and data consistency issues despite records existing in the database.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Jpa query (ex findByUserId) is returning null intermittently , even though the record exists in database, this is only happening inside the temporal workers",
      "number": 7377,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:17.825Z"
    },
    {
      "summary": "After upgrading to v1.27.0, the Temporal server encounters SQL errors when scanning NULL values from the new nullable `data_encoding` column in `current_executions` table, because the Go model expects a non-nullable string type. The issue was fixed in v1.27.1.",
      "category": "bug",
      "subcategory": "database-schema",
      "apis": [],
      "components": [
        "database",
        "schema-migration",
        "execution-persistence"
      ],
      "concepts": [
        "schema-mismatch",
        "null-handling",
        "database-compatibility",
        "type-casting",
        "migration"
      ],
      "severity": "critical",
      "userImpact": "Users upgrading to v1.27.0 experience workflow execution failures due to database scan errors when loading execution records with NULL data_encoding values.",
      "rootCause": "Schema migration in PR #7080 added a nullable data_encoding column to current_executions table, but the Go model definition uses a non-nullable string type, causing SQL scan errors when encountering NULL values. Additionally, schema definitions have inconsistencies across databases (sqlite NOT NULL vs mysql/postgresql nullable).",
      "proposedFix": "Use a nullable string type (pointer to string) in the Go model to handle NULL values from the database, and ensure consistency in schema definitions across all database backends.",
      "workaround": "Skip v1.27.0 and upgrade directly to v1.27.1 where the issue is fixed.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in v1.27.1 release by updating the model to properly handle nullable data_encoding column.",
      "related": [
        7080,
        7383
      ],
      "keyQuote": "converting NULL to string is unsupported",
      "number": 7376,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:19.127Z"
    },
    {
      "summary": "Activity failure truncation is applied uniformly regardless of the number of pending activities. The server should use a more intelligent strategy that only truncates when the aggregated failure size from all activities exceeds a larger threshold, to support both single-activity workflows and workflows with many pending activities.",
      "category": "feature",
      "subcategory": "activity-failure-handling",
      "apis": [],
      "components": [
        "mutable-state",
        "activity-executor",
        "failure-handling"
      ],
      "concepts": [
        "truncation",
        "failure-info",
        "pending-activities",
        "mutable-state-size",
        "workflow-termination",
        "buffered-events"
      ],
      "severity": "high",
      "userImpact": "Workflows with few activities are forced to truncate failure information with insufficient limits, while workflows with many pending activities may reach termination limits due to accumulated failure data.",
      "rootCause": "Fixed 2KB per-activity failure size limit does not account for the total number of pending activities or the actual criticality of preserving failure information.",
      "proposedFix": "Implement a total failure size limit across all pending activities instead of a per-activity limit, and consider flushing buffered events or truncating failure messages before terminating workflows when mutable state size reaches limits.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A total failure size limit across activities. When ms size reaches the limit, before directly terminating the workflow, see if we can flush buffered events or truncate activity failure message.",
      "number": 7367,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:06.580Z"
    },
    {
      "summary": "The temporal operator cluster describe command is missing failoverVersionIncrement and initialFailoverVersion fields that are available in tctl adm cl describe. Users need these fields exposed in the CLI or an alternative way to access them.",
      "category": "feature",
      "subcategory": "operator-cli",
      "apis": [
        "GetClusterInfo"
      ],
      "components": [
        "operator-service",
        "tdbg",
        "cli"
      ],
      "concepts": [
        "cluster-metadata",
        "failover-version",
        "cluster-configuration",
        "api-completeness",
        "operator-api"
      ],
      "severity": "medium",
      "userImpact": "Users cannot access failoverVersionIncrement and initialFailoverVersion fields through the modern temporal CLI, limiting their ability to inspect cluster configuration.",
      "rootCause": "The operator API GetClusterInfo method does not expose failoverVersionIncrement and initialFailoverVersion fields that are available in the older tctl implementation.",
      "proposedFix": "Add failoverVersionIncrement and initialFailoverVersion fields to the operator service API (either extend GetClusterInfo or create a new operator service API), and port tctl admin cluster describe to tdbg.",
      "workaround": "Use tctl adm cl describe to access these fields, though this is the legacy tool.",
      "resolution": "fixed",
      "resolutionDetails": "Team decided to add a corresponding command in tdbg for tctl admin cl describe as the first step to provide access to these fields.",
      "related": [],
      "keyQuote": "Synced with the team, as a first step we will add a corresponding command in tdbg for tctl admin cl describe",
      "number": 7294,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:05.326Z"
    },
    {
      "summary": "S3 archiver performs unnecessary HEAD requests before every PUT operation to check if objects exist, causing millions of 404 errors in monitoring. User requests an opt-out mechanism for single-cluster deployments where this check is unnecessary.",
      "category": "bug",
      "subcategory": "archiver-s3",
      "apis": [],
      "components": [
        "archiver",
        "s3-provider",
        "history-pod"
      ],
      "concepts": [
        "object-storage",
        "s3-optimization",
        "kubernetes-deployment",
        "monitoring-spam",
        "head-requests",
        "existence-check"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal on Kubernetes with S3 archiver experience millions of spurious 404 errors in S3 access logs, polluting monitoring and alerting systems.",
      "rootCause": "The archiver logic performs a HEAD request before every PUT request to prevent double-uploading from multiple clusters, but this is unnecessary overhead for single-cluster deployments.",
      "proposedFix": "Add configuration option to disable the existence check for single-cluster setups where duplicate uploads are not a concern.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "The HEAD request behavior is intentional design to prevent multi-cluster double-uploads. No workaround provided; users must accept this behavior.",
      "related": [],
      "keyQuote": "Unfortunately this is how the aws/gcp archiver logic works today to prevent two clusters from double uploading the same workflow",
      "number": 7273,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:18:07.259Z"
    },
    {
      "summary": "Documentation samples for mTLS and authorization configurations in samples-server are outdated and incompatible with the current version of Temporal, causing build failures when following the documented steps.",
      "category": "docs",
      "subcategory": "samples-documentation",
      "apis": [],
      "components": [
        "samples-server",
        "mTLS-configuration",
        "authorization"
      ],
      "concepts": [
        "configuration",
        "compatibility",
        "documentation-accuracy",
        "samples",
        "setup"
      ],
      "severity": "medium",
      "userImpact": "Users cannot successfully build and run the sample code referenced in official documentation, blocking their ability to learn and implement mTLS and authorization patterns.",
      "rootCause": "Sample code in samples-server repository is not compatible with the current version of Temporal, with outdated mTLS and authorization configurations.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by samples-server PR #109",
      "related": [
        100,
        109
      ],
      "keyQuote": "Temporal documentation ask us to refer to https://github.com/temporalio/samples-server/tree/main. But its mTLS and authorization configurations doesnt work.",
      "number": 7220,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:54.701Z"
    },
    {
      "summary": "Go SDK client fails with 'context deadline exceeded' error when connecting to Temporal server deployed on Kubernetes via port-forward, even though the temporal CLI works successfully on the same setup.",
      "category": "bug",
      "subcategory": "client-connection",
      "apis": [
        "client.Dial"
      ],
      "components": [
        "go-client",
        "connection",
        "kubernetes-deployment"
      ],
      "concepts": [
        "context-deadline",
        "connection-timeout",
        "grpc",
        "port-forwarding",
        "kubernetes-networking"
      ],
      "severity": "high",
      "userImpact": "Users cannot run official Go SDK samples against Kubernetes-deployed Temporal servers, blocking workflow development and testing.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Enable gRPC debugging with GRPC_GO_LOG_VERBOSITY_LEVEL=99 and GRPC_GO_LOG_SEVERITY_LEVEL=info to diagnose connection issues.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Unable to create client failed reaching server: context deadline exceeded",
      "number": 7200,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:53.021Z"
    },
    {
      "summary": "BigDecimal type causes IllegalFormatConversionException when used with System.out.printf formatting in activity code. The issue occurs in the money-transfer-project-java sample when attempting to use BigDecimal instead of integer types.",
      "category": "bug",
      "subcategory": "activity-execution",
      "apis": [],
      "components": [
        "activity-executor",
        "type-serialization"
      ],
      "concepts": [
        "type-handling",
        "serialization",
        "activity-implementation",
        "format-conversion",
        "data-types"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to use BigDecimal in activity implementations encounter runtime formatting exceptions.",
      "rootCause": "System.out.printf format specifier mismatch when BigDecimal is passed to integer format conversion (%d)",
      "proposedFix": null,
      "workaround": "Use integer types instead of BigDecimal, or handle formatting separately before printf",
      "resolution": "invalid",
      "resolutionDetails": "The failure was determined to be in the application's use of System.out.printf formatting, not an SDK issue",
      "related": [],
      "keyQuote": "Failure was in the formatting on the System.out",
      "number": 7172,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:53.770Z"
    },
    {
      "summary": "The update retry mechanism for ContinueAsNew operations currently relies on SDK client retries of ResourceExhaustedFailure, but the intention is to move retries to the server-side (gRPC interceptors/Frontend history client). Need to confirm that server-side retries are spread out in time to prevent races with CAN handling.",
      "category": "feature",
      "subcategory": "updates-continue-as-new",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "update-handler",
        "frontend-service",
        "history-client",
        "grpc-interceptor"
      ],
      "concepts": [
        "retry-mechanism",
        "timing-race-condition",
        "update-abort",
        "continue-as-new",
        "server-side-retry",
        "resource-exhaustion"
      ],
      "severity": "medium",
      "userImpact": "Updates may not be reliably retried during ContinueAsNew operations if server-side retry behavior is not properly implemented and timed.",
      "rootCause": "Update retry responsibility currently delegated to SDK client for ResourceExhaustedFailure, but original design intended server-side retry via gRPC interceptors/Frontend history client.",
      "proposedFix": "Implement server-side retry logic in gRPC interceptors and/or Frontend history client; ensure retry timing prevents races with CAN handling.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We need to make the server retry this internally (at least, that was out original intention)",
      "number": 7164,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:40.379Z"
    },
    {
      "summary": "Temporal server crashes with nil pointer dereference in history service when processing history tasks, causing workflow history/activity to be inaccessible in the UI. The crash occurs in the SQL execution store's visibility tasks getter during immediate task retrieval.",
      "category": "bug",
      "subcategory": "history-service-crash",
      "apis": [],
      "components": [
        "history-service",
        "sql-execution-store",
        "persistence-layer",
        "visibility-tasks"
      ],
      "concepts": [
        "nil-pointer-dereference",
        "segmentation-fault",
        "panic-recovery",
        "history-retrieval",
        "workflow-visibility"
      ],
      "severity": "critical",
      "userImpact": "Users cannot view workflow history or activity in the Temporal UI, completely blocking visibility into workflow execution state.",
      "rootCause": "Nil pointer dereference in sqlExecutionStore.getVisibilityTasks() at execution_tasks.go:767, occurring during history task retrieval in the immediate queue processor.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the nil pointer dereference bug was fixed in the persistence layer.",
      "related": [],
      "keyQuote": "panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x0",
      "number": 7159,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:42.723Z"
    },
    {
      "summary": "Security vulnerabilities in Temporal Server v1.26.2 detected through container image scanning, including CVEs in curl, logrus, and the server itself. Most were resolved in v1.27.1, with only 2 remaining CVEs related to a deprecated tctl tool dependency.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "server",
        "container-image",
        "dependencies",
        "tctl"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "CVE",
        "dependency-management",
        "container-security",
        "deprecation"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal Server v1.26.2 container images are exposed to multiple security vulnerabilities that can impact system stability and security posture.",
      "rootCause": "Outdated dependencies in server container image (curl 8.9.1, logrus v1.4.2) and legacy tctl tool bundled with the server.",
      "proposedFix": "Upgrade to Temporal v1.27.1 which addresses most CVEs. The remaining 2 CVEs (CVE-2024-2689, CVE-2023-3485) are in the deprecated tctl tool which should not be used.",
      "workaround": "Users should upgrade to Temporal v1.27.1 and avoid using the deprecated tctl tool, switching to the temporal CLI instead.",
      "resolution": "fixed",
      "resolutionDetails": "Most vulnerabilities resolved in v1.27.1 release. Remaining 2 CVEs deemed acceptable as they only affect deprecated tctl tool which is being phased out in favor of temporal CLI.",
      "related": [],
      "keyQuote": "Only 2 of these are still there: CVE-2024-2689 and CVE-2023-3485. But they are in server 1.18.0 which is referenced as a library to deprecated tctl tool.",
      "number": 7122,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:41.843Z"
    },
    {
      "summary": "Multiple high-severity CVEs found in the admin-tools Docker image (1.26.2-tctl-1.18.1-cli-1.2.0), including vulnerabilities in setuptools, pip, postgresql, golang.org/x/net/html, and curl. These security vulnerabilities pose significant risks to users running this image.",
      "category": "bug",
      "subcategory": "security-vulnerabilities",
      "apis": [],
      "components": [
        "docker-image",
        "admin-tools",
        "dependencies"
      ],
      "concepts": [
        "CVE",
        "vulnerability",
        "security",
        "dependency-management",
        "container-security",
        "patch-management"
      ],
      "severity": "critical",
      "userImpact": "Users running the admin-tools Docker image are exposed to multiple high-severity CVEs that could allow remote code execution and denial of service attacks.",
      "rootCause": "Outdated dependencies in the admin-tools Docker image including vulnerable versions of setuptools, pip, postgresql, golang.org/x/net/html, and curl.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Multiple CVEs are resolved in the latest version v1.27.1",
      "related": [],
      "keyQuote": "Multiple CVEs are resolved in latest version v1.27.1",
      "number": 7121,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:28.578Z"
    },
    {
      "summary": "User unable to register a second namespace with external PostgreSQL backend, receiving 'Namespace already exists' error. Issue resolved by using newer temporal CLI instead of deprecated tctl.",
      "category": "question",
      "subcategory": "namespace-management",
      "apis": [],
      "components": [
        "namespace-registry",
        "tctl",
        "cli"
      ],
      "concepts": [
        "namespace-registration",
        "database-backend",
        "cli-deprecation",
        "error-messaging"
      ],
      "severity": "low",
      "userImpact": "Users attempting to register multiple namespaces with tctl receive confusing error messages and need guidance to use the newer temporal CLI.",
      "rootCause": "User was using deprecated tctl command with incorrect syntax/behavior; newer temporal CLI handles the operation correctly.",
      "proposedFix": "Use the newer temporal CLI instead of deprecated tctl. Server could also include namespace name in error message for better debugging.",
      "workaround": "Switch from tctl to the newer temporal CLI for namespace management operations.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue was not a server bug but user error from using deprecated tctl command. Resolved by recommending migration to newer temporal CLI.",
      "related": [],
      "keyQuote": "This isn't a server bug, it's misuse of the `tctl` command (which is deprecated by the way), could you try with the newer `temporal` CLI",
      "number": 7106,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:29.972Z"
    },
    {
      "summary": "User mistakenly opened an issue in the wrong repository and requested deletion.",
      "category": "other",
      "subcategory": "administrative",
      "apis": [],
      "components": [],
      "concepts": [
        "issue-management",
        "repository-organization"
      ],
      "severity": "low",
      "userImpact": "No functional impact; this is a meta-issue about incorrect repository usage.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was opened in wrong repository by mistake; marked as invalid.",
      "related": [],
      "keyQuote": "My mistake, this is the wrong repo.",
      "number": 7087,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:28.039Z"
    },
    {
      "summary": "Temporal server uses deprecated gopkg.in/go-jose/go-jose.v2 v2.6.3 library which is vulnerable to DoS attacks. Can be replaced with the modern github.com/go-jose/go-jose module without code changes.",
      "category": "bug",
      "subcategory": "dependencies",
      "apis": [],
      "components": [
        "go-jose",
        "dependencies",
        "security"
      ],
      "concepts": [
        "vulnerability",
        "denial-of-service",
        "deprecated-dependency",
        "security",
        "library-upgrade",
        "go-modules"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to potential DoS attacks through the vulnerable go-jose dependency in the server.",
      "rootCause": "Temporal server continues to use deprecated go-jose.v2 instead of the modern go-jose module",
      "proposedFix": "Replace gopkg.in/go-jose/go-jose.v2 with github.com/go-jose/go-jose",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed in PR #7404",
      "related": [
        7404
      ],
      "keyQuote": "The go-jose/go-jose module is vulnerable to Denial of Service (DoS) attacks",
      "number": 7086,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:17.370Z"
    },
    {
      "summary": "The getConfigFiles function in the config loader doesn't handle permission errors properly, silently skipping files that can't be read instead of notifying users about the issue.",
      "category": "bug",
      "subcategory": "config-loading",
      "apis": [],
      "components": [
        "config-loader",
        "file-reader",
        "error-handling"
      ],
      "concepts": [
        "permission-errors",
        "error-handling",
        "file-access",
        "logging",
        "configuration",
        "silent-failures"
      ],
      "severity": "medium",
      "userImpact": "Users may not be aware when configuration files cannot be read due to permission issues, potentially leading to misconfiguration or unexpected behavior.",
      "rootCause": "The getConfigFiles function silently skips files with permission errors without logging or reporting the issue to the caller.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "if it can't read a file because of permission issues, it just skips it without letting us know",
      "number": 7048,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:17.938Z"
    },
    {
      "summary": "Request for a published Grafana dashboard to monitor Temporal with Prometheus or Victoria Metrics datasources. User seeks either a shareable dashboard or downloadable JSON file for import.",
      "category": "feature",
      "subcategory": "observability-monitoring",
      "apis": [],
      "components": [
        "monitoring",
        "grafana-dashboards",
        "observability"
      ],
      "concepts": [
        "visibility",
        "metrics",
        "monitoring",
        "prometheus",
        "grafana",
        "datasources"
      ],
      "severity": "medium",
      "userImpact": "Users lack built-in visibility tools to monitor Temporal deployments, requiring them to build custom monitoring solutions.",
      "rootCause": null,
      "proposedFix": "Create a Grafana dashboard compatible with Prometheus and Victoria Metrics datasources and publish it for community use.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Temporal provides ready-made dashboards available at https://github.com/temporalio/dashboards/",
      "related": [],
      "keyQuote": "A Grafana dashboard which builds visualisations on top of prominent datasources like Prometheus, Victoria Metrics etc.",
      "number": 7041,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:16.302Z"
    },
    {
      "summary": "Viewing and describing schedules throws 'context deadline exceeded' errors, preventing access to schedule details and execution. The issue appears to impact existing schedules selectively while newly created schedules remain accessible.",
      "category": "bug",
      "subcategory": "schedule-management",
      "apis": [
        "DescribeSchedule"
      ],
      "components": [
        "schedule-service",
        "frontend",
        "gRPC"
      ],
      "concepts": [
        "context-deadline",
        "performance-degradation",
        "database-query",
        "schedule-state"
      ],
      "severity": "high",
      "userImpact": "Users cannot view, pause, unpause, or trigger existing schedules, and affected schedules stop executing entirely, blocking critical workflow scheduling functionality.",
      "rootCause": "Unclear from issue description, but comments suggest potential issues with worker configuration or underlying workflow state that causes slow gRPC calls and context deadline exceeded errors",
      "proposedFix": null,
      "workaround": "Disabling worker via 'worker.enabled: false' in configuration resolves the issue for some cases; resetting the underlying workflow has helped in other cases",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Same here, can list all schedules, but cannot open/pause/unpause/trigger ~0.5% of all schedules in the namespace... Receiving `context deadline exceeded` from UI/CLI, and `slow gRPC call` from `frontend` deployment.",
      "number": 7037,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:06.878Z"
    },
    {
      "summary": "Feature request to support Markdown rendering in the Result panel so users can see workflow execution reports at a glance instead of raw text output.",
      "category": "feature",
      "subcategory": "ui-result-display",
      "apis": [],
      "components": [
        "result-panel",
        "ui",
        "output-formatting"
      ],
      "concepts": [
        "markdown",
        "rendering",
        "user-interface",
        "result-display",
        "workflow-reporting"
      ],
      "severity": "low",
      "userImpact": "Users would be able to view workflow results in a more readable, formatted way without manual conversion to Markdown.",
      "rootCause": null,
      "proposedFix": "Implement Markdown rendering support in the Result panel UI component.",
      "workaround": "Manually copy raw text output and paste into a Markdown converter.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would be really good if you could output results as markdown. It will mean that at a glance you could see a report on what went on in the workflow.",
      "number": 7011,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:06.916Z"
    },
    {
      "summary": "Temporal Server logging extreme amounts of \"context deadline exceeded\" errors during VisibilityDeleteExecution tasks, causing visibility index records to accumulate beyond retention period and degrade database performance.",
      "category": "bug",
      "subcategory": "visibility-management",
      "apis": [],
      "components": [
        "visibility-manager",
        "visibility-queue-processor",
        "mysql-persistence",
        "task-scheduler"
      ],
      "concepts": [
        "context-deadline",
        "visibility-cleanup",
        "retention-policy",
        "database-performance",
        "error-logging",
        "task-execution"
      ],
      "severity": "high",
      "userImpact": "Users experience degraded database performance and accumulation of stale workflow records in visibility tables despite retention policies being configured.",
      "rootCause": "Context deadline exceeded errors in DeleteWorkflowExecution visibility operations, potentially triggered by database connectivity issues following MySQL password changes and service restarts.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "we can see past events listed in temporal-ui, way after retention period... logs flooding since then",
      "number": 6995,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:17:05.312Z"
    },
    {
      "summary": "Multiple high and medium severity CVEs found in the temporalio/admin-tools Docker image version 1.25.2-tctl-1.18.1-cli-1.1.2, including vulnerabilities in setuptools, pip, PostgreSQL, curl, and Temporal Server itself.",
      "category": "bug",
      "subcategory": "security-vulnerabilities",
      "apis": [],
      "components": [
        "admin-tools",
        "docker-image",
        "dependencies",
        "setuptools",
        "pip",
        "postgresql",
        "curl"
      ],
      "concepts": [
        "security",
        "cve",
        "vulnerability",
        "dependency-management",
        "container-image",
        "patch-management",
        "supply-chain-security"
      ],
      "severity": "high",
      "userImpact": "Users deploying the admin-tools Docker image are exposed to multiple known security vulnerabilities that could allow remote code execution, denial of service, and other attacks.",
      "rootCause": "Outdated and unpatched dependencies bundled in the Docker image, including setuptools 65.5.0 (CVE-2024-6345), pip 24.0 (PRISMA-2022-0168), PostgreSQL 16.3 (CVE-2024-7348), curl 8.9.1 (CVE-2024-9681), and Temporal Server v1.18.1 (CVE-2024-2689, CVE-2023-3485).",
      "proposedFix": "Update all dependencies to patched versions: setuptools 70.0.0, PostgreSQL 16.4-r0, curl 8.11.0-r0, and upgrade Temporal Server to versions 1.20.5, 1.21.6, 1.22.7 or later.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "There are a lot of CVEs found from the latest Temporal image: temporalio/admin-tools:1.25.2-tctl-1.18.1-cli-1.1.2",
      "number": 6977,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:55.247Z"
    },
    {
      "summary": "Multiple security vulnerabilities (CVEs) have been discovered in the Temporal Server 1.25.2.0 Docker image, including high-severity issues in curl and medium-severity vulnerabilities in logrus, zlib, and busybox that could lead to denial of service, memory corruption, and buffer overflow attacks.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "docker-image",
        "dependencies",
        "curl",
        "logrus",
        "zlib",
        "busybox"
      ],
      "concepts": [
        "vulnerability",
        "cve",
        "docker",
        "dependency-management",
        "security-scanning",
        "denial-of-service",
        "memory-corruption"
      ],
      "severity": "critical",
      "userImpact": "Users running Temporal Server 1.25.2.0 are exposed to multiple known security vulnerabilities that could be exploited for attacks.",
      "rootCause": "Outdated or vulnerable versions of system and Go dependencies packaged in the Docker image (curl 8.5.0, logrus v1.4.2, zlib 1.3.1, busybox 1.36.1).",
      "proposedFix": null,
      "workaround": null,
      "related": [],
      "keyQuote": "There are a lot of CVEs found from the latest Temporal image: temporalio/server:1.25.2.0",
      "resolution": null,
      "number": 6976,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:52.042Z"
    },
    {
      "summary": "User reported that workflow.continue_as_new() was not creating a new workflow instance in Python SDK 1.8.0. After investigation, the issue was traced to incorrect serialization/deserialization of Pydantic and Modelina classes in custom payload converters, not a bug in continue_as_new itself.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [
        "continue_as_new"
      ],
      "components": [
        "workflow-execution",
        "payload-converter",
        "serialization"
      ],
      "concepts": [
        "continue_as_new",
        "workflow-lifecycle",
        "serialization",
        "deserialization",
        "payload-conversion",
        "pydantic"
      ],
      "severity": "medium",
      "userImpact": "Users experienced workflow.continue_as_new failing silently without creating new instances when using custom payload converters with Pydantic/Modelina classes.",
      "rootCause": "Custom payload converter (PydanticJSONPayloadConverter) was incorrectly handling complex types like collections.deque[MachineStatusUpdateEvent], causing silent serialization failures that prevented proper workflow continuation.",
      "proposedFix": "Use only Pydantic classes or single instances of Modelina classes in workflow parameters, avoiding complex collection types with custom objects.",
      "workaround": "Refactor interfaces to only use Pydantic classes or single instances of Modelina classes instead of collections of custom types.",
      "resolution": "invalid",
      "resolutionDetails": "Issue was not a bug in continue_as_new but rather a user code issue with custom payload converter implementation. User resolved by changing their data model interfaces.",
      "related": [],
      "keyQuote": "This is not a real bug. What was happening was that the PydanticJSONPayloadConverter was handling the events argument... Once I changed my interfaces to only use pydantic classes or single instances of Modelina classes the continue_as_new worked.",
      "number": 6969,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:54.984Z"
    },
    {
      "summary": "Add existing run ID field to StartChildWorkflowExecutionFailedEventAttributes when child workflow startup fails due to already-existing workflow. This provides the same information already available in WorkflowExecutionAlreadyStartedFailure for clients.",
      "category": "feature",
      "subcategory": "child-workflows",
      "apis": [
        "StartChildWorkflowExecutionFailedEventAttributes",
        "WorkflowExecutionAlreadyStartedFailure"
      ],
      "components": [
        "event-attributes",
        "child-workflow-executor",
        "failure-handling"
      ],
      "concepts": [
        "workflow-execution",
        "already-exists",
        "run-id",
        "child-workflow",
        "error-information"
      ],
      "severity": "medium",
      "userImpact": "Users will have access to the already-existing run ID when child workflow startup fails, improving error handling and debugging capabilities.",
      "rootCause": null,
      "proposedFix": "Add existing_run_id field to StartChildWorkflowExecutionFailedEventAttributes to contain the run ID of the already-existing workflow.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Need the already-exists run ID on StartChildWorkflowExecutionFailedEventAttributes when the child start fails because it already exists.",
      "number": 6961,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:39.363Z"
    },
    {
      "summary": "Child workflows terminated during reset operations incorrectly report to their parent using IsTerminatedByResetter check instead of verifying if the current run is the base run. This causes incorrect parent-child workflow behavior during reset scenarios.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "transfer_queue_active_task_executor",
        "workflow_execution_info",
        "replication"
      ],
      "concepts": [
        "reset",
        "child-workflow",
        "parent-workflow",
        "termination",
        "base-run",
        "state-based-replication"
      ],
      "severity": "high",
      "userImpact": "Child workflows may incorrectly report completion to parent workflows during reset operations, leading to incorrect workflow execution behavior in multi-level workflow hierarchies.",
      "rootCause": "The replyToParentWorkflow logic checks IsTerminatedByResetter instead of checking if the current run is the base run of a reset operation using the ResetRunId field.",
      "proposedFix": "Change replication logic to either: (1) Generate SyncWorkflowState task instead of historyReplicationTask when workflow closes during reset, or (2) Add new fields to Terminated event to indicate if workflow is base run. Current limitation: ResetRunID field not replicated in event-based replication until state-based replication goes live.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If current run is base run, then we should not report to parent, as we will have a new run for the child. If current run is not base run, then we should report to parent.",
      "number": 6954,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:40.863Z"
    },
    {
      "summary": "Activity retry expiration time is incorrectly calculated based on the original workflow's activityScheduleEvent time rather than being recalculated from the reset time, causing the activity scheduled time to become later than the retry expiration time.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-resetter",
        "activity-retry",
        "history-service"
      ],
      "concepts": [
        "activity-retry",
        "expiration-time",
        "workflow-reset",
        "scheduled-time",
        "retry-logic"
      ],
      "severity": "high",
      "userImpact": "Resetting workflows with retrying activities causes retry logic to fail because the expiration deadline passes before the activity can be rescheduled.",
      "rootCause": "Activity retry expiration time is calculated using the base workflow's activityScheduleEvent time, then the activity scheduled time is overwritten to the reset time, inverting the temporal relationship.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Activity retry expiration time is based calculated using the base workflow's activityScheduleEvent time. Then activity scheduled time is overwritten to the reset time, causing activity scheduled time to be later than retry expiration time.",
      "number": 6952,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:39.812Z"
    },
    {
      "summary": "User wants to configure Temporal to read PostgreSQL password from an environment variable instead of storing it plaintext in the YAML configuration file for security reasons.",
      "category": "question",
      "subcategory": "configuration",
      "apis": [],
      "components": [
        "persistence",
        "postgresql",
        "configuration"
      ],
      "concepts": [
        "security",
        "environment-variables",
        "password-management",
        "configuration",
        "credentials"
      ],
      "severity": "medium",
      "userImpact": "Users must store sensitive database credentials in plaintext in configuration files, creating a security risk.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I really don't like storing the database password in the configuration YAML file. How can I configure Temporal, when running it via a systemd unit, so that it retrieves the PostgreSQL password from an environment variable?",
      "number": 6946,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:26.859Z"
    },
    {
      "summary": "User requests support for custom search attributes of type Duration to filter workflows based on duration thresholds, similar to the built-in ExecutionDuration filter. Currently only Int and Datetime types are supported, limiting users' ability to apply semantic duration-based filters.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "search-attributes",
        "filtering",
        "persistence"
      ],
      "concepts": [
        "duration",
        "filtering",
        "custom-search-attributes",
        "workflow-execution",
        "time-based-queries"
      ],
      "severity": "medium",
      "userImpact": "Users cannot create duration-based custom search attributes, forcing them to use Int types with unclear semantics (milliseconds) and limiting filtering capabilities.",
      "rootCause": null,
      "proposedFix": "Add Duration as a supported type for custom search attributes, allowing users to filter using duration strings like '1m', '1h', or '1d'.",
      "workaround": "Store duration as Int type in milliseconds, though this creates confusion about the unit and type semantics.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add support for creating custom search attributes of type Duration that allow users to filter using duration strings like 1m, 1h or 1d etc.",
      "number": 6924,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:27.133Z"
    },
    {
      "summary": "When schedule activity task commands fail validation (e.g., payload size limits), error logs lack contextual information about the offending activity. Users need activity type and command details included in error messages to simplify debugging.",
      "category": "other",
      "subcategory": "error-logging",
      "apis": [],
      "components": [
        "workflow-task-handler",
        "schedule-activity-command",
        "validation"
      ],
      "concepts": [
        "error-reporting",
        "debugging",
        "command-validation",
        "payload-limits",
        "error-context",
        "logging"
      ],
      "severity": "medium",
      "userImpact": "When activity command validation fails, debugging is harder due to insufficient error context about which activity caused the failure.",
      "rootCause": null,
      "proposedFix": "Include command info (e.g., activity type) in error messages when schedule activity task command validation fails",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "here err does not include any info about the offending activity, would be great to have this info if possible as would make debugging easier",
      "number": 6896,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:28.747Z"
    },
    {
      "summary": "Request to add MaximumUpdatesPerExecution protection to limit the number of updates accepted in a workflow execution, preventing history bloat and ensuring continueAsNew can complete successfully.",
      "category": "feature",
      "subcategory": "workflow-updates",
      "apis": [],
      "components": [
        "workflow-execution",
        "history",
        "dynamic-config"
      ],
      "concepts": [
        "updates",
        "protection-limits",
        "history-management",
        "continueAsNew",
        "workflow-task-completion",
        "rate-limiting"
      ],
      "severity": "medium",
      "userImpact": "Without this protection, workflows receiving constant update requests can have their history blow up, preventing continueAsNew operations from completing successfully.",
      "rootCause": "Updates on a single workflow execution lack admission control, allowing unbounded accumulation of update events in history and blocking workflow task completion.",
      "proposedFix": "Add MaximumUpdatesPerExecution configuration similar to the existing MaximumSignalsPerExecution pattern, counting both new and admitted updates.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented following the suggested solution pattern similar to MaximumSignalsPerExecution, addressing the history bloat concern.",
      "related": [],
      "keyQuote": "the history could be blown up due to constant incoming requests and no chance for continueAsNew to complete",
      "number": 6872,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:16.586Z"
    },
    {
      "summary": "Query failures currently only support a single error string, preventing users from accessing stack traces, error details, and encryption capabilities available in other parts of the platform. The request is to support full `temporal.api.failure.v1.Failure` structures in query results, enabling proper error encoding and detailed failure information.",
      "category": "feature",
      "subcategory": "query-failures",
      "apis": [
        "WorkflowQueryResult",
        "QueryFailedFailure"
      ],
      "components": [
        "query-handler",
        "failure-encoding",
        "error-details"
      ],
      "concepts": [
        "error-handling",
        "structured-failures",
        "stack-traces",
        "error-encoding",
        "encryption",
        "payload"
      ],
      "severity": "medium",
      "userImpact": "Users cannot access detailed error information, stack traces, or encrypt query failures like they can for other platform operations.",
      "rootCause": "Query result API design only supports a single string error message instead of the full Failure structure used elsewhere",
      "proposedFix": "Add `temporal.api.failure.v1.Failure failure` field to `temporal.api.query.v1.WorkflowQueryResult` and `temporal.api.errordetails.v1.QueryFailedFailure`",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        6947,
        503
      ],
      "keyQuote": "Support a full `temporal.api.failure.v1.Failure` on query result (can be mutually exclusive with the string today)",
      "number": 6845,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:15.065Z"
    },
    {
      "summary": "Application crashes after upgrading from version 1.23.1 to 1.24.1, with database errors indicating attempts to access a visibility schema column (temporalnamespacedivision) that doesn't exist in the user's single-schema setup.",
      "category": "bug",
      "subcategory": "database-schema",
      "apis": [
        "CountWorkflowExecutions"
      ],
      "components": [
        "frontend-service",
        "database-persistence",
        "postgresql-schema",
        "visibility-schema"
      ],
      "concepts": [
        "schema-migration",
        "upgrade-compatibility",
        "database-configuration",
        "visibility-schema",
        "column-mapping",
        "backwards-compatibility"
      ],
      "severity": "high",
      "userImpact": "Users upgrading to 1.24.1 experience application crashes when using a single database schema without separate visibility schema.",
      "rootCause": "Query logic in 1.24.1 appears to reference visibility schema columns that don't exist in single-schema configurations, despite successful PostgreSQL schema upgrade to v1.12.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Our setup is we are using single db schema (core) and not using visibility schema but there are few tables from visibility schema in our core db.",
      "number": 6844,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:16.102Z"
    },
    {
      "summary": "Long poll connections are timing out at regular 60s intervals when the matching service restarts, causing request rate oscillations. Adding small jitter to the long poll timeout would help smooth out request distribution.",
      "category": "feature",
      "subcategory": "long-polling",
      "apis": [],
      "components": [
        "matching-service",
        "long-poll",
        "connection-pool",
        "retry-logic"
      ],
      "concepts": [
        "timeout",
        "jitter",
        "polling",
        "connection-restart",
        "request-rate",
        "oscillation",
        "resilience"
      ],
      "severity": "medium",
      "userImpact": "Users experience periodic spikes in long poll request rates every 60 seconds after matching service restarts, potentially impacting SDK performance and server load.",
      "rootCause": "When matching service restarts, all long poll connections are killed and SDKs retry simultaneously, causing requests to timeout at the same interval (60s) if no tasks arrive, creating synchronized oscillating request patterns.",
      "proposedFix": "Add small jitter to the long poll empty response timeout to desynchronize request timing across multiple SDK instances.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add some short jitter to the long poll timeout to make the requests smooth out faster.",
      "number": 6843,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:02.806Z"
    },
    {
      "summary": "Request to support start delay in continue-as-new options to allow workflows with loop-with-sleep patterns to be implemented as short-lived workflows instead of long-lived ones, avoiding the need to maintain long-term history compatibility.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "workflow-engine",
        "continue-as-new",
        "workflow-history"
      ],
      "concepts": [
        "start-delay",
        "continue-as-new",
        "workflow-sleep",
        "history-compatibility",
        "long-lived-workflows",
        "worker-versioning"
      ],
      "severity": "medium",
      "userImpact": "Developers implementing loop-with-sleep workflows could use short-lived workflows instead of long-lived ones, reducing history compatibility maintenance burden and improving worker versioning capabilities.",
      "rootCause": null,
      "proposedFix": "Add start delay parameter to continue-as-new options to schedule workflow restart after a specified delay.",
      "workaround": "Either sleep within the workflow (requiring long-term history compatibility) or start a new workflow within an activity (losing parent-child relationship and history).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        515
      ],
      "keyQuote": "adding a start delay to continue-as-new options would allow them to be represented as a series of short-lived workflows",
      "number": 6834,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:01.571Z"
    },
    {
      "summary": "Request to support environment variable substitution in the clusterMetadata section of Temporal configuration templates, enabling dynamic multi-cluster setup and feature configuration without manual file edits.",
      "category": "feature",
      "subcategory": "configuration",
      "apis": [],
      "components": [
        "config-templates",
        "cluster-metadata",
        "configuration-loader"
      ],
      "concepts": [
        "environment-variables",
        "dynamic-configuration",
        "template-substitution",
        "multi-cluster",
        "feature-flags",
        "global-namespace"
      ],
      "severity": "medium",
      "userImpact": "Users cannot dynamically configure multi-cluster setups or toggle features like enableGlobalNamespace without manually modifying configuration files.",
      "rootCause": null,
      "proposedFix": "Implement environment variable substitution syntax (e.g., {{ env \"VARIABLE_NAME\" | default \"value\" }}) in the clusterMetadata configuration template section.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Support for environment variables within the clusterMetadata section. Ability to set dynamic cluster configurations through environment variables.",
      "number": 6827,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:16:01.518Z"
    },
    {
      "summary": "Schedule workflow execution cycles are randomly skipped. User set a 30-second catchup window but experiences periodic missed runs. After investigation, the root cause was identified as cluster overload causing execution delays exceeding the catchup window.",
      "category": "question",
      "subcategory": "schedule-catchup",
      "apis": [
        "ScheduleClient",
        "Schedule",
        "ScheduleActionStartWorkflow",
        "ScheduleSpec",
        "SchedulePolicy",
        "ScheduleState"
      ],
      "components": [
        "scheduler",
        "schedule-worker",
        "catchup-window",
        "cluster-performance"
      ],
      "concepts": [
        "schedule-execution",
        "catchup-window",
        "cluster-overload",
        "workflow-delay",
        "performance-tuning",
        "jitter"
      ],
      "severity": "medium",
      "userImpact": "Scheduled workflows randomly fail to execute on time when cluster is under heavy load and execution delay exceeds the catchup window.",
      "rootCause": "Cluster performance issues causing scheduled workflow execution to delay beyond the configured 30-second catchup window, triggering the scheduler to skip execution.",
      "proposedFix": null,
      "workaround": "Increase catchup window duration (set to default 1 year) to tolerate longer delays, or use setCatchupWindow with longer duration like setJitter(Duration.ofSeconds(30)). Tune internal worker performance using dynamic config setting `worker.perNamespaceWorkerOptions`.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue is due to user's cluster configuration and performance limitations, not a bug in Temporal. User was advised to use support forums and scale their cluster or tune worker settings.",
      "related": [],
      "keyQuote": "setCatchupWindow(Duration.ofSeconds(30)) means \"don't start a workflow if it would be more than 30 seconds late\".",
      "number": 6820,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:50.915Z"
    },
    {
      "summary": "The Temporal server's gRPC MaxRecvMsgSize is set to the default 4MB, while the SDK client sets MaxSendMsgSize to 128MB, creating an inconsistency that causes RESOURCE_EXHAUSTED errors when clients send larger messages.",
      "category": "bug",
      "subcategory": "grpc-configuration",
      "apis": [],
      "components": [
        "grpc-server",
        "grpc-client",
        "message-size-limits"
      ],
      "concepts": [
        "grpc",
        "message-size",
        "configuration",
        "compatibility",
        "resource-limits",
        "client-server-mismatch"
      ],
      "severity": "high",
      "userImpact": "Clients encounter RESOURCE_EXHAUSTED errors in production when sending messages larger than 4MB despite configuring 128MB limits on the client side.",
      "rootCause": "Server's gRPC MaxRecvMsgSize uses default 4MB value while client SDK sets MaxSendMsgSize to 128MB, creating a mismatch in message size handling.",
      "proposedFix": "Update server gRPC configuration to use 128MB for MaxRecvMsgSize to match the client SDK's MaxSendMsgSize setting.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "this caused an critical error \"io.grpc.StatusRuntimeException: RESOURCE_EXHAUSTED: grpc: received message larger than max (5214441 vs. 4194304)\" in our prod envs",
      "number": 6819,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:47.298Z"
    },
    {
      "summary": "WorkflowTaskTimedOut error occurs when submitting 1,000 activities in a single workflow due to a hard-coded 4 MB gRPC response size limit for RespondWorkflowTaskCompleted requests. The issue manifests as a timeout even though activity timeouts are configured for longer periods.",
      "category": "bug",
      "subcategory": "activity-scheduling",
      "apis": [
        "ExecuteActivity",
        "WorkflowTaskCompleted"
      ],
      "components": [
        "workflow-task-processor",
        "gRPC-client",
        "activity-scheduler"
      ],
      "concepts": [
        "gRPC-message-size-limit",
        "activity-batching",
        "workflow-task-timeout",
        "concurrent-activities",
        "message-serialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot efficiently submit large numbers of activities (>1000) in a single workflow without encountering timeout errors that block execution.",
      "rootCause": "Hard-coded 4 MB limit for gRPC response in RespondWorkflowTaskCompleted request causes request rejection when accumulated schedule activity commands exceed this limit.",
      "proposedFix": "Feature request open for a fix: https://github.com/temporalio/features/issues/363",
      "workaround": "Add a sleep after every 100 activities scheduled to allow gRPC messages to be sent incrementally.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        363
      ],
      "keyQuote": "There's a hard coded 4 MB limit for a gRPC response. If the accumulated size of the schedule activity commands goes above that limit the RespondWorkflowTaskCompleted request will be rejected.",
      "number": 6806,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:48.509Z"
    },
    {
      "summary": "User seeks guidance on automatically scaling Temporal workers based on task queue load. They want to monitor metrics to scale workers up when workflows are queued and down when queue is empty, but lack a direct metric for pending tasks in the queue.",
      "category": "question",
      "subcategory": "worker-scaling",
      "apis": [],
      "components": [
        "worker",
        "task-queue",
        "metrics"
      ],
      "concepts": [
        "scaling",
        "auto-scaling",
        "task-queue-load",
        "workflow-scheduling",
        "latency",
        "capacity-management"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily implement automatic worker scaling based on task queue demand, making it difficult to optimize resource utilization and maintain responsive workflow processing.",
      "rootCause": null,
      "proposedFix": "Monitor schedule-to-start latency as a proxy metric for queue load, as suggested in the Temporal scaling documentation.",
      "workaround": "Use schedule-to-start latency monitoring to infer queue load and make scaling decisions based on that metric.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "what alternative methods are suggested to scale workers up and down based on task queue load?",
      "number": 6805,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:36.674Z"
    },
    {
      "summary": "Activity task queue `max_tasks_per_second` rate limiting is ignored when `request_eager_execution` is enabled. SDKs need the server to support both features working together, as eager execution is typically enabled by default.",
      "category": "feature",
      "subcategory": "activity-task-queue",
      "apis": [
        "ScheduleActivityTaskCommandAttributes"
      ],
      "components": [
        "activity-task-queue",
        "rate-limiter",
        "task-scheduling",
        "eager-execution"
      ],
      "concepts": [
        "rate-limiting",
        "task-queue-throttling",
        "eager-execution",
        "task-scheduling",
        "backpressure",
        "concurrency-control"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use rate limiting on activity task queues when eager execution is enabled, requiring SDKs to implement workarounds or disable eager execution.",
      "rootCause": "The server implementation does not apply `max_tasks_per_second` rate limiting when `request_eager_execution` is true in ScheduleActivityTaskCommandAttributes.",
      "proposedFix": "Modify the server to support both `max_tasks_per_second` and `request_eager_execution` working together simultaneously.",
      "workaround": "SDKs currently educate users on the incompatibility and provide workarounds, such as disabling eager execution when rate limiting is needed.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "SDKs have to work around this limitation and/or inform customers, especially since SDKs try to set `request_eager_execution` as true by default.",
      "number": 6800,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:36.313Z"
    },
    {
      "summary": "Add support for WorkflowIdConflictPolicy in child workflows to replace the deprecated WorkflowIdReusePolicy. This feature is needed to allow child workflows to use the new Terminate-Existing conflict resolution policy instead of the deprecated Terminate-if-Running.",
      "category": "feature",
      "subcategory": "child-workflow",
      "apis": [
        "StartChildWorkflowExecution",
        "WorkflowIdConflictPolicy"
      ],
      "components": [
        "child-workflow-executor",
        "workflow-id-conflict-resolution",
        "workflow-lifecycle"
      ],
      "concepts": [
        "workflow-id-reuse",
        "conflict-policy",
        "deprecation",
        "terminate-existing",
        "child-workflows"
      ],
      "severity": "high",
      "userImpact": "Users cannot migrate child workflows to the new WorkflowIdConflictPolicy and must continue using the deprecated WorkflowIdReusePolicy, blocking their migration path.",
      "rootCause": "WorkflowIdConflictPolicy support has been implemented for parent workflows but not yet extended to child workflow execution.",
      "proposedFix": null,
      "workaround": "Use activities with long-polling and retries to wait for external workflow completion, or use signals with local activities (both cumbersome alternatives).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        558
      ],
      "keyQuote": "To complete this effort, support for child workflows is needed.",
      "number": 6799,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:36.457Z"
    },
    {
      "summary": "CVE-2024-51744 identified in golang-jwt/jwt v4.5.0 dependency with low severity (3.1/10). Upgrade to version 4.5.1 recommended to address the vulnerability.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "dependencies",
        "jwt-library"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "dependency-management",
        "jwt",
        "token-validation"
      ],
      "severity": "low",
      "userImpact": "Users running Temporal with the vulnerable jwt library version may be exposed to a low-severity security vulnerability.",
      "rootCause": "Vulnerability in golang-jwt/jwt v4.5.0 library used as a dependency",
      "proposedFix": "Upgrade golang-jwt/jwt to version 4.5.1 or later",
      "workaround": "See https://github.com/golang-jwt/jwt/security/advisories/GHSA-29wx-vh33-7x7r for details",
      "resolution": "fixed",
      "resolutionDetails": "Fixed via PR #6748 which likely upgraded the dependency to the patched version",
      "related": [
        6748
      ],
      "keyQuote": "CVE-2024-51744 - Upgrade to version 4.5.1",
      "number": 6767,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:23.133Z"
    },
    {
      "summary": "Request to include in-memory sync match backlog count in ApproximateBacklogCount metric. Currently, tasks that never reach the durable backlog (such as queries and Nexus tasks) aren't reflected in backlog metrics, limiting visibility for autoscaling decisions in high sync match scenarios.",
      "category": "feature",
      "subcategory": "backlog-metrics",
      "apis": [],
      "components": [
        "backlog",
        "metrics",
        "sync-match",
        "worker-autoscaling"
      ],
      "concepts": [
        "backlog-visibility",
        "sync-matching",
        "autoscaling",
        "task-queuing",
        "in-memory-tasks",
        "durable-storage"
      ],
      "severity": "medium",
      "userImpact": "Users cannot accurately monitor and autoscale workers when high volumes of queries and Nexus tasks bypass the durable backlog.",
      "rootCause": null,
      "proposedFix": "Extend ApproximateBacklogCount to include in-memory sync match backlog in addition to durable backlog.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "There are some tasks that will never go to the durable backlog, such as queries and Nexus tasks. In those cases, and in cases where there's a high sync match rate, it's useful to know how many tasks are in the sync match backlog in order to autoscale workers.",
      "number": 6760,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:24.586Z"
    },
    {
      "summary": "Users cannot change activity task dispatch RPS limits after a worker has started without redeploying. This feature request seeks an API to dynamically set RPS limits at runtime, with API-set values taking precedence over worker configuration options.",
      "category": "feature",
      "subcategory": "activity-rate-limiting",
      "apis": [],
      "components": [
        "worker",
        "activity-task-dispatch",
        "rate-limiter"
      ],
      "concepts": [
        "rate-limiting",
        "dynamic-configuration",
        "runtime-updates",
        "RPS-control",
        "task-dispatch",
        "worker-options",
        "configuration-management"
      ],
      "severity": "medium",
      "userImpact": "Users currently must redeploy workers to change activity task dispatch RPS limits, which is operationally cumbersome; this feature would enable dynamic RPS adjustment without redeployment.",
      "rootCause": null,
      "proposedFix": "Implement an API to set RPS limits dynamically with priority hierarchy where API-set values override worker option configuration, and include an unset feature to revert to worker option values.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "After a worker has started and polls for tasks, this RPS value can only be changed by re-deploying the workers. This has proved to be a pain-point for some users",
      "number": 6758,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:23.371Z"
    },
    {
      "summary": "Dynamic config value conversion is performed on every retrieval, consuming excessive CPU resources. The proposal is to optimize this by converting only when values change, using NotifyingClient interface as the primary optimization path.",
      "category": "feature",
      "subcategory": "dynamic-config",
      "apis": [],
      "components": [
        "dynamic-config",
        "client",
        "config-conversion"
      ],
      "concepts": [
        "performance",
        "cpu-optimization",
        "caching",
        "client-interface",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users with high-frequency dynamic config value access experience unnecessary CPU overhead from repeated conversion operations.",
      "rootCause": "Conversion logic is executed on every value retrieval rather than only when config values change.",
      "proposedFix": "Optimize for NotifyingClient implementations to perform conversion only when values change, reducing redundant operations.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed as completed after implementing the optimization.",
      "related": [],
      "keyQuote": "The conversion code is run every time a value is retrieved and could end up taking up excessive CPU resources.",
      "number": 6756,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:09.578Z"
    },
    {
      "summary": "Request to add custom labels (dimensions) to Temporal's out-of-the-box metrics. Users want to tag metrics with custom Search Attributes (e.g., workflow subtype) to enable more granular monitoring and troubleshooting of workflow instances.",
      "category": "feature",
      "subcategory": "metrics-custom-labels",
      "apis": [],
      "components": [
        "metrics",
        "search-attributes"
      ],
      "concepts": [
        "custom-labels",
        "dimensions",
        "monitoring",
        "observability",
        "metric-enrichment",
        "troubleshooting"
      ],
      "severity": "medium",
      "userImpact": "Users cannot correlate custom Search Attributes with metrics, limiting the ability to slice metric data by business-relevant dimensions for analysis and troubleshooting.",
      "rootCause": null,
      "proposedFix": "Provide an option when adding a custom Search Attribute to include it as a metric label in the out-of-the-box metrics.",
      "workaround": "If possible to reference and copy default metrics within workflow/activity code to modify them with additional labels.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "adding custom labels (dimensions) to these metrics would offer more precise insights for troubleshooting",
      "number": 6754,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:10.824Z"
    },
    {
      "summary": "Child workflows in W2 are incorrectly executing activities from W3 instead of their own registered activities, causing unexpected workflow execution order. This appears to be caused by function reference ambiguity when workflows and activities are registered without disabling registration aliasing.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [
        "StartWorkflow",
        "ExecuteActivity"
      ],
      "components": [
        "worker",
        "activity-executor",
        "workflow-registration",
        "task-queue"
      ],
      "concepts": [
        "activity-scheduling",
        "workflow-composition",
        "registration-aliasing",
        "function-references",
        "child-workflows",
        "task-queue"
      ],
      "severity": "high",
      "userImpact": "Users with multiple child workflows and activities on the same task queue may experience incorrect activities being scheduled, breaking workflow logic and business processes.",
      "rootCause": "Function reference aliasing ambiguity when workflow and activity functions are registered with custom names without DisableRegistrationAliasing enabled, causing the worker to confuse which activity belongs to which workflow.",
      "proposedFix": "Set DisableRegistrationAliasing: true in worker.Options to prevent ambiguity between function names and aliased names when using function references for child workflows and activities.",
      "workaround": "Enable DisableRegistrationAliasing in worker configuration and use string names when executing child workflows or activities instead of function references.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by using DisableRegistrationAliasing configuration option to eliminate function reference ambiguity in worker registration.",
      "related": [],
      "keyQuote": "Users are strongly recommended to set this as true if they register any workflow or activity functions with custom names...ambiguity can occur between function names and aliased names",
      "number": 6749,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:12.653Z"
    },
    {
      "summary": "User encountered a deadlock detection error (TMPRL1101) and is seeking guidance on how to fix it. The issue contains only the error message in the title with no additional context or description provided.",
      "category": "question",
      "subcategory": "deadlock-detection",
      "apis": [],
      "components": [
        "deadlock-detector",
        "temporal-runtime"
      ],
      "concepts": [
        "deadlock",
        "concurrency",
        "error-handling",
        "synchronization"
      ],
      "severity": "high",
      "userImpact": "Users encountering deadlock detection errors need guidance to resolve the underlying concurrency issue in their workflows.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was closed likely due to lack of information or reproduction steps provided by the user",
      "related": [],
      "keyQuote": null,
      "number": 6732,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:57.808Z"
    },
    {
      "summary": "Request for additional batch reset types beyond resetting to first workflow task. Users need ability to reset to last workflow task, last task minus an offset, or by time to properly recover from failures.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "batch-operations",
        "workflow-execution",
        "reset-handler"
      ],
      "concepts": [
        "reset",
        "recovery",
        "failure-handling",
        "workflow-task",
        "offset",
        "time-based"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently recover workflows from certain failure states without workarounds, limiting operational flexibility for batch reset operations.",
      "rootCause": null,
      "proposedFix": "Provide more reset types with offset-based options relative to workflow first/last task, or implement time-based reset criteria.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "reset to last workflow task will just complete or fail the workflow. If the activity task fail and we wanted to reset, we should reset to last workflow task -1",
      "number": 6724,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:59.353Z"
    },
    {
      "summary": "Add metrics to measure how often queries are sent to closed workflows and the delay between workflow closure and query execution. This visibility would help determine whether sticky queries should be sent to closed workflows.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "query-handler",
        "metrics",
        "workflow-cache"
      ],
      "concepts": [
        "metrics",
        "closed-workflow",
        "query-delay",
        "cache-eviction",
        "observability"
      ],
      "severity": "medium",
      "userImpact": "Users lack visibility into query patterns for closed workflows, making it difficult to optimize query routing and cache policies.",
      "rootCause": null,
      "proposedFix": "Implement metrics to track query frequency to closed workflows and measure time delays between workflow closure and query execution.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented via pull request #7226",
      "related": [
        7226
      ],
      "keyQuote": "The longer the delay of the query after wf closed the higher chance the cache would be evicted on SDK side.",
      "number": 6711,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:15:00.095Z"
    },
    {
      "summary": "The CLI command to create KeywordList search attributes fails with 'unsupported search attribute type' error. The root cause was user error (trailing whitespace in the type parameter), but the team improved the error message to include quotes around the attribute type for clarity.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "cli",
        "operator",
        "search-attributes"
      ],
      "concepts": [
        "search-attributes",
        "KeywordList",
        "error-messaging",
        "command-parsing",
        "user-experience"
      ],
      "severity": "low",
      "userImpact": "Users attempting to create KeywordList search attributes encounter a confusing error message that doesn't immediately indicate the actual problem (trailing whitespace).",
      "rootCause": "Whitespace in the search attribute type parameter (e.g., 'KeywordList ' with trailing space) was not being trimmed before validation, causing the type to not match the supported types.",
      "proposedFix": "Trim whitespace from the search attribute type parameter before validation, and improve error messages to show the attribute type in quotes for clarity.",
      "workaround": "Remove any trailing whitespace from the --type parameter value when creating search attributes.",
      "resolution": "fixed",
      "resolutionDetails": "Error message was improved by adding quotes around the search attribute type (CLI PR #772), making it clearer when there are parsing issues.",
      "related": [],
      "keyQuote": "I had white space after the keywordList like this 'KeywordList ': containerState.execInContainer(...'--type', 'KeywordList '...",
      "number": 6700,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:47.728Z"
    },
    {
      "summary": "XDC cache continues to grow unbounded when multi-cluster replication is not enabled, causing potential OOM issues in the history service. The cache lacks TTL-based eviction and operators may not realize they should tune its size limit when not using multi-cluster replication.",
      "category": "bug",
      "subcategory": "memory-management",
      "apis": [],
      "components": [
        "history-service",
        "xdc-cache"
      ],
      "concepts": [
        "memory-leak",
        "cache-eviction",
        "multi-cluster-replication",
        "ttl",
        "configuration",
        "oom"
      ],
      "severity": "high",
      "userImpact": "History service can crash with out-of-memory errors if XDC cache is not manually tuned, particularly for operators not using multi-cluster replication features.",
      "rootCause": "XDC cache entries are not evicted according to their TTL, causing unbounded memory growth; the cache is active even when multi-cluster replication is disabled.",
      "proposedFix": "Minimize XDC cache overheads when multi-cluster replication is not enabled, likely by disabling or significantly reducing cache activity in non-MCR deployments.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4379
      ],
      "keyQuote": "If operators don't tune the XDC cache size limit (and it's not apparent that they should, especially if they're not using multi-cluster replication) the history service will OOM.",
      "number": 6695,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:49.059Z"
    },
    {
      "summary": "The activity_info_maps table in PostgreSQL may be experiencing unbounded growth without proper cleanup. Investigation needed to confirm if this is a bug or expected behavior.",
      "category": "bug",
      "subcategory": "database-storage",
      "apis": [],
      "components": [
        "postgres",
        "activity-info-map",
        "database-cleanup"
      ],
      "concepts": [
        "unbounded-growth",
        "table-cleanup",
        "memory-management",
        "database-maintenance",
        "storage-optimization"
      ],
      "severity": "high",
      "userImpact": "Users may experience database disk space exhaustion due to unchecked growth of the activity_info_maps table.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was investigated and resolved through database cleanup mechanisms",
      "related": [],
      "keyQuote": "activity_info_maps table in postgres suspected unbounded growth",
      "number": 6689,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:47.370Z"
    },
    {
      "summary": "CI test step timeout-minutes needs coordination with the `go test -timeout` flag to ensure consistent timeout behavior across CI and local testing.",
      "category": "feature",
      "subcategory": "ci-testing",
      "apis": [],
      "components": [
        "ci",
        "testing",
        "go-build"
      ],
      "concepts": [
        "timeout",
        "coordination",
        "testing",
        "ci-configuration",
        "go-test"
      ],
      "severity": "medium",
      "userImpact": "Misaligned timeouts between CI step configuration and go test flags can cause confusing test failures or hangs that behave differently in CI versus local development.",
      "rootCause": "CI step timeout-minutes and go test -timeout flag are configured independently without coordination, leading to potential timeout mismatches.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        6668
      ],
      "keyQuote": "Coordinate CI step timeout-minutes with `go test -timeout` flag",
      "number": 6680,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:35.472Z"
    },
    {
      "summary": "SQL search attributes operations fail with authorization errors because claims are not passed to internal DescribeNamespace calls. The authorization interceptor rejects requests for get-search-attributes and related operations when using SQL databases for visibility instead of Elasticsearch.",
      "category": "bug",
      "subcategory": "authorization",
      "apis": [
        "ListSearchAttributes",
        "DescribeNamespace",
        "UpdateNamespace"
      ],
      "components": [
        "authorization",
        "frontend",
        "admin_handler",
        "search-attributes",
        "SQL-visibility"
      ],
      "concepts": [
        "claims",
        "authorization",
        "internal-calls",
        "namespace-scoping",
        "visibility-store"
      ],
      "severity": "high",
      "userImpact": "Users cannot manage search attributes in SQL-based visibility setups without bypassing authorization, breaking admin workflows.",
      "rootCause": "Claims are not propagated to internal DescribeNamespace calls made by ListSearchAttributesSQL, causing the authorization interceptor to deny requests even for authorized admin users.",
      "proposedFix": "Pass claims context through internal frontend-to-frontend calls for DescribeNamespace and UpdateNamespace operations, or exempt internal service calls from authorization checks.",
      "workaround": "Bypass claims check for DescribeNamespace and UpdateNamespace endpoints in the default authorizer, or use Elasticsearch instead of SQL for visibility.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "claims are not send for DescribeNamespace... In docker-builds/temporal/service/frontend/admin_handler.go I see function `getSearchAttributesSQL` which executes the `DescribeNamespace` function.",
      "number": 6664,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:37.410Z"
    },
    {
      "summary": "Worker panicked during data deserialization when using a custom ChaCha20Poly1305 data converter. The issue was resolved by ensuring the underlying data converter was properly initialized with GetDefaultDataConverter().",
      "category": "bug",
      "subcategory": "data-converter",
      "apis": [],
      "components": [
        "data-converter",
        "worker",
        "codec"
      ],
      "concepts": [
        "encryption",
        "deserialization",
        "data-converter",
        "codec",
        "payload-handling"
      ],
      "severity": "medium",
      "userImpact": "Custom data converter implementations may crash workers if not properly initialized with a default underlying converter.",
      "rootCause": "Missing proper initialization of the underlying data converter in the custom CodecDataConverter setup - not using GetDefaultDataConverter() caused deserialization to fail.",
      "proposedFix": null,
      "workaround": "Use converter.GetDefaultDataConverter() as the underlying converter when creating a CodecDataConverter with custom codecs.",
      "resolution": "self_resolved",
      "resolutionDetails": "User identified the root cause themselves - they were not using GetDefaultDataConverter() as the underlying converter in their worker setup.",
      "related": [],
      "keyQuote": "My bad, the panic is caused by not using a `converter.GetDefaultDataConverter()` as the underlying converter in my worker",
      "number": 6661,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:35.125Z"
    },
    {
      "summary": "Feature request to add an option for UpsertSearchAttribute command to fail immediately if the search attribute doesn't exist, rather than causing infinite workflow task retries. This would help developers catch configuration errors early instead of silently failing at runtime.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [
        "UpsertSearchAttribute"
      ],
      "components": [
        "workflow-engine",
        "search-attributes",
        "command-processing"
      ],
      "concepts": [
        "search-attributes",
        "configuration-validation",
        "error-handling",
        "workflow-execution",
        "command-options"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to upsert non-existent search attributes experience infinite workflow task retries instead of fast-failing with clear feedback.",
      "rootCause": "The UpsertSearchAttribute command retries indefinitely when the search attribute doesn't exist, lacking an option to fail-fast for validation purposes.",
      "proposedFix": "Add a command option to make UpsertSearchAttribute fail immediately if the search attribute does not exist, rather than retrying.",
      "workaround": "Use an activity or local activity to check if the search attribute exists before calling UpsertSearchAttribute.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Give option for workflow to fail the command if SA does not exists",
      "number": 6647,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:24.112Z"
    },
    {
      "summary": "Workflow termination fails with transaction size exceeded error when workflow execution contains buffered events exceeding 4MB limit. User cannot terminate workflow through UI, receiving 'cannot terminate workflow' error.",
      "category": "bug",
      "subcategory": "workflow-termination",
      "apis": [],
      "components": [
        "workflow-execution",
        "persistence",
        "transaction-handling"
      ],
      "concepts": [
        "transaction-size-limit",
        "buffered-events",
        "workflow-termination",
        "state-management",
        "limits",
        "configuration"
      ],
      "severity": "high",
      "userImpact": "Users are unable to terminate workflows that have accumulated large buffered events, blocking workflow cleanup and potentially blocking system resources.",
      "rootCause": "Buffered events in workflow execution grow too large (>4MB), causing UpdateWorkflowExecution transaction to fail when termination attempts to flush events, preventing workflow termination.",
      "proposedFix": "Prevent buffered events from growing larger than 2MB to avoid exceeding transaction size limit during termination.",
      "workaround": "Temporarily increase system.transactionSizeLimit configuration value to allow termination to proceed, or delete workflow using temporal CLI which doesn't enforce size limit.",
      "resolution": "fixed",
      "resolutionDetails": "Issue closed due to inactivity. Fix implemented in later releases to prevent buffered events from exceeding 2MB limit.",
      "related": [],
      "keyQuote": "I am guessing there are too many buffered events in the workflow which exceeds the 4MB and can't be flushed upon workflow termination.",
      "number": 6638,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:24.302Z"
    },
    {
      "summary": "Request to support native/exponential histograms in Temporal Server and SDKs to reduce observability costs. Native histograms provide 10x resolution at half the price compared to classic histograms by storing data with fewer metric series.",
      "category": "feature",
      "subcategory": "observability-metrics",
      "apis": [],
      "components": [
        "metrics",
        "prometheus-exporter",
        "observability"
      ],
      "concepts": [
        "native-histograms",
        "prometheus",
        "cost-optimization",
        "metric-series",
        "observability-platforms",
        "monitoring"
      ],
      "severity": "medium",
      "userImpact": "Self-hosted deployments can significantly reduce observability platform costs by using native histograms instead of classic histograms for metric collection.",
      "rootCause": "Current Server/SDK implementation only emits classic histograms; native histogram support is not implemented despite Prometheus experimental support.",
      "proposedFix": "Update Server and SDK code to export exponential/native histograms as an option alongside classic histograms, allowing teams to reduce metric series volume and costs.",
      "workaround": "Use Grafana Alloy to scrape metrics, though native histogram scraping is not currently functional with Temporal's classic histogram emission.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "you get 10x the resolution at half the price",
      "number": 6633,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:22.611Z"
    },
    {
      "summary": "Temporal auto-setup Docker container fails to start with database connection error during schema version check. The error occurs because the container cannot connect to the database to verify schema compatibility, resulting in an fx dependency injection initialization failure.",
      "category": "bug",
      "subcategory": "docker-setup",
      "apis": [],
      "components": [
        "server-initialization",
        "database-connection",
        "schema-validation",
        "fx-dependency-injection"
      ],
      "concepts": [
        "database-connectivity",
        "docker-startup",
        "schema-version-check",
        "connection-pooling",
        "network-isolation",
        "initialization-error"
      ],
      "severity": "high",
      "userImpact": "Users cannot start the Temporal server using auto-setup Docker containers when the database is in a private network, blocking server deployment.",
      "rootCause": "The server performs a database schema version compatibility check during fx initialization before confirming database connectivity. When the database is unreachable from the Docker container, this check fails with a confusing error message nested in fx dependency injection logs.",
      "proposedFix": null,
      "workaround": "Ensure the database is accessible from the Docker container by configuring proper network connectivity between the container and the database.",
      "resolution": "invalid",
      "resolutionDetails": "The issue was determined to be a user configuration problem (database not accessible from the container), not a server bug. The confusing error message and initialization process were identified as problematic, but resolution was deferred to user configuration.",
      "related": [],
      "keyQuote": "sql schema version compatibility check failed: unable to read DB schema version keyspace/database: temporal error: no usable database connection found",
      "number": 6628,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:09.777Z"
    },
    {
      "summary": "Request to support Apache Cassandra 5, which has reached GA status. Users want to use Cassandra 5 with Temporal Server instead of older versions.",
      "category": "feature",
      "subcategory": "cassandra-support",
      "apis": [],
      "components": [
        "persistence",
        "cassandra-driver",
        "datastore"
      ],
      "concepts": [
        "database-compatibility",
        "version-support",
        "storage-backend",
        "cassandra"
      ],
      "severity": "medium",
      "userImpact": "Users cannot deploy Temporal Server with the latest Cassandra 5 GA release, limiting access to new features and improvements.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would be great to use the latest version of Cassandra 5 with Temporal.",
      "number": 6618,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:09.897Z"
    },
    {
      "summary": "Feature request for a hybrid architecture combining Redis for real-time workflow state, Kafka for event streaming and replay, and Cassandra for long-term durable storage. This would reduce persistence overhead and infrastructure costs by batching writes while maintaining strong consistency guarantees.",
      "category": "feature",
      "subcategory": "architecture-persistence",
      "apis": [],
      "components": [
        "persistence-layer",
        "workflow-state-management",
        "event-store",
        "cache-layer"
      ],
      "concepts": [
        "hybrid-architecture",
        "in-memory-caching",
        "event-streaming",
        "batch-writes",
        "fault-tolerance",
        "event-sourcing",
        "scalability"
      ],
      "severity": "medium",
      "userImpact": "Users could reduce persistence infrastructure costs and improve workflow execution performance by leveraging a tiered storage approach with real-time caching and eventual consistency.",
      "rootCause": "Current Temporal design requires immediate writes to persistence store for all events and activity results, preventing batching optimizations and increasing infrastructure costs.",
      "proposedFix": "Implement hybrid architecture: Redis for hot/active workflow state with eviction policies, Kafka for durable event log and replay capability, Cassandra for long-term storage with periodic batch writes from Redis.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Redis acts as a cache for workflow execution state and hot data, while Kafka provides reliable event log for replaying events in case of failures",
      "number": 6617,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:14:11.717Z"
    },
    {
      "summary": "Pending update requests are not aborted when a workflow times out, causing SDK clients to continue polling indefinitely instead of being signaled to stop.",
      "category": "bug",
      "subcategory": "workflow-updates",
      "apis": [
        "UpdateWorkflowExecution",
        "PollWorkflowExecutionUpdate"
      ],
      "components": [
        "workflow-timeout",
        "update-handler",
        "request-lifecycle"
      ],
      "concepts": [
        "timeout",
        "workflow-execution-timeout",
        "update-completion",
        "polling",
        "request-abortion"
      ],
      "severity": "high",
      "userImpact": "SDK clients hang indefinitely when polling for update outcomes after workflow timeout, requiring manual intervention to stop.",
      "rootCause": "Update requests are not aborted when workflow execution times out, leaving them in a pending state that clients continue to poll.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in #6621",
      "related": [
        6621
      ],
      "keyQuote": "Pending update requests should be aborted when workflow times out",
      "number": 6614,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:58.399Z"
    },
    {
      "summary": "Kubernetes deployments of the Temporal Frontend with mTLS client authentication cannot pass health checks from cloud load balancers that don't support client certificate authentication, causing health check failures. A request to expose an anonymous HTTP/HTTPS health endpoint that doesn't require authentication.",
      "category": "feature",
      "subcategory": "frontend-health-checks",
      "apis": [],
      "components": [
        "frontend",
        "health-endpoint",
        "kubernetes-deployment"
      ],
      "concepts": [
        "health-checks",
        "load-balancer",
        "mTLS",
        "authentication",
        "kubernetes",
        "ingress"
      ],
      "severity": "high",
      "userImpact": "Kubernetes-deployed Temporal frontends with mTLS enabled become non-functional because load balancers cannot authenticate for health checks.",
      "rootCause": "Frontend health endpoint requires client authentication via mTLS, but cloud load balancers (GKE, etc.) do not support client certificate authentication for health checks.",
      "proposedFix": "Expose an anonymous HTTP/HTTPS health endpoint that does not require authentication for load balancer health checks.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Enable a health endpoint without auth so the load balancer can use it for health checks",
      "number": 6611,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:58.152Z"
    },
    {
      "summary": "Add three new fields to PendingActivityInfo to help track activity status: last_failure_time (populated after any attempt failure), next_attempt_schedule_time (shown during backoff), and next_retry_interval.",
      "category": "feature",
      "subcategory": "activity-retry",
      "apis": [
        "PendingActivityInfo"
      ],
      "components": [
        "activity-executor",
        "worker",
        "pending-activity-tracking"
      ],
      "concepts": [
        "retry",
        "backoff",
        "failure-tracking",
        "attempt-management",
        "activity-status"
      ],
      "severity": "medium",
      "userImpact": "Users can better monitor and understand the status and retry scheduling of pending activities.",
      "rootCause": null,
      "proposedFix": "Add last_failure_time, next_attempt_schedule_time, and next_retry_interval fields to PendingActivityInfo proto message.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Enhancement implemented to add the requested fields to PendingActivityInfo for better activity status visibility.",
      "related": [],
      "keyQuote": "Add last_failure_time to PendingActivityInfo. Populated once an attempt failure is recorded for any reason that an attempt might fail, including any form of timeout.",
      "number": 6605,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:58.080Z"
    },
    {
      "summary": "Request for standardized documentation format (like Swagger/OpenAPI) for exposing Temporal Workflows as black-box services. Users want clear documentation templates and tooling to generate human-friendly API documentation that specifies workflow inputs/outputs without exposing internal implementation details.",
      "category": "feature",
      "subcategory": "api-documentation",
      "apis": [],
      "components": [
        "workflow",
        "documentation",
        "api-specification"
      ],
      "concepts": [
        "api-specification",
        "documentation-format",
        "workflow-abstraction",
        "service-interface",
        "standardization",
        "tooling"
      ],
      "severity": "medium",
      "userImpact": "Enables organizations to expose Temporal Workflows as documented services with standardized API contracts, improving developer experience and cross-project reusability.",
      "rootCause": null,
      "proposedFix": "Implement Swagger/OpenAPI-style documentation structure and create tooling to generate human-friendly documentation from workflow specifications.",
      "workaround": "Custom YAML files with team-specific standards (noted as alternative but suboptimal due to lack of standardization and reusability).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "define a consistent structure to use across projects and Creating tooling that can read in the consistent structure to generate a human-friendly documentation page",
      "number": 6600,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:47.364Z"
    },
    {
      "summary": "Add a history event to record when a Nexus operation request is delivered to the handler, enabling support for WAIT_CANCELLATION_REQUESTED CancellationType in SDKs similar to child workflow cancellation handling.",
      "category": "feature",
      "subcategory": "nexus-operations",
      "apis": [],
      "components": [
        "nexus-handler",
        "history-events",
        "cancellation"
      ],
      "concepts": [
        "cancellation-type",
        "operation-delivery",
        "history-recording",
        "handler-notification",
        "nexus-protocol"
      ],
      "severity": "medium",
      "userImpact": "SDKs cannot properly implement WAIT_CANCELLATION_REQUESTED behavior for Nexus operations without this history event.",
      "rootCause": null,
      "proposedFix": "Record a history event when a Nexus operation request is delivered to the handler",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Required to support the WAIT_CANCELLATION_REQUESTED CancellationType in the SDKs",
      "number": 6585,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:44.614Z"
    },
    {
      "summary": "Querying for `CloseTime is null` does not return running workflows, even though they have no close time in the JSON output. The opposite query (`CloseTime is not null`) incorrectly returns both closed and running workflows.",
      "category": "bug",
      "subcategory": "visibility-query",
      "apis": [],
      "components": [
        "visibility-store",
        "query-engine",
        "workflow-list"
      ],
      "concepts": [
        "null-handling",
        "query-semantics",
        "workflow-status",
        "filter-logic"
      ],
      "severity": "medium",
      "userImpact": "Users cannot query for running workflows using `CloseTime is null`, forcing them to use alternative queries based on status field instead.",
      "rootCause": "Issue with null value handling in the visibility query engine when filtering on close_time field",
      "proposedFix": null,
      "workaround": "Query based on execution status field instead of close_time (e.g., `status = Running` instead of `CloseTime is null`)",
      "resolution": "fixed",
      "resolutionDetails": "Addressed by issue #6965",
      "related": [
        6965
      ],
      "keyQuote": "Query returns no results. Oddly, if I do the same query but query where close time is not null, I get results for all past workflows AND the currently running workflow.",
      "number": 6576,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:46.644Z"
    },
    {
      "summary": "The Elasticsearch Scroll API in Temporal 1.24.0+ fails when performing batch workflow resets via tctl because track_total_hits is disabled, which is not allowed in scroll contexts. This prevents users from querying and batch-resetting workflows.",
      "category": "bug",
      "subcategory": "elasticsearch-visibility",
      "apis": [],
      "components": [
        "elasticsearch",
        "scroll-api",
        "workflow-visibility",
        "batch-operations"
      ],
      "concepts": [
        "elasticsearch-scroll",
        "track_total_hits",
        "query-validation",
        "batch-reset",
        "visibility-store"
      ],
      "severity": "high",
      "userImpact": "Users cannot batch reset workflows via tctl queries in Temporal 1.24.0+, blocking a critical workflow management operation.",
      "rootCause": "The track_total_hits setting was disabled in the Elasticsearch query for the Scroll API, but Elasticsearch 7.10 does not allow disabling this in scroll contexts, causing a validation error.",
      "proposedFix": "Re-enable track_total_hits for the Elasticsearch Scroll API query context.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in PR #6592, included in a subsequent patch release.",
      "related": [
        6592
      ],
      "keyQuote": "disabling [track_total_hits] is not allowed in a scroll context",
      "number": 6566,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:35.677Z"
    },
    {
      "summary": "The current Temporal server configuration is overly complex, requiring a directory-based structure with multiple files for static and dynamic configuration. This feature request seeks a simplified, single-file configuration format to improve developer experience and enable configuration support in the Temporal CLI.",
      "category": "feature",
      "subcategory": "configuration",
      "apis": [],
      "components": [
        "server-configuration",
        "config-loader",
        "dynamic-config",
        "cli"
      ],
      "concepts": [
        "configuration-simplification",
        "file-format",
        "developer-experience",
        "backward-compatibility",
        "configuration-merging"
      ],
      "severity": "medium",
      "userImpact": "Users struggle with complex multi-file configuration requirements, limiting server adoption and CLI integration capabilities.",
      "rootCause": "Current design requires separate static and dynamic config files with directory-based structure and complex merging logic.",
      "proposedFix": "Collapse configuration into a single file with optional dynamic configuration section (top-level `dynamic` key) that reloads on change, with optional polling interval setting.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This complexity hurts the developer experience and is the reason we don't expose server configuration in the Temporal CLI.",
      "number": 6561,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:33.382Z"
    },
    {
      "summary": "Request to add EdDSA (Ed25519) support to the default JWT ClaimMapper, which currently only supports RS256 and ES256 token types.",
      "category": "feature",
      "subcategory": "authentication-jwt",
      "apis": [],
      "components": [
        "authorization",
        "default_token_key_provider",
        "jwt-claim-mapper"
      ],
      "concepts": [
        "jwt",
        "authentication",
        "eddsa",
        "ed25519",
        "token-validation",
        "cryptography"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use EdDSA/Ed25519 tokens with Temporal's default JWT ClaimMapper, limiting authentication options.",
      "rootCause": null,
      "proposedFix": "Add EdDSA support using Ed25519 curve to the default ClaimMapper alongside existing RS256 and ES256 support.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add support for `EdDSA` using `Ed25519` curve.",
      "number": 6555,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:33.930Z"
    },
    {
      "summary": "User needs to run a blocking function (up to 10s) during workflow execution that cannot receive workflow context and must run on replays, but the workflow deadlock detector panics. Request to allow disabling deadlock detection for specific scopes.",
      "category": "feature",
      "subcategory": "deadlock-detection",
      "apis": [],
      "components": [
        "deadlock-detector",
        "workflow-execution",
        "data-converter"
      ],
      "concepts": [
        "blocking-operations",
        "workflow-replay",
        "determinism",
        "deadlock-detection",
        "synchronous-execution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot run necessary blocking operations during workflows without triggering panic, forcing them to use workarounds that compromise determinism.",
      "rootCause": "Workflow deadlock detector prevents blocking operations by design to ensure determinism, but legitimate use cases require blocking external functions that cannot be adapted to async patterns.",
      "proposedFix": "Make the deadlock detection disable mechanism public (e.g., making internal functions in workflow_deadlock.go public), or provide an official API to disable deadlock detection for specific scopes.",
      "workaround": "Abuse the DataConverterWithoutDeadlockDetection by implementing a custom data converter that executes the blocking function in its ToStrings method, though this is non-deterministic.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1647
      ],
      "keyQuote": "Allow to disable the deadlock detection for a specific scope. Possible just making workflow_deadlock.go public.",
      "number": 6546,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:23.557Z"
    },
    {
      "summary": "The temporal-history pod panics with a slice bounds out of range error during protobuf marshaling of a DescribeWorkflowExecutionResponse. The panic occurs in the WorkflowExecution.MarshalToSizedBuffer function with invalid slice indices.",
      "category": "bug",
      "subcategory": "protobuf-marshaling",
      "apis": [],
      "components": [
        "history-service",
        "protobuf-marshaling",
        "workflow-execution"
      ],
      "concepts": [
        "marshaling",
        "serialization",
        "buffer-overflow",
        "slice-bounds",
        "panic",
        "memory-safety"
      ],
      "severity": "critical",
      "userImpact": "Production temporal-history pods crash unexpectedly, causing service disruption and data processing failures.",
      "rootCause": "Protobuf marshaling code attempts to write to a buffer with invalid slice bounds (-30:), likely caused by incorrect buffer size calculations or data corruption in WorkflowExecution fields.",
      "proposedFix": null,
      "workaround": null,
      "related": [
        4159
      ],
      "keyQuote": "panic: runtime error: slice bounds out of range [-30:] in go.temporal.io/api/common/v1.(*WorkflowExecution).MarshalToSizedBuffer",
      "resolution": null,
      "resolutionDetails": null,
      "number": 6542,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:20.718Z"
    },
    {
      "summary": "ListQueues and other methods using getAnyClient bypass the cachingRedirector's redirectLoop, preventing failed history hosts from being removed from the cache when they become unavailable.",
      "category": "bug",
      "subcategory": "history-client",
      "apis": [
        "ListQueues"
      ],
      "components": [
        "cachingRedirector",
        "history-client",
        "client-routing"
      ],
      "concepts": [
        "fault-tolerance",
        "host-failure",
        "caching",
        "client-routing",
        "error-recovery"
      ],
      "severity": "high",
      "userImpact": "When a history host fails, ListQueues and similar methods fail to remove the broken host from cache, causing repeated failures until the cache entry expires or is manually cleared.",
      "rootCause": "ListQueues calls the corresponding client method directly instead of using redirectLoop, which is responsible for detecting failures and removing dead hosts from the cache.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "cachingRedirector handles history failures in its redirectLoop and removes failed hosts from the cache. However, ListQueues does not use redirectLoop and calls corresponding client's method directly.",
      "number": 6541,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:21.068Z"
    },
    {
      "summary": "Kubernetes service discovery for Elasticsearch seeds configuration fails when service names contain ports. The server incorrectly concatenates multiple seed IPs with commas, resulting in malformed requests to Elasticsearch.",
      "category": "bug",
      "subcategory": "elasticsearch-configuration",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "server-configuration",
        "seed-discovery"
      ],
      "concepts": [
        "kubernetes-service-discovery",
        "elasticsearch-connection",
        "configuration-parsing",
        "high-availability",
        "service-naming"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal on Kubernetes cannot reliably configure Elasticsearch seeds using service discovery, blocking highly available cluster deployments.",
      "rootCause": "The ES_SEEDS configuration parser does not properly handle Kubernetes service names with ports, and comma-separated seed lists are concatenated incorrectly in requests to Elasticsearch.",
      "proposedFix": "Make ES_PORT optional and handle Kubernetes service discovery properly, or allow ES_SERVER to directly override seed configuration.",
      "workaround": "Manually specify individual cluster node IPs instead of using Kubernetes service discovery.",
      "resolution": "unknown",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We don't want to give only one ip for ES_SEEDS because it can be changed thus we want to give our Kubernetes service definition.",
      "number": 6531,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:10.273Z"
    },
    {
      "summary": "Certificate-filters functionality is currently only available in Temporal Cloud but not in self-hosted deployments. This feature request asks for implementing certificate-based access control in on-prem environments to maintain security feature parity and address the limitation that self-hosted users lack similar security capabilities.",
      "category": "feature",
      "subcategory": "security-authorization",
      "apis": [],
      "components": [
        "authorization",
        "certificate-auth",
        "namespace-config"
      ],
      "concepts": [
        "certificate-filters",
        "security",
        "access-control",
        "claim-mapping",
        "on-prem",
        "feature-parity"
      ],
      "severity": "medium",
      "userImpact": "Self-hosted Temporal users cannot enforce certificate-based access restrictions, limiting their ability to implement the same security controls available to cloud users.",
      "rootCause": null,
      "proposedFix": "Implement a custom claim mapper (similar to myClaimMapper.go) that reads from a certificate-filters file and enforces restrictions based on certificate attributes, or provide native certificate-filter support in self-hosted version.",
      "workaround": "Use an nginx gRPC proxy with certificate-based access control at the network level, though this adds complexity compared to a native solution.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would be beneficial to have this functionality available in the self-hosted version as well, as it's frustrating for users running on-prem environments to lack similar security features.",
      "number": 6529,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:08.346Z"
    },
    {
      "summary": "WorkflowCache maintains deleted workflows in memory while they are removed from the database, causing inconsistency. This is particularly confusing for synchronous operations like the tdbg workflow deleted command, and results in deleted workflows still appearing in the Web UI with 404 errors when accessed.",
      "category": "bug",
      "subcategory": "workflow-cache",
      "apis": [],
      "components": [
        "WorkflowCache",
        "workflow-deletion",
        "Web UI"
      ],
      "concepts": [
        "cache-consistency",
        "state-synchronization",
        "deletion",
        "async-operations",
        "memory-database-mismatch"
      ],
      "severity": "medium",
      "userImpact": "Deleted workflows remain visible in the UI and cache until async deletion completes, causing confusion when attempting to access them results in 404 errors.",
      "rootCause": "WorkflowCache is not invalidated or updated synchronously when workflows are deleted from the database.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Cached workflow mutate state should always match state in DB.",
      "number": 6527,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:13:08.679Z"
    },
    {
      "summary": "Rare spurious failures in CLI and SDK tests after Server 1.25 update, with error 'Current branch token and request branch token doesn't match.' occurring on history fetches and activity operations without any actual workflow resets being performed. The issue appears to be server-side as tests are non-reproducible locally but fail intermittently in CI.",
      "category": "bug",
      "subcategory": "history-fetching",
      "apis": [],
      "components": [
        "history-service",
        "branch-token-management",
        "workflow-execution",
        "test-server"
      ],
      "concepts": [
        "branch-token",
        "history-divergence",
        "concurrent-requests",
        "state-mismatch",
        "race-condition",
        "dev-server"
      ],
      "severity": "high",
      "userImpact": "Users experience intermittent test failures when using the CLI and SDKs with Temporal Server 1.25, causing non-deterministic failures that are difficult to debug and reproduce locally.",
      "rootCause": "Server-side issue with branch token validation logic in version 1.25, possibly a race condition or state management bug in the history service that doesn't require actual workflow resets to manifest.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Current branch token and request branch token doesn't match.",
      "number": 6525,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:57.534Z"
    },
    {
      "summary": "Request to mark activity calls as 'optional' in workflows to allow adding/removing activity invocations without breaking determinism. This would enable easier workflow evolution for fire-and-forget activities that don't impact workflow results.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "workflow-engine",
        "activity-executor",
        "determinism-validator"
      ],
      "concepts": [
        "determinism",
        "workflow-replay",
        "fire-and-forget",
        "activity-invocation",
        "backward-compatibility",
        "workflow-evolution"
      ],
      "severity": "medium",
      "userImpact": "Users struggle to extend workflows with new activity calls without risking determinism violations, limiting the ability to evolve workflows safely over time.",
      "rootCause": null,
      "proposedFix": "Introduce an 'optional' marker for activity calls that informs the runtime these activities can be skipped during replay without affecting workflow determinism",
      "workaround": "Use external event publishing with custom listeners and separate infrastructure for fan-out patterns instead of direct activity invocations",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "adding/removing activities will break determinism. We would like to have an ability to mark an activity call as 'optional'",
      "number": 6521,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:56.321Z"
    },
    {
      "summary": "Temporal Server gets stuck when database connections become unusable, unable to recover even after restart. Resolved in patch after v1.25.0 by fixing connection pool recovery logic.",
      "category": "bug",
      "subcategory": "database-connectivity",
      "apis": [],
      "components": [
        "history-service",
        "queue-processor",
        "database-connection-pool",
        "transfer-queue"
      ],
      "concepts": [
        "database-connection",
        "connection-recovery",
        "connection-pooling",
        "persistence-layer",
        "server-stalling",
        "postgres-compatibility"
      ],
      "severity": "critical",
      "userImpact": "Workflows get stuck and server becomes unresponsive when any database connection fails, requiring server restart or downgrade to resolve.",
      "rootCause": "Database connection pool does not properly refresh or recover when connections become unusable, causing persistent 'no usable database connection found' errors.",
      "proposedFix": "Fix database connection pool recovery logic to allow connections to be re-established when they fail.",
      "workaround": "Downgrade to v1.24.2 or restart the temporal server and database.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was fixed in a patch release after v1.25.0 with improved connection pool recovery mechanism.",
      "related": [],
      "keyQuote": "The issue seems to be if for whatever reason a DB connection goes bad, it never recovers from that state",
      "number": 6514,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:58.329Z"
    },
    {
      "summary": "Update latency increases linearly with the number of updates sent to a workflow, unlike signal and query latency which remain constant. This performance degradation occurs with up to 2000 updates per workflow in both local and cloud deployments.",
      "category": "bug",
      "subcategory": "update-latency",
      "apis": [
        "UpdateWorkflow",
        "update_handler"
      ],
      "components": [
        "workflow-update",
        "update-queue",
        "message-processing"
      ],
      "concepts": [
        "latency",
        "performance",
        "throughput",
        "message-ordering",
        "state-mutation"
      ],
      "severity": "high",
      "userImpact": "Users sending multiple updates to workflows experience increasing latency that degrades performance and may exceed acceptable SLAs for applications relying on workflow updates.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "related": [],
      "keyQuote": "Update latency increases with the number of updates sent to the workflow",
      "resolution": null,
      "resolutionDetails": null,
      "number": 6512,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:44.139Z"
    },
    {
      "summary": "The PollActivityTaskQueueResponse API endpoint is not populating the RetryPolicy field as defined in the Protobuf specification, causing the retry policy to be null when polling activity tasks from a task queue.",
      "category": "bug",
      "subcategory": "activity-polling",
      "apis": [
        "PollActivityTaskQueueResponse"
      ],
      "components": [
        "matching-engine",
        "activity-task-queue",
        "api-response"
      ],
      "concepts": [
        "retry-policy",
        "activity-scheduling",
        "task-polling",
        "protobuf-api-compliance",
        "field-population"
      ],
      "severity": "medium",
      "userImpact": "Users cannot access retry policy information for activities when polling task queues, breaking workflows that depend on this data.",
      "rootCause": "The createPollActivityTaskQueueResponse function in matching_engine.go does not populate the RetryPolicy field when constructing the response object.",
      "proposedFix": "Populate the RetryPolicy field in the PollActivityTaskQueueResponse object during creation in the matching engine.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was fixed and included in version 1.28 release.",
      "related": [],
      "keyQuote": "Matching's createPollActivityTaskQueueResponse does not populate the RetryPolicy field when making a PollActivityTaskQueueResponse object",
      "number": 6502,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:46.387Z"
    },
    {
      "summary": "Feature request to add access to the original run ID in mutable state. Users need a way to retrieve the original run ID from the mutable state object for tracking and correlation purposes.",
      "category": "feature",
      "subcategory": "mutable-state",
      "apis": [],
      "components": [
        "mutable-state",
        "history",
        "workflow-state"
      ],
      "concepts": [
        "run-id",
        "state-management",
        "workflow-execution",
        "tracking",
        "correlation"
      ],
      "severity": "low",
      "userImpact": "Users will be able to access the original run ID directly from mutable state, enabling better tracking and correlation of workflow executions.",
      "rootCause": null,
      "proposedFix": "Add a new field in the mutable state to record the original run id",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        6499
      ],
      "keyQuote": "Added a new field in the mutable state to record the original run id",
      "number": 6501,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:44.861Z"
    },
    {
      "summary": "Request to extend the execution history API to support waiting for signals and display this capability in the Temporal UI. This feature would allow better visibility into signal-based workflow pauses.",
      "category": "feature",
      "subcategory": "execution-history-api",
      "apis": [
        "ExecutionHistory"
      ],
      "components": [
        "execution-history-api",
        "temporal-ui",
        "workflow-history"
      ],
      "concepts": [
        "signals",
        "execution-history",
        "workflow-visibility",
        "api-design",
        "user-interface"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently see signal wait states in the execution history API, limiting visibility into workflow execution with signal-based control flow.",
      "rootCause": null,
      "proposedFix": "Extend execution history API to include signal wait events and update Temporal UI to display these events.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Support wait for signal in execution history API",
      "number": 6481,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:32.870Z"
    },
    {
      "summary": "Web UI returns 404 errors when accessing workflow details from archived workflows in GKE deployment with Google Cloud Storage archival. Multiple users report the same issue when trying to view workflow details despite archival files being present in the bucket.",
      "category": "bug",
      "subcategory": "web-ui-archival",
      "apis": [],
      "components": [
        "web-ui",
        "archival",
        "gcs-provider",
        "workflow-visibility"
      ],
      "concepts": [
        "archival",
        "visibility",
        "workflow-details",
        "gke-deployment",
        "api-endpoint",
        "http-404"
      ],
      "severity": "high",
      "userImpact": "Users cannot view details of archived workflows through the Web UI, making it impossible to access historical workflow information despite successful archival to Google Cloud Storage.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Web UI 404 when accessing detail of workflow id from archival",
      "number": 6479,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:33.972Z"
    },
    {
      "summary": "CLI currently races against operator service when registering search attributes after server start. Request to add a server option/config that accepts search attributes and prevents startup until they are confirmed set with the proper type.",
      "category": "feature",
      "subcategory": "server-startup",
      "apis": [],
      "components": [
        "server",
        "operator-service",
        "search-attributes",
        "cli"
      ],
      "concepts": [
        "startup-configuration",
        "search-attributes",
        "type-validation",
        "initialization",
        "race-condition"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably register custom search attributes during server startup without risking race conditions that prevent proper initialization.",
      "rootCause": "CLI registers search attributes after server start via operator service, but users check port availability to determine if server is ready, creating a race condition.",
      "proposedFix": "Add server option (similar to WithSearchAttributes) that accepts map[string]enums.IndexedValueType, registers attributes before startup completes, and validates existing attributes match expected types.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "register if attr name not there, do nothing if attr name there and the proper type, and fail startup if attr there but wrong type",
      "number": 6475,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:34.725Z"
    },
    {
      "summary": "Test issue created by webchick-test with incomplete template content. Appears to be a placeholder or test artifact with no actual problem description or reproduction steps provided.",
      "category": "other",
      "subcategory": "test-artifact",
      "apis": [],
      "components": [],
      "concepts": [
        "testing",
        "placeholder",
        "incomplete"
      ],
      "severity": "low",
      "userImpact": "No user impact - this is a test or placeholder issue with no actual content.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Angie testing something (to delete)",
      "number": 6471,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:22.449Z"
    },
    {
      "summary": "GenerateReplicationTasks activity in ForceReplicationWorkflow incorrectly returns 'not found' errors instead of only logging them. This causes activities to retry indefinitely when workflows are archived between being listed and processed, preventing the replication workflow from making progress.",
      "category": "bug",
      "subcategory": "replication-workflow",
      "apis": [],
      "components": [
        "migration-worker",
        "replication-activities",
        "history-client",
        "force-replication-workflow"
      ],
      "concepts": [
        "error-handling",
        "activity-retry",
        "replication",
        "workflow-archival",
        "not-found-errors",
        "progress-blocking"
      ],
      "severity": "high",
      "userImpact": "ForceReplicationWorkflow gets stuck in infinite retries when workflows are archived between listing and processing, blocking the entire replication process.",
      "rootCause": "GenerateReplicationTasks returns not found errors from historyClient.GenerateLastHistoryReplicationTasks instead of only logging them, triggering activity retries that cannot succeed because the workflow is archived.",
      "proposedFix": "Modify GenerateReplicationTasks to only log not found errors using a.logger.Error() instead of returning them, similar to the pattern shown in the code snippet where isNotFoundServiceError(err) should only log without returning.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A not found history should just be logged. Returning the error keep retrying the activity",
      "number": 6468,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:23.533Z"
    },
    {
      "summary": "Docker images (admin-tools, server, ui) are using a Go version impacted by CVE-2024-24790. Updating Go to 1.21.11 or newer would resolve the security vulnerability.",
      "category": "bug",
      "subcategory": "security",
      "apis": [],
      "components": [
        "admin-tools",
        "server",
        "ui",
        "docker-images"
      ],
      "concepts": [
        "security",
        "cve",
        "vulnerability",
        "go-version",
        "dependency",
        "docker",
        "container"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal server, admin-tools, and ui images are exposed to a known Go security vulnerability.",
      "rootCause": "Docker images are built with Go version prior to 1.21.11 which contains CVE-2024-24790",
      "proposedFix": "Update Go to version 1.21.11 or newer in the Docker image builds",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Updating Go to 1.21.11 or newer would fix the issue.",
      "number": 6467,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:22.089Z"
    },
    {
      "summary": "Excessive new connections being made to Postgres during load testing (200+ per second) instead of reaching a steady state. Suspected cause is the connection pool's get() method not properly incrementing reference count, causing connections to be discarded and recreated rather than reused.",
      "category": "bug",
      "subcategory": "database-connection-pool",
      "apis": [],
      "components": [
        "persistence-sql-factory",
        "connection-pool",
        "postgres-driver"
      ],
      "concepts": [
        "connection-pooling",
        "resource-management",
        "reference-counting",
        "steady-state",
        "load-testing",
        "database-connection"
      ],
      "severity": "high",
      "userImpact": "Self-hosted deployments using Postgres experience connection thrashing under load, causing performance degradation and increased database overhead.",
      "rootCause": "The connection pool's get() method may not be properly incrementing the reference count, keeping it at 0, which causes connections to be discarded and new ones created instead of being reused.",
      "proposedFix": null,
      "workaround": "Set sufficiently high values for maxIdleConns, maxConns, and maxConnLifetime in the Helm chart values.yaml persistence configuration.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "200+ new connections per second during load test... the number of connections may rise to handle the load but it should achieve a steady state and relatively few connections killed and re-established.",
      "number": 6459,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:11.289Z"
    },
    {
      "summary": "The temporal-sql-tool create-database command fails to use the configured database name (citus) from environment variables, instead trying hardcoded default database names (postgres, defaultdb). This causes connection failures when using custom database names in Helm chart deployments.",
      "category": "bug",
      "subcategory": "database-initialization",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "helm-charts",
        "database-schema-setup"
      ],
      "concepts": [
        "database-initialization",
        "configuration",
        "helm-deployment",
        "environment-variables",
        "PostgreSQL"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal with Helm charts and custom database names encounter initialization failures that block cluster startup.",
      "rootCause": "The create-database command ignores the SQL_DATABASE environment variable and only attempts to connect using hardcoded default database names (postgres, defaultdb).",
      "proposedFix": null,
      "workaround": "Enable TLS and disable host verification in the PostgreSQL configuration.",
      "resolution": "duplicate",
      "resolutionDetails": "Issue was identified as related to Helm chart configuration and duplicated in the helm-charts repository (temporalio/helm-charts#549).",
      "related": [],
      "keyQuote": "unable to connect to DB, tried default DB names: postgres,defaultdb",
      "number": 6435,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:11.568Z"
    },
    {
      "summary": "User metadata from schedule actions is not being propagated to child workflow execution configs, unlike the behavior with direct start workflow calls.",
      "category": "bug",
      "subcategory": "workflow-scheduling",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "scheduler",
        "workflow-execution",
        "execution-config"
      ],
      "concepts": [
        "user-metadata",
        "propagation",
        "schedule-action",
        "child-workflow",
        "execution-context"
      ],
      "severity": "medium",
      "userImpact": "Users cannot pass custom metadata through scheduled workflows to child workflows, limiting metadata context propagation in scheduled execution scenarios.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Like start workflow, I would expect scheduled workflows' user metadata to end up on the execution config on the child workflow.",
      "number": 6413,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:11.569Z"
    },
    {
      "summary": "User metadata specified in StartChildWorkflow command is not propagated to the child workflow's execution config, only appearing in the event. This inconsistency differs from the behavior for regular start workflow operations where user metadata is properly propagated.",
      "category": "bug",
      "subcategory": "child-workflows",
      "apis": [
        "StartChildWorkflow",
        "StartWorkflow"
      ],
      "components": [
        "child-workflow-command",
        "execution-config",
        "user-metadata"
      ],
      "concepts": [
        "metadata-propagation",
        "child-workflows",
        "execution-configuration",
        "workflow-initialization",
        "command-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably access user metadata on child workflow execution configs, forcing workarounds and inconsistent behavior compared to parent workflows.",
      "rootCause": "The user metadata propagation logic added in PR #5857 for start workflow was not applied to child workflow command handling in the same way.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "User metadata propagation from child workflow command to execution config was implemented to match the behavior of parent workflow metadata handling.",
      "related": [
        5857
      ],
      "keyQuote": "Like start workflow, I would expect start child workflow user metadata to end up on the execution config on the child workflow.",
      "number": 6412,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:58.949Z"
    },
    {
      "summary": "QueryWorkflow RPC fails with 'Workflow Task in failed state' error when handling 1000 parallel requests under load. User is unable to query workflows when temporal server experiences high concurrency, suggesting a potential issue with workflow task state management under load.",
      "category": "bug",
      "subcategory": "query-workflow",
      "apis": [
        "QueryWorkflow"
      ],
      "components": [
        "history-service",
        "workflow-task-processor",
        "query-handler"
      ],
      "concepts": [
        "concurrency",
        "load-testing",
        "workflow-state",
        "failed-task",
        "rpc-error"
      ],
      "severity": "high",
      "userImpact": "Users cannot query workflows under high load, blocking critical operations that depend on workflow state inspection.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Error: Cannot query workflow due to Workflow Task in failed state",
      "number": 6411,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:12:00.266Z"
    },
    {
      "summary": "Announcement of a new Quarkus integration for Temporal framework. The integration is being built as a community project and documentation is available on the Quarkiverse documentation site.",
      "category": "other",
      "subcategory": "integration",
      "apis": [],
      "components": [
        "quarkus-integration",
        "sdk"
      ],
      "concepts": [
        "integration",
        "framework",
        "quarkus",
        "extension",
        "interoperability"
      ],
      "severity": "low",
      "userImpact": "Quarkus users can now use Temporal with native Quarkus support through the quarkus-temporal integration.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "we started to build a temporal / quarkus integration",
      "number": 6403,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:58.644Z"
    },
    {
      "summary": "Workflow history API returns 501 errors for some records in Temporal Server 1.24.2. User reports that a portion of executed workflow history records are displayed as 501 errors, with the issue affecting the UI display but not all records.",
      "category": "bug",
      "subcategory": "workflow-history-api",
      "apis": [],
      "components": [
        "workflow-history",
        "server-api",
        "ui"
      ],
      "concepts": [
        "workflow-execution",
        "history-retrieval",
        "api-error",
        "versioning",
        "compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users cannot view complete workflow history in the Temporal UI due to 501 errors on some history records.",
      "rootCause": "Potentially an API version mismatch or missing method implementation (GetWorkflowExecutionHistoryReverse) between UI and server components.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A portion of the workflow history records that have been executed are displayed as 501. Note that not all of them are.",
      "number": 6378,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:46.349Z"
    },
    {
      "summary": "CompleteUpdate message is sometimes not honored when submitted in the same workflow task completion as ContinueAsNew. The update is occasionally lost and applied to the post-ContinueAsNew run instead, causing the update caller to receive a server error and the update to be processed twice.",
      "category": "bug",
      "subcategory": "workflow-updates-continue-as-new",
      "apis": [
        "UpdateWorkflowExecution",
        "ContinueAsNew"
      ],
      "components": [
        "history-service",
        "frontend-service",
        "workflow-execution",
        "statemachine"
      ],
      "concepts": [
        "update-processing",
        "continue-as-new",
        "workflow-task-completion",
        "retry-logic",
        "race-condition",
        "execution-run"
      ],
      "severity": "high",
      "userImpact": "Users lose workflow updates or have them applied twice when updating and immediately using ContinueAsNew in the same task, causing inconsistent workflow state and broken update semantics.",
      "rootCause": "When completing an update in the same WFT as ContinueAsNew, an internal error can occur causing the history service to fail with 'unable to locate current workflow execution'. The frontend service retries the update request, which then lands on the new run created by ContinueAsNew and is processed a second time.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1459
      ],
      "keyQuote": "server error causes an internal retry (FE => History) of the UpdateWorkflowExecution, and this retry lands on the 2nd run, after CAN",
      "number": 6375,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:48.563Z"
    },
    {
      "summary": "Users need a way to validate which schedule initiated a workflow. The request is to add TemporalScheduledStartTime and TemporalScheduledById fields to the StartWorkflow event message, similar to existing cron schedule fields, to avoid reliance on potentially spoofable search attributes.",
      "category": "feature",
      "subcategory": "schedule-tracking",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "event-message",
        "schedule-engine",
        "workflow-event"
      ],
      "concepts": [
        "schedule-tracking",
        "workflow-identity",
        "event-metadata",
        "field-addition",
        "data-validation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably validate which schedule started a workflow without using searchable attributes that can be spoofed or contain problematic timestamps.",
      "rootCause": null,
      "proposedFix": "Add TemporalScheduledStartTime and TemporalScheduledById fields to the StartWorkflow event message",
      "workaround": "Use Search Attributes (though this has security and timestamp issues)",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "add TemporalScheduledStartTime and TemporalScheduledById to the start event message as opposed to just search attributes",
      "number": 6364,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:47.531Z"
    },
    {
      "summary": "Java HelloUpdate example hangs when using `temporal server start-dev` with UpdateWorkflowExecution enabled. The client hangs at the first update call and the server pegs CPU. Issue was resolved by updating to a newer CLI version.",
      "category": "bug",
      "subcategory": "update-workflow-execution",
      "apis": [
        "UpdateWorkflowExecution"
      ],
      "components": [
        "server",
        "client",
        "update-handler"
      ],
      "concepts": [
        "workflow-update",
        "dev-server",
        "cli-version",
        "cpu-usage",
        "deadlock"
      ],
      "severity": "high",
      "userImpact": "Users following the Java tutorial cannot run the HelloUpdate example with the recommended dev server setup, making it impossible to learn about workflow updates locally.",
      "rootCause": "Outdated CLI version (0.11.0) had a bug with UpdateWorkflowExecution in start-dev mode that was fixed in later versions.",
      "proposedFix": "Update temporal CLI to version 0.13.2 or later.",
      "workaround": "Use docker-compose to run Temporal Server instead of `temporal server start-dev`.",
      "resolution": "fixed",
      "resolutionDetails": "Updating to CLI version 0.13.2 resolved the hanging issue. The download link in the Java setup guide was pointing to an outdated version.",
      "related": [],
      "keyQuote": "The HelloUpdate example runs correctly when using the latest CLI version 0.13.2",
      "number": 6352,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:36.010Z"
    },
    {
      "summary": "Request to expose FirstRunID in WorkflowExecutionInfo via the DescribeWorkflowExecutionResponse API. This is needed to reliably identify workflows across continueAsNew and reset operations, which currently change the runId but not the firstRunId.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "DescribeWorkflowExecution",
        "WorkflowExecutionInfo"
      ],
      "components": [
        "api",
        "server",
        "workflow-execution"
      ],
      "concepts": [
        "workflow-identity",
        "continueAsNew",
        "reset",
        "idReusePolicy",
        "child-workflows",
        "run-stability"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably track workflows across continueAsNew or reset operations when using IdReusePolicy, hindering production workflows that depend on stable workflow identification.",
      "rootCause": "FirstRunID is available in SDK internals but not exposed through the public DescribeWorkflowExecution API, forcing workarounds for production systems.",
      "proposedFix": "Expose FirstRunID in WorkflowExecutionInfo within DescribeWorkflowExecutionResponse, requiring changes in both the API definition and server implementation.",
      "workaround": "CurrentRunID can be used but fails for workflows using continueAsNew or reset; alternative is to redesign the workflow system.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        404
      ],
      "keyQuote": "When workflowA is running, it may kickoff a workflowA1 as a child workflow and then waiting for external to signal A1...this is not sufficient when workflowA is using IdReusePolicy to reuse the workflowID.",
      "number": 6348,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:35.057Z"
    },
    {
      "summary": "Support updating activity options and triggering backed-off activities directly from a client without waiting for automatic retry. This is a capability Temporal is actively designing with details to be determined.",
      "category": "feature",
      "subcategory": "activity-management",
      "apis": [],
      "components": [
        "activity-executor",
        "client",
        "activity-retry"
      ],
      "concepts": [
        "activity-control",
        "backoff",
        "client-side-management",
        "retry-mechanism",
        "activity-options"
      ],
      "severity": "medium",
      "userImpact": "Users would gain the ability to manage activity behavior dynamically from the client side, enabling more flexible handling of backed-off activities.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Support updating activity options and/or triggering a backing-off activity from a client.",
      "number": 6339,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:32.517Z"
    },
    {
      "summary": "Temporal server logs duplicate prometheus metric registration warnings for metrics like AccessHistoryNew and task_schedule_to_start_latency, indicating conflicting label definitions for the same metric being registered multiple times.",
      "category": "bug",
      "subcategory": "prometheus-metrics",
      "apis": [],
      "components": [
        "prometheus-reporter",
        "metrics",
        "telemetry"
      ],
      "concepts": [
        "prometheus",
        "metrics-registration",
        "label-conflict",
        "metric-deduplication",
        "telemetry"
      ],
      "severity": "medium",
      "userImpact": "Users see warning logs during server operation that indicate potential metric collection issues, causing log noise and confusion about metric validity.",
      "rootCause": "Metrics are being registered multiple times with different label sets or help strings, likely due to metrics being registered from multiple sources (sdk-core and server) with incompatible configurations.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "a previously registered descriptor with the same fully-qualified name as Desc{fqName: \"AccessHistoryNew\"... has different label names or a different help string",
      "number": 6338,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:21.192Z"
    },
    {
      "summary": "Request to expose the start delay option in child workflow execution parameters. This feature would allow developers to configure delayed execution for child workflows, similar to existing capabilities in the features specification.",
      "category": "feature",
      "subcategory": "child-workflow-options",
      "apis": [
        "StartChildWorkflowExecution"
      ],
      "components": [
        "child-workflow",
        "workflow-options",
        "api"
      ],
      "concepts": [
        "start-delay",
        "child-workflow",
        "workflow-execution",
        "scheduling",
        "timing",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot currently configure start delay for child workflows, limiting scheduling flexibility for child workflow executions.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        515
      ],
      "keyQuote": "See https://github.com/temporalio/features/issues/515 for more information",
      "number": 6336,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:22.313Z"
    },
    {
      "summary": "Rate limit tokens are consumed when tasks are assigned to pollers even if the tasks are invalid (expired or workflow closed), wasting rate limit capacity. The system should only consume tokens when tasks are successfully delivered.",
      "category": "bug",
      "subcategory": "rate-limiting",
      "apis": [],
      "components": [
        "rate-limiter",
        "task-matcher",
        "task-delivery",
        "poller"
      ],
      "concepts": [
        "rate-limiting",
        "token-consumption",
        "task-validation",
        "resource-efficiency",
        "task-delivery",
        "rate-limit-reservation"
      ],
      "severity": "medium",
      "userImpact": "Users with rate-limited task queues experience reduced effective throughput due to tokens being wasted on invalid tasks that cannot be processed.",
      "rootCause": "Rate limit token is consumed at task assignment time (when matched to poller) rather than at successful task delivery time, so invalid tasks consume tokens without delivering value.",
      "proposedFix": "Return the rate limit token if task delivery fails after the token has been consumed, so the token can be used by another task. Alternatively, make the task validator more aggressive to filter invalid tasks before they are matched to pollers.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the token is consumed when task is attempted to be delivered. However, the task could be invalid (expired or workflow already closed), which lead to waste of rate limit token.",
      "number": 6333,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:22.642Z"
    },
    {
      "summary": "User requests the ability to attach a local debugger (JetBrains or VSCode) to a Temporal Worker running locally for step-by-step tracing, as the Web UI debugging is insufficient for troubleshooting.",
      "category": "feature",
      "subcategory": "debugging",
      "apis": [],
      "components": [
        "worker",
        "local-development",
        "debugging-tools"
      ],
      "concepts": [
        "debugging",
        "local-testing",
        "breakpoints",
        "step-through",
        "ide-integration",
        "trace"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot efficiently debug local Temporal Workers, limiting their ability to diagnose issues during development.",
      "rootCause": null,
      "proposedFix": "Expose a debug port on the Local Temporal Worker that allows standard debuggers (JetBrains IDEs, VSCode) to attach and trace execution.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I would like the ability to expose a port and attach a debugger (either in a JetBrains product or VSCode) that allows me to trace each step the Worker takes.",
      "number": 6329,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:08.621Z"
    },
    {
      "summary": "Unit tests fail in v1.24.2 due to missing mock method calls in the MutableState test suite. The TestTotalEntitiesCount test expects specific calls to MockMetadata.ClusterNameForFailoverVersion() and GetCurrentClusterName() that are not being made.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "mutable-state",
        "cluster-metadata",
        "test-controller"
      ],
      "concepts": [
        "unit-testing",
        "mock-verification",
        "test-failure",
        "method-calls",
        "cluster-failover"
      ],
      "severity": "high",
      "userImpact": "Developers cannot run unit tests on v1.24.2, blocking validation of changes and releases.",
      "rootCause": "Test expectations for mock calls to cluster metadata methods are not satisfied during test execution, indicating missing implementation or incorrect mock setup.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "missing call(s) to *cluster.MockMetadata.ClusterNameForFailoverVersion(is equal to false (bool), is equal to 1234 (int64))",
      "number": 6328,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:10.252Z"
    },
    {
      "summary": "Frontend service experiences unbounded growth of goroutines, heap objects, and memory allocations even under minimal load, causing CPU and memory increases that eventually lead to outages. The issue appears to be caused by improper gRPC ClientConn cleanup and possibly a stub-level caching issue.",
      "category": "bug",
      "subcategory": "goroutine-leak",
      "apis": [],
      "components": [
        "frontend-service",
        "grpc-client-conn",
        "memory-management"
      ],
      "concepts": [
        "goroutine-leak",
        "memory-leak",
        "garbage-collection",
        "connection-cleanup",
        "resource-management",
        "cpu-usage",
        "heap-allocation"
      ],
      "severity": "critical",
      "userImpact": "Production services experience uncontrolled memory and CPU growth requiring frequent restarts to maintain availability.",
      "rootCause": "Improper gRPC ClientConn cleanup; grpc.ClientConn objects are created frequently without being properly closed, and the stub-level cache may not be preventing duplicate connections as intended.",
      "proposedFix": "Cache gRPC connections to avoid creating them frequently on demand (PR #6441 mentioned as potential fix).",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        6441
      ],
      "keyQuote": "Looking at the hung Goroutines, it seems to be coming from improper `grpc.ClientConn` cleanup.",
      "number": 6323,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:11:11.173Z"
    },
    {
      "summary": "Multiple test flakes have been encountered in the Go SDK test suite after upgrading to server 1.25.0-rc.0, particularly related to versioning. These flakes need investigation and server-side fixes before the 1.25.0 release.",
      "category": "bug",
      "subcategory": "test-flakes",
      "apis": [],
      "components": [
        "test-suite",
        "server",
        "nexus"
      ],
      "concepts": [
        "versioning",
        "test-stability",
        "server-upgrade",
        "compatibility"
      ],
      "severity": "high",
      "userImpact": "Users testing against server 1.25.0-rc.0 may encounter flaky test behavior in their applications, blocking upgrade validation.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1555
      ],
      "keyQuote": "The cause needs to be investigated and server code fixed before we cut an official 1.25.0 release.",
      "number": 6320,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:55.082Z"
    },
    {
      "summary": "Schedule list with visibility query times out on server 1.25.0-rc.0, discovered when upgrading the server version in the Go SDK test suite.",
      "category": "bug",
      "subcategory": "schedule-visibility-query",
      "apis": [
        "ListSchedules"
      ],
      "components": [
        "schedule-service",
        "visibility-query",
        "test-suite"
      ],
      "concepts": [
        "timeout",
        "query-performance",
        "visibility-filtering",
        "schedule-listing",
        "backward-compatibility"
      ],
      "severity": "high",
      "userImpact": "Users upgrading to server 1.25.0-rc.0 experience timeouts when listing schedules with visibility queries.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "May no longer be relevant, requires reproduction.",
      "number": 6319,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:55.557Z"
    },
    {
      "summary": "History pods panic during upgrade from 1.20.4 to 1.21.6 when archival is disabled, due to compatibility check code trying to load archival queue category that was never registered when archival is disabled cluster-wide.",
      "category": "bug",
      "subcategory": "upgrade-compatibility",
      "apis": [],
      "components": [
        "history-shard",
        "shard-context",
        "queue-category",
        "compatibility-check"
      ],
      "concepts": [
        "version-upgrade",
        "archival",
        "queue-management",
        "persistence-layer",
        "backward-compatibility",
        "panic-recovery"
      ],
      "severity": "high",
      "userImpact": "Users attempting to upgrade from 1.20.4 to 1.21.6 with archival disabled experience service crashes, blocking the upgrade path.",
      "rootCause": "PR #4190 introduced compatibility check code that unconditionally tries to load archival queue category by ID, but the category is not registered when archival is disabled in static config, causing a panic in loadShardInfoCompatibilityCheckWithoutReplication.",
      "proposedFix": "Enable archival in static config during upgrade as a workaround, or fix the compatibility check code in 1.21.x to handle missing archival queue category gracefully.",
      "workaround": "Set history.archivalProcessorSchedulerWorkerCount to 0, enable archival in static config, upgrade to 1.21 then 1.22, then disable archival and restart.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4190
      ],
      "keyQuote": "unable to find queue category by queye category ID: 5",
      "number": 6307,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:58.884Z"
    },
    {
      "summary": "User inquires about running Temporal TypeScript SDK on Bun runtime and reports encountering a Rust crash. The issue was resolved after a Bun upgrade, but broader compatibility with Bun runtime remains under investigation.",
      "category": "question",
      "subcategory": "runtime-compatibility",
      "apis": [],
      "components": [
        "typescript-sdk",
        "bun-runtime"
      ],
      "concepts": [
        "runtime-compatibility",
        "bun",
        "javascript-runtime",
        "sdk-support"
      ],
      "severity": "low",
      "userImpact": "Users attempting to run Temporal TypeScript SDK on Bun runtime may encounter compatibility issues or crashes.",
      "rootCause": "Bun runtime compatibility issue (possibly in Bun itself, resolved in version 1.1.20)",
      "proposedFix": null,
      "workaround": "Upgrade to Bun 1.1.20 or later",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by Bun 1.1.20 release which fixed the Rust crash reported in the issue",
      "related": [],
      "keyQuote": "bun 1.1.20 was released just a few hours ago, and running bun upgrade now seems to have resolved this issue",
      "number": 6285,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:46.123Z"
    },
    {
      "summary": "User requests a way to detect when a workflow is in a failed state but still shows as \"Running\" in the UI. This occurs when workflows fail internally (e.g., due to missing exports or unhandled initialization errors) but don't transition to the actual Failed state, making error detection difficult.",
      "category": "feature",
      "subcategory": "workflow-state-management",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-runtime",
        "workflow-state-machine",
        "logging"
      ],
      "concepts": [
        "workflow-failure",
        "state-detection",
        "error-visibility",
        "workflow-initialization",
        "logging"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably detect when workflows have failed internally, leading to silent failures and operational blindness in production systems.",
      "rootCause": "Workflow runtime does not properly transition to Failed state for certain initialization errors; error information is not surfaced through standard logging or metadata.",
      "proposedFix": "Implement a mechanism to either: (1) properly transition workflows to Failed state when initialization errors occur, or (2) expose workflow failure information through logs/metadata/state queries.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If I do this [...] and then start the workflow it will show up as \"Running\" but it's actually in a sort of failed state.",
      "number": 6275,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:46.271Z"
    },
    {
      "summary": "PostgreSQL schema updates fail on busy Temporal instances when creating indexes due to query timeouts. The issue occurs when index creation blocks longer than PostgreSQL's configured deadlock detection limit, causing the schema migration to fail with 'bad connection' errors.",
      "category": "bug",
      "subcategory": "schema-migration",
      "apis": [],
      "components": [
        "schema-migration",
        "postgresql-driver",
        "index-management"
      ],
      "concepts": [
        "index-creation",
        "concurrent-operations",
        "database-locking",
        "query-timeout",
        "schema-versioning",
        "production-deployment"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy schema updates to production Temporal instances without experiencing migration failures during peak usage.",
      "rootCause": "CREATE INDEX operations block table access on large, actively-used tables, causing PostgreSQL to terminate the query when it exceeds the configured timeout limit.",
      "proposedFix": "Use CREATE INDEX CONCURRENTLY instead of CREATE INDEX in schema migration SQL files to allow concurrent access to the table during index creation.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "CREATE INDEX CONCURRENTLY by_root_workflow_id ON executions_visibility... would allow concurrent access during index creation.",
      "number": 6273,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:45.060Z"
    },
    {
      "summary": "Users need the ability to validate and authenticate encrypted payloads sent to the server. Currently, the server accepts all payloads without verification, but many users want to restrict payloads to only those they've created and signed.",
      "category": "feature",
      "subcategory": "payload-validation",
      "apis": [],
      "components": [
        "server",
        "payload-handler",
        "interceptor",
        "namespace-configuration"
      ],
      "concepts": [
        "encryption",
        "payload-validation",
        "authentication",
        "digital-signatures",
        "public-key-infrastructure",
        "security"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently validate that payloads were created by trusted sources, creating a security gap for applications requiring payload authenticity verification.",
      "rootCause": null,
      "proposedFix": "Implement server/namespace-level public key management (RSA or ECDSA) with CRUD API. Use VisitPayloads interceptor to validate that each non-search-attribute payload has keyId and signature metadata entries matching a configured public key.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Many users want to restrict payloads to only ones they create... use VisitPayloads via interceptor to ensure that every non-search-attribute payload has a keyId payload metadata entry",
      "number": 6259,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:34.274Z"
    },
    {
      "summary": "Request to optimize workflow history storage for successful workflows by retaining only the workflow ID and final state while removing full history. This would reduce storage requirements in Cassandra while preserving de-duplication and final-state retrieval capabilities.",
      "category": "feature",
      "subcategory": "history-storage",
      "apis": [],
      "components": [
        "cassandra-storage",
        "history-service",
        "workflow-archival"
      ],
      "concepts": [
        "history-retention",
        "storage-optimization",
        "de-duplication",
        "final-state",
        "workflow-completion"
      ],
      "severity": "medium",
      "userImpact": "Users with high-volume workflows would reduce storage costs by eliminating unnecessary history for successfully completed workflows.",
      "rootCause": null,
      "proposedFix": "Configure per-workflow definition or invocation to indicate full history is unnecessary for successful workflows. The workflow ID and final state remain available while larger parts like data payloads are optimized out of storage.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        487
      ],
      "keyQuote": "When enabled, the workflow ID and final state would remain available for de-duplication and final-state retrieval, but the rest of the history could be optimized out of storage.",
      "number": 6255,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:34.797Z"
    },
    {
      "summary": "Request to add support for Elastic serverless by allowing custom HTTP headers (specifically Authorization) in the Elasticsearch client configuration. Currently, the ES client config does not support custom headers needed to connect to Elastic's serverless offering.",
      "category": "feature",
      "subcategory": "elasticsearch-configuration",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "visibility-store",
        "persistence-config"
      ],
      "concepts": [
        "serverless",
        "elasticsearch",
        "authentication",
        "http-headers",
        "api-key",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently use Elastic's serverless option for visibility storage because the client lacks support for custom Authorization headers required by the serverless API.",
      "rootCause": null,
      "proposedFix": "Add a headers configuration field to the Elasticsearch client config to allow custom HTTP headers, or implement dedicated auth methods (basic auth or API key).",
      "workaround": "Set up a proxy that adds the required Authorization header before forwarding requests to Elasticsearch.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I'd like to be able to add a configuration like the following: headers: Authorization: \"ApiKey {{ .Env.TEMPORAL_ES_VISIBIILITY_STORE_PASSWORD }}\"",
      "number": 6253,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:33.461Z"
    },
    {
      "summary": "Feature request to add WorkerOptions configuration for workflow task max retry interval. Currently users can only change the workflow task timeout, but need ability to customize the retry interval (currently fixed at 10 minutes) via WorkerOptions in SDKs.",
      "category": "feature",
      "subcategory": "worker-configuration",
      "apis": [
        "WorkerOptions"
      ],
      "components": [
        "worker",
        "workflow-task-handler",
        "configuration"
      ],
      "concepts": [
        "retry",
        "timeout",
        "max-retry-interval",
        "worker-options",
        "configuration",
        "workflow-execution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot configure the workflow task max retry interval for their specific use cases, limiting their ability to tune worker behavior.",
      "rootCause": null,
      "proposedFix": "Add a new WorkerOptions configuration parameter to allow users to customize the workflow task max retry interval.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "In some use cases users want to also change the max retry interval for workflow tasks which is 10min.",
      "number": 6239,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:23.043Z"
    },
    {
      "summary": "Feature request to support wildcard patterns in namespace RPS (requests per second) rate limiting configuration. Currently, each namespace requires an individual configuration entry; the request proposes allowing patterns like 'stg-*' to apply rules to multiple namespaces matching that pattern.",
      "category": "feature",
      "subcategory": "rate-limiting",
      "apis": [],
      "components": [
        "frontend",
        "namespace-rps",
        "configuration"
      ],
      "concepts": [
        "rate-limiting",
        "namespace-management",
        "dynamic-configuration",
        "wildcard-patterns",
        "RPS-constraints"
      ],
      "severity": "medium",
      "userImpact": "Users managing multiple namespaces with similar rate-limiting rules must currently update the server configuration for each new namespace instead of using a single wildcard pattern.",
      "rootCause": null,
      "proposedFix": "Add wildcard pattern support (e.g., 'stg-*', 'prod-*') to namespace constraints in the frontend.namespacerps configuration, allowing rules to apply to all matching namespaces.",
      "workaround": "Update server configuration manually for each namespace that requires rate-limiting rules.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would be useful to be able to define wildcard rules that get applied to namespaces (e.g. `stg-*` means any namespaces matching this rule such as `stg-test` will have this rule applied to it).",
      "number": 6237,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:23.320Z"
    },
    {
      "summary": "Request for a metric to track workflows and activities that permanently fail after exhausting all retries. Users need visibility into actual final failures during service outages, not just intermediate retry attempts.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "workflow-execution",
        "activity-executor",
        "retry-mechanism"
      ],
      "concepts": [
        "failure-tracking",
        "retry-exhaustion",
        "service-outage",
        "monitoring",
        "observability",
        "permanent-failure",
        "impact-assessment"
      ],
      "severity": "medium",
      "userImpact": "Users cannot distinguish between transient failures that will be retried and final failures after all retries exhausted, making it difficult to assess service outage impact.",
      "rootCause": null,
      "proposedFix": "Add a new metric (e.g., `temporal_workflow_permanently_failed` or `temporal_activity_permanently_failed`) that only increments when all retries have been exhausted and the workflow/activity will not be re-attempted.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Quite reasonably, we want to \"assess impact\" of the service outage by knowing how many activities or workflows \"actually, permanently failed\"",
      "number": 6227,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:21.745Z"
    },
    {
      "summary": "The `-c` flag for specifying a config path in `temporal-server start` is not working as expected, returning a \"flag provided but not defined\" error. The correct usage requires `-c` as a global flag before `start` and `-e` for the filename.",
      "category": "docs",
      "subcategory": "cli-help",
      "apis": [],
      "components": [
        "temporal-server",
        "cli",
        "configuration"
      ],
      "concepts": [
        "config-path",
        "flag-parsing",
        "cli-usage",
        "global-flags",
        "documentation"
      ],
      "severity": "low",
      "userImpact": "Users are confused about the correct syntax for specifying config paths, leading to errors and poor developer experience.",
      "rootCause": "CLI help messages do not clearly indicate that `-c` is a global flag that must precede the `start` subcommand, and that `-e` is needed to specify the filename.",
      "proposedFix": "Update CLI help messages to clarify the correct flag usage and ordering.",
      "workaround": "Use `-c` as a global flag before `start` and specify the filename with `-e` (without the `.yaml` extension).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We should probably change the CLI help messages then, since this isn't clear at all otherwise.",
      "number": 6226,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:11.866Z"
    },
    {
      "summary": "Allow users to specify a configurable root path prefix for the HTTP API endpoints (/namespaces, /cluster, /system-info) instead of relying solely on proxy trimming.",
      "category": "feature",
      "subcategory": "http-api",
      "apis": [],
      "components": [
        "http-api",
        "server"
      ],
      "concepts": [
        "path-prefix",
        "routing",
        "configuration",
        "http-endpoints",
        "api-gateway"
      ],
      "severity": "low",
      "userImpact": "Users can configure a custom root path for HTTP API endpoints without requiring proxy-level path manipulation.",
      "rootCause": null,
      "proposedFix": "Add configuration option to specify a path prefix for HTTP API endpoints",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "allow users to specify a path prefix",
      "number": 6212,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:10.174Z"
    },
    {
      "summary": "Worker versioning test flakes intermittently when querying workflows on pre-1.24 servers, with RPC timeouts occurring sporadically. Suspected issue with UpdateWorkerBuildIdCompatibility not being strongly consistent, particularly on slower GitHub runners.",
      "category": "bug",
      "subcategory": "worker-versioning",
      "apis": [
        "UpdateWorkerBuildIdCompatibility"
      ],
      "components": [
        "worker",
        "versioning",
        "test-framework"
      ],
      "concepts": [
        "consistency",
        "timeout",
        "flakiness",
        "query",
        "build-id-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users may experience intermittent RPC timeout failures when querying workflows with versioning enabled, particularly in resource-constrained environments.",
      "rootCause": "UpdateWorkerBuildIdCompatibility may not be strongly consistent, causing timing-dependent failures in versioning queries",
      "proposedFix": null,
      "workaround": "Test is currently skipped to prevent flakiness",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Querying a workflow with versioning for some reason sometimes now times out (i.e. gets \"Timeout expired\" RPC failure).",
      "number": 6211,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:09.564Z"
    },
    {
      "summary": "Temporal server start-dev generates excessive log noise when restarting with SQLite persistence. Restarting the server after a graceful shutdown produces logging beyond the normal startup/shutdown messages.",
      "category": "bug",
      "subcategory": "persistence-sqlite",
      "apis": [],
      "components": [
        "cli",
        "server",
        "sqlite-persistence",
        "logging"
      ],
      "concepts": [
        "log-noise",
        "graceful-shutdown",
        "persistence",
        "restart",
        "sqlite"
      ],
      "severity": "medium",
      "userImpact": "Users experience cluttered logs when restarting the development server with SQLite, making debugging and development harder.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Doing a simple `temporal server start-dev --db-filename my-db.db`, interrupting it (does graceful shutdown), then running that command again gives lots of logs.",
      "number": 6210,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:56.602Z"
    },
    {
      "summary": "User downloaded Temporal CLI from the documented link but received a tarball instead of an executable binary, causing 'Exec format error'. The download URL provides a tar.gz file but the documentation doesn't clarify this, leading to confusion.",
      "category": "docs",
      "subcategory": "installation-guide",
      "apis": [],
      "components": [
        "cli-distribution",
        "download-facade",
        "installation-docs"
      ],
      "concepts": [
        "binary-format",
        "installation",
        "download-link",
        "documentation-clarity",
        "architecture-compatibility"
      ],
      "severity": "low",
      "userImpact": "Users following the official documentation may download an incorrect file format and encounter installation errors, requiring troubleshooting to resolve.",
      "rootCause": "The download link is a facade to GitHub releases that serves a tarball, but the documentation doesn't indicate this requires extraction before execution.",
      "proposedFix": "Update the documentation to clarify that the download link provides a tar.gz file that must be extracted, or provide direct links to pre-built binaries.",
      "workaround": "Use the direct GitHub releases URL (https://github.com/temporalio/cli/releases) instead of the facade download link.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed by maintainer with explanation that the download is intentionally a facade to GitHub releases. Users should use direct GitHub links for stable URLs rather than relying on the facade.",
      "related": [],
      "keyQuote": "While it shouldn't, it's just a facade to the GitHub releases area. If you want a stable link, use a link from GitHub directly",
      "number": 6209,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:10:00.303Z"
    },
    {
      "summary": "User reports extremely slow loading (~1 minute) of the workflow-count API endpoint in the Temporal Web UI when using MySQL database backend. Responder suggests troubleshooting by checking if the issue is with the web server or temporal server itself.",
      "category": "question",
      "subcategory": "web-ui-performance",
      "apis": [],
      "components": [
        "web-ui",
        "api-server",
        "database-query"
      ],
      "concepts": [
        "performance",
        "latency",
        "web-loading",
        "api-response-time",
        "mysql-database"
      ],
      "severity": "medium",
      "userImpact": "Users experience significant delays (1+ minute) when loading workflow statistics in the web interface, impacting operational visibility and usability.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I'm seeing a very slow loading of the resource http://10.0.0.57:32080/api/v1/namespaces/Answer-Flow/workflow-count?query=GROUP+BY+ExecutionStatus, which takes nearly 1 minute.",
      "number": 6206,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:57.879Z"
    },
    {
      "summary": "Request for schema validation checks at server startup to detect database schema mismatches and exit early with errors, plus telemetry logging for Visibility Store schema mismatches.",
      "category": "feature",
      "subcategory": "schema-validation",
      "apis": [],
      "components": [
        "persistence-store",
        "visibility-store",
        "database-connection",
        "server-startup"
      ],
      "concepts": [
        "schema-validation",
        "database-compatibility",
        "error-handling",
        "telemetry",
        "startup-checks"
      ],
      "severity": "medium",
      "userImpact": "Prevents server from running with incompatible database schemas, reducing silent failures and data corruption risks.",
      "rootCause": null,
      "proposedFix": "Add schema version check on initial database connection at server startup; implement error exit if schema mismatch detected.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Already implemented - server checks schema version at startup against minimal version requirement and exits with error if mismatch is detected.",
      "related": [],
      "keyQuote": "Upon server startup, it checks the schema version from database and match with what the minimal version requirement from the code. If the don't match, server exist with error message.",
      "number": 6202,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:45.691Z"
    },
    {
      "summary": "Add a tdbg diagnose command to help debug stuck workflows by dumping relevant state information (mutable state, history, shard info, task queue info, namespace info) and performing basic analysis to identify scheduling issues, missed timers, and worker polling problems.",
      "category": "feature",
      "subcategory": "debugging-tools",
      "apis": [],
      "components": [
        "tdbg",
        "workflow-executor",
        "task-queue",
        "shard",
        "namespace"
      ],
      "concepts": [
        "workflow-diagnosis",
        "debugging",
        "state-inspection",
        "task-scheduling",
        "timer-execution",
        "worker-polling"
      ],
      "severity": "medium",
      "userImpact": "Users will have better tooling to diagnose and understand why workflows are stuck, reducing debugging time and improving operational observability.",
      "rootCause": null,
      "proposedFix": "Implement tdbg workflow diagnose command that collects mutable state, workflow history, shard info, task queue info, and namespace info, then performs checks for scheduling issues, missed timers, and worker polling.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "tdbg command that could help dump all relevant information and do some basic analysis",
      "number": 6201,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:46.866Z"
    },
    {
      "summary": "Search attributes registered immediately after server startup with cache refresh enabled are sometimes not available for workflows, causing failures with namespace mapping errors.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [
        "Server.Start",
        "OperatorService.AddSearchAttributes"
      ],
      "components": [
        "frontend-service",
        "search-attributes-cache",
        "namespace-mapping",
        "visibility-service"
      ],
      "concepts": [
        "cache-refresh",
        "dynamic-configuration",
        "namespace-initialization",
        "search-attribute-registration",
        "race-condition",
        "timing-issue"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably register search attributes immediately after server startup and start workflows, experiencing intermittent failures.",
      "rootCause": "Likely race condition between cache refresh initialization and search attribute registration, or improper cache invalidation after attribute addition.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Sometimes the workflow fails with \"Namespace default has no mapping defined for search attribute <whatever>\"",
      "number": 6195,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:45.561Z"
    },
    {
      "summary": "DescribeWorkflowExecution in the UI frontend does not handle archived workflows, causing 404 errors when trying to view archived workflow details. GetWorkflowExecutionHistory handles archived workflows correctly, but the describe endpoint needs the same archival logic for the archived workflow page to function.",
      "category": "bug",
      "subcategory": "workflow-archival",
      "apis": [
        "DescribeWorkflowExecution",
        "GetWorkflowExecutionHistory"
      ],
      "components": [
        "ui-frontend",
        "workflow-handler",
        "archival-provider"
      ],
      "concepts": [
        "workflow-archival",
        "s3-archival",
        "workflow-retention",
        "ui-navigation",
        "archived-workflows"
      ],
      "severity": "high",
      "userImpact": "Users cannot view archived workflow execution details from the UI's archived page, making it impossible to review historical workflow data after retention expiration.",
      "rootCause": "DescribeWorkflowExecution endpoint in frontend workflow_handler does not implement special logic for archived workflows, unlike GetWorkflowExecutionHistory which handles them correctly.",
      "proposedFix": "Add archived workflow handling logic to DescribeWorkflowExecution endpoint similar to what exists in GetWorkflowExecutionHistory.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1174,
        1702,
        5117
      ],
      "keyQuote": "DescribeWorkflowExecution in frontend workflow_handler does not take into account that a workflow can be archived. GetWorkflowExecutionHistory on the other hand, handles archived workflows perfectly.",
      "number": 6193,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:34.707Z"
    },
    {
      "summary": "Request to support externally-provided unique tokens for async activities instead of Temporal-generated task tokens. This would enable better integration with external systems (like GitHub webhooks) that provide their own unique IDs, eliminating the need for manual ID correlation via side databases.",
      "category": "feature",
      "subcategory": "async-activities",
      "apis": [
        "ExecuteAsyncActivity"
      ],
      "components": [
        "activity-executor",
        "async-completion",
        "task-routing"
      ],
      "concepts": [
        "async-completion",
        "external-id-correlation",
        "webhook-integration",
        "task-token",
        "idempotency",
        "external-system-integration"
      ],
      "severity": "medium",
      "userImpact": "Users integrating with external systems that provide unique IDs must maintain separate ID correlation databases instead of using Temporal's native async activity mechanism.",
      "rootCause": null,
      "proposedFix": "Provide an async activity API where users supply their own unique ID instead of receiving a task token from Temporal, with Temporal responsible for validating uniqueness and throwing errors on conflicts.",
      "workaround": "Manually maintain a side database table to correlate Temporal task tokens with external system IDs (e.g., GitHub workflow_run_id).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Provide the same async activity mechanism, but with an API where I hand temporal a unique ID instead of getting a unique Id from temporal.",
      "number": 6192,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:34.877Z"
    },
    {
      "summary": "CreateSchedule API accepts empty workflowID when it should fail validation, causing duplicate workflow ID conflicts when called multiple times within a second interval.",
      "category": "bug",
      "subcategory": "schedule-validation",
      "apis": [
        "CreateSchedule"
      ],
      "components": [
        "schedule-service",
        "workflow-id-validation",
        "schedule-creation"
      ],
      "concepts": [
        "validation",
        "input-constraints",
        "workflow-id",
        "duplicate-detection",
        "race-condition"
      ],
      "severity": "high",
      "userImpact": "Users can create schedules with invalid empty workflowID, leading to unpredictable behavior including workflow ID conflicts and inconsistent execution.",
      "rootCause": "Missing validation for empty workflowID in CreateSchedule API",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This cause some of them to succeed, but others to fail with workflow ID conflict if you call it multiple times in 1s.",
      "number": 6188,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:34.733Z"
    },
    {
      "summary": "Schedule's StartAt parameter is not being used correctly when calculating intervals. When creating a schedule with a 365-day interval and specifying a StartAt date, the first run occurs much sooner than expected (within 6 months) instead of at the specified start time.",
      "category": "bug",
      "subcategory": "schedule-intervals",
      "apis": [],
      "components": [
        "schedule-engine",
        "interval-calculation"
      ],
      "concepts": [
        "schedule-calculation",
        "interval-timing",
        "startAt-parameter",
        "temporal-scheduling",
        "date-calculation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably control when scheduled workflows first run when specifying both a long interval and a StartAt time, making it impossible to schedule workflows to begin at a specific future date.",
      "rootCause": "The StartAt parameter appears to be ignored or not properly integrated into the interval calculation logic, resulting in incorrect first-run times.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Start this workflow every 365 days with start at 2025-02-03 04:05:06, then the upcoming runs would look like: 2025-02-03 04:05:06, 2026-02-03 04:05:06, 2027-02-03 04:05:06",
      "number": 6173,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:21.720Z"
    },
    {
      "summary": "Feature request to automatically load all task queue partitions when the root partition is loaded, which helps prevent dispatch delays in low-traffic scenarios with long worker poll timeouts.",
      "category": "feature",
      "subcategory": "task-queue-partitioning",
      "apis": [],
      "components": [
        "task-queue",
        "worker",
        "dispatcher"
      ],
      "concepts": [
        "partitioning",
        "task-dispatch",
        "poll-timeout",
        "low-traffic",
        "partition-loading"
      ],
      "severity": "medium",
      "userImpact": "Users experience reduced task dispatch delays in low-traffic situations by having all task queue partitions preloaded.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented via issue #6324 which added the feature to load all task queue partitions when root partition is loaded.",
      "related": [
        6324
      ],
      "keyQuote": "Load all task queue partitions when root one is loaded. This helps prevent possible delays in task dispatch in low traffic situations",
      "number": 6160,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:22.567Z"
    },
    {
      "summary": "ARM64 Linux builds of Temporal server components are packaged with amd64 binaries instead of arm64, causing \"Exec format error\" failures when deployed on ARM64 infrastructure like M1 Macs running K3s. Multiple server and admin-tool binaries are affected.",
      "category": "bug",
      "subcategory": "build-architecture",
      "apis": [],
      "components": [
        "docker-builds",
        "temporal-server",
        "admin-tools",
        "temporal-cli"
      ],
      "concepts": [
        "architecture-mismatch",
        "binary-compatibility",
        "docker-images",
        "arm64-support",
        "multi-architecture-builds"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy Temporal on ARM64 Linux systems as server components fail to execute with binary format errors.",
      "rootCause": "Docker images for ARM64 contain amd64 binaries instead of properly compiled arm64 binaries, detected via file command showing 'x86-64' architecture.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        213
      ],
      "keyQuote": "temporal-sql-tool: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked... It is an amd64 binary?",
      "number": 6146,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:23.356Z"
    },
    {
      "summary": "ActivityStateReplicator drops sync activity replication tasks when the target cluster's workflow mutable state is not found. The component should trigger a history resend via RetryReplication error instead of returning nil, allowing the history resender to handle workflows that no longer exist in the source cluster.",
      "category": "bug",
      "subcategory": "replication",
      "apis": [],
      "components": [
        "ActivityStateReplicator",
        "history-replicator",
        "ndc"
      ],
      "concepts": [
        "replication",
        "mutable-state",
        "history-resend",
        "cross-cluster",
        "error-handling"
      ],
      "severity": "high",
      "userImpact": "Activity replication tasks are silently dropped during cross-cluster replication when the target workflow is missing, potentially causing data loss and inconsistency between clusters.",
      "rootCause": "ActivityStateReplicator returns nil instead of a RetryReplication error when mutable state is not found, causing replication tasks to be dropped rather than triggering history resend.",
      "proposedFix": "Return a RetryReplication error instead of nil to trigger history resend when workflow is not found in target cluster.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Return a RetryReplication error instead off nil to trigger history resend. History resender can handler the case where the workflow no longer existing in the source cluster.",
      "number": 6144,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:08.540Z"
    },
    {
      "summary": "Request to expand CI testing to cover additional supported platforms beyond Linux x64, including Linux ARM, macOS x64/ARM, and Windows x64.",
      "category": "feature",
      "subcategory": "ci-testing",
      "apis": [],
      "components": [
        "ci",
        "test-suite",
        "platform-support"
      ],
      "concepts": [
        "cross-platform",
        "continuous-integration",
        "test-coverage",
        "platform-compatibility",
        "automation"
      ],
      "severity": "medium",
      "userImpact": "Users on unsupported platforms have limited assurance that the SDK works correctly on their systems.",
      "rootCause": null,
      "proposedFix": "Extend CI pipeline to run test suite on Linux ARM, macOS x64, macOS ARM, and Windows x64 platforms",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Would like test suite to run in more supported platforms than just the one Linux x64.",
      "number": 6104,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:08.795Z"
    },
    {
      "summary": "Feature request for multi-AZ PostgreSQL with read-only replication support. Currently Temporal assumes static read-write PostgreSQL configurations and fails during HA failover. Proposal is to support dynamic routing where write operations go to read-write (master) nodes and detect read-only instances using pg_is_in_recovery().",
      "category": "feature",
      "subcategory": "database-ha",
      "apis": [],
      "components": [
        "database-connection",
        "postgres-driver",
        "persistence-layer"
      ],
      "concepts": [
        "high-availability",
        "failover",
        "read-only-replica",
        "connection-routing",
        "database-recovery",
        "master-replica"
      ],
      "severity": "medium",
      "userImpact": "Users with multi-AZ PostgreSQL HA setups experience Temporal server failures during instance recovery or failover events.",
      "rootCause": "Temporal server treats all database connections as read-write and cannot distinguish between or route around read-only replicas; no pg_is_in_recovery() check is performed.",
      "proposedFix": "Implement pg_is_in_recovery() checks on connection initialization to detect read-only instances, drop those connections, and route all queries to available read-write (master) instances.",
      "workaround": "Implement pg_is_in_recovery() check when initiating connections and drop read-only connections; however this has limitations when PG enters recovery after connection establishment or no read-write hosts remain available.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the client could distinguish at the minimum between writable database hosts at runtime and reroute all queries to that database",
      "number": 6100,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:09:11.230Z"
    },
    {
      "summary": "UpdateWorkflowExecution should succeed against completed workflows to avoid race conditions when clients retry requests. Currently, it returns WorkflowNotFound error, but clients need to be able to check update status after workflow completion within the retention period.",
      "category": "bug",
      "subcategory": "workflow-update",
      "apis": [
        "UpdateWorkflowExecution"
      ],
      "components": [
        "workflow-service",
        "update-handler",
        "workflow-completion"
      ],
      "concepts": [
        "idempotency",
        "race-condition",
        "update-status",
        "workflow-retention",
        "durable-state"
      ],
      "severity": "high",
      "userImpact": "Clients cannot safely retry UpdateWorkflowExecution requests after workflow completion, creating unavoidable race conditions in update submission logic.",
      "rootCause": "UpdateWorkflowExecution returns WorkflowNotFound error when workflow is completed, instead of allowing clients to query existing update status within retention period.",
      "proposedFix": "Allow UpdateWorkflowExecution requests to succeed against completed workflows when the workflow is still within the retention period, returning the update state/outcome.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by implementing support for UpdateWorkflowExecution against completed workflows within the retention period.",
      "related": [],
      "keyQuote": "Clients are encouraged to make use of this behavior by submitting the request repeatedly until the response indicates that the update has reached a durable stage.",
      "number": 6085,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:58.460Z"
    },
    {
      "summary": "Release notes for version 1.24.0 failed to mention a required schema upgrade, which should have been documented according to the upgrade guidelines.",
      "category": "docs",
      "subcategory": "release-notes",
      "apis": [],
      "components": [
        "release-notes",
        "documentation",
        "schema-migration"
      ],
      "concepts": [
        "schema-upgrade",
        "documentation",
        "release-process",
        "deployment",
        "version-upgrade"
      ],
      "severity": "medium",
      "userImpact": "Users upgrading to 1.24.0 were not informed of the required schema upgrade, potentially causing confusion during deployment.",
      "rootCause": "Release notes documentation was incomplete and did not include schema upgrade information as required by the upgrade process guidelines.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Release notes were updated to include the schema upgrade requirement.",
      "related": [],
      "keyQuote": "The release notes have been updated now.",
      "number": 6059,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:58.503Z"
    },
    {
      "summary": "PostgreSQL driver support changed in 1.24.0: the `postgres` plugin is no longer supported for the default schema. Users must migrate to `postgres12` or `postgres12_pgx` drivers for all schemas.",
      "category": "bug",
      "subcategory": "database-schema",
      "apis": [],
      "components": [
        "database-plugin",
        "schema-validation",
        "server-initialization"
      ],
      "concepts": [
        "database-compatibility",
        "driver-migration",
        "schema-configuration",
        "postgresql",
        "breaking-change"
      ],
      "severity": "high",
      "userImpact": "Users with existing `postgres` driver configurations for the default schema cannot upgrade to 1.24.0 without switching to `postgres12` or `postgres12_pgx`.",
      "rootCause": "SQL schema version compatibility check in server initialization no longer recognizes `postgres` as a valid plugin, only `mysql8`, `postgres12`, `postgres12_pgx`, and `sqlite`.",
      "proposedFix": "Switch default schema driver configuration from `postgres` to `postgres12` or `postgres12_pgx`.",
      "workaround": "Use `postgres12` driver for the default schema instead of `postgres`.",
      "resolution": "fixed",
      "resolutionDetails": "Issue acknowledged as expected behavior; release notes were updated to document the breaking change requiring migration from `postgres` to `postgres12` driver.",
      "related": [],
      "keyQuote": "sql schema version compatibility check failed: not supported plugin postgres, only supported: map[mysql8:0x42f15c0 postgres12:0xc000721380 postgres12_pgx:0xc000721390 sqlite:0x42aef78]",
      "number": 6053,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:58.042Z"
    },
    {
      "summary": "Docker image temporalio/admin-tools:1.24.0 was not published to Docker Hub, causing pull failures. The release process was changed to include component versions in the tag.",
      "category": "bug",
      "subcategory": "docker-image-release",
      "apis": [],
      "components": [
        "admin-tools",
        "docker-image",
        "tctl",
        "cli"
      ],
      "concepts": [
        "docker",
        "release",
        "versioning",
        "image-tag",
        "distribution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot pull the admin-tools docker image with the expected version tag, requiring them to use component-specific version tags instead.",
      "rootCause": "Release process was separated to track individual component versions (tctl and temporal CLI) in the docker image tag.",
      "proposedFix": null,
      "workaround": "Use the full tag format that includes component versions, e.g., 1.24.0-tctl-1.18.1-cli-0.12.0",
      "resolution": "fixed",
      "resolutionDetails": "Clarified that the image versioning scheme changed to include component versions. The correct tag for 1.24.0 is 1.24.0-tctl-1.18.1-cli-0.12.0.",
      "related": [],
      "keyQuote": "Version tag now includes versions of tctl (deprecated but still supported CLI) and temporal (modern CLI) binaries.",
      "number": 6052,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:46.657Z"
    },
    {
      "summary": "Upgrading Temporal from 1.23.1 to 1.24.0 causes ListWorkflowExecutions to fail with SQL scan errors when encountering NULL values in the new root_workflow_id column. The schema migration introduced this column but the application code wasn't prepared to handle NULL values, causing crashes until enough non-NULL entries populate the database.",
      "category": "bug",
      "subcategory": "schema-migration",
      "apis": [
        "ListWorkflowExecutions"
      ],
      "components": [
        "visibility-manager",
        "persistence-layer",
        "mysql-driver",
        "frontend-service"
      ],
      "concepts": [
        "schema-upgrade",
        "null-handling",
        "database-migration",
        "backwards-compatibility",
        "version-compatibility"
      ],
      "severity": "high",
      "userImpact": "Users upgrading to version 1.24.0 experience application crashes when querying workflows, requiring manual workarounds or rollbacks until patch is released.",
      "rootCause": "Schema migration added root_workflow_id column with NULL defaults, but visibility query code attempts to scan into string field without handling NULL values.",
      "proposedFix": "Update the SQL scanning logic to properly handle NULL values in the root_workflow_id column, or ensure schema migration sets appropriate default values.",
      "workaround": "Clean up old entries in namespaces, wait for them to expire, or avoid filtering that brings NULL entries to first page of results.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in v1.24.1 release with schema upgrade.",
      "related": [],
      "keyQuote": "Latest schema change introduced a new column `root_workflow_id` and defaulted it to null. However, the code is not ready to handle the null values.",
      "number": 6050,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:46.949Z"
    },
    {
      "summary": "Feature request to support prefix-based matching for HTTP headers that should be forwarded, allowing operators to add new headers with a common prefix without requiring server redeployment.",
      "category": "feature",
      "subcategory": "http-configuration",
      "apis": [],
      "components": [
        "http-server",
        "config",
        "header-forwarding"
      ],
      "concepts": [
        "configuration",
        "http-headers",
        "prefix-matching",
        "operational-flexibility",
        "deployment"
      ],
      "severity": "low",
      "userImpact": "Allows operators to dynamically forward HTTP headers with a common prefix without requiring server redeployment.",
      "rootCause": null,
      "proposedFix": "Add support for prefix matching in HTTPAdditionalForwardHeaders, either through a new HTTPAdditionalForwardedHeaderPrefixes field or by treating values with trailing asterisks as prefix patterns.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to support prefix-based HTTP header forwarding.",
      "related": [],
      "keyQuote": "It's helpful to be able to add new headers using a common prefix and not need to do a temporal server deployment",
      "number": 6024,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:47.384Z"
    },
    {
      "summary": "User asks how to prevent activity starvation when multiple activities are part of the same workflow and a batch of tasks is initiated. The concern is about prioritization and execution ordering to ensure fair resource allocation.",
      "category": "question",
      "subcategory": "activity-scheduling",
      "apis": [],
      "components": [
        "activity-executor",
        "worker",
        "workflow-task-dispatcher"
      ],
      "concepts": [
        "starvation",
        "prioritization",
        "task-scheduling",
        "batch-execution",
        "resource-allocation",
        "fairness"
      ],
      "severity": "medium",
      "userImpact": "Users cannot guarantee fair execution of multiple activities within the same workflow when processing batches, risking some activities being starved of resources.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Use local activities for some tasks instead of regular activities",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "this isn't really possible right now. We're starting to work on related issues so we might have a better answer in a while.",
      "number": 6000,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:32.321Z"
    },
    {
      "summary": "Cassandra clusters using LDAP authentication fail to connect because gocql doesn't recognize the LDAPAuthenticator type. Users requested support for configuring custom authenticators via Helm to allow LDAP and other non-standard Cassandra authentication types.",
      "category": "feature",
      "subcategory": "cassandra-authentication",
      "apis": [],
      "components": [
        "cassandra-client",
        "gocql-integration",
        "helm-configuration",
        "authentication"
      ],
      "concepts": [
        "cassandra",
        "ldap-authentication",
        "authenticator-configuration",
        "connection-setup",
        "environment-variables"
      ],
      "severity": "high",
      "userImpact": "Users with LDAP-authenticated Cassandra clusters cannot connect Temporal to their database without this feature.",
      "rootCause": "gocql library has a hardcoded list of supported authenticators and rejects unknown types like LDAPAuthenticator; AllowedAuthenticators configuration was not exposed in Temporal's Cassandra setup.",
      "proposedFix": "Add CASSANDRA_ALLOWED_AUTHENTICATORS environment variable and configuration option to pass custom authenticator types to gocql's PasswordAuthenticator.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented via PR #6194 by adding CASSANDRA_ALLOWED_AUTHENTICATORS environment variable that accepts a list of custom authenticators.",
      "related": [
        6194
      ],
      "keyQuote": "export CASSANDRA_ALLOWED_AUTHENTICATORS=\"org.apache.cassandra.auth.LDAPAuthenticator\"",
      "number": 5989,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:34.056Z"
    },
    {
      "summary": "The dns:/// prefix cannot be used for clusterInformation.rpcAddress configuration because CreateRemoteFrontendGRPCConnection uses net.SplitHostPort which cannot parse DNS-prefixed addresses, causing a 'too many colons' error.",
      "category": "bug",
      "subcategory": "cluster-configuration",
      "apis": [],
      "components": [
        "RPCFactory",
        "cluster-metadata",
        "gRPC-connection"
      ],
      "concepts": [
        "dns-resolver",
        "cluster-replication",
        "address-parsing",
        "gRPC-client",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot configure multi-cluster replication with dns:// prefix for remote cluster addresses, preventing DNS round-robin load balancing.",
      "rootCause": "net.SplitHostPort in CreateRemoteFrontendGRPCConnection cannot parse addresses with resolver prefixes like dns:///",
      "proposedFix": "Parse the address using url.Parse to handle resolver prefixes, extracting hostname from either the Host or Path field depending on whether a resolver prefix is present",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Merged in PR #6430 to use dns resolver by default in newClient",
      "related": [
        6430
      ],
      "keyQuote": "Use dns:/// prefix to enable round-robin between IP address for DNS name... cannot handle the dns:/// prefix.",
      "number": 5979,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:35.513Z"
    },
    {
      "summary": "The gRPC API incorrectly requires Namespace to be set on DescribeHistoryHostRequest, but the tdbg CLI tool doesn't provide this field, causing the request to fail. The issue appears to be in the frontend namespace validator interceptor which should not enforce this requirement for history host describe operations.",
      "category": "bug",
      "subcategory": "rpc-interceptor",
      "apis": [
        "DescribeHistoryHostRequest"
      ],
      "components": [
        "namespace-validator",
        "rpc-interceptor",
        "frontend"
      ],
      "concepts": [
        "validation",
        "namespace",
        "interceptor",
        "history-host",
        "rpc-error"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use the tdbg history-host describe command to inspect history host details due to incorrect namespace validation.",
      "rootCause": "Frontend namespace validator interceptor incorrectly enforces Namespace requirement for DescribeHistoryHostRequest, which should not require this field.",
      "proposedFix": "Update the namespace validator interceptor to exclude DescribeHistoryHostRequest from namespace validation requirements.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The namespace validator was updated to allow DescribeHistoryHostRequest without requiring Namespace to be set.",
      "related": [],
      "keyQuote": "rpc error: code = InvalidArgument desc = Namespace not set on request.",
      "number": 5933,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:22.767Z"
    },
    {
      "summary": "Activity timeout failure messages and timeoutFailureInfo are mismatched when activity retry exceeds the schedule-to-close timeout. The timeoutFailureInfo is updated but the failure message is not kept in sync, leading to inconsistent error information.",
      "category": "bug",
      "subcategory": "activity-timeout",
      "apis": [],
      "components": [
        "timer-queue",
        "activity-executor",
        "failure-info"
      ],
      "concepts": [
        "timeout",
        "retry",
        "activity-lifecycle",
        "schedule-to-close",
        "failure-message",
        "consistency"
      ],
      "severity": "medium",
      "userImpact": "Users receive inconsistent failure information when activities timeout after retries, making debugging difficult.",
      "rootCause": "The timeoutFailureInfo is updated during activity retry timeout handling (timer_queue_active_task_executor.go:307) but the failure message is not updated to match, causing a mismatch between the message and the structured timeout info.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The mismatch between failure message and timeoutFailureInfo was fixed by ensuring both are updated consistently.",
      "related": [],
      "keyQuote": "The timeoutFailureInfo got updated but not the failure message.",
      "number": 5914,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:21.503Z"
    },
    {
      "summary": "Temporal Server 1.23.0 emits multiple \"Unspecified task queue kind\" warnings when running workflows via the CLI development server, despite normal operation. The warnings appear to be benign but create unnecessary noise in the terminal output.",
      "category": "bug",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "frontend",
        "task-queue",
        "logging"
      ],
      "concepts": [
        "task-queue-kind",
        "warning-messages",
        "development-server",
        "logging-noise",
        "cli-output"
      ],
      "severity": "low",
      "userImpact": "Users see warning messages in terminal output that may cause concern despite the server functioning normally.",
      "rootCause": "Server code not properly specifying task queue kind in certain scenarios, leading to unspecified kind warnings.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Already fixed in server code via PR #5661, to be included in version 1.23.2 or 1.24.0.",
      "related": [
        5897,
        5661
      ],
      "keyQuote": "Unspecified task queue kind warnings appear despite normal operation of the development server.",
      "number": 5903,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:21.978Z"
    },
    {
      "summary": "Prometheus reporter warnings appear when starting Temporal development server via CLI, showing \"error in prometheus reporter\" messages about previously registered descriptors with conflicting label names or help strings.",
      "category": "bug",
      "subcategory": "metrics-prometheus",
      "apis": [],
      "components": [
        "prometheus-reporter",
        "metrics",
        "server-startup"
      ],
      "concepts": [
        "prometheus",
        "metrics",
        "descriptor-registration",
        "label-naming",
        "metric-conflict"
      ],
      "severity": "low",
      "userImpact": "Users see warning messages in their terminal when starting the development server, which may cause confusion despite not affecting core functionality.",
      "rootCause": "A metric descriptor with the same fully-qualified name (AccessHistoryOld) is being registered multiple times with different label names or help strings, violating Prometheus descriptor uniqueness constraints.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved, likely through correcting the duplicate metric registration or standardizing label definitions.",
      "related": [],
      "keyQuote": "a previously registered descriptor with the same fully-qualified name as Desc{fqName: \"AccessHistoryOld\"...} has different label names or a different help string",
      "number": 5897,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:09.910Z"
    },
    {
      "summary": "Support continue-as-new with a different WorkflowID to address hot shard issues when workflows frequently call continue-as-new for processing large datasets.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "workflow-engine",
        "sharding",
        "visibility-api"
      ],
      "concepts": [
        "hot-shards",
        "workflow-chaining",
        "continue-as-new",
        "dataset-processing",
        "scalability"
      ],
      "severity": "high",
      "userImpact": "Workflows processing large datasets via continue-as-new can cause performance issues on hot shards, limiting scalability.",
      "rootCause": "Workflows using continue-as-new maintain the same WorkflowID throughout the chain, concentrating execution on specific shards.",
      "proposedFix": "Add an option to continue-as-new that allows changing the WorkflowID while maintaining workflow chain navigation through event IDs and visibility queries.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Many workflows rely on continue-as-new to process large datasets...The proposal is to support an option to continue-as-new to change WorkflowID.",
      "number": 5881,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:11.107Z"
    },
    {
      "summary": "Issue posted in wrong location. Limited context available about the actual problem with TEMPORAL_UI_PUBLIC_PATH environment variable.",
      "category": "other",
      "subcategory": "ui-configuration",
      "apis": [],
      "components": [
        "ui",
        "environment-config"
      ],
      "concepts": [
        "environment-variables",
        "public-path",
        "ui-setup",
        "configuration"
      ],
      "severity": "low",
      "userImpact": "Users may have difficulty configuring the Temporal UI public path via environment variables.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was closed as it was posted in the wrong location without sufficient problem description",
      "related": [],
      "keyQuote": "posted in the wrong spot",
      "number": 5879,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:08:10.720Z"
    },
    {
      "summary": "Handle activity failure when CompleteByID request is received during retry backoff. Need to allow force-failing retryable errors and fail non-retryable errors immediately, while avoiding wasted attempts.",
      "category": "feature",
      "subcategory": "activity-completion",
      "apis": [
        "RespondActivityTaskFailedById",
        "RespondActivityTaskCanceledById",
        "CompleteByID"
      ],
      "components": [
        "activity-executor",
        "retry-policy",
        "backoff-handler",
        "activity-resolver"
      ],
      "concepts": [
        "retry",
        "backoff",
        "force-fail",
        "activity-completion",
        "non-retryable-error",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Users need better control over activity failure resolution when requests arrive during retry backoff periods to prevent workflow deadlocks.",
      "rootCause": "When CompleteByID receives a failure request while activity is in retry backoff, the server doesn't distinguish between force-fail and retry-backoff scenarios, potentially wasting attempts.",
      "proposedFix": "Implement behavior: (1) Force-fail non-retryable failures immediately, (2) Make retryable failures a noop or trigger immediate retry, (3) Consider adding explicit flag to force failure.",
      "workaround": "Use non-retryable ApplicationFailure as a marker to force activity failure.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        987,
        5724
      ],
      "keyQuote": "This call should IMHO be a noop to avoid wasting an activity attempt without having a chance to get started and triggering the next retry backoff.",
      "number": 5877,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:58.246Z"
    },
    {
      "summary": "Request to provide additional context information when signals are reapplied after a workflow reset, including whether a signal is from reapply and its sending time, to enable selective signal reapplication based on signal origin.",
      "category": "feature",
      "subcategory": "signal-reapply",
      "apis": [
        "ReceiveAsyncWithInfo"
      ],
      "components": [
        "signal-handler",
        "workflow-replay",
        "history-processing"
      ],
      "concepts": [
        "signal-reapply",
        "workflow-reset",
        "history-events",
        "selective-reapplication",
        "signal-metadata"
      ],
      "severity": "medium",
      "userImpact": "Workflow authors cannot currently distinguish between signals from before/after reset point during reapplication, preventing selective signal filtering based on origin.",
      "rootCause": "Server-side signals during reapplication are not marked to distinguish them from newly received signals, and signal metadata like sending time is not exposed in the SDK despite being available in history events.",
      "proposedFix": "Add a SignalInfo struct with FromReapply flag and SendingTime field, and provide API like channel.ReceiveAsyncWithInfo() to expose this information. Requires server-side change to mark reapplied signals.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would be nice if SDK can provide a flag to tell whether or not the signal is from reapply. The new api may be something like: type SignalInfo struct{FromReapply bool, SendingTime time.Time}",
      "number": 5874,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:59.000Z"
    },
    {
      "summary": "Feature request to expose a boolean flag in WorkflowContext indicating whether the current workflow task is from a reset, enabling workflows to implement soft timeout behavior by failing and later resuming via reset.",
      "category": "feature",
      "subcategory": "workflow-context",
      "apis": [
        "WorkflowContext"
      ],
      "components": [
        "workflow-engine",
        "task-history",
        "context-api"
      ],
      "concepts": [
        "reset",
        "soft-timeout",
        "workflow-recovery",
        "task-state",
        "history-replay",
        "resume"
      ],
      "severity": "medium",
      "userImpact": "Users need to know if a workflow task is executing after a reset to implement soft timeout patterns and conditional recovery logic.",
      "rootCause": "WorkflowContext does not expose whether the current task is from a reset, even though the SDK internally uses this information to reseed the RNG.",
      "proposedFix": "Add a boolean flag to WorkflowContext that is true only for the first workflow task after a reset (remains false on subsequent tasks).",
      "workaround": "Create a timer and cancel the root workflow scope for a soft timeout as a temporary solution.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1412
      ],
      "keyQuote": "I need to implement a soft timeout for a workflow. It will fail the workflow on the timeout. But later on I will let a human to manually restart it by resetting the workflow back.",
      "number": 5873,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:56.791Z"
    },
    {
      "summary": "Search attributes created in global namespaces via the temporal operator CLI are not persisted across replicated clusters. Changes made to the active cluster don't replicate to non-active clusters, and failover doesn't restore the attributes.",
      "category": "bug",
      "subcategory": "cross-replication",
      "apis": [],
      "components": [
        "search-attributes",
        "replication",
        "namespace-management",
        "global-namespace"
      ],
      "concepts": [
        "persistence",
        "replication",
        "failover",
        "cluster-synchronization",
        "search-attribute-management"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably manage search attributes in cross-replicated global namespaces, breaking critical operational workflows during cluster failovers.",
      "rootCause": "Search attribute changes made to the active cluster are not being replicated to non-active clusters, indicating a replication mechanism failure in the global namespace configuration.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "No effect / change to search attributes",
      "number": 5853,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:46.456Z"
    },
    {
      "summary": "Feature request to support scheduling tasks on the last day of each month, which varies by month (28-31 days). Currently requires manually specifying all 12 months or using calendar specs with explicit dates for each month.",
      "category": "feature",
      "subcategory": "schedules",
      "apis": [
        "ScheduleCalendarSpec"
      ],
      "components": [
        "schedules",
        "calendar-spec",
        "temporal-python-sdk"
      ],
      "concepts": [
        "recurring-schedule",
        "month-end",
        "calendar-logic",
        "schedule-configuration",
        "temporal-patterns"
      ],
      "severity": "medium",
      "userImpact": "Users needing month-end recurring tasks must manually specify 12 different calendar specs or use workarounds instead of a single flexible schedule configuration.",
      "rootCause": "The ScheduleCalendarSpec day_of_month parameter only accepts fixed values 1-31 and cannot represent variable month-end dates.",
      "proposedFix": "Allow a special value like 31 or -1 in day_of_month to match the last day of any month regardless of how many days that month contains.",
      "workaround": "Assemble a list of calendar specs, one per month, with explicitly provided last days for each month (but does not work for indefinite schedules).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Because the last day of the month changes month to month, there isn't currently a temporal schedule configuration which supports this.",
      "number": 5811,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:45.861Z"
    },
    {
      "summary": "Feature request to allow application errors to include a suggested retry interval duration, enabling the Temporal server to delay activity retries based on dynamic third-party rate limit responses rather than using fixed retry policies.",
      "category": "feature",
      "subcategory": "activity-retry-policy",
      "apis": [
        "RetryPolicy"
      ],
      "components": [
        "activity-executor",
        "retry-policy",
        "error-handling"
      ],
      "concepts": [
        "retry-interval",
        "rate-limiting",
        "activity-retry",
        "backoff-strategy",
        "dynamic-delay"
      ],
      "severity": "medium",
      "userImpact": "Users dealing with rate-limited third-party services must choose between inefficient fixed retry intervals or complex manual retry logic in workflows.",
      "rootCause": "Application errors currently cannot specify a retry delay interval, forcing developers to either use overly conservative static intervals or implement custom retry mechanisms.",
      "proposedFix": "Extend application error to hold a retry interval value that the Temporal server uses when scheduling activity retries, with corresponding SDK updates to support sending this option.",
      "workaround": "Set RetryPolicy.InitialInterval to maximum expected delay (inefficient), or use non-retryable errors with workflow timers (complex and manual).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        5946
      ],
      "keyQuote": "It would be nice if the application error could also hold a `retry interval` value so that the Temporal server would not schedule the activity retry until such time has elapse.",
      "number": 5796,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:46.145Z"
    },
    {
      "summary": "A deleted schedule continues to appear in the schedule list command and UI even after deletion. Attempting to delete it again results in an error about the workflow execution already being completed.",
      "category": "bug",
      "subcategory": "schedule-management",
      "apis": [],
      "components": [
        "schedule-service",
        "visibility-backend",
        "workflow-execution"
      ],
      "concepts": [
        "schedule-deletion",
        "visibility-sync",
        "state-consistency",
        "elasticsearch",
        "postgres"
      ],
      "severity": "medium",
      "userImpact": "Users cannot fully delete schedules; deleted schedules continue to appear in listings and cannot be removed via CLI, requiring namespace recreation to resolve.",
      "rootCause": "The status change of the schedule workflow hasn't propagated to the visibility backend, causing a mismatch between the actual workflow state and what visibility queries return.",
      "proposedFix": null,
      "workaround": "Delete and recreate the namespace to clean up the orphaned schedule entries.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "That looks like the change to the status of the schedule workflow hasn't made it to visibility.",
      "number": 5785,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:34.836Z"
    },
    {
      "summary": "User requests ability to start child workflows from within a parent workflow that continue independently after the parent completes, without being terminated by parent closure. Currently, child workflows are always terminated when the parent workflow terminates.",
      "category": "feature",
      "subcategory": "child-workflow-lifecycle",
      "apis": [
        "start_child_workflow"
      ],
      "components": [
        "workflow-engine",
        "child-workflow-manager",
        "lifecycle-management"
      ],
      "concepts": [
        "parent-child-relationships",
        "workflow-independence",
        "lifecycle-isolation",
        "orphan-workflows",
        "parent-close-policy",
        "execution-decoupling"
      ],
      "severity": "medium",
      "userImpact": "Users must implement workarounds using activities and clients to start workflows that survive parent completion, when they should be able to do this directly.",
      "rootCause": "Current child workflow implementation ties child lifetime to parent lifecycle, with parent-close-policy Abandon not fully decoupling execution as expected.",
      "proposedFix": "Implement start_orphan_workflow() method that allows starting workflows from within a workflow that are not terminated when parent completes, with support for delay_start and other workflow options.",
      "workaround": "Call an activity that uses Client to start_workflow() instead of starting child workflows directly from within the parent workflow.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Support starting an orphan workflow from inside workflow and forget it.",
      "number": 5777,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:34.357Z"
    },
    {
      "summary": "Feature request to add a workflow_retried counter metric that can be filtered by namespace, to track the global retry count across workflows. Currently only workflow_retry_backoff_timer is available.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [
        "GetInfo"
      ],
      "components": [
        "metrics",
        "workflow-engine",
        "namespace"
      ],
      "concepts": [
        "retry",
        "counter-metric",
        "cardinality",
        "namespace-filtering",
        "observability",
        "workflow-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Users managing control planes need visibility into global workflow retry counts by namespace for monitoring and debugging.",
      "rootCause": null,
      "proposedFix": "Add a workflow_retried counter metric with namespace tag support",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "we are in a situation where we manage the control plane, but not the workflows themselves. We would like to know the global retry count, ideally by namespace",
      "number": 5768,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:33.441Z"
    },
    {
      "summary": "Deeply nested failure causes can exceed the depth limits of some protobuf parsers (particularly upb-based ones used by Python, Ruby, and C/C++), breaking serialization. The issue proposes limiting failure cause depth to approximately 10-15 levels.",
      "category": "bug",
      "subcategory": "failure-handling",
      "apis": [],
      "components": [
        "protobuf-serialization",
        "failure-cause-chain",
        "server-sdk-communication"
      ],
      "concepts": [
        "serialization",
        "protobuf",
        "error-handling",
        "depth-limitation",
        "cross-language-compatibility"
      ],
      "severity": "high",
      "userImpact": "Users with deeply nested failure causes experience broken protobuf parsing, particularly in Python, Ruby, and C/C++ SDKs.",
      "rootCause": "Protobuf parsers based on upb library have depth limits that are exceeded when failure causes are nested beyond ~30 levels.",
      "proposedFix": "Implement failure cause depth limiting, restricting maximum depth to 10-15 levels. Design details regarding whether to reject or handle deep failures remain open.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Users can set very deep failure causes, and once it gets past 30 or so it breaks some protobuf parsers (any based on `upb` like Python and maybe Ruby/C/C++).",
      "number": 5763,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:22.931Z"
    },
    {
      "summary": "DLQ operator tooling currently displays internal queue names that are implementation details rather than the actionable information operators need. The request is to improve the output to show DLQ type, source cluster, and target cluster instead of the complex internal queue name format.",
      "category": "feature",
      "subcategory": "dlq-tooling",
      "apis": [],
      "components": [
        "dlq-operator-tooling",
        "queue-management"
      ],
      "concepts": [
        "dlq",
        "cluster-replication",
        "operator-experience",
        "debugging",
        "queue-naming",
        "internal-implementation"
      ],
      "severity": "medium",
      "userImpact": "Operators must understand internal queue naming conventions to use DLQ tools, reducing usability and introducing unnecessary friction.",
      "rootCause": null,
      "proposedFix": "Change DLQ list output to display structured columns showing DLQ type, source cluster, and target cluster instead of the internal queue name.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Those are the values that should be dumped, so I'd expect something like DLQ TYPE | SOURCE CLUSTER | TARGET CLUSTER | MESSAGECOUNT",
      "number": 5743,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:21.562Z"
    },
    {
      "summary": "The admin-tools Docker image v1.23.0 contains 30 security vulnerabilities (7 high, 20 medium, 3 low CVEs) including issues in pip, OpenTelemetry, and Go HTTP/2 libraries. Multiple follow-up reports indicate the issue persists in later versions (1.25.x) with 76 total vulnerabilities.",
      "category": "bug",
      "subcategory": "security-vulnerabilities",
      "apis": [],
      "components": [
        "admin-tools",
        "docker-image",
        "alpine-base-image",
        "pip",
        "opentelemetry-contrib",
        "golang-net"
      ],
      "concepts": [
        "security",
        "cve",
        "vulnerability",
        "dependency",
        "docker",
        "supply-chain"
      ],
      "severity": "critical",
      "userImpact": "Users of the admin-tools Docker image are exposed to multiple known security vulnerabilities that could be exploited for privilege escalation, denial of service, or resource consumption attacks.",
      "rootCause": "Outdated dependencies in the admin-tools Docker image, including pip 24.0, go.opentelemetry.io/contrib v0.42.0/v0.36.4, and golang.org/x/net v0.7.0. Some vulnerabilities (HTTP/2) are server-side issues not directly used by tctl but present in the linked code.",
      "proposedFix": "Update base image to Alpine 3.19 and upgrade vulnerable dependencies (OpenTelemetry to 0.46.0+, golang.org/x/net to 0.17.0+). The team merged a fix for tctl-relevant vulnerabilities in v1.18.1.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "There are **30** vulnerabilities found for image temporalio/admin-tools:1.23.0, including 7 high, 20 medium and 3 low CVEs.",
      "number": 5741,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:22.837Z"
    },
    {
      "summary": "The temporalio/server:1.23.0 Docker image contains 27 security vulnerabilities (5 high, 19 medium, 3 low), primarily in dependencies like OpenTelemetry gRPC instrumentation, golang.org/x/net HTTP/2, and google.golang.org/grpc that need to be updated to patched versions.",
      "category": "bug",
      "subcategory": "security-vulnerabilities",
      "apis": [],
      "components": [
        "docker-image",
        "dependencies",
        "grpc",
        "http2",
        "opentelemetry"
      ],
      "concepts": [
        "security",
        "cve",
        "vulnerability",
        "dependency-management",
        "docker-security",
        "http2-dos"
      ],
      "severity": "high",
      "userImpact": "Users deploying temporalio/server:1.23.0 are exposed to known security vulnerabilities that could be exploited for denial-of-service attacks and other security compromises.",
      "rootCause": "Outdated dependencies in the server image: OpenTelemetry gRPC instrumentation v0.36.4/v0.42.0 (fixed in 0.46.0), golang.org/x/net v0.7.0 (fixed in 0.17.0), and google.golang.org/grpc v1.53.0 (fixed in 1.58.3+)",
      "proposedFix": "Update dependencies to patched versions: otelgrpc 0.46.0, golang.org/x/net 0.17.0, and google.golang.org/grpc 1.56.3",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Dependencies were updated in later releases to address the CVEs",
      "related": [],
      "keyQuote": "There are 27 vulnerabilities found for image temporalio/server:1.23.0, including 5 high, 19 medium and 3 low CVEs",
      "number": 5740,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:10.125Z"
    },
    {
      "summary": "Cassandra-based docker compose configuration fails to start because the standard visibility keyspace 'temporal_visibility' does not exist. The docker image no longer supports standard visibility, and users should use the Elasticsearch-based compose file instead.",
      "category": "bug",
      "subcategory": "docker-setup",
      "apis": [],
      "components": [
        "docker-compose",
        "cassandra",
        "visibility-store"
      ],
      "concepts": [
        "docker",
        "cassandra",
        "keyspace",
        "visibility",
        "initialization",
        "health-check"
      ],
      "severity": "high",
      "userImpact": "Users following the standard docker compose setup instructions cannot start Temporal with Cassandra due to missing visibility keyspace configuration.",
      "rootCause": "Standard visibility support was removed from docker images but the docker-compose-cass.yml file was not removed, causing health checks to fail when the visibility keyspace doesn't exist.",
      "proposedFix": "Use docker-compose-cass-es.yml (Elasticsearch-based) instead of docker-compose-cass.yml, or remove the outdated docker-compose-cass.yml file from the repository.",
      "workaround": "Use the Elasticsearch-based docker compose file (docker-compose-cass-es.yml) as an alternative configuration.",
      "resolution": "wontfix",
      "resolutionDetails": "The docker-compose-cass.yml file is outdated and should be removed or deprecated. Users should migrate to the Elasticsearch-based compose file.",
      "related": [],
      "keyQuote": "standard visibility support has been removed from docker images, and this `docker-compose-cass.yml` should've been removed as well",
      "number": 5734,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:10.883Z"
    },
    {
      "summary": "Batch operations fail with a 400 error when using standard visibility instead of advanced visibility (Elasticsearch or SQL). Users attempting batch operations receive an error about 'TemporalNamespaceDivision' not being supported by standard visibility.",
      "category": "bug",
      "subcategory": "batch-operations",
      "apis": [],
      "components": [
        "batch-operations",
        "visibility",
        "query-filter"
      ],
      "concepts": [
        "visibility-modes",
        "standard-visibility",
        "advanced-visibility",
        "deprecation",
        "elasticsearch",
        "sql"
      ],
      "severity": "high",
      "userImpact": "Users attempting batch operations with standard visibility configurations receive errors and cannot proceed, forcing migration to advanced visibility.",
      "rootCause": "Batch operations require advanced visibility (Elasticsearch or SQL), but standard visibility does not support the 'TemporalNamespaceDivision' filter attribute used by batch operations.",
      "proposedFix": null,
      "workaround": "Migrate to advanced visibility (Elasticsearch or SQL based) as per v1.20.0+ upgrade instructions.",
      "resolution": "wontfix",
      "resolutionDetails": "Standard visibility is deprecated and being removed. Batch operations require advanced visibility. Users must upgrade to Elasticsearch or SQL-based visibility.",
      "related": [],
      "keyQuote": "Batch operation require advanced visibility (ElasticSearch or SQL based). Standard visibility has been deprecated.",
      "number": 5732,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:07:10.227Z"
    },
    {
      "summary": "Temporal server fails to connect to PostgreSQL when the database password contains certain special characters like backslashes and tabs, despite the temporal-sql-tool successfully using the same password to create schemas. The issue likely involves incorrect password escaping in DSN generation.",
      "category": "bug",
      "subcategory": "database-connection",
      "apis": [],
      "components": [
        "database-connection",
        "dsn-generation",
        "postgres-plugin",
        "server-initialization"
      ],
      "concepts": [
        "password-escaping",
        "connection-string",
        "special-characters",
        "authentication",
        "dsn-generation",
        "url-encoding"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy Temporal on cloud databases (Google Cloud SQL, Azure) when their passwords contain special characters, blocking production deployments.",
      "rootCause": "Password escaping using net/url.QueryEscape in DSN generation may not properly handle certain characters like backslash and tab (\\t), causing authentication failures despite valid passwords.",
      "proposedFix": "Ensure proper password escaping in DSN generation, possibly by replacing or supplementing net/url.QueryEscape with a more robust solution that handles all special characters correctly.",
      "workaround": "Remove special characters (particularly backslashes and tabs) from database passwords to work around the connection issue.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        551
      ],
      "keyQuote": "the temporal-sql-tool appears to run correctly... This also proves that the password is, in fact, _valid_. But that's where success ends... connections begin to error",
      "number": 5729,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:58.448Z"
    },
    {
      "summary": "Request to add PARENT_CLOSE_POLICY_DISCONNECTED mode for child workflows that completely disconnects them from parent supervision, eliminating overhead and UnhandledCommand errors. Child start events are reported but completion/failure events are not, and parent completion doesn't affect the child.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "StartChildWorkflow",
        "ParentClosePolicy"
      ],
      "components": [
        "workflow-execution",
        "child-workflow",
        "parent-close-policy"
      ],
      "concepts": [
        "disconnected-workflows",
        "parent-child-relationship",
        "performance-optimization",
        "workflow-supervision",
        "event-reporting"
      ],
      "severity": "medium",
      "userImpact": "Users cannot start independent child workflows without incurring performance overhead and UnhandledCommand errors from completion/failure event handling.",
      "rootCause": "Current ParentClosePolicy options always maintain parent-child event coupling, even when child independence is desired.",
      "proposedFix": "Add DISCONNECTED mode to ParentClosePolicy where only ChildWorkflowExecutionStartedEvent is reported to parent, while completion and failure events are not.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It is not possible to start child workflows in a fully disconnected mode which will not report completion/failure to the parent.",
      "number": 5725,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:56.787Z"
    },
    {
      "summary": "Fix the outgoing service registry's handling of namespace notification version to align with the namespace handler's UpdateNamespace implementation pattern.",
      "category": "bug",
      "subcategory": "service-registry",
      "apis": [],
      "components": [
        "outgoing-service-registry",
        "namespace-handler",
        "namespace-notifications"
      ],
      "concepts": [
        "namespace-management",
        "version-handling",
        "service-registry",
        "notification-synchronization",
        "state-consistency"
      ],
      "severity": "medium",
      "userImpact": "Incorrect namespace notification version handling could cause synchronization issues across service registry components.",
      "rootCause": "Outgoing service registry not following the established pattern used in namespace handler for UpdateNamespace",
      "proposedFix": "Use namespace_handler UpdateNamespace implementation as reference for proper version handling",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved by aligning outgoing service registry namespace notification version handling with the namespace handler pattern",
      "related": [
        5686
      ],
      "keyQuote": "Use namespace_handler UpdateNamespace for reference.",
      "number": 5695,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:55.848Z"
    },
    {
      "summary": "Replace namespace failover version with an opaque consistency token in NexusOperationCompletion to avoid exposing internal implementation details to the application layer in the new state machine framework.",
      "category": "feature",
      "subcategory": "nexus-operations",
      "apis": [
        "NexusOperationCompletion"
      ],
      "components": [
        "nexus",
        "state-machine-framework",
        "consistency-token"
      ],
      "concepts": [
        "opaque-token",
        "consistency",
        "namespace-failover",
        "abstraction",
        "internal-details",
        "api-design"
      ],
      "severity": "medium",
      "userImpact": "Prevents internal implementation details from leaking into the application layer when using Nexus operations.",
      "rootCause": "Current design exposes namespace failover version to application layer instead of using an opaque consistency token.",
      "proposedFix": "Replace namespace failover version with an opaque consistency token in NexusOperationCompletion.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation completed via PR #5686 with discussion about using opaque consistency tokens.",
      "related": [
        5686
      ],
      "keyQuote": "We're using namespace failover version and we'd rather not leak that into the \"application layer\" in the new state machine framework.",
      "number": 5694,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:43.961Z"
    },
    {
      "summary": "POSTGRES_SEEDS environment variable doesn't properly handle multiple PostgreSQL hosts in comma-separated format. The string is treated as a single hostname instead of being parsed as multiple hosts, causing connection failures. Even with pgx driver support for multiple hosts, there's no way to configure target_session_attrs to detect read-write replicas.",
      "category": "bug",
      "subcategory": "postgres-connection",
      "apis": [],
      "components": [
        "sql-connector",
        "postgres-driver",
        "config-template"
      ],
      "concepts": [
        "connection-string",
        "multi-host",
        "replication",
        "replica-detection",
        "failover"
      ],
      "severity": "medium",
      "userImpact": "Users with PostgreSQL master/replica clusters cannot properly configure Temporal to connect to multiple hosts, forcing them to use workarounds or single-host configurations.",
      "rootCause": "The config template appends ':$DB_PORT' to POSTGRES_SEEDS assuming it's a single hostname, and doesn't support passing pgx-specific connection attributes like target_session_attrs for replica detection.",
      "proposedFix": "Allow POSTGRES_SEEDS to accept pgx multiple-host format (host1:port1,host2:port2) and support passing connection string parameters like target_session_attrs=read-write through environment variables or config template.",
      "workaround": "Override the config template to manually add target_session_attrs parameter and format multiple hosts correctly for pgx driver, or use postgres12_pgx driver with workaround for replica detection.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Despite the name, we unfortunately expect only one hostname in that variable, so we append \":$DB_PORT\" to it, which won't work with multiple host names.",
      "number": 5693,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:45.675Z"
    },
    {
      "summary": "Request for official documentation and support for OpenSearch as a visibility datastore alongside Elasticsearch. Users are successfully running Temporal with OpenSearch but lack official guidance, and compatibility issues exist with newer OpenSearch versions.",
      "category": "feature",
      "subcategory": "visibility-opensearch",
      "apis": [],
      "components": [
        "visibility",
        "elasticsearch-client",
        "opensearch-integration"
      ],
      "concepts": [
        "opensearch",
        "elasticsearch",
        "compatibility",
        "visibility-datastore",
        "self-hosted",
        "api-divergence"
      ],
      "severity": "medium",
      "userImpact": "Users running OpenSearch need official support documentation and compatibility assurance when choosing a visibility datastore for Temporal deployments.",
      "rootCause": "OpenSearch and Elasticsearch APIs have diverged with newer versions, causing compatibility issues (e.g., pagination bug in OpenSearch 2.8.0+) not documented in Temporal's official guidance.",
      "proposedFix": "Add official OpenSearch support with compatibility matrix documentation and implement workarounds for known OpenSearch version-specific issues.",
      "workaround": "Temporal 1.29.0+ works with OpenSearch up to v2.7.0; PR #8964 provides workaround for OpenSearch 2.8.0+ pagination bug.",
      "resolution": "fixed",
      "resolutionDetails": "PR #8964 added workaround for OpenSearch 2.8.0+ pagination bug, making Temporal compatible with latest OpenSearch versions and providing de-facto support.",
      "related": [
        8964
      ],
      "keyQuote": "Temporal 1.29.0+ should work with OpenSearch up to v2.7.0. OpenSearch 2.8.0+ has a bug pagination... We are working on a work around.",
      "number": 5680,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:43.711Z"
    },
    {
      "summary": "User requested that Temporal metrics be grouped under a common prefix for better organization in monitoring systems. The issue was resolved by pointing out that the `global.metrics.prefix` configuration option already exists for this purpose.",
      "category": "feature",
      "subcategory": "metrics-configuration",
      "apis": [],
      "components": [
        "metrics",
        "configuration",
        "monitoring"
      ],
      "concepts": [
        "metrics-prefix",
        "namespace-organization",
        "monitoring-observability",
        "backward-compatibility",
        "configuration-management"
      ],
      "severity": "low",
      "userImpact": "Users setting up self-hosted Temporal clusters can organize their metrics under a common prefix to improve clarity in monitoring systems.",
      "rootCause": null,
      "proposedFix": "Use the existing `global.metrics.prefix` configuration option to group metrics under a common label.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The feature was already implemented via the `global.metrics.prefix` config option. The resolution documented the existing capability and clarified that custom metric naming should not be allowed to avoid user confusion.",
      "related": [],
      "keyQuote": "Temporal supports a `global.metrics.prefix` config option. I've confirmed that this works as expected with a self-hosted Temporal cluster.",
      "number": 5679,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:32.534Z"
    },
    {
      "summary": "Cron expressions provided to ScheduleSpec are not returned when describing the schedule. Instead, the cron expression is translated to calendar specs during creation, making it input-only.",
      "category": "question",
      "subcategory": "schedule-spec",
      "apis": [
        "ScheduleSpec",
        "Schedule",
        "ScheduleActionStartWorkflow",
        "get_schedule_handle",
        "describe"
      ],
      "components": [
        "schedule-api",
        "schedule-storage",
        "schedule-parser"
      ],
      "concepts": [
        "cron-expression",
        "calendar-spec",
        "schedule-creation",
        "schedule-retrieval",
        "data-translation",
        "api-contract"
      ],
      "severity": "low",
      "userImpact": "Users cannot retrieve the original cron expression they used to create a schedule, which may complicate updates or debugging.",
      "rootCause": "Cron expressions are intentionally translated to calendar specs during schedule creation and not stored in their original form.",
      "proposedFix": null,
      "workaround": "Add a comment to the cron expression (e.g., '0 0 * * * # comment here') which will be preserved in the calendar spec notes field.",
      "resolution": "wontfix",
      "resolutionDetails": "This is expected behavior by design. Cron expressions are translated to calendar specs on creation. A better solution for diffing and updating schedules is planned for future releases.",
      "related": [],
      "keyQuote": "Cron expressions are translated to calendar specs, so that is expected. Basically, they're input-only.",
      "number": 5673,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:30.809Z"
    },
    {
      "summary": "Custom search attribute names with more than 2 dot separators cause SQL parsing errors in workflow list queries. Users must use backticks to escape attribute names containing dots, but the escaping rules are inconsistent and undocumented.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [
        "ListWorkflowExecutions"
      ],
      "components": [
        "search-attributes",
        "query-parser",
        "sql-engine"
      ],
      "concepts": [
        "naming-convention",
        "escaping",
        "query-syntax",
        "dot-notation",
        "sql-parsing"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably query workflows using custom search attributes with multiple dots in their names without understanding undocumented escaping rules.",
      "rootCause": "SQL query parser fails to properly handle search attribute names containing dots without backtick escaping; escaping rules are not clearly defined.",
      "proposedFix": null,
      "workaround": "Enclose search attribute names with backticks in queries (e.g., `A.B.C.D` or 'glossary.GlossDiv.GlossList.GlossEntry.GlossTerm').",
      "resolution": "wontfix",
      "resolutionDetails": "Issue was closed as potential-bug but appears to be expected behavior requiring documentation of escaping rules rather than a code fix.",
      "related": [],
      "keyQuote": "You need to enclose the search attribute name with backticks. Eg: `A.B.C.D` = 'abcd'.",
      "number": 5670,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:32.316Z"
    },
    {
      "summary": "Search attributes created via `temporal operator search-attribute create` appear to succeed but aren't actually persisted in v1.23.0, unlike v1.22.4. Additionally, error handling for duplicate attributes changed behavior between versions.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "operator-cli",
        "search-attribute-service",
        "database"
      ],
      "concepts": [
        "persistence",
        "database-compatibility",
        "command-execution",
        "state-verification",
        "backward-compatibility"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably create search attributes in v1.23.0 despite receiving success messages, breaking a core operational feature.",
      "rootCause": "MySQL 5.7 compatibility issue with v1.23.0 search attribute creation logic",
      "proposedFix": null,
      "workaround": "Use MySQL 8 instead of MySQL 5.7 as the database backend",
      "resolution": "invalid",
      "resolutionDetails": "Resolved as database version incompatibility - issue was user configuration (MySQL 5.7) rather than a product bug. Works correctly with MySQL 8.",
      "related": [],
      "keyQuote": "Turns out this was because I was using mysql 5.7 as the database. It works with mysql 8.",
      "number": 5669,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:19.746Z"
    },
    {
      "summary": "Visibility data becomes inconsistent when visibility tasks execute out of order, causing closed workflows to appear as running in ListWorkflowExecutions until they're deleted by retention policy. The issue occurs because visibility tasks lack concurrency control and can be executed in arbitrary order by the queue processor.",
      "category": "bug",
      "subcategory": "visibility-consistency",
      "apis": [
        "ListWorkflowExecutions"
      ],
      "components": [
        "visibility-queue-processor",
        "visibility-storage",
        "mutable-state",
        "transfer-tasks"
      ],
      "concepts": [
        "eventual-consistency",
        "race-condition",
        "task-ordering",
        "concurrency-control",
        "database-synchronization",
        "workflow-state"
      ],
      "severity": "high",
      "userImpact": "Users see incorrect workflow statuses in the UI (completed workflows showing as running) and cluttered dashboards with stale data until retention policy deletes them.",
      "rootCause": "The immediate queue implementation and FIFO scheduler dispatch visibility tasks among multiple workers without guaranteeing execution order. If a visibility task gets delayed (database/context busy), earlier tasks execute first, causing UpsertExecutionVisibilityTask to complete after CloseExecutionVisibilityTask, leaving visibility storage inconsistent.",
      "proposedFix": null,
      "workaround": "For MySQL: manually synchronize visibility table with current_executions table using a join query to correct status mismatches.",
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "visibility tasks seem to allow arbitrary order of execution, and the visibility storage doesn't implement any concurrency control",
      "number": 5643,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:18.482Z"
    },
    {
      "summary": "Server fails to start when using `--dynamic-config-value` flag with JSON configuration values, returning 'invalid JSON value' error even for valid JSON. User cannot apply dynamic configuration via CLI despite the feature being documented.",
      "category": "bug",
      "subcategory": "configuration-dynamic-config",
      "apis": [],
      "components": [
        "server-startup",
        "cli-arguments",
        "config-parser"
      ],
      "concepts": [
        "configuration",
        "dynamic-config",
        "JSON-parsing",
        "CLI-flags",
        "initialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot configure the Temporal server via command-line dynamic config values, blocking server startup with valid configuration parameters.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The server fails to start and gives the error \"Error: invalid JSON value for key \"persistence.defaultStore\"",
      "number": 5642,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:18.082Z"
    },
    {
      "summary": "Update official Docker images to use the new pre-release CLI rewrite branch instead of CLI v0.11.0 once it has a release available.",
      "category": "feature",
      "subcategory": "docker-builds",
      "apis": [],
      "components": [
        "docker-builds",
        "cli",
        "admin-tools"
      ],
      "concepts": [
        "versioning",
        "cli-rewrite",
        "docker-images",
        "releases",
        "official-images"
      ],
      "severity": "medium",
      "userImpact": "Users will have access to updated CLI features and improvements when official Temporal Docker images are built with the new CLI rewrite.",
      "rootCause": null,
      "proposedFix": "Update admin-tools and other official images to use the new pre-release CLI branch once it has a release",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        190
      ],
      "keyQuote": "Our admin-tools and other official images should use the new pre-release CLI branch once it has a release",
      "number": 5634,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:03.605Z"
    },
    {
      "summary": "Task queue validation logic is duplicated in two locations (frontend workflow handler and history service command checker) with inconsistent rules and defaults. This feature request seeks to consolidate validation into a common helper function for standardization across the codebase.",
      "category": "feature",
      "subcategory": "validation",
      "apis": [],
      "components": [
        "frontend",
        "history-service",
        "task-queue",
        "command-checker"
      ],
      "concepts": [
        "validation",
        "standardization",
        "task-queue",
        "code-duplication",
        "refactoring"
      ],
      "severity": "medium",
      "userImpact": "Inconsistent task queue validation rules across components may lead to unexpected behavior or errors in workflow execution depending on the code path.",
      "rootCause": "Task queue validation logic was implemented independently in two different services without a shared utility function.",
      "proposedFix": "Move task queue validation to a common helper function and standardize its application across all service components.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Validation was consolidated into a shared helper function with standardized rules applied consistently.",
      "related": [],
      "keyQuote": "Task queue validation should be moved to a common helper and standardized across the code base.",
      "number": 5628,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:05.307Z"
    },
    {
      "summary": "The Archival tab in Temporal UI fails to display archived workflows, returning HTTP 400 error with message 'Cluster is not configured for reading archived visibility records' despite the configuration having archival visibility read enabled.",
      "category": "bug",
      "subcategory": "archival-visibility",
      "apis": [],
      "components": [
        "frontend",
        "archival-metadata",
        "visibility-archival",
        "workflow-handler"
      ],
      "concepts": [
        "archival",
        "visibility",
        "configuration",
        "S3-backend",
        "namespace-settings",
        "read-enabled"
      ],
      "severity": "high",
      "userImpact": "Users with archived workflows cannot view them in the Archival tab despite proper configuration, making archived workflow data inaccessible through the UI.",
      "rootCause": "The archivalMetadata.GetVisibilityConfig().ReadEnabled() check is failing even though archival.visibility.enableRead is set to true in the configuration file, suggesting a configuration loading or mapping issue.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Cluster is not configured for reading archived visibility records.",
      "number": 5624,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:06:06.774Z"
    },
    {
      "summary": "TLS encryption is not applied to the worker gRPC port (7239) and membership port (6939) in Temporal Server, despite proper TLS configuration. Only frontend and inter-node traffic use TLS.",
      "category": "bug",
      "subcategory": "tls-configuration",
      "apis": [],
      "components": [
        "worker",
        "grpc-server",
        "membership-port",
        "tls-config"
      ],
      "concepts": [
        "tls",
        "encryption",
        "security",
        "configuration",
        "worker-communication",
        "membership-protocol"
      ],
      "severity": "high",
      "userImpact": "Users cannot enable TLS encryption for worker-to-server communication, leaving sensitive gRPC traffic unencrypted in mTLS deployments.",
      "rootCause": "TLS configuration is not applied to worker RPC ports (grpcPort and membershipPort) in the server configuration, only to frontend and inter-node ports.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Author moved discussion to forum as this may be a feature request rather than a bug, indicating it was not fixed as a bug but rather deprioritized or reclassified.",
      "related": [],
      "keyQuote": "we could not find a way to enable tls for the traffic on the membership port and the worker grpc port",
      "number": 5614,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:52.567Z"
    },
    {
      "summary": "The official temporalio/server:1.23.0 Docker image reports version 1.23.0-rc16 instead of the expected release version 1.23.0. This version string mismatch affects users who check the server version via command line or UI.",
      "category": "bug",
      "subcategory": "release-packaging",
      "apis": [],
      "components": [
        "docker-image",
        "server-binary",
        "version-reporting"
      ],
      "concepts": [
        "version-string",
        "release-version",
        "rc-build",
        "docker-hub",
        "version-mismatch"
      ],
      "severity": "medium",
      "userImpact": "Users see incorrect RC version labels in production systems, causing confusion about which version is actually running.",
      "rootCause": "The Docker image for 1.23.0 was built with an RC binary instead of the final release binary.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The temporal-server binary in the official temporalio/server@1.23.0 image on Docker reports a -rc version: temporal version 1.23.0-rc16",
      "number": 5604,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:54.272Z"
    },
    {
      "summary": "The new Temporal CLI is missing functionality to update a namespace with multiple clusters. Users need a command equivalent to the old tctl namespace update command that accepts a list of clusters.",
      "category": "feature",
      "subcategory": "cli-namespace-management",
      "apis": [],
      "components": [
        "cli",
        "namespace-operator",
        "multi-cluster-replication"
      ],
      "concepts": [
        "namespace-update",
        "cluster-replication",
        "multi-cluster",
        "cli-parity",
        "migration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot configure multi-cluster replication for existing namespaces using the new CLI, forcing them to rely on deprecated tctl or manually manage namespace configuration.",
      "rootCause": "The new temporal operator namespace update command does not support accepting multiple clusters as a list parameter.",
      "proposedFix": "Modify the temporal operator namespace update command to accept a --cluster parameter that can handle multiple cluster names (e.g., --cluster 'clusterA clusterB')",
      "workaround": "Continue using the deprecated tctl CLI for namespace updates with multiple clusters until the new CLI is updated.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Have the cluster accept a list of clusters and not only a single one.",
      "number": 5589,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:53.070Z"
    },
    {
      "summary": "The dev-server crashes when receiving an AddOrUpdateRemoteCluster RPC with an invalid frontend address (missing port). The server should reject the request with an error instead of calling Fatal() and crashing.",
      "category": "bug",
      "subcategory": "cluster-management",
      "apis": [
        "AddOrUpdateRemoteCluster"
      ],
      "components": [
        "frontend-operator-handler",
        "rpc-factory",
        "client-factory"
      ],
      "concepts": [
        "error-handling",
        "input-validation",
        "remote-cluster",
        "rpc-address",
        "crash-recovery"
      ],
      "severity": "high",
      "userImpact": "Users cannot run the dev-server if an invalid remote cluster address is accidentally provided, as the server crashes instead of rejecting the request gracefully.",
      "rootCause": "CreateRemoteFrontendGRPCConnection calls Fatal() when address parsing fails instead of returning an error that can be handled by the caller.",
      "proposedFix": "Replace Fatal() call with proper error handling and return an error to the RPC handler so it can reject the request with a gRPC error status.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Server rejects the RPC with an error, but continues to run.",
      "number": 5587,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:39.628Z"
    },
    {
      "summary": "Custom search attributes with hyphens in their names cause query filtering errors in the Temporal UI, where the hyphen is incorrectly treated as a delimiter. The issue was resolved in a UI update that properly handles special characters in custom search attribute names.",
      "category": "bug",
      "subcategory": "custom-search-attributes",
      "apis": [],
      "components": [
        "ui",
        "elasticsearch",
        "query-parser",
        "search-attributes"
      ],
      "concepts": [
        "custom-search-attributes",
        "filtering",
        "query-parsing",
        "special-characters",
        "visibility",
        "workflow-querying"
      ],
      "severity": "medium",
      "userImpact": "Users cannot filter workflows by custom search attributes that contain hyphens in their names, making certain search attribute configurations unusable.",
      "rootCause": "The query parser was treating hyphens as delimiters and inserting spaces around them, converting 'abc-test' to 'abc - test' which caused the SQL parser to fail.",
      "proposedFix": "Fix the query parser to properly handle special characters like hyphens in custom search attribute names.",
      "workaround": "Avoid using hyphens in custom search attribute names and use alternative naming conventions instead.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed via UI PR #2015 which properly handles hyphens in custom search attribute names. Verified as working in Server v1.26.2 and UI v2.28.0.",
      "related": [],
      "keyQuote": "It seems that although there is no restriction on using hyphen it is somewhat acting as a delimeter and inserting spaces while filtering.",
      "number": 5557,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:41.422Z"
    },
    {
      "summary": "Update the license check script to verify that the current year is used in copyright notices for new files. This issue tracks the need to enhance the automated license verification tooling.",
      "category": "feature",
      "subcategory": "license-verification",
      "apis": [],
      "components": [
        "license-check-script",
        "build-tools",
        "ci-pipeline"
      ],
      "concepts": [
        "copyright",
        "license-compliance",
        "automation",
        "file-validation",
        "year-verification"
      ],
      "severity": "low",
      "userImpact": "Ensures license headers in new files automatically include the current year, improving compliance automation.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        5523
      ],
      "keyQuote": "Update license check script to verify current year is used in new files",
      "number": 5553,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:39.387Z"
    },
    {
      "summary": "Signal validation currently uses the 'SignalNameTooLong' error message when the signal name is empty, which is confusing and inaccurate. The feature request asks for a distinct error message specifically for empty signal names to provide clearer feedback.",
      "category": "other",
      "subcategory": "signal-validation",
      "apis": [],
      "components": [
        "frontend",
        "workflow-handler",
        "signal-validation"
      ],
      "concepts": [
        "error-messages",
        "signal-validation",
        "user-feedback",
        "clarity"
      ],
      "severity": "low",
      "userImpact": "Users receive a confusing error message when passing an empty signal name, making it harder to understand what went wrong.",
      "rootCause": "Signal validation logic reuses SignalNameTooLong error for both empty and oversized signal names, conflating two different error conditions.",
      "proposedFix": "Add a new distinct error message specifically for empty signal names.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "add a new error message when signalName is empty: Currently used is SignalNameTooLong which is a bit confusing",
      "number": 5544,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:27.705Z"
    },
    {
      "summary": "Cassandra indexes on cluster_membership table (last_heartbeat, session_start) have scalability limitations for large clusters. Request to replace with inverted tables (cluster_membership_by_last_heartbeat, cluster_membership_by_session_start) for better query performance.",
      "category": "other",
      "subcategory": "cassandra-schema",
      "apis": [],
      "components": [
        "cassandra-schema",
        "cluster-membership",
        "database"
      ],
      "concepts": [
        "indexing",
        "cassandra-limitations",
        "schema-design",
        "query-optimization",
        "inverted-tables",
        "scalability"
      ],
      "severity": "medium",
      "userImpact": "Users operating large Cassandra clusters experience potential performance degradation due to index restrictions when querying cluster membership state.",
      "rootCause": "Cassandra indexes have documented restrictions that make them ineffective for large clusters, particularly for queries on cluster_membership table columns.",
      "proposedFix": "Replace secondary indexes with inverted tables (cluster_membership_by_last_heartbeat, cluster_membership_by_session_start) that partition by the queried column and reference the membership_partition.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "indexes are not very good works in cassandra for big clusters because have a lot of restrictions",
      "number": 5543,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:28.288Z"
    },
    {
      "summary": "Request for official NestJS framework support or endorsement to enable backend teams using NestJS to adopt Temporal. User is concerned about relying on community packages that may not be maintained long-term.",
      "category": "feature",
      "subcategory": "framework-integration",
      "apis": [],
      "components": [
        "sdk-nodejs",
        "framework-integration"
      ],
      "concepts": [
        "nestjs-integration",
        "framework-support",
        "community-packages",
        "maintenance-risk",
        "nodejs-ecosystem"
      ],
      "severity": "low",
      "userImpact": "Organizations standardized on NestJS cannot confidently adopt Temporal without official support or guidance, limiting Temporal adoption in NestJS-first teams.",
      "rootCause": null,
      "proposedFix": "Official NestJS framework support or formal endorsement/maintenance of community packages",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "it would be a risk to our department If we invested then the pkg is not updated",
      "number": 5541,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:26.097Z"
    },
    {
      "summary": "Add support for ResetReapplyExcludeType in batch reset operations by introducing a new reset_reapply_exclude_types field and deprecating the existing reset_reapply_type field in ResetOptions proto message.",
      "category": "feature",
      "subcategory": "batch-reset",
      "apis": [
        "ResetOptions",
        "ResetReapplyExcludeType"
      ],
      "components": [
        "proto-api",
        "batch-reset",
        "reset-workflow"
      ],
      "concepts": [
        "reset",
        "reapply",
        "batch-operations",
        "proto-deprecation",
        "api-evolution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use the new ResetReapplyExcludeType filtering with batch reset operations, limiting their control over which event types are reapplied.",
      "rootCause": "Batch reset proto messages and server implementation were not updated to support the ResetReapplyExcludeType introduced in recent API changes.",
      "proposedFix": "Add reset_reapply_exclude_types field to ResetOptions proto message and implement corresponding server logic for batch reset operations.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Proto and server implementation were updated to support ResetReapplyExcludeType in batch reset operations.",
      "related": [
        348,
        5360
      ],
      "keyQuote": "However, we need to make the corresponding proto api and server changes for batch reset.",
      "number": 5534,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:15.153Z"
    },
    {
      "summary": "User requests support for Azure Blob Storage as an archival destination alongside existing GCloud and AWS S3 support. They propose a plugin-based solution to enable custom implementations without forking the codebase.",
      "category": "feature",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "archival",
        "storage-plugin",
        "server-options"
      ],
      "concepts": [
        "cloud-storage",
        "plugin-architecture",
        "Azure",
        "extensibility",
        "data-archival"
      ],
      "severity": "medium",
      "userImpact": "Azure customers cannot use Temporal's archival feature and must either fork the codebase or find alternative solutions.",
      "rootCause": null,
      "proposedFix": "Implement plugin-based archival solution allowing custom cloud storage implementations, or add native Azure Blob Storage support.",
      "workaround": "Implement Azure support locally by forking the codebase (high maintenance burden).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We would like a plugin based solution where we can go ahead and implement our own code and pass them to something like the Server Options",
      "number": 5516,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:16.193Z"
    },
    {
      "summary": "Python SDK's ScheduleHandle.update() method doesn't support updating the memo field, which users need to keep schedule metadata in sync when updating other schedule configuration. Request to add memo support to ScheduleUpdate object.",
      "category": "feature",
      "subcategory": "schedule-management",
      "apis": [
        "ScheduleHandle.update",
        "ScheduleUpdateInput",
        "ScheduleUpdate",
        "client.create_schedule"
      ],
      "components": [
        "schedule-api",
        "python-sdk",
        "schedule-update"
      ],
      "concepts": [
        "schedule-metadata",
        "memo-field",
        "schedule-configuration",
        "update-operations",
        "metadata-management"
      ],
      "severity": "medium",
      "userImpact": "Users cannot update schedule memo when modifying other schedule properties, forcing them to work with stale metadata or rebuild schedules entirely.",
      "rootCause": null,
      "proposedFix": "Add memo field support to ScheduleUpdate object and ScheduleUpdateInput in Python SDK's ScheduleHandle.update() method.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Allow an updated memo object to be passed in the ScheduleHandle.update call in the ScheduleUpdate object.",
      "number": 5510,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:16.576Z"
    },
    {
      "summary": "Reset operations fail when reapplied events (signals and updates) exceed the 4MB batch size limit enforced by the persistence layer. The feature request seeks to automatically split large event batches during reset to support reapplying events larger than 4MB.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "history-builder",
        "persistence-layer",
        "reset-logic"
      ],
      "concepts": [
        "batch-size-limit",
        "event-reapplication",
        "reset",
        "signals",
        "updates",
        "conflict-resolution",
        "event-batching"
      ],
      "severity": "high",
      "userImpact": "Users cannot reset workflows when the combined size of reapplied signals and updates exceeds 4MB, blocking an important operational capability.",
      "rootCause": "Event reapplication during reset groups all reapplied events into a single batch without checking against the 4MB persistence layer size limit.",
      "proposedFix": "Approach 1: Automatically track event batch size in history builder code and create new batches when limit is exceeded. Approach 2: Make this reset-specific by tracking reapplied event size and calling a history builder method to create new batches.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Reset with more than 4MB reapplied events should be supported.",
      "number": 5493,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:04.924Z"
    },
    {
      "summary": "User encounters validation error when attempting to configure dual visibility store with Elasticsearch as primary and PostgreSQL as secondary. The server rejects the configuration with 'cannot set secondaryVisibilityStore when visibilityStore is setting elasticsearch datastore' error.",
      "category": "bug",
      "subcategory": "visibility-store",
      "apis": [],
      "components": [
        "visibility-store",
        "persistence-config",
        "elasticsearch",
        "postgres"
      ],
      "concepts": [
        "dual-visibility",
        "migration",
        "configuration-validation",
        "datastore"
      ],
      "severity": "high",
      "userImpact": "Users cannot migrate from Elasticsearch to PostgreSQL using the dual visibility store feature as documented.",
      "rootCause": "Validation logic in persistence config incorrectly prevents secondary visibility store when primary is Elasticsearch, contradicting documented dual visibility store feature.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "cannot set secondaryVisibilityStore when visibilityStore is setting elasticsearch datastore",
      "number": 5489,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:05.514Z"
    },
    {
      "summary": "Temporal UI cannot display workflow data when using Elasticsearch/OpenSearch as the advanced visibility store. The issue was resolved by ensuring the Elasticsearch index name matches the expected naming pattern (prefix of `temporal_visibility_v1`).",
      "category": "bug",
      "subcategory": "elasticsearch-visibility",
      "apis": [],
      "components": [
        "frontend",
        "visibility-store",
        "elasticsearch"
      ],
      "concepts": [
        "visibility",
        "elasticsearch",
        "index-pattern",
        "configuration",
        "ui-connectivity"
      ],
      "severity": "high",
      "userImpact": "Users cannot view workflow data in the Temporal UI despite data being successfully written to Elasticsearch.",
      "rootCause": "The Elasticsearch index template is hardcoded to expect indices with a prefix of `temporal_visibility_v1`, but users may configure different index names that don't match this pattern.",
      "proposedFix": "Change the Elasticsearch index name to follow the `temporal_visibility_v1` prefix pattern, or make the index template pattern configurable.",
      "workaround": "Rename the Elasticsearch index to use the `temporal_visibility_v1` prefix (e.g., `temporal_visibility_v1_dev`).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Pattern for template index is fixed to match index with prefix of `temporal_visibility_v1`",
      "number": 5475,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:05:05.167Z"
    },
    {
      "summary": "When using the Temporal library with a metrics port configured as `0` (auto-bind), there's no programmatic way to extract the actual bound port. Users must find a free port in advance as a workaround, defeating the purpose of auto-binding.",
      "category": "feature",
      "subcategory": "metrics-configuration",
      "apis": [],
      "components": [
        "metrics",
        "dev-server",
        "configuration"
      ],
      "concepts": [
        "port-binding",
        "auto-discovery",
        "metrics-export",
        "library-usage",
        "configuration-api"
      ],
      "severity": "medium",
      "userImpact": "Users embedding the Temporal library cannot easily discover the metrics port when using auto-bind, requiring manual workarounds or extra port management logic.",
      "rootCause": null,
      "proposedFix": "Add an API method to retrieve the bound metrics port after server instantiation or startup.",
      "workaround": "Manually find and assign a free port in the config instead of using auto-bind.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Have some easy way to fetch the chosen metrics port after instantiating / starting a dev server",
      "number": 5461,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:51.450Z"
    },
    {
      "summary": "Query handler returning incorrect time.Time values in query responses due to using time.Now() instead of workflow.Now() in workflow code. The StartedAt and FinishedAt pointer fields were being evaluated at query time rather than being captured at workflow execution time, causing non-deterministic values.",
      "category": "question",
      "subcategory": "workflow-query",
      "apis": [
        "SetQueryHandler"
      ],
      "components": [
        "query-handler",
        "workflow-execution",
        "serialization"
      ],
      "concepts": [
        "determinism",
        "replay",
        "time-handling",
        "pointers",
        "query-responses",
        "workflow-code"
      ],
      "severity": "low",
      "userImpact": "Users may observe incorrect timestamp values in query responses when using time.Now() instead of workflow.Now() in workflow code, leading to non-deterministic behavior across replays.",
      "rootCause": "User was calling time.Now() in workflow code instead of workflow.Now(), which is non-deterministic across replays and causes pointers to time.Time values to be re-evaluated on each query call.",
      "proposedFix": "Replace time.Now() with workflow.Now() in workflow code to ensure deterministic time handling.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "User confirmed the issue was in their workflow code, not in Temporal. Using workflow.Now() instead of time.Now() fixed the problem.",
      "related": [],
      "keyQuote": "This is a bug in your workflow code, you should not be using `time.Now` in workflow code since the result is not deterministic across replay.",
      "number": 5460,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:51.310Z"
    },
    {
      "summary": "The docker-compose-multirole.yaml example sometimes gets stuck on boot with temporal-history unable to reach the frontend service through nginx, resulting in health check failures with 204 responses lacking proper gRPC content-type headers.",
      "category": "bug",
      "subcategory": "docker-compose-setup",
      "apis": [],
      "components": [
        "docker-compose",
        "nginx",
        "health-check",
        "grpc"
      ],
      "concepts": [
        "startup-synchronization",
        "service-discovery",
        "health-check",
        "race-condition",
        "grpc-compatibility",
        "docker-networking"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably start the multirole Temporal cluster locally, experiencing intermittent startup failures (~20% chance) that require manual service restarts to resolve.",
      "rootCause": "Race condition in docker-compose startup where nginx and frontend services are not properly synchronized, causing temporal-history to receive invalid gRPC health check responses (204 No Content without Content-Type header) from nginx before frontend is ready.",
      "proposedFix": null,
      "workaround": "Restart temporal-frontend or temporal-frontend2 services manually, though this doesn't always resolve the issue.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Error: unable to health check temporal.api.workflowservice.v1.WorkflowService service: unexpected HTTP status code received from server: 204 (No Content); malformed header: missing HTTP content-type",
      "number": 5455,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:54.403Z"
    },
    {
      "summary": "PostgreSQL schema limits identifier lengths to 255 characters, but Go's len() function counts bytes not characters, causing validation issues with multi-byte characters. The request is to change maxIDLength validation to count characters/symbols instead of bytes.",
      "category": "bug",
      "subcategory": "identifier-validation",
      "apis": [],
      "components": [
        "server",
        "validation",
        "postgresql-schema"
      ],
      "concepts": [
        "identifier-length",
        "character-encoding",
        "multi-byte-characters",
        "validation",
        "postgresql-limits",
        "internationalization"
      ],
      "severity": "medium",
      "userImpact": "Users with multi-byte characters in identifiers face validation failures and database errors when maxIDLength is set to match PostgreSQL's 255-character limit.",
      "rootCause": "Go's len() function returns byte count rather than character count, leading to incorrect validation logic when identifiers contain multi-byte UTF-8 characters.",
      "proposedFix": "Change limit.maxIDLength validation to count symbols/characters rather than bytes.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "limit.maxIDLength in symbols rather than bytes.",
      "number": 5454,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:39.939Z"
    },
    {
      "summary": "Docker images for the v1.22.5 release were not published to Docker Hub. Users attempting to pull temporalio/admin-tools:1.22.5 and other images for this version encountered 'not found' errors.",
      "category": "bug",
      "subcategory": "release-artifacts",
      "apis": [],
      "components": [
        "docker-images",
        "release-process",
        "admin-tools"
      ],
      "concepts": [
        "docker-registry",
        "release-publishing",
        "image-availability",
        "artifact-distribution"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy Temporal 1.22.5 with Docker as the required container images are unavailable on Docker Hub.",
      "rootCause": "Release images were not built and published when the v1.22.5 tag was created, likely due to an oversight in the release process.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The maintainer rebuilt and published the missing Docker images after discovering the oversight.",
      "related": [],
      "keyQuote": "Looks like we forgot to release these when we tagged the release last week. I'm building them now",
      "number": 5451,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:38.960Z"
    },
    {
      "summary": "serviceerror.NotFound error messages expose SQL database implementation details instead of returning persistence-agnostic error messages, leaking internal server information to users.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "error-handling",
        "persistence-layer",
        "batch-operations"
      ],
      "concepts": [
        "error-message",
        "information-disclosure",
        "persistence-abstraction",
        "sql-internals",
        "batch-jobs"
      ],
      "severity": "medium",
      "userImpact": "Users see confusing SQL error messages that expose internal database implementation details, creating poor API boundaries and potentially revealing system internals.",
      "rootCause": "serviceerror.NotFound is returning raw database errors without wrapping them in persistence-agnostic messages.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed as duplicate of issue #4118",
      "related": [
        4118
      ],
      "keyQuote": "The error message for serviceerror.NotFound should return a persistence-unrelated error message.",
      "number": 5443,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:40.407Z"
    },
    {
      "summary": "When workers restart or crash, cached workflow histories are lost, causing increased latency for previously cached workflows. This feature request proposes a cache handover mechanism where workflows are recovered on other workers before new tasks are generated.",
      "category": "feature",
      "subcategory": "worker-cache-management",
      "apis": [],
      "components": [
        "worker",
        "workflow-cache",
        "history-cache"
      ],
      "concepts": [
        "cache-invalidation",
        "worker-restart",
        "graceful-shutdown",
        "cache-recovery",
        "latency-optimization",
        "workflow-distribution"
      ],
      "severity": "medium",
      "userImpact": "Users experience increased latency when workers restart due to loss of cached workflow histories, especially problematic during updates or maintenance.",
      "rootCause": "Worker restart invalidates all cached histories on that worker, requiring cache rebuilding on other workers before processing new tasks",
      "proposedFix": "Implement a cache handover mechanism during graceful worker shutdown (or crash recovery) that proactively recovers cached workflows on other workers before task generation",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Moving workflows that have a high history size from one worker to another when you have to kill the worker (to update for example) can be painful",
      "number": 5439,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:28.451Z"
    },
    {
      "summary": "The search-attributes API endpoint intermittently returns HTTP 503 errors instead of 200. Reproducing the issue requires refreshing the page 20-30 times, with errors occurring in the OperatorListSearchAttributes operation when namespace info retrieval fails.",
      "category": "bug",
      "subcategory": "search-attributes-api",
      "apis": [
        "ListSearchAttributes"
      ],
      "components": [
        "frontend",
        "operator-handler",
        "namespace-validation",
        "telemetry-interceptor"
      ],
      "concepts": [
        "api-reliability",
        "error-handling",
        "namespace-resolution",
        "service-availability",
        "intermittent-failures",
        "503-errors"
      ],
      "severity": "medium",
      "userImpact": "Users experience intermittent 503 Service Unavailable errors when querying search attributes, disrupting API availability and requiring page refreshes to recover.",
      "rootCause": "The OperatorListSearchAttributes operation fails with 'Unable to get namespace info' error, but the exact underlying cause is not logged with sufficient detail. The error originates in the operator_handler.go around lines 518-526.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        5473
      ],
      "keyQuote": "Unable to get namespace info with error: les - insufficient error context in logs to diagnose root cause",
      "number": 5436,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:27.534Z"
    },
    {
      "summary": "Data race detected in cluster metadata refresh loop during testing in v1.23.0-rc9. The race occurs between concurrent goroutines writing to and reading from cluster metadata without proper synchronization.",
      "category": "bug",
      "subcategory": "cluster-metadata",
      "apis": [],
      "components": [
        "cluster-metadata",
        "replication-stream-receiver",
        "synchronization"
      ],
      "concepts": [
        "data-race",
        "concurrency",
        "goroutine",
        "mutual-exclusion",
        "cluster-metadata",
        "refresh-loop"
      ],
      "severity": "high",
      "userImpact": "Users may experience unpredictable behavior or crashes in production environments due to concurrent access to cluster metadata without proper locking.",
      "rootCause": "Missing synchronization in metadataImpl.refreshClusterMetadata() when updating cluster info map, causing concurrent writes and reads from different goroutines.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Already fixed in main branch via PR #5317",
      "related": [
        5317
      ],
      "keyQuote": "Write at 0x00c0041c4b10 by goroutine 15520: runtime.mapassign_faststr() in metadataImpl.updateClusterInfoLocked() Previous read at 0x00c0041c4b10 by goroutine 16167",
      "number": 5432,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:28.514Z"
    },
    {
      "summary": "Security vulnerability in otelgrpc package (< 0.46.0) used by Temporal Server 1.22.4. The vulnerability was already fixed in the main branch but won't be included in the next 1.23 release, only in a followup 1.23.1 release.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "otelgrpc",
        "dependencies",
        "grpc-instrumentation"
      ],
      "concepts": [
        "security-vulnerability",
        "dependency-management",
        "version-upgrade",
        "opentelemetry"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal Server 1.22.4 and earlier are exposed to a known security vulnerability in the otelgrpc package until they upgrade.",
      "rootCause": "go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc version 0.42.0 contains a security vulnerability fixed in 0.46.0.",
      "proposedFix": "Update otelgrpc dependency to version 0.46.0 or later.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Library was already updated in PR #5396, scheduled for inclusion in Temporal 1.23.1 release.",
      "related": [
        5396
      ],
      "keyQuote": "the library was already updated 2 days ago... it should be part of a followup 1.23.1",
      "number": 5403,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:15.411Z"
    },
    {
      "summary": "After restarting Temporal clusters, old IP addresses remain in the cluster_membership table and are cached in memory, causing network packet drops with 'stale_or_unroutable_ip' errors. The old entries should be invalidated from both the database and connection cache within minutes rather than persisting for hours or days.",
      "category": "bug",
      "subcategory": "cluster-membership",
      "apis": [],
      "components": [
        "cluster-membership",
        "grpc-connection-cache",
        "service-discovery"
      ],
      "concepts": [
        "connection-caching",
        "stale-addresses",
        "cluster-restart",
        "membership-management",
        "network-connectivity",
        "cache-invalidation"
      ],
      "severity": "medium",
      "userImpact": "Network packet drops occur after cluster restarts due to stale IP addresses in memory and database, causing unnecessary traffic and potential connectivity issues.",
      "rootCause": "IP address to gRPC connection cache entries are never removed from memory even when addresses become unreachable. Old IP addresses persist in cluster_membership table without timely expiration.",
      "proposedFix": "Implement automatic invalidation of old IP addresses from both the in-memory connection cache and cluster_membership database after a few minutes of unreachability.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The old IPs should be invalidated from the cache/db after a few minutes (not hours or days).",
      "number": 5394,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:16.882Z"
    },
    {
      "summary": "User asks whether one workflow can trigger another workflow and access activity outputs from the triggered workflow. The inquiry also considers security and isolation approaches for multiple teams sharing the same Temporal server.",
      "category": "question",
      "subcategory": "workflow-orchestration",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-engine",
        "namespace",
        "activity-executor"
      ],
      "concepts": [
        "workflow-triggering",
        "inter-workflow-communication",
        "activity-results",
        "multi-team-isolation",
        "namespace-separation",
        "security"
      ],
      "severity": "low",
      "userImpact": "Users need clarity on whether workflows can orchestrate other workflows and how to safely share a Temporal server across teams.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Use separate namespaces for different teams to prevent accidental interference with activity and workflow names, or use separate servers for complete isolation.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Yes, it is possible to trigger workflows from within a workflow. Accessing activity results is slightly more difficult but still possible.",
      "number": 5372,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:14.798Z"
    },
    {
      "summary": "User reported that namespace retention settings were being ignored, with workflows still being deleted after 3 days despite updating retention to 31 days. The issue was resolved when the maintainer explained that retention timers are applied at workflow close time, so only workflows completing after the setting change receive the new retention period.",
      "category": "question",
      "subcategory": "namespace-configuration",
      "apis": [],
      "components": [
        "namespace-manager",
        "persistence",
        "cassandra"
      ],
      "concepts": [
        "retention",
        "workflow-lifecycle",
        "namespace-settings",
        "data-expiration",
        "configuration-propagation"
      ],
      "severity": "low",
      "userImpact": "Users may be confused about when retention setting changes take effect on running workflows.",
      "rootCause": "User misunderstanding: retention timer is applied at workflow completion time, not at configuration change time.",
      "proposedFix": null,
      "workaround": "To inspect old workflows with retention changes, adjust machine clock back before attaching temporal server/UI.",
      "resolution": "invalid",
      "resolutionDetails": "Determined to be user misunderstanding rather than a bug. The retention system is working as designed - retention timers are applied when workflows complete, not when settings change.",
      "related": [],
      "keyQuote": "The retention timer is applied at workflow close time, so only workflows that complete (or fail) after the setting was changed will get the new retention.",
      "number": 5370,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:02.811Z"
    },
    {
      "summary": "The `tctl workflow list` command with `ExecutionStatus=1` query returns inconsistent results when run repeatedly - sometimes showing an empty list and sometimes returning completed workflows that should not match the query. The issue is that completed workflows are appearing in results for a query that should only return running workflows.",
      "category": "bug",
      "subcategory": "tctl-cli",
      "apis": [],
      "components": [
        "tctl",
        "workflow-list",
        "query-engine",
        "execution-status-filter"
      ],
      "concepts": [
        "execution-status",
        "query-filtering",
        "consistency",
        "completed-workflows",
        "running-workflows"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably query for running workflows using tctl, as results are inconsistent and may include completed workflows that should be excluded.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When running tctl -n prd workflow list --pdt --query=\"ExecutionStatus=1\" multiple times, occasionally a set of completed WFs are returned",
      "number": 5364,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:04.102Z"
    },
    {
      "summary": "Resource exhaustion error codes lack granularity and have semantically misleading mappings. Three different errors map to BUSY_WORKFLOW reason, making it impossible to distinguish error types in metrics and alerting.",
      "category": "other",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "service",
        "history",
        "error-handling"
      ],
      "concepts": [
        "resource-exhaustion",
        "error-codes",
        "metrics",
        "alerting",
        "error-semantics"
      ],
      "severity": "medium",
      "userImpact": "Users cannot distinguish between different resource exhaustion scenarios in metrics and alerting, making it difficult to exclude non-exhaustion errors like WorkflowClosing from resource exhaustion alerts.",
      "rootCause": "Three semantically different errors (ErrWorkflowClosing, ErrConsistentQueryBufferExceeded, ErrResourceExhaustedBusyWorkflow) all map to the same RESOURCE_EXHAUSTED_CAUSE_BUSY_WORKFLOW reason code, creating ambiguity and preventing proper error handling and monitoring.",
      "proposedFix": "Introduce more granular reasons for service_errors_resource_exhausted to distinguish between different exhaustion scenarios.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "These errors are indistinguishable in metrics, so I cannot exclude ErrWorkflowClosing (which is valid in my workload) from alerts.",
      "number": 5352,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:04:01.801Z"
    },
    {
      "summary": "Schedule list and detail pages show inconsistent \"Upcoming runs\" and \"Interval\" values. The schedule functionality appears to work correctly, suggesting the issue is a data synchronization problem between two storage systems.",
      "category": "bug",
      "subcategory": "schedule-visibility",
      "apis": [],
      "components": [
        "ui",
        "visibility-store",
        "schedule-service"
      ],
      "concepts": [
        "schedule-metadata",
        "data-sync",
        "visibility",
        "memoization",
        "list-vs-detail-discrepancy"
      ],
      "severity": "medium",
      "userImpact": "Users see different schedule information when listing schedules versus viewing individual schedule details, causing confusion about actual schedule timing.",
      "rootCause": "Schedule metadata updates are not propagating to the visibility store; resolved by using advanced visibility instead of standard visibility on PostgreSQL",
      "proposedFix": "Configure advanced visibility on PostgreSQL (or Elasticsearch) instead of standard visibility, as schedule metadata in list results is not supported with standard visibility",
      "workaround": "Enable advanced visibility configuration as documented in https://docs.temporal.io/visibility",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by clarifying that standard visibility on PostgreSQL does not support schedule metadata in list results. Users need to configure advanced visibility instead.",
      "related": [],
      "keyQuote": "Schedule metadata in list results isn't supported with standard visibility. You should configure advanced visibility on postgres",
      "number": 5346,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:51.074Z"
    },
    {
      "summary": "User questions whether there are special considerations for accessing resources (activities/workflows) across different Temporal namespaces within the same server, particularly when activity outputs from one namespace need to feed into another namespace's activities.",
      "category": "question",
      "subcategory": "cross-namespace-communication",
      "apis": [],
      "components": [
        "namespace",
        "activity",
        "workflow"
      ],
      "concepts": [
        "cross-namespace",
        "resource-access",
        "data-flow",
        "namespace-isolation",
        "multi-namespace-architecture"
      ],
      "severity": "low",
      "userImpact": "Users need clarity on whether Temporal namespaces isolate resources like Kubernetes namespaces or allow cross-namespace activity/workflow invocation.",
      "rootCause": null,
      "proposedFix": "Cross namespace invocation will be supported by nexus service",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "would we run into any issues if the inputs for activity_2 depends on outputs from activity_1?",
      "number": 5324,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:49.084Z"
    },
    {
      "summary": "Activity context cancellation doesn't properly propagate to the activity function. The activity continues executing even after workflow.WithCancel() is called because the context.Done() event is not being received in the activity.",
      "category": "bug",
      "subcategory": "activity-cancellation",
      "apis": [
        "ExecuteActivity",
        "WithCancel",
        "WithActivityOptions"
      ],
      "components": [
        "activity-executor",
        "context-propagation",
        "cancellation-handling"
      ],
      "concepts": [
        "context-cancellation",
        "activity-heartbeat",
        "context-done-event",
        "activity-lifecycle",
        "signal-propagation"
      ],
      "severity": "medium",
      "userImpact": "Activities continue executing after cancellation is requested, potentially causing resource waste and unexpected behavior in workflows.",
      "rootCause": "Activity context cancellation requires heartbeat mechanism; without heartbeats, the activity doesn't receive the cancellation signal.",
      "proposedFix": "Activity must use heartbeat mechanism to detect context cancellation, as documented in Temporal activity documentation.",
      "workaround": "Add heartbeat calls in the activity loop to enable cancellation detection.",
      "resolution": "invalid",
      "resolutionDetails": "Resolved as invalid - this is expected behavior per Temporal documentation. Activities need to use heartbeat to detect cancellation.",
      "related": [],
      "keyQuote": "Activity needs to do heartbeat to know that context is cancelled.",
      "number": 5316,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:51.498Z"
    },
    {
      "summary": "User asks how to enable the Worker versioning feature when starting a local Temporal server using the CLI command `temporal server start-dev`, as it is disabled by default.",
      "category": "question",
      "subcategory": "server-configuration",
      "apis": [],
      "components": [
        "server",
        "cli",
        "worker-versioning"
      ],
      "concepts": [
        "worker-versioning",
        "server-startup",
        "feature-enablement",
        "local-development",
        "cli-configuration"
      ],
      "severity": "low",
      "userImpact": "Users cannot test Worker versioning functionality locally without knowing how to enable it in the development server.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "By default the Worker versioning feature of the server is disabled, how can I start the local server with the Worker versioning API enabled.",
      "number": 5315,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:36.495Z"
    },
    {
      "summary": "Allow workflows to optionally omit activity input/arguments in the activity task scheduled event to reduce storage costs, similar to local activity options. This is particularly useful since the input is primarily for debugging rather than replay.",
      "category": "feature",
      "subcategory": "activity-input-optimization",
      "apis": [
        "ExecuteActivity",
        "LocalActivityOptions"
      ],
      "components": [
        "activity-executor",
        "history-event",
        "task-scheduling",
        "server-event-storage"
      ],
      "concepts": [
        "storage-optimization",
        "activity-input",
        "history-replay",
        "debugging",
        "cost-reduction",
        "local-activity",
        "sideffect-management"
      ],
      "severity": "medium",
      "userImpact": "Users can reduce storage costs for workflows with many activities by optionally omitting activity input from scheduled events, addressing a practical scalability concern.",
      "rootCause": "Activity input is retained in task scheduled events primarily for debugging purposes, but this increases storage overhead for large-scale deployments.",
      "proposedFix": "Implement optional input omission in activity task scheduled events similar to LocalActivityOptions, requiring server-side support to still pass input to activity workers.",
      "workaround": "Use local activities, though they have tradeoffs with failure handling and retry management that can cause history bloat with timer events.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Allow workflow to not include argument/input in the activity task scheduled event. Just like local activity options",
      "number": 5310,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:39.750Z"
    },
    {
      "summary": "Users cannot specify S3 storage class (e.g., STANDARD_IA) in S3 archival configurations, preventing adoption of S3 archival for organizations with storage class policies. Request to add storage_class option or support S3 connection kwargs.",
      "category": "feature",
      "subcategory": "archival-s3",
      "apis": [],
      "components": [
        "archival",
        "s3-archival",
        "archival-configuration"
      ],
      "concepts": [
        "storage-class",
        "s3-configuration",
        "archival-policy",
        "connection-options",
        "cost-optimization"
      ],
      "severity": "medium",
      "userImpact": "Organizations with storage class policies cannot use S3 archival and must choose alternative archival strategies or implement custom solutions.",
      "rootCause": "S3 archival configuration lacks support for specifying storage class parameters or accepting arbitrary S3 connection kwargs.",
      "proposedFix": "Add storage_class as an S3 archival option, or support additional S3 connection kwargs that can be passed through to the S3 connection.",
      "workaround": "Use filestore archival instead, or implement a custom archiver.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We have a policy of using only STANDARD_IA storage class. Currently, there is no ability to specify the storage class to use in S3 Archival configurations.",
      "number": 5306,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:38.687Z"
    },
    {
      "summary": "When signaling a closed workflow, the API throws an error without indicating whether the workflow is truly non-existent or simply closed. The request is to include WorkflowStatus in the error response to allow applications to handle these two cases differently without requiring a separate DescribeWorkflowExecution API call.",
      "category": "feature",
      "subcategory": "workflow-signaling",
      "apis": [
        "SignalWorkflowExecution",
        "DescribeWorkflowExecution"
      ],
      "components": [
        "signal-handler",
        "workflow-execution",
        "error-response"
      ],
      "concepts": [
        "error-handling",
        "workflow-status",
        "api-efficiency",
        "closed-workflows",
        "signal-semantics"
      ],
      "severity": "medium",
      "userImpact": "Applications must make additional API calls to distinguish between non-existent and closed workflows when signaling, increasing latency and code complexity.",
      "rootCause": null,
      "proposedFix": "Return WorkflowStatus in the error response when attempting to signal a closed workflow.",
      "workaround": "Make a separate DescribeWorkflowExecution API call to determine workflow status before or after signaling.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1969
      ],
      "keyQuote": "The error should be able to return \"WorkflowStatus\" inside.",
      "number": 5303,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:27.305Z"
    },
    {
      "summary": "Request to add native YDB (Yandex Database) support as a storage backend for Temporal, allowing developers to leverage YDB's geographic replication, automatic scaling, and fault tolerance features. While a community fork exists, the request is for out-of-the-box YDB support with selectable database storage options.",
      "category": "feature",
      "subcategory": "database-support",
      "apis": [],
      "components": [
        "persistence",
        "storage-backend",
        "database-driver"
      ],
      "concepts": [
        "database-integration",
        "scalability",
        "fault-tolerance",
        "replication",
        "storage-backend-selection"
      ],
      "severity": "low",
      "userImpact": "YDB users interested in building long-lived applications with Temporal would be able to use native YDB integration instead of relying on community forks or alternative storage solutions.",
      "rootCause": null,
      "proposedFix": "Implement YDB as a supported persistence backend option in Temporal, similar to existing PostgreSQL, MySQL, and other database drivers, with proper integration into the Temporal SDK.",
      "workaround": "Use the community fork at https://github.com/yandex/temporal-over-ydb, though it lacks full compatibility with the main Temporal codebase.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This fork is not a 100% functional solution, it would be great to see YDB support in Temporal out of the box. With the option to select DB as storage.",
      "number": 5302,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:27.032Z"
    },
    {
      "summary": "Feature request to add database connectivity checks to Kubernetes liveness probes so that Temporal pods restart automatically when database credentials or certificates are renewed, preventing silent failures.",
      "category": "feature",
      "subcategory": "deployment-health-checks",
      "apis": [],
      "components": [
        "liveness-probe",
        "worker",
        "frontend",
        "matching",
        "history"
      ],
      "concepts": [
        "database-connectivity",
        "kubernetes",
        "pod-restart",
        "credential-rotation",
        "health-check",
        "certificate-renewal"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal in Kubernetes with rotating database credentials experience silent failures and manual pod restarts instead of automatic recovery.",
      "rootCause": "Liveness probes do not verify database connectivity, so pods with invalid credentials continue running and log errors without triggering automatic restart.",
      "proposedFix": "Implement database connectivity checks within the existing liveness probe to detect authentication failures and trigger pod restarts.",
      "workaround": "Manually restart pods or use tctl namespace list, though tctl cannot connect to localhost even with correct port configuration.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Check DB connectivity in liveness probe. If the DB password is changed the temporal pods would restart and load the new secret.",
      "number": 5299,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:25.583Z"
    },
    {
      "summary": "User asks whether child workflows can inherit search attributes from their parent workflow. When starting a child workflow with search attributes using Go SDK, the child workflow does not appear to inherit those search attributes from the parent context.",
      "category": "question",
      "subcategory": "child-workflows",
      "apis": [
        "GetChildWorkflowOptions",
        "WithChildOptions"
      ],
      "components": [
        "workflow-context",
        "child-workflow-executor",
        "search-attributes"
      ],
      "concepts": [
        "search-attributes",
        "context-inheritance",
        "workflow-hierarchy",
        "child-workflows",
        "metadata-propagation"
      ],
      "severity": "low",
      "userImpact": "Users attempting to propagate search attributes to child workflows are unable to do so, requiring manual reimplementation of search attributes for nested workflows.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "when that workflow then spawns another workflow, I was hoping the SearchAttributes would be inherited",
      "number": 5298,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:14.917Z"
    },
    {
      "summary": "Docker images for version 1.22.4 were referenced in release notes but were not initially published to DockerHub. The issue was quickly resolved when the images were published.",
      "category": "bug",
      "subcategory": "release-management",
      "apis": [],
      "components": [
        "docker-images",
        "release-process"
      ],
      "concepts": [
        "release-management",
        "docker-publishing",
        "versioning",
        "distribution"
      ],
      "severity": "medium",
      "userImpact": "Users following the release notes could not pull the 1.22.4 Docker images from DockerHub, blocking their ability to deploy that version.",
      "rootCause": "Docker images for 1.22.4 were not published to DockerHub at the time the release notes were published.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The Docker images were published to DockerHub, resolving the discrepancy between the release notes and available images.",
      "related": [],
      "keyQuote": "Sorry, my bad. They are published now.",
      "number": 5296,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:15.787Z"
    },
    {
      "summary": "Request for built-in Slack notification integration in Temporal Web UI to send custom messages on workflow events (failures, completion, etc.) without requiring custom SDK metrics implementation.",
      "category": "feature",
      "subcategory": "notifications",
      "apis": [],
      "components": [
        "web-ui",
        "metrics",
        "workflow-events"
      ],
      "concepts": [
        "notifications",
        "messaging-platform",
        "observability",
        "alerting",
        "slack-integration"
      ],
      "severity": "medium",
      "userImpact": "Users currently must implement custom SDK metrics for workflow notifications; built-in Slack integration would reduce development overhead.",
      "rootCause": null,
      "proposedFix": "Add Slack menu to Temporal Web UI with configurable custom message templates for workflow events, or export flexible Prometheus metrics with WorkflowID and WorkflowType.",
      "workaround": "Implement custom SDK metrics to create notifications externally.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would be great if there is a Slack menu in temporal web ui, allowing us to integrate with Slack and make custom messages",
      "number": 5286,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:14.112Z"
    },
    {
      "summary": "User asks about upgrading from the 'postgres' driver to 'postgres12' driver in Temporal Server and whether a database schema update is required. Concern about potential downtime on a 50GB production database.",
      "category": "question",
      "subcategory": "database-migration",
      "apis": [],
      "components": [
        "database",
        "persistence",
        "postgresql-driver"
      ],
      "concepts": [
        "database-upgrade",
        "schema-migration",
        "driver-compatibility",
        "production-downtime",
        "data-preservation"
      ],
      "severity": "medium",
      "userImpact": "Users need guidance on safely upgrading PostgreSQL drivers in production environments without causing downtime or data loss.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Maintainer confirmed that no schema update is needed between PostgreSQL versions, answering the user's main concern.",
      "related": [],
      "keyQuote": "There is no schema update needed between PostgreSQL versions.",
      "number": 5260,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:02.601Z"
    },
    {
      "summary": "Vulnerability in modernc.org/libc-v1.24.1 (transitive dependency of modernc.org/SQLite) reported as CVE-2020-28928, a buffer overflow in musl libc wcsnrtombs function with CVSS score 5.5.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "go-modules",
        "security"
      ],
      "concepts": [
        "vulnerability",
        "buffer-overflow",
        "dependency-chain",
        "security-patch",
        "cvss-score"
      ],
      "severity": "medium",
      "userImpact": "Users are exposed to a medium severity buffer overflow vulnerability in a transitive dependency that could affect availability.",
      "rootCause": "musl libc wcsnrtombs function mishandles particular combinations of destination buffer size and source character limit, causing invalid write access",
      "proposedFix": "Upgrade modernc.org/libc to version 1.2.2-1 or later where the vulnerability is fixed in musl",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by upgrading the vulnerable transitive dependency modernc.org/libc to a patched version",
      "related": [],
      "keyQuote": "In musl libc through 1.2.1, wcsnrtombs mishandles particular combinations of destination buffer size and source character limit",
      "number": 5257,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:04.714Z"
    },
    {
      "summary": "User requests ability to schedule workflows to run at sunrise or sunset for specific GPS coordinates, with offset parameters (e.g., SUNSET+10m). This is needed for IoT platforms to trigger workflows based on astronomical events.",
      "category": "feature",
      "subcategory": "scheduling",
      "apis": [
        "Schedule"
      ],
      "components": [
        "scheduler",
        "temporal-server"
      ],
      "concepts": [
        "astronomical-scheduling",
        "time-based-triggers",
        "location-based-events",
        "workflow-scheduling",
        "cron-alternatives"
      ],
      "severity": "low",
      "userImpact": "Users managing IoT workflows cannot schedule tasks based on sunrise/sunset times at specific locations without external computation and manual timestamp management.",
      "rootCause": null,
      "proposedFix": "Expose an option in the schedules feature to supply a list of absolute timestamps that users can compute themselves based on GPS coordinates and astronomical calculations.",
      "workaround": "Use the schedules feature with a list of calendar specs, or compute sunrise/sunset times externally and supply absolute timestamps to the scheduler.",
      "resolution": "wontfix",
      "resolutionDetails": "Deemed too niche for server-level support. Solution deferred to user-side computation with timestamps supplied to the schedules feature.",
      "related": [],
      "keyQuote": "This is an interesting idea but it seems like very niche functionality, and I don't think we want to build that into the server.",
      "number": 5251,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:03:04.552Z"
    },
    {
      "summary": "Deleted namespaces sometimes remain in Registered state in the namespace list and cannot be deleted again, requiring either proper state reflection or removal after deletion.",
      "category": "bug",
      "subcategory": "namespace-management",
      "apis": [],
      "components": [
        "namespace-registry",
        "namespace-cache",
        "cli"
      ],
      "concepts": [
        "namespace-state",
        "state-consistency",
        "delete-operation",
        "cache-propagation",
        "multi-node-setup"
      ],
      "severity": "medium",
      "userImpact": "Users cannot delete namespaces that appear in Registered state after previous deletions, causing operational issues.",
      "rootCause": "Namespace cache refresh interval and state propagation delay in multi-node setup causing stale state to be reported.",
      "proposedFix": null,
      "workaround": "Wait for namespace cache refresh interval (default 10s) to propagate namespace changes in multi-node setup.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in 1.25.0 release through major refactoring of namespace registry in August 2024.",
      "related": [],
      "keyQuote": "I can't repro this issue anymore on the current `1.25.0` release. I believe it is because on Aug 2024 there was a major refactoring of namespace registry.",
      "number": 5250,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:52.411Z"
    },
    {
      "summary": "Retry policy is not automatically copied when using ContinueAsNewWorkflowExecution, causing the continued workflow to lose the retry configuration from the previous run. The system should merge the retry policy from the original workflow into the continued workflow when not explicitly set.",
      "category": "bug",
      "subcategory": "workflow-continuation",
      "apis": [
        "ContinueAsNewWorkflowExecutionCommandAttributes"
      ],
      "components": [
        "history",
        "mutable-state",
        "workflow-execution"
      ],
      "concepts": [
        "retry-policy",
        "continue-as-new",
        "workflow-state",
        "policy-inheritance",
        "configuration-merge"
      ],
      "severity": "medium",
      "userImpact": "Workflows that use ContinueAsNew lose their retry policy, potentially causing them to fail without retrying on subsequent executions.",
      "rootCause": "The merge logic in mutable_state_impl.go:1630 does not properly copy the retry policy from the original workflow to the continued workflow execution.",
      "proposedFix": "Implement retry policy copying in ContinueAsNewWorkflowExecutionCommandAttributes similar to how other workflow attributes are merged.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When a retry policy is not set in the `ContinueAsNewWorkflowExecutionCommandAttributes` continued workflow should have the same policy as the previous workflow run.",
      "number": 5249,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:51.589Z"
    },
    {
      "summary": "Prometheus metrics histogram buckets are missing intermediate boundaries, only showing le=\"1\" and le=\"+Inf\" instead of the expected multiple boundaries. The user discovered this was caused by a minimal default histogram configuration with only one boundary defined.",
      "category": "bug",
      "subcategory": "metrics-prometheus",
      "apis": [],
      "components": [
        "metrics",
        "prometheus",
        "tally"
      ],
      "concepts": [
        "histogram-buckets",
        "prometheus-metrics",
        "observability",
        "configuration",
        "metric-boundaries"
      ],
      "severity": "medium",
      "userImpact": "Prometheus histogram metrics produce incomplete data with only two buckets, making latency analysis and visualization in Grafana unreliable.",
      "rootCause": "Default histogram boundaries configuration in Temporal 1.22.3 only includes a single boundary value of 1, resulting in incomplete histogram buckets.",
      "proposedFix": null,
      "workaround": "Configure additional histogram boundaries in the metrics prometheus configuration instead of relying on defaults.",
      "resolution": "invalid",
      "resolutionDetails": "Issue reporter identified the cause as a configuration problem (defaultHistogramBoundaries set to [1] only) rather than a bug in Temporal itself.",
      "related": [],
      "keyQuote": "Look what I found while viewing the config for something else: metrics: prometheus: defaultHistogramBoundaries: - 1",
      "number": 5247,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:52.127Z"
    },
    {
      "summary": "ScheduleClient.List API is missing a SearchAttributes parameter that exists in ScheduleOptions for creating schedules. The List API should support filtering schedules by SearchAttributes to match the capability described in the create API.",
      "category": "feature",
      "subcategory": "schedules-api",
      "apis": [
        "ScheduleClient.List",
        "ScheduleListOptions",
        "ScheduleOptions"
      ],
      "components": [
        "schedules",
        "list-api",
        "search-attributes"
      ],
      "concepts": [
        "api-consistency",
        "search-filtering",
        "query-capability",
        "schedule-management",
        "indexed-data"
      ],
      "severity": "medium",
      "userImpact": "Users cannot filter schedules by SearchAttributes when listing, limiting the usefulness of indexed schedule metadata.",
      "rootCause": "API inconsistency between ScheduleOptions (which supports SearchAttributes) and ScheduleListOptions (which does not).",
      "proposedFix": "Add SearchAttributes parameter to ScheduleListOptions and implement filtering logic in ScheduleClient.List API.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Either the SearchAttributes field description is incorrect or it should be one of the parameters in ScheduleListOptions.SearchAttributes field.",
      "number": 5245,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:38.652Z"
    },
    {
      "summary": "Binaries for the 1.22.3 release failed to build due to a missing cherry-picked fix in the goreleaser workflow. The issue was resolved in the 1.22.4 release which included the binaries.",
      "category": "bug",
      "subcategory": "release-build",
      "apis": [],
      "components": [
        "goreleaser",
        "release-automation",
        "binary-distribution"
      ],
      "concepts": [
        "build-failure",
        "release-process",
        "binary-distribution",
        "ci-cd"
      ],
      "severity": "medium",
      "userImpact": "Users were unable to download pre-built binaries for the 1.22.3 release and had to find alternative ways to obtain Temporal Server binaries.",
      "rootCause": "A fix from the main branch was not cherry-picked to the release branch before running goreleaser.",
      "proposedFix": null,
      "workaround": "Use the official Docker images or the Temporal CLI instead of the binaries, or build custom Docker images using binaries from the next release.",
      "resolution": "fixed",
      "resolutionDetails": "The issue was resolved by releasing v1.22.4 with the binaries included.",
      "related": [],
      "keyQuote": "We just release v1.22.4 which included the binaries.",
      "number": 5223,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:40.328Z"
    },
    {
      "summary": "During rolling restarts, ringpop ownership changes for task queue partitions may not propagate synchronously to history, frontend, and matching services, causing tasks to sit unprocessed in the database. The matching service needs to subscribe to ringpop events and proactively unload task queues when ownership is lost.",
      "category": "bug",
      "subcategory": "task-queue-ownership",
      "apis": [],
      "components": [
        "matching-service",
        "ringpop",
        "history-service",
        "frontend-service",
        "task-queue-manager"
      ],
      "concepts": [
        "task-queue-membership",
        "ownership-transfer",
        "rolling-restart",
        "event-propagation",
        "long-poll",
        "synchronization"
      ],
      "severity": "high",
      "userImpact": "Tasks become stuck and are not processed during rolling restarts when task queue ownership changes, causing increased schedule_to_start latency and delayed task execution.",
      "rootCause": "Ringpop ownership changes propagate asynchronously to matching, history, and frontend services, causing a state mismatch where history pushes tasks to the new owner but the old matching pod still holds long-poll connections and the new pod doesn't know about pending tasks.",
      "proposedFix": "Matching service should subscribe to ringpop events and proactively unload task queue managers when it detects loss of ownership for a partition.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Matching service was updated to subscribe to ringpop events and proactively unload task queue managers on ownership loss.",
      "related": [],
      "keyQuote": "Matching service need to subscribe to ringpop events, and proactively unload task queue manager if it detect lost of ownership.",
      "number": 5198,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:39.918Z"
    },
    {
      "summary": "Refactor metrics recording to use the new metricDefinition.With() pattern instead of the redundant metrics.Handler approach, eliminating duplicate information and safety risks in metric recording calls.",
      "category": "feature",
      "subcategory": "metrics-refactoring",
      "apis": [],
      "components": [
        "metrics-handler",
        "metric-definitions"
      ],
      "concepts": [
        "code-refactoring",
        "metrics-recording",
        "api-safety",
        "redundancy-elimination"
      ],
      "severity": "low",
      "userImpact": "Improves code maintainability and safety for developers working with Temporal metrics by reducing redundant and error-prone metric recording patterns.",
      "rootCause": "The metrics.Handler API design required redundant specification of metric type (e.g., 'Histogram') alongside metric definition objects that already contained this information.",
      "proposedFix": "Migrate all metrics.metricDefinition usages to the new metricDefinition.With() pattern introduced in PR #5187, which eliminates redundancy by recording metrics directly from the definition object.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Completed via PR #5236 which migrated all usage of metrics.metricDefinition objects to the new recording style.",
      "related": [
        5187,
        5236
      ],
      "keyQuote": "Redundant and Unsafe: 'Histogram' is specified despite knowing 'myHisto' is a histogram. There's also a risk if 'myHisto.Name()' refers to a metric definition that is not a histogram.",
      "number": 5189,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:28.187Z"
    },
    {
      "summary": "When a workflow completes before pending update requests are processed, clients hang instead of receiving failure notifications. The server should fail incomplete updates and notify callers, and the same logic should apply to terminated workflows.",
      "category": "bug",
      "subcategory": "workflow-updates",
      "apis": [],
      "components": [
        "workflow-engine",
        "update-handler",
        "client-notification"
      ],
      "concepts": [
        "workflow-completion",
        "pending-updates",
        "client-notification",
        "error-handling",
        "workflow-termination"
      ],
      "severity": "high",
      "userImpact": "Clients sending update requests to workflows that complete or terminate before the update is processed experience hangs instead of receiving timely error notifications.",
      "rootCause": "The server does not properly fail pending update requests and notify clients when a workflow completes or is terminated with incomplete updates.",
      "proposedFix": "Implement logic to fail incomplete updates and notify the caller when a workflow completes or terminates with pending update requests.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Closed by PR #5848 with tests added in PR #5854",
      "related": [
        5848,
        5854
      ],
      "keyQuote": "If a client sends an update request to a workflow, but the workflow completes before the update is complete the client hangs.",
      "number": 5174,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:28.572Z"
    },
    {
      "summary": "WorkflowTaskFailure validation errors lack command context, making it unclear which activity or command caused the failure. The feature request asks to include activity ID and type information in error messages.",
      "category": "feature",
      "subcategory": "error-messages",
      "apis": [],
      "components": [
        "worker",
        "validation",
        "error-handling"
      ],
      "concepts": [
        "validation",
        "error-messages",
        "activity-identification",
        "debugging",
        "command-context"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily identify which activity or command caused validation errors, hindering debugging and troubleshooting.",
      "rootCause": "WorkflowTaskFailure validation errors do not include command context (activity ID, type) in the error message.",
      "proposedFix": "Include activity ID and type information in WorkflowTaskFailure error messages for validation errors.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Include information about activity id and type into the error message.",
      "number": 5172,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:28.608Z"
    },
    {
      "summary": "Invalid task tokens passed to activityCompletionClient.complete() cause a timeout instead of an immediate error. The server correctly rejects the token but returns UNKNOWN status code, which all SDKs treat as retryable, resulting in multi-minute timeouts.",
      "category": "bug",
      "subcategory": "activity-completion",
      "apis": [
        "activityCompletionClient.complete"
      ],
      "components": [
        "grpc-client",
        "activity-completion",
        "grpc-retry",
        "error-handling"
      ],
      "concepts": [
        "timeout",
        "retry",
        "invalid-token",
        "grpc-status",
        "error-response"
      ],
      "severity": "high",
      "userImpact": "Users experience long timeouts (several minutes) when accidentally using invalid task tokens instead of getting immediate errors, degrading debugging experience.",
      "rootCause": "Server returns UNKNOWN gRPC status code for invalid task tokens. All SDKs treat UNKNOWN as retryable, causing retry loops until timeout instead of failing immediately.",
      "proposedFix": "Server should return a different status code like INVALID_ARGUMENTS for invalid task tokens instead of UNKNOWN to prevent retryable behavior.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the server realized the invalid task token and returned the error immediately to the client. However, by design the grpcRetryer implementation catches this error, keeps retrying until expiration",
      "number": 5171,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:17.770Z"
    },
    {
      "summary": "Allow configuration of JWT audience (aud) claim validation in Temporal's authorization config. Currently, audience validation is only possible with custom builds; this feature requests exposing it via the docker config file alongside existing JWTClaimMapper and authorizer settings.",
      "category": "feature",
      "subcategory": "authorization-jwt",
      "apis": [],
      "components": [
        "authorization",
        "jwt-claim-mapper",
        "docker-config"
      ],
      "concepts": [
        "jwt-validation",
        "audience-claim",
        "authentication",
        "security",
        "enterprise-oauth2",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Enables enterprises using JWT-based authentication (e.g., Azure AD) to validate audience claims via configuration without requiring custom Temporal builds.",
      "rootCause": "The audienceGetter is not configurable via docker config file and defaults to being undefined, skipping audience validation unless a custom build is used.",
      "proposedFix": "Extend the global:authorization section of docker config to allow specifying expected audience values for JWT validation.",
      "workaround": "Build a custom Temporal binary with a custom audienceGetter implementation.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This security feature should be achievable using configuration file and without custom Temporal builds.",
      "number": 5156,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:15.655Z"
    },
    {
      "summary": "Activity cancellation notifications have excessive latency due to the heartbeat suppression algorithm, which only notifies at the 80% timeout threshold. The request is for a low-overhead mechanism to deliver cancellation notifications immediately, either via a non-heartbeat-dependent API like isCanceled() or via server-side push notifications to the worker.",
      "category": "feature",
      "subcategory": "activity-cancellation",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "heartbeat-suppression",
        "cancellation-handler"
      ],
      "concepts": [
        "cancellation",
        "notification-latency",
        "heartbeat-optimization",
        "push-notification",
        "real-time-synchronous",
        "activity-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Users with long-running activities requiring quick cancellation confirmation must use aggressive heartbeat settings, incurring significant overhead, or accept 15-20 second cancellation notification delays.",
      "rootCause": "The heartbeat suppression algorithm only delivers ActivityCancelled exception at 80% of the heartbeat timeout threshold, creating a minimum latency floor regardless of heartbeat frequency.",
      "proposedFix": "Implement a server-side push-based notification mechanism from the server to the worker to proactively notify activities of cancellation without waiting for heartbeat requests, or provide an isCanceled() API not subject to heartbeat optimizations.",
      "workaround": "Use more aggressive heartbeat strategy (e.g., 1-5 second intervals) but this incurs significant overhead for activities with rare cancellations.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the server needs to somehow notify the worker that the activity is cancelled proactively without waiting for the worker to send a heartbeat request",
      "number": 5135,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:17.088Z"
    },
    {
      "summary": "Operators need visibility into workflows that fail during batch operations (especially batch reset). Currently there's no way to retrieve a list of failed workflows, making it difficult to track job status and failures.",
      "category": "feature",
      "subcategory": "batch-operations",
      "apis": [],
      "components": [
        "batch-operations",
        "workflow-reset",
        "visibility"
      ],
      "concepts": [
        "batch-processing",
        "failure-tracking",
        "operator-visibility",
        "job-status",
        "search-attributes"
      ],
      "severity": "medium",
      "userImpact": "Operators cannot easily identify and track which workflows have failed during batch operations, hindering operational visibility and troubleshooting.",
      "rootCause": null,
      "proposedFix": "Either tag workflows with search attributes for post-operation filtering via visibility API, or return a list of failed workflows as part of the batch operation result.",
      "workaround": "Failed workflowIDs are currently logged, which may provide a temporary solution.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Operator will like to know the status of the job and if the operation fails for any workflow. And if the operation does fail, a list of those failed workflows.",
      "number": 5133,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:03.232Z"
    },
    {
      "summary": "Temporal caches RSA keys from Azure AD key source URIs and does not automatically refresh them when keys are rotated. Users need a way for Temporal to periodically fetch updated keys to handle key rotation gracefully.",
      "category": "bug",
      "subcategory": "authentication-key-rotation",
      "apis": [],
      "components": [
        "authorization",
        "token-key-provider",
        "jwt-validation"
      ],
      "concepts": [
        "key-rotation",
        "jwt-authentication",
        "azure-ad",
        "certificate-refresh",
        "token-validation"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal with Azure AD authentication cannot automatically handle key rotation events, requiring manual server restarts or configuration updates to accept new keys.",
      "rootCause": "The defaultTokenKeyProvider caches RSA keys from keySourceURIs but the periodic refresh callback was not implemented or not functioning properly.",
      "proposedFix": "Implement a timer callback in defaultTokenKeyProvider that periodically calls updateKeys() at intervals specified by refreshInterval to fetch latest keys from the key source URIs.",
      "workaround": "Manually update the Temporal configuration and redeploy the cluster when Azure AD keys are rotated.",
      "resolution": "fixed",
      "resolutionDetails": "A timer-based callback mechanism was implemented to periodically refresh the token keys at configured intervals, allowing automatic handling of key rotation without manual intervention.",
      "related": [],
      "keyQuote": "Temporal caches the RSA keys returned from 'keySourceURIs'. It looks like the callback should periodically refresh the key",
      "number": 5127,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:05.313Z"
    },
    {
      "summary": "A concurrent map read/write panic occurs in tchannel-go's Register function when handlers are registered while the channel is simultaneously processing requests. The bug stems from the handlers map being read by active request handlers while still being written to during the registration loop.",
      "category": "bug",
      "subcategory": "concurrency",
      "apis": [],
      "components": [
        "tchannel-go",
        "membership-monitor",
        "ringpop",
        "handler-registration"
      ],
      "concepts": [
        "concurrent-access",
        "race-condition",
        "map-safety",
        "goroutine-synchronization",
        "initialization-order",
        "channel-lifecycle"
      ],
      "severity": "high",
      "userImpact": "Server fails with a fatal concurrent map read/write panic during initialization or under concurrent load when handlers are registered, causing service startup failures.",
      "rootCause": "The tchannel-go Register function writes to the handlers map inside a loop while calling registrar.Register(handler, m) for each handler, which exposes the incomplete map to concurrent reads from active request handlers before all writes complete.",
      "proposedFix": "Complete all map assignments before calling registrar.Register() to ensure the handlers map is fully populated and stable before any handler can be invoked.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was diagnosed as a bug in the tchannel-go library where handler registration writes to a map that concurrent request handlers read from. The fix involves deferring registrar.Register calls until after all map assignments are complete.",
      "related": [
        5055
      ],
      "keyQuote": "The first goroutine is someone calling one of the registered handlers, which looks into the handlers map, and the other goroutine is concurrently writing to the map still registering handlers.",
      "number": 5123,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:02:04.326Z"
    },
    {
      "summary": "Eager workflow start task uses generic search attribute names (e.g., `Keyword03`) instead of proper custom attribute names that are used in polling workflow tasks. This inconsistency occurs when starting a workflow with custom search attributes enabled.",
      "category": "bug",
      "subcategory": "workflow-start",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-start",
        "search-attributes",
        "task-creation"
      ],
      "concepts": [
        "search-attributes",
        "eager-start",
        "attribute-mapping",
        "task-consistency"
      ],
      "severity": "medium",
      "userImpact": "Users cannot rely on consistent search attribute naming when using eager workflow start with custom search attributes, breaking compatibility with polling-based workflows.",
      "rootCause": "Eager workflow start task generation is not using the proper custom attribute name mapping when populating search attributes in the gRPC response.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #5124",
      "related": [
        5124
      ],
      "keyQuote": "Eager workflow start task has search attribute like `Keyword03`, whereas the polling workflow task has the proper attribute name.",
      "number": 5119,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:51.270Z"
    },
    {
      "summary": "User encountered an import error when following the Go SDK tutorial, unable to import go.temporal.io/sdk/worker package. The issue appears to be a support question rather than a reproducible bug, as the maintainer could not replicate it following the exact same tutorial steps.",
      "category": "question",
      "subcategory": "sdk-setup",
      "apis": [],
      "components": [
        "sdk-go",
        "module-imports"
      ],
      "concepts": [
        "dependency-management",
        "go-modules",
        "tutorial-setup",
        "import-errors"
      ],
      "severity": "low",
      "userImpact": "Users following the official tutorial encounter import errors that prevent them from getting started with the Go SDK.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Maintainer could not reproduce the issue when following the exact tutorial steps. Suggested user seek help via community channels instead, indicating this was likely a user environment issue or misunderstanding rather than a product bug.",
      "related": [],
      "keyQuote": "Cannot repo. Followed https://learn.temporal.io/getting_started/go/hello_world_in_go/",
      "number": 5113,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:52.120Z"
    },
    {
      "summary": "Docker image tags for version 1.22.2 were published with a 'v' prefix (v1.22.2) instead of the traditional format (1.22.2), creating duplicate tags pointing to different images. This caused issues with automation tools like Renovatebot that depend on consistent tag naming.",
      "category": "bug",
      "subcategory": "docker-release",
      "apis": [],
      "components": [
        "docker-registry",
        "release-pipeline"
      ],
      "concepts": [
        "versioning",
        "docker-tagging",
        "release-automation",
        "image-registry",
        "tag-consistency"
      ],
      "severity": "medium",
      "userImpact": "Users relying on automated tooling for Docker image updates experienced broken deployments when the tag naming convention changed unexpectedly.",
      "rootCause": "Inconsistent Docker image tagging in the release pipeline introduced 'v' prefix for version 1.22.2, deviating from previous versioning convention.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Correct tags without the 'v' prefix were published. Team acknowledged the error and committed to avoiding deletion of published tags and maintaining consistency.",
      "related": [],
      "keyQuote": "Docker image tags should only be removed under critical circumstances if there is a serious bug in the release or security flaw not just because there was a mistake in the naming.",
      "number": 5112,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:52.059Z"
    },
    {
      "summary": "Request to allow activities to override the retry backoff duration specified in the retry policy. This would enable dynamic backoff control at the activity level, allowing activities to specify custom retry delays based on runtime conditions.",
      "category": "feature",
      "subcategory": "activity-retry",
      "apis": [],
      "components": [
        "activity-executor",
        "retry-policy",
        "worker"
      ],
      "concepts": [
        "retry",
        "backoff",
        "activity-failure",
        "retry-policy",
        "dynamic-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot dynamically control retry backoff durations from within activities, limiting flexibility in handling transient failures with custom retry strategies.",
      "rootCause": null,
      "proposedFix": "Add an option/mechanism for activities to specify custom backoff duration that overrides the retry policy's backoff configuration.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented to allow activities to override retry backoff through activity execution options.",
      "related": [],
      "keyQuote": "activity should have an option to specify how long to backoff for next retry. This would overwrite the backoff from the retrypolicy.",
      "number": 5111,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:37.620Z"
    },
    {
      "summary": "Request to add support for rolling Elasticsearch indexes in visibility storage, allowing periodic creation of new index names based on configurable patterns (e.g., monthly or yearly). This enables more efficient searches by date range and allows automatic migration of older content to cheaper tiers or archiving.",
      "category": "feature",
      "subcategory": "elasticsearch-visibility",
      "apis": [],
      "components": [
        "visibility-storage",
        "elasticsearch-provider",
        "indexing"
      ],
      "concepts": [
        "index-rotation",
        "time-based-partitioning",
        "index-management",
        "performance-optimization",
        "cost-reduction",
        "search-efficiency"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently manage large Elasticsearch clusters or cost-effectively archive older data, forcing them to maintain single growing indexes that become increasingly expensive and slow to search.",
      "rootCause": "Temporal's Elasticsearch visibility provider does not support rolling or time-based index naming patterns, requiring all data to be written to a single index.",
      "proposedFix": "Implement configurable index name patterns (e.g., {namespace}-{date}) that allow periodic creation of new indexes based on document timestamps, with the ability to search across multiple indexes by date range.",
      "workaround": "Using Elasticsearch index aliases with wildcard patterns (e.g., my-index-*), though this has limitations as it always queries all matching indexes and prevents moving old indexes to cold nodes.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "In order for this feature to be useful and idempotent, the date needs to come from an actual document field, not just now.",
      "number": 5105,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:40.170Z"
    },
    {
      "summary": "Bulk processor fails when indexing to OpenSearch 2.11 with JSON parse errors due to control characters in serialized data. The root cause was identified as an OpenSearch issue that has been resolved upstream.",
      "category": "bug",
      "subcategory": "elasticsearch-integration",
      "apis": [],
      "components": [
        "bulk-processor",
        "elasticsearch-client",
        "visibility-store"
      ],
      "concepts": [
        "opensearch",
        "elasticsearch",
        "bulk-indexing",
        "json-serialization",
        "control-characters",
        "data-corruption"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use OpenSearch 2.11 for advanced visibility due to bulk processor failures when indexing workflow data.",
      "rootCause": "OpenSearch 2.11 introduced stricter JSON parsing that rejects control characters in bulk API requests; this was an OpenSearch bug (opensearch-project/OpenSearch#10802) that has been resolved.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Determined to be an OpenSearch issue rather than a Temporal issue. The problem was resolved in OpenSearch upstream.",
      "related": [],
      "keyQuote": "looks like that issue on open search is resolved... It seems that it's not a Temporal issue",
      "number": 5103,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:40.238Z"
    },
    {
      "summary": "Temporal server crashes during namespace initialization when the system namespace already exists in the database, failing with a duplicate key constraint violation. The issue occurs when restarting the server without wiping the database, and is related to error handling differences between PostgreSQL drivers (pg vs pgx).",
      "category": "bug",
      "subcategory": "database-initialization",
      "apis": [],
      "components": [
        "persistence",
        "postgresql-driver",
        "namespace-initialization",
        "sql-plugin"
      ],
      "concepts": [
        "database-constraints",
        "connection-management",
        "error-handling",
        "namespace-registration",
        "driver-compatibility",
        "duplicate-key",
        "initialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot restart the Temporal server without wiping the database, disrupting deployment and maintenance operations.",
      "rootCause": "The pgx driver's error handling differs from the pg driver, causing duplicate key constraint errors to not be properly caught during namespace initialization. The pgConn type assertion fails to recognize the duplicate entry error.",
      "proposedFix": "Fix the error handling logic to properly recognize duplicate key errors from both pg and pgx drivers when initializing the system namespace.",
      "workaround": "Wipe the database before restarting the server, or use the postgres_pgx plugin in the latest main branch version.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in latest main branch by properly handling the pgx driver error recognition during namespace initialization. The issue was caused by incorrect pgConn type assertion that failed to catch duplicate entry errors.",
      "related": [
        3077,
        4913
      ],
      "keyQuote": "the pgConn type assertion failed to recognize the dub entry error and bypass the logic to catch this error when namespace is initialized",
      "number": 5097,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:26.668Z"
    },
    {
      "summary": "Users upgrading from Temporal v1.19 to v1.22 encounter errors when attempting to add search attributes, with the error message indicating zero Keyword search attributes are allowed. The issue appears related to missing Custom Attributes configuration in the cluster_metadata_info during the upgrade process.",
      "category": "bug",
      "subcategory": "search-attributes-upgrade",
      "apis": [],
      "components": [
        "cluster-metadata",
        "search-attributes",
        "operator-cli",
        "persistence"
      ],
      "concepts": [
        "upgrade-migration",
        "database-schema",
        "search-attributes",
        "postgres",
        "cluster-configuration",
        "metadata-corruption"
      ],
      "severity": "high",
      "userImpact": "Users cannot add search attributes after upgrading their Temporal cluster, blocking workflow configuration and visibility features.",
      "rootCause": "The cluster_metadata_info binary data is missing Custom Attributes configuration after upgrade from v1.19 to v1.22, preventing new search attributes from being created.",
      "proposedFix": null,
      "workaround": "Use pure PostgreSQL instead of Aurora PostgreSQL clusters, as the issue appears specific to Aurora.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue identified as Aurora-specific; workaround is to use standard PostgreSQL. Not a general Temporal bug.",
      "related": [],
      "keyQuote": "This happens with Aurora PostgreSQL clusters even with fresh Temporal installs.",
      "number": 5079,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:27.545Z"
    },
    {
      "summary": "Workflow task timeouts specified in WorkflowTaskStartedToCloseTimeout are not always respected, particularly when tasks are processed on sticky queues. A task with a 1-second timeout may not timeout for 10 seconds, and the issue appears to be related to outstanding workflow updates.",
      "category": "bug",
      "subcategory": "workflow-task-timeout",
      "apis": [],
      "components": [
        "workflow-task-processor",
        "sticky-queue",
        "timeout-handler"
      ],
      "concepts": [
        "timeout",
        "sticky-queue",
        "workflow-task",
        "task-timeout",
        "timing",
        "update-handling"
      ],
      "severity": "high",
      "userImpact": "Workflow tasks may not timeout as configured, causing unexpected delays and incorrect workflow behavior when using sticky queues.",
      "rootCause": "Sticky queue processing may not properly enforce task timeouts, particularly when outstanding workflow updates are present.",
      "proposedFix": null,
      "workaround": "Explicitly reset the sticky task queue to force task processing without waiting for the timeout.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        624
      ],
      "keyQuote": "Sometimes the timeout may not be respected. In particular, if the task is being processed on a sticky queue.",
      "number": 5063,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:26.052Z"
    },
    {
      "summary": "User unable to sign up for community.temporal.io account, receiving conflicting error messages about email registration status across multiple email addresses and login methods.",
      "category": "bug",
      "subcategory": "authentication",
      "apis": [],
      "components": [
        "community-portal",
        "authentication",
        "email-verification"
      ],
      "concepts": [
        "account-signup",
        "email-registration",
        "password-recovery",
        "user-authentication",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users are blocked from creating accounts on the community forum due to authentication and email verification issues.",
      "rootCause": "Email registration system has conflicting logic between signup and password recovery flows, incorrectly reporting emails as both already registered and non-existent.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved and confirmed fixed by maintainer.",
      "related": [],
      "keyQuote": "Gives me an error saying the `Something went wrong, perhaps this email is already registered, try the forgot password link` but I try the forgot password link it says email does not exist.",
      "number": 5053,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:14.004Z"
    },
    {
      "summary": "User running Temporal on Kubernetes with EKS needs IRSA (IAM Roles for Service Accounts) support for S3 archival authentication. The Go SDK supports IRSA through environment variables, but Temporal's archival configuration appears to require using NewSessionWithOptions instead of NewSession to properly handle these credentials.",
      "category": "feature",
      "subcategory": "archival-s3",
      "apis": [],
      "components": [
        "archiver",
        "s3store",
        "history-archiver",
        "aws-sdk"
      ],
      "concepts": [
        "authentication",
        "irsa",
        "iam-roles",
        "kubernetes",
        "service-accounts",
        "aws-credentials"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal on EKS with IRSA cannot authenticate to S3 for archival without manual credential management, limiting cloud-native deployment patterns.",
      "rootCause": "AWS Go SDK's session.NewSession does not automatically load credentials from IRSA environment variables; NewSessionWithOptions is required for proper IRSA integration.",
      "proposedFix": "Replace session.NewSession with session.NewSessionWithOptions in history_archiver.go to properly support IRSA credential loading from environment variables.",
      "workaround": "Manually configure AWS credentials instead of relying on IRSA-injected environment variables, or use alternative authentication methods.",
      "resolution": "fixed",
      "resolutionDetails": "User created PR #5059 to add AWS SDK debug logging support for troubleshooting. Issue was resolved by enabling debug logging and fixing underlying IAM permissions.",
      "related": [
        983,
        5059
      ],
      "keyQuote": "AWS Go SDK's session.NewSession must be probably replaced with session.NewSessionWithOptions",
      "number": 5051,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:14.845Z"
    },
    {
      "summary": "The serverVersion property in the Temporal server is incorrectly reported as 1.22.0 when running version 1.22.1. This can be verified through tctl or temporal CLI commands that display server version information.",
      "category": "bug",
      "subcategory": "server-version-reporting",
      "apis": [],
      "components": [
        "server-version-management",
        "cluster-describe",
        "admin-cli"
      ],
      "concepts": [
        "version-mismatch",
        "server-metadata",
        "version-reporting",
        "cluster-info"
      ],
      "severity": "low",
      "userImpact": "Users cannot accurately determine the installed server version through standard diagnostic commands, potentially causing confusion during support and troubleshooting.",
      "rootCause": "Version number not properly updated in server release 1.22.1 build or version reporting mechanism",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Will be fixed in v1.22.2 release",
      "related": [],
      "keyQuote": "For server release 1.22.1 serverVersion is set as 1.22.0",
      "number": 5048,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:12.683Z"
    },
    {
      "summary": "User requests a configuration option to disable or reduce the verbosity of the 'ignoring permission in unexpected format' warning message in the JWT claim mapper, as this error is expected in their setup.",
      "category": "feature",
      "subcategory": "authorization",
      "apis": [],
      "components": [
        "authorization",
        "jwt-claim-mapper",
        "config"
      ],
      "concepts": [
        "logging",
        "configuration",
        "jwt",
        "permissions",
        "error-handling"
      ],
      "severity": "low",
      "userImpact": "Users with expected permission format variations are unable to suppress noisy warning messages, reducing observability clarity.",
      "rootCause": null,
      "proposedFix": "Add a configuration option to disable the warning message, or change it to debug level logging instead of warning level.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add a configuration option to turn off this message as this error is expected in our setup.",
      "number": 5047,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:59.436Z"
    },
    {
      "summary": "User reports excessive memory usage on the matching service when over 200 workers are connected, with disk IO also maxed out. They're seeking configuration guidance for deploying Temporal with tens of thousands of workers on a small Kubernetes cluster (2 CPU, 2GB RAM).",
      "category": "question",
      "subcategory": "resource-management",
      "apis": [],
      "components": [
        "matching-service",
        "worker-connection",
        "memory-management"
      ],
      "concepts": [
        "memory-usage",
        "scalability",
        "resource-limits",
        "worker-connections",
        "disk-io",
        "performance-tuning"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal with many worker connections experience memory exhaustion and disk IO saturation, limiting cluster scalability.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When I tested with more than 200 workers connected, I encountered some issues; my server memory was full",
      "number": 5026,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:00.590Z"
    },
    {
      "summary": "Archived workflows that have passed the retention period cannot be displayed in the web UI or accessed via tctl describe command, though they can be retrieved using tctl show with the runID. The issue affects the ability to view archived workflow details after retention expiration.",
      "category": "bug",
      "subcategory": "archival-visibility",
      "apis": [
        "DescribeWorkflowExecution",
        "GetWorkflowExecutionHistory",
        "ListArchivedWorkflowExecutions"
      ],
      "components": [
        "web-ui",
        "tctl",
        "archival-storage",
        "visibility-archiver"
      ],
      "concepts": [
        "archival",
        "retention",
        "workflow-visibility",
        "archived-workflows",
        "s3-storage",
        "workflow-retrieval"
      ],
      "severity": "high",
      "userImpact": "Users cannot view or describe archived workflows through standard tools after retention period expires, limiting visibility and debugging capabilities for older workflow executions.",
      "rootCause": "DescribeWorkflowExecution API does not support retrieving from archival storage for workflows past retention period, and UI fails to handle archived workflows appropriately.",
      "proposedFix": null,
      "workaround": "Use `tctl wf show` command with runID instead of `tctl wf desc` to retrieve history from archival storage.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "History for archived workflows can be retrieved from archival storage and returned via the GetWorkflowExecutionHistory API or `tctl wf show` when runID is specified.",
      "number": 5022,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:01:02.296Z"
    },
    {
      "summary": "Add support for non-blocking PollWorkflowExecutionUpdate API calls. The request proposes adding a bool non_blocking option to PollWorkflowExecutionUpdateRequest to allow polling without blocking, while ensuring update details and status are properly included in the poll response.",
      "category": "feature",
      "subcategory": "workflow-updates",
      "apis": [
        "PollWorkflowExecutionUpdate"
      ],
      "components": [
        "workflow-update-handler",
        "poll-api",
        "response-builder"
      ],
      "concepts": [
        "non-blocking",
        "polling",
        "update-status",
        "async-operations",
        "request-options"
      ],
      "severity": "medium",
      "userImpact": "Enables developers to poll for workflow updates without blocking, providing more flexibility in update handling patterns.",
      "rootCause": null,
      "proposedFix": "Add a bool non_blocking option to PollWorkflowExecutionUpdateRequest, or use an unset or 'admitted' wait stage to imply non-blocking behavior. Ensure update details and status are included in poll responses for both blocking and non-blocking calls.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add a bool non_blocking option to PollWorkflowExecutionUpdateRequest (or maybe you can just assume unset or 'admitted' wait stage implies non blocking)",
      "number": 5019,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:47.344Z"
    },
    {
      "summary": "Request to support an Accepted stage on UpdateWorkflowExecution that allows the caller to continue without waiting for the update to complete, enabling long-running async operations initiated by updates.",
      "category": "feature",
      "subcategory": "workflow-updates",
      "apis": [
        "UpdateWorkflowExecution"
      ],
      "components": [
        "workflow-execution",
        "update-handler",
        "client-api"
      ],
      "concepts": [
        "asynchronous-operations",
        "workflow-updates",
        "accepted-stage",
        "long-running-tasks",
        "async-pattern"
      ],
      "severity": "medium",
      "userImpact": "Users can now start long-running async operations via workflow updates without blocking on update completion.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented in pull request #4309",
      "related": [
        4309
      ],
      "keyQuote": "you want the update to start long running async operations and you don't want to return from UpdateWorkflowExecution as soon as update is accepted",
      "number": 5018,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:47.646Z"
    },
    {
      "summary": "gRPC health check incorrectly reports server as unhealthy (NOT_SERVING) for up to 1 second after GetSystemInfo succeeds, causing intermittent connection failures in client initialization.",
      "category": "bug",
      "subcategory": "health-check",
      "apis": [
        "GetSystemInfo"
      ],
      "components": [
        "grpc-server",
        "health-check",
        "frontend"
      ],
      "concepts": [
        "health-status",
        "server-readiness",
        "initialization",
        "race-condition",
        "connection-timing"
      ],
      "severity": "high",
      "userImpact": "SDK clients may fail to connect immediately after server startup even though the server is responding, causing CI/CD failures and requiring retry workarounds.",
      "rootCause": "Health check state is not synchronized with GetSystemInfo readiness, creating a window where the service is ready but health check reports NOT_SERVING.",
      "proposedFix": "Make all frontend methods return an error if the server is not healthy, ensuring health check state and service readiness are consistent.",
      "workaround": "Add retry logic in client connection attempts (implemented in temporalio/cli#368).",
      "resolution": "fixed",
      "resolutionDetails": "Implemented by making frontend methods return errors when server is unhealthy, ensuring consistency between health check and service readiness (PR #5069).",
      "related": [
        5069
      ],
      "keyQuote": "The goal is to not allow an SDK client connection to return successfully if the server is not ready.",
      "number": 5015,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:48.713Z"
    },
    {
      "summary": "HTTP/2 vulnerability (CVE-2023-39325) in golang.org/x/net-v0.15.0 transitive dependency allowing malicious clients to cause excessive server resource consumption through rapid request creation and resets. CVSS score 7.5.",
      "category": "bug",
      "subcategory": "dependency-security-vulnerability",
      "apis": [],
      "components": [
        "http2-server",
        "dependency-management",
        "networking"
      ],
      "concepts": [
        "http2",
        "resource-exhaustion",
        "denial-of-service",
        "security-vulnerability",
        "cvss",
        "transitive-dependency"
      ],
      "severity": "high",
      "userImpact": "Temporal servers using vulnerable google.golang.org/api library are exposed to DoS attacks via malicious HTTP/2 clients that can exhaust server resources.",
      "rootCause": "HTTP/2 servers did not properly bound the number of simultaneously executing handler goroutines, allowing attackers to reset in-progress requests and create new ones beyond the stream concurrency limit.",
      "proposedFix": "Upgrade golang.org/x/net to v0.17.0 or later, or upgrade Go to version go1.20.10 or go1.21.3.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was automatically closed by Mend as the vulnerable library was either marked as ignored or is no longer part of the Mend inventory in the monitored branches.",
      "related": [],
      "keyQuote": "A malicious HTTP/2 client which rapidly creates requests and immediately resets them can cause excessive server resource consumption.",
      "number": 5006,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:36.542Z"
    },
    {
      "summary": "Feature request to allow overriding search attributes when backfilling schedules, enabling users to distinguish backfilled workflow executions from regularly scheduled ones in the UI and for tracking purposes.",
      "category": "feature",
      "subcategory": "schedules-backfill",
      "apis": [
        "ScheduleBackfill"
      ],
      "components": [
        "scheduler",
        "workflow",
        "search-attributes"
      ],
      "concepts": [
        "backfilling",
        "schedule-overlap",
        "workflow-identification",
        "search-attributes",
        "tracking",
        "buffer-limit"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily distinguish backfilled workflow executions from regularly scheduled ones, making it difficult to track and understand backfill operations in the UI.",
      "rootCause": null,
      "proposedFix": "Extend the ScheduleBackfill request to support overriding search attributes of workflow executions, similar to how ScheduleOverlapPolicy can be overridden. Modify the `addStart` function to accept a mapping of search attributes and extend the `BufferedStart` message to support search attributes.",
      "workaround": "Users can develop their own backfill abstraction on top of Temporal and track backfill metadata separately, though this is imprecise and requires external solutions.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A potential solution to this problem would be to extend the ScheduleBackfill request to support overriding search attributes of the Workflow Executions that are part of the backfill.",
      "number": 5005,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:35.291Z"
    },
    {
      "summary": "Update OpenTelemetry library from v1.16.0 to v1.19.0 to support backwards incompatible changes and resolve dependency conflicts for users wanting to upgrade otel and packages that depend on it.",
      "category": "other",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "otel",
        "tracing",
        "dependency-management"
      ],
      "concepts": [
        "backwards-compatibility",
        "dependency-versioning",
        "observability",
        "library-upgrade",
        "conflict-resolution"
      ],
      "severity": "medium",
      "userImpact": "Users wanting to upgrade OpenTelemetry or use packages with newer otel versions experience dependency conflicts when using Temporal server as a library.",
      "rootCause": "go.opentelemetry.io/otel library made backwards incompatible changes to dependent libraries between v1.16.0 and v1.19.0, causing version conflicts for downstream consumers.",
      "proposedFix": "Upgrade go.opentelemetry.io/otel to v1.19.0 or latest version.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was merged approximately 3 weeks before comment 1, indicating the upgrade was completed and included in a release.",
      "related": [],
      "keyQuote": "Upgrade needed for others that want to upgrade otel and depend on server as a library.",
      "number": 4996,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:34.131Z"
    },
    {
      "summary": "Request to create an UpdateWithStart API similar to SignalWithStart, allowing callers to update a workflow without needing to check if it exists first. The feature request was resolved by the availability of the MultiOperation API which supports Update-with-Start functionality.",
      "category": "feature",
      "subcategory": "workflow-update",
      "apis": [
        "UpdateWorkflowExecution",
        "SignalWithStart",
        "MultiOperation"
      ],
      "components": [
        "workflow-execution",
        "update-handler",
        "api-gateway"
      ],
      "concepts": [
        "workflow-creation",
        "update-operation",
        "conditional-execution",
        "workflow-lifecycle",
        "api-convenience",
        "client-simplification"
      ],
      "severity": "medium",
      "userImpact": "Users can now update workflows without manually checking for existence first, simplifying the developer experience for Update operations.",
      "rootCause": null,
      "proposedFix": "Implement UpdateWithStart API as an experimental feature through the MultiOperation API",
      "workaround": "Check if workflow exists before calling UpdateWorkflowExecution, or use SignalWithStart for similar functionality",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by making MultiOperation API available (as experimental) which supports Update-with-Start",
      "related": [],
      "keyQuote": "It would be very useful to have UpdateWithStart API available.",
      "number": 4985,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:21.606Z"
    },
    {
      "summary": "Users cannot see buffered executions when using BUFFER_ALL overlap policy on schedules. The request is to add a 'buffered_actions' field to ScheduleInfo and ScheduleListInfo to expose these queued executions.",
      "category": "feature",
      "subcategory": "schedules-api",
      "apis": [
        "DescribeSchedule",
        "ListSchedules"
      ],
      "components": [
        "schedules",
        "api",
        "schedule-info"
      ],
      "concepts": [
        "buffering",
        "overlap-policy",
        "queued-executions",
        "schedule-visibility",
        "execution-queueing"
      ],
      "severity": "medium",
      "userImpact": "Users cannot inspect or monitor executions that are buffered and waiting to run after the current execution completes.",
      "rootCause": null,
      "proposedFix": "Add 'buffered_actions' field to ScheduleInfo and ScheduleListInfo protobuf structures. Could be simple (array of timestamps) or enhanced (array of objects with timestamp and queue-reason).",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "buffered executions appears neither in `info.futureActionTimes`, `info.recentActions` nor `info.runningWorkflows`, and no other field hints that some executions are currently buffered.",
      "number": 4984,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:23.939Z"
    },
    {
      "summary": "The server currently rejects UpdateWorkflowExecution requests with the ADMITTED lifecycle stage, returning an error. This feature request seeks to implement support for this stage, allowing callers to wait until an update is admitted (persisted and guaranteed not to be lost) without waiting for worker execution.",
      "category": "feature",
      "subcategory": "workflow-update",
      "apis": [
        "UpdateWorkflowExecution"
      ],
      "components": [
        "update-handler",
        "workflow-execution",
        "lifecycle-stage-support"
      ],
      "concepts": [
        "workflow-update",
        "admission",
        "persistence",
        "lifecycle-stage",
        "update-policy",
        "distributed-systems"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use the ADMITTED lifecycle stage for workflow updates, limiting their ability to wait for safe persistence without full execution completion.",
      "rootCause": "The ADMITTED lifecycle stage is not implemented in the server's UpdateWorkflowExecution handler.",
      "proposedFix": "Implement the ADMITTED stage in UpdateWorkflowExecution to return once the update is admitted (persisted and guaranteed to reach the worker).",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We should support returning once the update is \"admitted\" (i.e. will not be lost but may not have reached worker yet)",
      "number": 4979,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:22.513Z"
    },
    {
      "summary": "Visibility query syntax errors on SQL advanced visibility are not being returned to the client immediately. Instead, errors get swallowed and retried internally, causing the client to block and eventually timeout.",
      "category": "bug",
      "subcategory": "visibility-query",
      "apis": [
        "ListWorkflowExecutions"
      ],
      "components": [
        "visibility-manager",
        "visibility-persistence",
        "sql-visibility",
        "query-parser"
      ],
      "concepts": [
        "error-handling",
        "query-parsing",
        "client-communication",
        "timeout",
        "error-propagation",
        "retry-logic"
      ],
      "severity": "high",
      "userImpact": "Users with invalid visibility queries experience client timeouts instead of receiving immediate error feedback, making debugging query issues difficult.",
      "rootCause": "Query syntax errors in SQL advanced visibility are being logged but not properly returned to the client in the gRPC response, causing the RetryableInterceptor to retry the operation internally until timeout.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Query syntax errors are now properly propagated to the client immediately instead of being retried internally.",
      "related": [],
      "keyQuote": "On SQL advanced visibility (not ES) the error gets swallowed somewhere and retried internally, while the client is blocked and eventually times out.",
      "number": 4978,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:10.781Z"
    },
    {
      "summary": "PostgreSQL connection pooling was not working due to very low default idle connection settings (2 idle conns, unbounded pool size) in sqlx, causing 50% CPU load from constantly opening new connections instead of reusing existing ones.",
      "category": "bug",
      "subcategory": "postgres-persistence",
      "apis": [],
      "components": [
        "persistence",
        "postgres-plugin",
        "connection-pool",
        "sqlx"
      ],
      "concepts": [
        "connection-pooling",
        "idle-connections",
        "cpu-load",
        "database-configuration",
        "performance",
        "resource-management"
      ],
      "severity": "high",
      "userImpact": "High CPU usage and memory allocation overhead when using Postgres with large shard counts due to connections not being reused.",
      "rootCause": "Default sqlx connection pool settings were too conservative (max 2 idle connections, unbounded pool size), causing new connections to be opened constantly instead of reused.",
      "proposedFix": "Configure persistence.datastores.*.sql.maxConns and persistence.datastores.*.sql.maxIdleConns settings to appropriate values for the deployment.",
      "workaround": "Set higher maxIdleConns configuration value to force connection reuse instead of creating new connections.",
      "resolution": "fixed",
      "resolutionDetails": "User resolved the issue by configuring proper connection pool settings (maxConns and maxIdleConns), which eliminated connection handling from CPU and memory profiles.",
      "related": [],
      "keyQuote": "we didn't have that one configured and adding it seems to have basically removed connection handling from CPU and memory profiles",
      "number": 4975,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:11.429Z"
    },
    {
      "summary": "GetMutableStateRequest is currently used in poll mutable state but shouldn't require branch token or version-related data. The request should be simplified to only return the current version of mutable state, with version history and branch token data being handled separately by PollMutableState.",
      "category": "feature",
      "subcategory": "mutable-state",
      "apis": [
        "GetMutableState",
        "PollMutableState"
      ],
      "components": [
        "history-service",
        "mutable-state",
        "workflow-state-management"
      ],
      "concepts": [
        "state-management",
        "versioning",
        "branch-token",
        "api-simplification",
        "protocol-optimization"
      ],
      "severity": "medium",
      "userImpact": "Simplifying GetMutableStateRequest reduces unnecessary data transmission and simplifies the API for internal state management operations.",
      "rootCause": "GetMutableStateRequest includes unnecessary branch token and version-related data that should be handled by PollMutableState instead.",
      "proposedFix": "Refactor GetMutableState to return only the current version of mutable state without branch token or version-related data; move version history handling to PollMutableState.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "GetMutableState should just return the current version of mutable state, no branch token nor version related data is required in the request.",
      "number": 4958,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T03:00:11.260Z"
    },
    {
      "summary": "The CurrentBranchChanged error should identify branches by event ID and version rather than branch token bytes, improving consistency in branch identification.",
      "category": "other",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "workflow-engine",
        "event-handling",
        "branch-management"
      ],
      "concepts": [
        "branch-identification",
        "event-id",
        "versioning",
        "error-handling",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Users may experience incorrect branch identification in error messages and logs when CurrentBranchChanged errors occur.",
      "rootCause": "CurrentBranchChanged error currently uses branch token bytes as the branch identifier instead of a more stable identifier based on event ID and version.",
      "proposedFix": "Change the branch ID mechanism in CurrentBranchChanged error to use event ID + version instead of branch token bytes.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "CurrentBranchChanged Error should use event id + version as branch id",
      "number": 4957,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:58.458Z"
    },
    {
      "summary": "The ListBatchOperations and DescribeBatchOperation APIs return limited fields, making it difficult for users to differentiate batch operations and associate them with affected workflows. This feature request asks for operation type in ListBatchOperations response and visibility query/workflow execution list in DescribeBatchOperation response.",
      "category": "feature",
      "subcategory": "batch-operations-api",
      "apis": [
        "ListBatchOperations",
        "DescribeBatchOperation"
      ],
      "components": [
        "batch-operations",
        "web-ui",
        "api-response"
      ],
      "concepts": [
        "api-design",
        "data-visibility",
        "workflow-querying",
        "batch-processing",
        "user-context"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily differentiate or track batch operations in UI tables and cannot associate batch operations back to affected workflows, limiting operational visibility.",
      "rootCause": null,
      "proposedFix": "Add operation type field to ListBatchOperations response and include visibility query or workflow execution list in DescribeBatchOperation response.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the only uniquely identifiable field that is returned is the Job ID. Additionally, there is no way for a user to associate the Batch Operation back to a list of Workflows",
      "number": 4947,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:57.967Z"
    },
    {
      "summary": "Request to add ExecutionDuration attribute for advanced visibility in SQL-based databases. This metric is missing and needed for comprehensive execution analysis in SQL environments.",
      "category": "feature",
      "subcategory": "visibility-sql",
      "apis": [],
      "components": [
        "visibility",
        "sql-storage",
        "advanced-visibility"
      ],
      "concepts": [
        "execution-duration",
        "metrics",
        "observability",
        "sql-databases",
        "performance-tracking"
      ],
      "severity": "medium",
      "userImpact": "Users cannot track execution duration through advanced visibility when using SQL-based persistence backends.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Please add ExecutionDuration attribute for SQL based advanced vis.",
      "number": 4942,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:59.372Z"
    },
    {
      "summary": "DescribeBatchOperation API returns a 400 Bad Request error when attempting to describe batch operations of type 'reset', even though the reset operation was successfully created. The API incorrectly rejects the reset operation type during the describe call.",
      "category": "bug",
      "subcategory": "batch-operations",
      "apis": [
        "DescribeBatchOperation"
      ],
      "components": [
        "frontend",
        "workflow-handler",
        "batch-operations"
      ],
      "concepts": [
        "batch-operations",
        "reset",
        "api-compatibility",
        "operation-types",
        "error-handling"
      ],
      "severity": "high",
      "userImpact": "Users cannot inspect or manage reset batch operations after creating them, blocking workflow reset functionality.",
      "rootCause": "The DescribeBatchOperation API handler does not support the 'reset' operation type in its validation logic, despite the operation type being supported for creation.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The DescribeBatchOperation API was updated to support the 'reset' operation type.",
      "related": [],
      "keyQuote": "After creating a \"reset\" batch operation, call DescribeBatchOperation API and receive a 400 Bad Request response, with error message \"The operation type reset is not supported\"",
      "number": 4939,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:46.639Z"
    },
    {
      "summary": "DynamicRateLimiter's Rate() and Burst() functions don't update dynamically after the refresh interval when used as a RateBurst. The rate and burst quotas remain stale even though they should reflect updates made to the limiter.",
      "category": "bug",
      "subcategory": "rate-limiting",
      "apis": [],
      "components": [
        "DynamicRateLimiterImpl",
        "RateBurst",
        "rate-limiter"
      ],
      "concepts": [
        "rate-limiting",
        "dynamic-updates",
        "refresh-interval",
        "concurrency",
        "quota-management"
      ],
      "severity": "medium",
      "userImpact": "Users relying on DynamicRateLimiter for dynamic rate control experience stale rate and burst values that don't reflect updates, breaking the expected behavior of adaptive rate limiting.",
      "rootCause": "The Rate() and Burst() functions are not refreshing their cached values based on the refresh interval updates to the limiter configuration.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4933
      ],
      "keyQuote": "When you use DynamicRateLimiter as just a RateBurst, its rate and burst quotas should update dynamically after the refresh interval. They don't.",
      "number": 4934,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:47.391Z"
    },
    {
      "summary": "Add rate limiting capabilities to server-side batch operations to prevent overloading workers. Users lost the `--rps` and `--concurrency` options that existed in the previous client-side `tctl` implementation when batch operations were moved to the server.",
      "category": "feature",
      "subcategory": "batch-operations",
      "apis": [],
      "components": [
        "batch-operations",
        "server",
        "rate-limiter"
      ],
      "concepts": [
        "rate-limiting",
        "batch-operations",
        "worker-capacity",
        "concurrency",
        "throughput-control"
      ],
      "severity": "medium",
      "userImpact": "Users cannot control the rate at which batch operations execute, risking worker overload and service disruption when operating on millions of executions.",
      "rootCause": null,
      "proposedFix": "Implement rate limiting options (--rps, --concurrency) for all types of batch operations on the server side, similar to the previous client-side tctl implementation.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Allow rate limiting a batch operation, this option should be allowed for all types of batch operations.",
      "number": 4926,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:48.105Z"
    },
    {
      "summary": "User is asking whether Protobuf message structures can be used as input parameters directly in RegisterWorkflow and RegisterActivity functions, rather than requiring custom type definitions.",
      "category": "question",
      "subcategory": "api-design",
      "apis": [
        "RegisterWorkflow",
        "RegisterActivity"
      ],
      "components": [
        "workflow-registration",
        "activity-registration",
        "sdk-api"
      ],
      "concepts": [
        "message-structure",
        "protobuf",
        "input-parameters",
        "type-system",
        "api-design"
      ],
      "severity": "low",
      "userImpact": "Users may be unclear about what types can be used as parameters when registering workflows and activities, potentially leading to confusion during SDK usage.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Issue was closed after minimal discussion and clarification request was redirected to community forum, suggesting it was addressed through community support rather than a code change.",
      "related": [],
      "keyQuote": "Can the PB message structure be the input parameter for RegisterWorkflow or RegisterActivity function",
      "number": 4925,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:34.297Z"
    },
    {
      "summary": "Internal-Frontend service cannot perform namespace registration/update operations due to hardcoded service name checks that expect the regular Frontend service. The issue occurs because the RegisterNamespace API implementation uses a hardcoded reference to FrontendService instead of dynamically fetching the current service name.",
      "category": "bug",
      "subcategory": "namespace-management",
      "apis": [
        "RegisterNamespace",
        "UpdateNamespace"
      ],
      "components": [
        "frontend",
        "internal-frontend",
        "namespace-handler",
        "bootstrap-container"
      ],
      "concepts": [
        "service-name-resolution",
        "internal-service-communication",
        "namespace-administration",
        "service-discovery",
        "bootstrap-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot perform namespace management operations through the internal-frontend service, limiting administrative capabilities and requiring all such operations to go through the regular frontend.",
      "rootCause": "The RegisterNamespace workflow service API implementation uses a hardcoded service name of primitives.FrontendService instead of dynamically fetching the current service name from request context, causing bootstrap container lookup failures when called via internal-frontend.",
      "proposedFix": "Make code changes to dynamically fetch the current service name instead of using hardcoded FrontendService reference; also fix the issue where migration workflow needs UpdateNamespace on internal-frontend.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "RegisterNamespace workflow service API's implementation is using a hard-coded service name of primitives.FrontendService instead of fetching the current service name",
      "number": 4919,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:36.553Z"
    },
    {
      "summary": "Issue reported TLS certificate updates not being propagated to admin/frontend/matching/history clients due to client caching. Investigation determined the root cause was a missing refreshInterval configuration; cert reloading actually works through Go TLS callbacks without requiring client recreation.",
      "category": "bug",
      "subcategory": "tls-certificates",
      "apis": [],
      "components": [
        "client_bean",
        "tls_config_helper",
        "local_store_tls_provider",
        "local_store_cert_provider",
        "grpc"
      ],
      "concepts": [
        "tls-certificates",
        "certificate-refresh",
        "client-caching",
        "grpc-connections",
        "certificate-rotation",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users experience connection failures to remote services when TLS certificates are updated without server restart, requiring manual rolling restart as workaround.",
      "rootCause": "User had not configured refreshInterval in global.tls (defaults to 0, disabling refresh). The caching is correct; cert reloading happens through Go TLS stack callbacks.",
      "proposedFix": "Ensure refreshInterval is properly configured in global.tls settings; no code changes needed.",
      "workaround": "Rolling restart of Temporal server to re-establish gRPC connections with new certificates.",
      "resolution": "invalid",
      "resolutionDetails": "Closed as not a bug; investigation showed cert reloading works correctly through Go TLS callbacks. User likely had refreshInterval set to 0 or missing from configuration.",
      "related": [],
      "keyQuote": "Reloading certs happens way below creation of grpc clients...the Go TLS stack calls them when appropriate, you don't need to recreate the grpc client.",
      "number": 4918,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:35.901Z"
    },
    {
      "summary": "The ListBatchOperations API ignores the pageSize query parameter and returns all batch operations up to the visibility maximum instead of respecting the requested page size.",
      "category": "bug",
      "subcategory": "batch-operations-api",
      "apis": [
        "ListBatchOperations"
      ],
      "components": [
        "api-server",
        "batch-operations",
        "pagination"
      ],
      "concepts": [
        "pagination",
        "query-parameters",
        "api-response",
        "page-size",
        "filtering"
      ],
      "severity": "medium",
      "userImpact": "Users cannot limit the number of batch operations returned by the API, making it impossible to implement proper pagination in client applications.",
      "rootCause": "The ListBatchOperations API endpoint is not parsing or respecting the pageSize query parameter when processing requests.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Making a GET request to /api/v1/namespaces/<namespace>/batch-operations?pageSize=5 returns all Batch Operations (up to the visibility maximum page size set via dynamic config)",
      "number": 4907,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:21.328Z"
    },
    {
      "summary": "Request to implement custom authentication between Temporal microservices (frontend, matching, history, worker). Currently, only mTLS is supported for inter-node communication, but users need the ability to send authorization tokens between internal services similar to how the Go SDK's HeadersProvider works for external clients.",
      "category": "feature",
      "subcategory": "authentication",
      "apis": [],
      "components": [
        "frontend",
        "matching",
        "history",
        "worker",
        "authorization"
      ],
      "concepts": [
        "authentication",
        "authorization",
        "internode-communication",
        "mTLS",
        "tokens",
        "custom-authenticator",
        "HeadersProvider"
      ],
      "severity": "medium",
      "userImpact": "Users cannot implement custom authentication mechanisms for inter-service communication in Temporal clusters, limiting authentication flexibility and forcing reliance on mTLS.",
      "rootCause": "The server-side lacks a HeadersProvider-like interface for microservices to inject custom authentication headers, unlike the Go SDK which supports this for external clients.",
      "proposedFix": "Implement an interceptor mechanism to allow custom authentication headers in all requests between Temporal's microservices, similar to the HeadersProvider interface in the Go SDK.",
      "workaround": "Configure mTLS for internode communication, but this doesn't support custom claim mappers and is less flexible than token-based authentication.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "there is no option to send authorization token from \"matching\", \"history\" and \"worker\" services to the \"frontend\"",
      "number": 4902,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:23.791Z"
    },
    {
      "summary": "The matchingEngine.getTask() function returned nil for both internalTask pointer and error in CI, causing a nil pointer dereference panic in the TestAddTaskAfterStartFailure test. The issue was a flaky test condition that didn't reproduce locally.",
      "category": "bug",
      "subcategory": "matching-engine",
      "apis": [],
      "components": [
        "matching-engine",
        "task-management",
        "test-framework"
      ],
      "concepts": [
        "nil-pointer-dereference",
        "error-handling",
        "flaky-test",
        "concurrent-execution",
        "task-retrieval"
      ],
      "severity": "high",
      "userImpact": "Intermittent test failures cause unreliable CI builds and reduce confidence in the matching engine's correctness.",
      "rootCause": "matchingEngine.getTask() violated its contract by returning both nil error and nil internalTask pointer, indicating a race condition or logic error that only manifested under specific CI timing conditions.",
      "proposedFix": "Fixed by #4989",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved via PR #4989 which addressed the underlying condition causing the flaky test",
      "related": [
        4989
      ],
      "keyQuote": "matchingEngine.getTask returned both a nil error and a nil internalTask",
      "number": 4892,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:21.581Z"
    },
    {
      "summary": "User requests support for specifying multiple Elasticsearch hosts in the configuration to connect to a multi-node cluster without a load balancer. The elastic client library already supports multiple URLs, but the configuration format needs to be updated.",
      "category": "feature",
      "subcategory": "elasticsearch-client",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "configuration",
        "cluster-connectivity"
      ],
      "concepts": [
        "multi-node",
        "load-balancing",
        "cluster",
        "connection",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users with multi-node Elasticsearch clusters without load balancers cannot properly configure the Temporal server connection.",
      "rootCause": null,
      "proposedFix": "Modify the configuration format to accept multiple hosts/URLs in the Elasticsearch config, leveraging the existing elastic.SetURL(url string...) capability.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I have an Elasticsearch cluster with no load balancer, so I must specify more than one host when creating a client.",
      "number": 4890,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:10.388Z"
    },
    {
      "summary": "User reported that logs were not being written to the configured outputFile when running the Temporal server from source code with `go run`. The issue was caused by a configuration setting where stdout logging was enabled by default, preventing file-based logging.",
      "category": "bug",
      "subcategory": "logging-configuration",
      "apis": [],
      "components": [
        "logger",
        "configuration",
        "server-startup"
      ],
      "concepts": [
        "logging",
        "file-output",
        "stdout",
        "configuration",
        "server-initialization"
      ],
      "severity": "low",
      "userImpact": "Users running the server from source code cannot verify logs are being written to files if stdout logging is unintentionally enabled.",
      "rootCause": "The stdout logging configuration was not properly disabled, causing all logs to be written to standard output instead of the configured outputFile.",
      "proposedFix": null,
      "workaround": "Set stdout configuration to false in the logging configuration.",
      "resolution": "fixed",
      "resolutionDetails": "User self-resolved by discovering the stdout configuration option needed to be disabled to enable file-based logging.",
      "related": [],
      "keyQuote": "Ah, I see, I need to set the stdout as false",
      "number": 4889,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:10.034Z"
    },
    {
      "summary": "Users want to track scheduled workflow executions using run_id, but run_id is only available for already-started executions. The request is to prepopulate run_id in future_action_times for scheduled workflows so they can be uniquely identified before execution.",
      "category": "feature",
      "subcategory": "schedule-tracking",
      "apis": [
        "ListWorkflows"
      ],
      "components": [
        "schedule",
        "workflow-execution",
        "api-response"
      ],
      "concepts": [
        "workflow-id",
        "run-id",
        "unique-identifier",
        "schedule-execution",
        "tracking",
        "future-actions"
      ],
      "severity": "medium",
      "userImpact": "Users cannot uniquely identify future scheduled workflow executions, making it difficult to track and correlate scheduled executions with their on-demand counterparts.",
      "rootCause": "run_id is generated at workflow start time, not ahead of time, so it cannot be prepopulated for future scheduled executions.",
      "proposedFix": "Add a new field parallel to future_action_time that includes the scheduled time without jitter applied, allowing clients to derive the workflow_id (which includes timestamp) for future executions.",
      "workaround": "Use workflow_id prefix + start time or TemporalScheduledStartTime search attribute to identify future executions, though this doesn't include run_id.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4795
      ],
      "keyQuote": "we could add a new field parallel to future_action_time that has the time without jitter so it'll match the workflow id",
      "number": 4887,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:59:10.777Z"
    },
    {
      "summary": "Replace the confusing signalWithStart API with a clearer signalOrStart alternative that better handles the use case where users want to signal a workflow only if it's not already running, addressing confusion about the 'with' terminology and the race condition where signals can arrive before the workflow starts.",
      "category": "feature",
      "subcategory": "workflow-signals",
      "apis": [
        "SignalWithStart",
        "SignalOrStart"
      ],
      "components": [
        "workflow-client",
        "signal-handler",
        "workflow-engine"
      ],
      "concepts": [
        "signal-workflow",
        "workflow-startup",
        "api-design",
        "race-condition",
        "developer-experience"
      ],
      "severity": "medium",
      "userImpact": "Provides a clearer, more intuitive API for signaling workflows at startup, reducing confusion and preventing race conditions where signals arrive before the workflow method executes.",
      "rootCause": "The signalWithStart API name and semantics are confusing - the 'with' terminology is ambiguous and doesn't clearly convey the intent, plus it requires handling the case where signals arrive before workflow execution.",
      "proposedFix": "Build a new signalOrStart API based on the existing signalWithStart implementation that better represents the semantic intent of 'signal if not already running, otherwise start and signal'.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "The feature was not implemented. A commenter noted that adding another API would increase confusion rather than resolve it, and there's no way to deprecate the existing signalWithStart API.",
      "related": [],
      "keyQuote": "At this point, adding more API to do the same probably just add more confusion and no way to deprecate existing one.",
      "number": 4885,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:59.173Z"
    },
    {
      "summary": "History service panics when visibility archival is enabled but history archival is disabled for a namespace, causing the service to continuously crash during workflow archival attempts.",
      "category": "bug",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "history-service",
        "archival-system",
        "archival-queue-task-executor"
      ],
      "concepts": [
        "archival",
        "visibility",
        "history",
        "namespace-configuration",
        "nil-dereference",
        "panic"
      ],
      "severity": "critical",
      "userImpact": "Users cannot disable history archival for a namespace without causing the history service to enter an unrecoverable panic state.",
      "rootCause": "Nil dereference in archiver.go when HistoryURI is not initialized, which occurs when history archival is disabled but visibility archival is enabled.",
      "proposedFix": "Make archiver.uri.String() method return an empty string instead of panicking when HistoryURI is not set, or handle the nil case in the archival queue task executor.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed via targeted fix in PR #4880 to handle the case where HistoryURI is not set.",
      "related": [
        4880
      ],
      "keyQuote": "Visibility Archival On, History Archival Off for a namespace causes panic in history service",
      "number": 4879,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:57.679Z"
    },
    {
      "summary": "User asks how to start Temporal server with a custom configuration file (development.yaml) instead of using the default 'temporal server start-dev' command.",
      "category": "question",
      "subcategory": "server-setup",
      "apis": [],
      "components": [
        "server",
        "configuration"
      ],
      "concepts": [
        "server startup",
        "configuration files",
        "development setup",
        "command-line arguments"
      ],
      "severity": "low",
      "userImpact": "Users unfamiliar with Temporal need guidance on how to configure and start the server with custom configuration files.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Redirected to community forum (community.temporal.io) as this is a support question, not a bug or feature request.",
      "related": [],
      "keyQuote": "So if there are any keys can I start the temporal server with files like development.yaml in the config folder in the command line",
      "number": 4878,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:55.753Z"
    },
    {
      "summary": "Request to add support for static JWKS configuration in JWT key provider to enable JWT authentication in isolated environments where the external IDP is unreachable. Should support dynamic config loading for key rotation.",
      "category": "feature",
      "subcategory": "authentication",
      "apis": [],
      "components": [
        "auth",
        "jwt",
        "config"
      ],
      "concepts": [
        "authentication",
        "authorization",
        "jwks",
        "key-management",
        "isolation",
        "dynamic-config"
      ],
      "severity": "medium",
      "userImpact": "Users in isolated environments cannot use JWT authentication without the ability to configure static JWKS keys.",
      "rootCause": null,
      "proposedFix": "Add a `jwkStaticKeySource` string parameter that contains JWKS keys equivalent to the JWKS endpoint response, with support for loading via dynamic config for key rotation.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "temporal is isolated and cannot reach out to our externally hosted IDP and get the keys necessary for authz",
      "number": 4870,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:46.212Z"
    },
    {
      "summary": "Schedule updates incorrectly discard previously scheduled actions when the update occurs between the nominal and jittered time. A scheduled action with jitter (e.g., 12:20:33) is lost if the schedule is updated at 12:20:10, as the calculation restarts from the update time instead of from the previous action time.",
      "category": "bug",
      "subcategory": "schedules",
      "apis": [],
      "components": [
        "schedules",
        "schedule-engine",
        "jitter"
      ],
      "concepts": [
        "scheduling",
        "jitter",
        "update",
        "nominal-time",
        "action-retention",
        "idempotency"
      ],
      "severity": "high",
      "userImpact": "Users updating schedules lose previously calculated actions that fall between the update time and their jittered execution time, causing scheduled tasks to be skipped unexpectedly.",
      "rootCause": "Schedule update logic discards previous times and recalculates from the current update time instead of looking back to the previous action time to preserve scheduled actions within the jitter window.",
      "proposedFix": "Start schedule recalculation from max(create_time, last_update_time, previous_action_time) instead of from the update time, then discard any calculated times that fall in the past relative to the update time.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If there is a matching time including jitter after the update time, it should still run, even if its nominal time was before the update time.",
      "number": 4866,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:46.250Z"
    },
    {
      "summary": "Temporal Server fails to start with authorization enabled when using the default claimMapper configuration. The worker service crashes during startup with an \"Request unauthorized\" error.",
      "category": "bug",
      "subcategory": "authorization",
      "apis": [],
      "components": [
        "worker",
        "authorization",
        "scanner",
        "frontend"
      ],
      "concepts": [
        "JWT",
        "authorization",
        "claims",
        "authentication",
        "startup",
        "configuration"
      ],
      "severity": "high",
      "userImpact": "Users cannot start the Temporal Server with authorization and claimMapper set to 'default' configuration.",
      "rootCause": "The worker service cannot obtain a valid JWT token that passes authorization checks when connecting with the default claimMapper configuration.",
      "proposedFix": "Use internal-frontend configuration so the worker connects to a different frontend that doesn't check JWT, as documented in the 1.20.0 release notes.",
      "workaround": "Configure the server to use internal-frontend for the worker service to bypass JWT authorization checks.",
      "resolution": "wontfix",
      "resolutionDetails": "No direct fix provided. The issue is a configuration limitation where there's no easy way for the worker role to pass a JWT accepted by the frontend. The recommended solution is architectural - use internal-frontend.",
      "related": [],
      "keyQuote": "There's no (easy) way to get the worker role to pass a JWT that the frontend will accept. The most practical solution is to use internal-frontend",
      "number": 4864,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:45.626Z"
    },
    {
      "summary": "Request to add the ability to update search attributes for existing schedules, similar to the existing workflow search attributes update functionality. Currently, updating schedule search attributes requires deleting and recreating schedules, which is risky in production environments.",
      "category": "feature",
      "subcategory": "schedules",
      "apis": [],
      "components": [
        "schedules",
        "search-attributes"
      ],
      "concepts": [
        "search-attributes",
        "schedule-management",
        "metadata-update",
        "production-safety",
        "workflow-parity"
      ],
      "severity": "medium",
      "userImpact": "Users cannot safely update search attributes for existing schedules in production without the risk of data loss from deletion and recreation.",
      "rootCause": null,
      "proposedFix": "Implement schedule search attribute update capability matching the workflow search attributes update API.",
      "workaround": "Delete all schedules and re-create them with new search attributes (high-risk operation).",
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to provide schedule search attribute update capability.",
      "related": [],
      "keyQuote": "I'd like to update search attributes for existing schedules the same way you can do so for workflows.",
      "number": 4858,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:32.153Z"
    },
    {
      "summary": "Workflow needs ability to reject signals by type, especially when performing cleanup operations. Currently workers must run to reject updates, but signal rejection should work synchronously without active workers.",
      "category": "feature",
      "subcategory": "signal-handling",
      "apis": [],
      "components": [
        "signal-handler",
        "workflow-engine",
        "command-processor"
      ],
      "concepts": [
        "signal-rejection",
        "workflow-state-management",
        "cleanup-operations",
        "worker-availability",
        "command-execution"
      ],
      "severity": "medium",
      "userImpact": "Workflows can more robustly handle cleanup operations by rejecting unwanted signals without requiring active workers.",
      "rootCause": null,
      "proposedFix": "Add commands to enable/disable signal types, with rejected signals handled synchronously without worker execution.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add commands to enable/disable signal types. The signal type which is not enabled will be rejected synchronously.",
      "number": 4845,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:30.859Z"
    },
    {
      "summary": "FreeBSD build fails due to undefined getSendQueueLen function in tchannel-go dependency. User attempted to build Temporal Server on FreeBSD 14.0-ALPHA4 but encountered a compilation error in the tchannel-go package's Unix socket implementation.",
      "category": "bug",
      "subcategory": "build-system",
      "apis": [],
      "components": [
        "tchannel-go",
        "build-system",
        "unix-socket"
      ],
      "concepts": [
        "cross-platform-support",
        "build-failure",
        "dependency-issue",
        "platform-compatibility",
        "freebsd"
      ],
      "severity": "low",
      "userImpact": "Users attempting to build Temporal Server on FreeBSD encounter a compilation failure preventing the binary from being built.",
      "rootCause": "The tchannel-go dependency has an undefined function getSendQueueLen in sockio_unix.go that is not compatible with FreeBSD's socket implementation.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed as duplicate of issue #2703, which addresses the same tchannel-go FreeBSD compatibility problem.",
      "related": [
        2703
      ],
      "keyQuote": "undefined: getSendQueueLen in ../go/pkg/mod/github.com/temporalio/tchannel-go@v1.22.1-0.20220818200552-1be8d8cffa5b/sockio_unix.go",
      "number": 4842,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:33.762Z"
    },
    {
      "summary": "Expose OpenAPI specification on the Temporal HTTP API server to allow API consumers to discover and understand available endpoints. Requires investigation into spec generation tools, versioning strategy, and proper endpoint naming.",
      "category": "feature",
      "subcategory": "api-documentation",
      "apis": [],
      "components": [
        "http-api-server",
        "api-go",
        "openapi-spec"
      ],
      "concepts": [
        "api-documentation",
        "openapi",
        "spec-generation",
        "grpc-gateway",
        "api-discovery",
        "backwards-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users cannot discover or understand available HTTP API endpoints without OpenAPI documentation, making integration more difficult.",
      "rootCause": null,
      "proposedFix": "Generate OpenAPI spec in api-go repo using protoc-gen-openapiv2 or protoc-gen-openapi tools, embed it as go:embed, and expose it at a standard endpoint (e.g., openapi.json). Mark as experimental until stable.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "OpenAPI spec exposure was implemented in #5393, though HTTP API may not yet be fully documented as a feature.",
      "related": [
        5393
      ],
      "keyQuote": "Exposing the spec was done in #5393. HTTP API may not yet be a documented feature.",
      "number": 4837,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:18.046Z"
    },
    {
      "summary": "User asks how to scale Temporal services independently in a self-hosted production deployment. They read that production clusters can have multiple Frontend, History, Matching, and Worker services but cannot find operational documentation on how to configure this scaling.",
      "category": "docs",
      "subcategory": "deployment-configuration",
      "apis": [],
      "components": [
        "frontend-service",
        "history-service",
        "matching-service",
        "worker-service"
      ],
      "concepts": [
        "scaling",
        "production-deployment",
        "cluster-configuration",
        "service-architecture",
        "self-hosted"
      ],
      "severity": "low",
      "userImpact": "Users attempting to deploy Temporal in production lack clear documentation on how to scale different server roles independently.",
      "rootCause": "Documentation on scaling individual server roles in self-hosted Temporal deployments is missing or difficult to find.",
      "proposedFix": "Refer users to Helm charts documentation and community forum for production deployment guidance.",
      "workaround": "Use Helm charts (https://github.com/temporalio/helm-charts) for production deployments with independent service scaling.",
      "resolution": "invalid",
      "resolutionDetails": "Closed as this is a question better suited for community forum rather than a documentation bug or feature request. Respondent provided links to Helm charts and community resources.",
      "related": [],
      "keyQuote": "For production setup, you want to run different server roles separately and they can be scaled differently based on needs.",
      "number": 4836,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:20.293Z"
    },
    {
      "summary": "User created a cluster with only 1 history shard without understanding the implications, causing severe latency issues. They need a way to either scale up history shards after cluster creation, retroactively enable replication by converting a local namespace to global, or import workflows to a new cluster.",
      "category": "feature",
      "subcategory": "namespace-configuration",
      "apis": [],
      "components": [
        "namespace",
        "history-shards",
        "replication",
        "persistence"
      ],
      "concepts": [
        "namespace-configuration",
        "history-shards",
        "replication",
        "cluster-migration",
        "scalability",
        "multi-cluster"
      ],
      "severity": "high",
      "userImpact": "Users who configure clusters with insufficient history shards are locked into a suboptimal configuration with no path to migrate existing workflows to properly-configured clusters.",
      "rootCause": "The 'global' namespace flag cannot be changed after namespace creation, and history shard count is immutable, preventing replication setup and shard scaling for existing namespaces.",
      "proposedFix": "Either allow history shards to be scaled up after namespace creation, allow namespaces to be retroactively promoted to global status, or support importing events from one namespace/cluster to another.",
      "workaround": "Route new workflows to a parallel cluster at the application level (not ideal for hobby projects).",
      "resolution": "fixed",
      "resolutionDetails": "The --promote-global flag exists to convert local namespaces to global, though the user encountered additional issues with adding a second cluster.",
      "related": [],
      "keyQuote": "These foot-guns are very easy to trigger and very hard to undo.",
      "number": 4835,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:20.725Z"
    },
    {
      "summary": "Request to add the ability to specify a task queue when calling the reset API. Currently, the reset functionality does not allow users to override or specify which task queue the reset should be applied to.",
      "category": "feature",
      "subcategory": "reset-api",
      "apis": [
        "Reset"
      ],
      "components": [
        "reset-api",
        "task-queue",
        "workflow-execution"
      ],
      "concepts": [
        "reset",
        "task-queue",
        "execution",
        "api-flexibility",
        "workflow-management"
      ],
      "severity": "low",
      "userImpact": "Users cannot specify a task queue when resetting an execution, limiting flexibility in reset operations and potentially requiring workarounds.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Request is for adding ability to specify task queue for reset api",
      "number": 4829,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:05.464Z"
    },
    {
      "summary": "Bearer tokens are not propagated when the Temporal Server makes inter-cluster requests for addOrUpsertRemoteCluster operations. When Cluster-1 receives an authorized request with a bearer token and needs to communicate with Cluster-2, the token is not included in the outgoing request, causing authorization failures on the remote cluster.",
      "category": "bug",
      "subcategory": "remote-cluster-authorization",
      "apis": [
        "AddOrUpsertRemoteCluster"
      ],
      "components": [
        "frontend-service",
        "cluster-replication",
        "authorization-handler",
        "grpc-client"
      ],
      "concepts": [
        "token-propagation",
        "bearer-token",
        "oauth-authorization",
        "inter-cluster-communication",
        "context-propagation"
      ],
      "severity": "high",
      "userImpact": "Multi-cluster setups with OAuth authorization fail when performing cluster management operations because tokens from client requests are not passed to remote clusters, blocking legitimate cross-cluster operations.",
      "rootCause": "The frontend service does not propagate the authorization context (bearer token) from the incoming request when making outgoing gRPC calls to remote clusters.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Frontend service from Cluster-1 makes a Request-2 to Cluster-2 does not propagate token from Request-1",
      "number": 4823,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:07.020Z"
    },
    {
      "summary": "MySQL startup fails with 'Out of range value for column rpc_port' error when using development-mysql8.yaml config on Linux. The ringpop RPC port value exceeds the smallint column limit.",
      "category": "bug",
      "subcategory": "cluster-membership",
      "apis": [],
      "components": [
        "cluster-membership",
        "ringpop",
        "persistence",
        "mysql"
      ],
      "concepts": [
        "port-overflow",
        "data-type-mismatch",
        "cluster-configuration",
        "database-schema"
      ],
      "severity": "high",
      "userImpact": "Users cannot start Temporal server with MySQL 8 on Linux due to RPC port value exceeding column data type limits.",
      "rootCause": "The rpc_port column in temporal.cluster_membership is defined as smallint (max 32767) but receives values like 52336 from dynamically assigned ports.",
      "proposedFix": "Change rpc_port column type from smallint to smallint unsigned or a larger integer type to accommodate port values up to 65535.",
      "workaround": "Manually configure membershipPort in the config file to use a value within smallint range (below 32767).",
      "resolution": "invalid",
      "resolutionDetails": "Issue was user error - user had deleted membershipPort from their config file, causing invalid dynamic port assignment.",
      "related": [],
      "keyQuote": "param `ringpopHostAddress` is `127.0.0.1:0`. after `tChannel.Serve(listener)`, could get peerInfo, like `127.0.0.1:52336`. then save 52336 to temporal.cluster_membership, column rpc_port, but it's type is `smallint`",
      "number": 4822,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:58:06.599Z"
    },
    {
      "summary": "Temporal's Elasticsearch integration uses deprecated index templates (v1 format) instead of the new composable index templates format introduced in Elasticsearch 7.x. Settings from the new template version are not combined, requiring migration to the modern template format.",
      "category": "feature",
      "subcategory": "elasticsearch-integration",
      "apis": [],
      "components": [
        "elasticsearch-integration",
        "index-templates",
        "persistence"
      ],
      "concepts": [
        "elasticsearch",
        "index-templates",
        "schema-migration",
        "backwards-compatibility",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal with Elasticsearch cannot use the modern composable index template format and may experience configuration management issues.",
      "rootCause": "Temporal hardcodes deprecated v1 index template format instead of supporting Elasticsearch 7.17+ composable templates",
      "proposedFix": "Convert index templates to use the new composable format as documented in Elasticsearch 7.17 reference",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal uses deprecated index templates to work with Elasticsearch and does not combine the settings from the index template of the new version",
      "number": 4816,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:54.945Z"
    },
    {
      "summary": "Child workflows started immediately before parent closure fail to receive their first workflow task, remaining open indefinitely. These orphaned executions block their workflow IDs and require manual termination.",
      "category": "bug",
      "subcategory": "child-workflows",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-execution",
        "history-processing",
        "task-scheduler"
      ],
      "concepts": [
        "abandonment",
        "closure-timing",
        "workflow-task-delivery",
        "state-synchronization",
        "resource-cleanup"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably start child workflows near parent closure boundaries, leading to resource leaks and blocking of workflow IDs.",
      "rootCause": "Child workflow execution is created but never receives its first workflow task when parent closes simultaneously, leaving it in a limbo state.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #2829",
      "related": [
        2829
      ],
      "keyQuote": "Attempting to start a child workflow right before parent closes creates the child workflow execution but this execution never receives the first workflow task",
      "number": 4812,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:54.531Z"
    },
    {
      "summary": "Request to convert per-instance rate limiters (task scheduler and persistence) to global cluster-wide configuration that automatically adjusts per-instance limits based on the number of instances in the ring.",
      "category": "feature",
      "subcategory": "rate-limiting",
      "apis": [],
      "components": [
        "task-scheduler",
        "persistence",
        "rate-limiter"
      ],
      "concepts": [
        "rate-limiting",
        "cluster-configuration",
        "scaling",
        "per-instance-limits",
        "cluster-wide-config"
      ],
      "severity": "medium",
      "userImpact": "Current per-instance rate limiter configuration is difficult to use and requires manual adjustment; global configuration would simplify cluster management.",
      "rootCause": null,
      "proposedFix": "Provide global (cluster-wide) rate limiter configuration that automatically adjusts per-instance limits based on the number of instances in the ring",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The config for lost of rate limiters in the system is per instance, which is very hard to use. The ask is to provide a global (cluster-wide) config",
      "number": 4806,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:52.073Z"
    },
    {
      "summary": "Shard info persistence is currently time-based (5 minutes by default), causing significant task reprocessing when the cluster experiences high load due to losing task processing progress. Propose making persistence condition based on the number of tasks completed instead of time to checkpoint more frequently under high load.",
      "category": "feature",
      "subcategory": "shard-persistence",
      "apis": [],
      "components": [
        "shard-manager",
        "history-task-processor",
        "queue-state"
      ],
      "concepts": [
        "persistence",
        "checkpointing",
        "task-deduplication",
        "load-handling",
        "progress-tracking"
      ],
      "severity": "medium",
      "userImpact": "Users experience increased task reprocessing and harder rate limiting when cluster load is high due to stale shard info.",
      "rootCause": "Time-based shard info persistence does not adapt to varying load levels, causing 5 minutes of progress loss during high load periods.",
      "proposedFix": "Replace time-based persistence condition with task-count-based condition that checkpoints more frequently when load is high.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If instead of time based, we can make the condition based on # of task processed, we'll be able to check point more often when load is high",
      "number": 4805,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:42.028Z"
    },
    {
      "summary": "CLI server has a hard-coded, unconfigurable limit on custom search attributes that is too restrictive for development use cases. Users need the ability to add more than the pre-allocated custom search attributes to match production environment capabilities.",
      "category": "feature",
      "subcategory": "cli-server",
      "apis": [],
      "components": [
        "cli-server",
        "visibility-schema",
        "search-attributes"
      ],
      "concepts": [
        "custom-search-attributes",
        "development-environment",
        "configuration-limit",
        "search-capabilities",
        "schema-design"
      ],
      "severity": "medium",
      "userImpact": "Developers are forced to use Docker Compose instead of the CLI server for development when they need more than the pre-allocated custom search attributes, degrading developer experience.",
      "rootCause": "Hard-coded pre-allocated number of custom search attributes in the visibility schema that is not configurable and is lower than production limits.",
      "proposedFix": "Have a large upper limitone that's much less likely to be run into.",
      "workaround": "Going back to Docker Compose for local development.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        318
      ],
      "keyQuote": "This hard (unconfigurable) upper limit makes the CLI server unusable in dev for some use cases.",
      "number": 4802,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:42.497Z"
    },
    {
      "summary": "Initial report suggested server version 1.21.4 incorrectly required all workflow task completions to handle buffered signals, but investigation revealed the actual issue was a server change rejecting signal API calls when the last workflow task has failures, which occurs during continueAsNew with buffered signals.",
      "category": "bug",
      "subcategory": "signal-handling",
      "apis": [],
      "components": [
        "signal-processing",
        "workflow-task-completion",
        "continueAsNew"
      ],
      "concepts": [
        "signal-buffering",
        "workflow-execution",
        "error-handling",
        "backward-compatibility",
        "task-failure"
      ],
      "severity": "medium",
      "userImpact": "Users may experience unexpected signal rejection errors when workflow tasks fail during continueAsNew operations with buffered signals, requiring code changes to handle this new behavior.",
      "rootCause": "Server change (commit 759aa2cfe6c1b0c291dc932f4879ca16d9e8284e) that rejects signal API when last workflow task has failure, combined with existing behavior of task failure when continueAsNew is attempted with buffered events.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was misdiagnosed; the reporter determined the actual cause was a feature request implementation change (issue #4137) that rejects signals when workflow task fails, not a requirement for all WFTs to process signals.",
      "related": [
        4137
      ],
      "keyQuote": "There is a change in the server that signal API will be rejected when last workflow task has failure",
      "number": 4801,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:42.027Z"
    },
    {
      "summary": "User requests ability to save and reuse tokens from long-running operations (like Azure SDK's BeginDelete) across activity retries, enabling resumption without starting over.",
      "category": "feature",
      "subcategory": "activity-retry",
      "apis": [],
      "components": [
        "activity-executor",
        "retry-handler"
      ],
      "concepts": [
        "retry",
        "token-persistence",
        "long-running-operations",
        "state-recovery",
        "external-sdk-integration"
      ],
      "severity": "medium",
      "userImpact": "Users calling external SDOs with expensive token-based operations cannot efficiently retry activities without re-executing expensive initial steps.",
      "rootCause": null,
      "proposedFix": "Allow activities to save tokens via heartbeat or structured state mechanism that persists across retries, enabling resumption from saved state.",
      "workaround": "Split activity into two: first fetches token and returns it, second uses token as input parameter. Alternatively, implement custom database to track tokens.",
      "resolution": "wontfix",
      "resolutionDetails": "Temporal team indicated heartbeat details aren't guaranteed, and suggested workarounds (splitting activities or custom state management) are sufficient for the use case.",
      "related": [],
      "keyQuote": "i think you could heartbeat to record the token, and retrieve it from later retry attempt.",
      "number": 4799,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:30.494Z"
    },
    {
      "summary": "Request to add an option for scheduled workflows to use a static workflow ID instead of having Temporal automatically append a timestamp suffix. This would allow users to define predictable workflow IDs for scheduled executions and enable dependent workflows to construct and reference these IDs.",
      "category": "feature",
      "subcategory": "schedules",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "schedules",
        "workflow-id-generation"
      ],
      "concepts": [
        "workflow-id",
        "scheduling",
        "predictability",
        "custom-formatting",
        "timestamp-suffix"
      ],
      "severity": "medium",
      "userImpact": "Users cannot control workflow ID formatting for scheduled executions, preventing predictable IDs needed for inter-workflow dependencies and custom scheduling patterns.",
      "rootCause": null,
      "proposedFix": "Add configuration option to control timestamp suffix behavior: allow static IDs, custom formatting, hardcoded suffixes, or callback-based ID generation",
      "workaround": "Use schedule.trigger() instead of automatic execution for workflows requiring custom IDs",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add option for workflow id used to be as user defines in workflow options for each execution schedule creates",
      "number": 4795,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:28.466Z"
    },
    {
      "summary": "DescribeWorkflow and similar RPCs with an empty Run ID fail when the current run has been deleted, even if other runs with the same Workflow ID exist. The server doesn't properly fall back to previous runs when the current run is missing.",
      "category": "bug",
      "subcategory": "workflow-state-management",
      "apis": [
        "DescribeWorkflow",
        "GetWorkflowHistory"
      ],
      "components": [
        "workflow-service",
        "persistence",
        "run-management"
      ],
      "concepts": [
        "run-deletion",
        "continue-as-new",
        "workflow-reset",
        "empty-run-id",
        "state-consistency"
      ],
      "severity": "medium",
      "userImpact": "Users cannot query workflow information with an empty Run ID after deleting the current run, breaking expected fallback behavior to previous runs.",
      "rootCause": "The server fetches the current run-id when one is not provided, but deleting the current run does not automatically make a previous run current, causing subsequent queries to fail with not found errors.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed by improving run lookup logic to handle deleted current runs properly.",
      "related": [],
      "keyQuote": "Delete current run does not make previous run as current run.",
      "number": 4792,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:30.503Z"
    },
    {
      "summary": "Search attributes set on workflows within schedules lack eager validation and metadata, unlike all other search attribute contexts. This needs to be brought into compliance with validation and metadata requirements while considering backwards-compatibility implications for existing automated processes.",
      "category": "other",
      "subcategory": "schedules",
      "apis": [],
      "components": [
        "schedules",
        "search-attributes",
        "workflow-execution",
        "metadata-validation"
      ],
      "concepts": [
        "search-attributes",
        "validation",
        "metadata",
        "schedules",
        "backwards-compatibility",
        "type-metadata"
      ],
      "severity": "medium",
      "userImpact": "Users with automated processes that create schedules with workflow search attributes may experience unexpected failures if validation is enforced retroactively.",
      "rootCause": "Search attributes on scheduled workflows are not eagerly validated and lack type metadata, unlike search attributes in other contexts.",
      "proposedFix": "Implement eager validation and metadata application for search attributes on scheduled workflows, with careful consideration of backwards-compatibility.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Search attribute validation and metadata handling for scheduled workflows was brought into compliance with other contexts.",
      "related": [],
      "keyQuote": "We need to bring search attributes on workflows of schedules in line w/ all others with regards to validation and metadata.",
      "number": 4787,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:18.611Z"
    },
    {
      "summary": "Users with temporal-system:read permission should have access to the ListSearchAttributes read-only API when using Temporal's default JWT authorizer, but currently lack this access, preventing them from using the Temporal UI and CLI.",
      "category": "bug",
      "subcategory": "authorization",
      "apis": [
        "ListSearchAttributes"
      ],
      "components": [
        "authorization",
        "default-authorizer",
        "frontend-api"
      ],
      "concepts": [
        "permissions",
        "jwt",
        "access-control",
        "search-attributes",
        "role-based-access"
      ],
      "severity": "high",
      "userImpact": "Users with temporal-system:read role cannot use Temporal UI or CLI to list search attributes, blocking their upgrade to v1.21.x.",
      "rootCause": "The ListSearchAttributes API is not included in the temporal-system:read permission scope in the default JWT authorizer implementation.",
      "proposedFix": "Add ListSearchAttributes API to the authorization rules for temporal-system:read permission in the default authorizer.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Closed by PR #4794 which added ListSearchAttributes access to the temporal-system:read role.",
      "related": [
        4654,
        4794
      ],
      "keyQuote": "Users with temporal-system:read role cannot use Temporal UI (after logging in they are immediately redirected back to the login screen because of the 403 error for the ListSearchAttributes API).",
      "number": 4783,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:16.895Z"
    },
    {
      "summary": "Each activity heartbeat incurs two state transitions: one when RecordActivityHeartbeat is called and another when the previous heartbeat timeout fires. The issue proposes optimizing this by creating the new heartbeat timeout timer during the API call to make the previous timer a no-op.",
      "category": "bug",
      "subcategory": "activity-heartbeat",
      "apis": [
        "RecordActivityHeartbeat"
      ],
      "components": [
        "heartbeat-timeout",
        "state-machine",
        "activity-executor"
      ],
      "concepts": [
        "state-transition",
        "timeout",
        "optimization",
        "timer",
        "heartbeat"
      ],
      "severity": "low",
      "userImpact": "Each activity heartbeat causes unnecessary state transitions that consume system resources and may impact performance.",
      "rootCause": "The heartbeat timeout timer is created after the previous timeout fires rather than during the RecordActivityHeartbeat API call, resulting in redundant state transitions.",
      "proposedFix": "Create the new heartbeat timeout timer when RecordActivityHeartbeat API is called, making the previous heartbeat timeout timer a no-op when it fires.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "After discussion, the optimization was deemed to introduce many edge cases that could perform worse than the current approach.",
      "related": [],
      "keyQuote": "Trying to optimize it would result in many edge cases that may perform worse.",
      "number": 4781,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:18.536Z"
    },
    {
      "summary": "User requests the ability to split log files by date rather than having all logs in a single file, making log management easier. A suggested solution points to time-based log rotation using Zap, with community interest in contributing.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "logging",
        "file-rotation",
        "log-management"
      ],
      "concepts": [
        "log-rotation",
        "file-management",
        "time-based-splitting",
        "log-organization",
        "file-size-management"
      ],
      "severity": "low",
      "userImpact": "Users currently struggle to manage and handle logs when they are all written to a single file, affecting operational visibility and log analysis.",
      "rootCause": null,
      "proposedFix": "Implement time-based log file rotation using Zap, as suggested in the referenced blog post about time-based log file rotation with Zap.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the logs are all printed in one file, which is difficult to handle",
      "number": 4778,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:06.496Z"
    },
    {
      "summary": "Users encounter a 403 Forbidden error when attempting to create custom search attributes in Elasticsearch/OpenSearch due to permission issues with the Temporal system workflow. The error occurs with both CLI commands and affects visibility configuration with advanced visibility store enabled.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "elasticsearch",
        "visibility-store",
        "search-attributes",
        "system-workflow"
      ],
      "concepts": [
        "permissions",
        "elasticsearch-mapping",
        "advanced-visibility",
        "configuration",
        "custom-attributes"
      ],
      "severity": "high",
      "userImpact": "Users cannot create custom search attributes when using Elasticsearch/OpenSearch with Temporal self-hosted deployments, blocking visibility feature usage.",
      "rootCause": "Elasticsearch/OpenSearch user (temporal) lacks permissions to create or update mappings for custom search attributes, likely a configuration or security setup issue.",
      "proposedFix": "Adjust Elasticsearch/OpenSearch security policies and permissions to allow the temporal user to create/update field mappings, or properly configure visibility store settings to use only advanced visibility without dual storage.",
      "workaround": "Configure only advancedVisibilityStore and remove visibilityStore and secondaryVisibilityStore to avoid dual-store conflicts and permission issues.",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved by adjusting configuration to use only advancedVisibilityStore and implementing proper Elasticsearch permission settings (related to v1.21.0 visibility config changes).",
      "related": [
        3889
      ],
      "keyQuote": "unable to update Elasticsearch mapping: elastic: Error 403 (Forbidden): no permissions for [] and User [name=temporal",
      "number": 4773,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:06.610Z"
    },
    {
      "summary": "SignalWithStart operations were failing with a non-retryable FAILED_PRECONDITION error when workflows were closing, introduced in server 1.21. The server rejects signals for executions attempting to complete, causing client errors that cannot be automatically retried.",
      "category": "bug",
      "subcategory": "signal-with-start",
      "apis": [
        "SignalWithStart"
      ],
      "components": [
        "server",
        "signal-handler",
        "workflow-execution"
      ],
      "concepts": [
        "signal-rejection",
        "workflow-closing",
        "error-handling",
        "retry-behavior",
        "resource-exhaustion"
      ],
      "severity": "high",
      "userImpact": "Applications experience non-retryable errors when signaling workflows that are closing, preventing workflow signal operations from succeeding even with retry logic.",
      "rootCause": "Server 1.21 change to reject signals for workflow executions attempting to complete, using non-retryable FAILED_PRECONDITION error code instead of retryable error.",
      "proposedFix": "Change error code from non-retryable FAILED_PRECONDITION to retryable RESOURCE_EXHAUSTED (ResourceExhaustedError with WorkflowBusy cause), with future plans to carry signals to next execution on continue-as-new.",
      "workaround": "Implement slow retry on client side to allow workflow time to close before retrying SignalWithStart operation.",
      "resolution": "fixed",
      "resolutionDetails": "Short-term fix implemented in PR #4765, included in server 1.21.5 and 1.22 releases, converting error to retryable RESOURCE_EXHAUSTED.",
      "related": [
        4765
      ],
      "keyQuote": "This error should not use a non retryable FAILED_PRECONDITION code... will change to a retryable RESOURCE_EXHAUSTED",
      "number": 4764,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:57:07.033Z"
    },
    {
      "summary": "Cassandra schema update tool is not idempotent - retrying after partial failure causes table creation to fail with 'table already exists' error instead of succeeding.",
      "category": "bug",
      "subcategory": "cassandra-schema",
      "apis": [],
      "components": [
        "cassandra",
        "schema-update-tool",
        "persistence"
      ],
      "concepts": [
        "idempotency",
        "retry",
        "schema-migration",
        "table-creation",
        "error-handling"
      ],
      "severity": "high",
      "userImpact": "Users cannot safely retry failed schema updates, making it difficult to recover from partial failures during database schema initialization.",
      "rootCause": "Schema update logic does not check if table already exists before attempting creation, and version update is not atomic with table creation.",
      "proposedFix": "Make schema update tool idempotent by checking table existence before creation and ensuring version updates are properly handled on retry.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #4761 which made the cassandra schema update tool idempotent.",
      "related": [
        4761
      ],
      "keyQuote": "If the table is created but the version was not updated, the retry would keep failing on table already exists error.",
      "number": 4760,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:54.862Z"
    },
    {
      "summary": "Add support for Temporal server to listen on both IPv4 and IPv6 addresses simultaneously in dual-stack environments, allowing operators to support both protocols during the transition to IPv6 without running multiple service instances.",
      "category": "feature",
      "subcategory": "network-binding",
      "apis": [],
      "components": [
        "server",
        "rpc-config",
        "network-listener"
      ],
      "concepts": [
        "ipv6-migration",
        "dual-stack",
        "network-binding",
        "port-sharing",
        "multihoming"
      ],
      "severity": "medium",
      "userImpact": "Operators can more easily support IPv6 migration by binding to both IPv4 and IPv6 addresses simultaneously without running multiple service instances.",
      "rootCause": null,
      "proposedFix": "By default, bind to both IPv4 and IPv6 addresses when available using the same port. Modify BindOnLocalHost to bind to both 127.0.0.1 and ::1, and add configuration to specify multiple IPs for multihomed environments.",
      "workaround": "Run multiple instances of the same service on a host, one bound to IPv4 and one to IPv6, though this is more resource intensive.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "by default, if an external ipv4 and ipv6 address are available, bind to both using the same port",
      "number": 4749,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:54.335Z"
    },
    {
      "summary": "Discussion about the deprecation of ringpop library and long-term strategy for replacing it with a well-maintained alternative. The core question is whether Temporal has plans to replace ringpop for membership management, given that it's been abstracted.",
      "category": "question",
      "subcategory": "infrastructure-architecture",
      "apis": [],
      "components": [
        "ringpop",
        "membership-management",
        "cluster-management"
      ],
      "concepts": [
        "active-active",
        "datacenter-replication",
        "consensus",
        "membership-discovery",
        "deprecation"
      ],
      "severity": "low",
      "userImpact": "Users interested in active-active deployments or running Temporal across multiple datacenters need clarity on the long-term replacement strategy for ringpop.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Closed by issue author after receiving clarification that ringpop replacement is desired but not currently a priority. Not a bug or feature request, just an informational discussion.",
      "related": [
        2471
      ],
      "keyQuote": "Yes, we do want to replace ringpop with a well maintained library, but this is currently not a priority.",
      "number": 4748,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:53.311Z"
    },
    {
      "summary": "Search attributes registered for a namespace on the current cluster are not replicated when that namespace is replicated to a new cluster via the clusters list update. This feature request asks for custom search attributes to be included in the namespace replication process.",
      "category": "feature",
      "subcategory": "namespace-replication",
      "apis": [],
      "components": [
        "namespace-replication",
        "search-attributes",
        "cluster-management"
      ],
      "concepts": [
        "replication",
        "namespace-management",
        "search-attributes",
        "multi-cluster",
        "metadata-sync"
      ],
      "severity": "medium",
      "userImpact": "Users must manually recreate search attributes on new clusters when replicating namespaces, adding operational overhead.",
      "rootCause": null,
      "proposedFix": "Replicate custom search attributes as part of the namespace replication process when updating a namespace's clusters list.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Replicate custom search attributes as part of namespace replication.",
      "number": 4745,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:41.764Z"
    },
    {
      "summary": "PollWorkflowExecutionUpdate API returns DeadlineExceededError when results are not ready, but should return an empty response like PollWorkflowTaskQueue and PollActivityTaskQueue for consistency.",
      "category": "bug",
      "subcategory": "workflow-update-polling",
      "apis": [
        "PollWorkflowExecutionUpdate",
        "PollWorkflowTaskQueue",
        "PollActivityTaskQueue"
      ],
      "components": [
        "workflow-update-handler",
        "polling-engine",
        "api-consistency"
      ],
      "concepts": [
        "polling",
        "deadline-exceeded",
        "empty-response",
        "api-consistency",
        "result-readiness"
      ],
      "severity": "medium",
      "userImpact": "Users experience inconsistent API behavior when polling for workflow execution updates, complicating client code that expects uniform polling semantics.",
      "rootCause": "PollWorkflowExecutionUpdate implementation differs from PollWorkflowTaskQueue and PollActivityTaskQueue in error handling for missing results.",
      "proposedFix": "Return an empty response instead of DeadlineExceededError when results are not ready, matching the behavior of PollWorkflowTaskQueue and PollActivityTaskQueue.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation updated to return empty response instead of DeadlineExceededError for consistency with other polling operations.",
      "related": [],
      "keyQuote": "PollWorkflowExecutionUpdate should return empty response instead of returning DeadlineExceededError. The behavior should be the same as PollWorkflowTaskQueue/PollActivityTaskQueue.",
      "number": 4742,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:43.422Z"
    },
    {
      "summary": "User requests additional schedule states beyond Running and Pause (e.g., Completed, Terminated) to track one-time schedules and maintain record history longer than the default 1-week auto-deletion.",
      "category": "feature",
      "subcategory": "schedule-management",
      "apis": [],
      "components": [
        "schedule",
        "workflow-execution",
        "state-management"
      ],
      "concepts": [
        "schedule-states",
        "lifecycle-management",
        "record-retention",
        "one-time-execution",
        "auto-deletion"
      ],
      "severity": "low",
      "userImpact": "Users cannot track schedule completion status or retain schedule records beyond automatic deletion, limiting their ability to maintain schedule execution history.",
      "rootCause": null,
      "proposedFix": "Introduce additional schedule states (e.g., Completed, Terminated) similar to Workflow states to provide better lifecycle tracking and state persistence.",
      "workaround": "Use StartDelay to schedule one-time workflow execution instead of creating a schedule that runs once.",
      "resolution": "wontfix",
      "resolutionDetails": "The feature request was not implemented. Instead, guidance was provided on using StartDelay as an alternative for one-time workflow execution.",
      "related": [
        338
      ],
      "keyQuote": "i want to more schedule state, example: completed, terminated... just like WorkFlow state.",
      "number": 4737,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:42.064Z"
    },
    {
      "summary": "Build fails on 32-bit platforms due to math.MaxInt64 constant overflowing untyped int variable declarations. The issue was identified in cassandra/errors.go but may exist elsewhere in the codebase.",
      "category": "bug",
      "subcategory": "build-compatibility",
      "apis": [],
      "components": [
        "persistence",
        "cassandra",
        "build-system"
      ],
      "concepts": [
        "32-bit platforms",
        "type inference",
        "integer overflow",
        "golang",
        "compilation",
        "platform support"
      ],
      "severity": "low",
      "userImpact": "Users on 32-bit platforms (e.g., Raspberry Pi) cannot build Temporal from source.",
      "rootCause": "Untyped int variable declarations default to native int type (4 bytes on 32-bit platforms), causing overflow when assigned math.MaxInt64 value",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers explicitly stated no plan to support 32-bit platforms",
      "related": [],
      "keyQuote": "No plan to support 32-bit platform at the moment.",
      "number": 4736,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:30.409Z"
    },
    {
      "summary": "Frontend handler retry count is set too high (5) compared to other system components (2), causing excessive retries of up to 18 times for failed workflow history requests. Proposal is to reduce frontendHandlerRetryMaxAttempts to 3 after fixing a related retry amplification issue.",
      "category": "bug",
      "subcategory": "frontend-handler",
      "apis": [],
      "components": [
        "frontend-handler",
        "history-client",
        "workflow-history"
      ],
      "concepts": [
        "retry",
        "performance",
        "max-attempts",
        "exponential-backoff",
        "system-reliability"
      ],
      "severity": "medium",
      "userImpact": "Excessive retries increase latency and reduce system efficiency for workflow history operations.",
      "rootCause": "Frontend handler retry configuration is inconsistent with rest of the system; combined with retry amplification in issue #4732, causes up to 18 retry attempts",
      "proposedFix": "Reduce frontendHandlerRetryMaxAttempts from 5 to 3, ensuring maximum 6 retries when combined with client layer retries",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue has been closed, indicating the retry count reduction and associated fix have been implemented",
      "related": [
        4732
      ],
      "keyQuote": "a bad read workflow history request can be retried up to (5+1) * (2+1) = 18 times!",
      "number": 4733,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:31.103Z"
    },
    {
      "summary": "RetryPolicy.WithMaximumAttempts(X) allows operations to run X+1 times instead of X times, causing one more retry attempt than expected.",
      "category": "bug",
      "subcategory": "retry-policy",
      "apis": [
        "ThrottleRetry",
        "NewExponentialRetryPolicy",
        "WithMaximumAttempts"
      ],
      "components": [
        "backoff",
        "retry-policy",
        "retry-logic"
      ],
      "concepts": [
        "retry",
        "exponential-backoff",
        "maximum-attempts",
        "attempt-counting"
      ],
      "severity": "medium",
      "userImpact": "Users expecting a specific maximum number of retry attempts will experience one additional attempt than configured, potentially causing unexpected behavior in retry logic.",
      "rootCause": "Off-by-one error in the retry attempt counting logic within ThrottleRetry implementation",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When a retry policy is created with backoff.NewExponentialRetryPolicy(initialInterval).WithMaximumAttempts(X), backoff.ThrottleRetry(op, policy, nil) will run op at most X+1 times",
      "number": 4732,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:31.315Z"
    },
    {
      "summary": "Internal SDK client uses report themselves as 'temporal-go' clients, conflating them with actual SDK users. The request is to distinguish internal server uses by setting a different client-name in gRPC headers.",
      "category": "other",
      "subcategory": "client-identification",
      "apis": [],
      "components": [
        "grpc-client",
        "client-headers",
        "metadata"
      ],
      "concepts": [
        "client-identification",
        "telemetry",
        "internal-vs-external",
        "header-metadata",
        "server-instrumentation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot distinguish between real SDK client usage and internal server usage of the SDK in monitoring and analytics.",
      "rootCause": "Internal SDK client instances set the same gRPC client-name header as external SDK users, making them indistinguishable in telemetry.",
      "proposedFix": "Set client-name to 'temporal-server' or similar for internal uses, either via metadata after initialization or through a new 'client-name-override' field.",
      "workaround": "Use a client interceptor to modify gRPC metadata after client creation to set a different client-name.",
      "resolution": "fixed",
      "resolutionDetails": "Enhancement implemented to allow distinguishing internal SDK usage from external client usage through gRPC headers.",
      "related": [],
      "keyQuote": "The internal uses of the SDK should set it to something besides `temporal-go` to not conflate it with actual `temporal-go` clients.",
      "number": 4730,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:18.096Z"
    },
    {
      "summary": "Custom text search attributes with dashes in values fail in SQLite backend with column not found errors, while working correctly with Elasticsearch. Query parsing incorrectly splits the dashed value, treating the suffix as a separate column name.",
      "category": "bug",
      "subcategory": "visibility-search",
      "apis": [],
      "components": [
        "sqlite-backend",
        "visibility-search",
        "custom-attributes",
        "query-parser"
      ],
      "concepts": [
        "search-attributes",
        "sqlite",
        "escaping",
        "parsing",
        "custom-fields",
        "text-search"
      ],
      "severity": "high",
      "userImpact": "Users cannot query workflows by custom text fields containing dashes when using SQLite backend, causing visibility feature to fail.",
      "rootCause": "Escaping or parsing issue with dashes in custom attribute values - the dash is incorrectly handled causing the value to be split and treated as separate column references.",
      "proposedFix": "Fix escaping/parsing logic in SQLite query handler to properly handle dashed values in custom text fields, as referenced in PR #4755.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #4755 which addresses the escaping/parsing issue with dashes in SQLite custom attribute queries.",
      "related": [
        4755
      ],
      "keyQuote": "SQL logic error: no such column: stresstest (1) - occurs when querying custom text fields with dash-containing values like 'configdriven-stresstest'",
      "number": 4729,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:17.573Z"
    },
    {
      "summary": "Worker processes experience severe GC overhead and memory growth due to inefficient metric tag allocations in the tagsToAttributes function. Metrics scraping takes 7+ seconds even under light load, eventually causing CPU to spike and the process to become unresponsive.",
      "category": "bug",
      "subcategory": "metrics-performance",
      "apis": [],
      "components": [
        "metrics",
        "prometheus",
        "opentelemetry",
        "garbage-collector"
      ],
      "concepts": [
        "memory-leak",
        "garbage-collection",
        "cpu-overhead",
        "metric-tags",
        "allocation-efficiency",
        "performance-degradation",
        "resource-exhaustion"
      ],
      "severity": "high",
      "userImpact": "Workers gradually consume more CPU and memory over time, eventually becoming unresponsive and requiring manual restarts.",
      "rootCause": "The tagsToAttributes function recalculates metric tags on every call despite tags being static (from config or compilation), causing excessive allocations and triggering frequent GC cycles.",
      "proposedFix": "Cache metric tag attributes instead of recalculating them each time, as tags don't change after initialization.",
      "workaround": "Increase metrics scrape interval from 15 seconds to 60 seconds to delay the degradation.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #4722 which was merged and released in server version 1.22. User confirmed the fix resolved the issue.",
      "related": [
        3578,
        3015,
        4722
      ],
      "keyQuote": "the majority of the allocations were in metrics.tagsToAttributes... recalculating this every time does seem inefficient",
      "number": 4716,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:19.223Z"
    },
    {
      "summary": "Server crashes when upgrading from v1.20.4 to v1.21.4 with archival disabled. The cluster fails to start after the upgrade even though archival was never enabled in the previous version.",
      "category": "bug",
      "subcategory": "archival-configuration",
      "apis": [],
      "components": [
        "server",
        "archival",
        "configuration",
        "upgrade-migration"
      ],
      "concepts": [
        "cluster-upgrade",
        "configuration-compatibility",
        "archival-state",
        "server-startup",
        "backward-compatibility"
      ],
      "severity": "high",
      "userImpact": "Users cannot upgrade their Temporal clusters from v1.20.4 to v1.21.4 when archival is disabled, causing service downtime.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4638,
        4634
      ],
      "keyQuote": "Able to upgrade temporal cluster from 1.20.4 to 1.21.4 with archival disabled",
      "number": 4715,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:05.065Z"
    },
    {
      "summary": "History task executor uses retryable history/matching client which causes unnecessary task retries while holding goroutines, particularly problematic when requests are rate limited and retried with backoff.",
      "category": "bug",
      "subcategory": "history-task-executor",
      "apis": [],
      "components": [
        "history-task-executor",
        "history-client",
        "matching-client"
      ],
      "concepts": [
        "retry-policy",
        "rate-limiting",
        "goroutine-blocking",
        "client-backoff",
        "task-processing"
      ],
      "severity": "high",
      "userImpact": "Task processing performance degrades significantly during rate limiting as goroutines are held while requests are slowly retried with backoff.",
      "rootCause": "History task executor uses retryable client for remote service calls instead of raw client, causing tasks to retry unnecessarily while holding goroutines.",
      "proposedFix": "Use raw history/matching client instead of retryable client in history task executor.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Using retryable history/matching client for talking to remote service when processing tasks are unnecessary and will cause tasks to retry while holding the goroutine.",
      "number": 4707,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:04.760Z"
    },
    {
      "summary": "Users have requested the ability to specify a list of datetimes in ScheduleSpec for more flexible scheduling options. This feature is on the internal roadmap and is being tracked as an enhancement.",
      "category": "feature",
      "subcategory": "schedules",
      "apis": [
        "ScheduleSpec"
      ],
      "components": [
        "schedules",
        "schedule-spec",
        "temporal-api"
      ],
      "concepts": [
        "scheduling",
        "datetime-specification",
        "schedule-flexibility",
        "temporal-expressions",
        "workflow-scheduling",
        "time-specification"
      ],
      "severity": "medium",
      "userImpact": "Users currently cannot specify arbitrary lists of datetimes for scheduling workflows, limiting flexibility in complex scheduling scenarios.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Been requested by a few users, and I think is on internal roadmap.",
      "number": 4703,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:56:04.188Z"
    },
    {
      "summary": "Temporal Server fails to connect to Elasticsearch when TLS is enabled on the HTTP layer, returning 'no ElasticSearch node available' error. The issue occurs with Elasticsearch 8.x configured with HTTPS, though the same certificates work with other tools.",
      "category": "bug",
      "subcategory": "elasticsearch-connectivity",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "server-config",
        "http-transport"
      ],
      "concepts": [
        "tls",
        "ssl-certificate",
        "elasticsearch",
        "connection",
        "authentication",
        "https"
      ],
      "severity": "high",
      "userImpact": "Users with Elasticsearch clusters using TLS certificates cannot connect their Temporal Server, blocking visibility into workflow history.",
      "rootCause": "Temporal Server's Elasticsearch HTTP client does not properly handle or validate TLS certificates from known issuers (Let's Encrypt), requiring custom HTTP client configuration to disable certificate verification.",
      "proposedFix": "The Temporal Server should either support certificate configuration in the server config YAML or provide better error messages. Alternatively, implement proper TLS certificate validation for standard certificate authorities.",
      "workaround": "Create a custom HTTP client with InsecureSkipVerify set to true and pass it via temporal.WithElasticsearchHttpClient() when instantiating the server.",
      "resolution": "fixed",
      "resolutionDetails": "Community discovered workaround of using custom HTTP client with InsecureSkipVerify. Issue likely resolved by documentation update or server code fix supporting proper TLS configuration.",
      "related": [],
      "keyQuote": "Would be helpful to mention the ability to define a custom HTTP client for Elasticsearch in server-options on docs.temporal.io",
      "number": 4695,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:53.857Z"
    },
    {
      "summary": "User cannot filter by both ExecutionStatus and WorkflowType simultaneously and is asking why this limitation exists. The response explains that standard visibility (Cassandra/SQL) has limited filtering and advanced visibility (Elasticsearch/SQL) is needed for complex filter combinations.",
      "category": "question",
      "subcategory": "visibility-filtering",
      "apis": [],
      "components": [
        "visibility",
        "cassandra",
        "elasticsearch",
        "sql"
      ],
      "concepts": [
        "filtering",
        "query-limitations",
        "visibility-backend",
        "advanced-visibility",
        "execution-status"
      ],
      "severity": "low",
      "userImpact": "Users cannot perform combined filtering on ExecutionStatus and WorkflowType with standard visibility, requiring them to switch to advanced visibility to use more complex filter combinations.",
      "rootCause": "Standard visibility backend (Cassandra/SQL) has limited filtering functionality and does not support this combination of filters.",
      "proposedFix": "Enable advanced visibility backed by Elasticsearch or SQL to support different filter combinations.",
      "workaround": "Switch from standard visibility to advanced visibility (Elasticsearch or SQL backend) to enable combined filtering capabilities.",
      "resolution": "wontfix",
      "resolutionDetails": "The limitation is by design in standard visibility; users should use advanced visibility (Elasticsearch or SQL) for complex filtering needs.",
      "related": [],
      "keyQuote": "It supports only limited filtering functionality... To use different filter combinations you need to enable advanced visibility.",
      "number": 4693,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:50.691Z"
    },
    {
      "summary": "Optimization request to reduce unnecessary database writes by checking if the ack level has changed before persisting, avoiding duplicate persistence operations when workers are idle.",
      "category": "feature",
      "subcategory": "persistence-optimization",
      "apis": [],
      "components": [
        "taskReader",
        "ackLevel",
        "persistence",
        "updateAckTimer"
      ],
      "concepts": [
        "database-optimization",
        "duplicate-elimination",
        "idle-worker",
        "persistence",
        "ackLevel-tracking"
      ],
      "severity": "low",
      "userImpact": "Reduces unnecessary database requests and improves performance for long-idle workflow or activity workers by eliminating redundant ack level persistence.",
      "rootCause": "The taskReader persists ack level on every updateAckTimer without checking if the ack level has actually changed, causing unnecessary database writes.",
      "proposedFix": "Store the last persisted ack level in taskReader and only persist when the current ack level differs from the last persisted value.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "taskReader store last persist ack level and do persistAckLevel only when current ack level not match last persist ack level",
      "number": 4692,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:51.329Z"
    },
    {
      "summary": "Workflow with disconnected context for cancellation handling fails with 'BadRequestCancelActivityAttributes: invalid history builder state' error when trying to execute activities after cancellation using a disconnected context.",
      "category": "bug",
      "subcategory": "cancellation-handling",
      "apis": [
        "ExecuteActivity",
        "NewDisconnectedContext",
        "WithCancel",
        "NewSelector",
        "NewTimer"
      ],
      "components": [
        "workflow-execution",
        "activity-cancellation",
        "history-builder",
        "context-management"
      ],
      "concepts": [
        "cancellation",
        "disconnected-context",
        "activity-execution",
        "history-validation",
        "graceful-shutdown"
      ],
      "severity": "high",
      "userImpact": "Users attempting to use disconnected contexts for graceful cancellation handling encounter workflow execution failures that prevent proper cleanup logic.",
      "rootCause": "Invalid history builder state when attempting to add activity cancel-requested events using a disconnected context in a specific cancellation workflow pattern.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Duplicate of sdk-go#1176, fixed by sdk-go PR #1181",
      "related": [
        1176
      ],
      "keyQuote": "The workflow fails with BadRequestCancelActivityAttributes: invalid history builder state for action: add-activitytask-cancel-requested-event",
      "number": 4673,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:40.649Z"
    },
    {
      "summary": "Default authorizer blocks OperatorService calls that should be allowed for namespace-reader and namespace-admin users. ListSearchAttributes should be accessible to namespace-reader, while Add/RemoveSearchAttributes and DeleteNamespace should be accessible to namespace-admin.",
      "category": "bug",
      "subcategory": "authorization",
      "apis": [
        "ListSearchAttributes",
        "AddSearchAttributes",
        "RemoveSearchAttributes",
        "DeleteNamespace"
      ],
      "components": [
        "authorizer",
        "operator-service",
        "namespace-management"
      ],
      "concepts": [
        "authorization",
        "permissions",
        "namespace-claims",
        "access-control",
        "search-attributes"
      ],
      "severity": "high",
      "userImpact": "UI and SDK operations fail because the default authorizer incorrectly blocks OperatorService calls that legitimate namespace-reader and namespace-admin users need to perform.",
      "rootCause": "Default authorizer applies overly restrictive permission requirements to OperatorService calls, requiring system permissions for operations that should be allowed with namespace-level claims.",
      "proposedFix": "Update default authorizer to allow ListSearchAttributes for namespace-reader, and Add/RemoveSearchAttributes & DeleteNamespace for namespace-admin, while requiring system permissions for remote cluster calls.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Default authorizer was updated to allow specified OperatorService calls for appropriate namespace-level permission claims.",
      "related": [],
      "keyQuote": "Default authorizer should allow ListSearchAttributes to namespace-reader and Add/RemoveSearchAttributes & DeleteNamespace to namespace-admin.",
      "number": 4654,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:39.862Z"
    },
    {
      "summary": "Flaky test that fails intermittently due to index out of range panic in TestUpdateWorkflow_NormalScheduledWorkflowTask_AcceptComplete. The test fails approximately once every 10 runs, suggesting a race condition or timing issue in workflow task processing.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "workflow-task-processor",
        "task-poller",
        "integration-tests"
      ],
      "concepts": [
        "flaky-test",
        "race-condition",
        "index-bounds",
        "workflow-update",
        "test-reliability"
      ],
      "severity": "medium",
      "userImpact": "Developers experience intermittent test failures that reduce confidence in the test suite and can block CI/CD pipelines.",
      "rootCause": "Index out of range panic suggests the expected workflow task list is empty when accessed, likely due to a race condition or timing issue in task polling.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was resolved through fixes to the workflow update task handling logic to ensure proper synchronization and prevent race conditions.",
      "related": [],
      "keyQuote": "panic: runtime error: index out of range [0] with length 0",
      "number": 4641,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:38.937Z"
    },
    {
      "summary": "Flaky test TestDisableUserData_QueryTimesOut fails intermittently (1-2 out of 10 runs) with a DeadlineExceeded error not appearing in the expected error chain.",
      "category": "bug",
      "subcategory": "test-flakiness",
      "apis": [],
      "components": [
        "versioning-test",
        "query-timeout",
        "error-handling"
      ],
      "concepts": [
        "test-flakiness",
        "deadline-exceeded",
        "error-chain",
        "timing-sensitivity"
      ],
      "severity": "medium",
      "userImpact": "Flaky tests reduce confidence in the codebase and make CI/CD pipelines unreliable.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Should be in error chain: expected: %!q(**serviceerror.DeadlineExceeded=0x14000c9e780) in chain:",
      "number": 4640,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:27.529Z"
    },
    {
      "summary": "Design and implement a proper deprecation strategy for history queues. Currently, removing a queue definition causes undefined behavior (panic in 1.21, silent failure in master), risking data loss or configuration issues.",
      "category": "feature",
      "subcategory": "history-queue-lifecycle",
      "apis": [],
      "components": [
        "history-queue",
        "shard-info",
        "task-category",
        "archival-queue"
      ],
      "concepts": [
        "queue-deprecation",
        "data-retention",
        "configuration-management",
        "graceful-shutdown",
        "backlog-handling",
        "lifecycle-management"
      ],
      "severity": "high",
      "userImpact": "Users cannot safely remove or disable history queues (including archival) without risking data loss or server panics, complicating queue management and disabling archival.",
      "rootCause": "No proper lifecycle management for history queues; skipping queue registration causes undefined behavior due to stale queue references in shard metadata.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We should have a proper design for history queue deprecation. This could be useful for disabling archival queue and in the future if more history queues needs to be dynamically created and deleted.",
      "number": 4638,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:27.996Z"
    },
    {
      "summary": "The child_workflow_command counter in the history service handler increments for each StartChildWorkflow command but cannot be filtered by namespace. A feature request to add namespace filtering capability to this metric, potentially by consolidating metrics and adding command_type and namespace tags.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [
        "StartChildWorkflow"
      ],
      "components": [
        "history-service",
        "metrics",
        "command-handler"
      ],
      "concepts": [
        "filtering",
        "metrics-tagging",
        "namespace-isolation",
        "observability",
        "command-tracking"
      ],
      "severity": "low",
      "userImpact": "Users cannot filter child_workflow_command metrics by namespace, limiting observability and troubleshooting capabilities in multi-tenant or multi-namespace deployments.",
      "rootCause": "The child_workflow_command metric lacks namespace tagging in its current implementation.",
      "proposedFix": "Consolidate individual command metrics into a single metric with command_type and namespace tags for better filtering capabilities.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented namespace tagging and command-type tagging for metrics as suggested in the discussion.",
      "related": [],
      "keyQuote": "We probably should change to emit one metric for all command and tag a command_type to that metric. Should also tag namespace to it.",
      "number": 4628,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:27.459Z"
    },
    {
      "summary": "Matching service enters a busy loop generating excessive database requests when the history service is unavailable, as failed task dispatches trigger task acking and recreation indefinitely.",
      "category": "bug",
      "subcategory": "task-dispatch",
      "apis": [],
      "components": [
        "matching-service",
        "task-dispatch",
        "persistence",
        "history-service"
      ],
      "concepts": [
        "busy-loop",
        "resource-exhaustion",
        "service-availability",
        "task-queue",
        "database-load",
        "service-startup-order"
      ],
      "severity": "high",
      "userImpact": "Operators experience severe database resource consumption and potential cluster instability when history service is unavailable, requiring careful service startup sequencing.",
      "rootCause": "Failed task dispatch logic acknowledges the loaded task and creates a new one to prevent blocking, creating an infinite loop when history service is unavailable.",
      "proposedFix": null,
      "workaround": "Start services in specific sequence: history, matching, frontend, worker.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When task in matching tasks table failed to be dispatched, the loaded task will be acked and a new task will be created. This is essentially a busy loop and consume tons of persistence resources.",
      "number": 4612,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:15.968Z"
    },
    {
      "summary": "Request to use separate task queues for activity and workflow task retries to enable different retry behaviors based on error codes and prevent cascading failures across healthy tasks.",
      "category": "feature",
      "subcategory": "retry-policy",
      "apis": [],
      "components": [
        "retry-policy",
        "task-queue",
        "activity-executor",
        "workflow-task-processor"
      ],
      "concepts": [
        "retry",
        "task-queue",
        "error-handling",
        "rate-limiting",
        "backoff",
        "error-code-based-routing"
      ],
      "severity": "medium",
      "userImpact": "Applications need fine-grained control over retry behavior based on error types, and improved isolation between failing and healthy task processing.",
      "rootCause": "Current retry mechanism uses a single task queue for all retries, preventing error-specific retry strategies and causing failures to cascade across all tasks in the queue.",
      "proposedFix": "Implement separate task queues (possibly dynamic and rate-limited) for retrying activities/workflows that fail with specific error codes, allowing differentiated retry behavior and preventing cascading failures.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Use a different rate limited task queue to retry activities in certain scenarios. This way retries of activities that failed with a specific error code would be scheduled in a separate task queue.",
      "number": 4600,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:14.338Z"
    },
    {
      "summary": "Data race detected in TestTQMFetchesUserDataFailsAndTriesAgain test where goroutine 586 writes to shared memory at 0x00010506ff60 while goroutine 490 reads from it. The race occurs in the taskQueueManager's fetchUserData operation during test execution.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "task-queue-manager",
        "matching-service",
        "goroutine-group"
      ],
      "concepts": [
        "data-race",
        "concurrent-access",
        "test-synchronization",
        "memory-safety",
        "goroutine-coordination"
      ],
      "severity": "high",
      "userImpact": "Test failures prevent reliable server testing and indicate potential concurrency issues in production code paths.",
      "rootCause": "Unsynchronized access to shared memory in task queue manager between test setup (setScoped) and fetchUserData execution in separate goroutines.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #4604",
      "related": [
        4604
      ],
      "keyQuote": "WARNING: DATA RACE Write at 0x00010506ff60 by goroutine 586... Previous read at 0x00010506ff60 by goroutine 490",
      "number": 4590,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:15.577Z"
    },
    {
      "summary": "Request to add a service metric (gauge) tracking the number of currently running executions in Temporal, as this information is frequently requested but not available through metrics outside of visibility queries.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "server"
      ],
      "concepts": [
        "monitoring",
        "observability",
        "execution-tracking",
        "gauge-metric",
        "visibility"
      ],
      "severity": "medium",
      "userImpact": "Users need a direct metric to monitor running executions without relying on visibility queries, improving observability and operational monitoring.",
      "rootCause": null,
      "proposedFix": "Add a service metric (gauge) to track the number of running executions",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Metric was implemented to provide gauge tracking of running executions",
      "related": [],
      "keyQuote": "Would be nice to have a service metric (gauge) on number of running executions instead of just completion counters.",
      "number": 4581,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:01.700Z"
    },
    {
      "summary": "Workflow task handler incorrectly fails workflow execution when encountering payload size violations or pending activity limits. The service should fail only the task and terminate execution through proper channels, not fail the entire execution.",
      "category": "bug",
      "subcategory": "workflow-task-handler",
      "apis": [],
      "components": [
        "workflow-task-handler",
        "history-service",
        "execution"
      ],
      "concepts": [
        "payload-size",
        "activity-limits",
        "task-failure",
        "execution-termination",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Workflows may unexpectedly fail execution when hitting size or activity limits instead of properly terminating through intended mechanisms.",
      "rootCause": "workflowTaskHandler fails execution directly on payload size checks and pending activity limits instead of failing the task and allowing proper execution termination flow",
      "proposedFix": "Modify workflowTaskHandler to fail only the workflow task on size violations, then terminate execution if intended, rather than failing execution directly",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by correcting the error handling in workflowTaskHandler to properly fail tasks instead of executions",
      "related": [],
      "keyQuote": "Service should fail the workflow task and then terminate execution if that is the intend, but not fail (only should fail exec on sdk request)",
      "number": 4579,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:04.102Z"
    },
    {
      "summary": "Task queue partition names are being written to event history instead of the actual task queue names in some cases, appearing as '/_sys/myTaskQueueName/2' instead of just the queue name. While workflows still process correctly, this causes confusion when viewing event history.",
      "category": "bug",
      "subcategory": "task-queue",
      "apis": [],
      "components": [
        "history",
        "task-queue",
        "event-history"
      ],
      "concepts": [
        "task-queue-partitioning",
        "event-history",
        "data-consistency",
        "visibility"
      ],
      "severity": "medium",
      "userImpact": "Users see confusing partition names in event history instead of the actual task queue names, potentially causing misunderstanding of their workflows.",
      "rootCause": "The service is writing task queue partition references instead of the base task queue name to event history, likely in workflowTaskHandlerCallbacks.go and workflow_task_state_machine.go.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "taskQueue name exposed as partition like '/_sys/myTaskQueueName/2' - confusing in event history but does not fail workflow task execution",
      "number": 4557,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:55:03.068Z"
    },
    {
      "summary": "Request to implement activity result caching through sticky routing using user-provided keys. Users want a built-in mechanism to route activities to the same host based on a routing key for performance improvement, similar to Kafka consumer guarantees.",
      "category": "feature",
      "subcategory": "activity-routing",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "worker",
        "activity-routing",
        "activity-executor"
      ],
      "concepts": [
        "caching",
        "routing",
        "sticky-routing",
        "performance",
        "activity-affinity",
        "load-distribution"
      ],
      "severity": "medium",
      "userImpact": "Users can improve activity performance by caching results and routing related activities to the same host using custom keys.",
      "rootCause": null,
      "proposedFix": "Add worker options to enable activity caching and activity invocation options to specify routing keys",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Users want to cache activity results to improve performance. Currently there is no built-in way to provide activity routing to the same host by user provided id.",
      "number": 4542,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:52.228Z"
    },
    {
      "summary": "Add ActivityType field to MutableState's ActivityInfo to avoid expensive history lookups when describing workflow execution. Currently, determining activity types requires reading workflow history events, which is costly with up to 2k pending activities.",
      "category": "feature",
      "subcategory": "mutable-state",
      "apis": [
        "DescribeWorkflowExecution"
      ],
      "components": [
        "mutable-state",
        "history-service",
        "activity-info"
      ],
      "concepts": [
        "performance-optimization",
        "activity-scheduling",
        "workflow-history",
        "pending-activities"
      ],
      "severity": "medium",
      "userImpact": "Improves performance of DescribeWorkflowExecution by eliminating expensive history lookups for activity type information.",
      "rootCause": "ActivityType information not stored in MutableState, requiring expensive history event lookups for each pending activity during DescribeWorkflowExecution.",
      "proposedFix": "Add ActivityType field to MutableState's ActivityInfo structure and populate it during activity scheduling to avoid history reads.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "ActivityType was added to the ActivityInfo structure in MutableState to cache the activity type without requiring history lookups.",
      "related": [],
      "keyQuote": "Add ActivityType to MutableState's ActivityInfo... Do not read history event if activityType is available in activityInfo",
      "number": 4537,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:51.045Z"
    },
    {
      "summary": "Dependency vulnerability in modernc.org/libc-v1.24.1 (a transitive dependency of modernc.org/sqlite) with a buffer overflow issue in wcsnrtombs. The issue was automatically closed by Mend after the vulnerable library was removed from the inventory.",
      "category": "bug",
      "subcategory": "dependency-security-vulnerability",
      "apis": [],
      "components": [
        "dependency-management",
        "go-module-cache",
        "modernc.org/sqlite"
      ],
      "concepts": [
        "buffer-overflow",
        "memory-safety",
        "vulnerability-management",
        "transitive-dependency",
        "security-patch"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal Server with the vulnerable modernc.org/sqlite dependency are exposed to a potential buffer overflow vulnerability in the underlying libc library.",
      "rootCause": "wcsnrtombs in musl libc through 1.2.1 mishandles particular combinations of destination buffer size and source character limit, causing invalid write access.",
      "proposedFix": "Upgrade musl to version 1.2.2-1 or later as suggested by the vulnerability database.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Automatically closed by Mend after the vulnerable library was removed from the Mend inventory, indicating the dependency was either upgraded or removed from the codebase.",
      "related": [],
      "keyQuote": "wcsnrtombs mishandles particular combinations of destination buffer size and source character limit, as demonstrated by an invalid write access (buffer overflow)",
      "number": 4536,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:52.330Z"
    },
    {
      "summary": "Uber-go/tally v4.1.7 contains two high-severity vulnerabilities (CVE-2019-0205 and CVE-2019-0210) in Apache Thrift dependencies that can cause denial of service through endless loops or panics when processing malicious input.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "tally-metrics",
        "thrift-library"
      ],
      "concepts": [
        "vulnerability",
        "security",
        "denial-of-service",
        "dependency",
        "exploit",
        "availability"
      ],
      "severity": "high",
      "userImpact": "Temporal server could be vulnerable to DoS attacks through malformed input to the tally metrics library.",
      "rootCause": "Apache Thrift versions 0.9.3 through 0.12.0 contain bugs in JSON protocol parsing that cause endless loops or panics on invalid input.",
      "proposedFix": "Upgrade org.apache.thrift:libthrift to version 0.13.0 or later to patch CVE-2019-0205 and CVE-2019-0210.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was automatically closed by Mend when the vulnerable library was removed from inventory or marked as ignored in the codebase.",
      "related": [],
      "keyQuote": "a server implemented in Go using TJSONProtocol or TSimpleJSONProtocol may panic when feed with invalid input data",
      "number": 4535,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:39.445Z"
    },
    {
      "summary": "User reports that calling CancelWorkflow does not cancel currently executing activities in a workflow. Activities continue to run even after workflow cancellation is requested, and the user seeks a way to properly cancel all running workflow tasks.",
      "category": "question",
      "subcategory": "workflow-cancellation",
      "apis": [
        "CancelWorkflow",
        "RequestCancelWorkflowExecution",
        "ExecuteActivity"
      ],
      "components": [
        "workflow-execution",
        "activity-executor",
        "cancellation-handler"
      ],
      "concepts": [
        "cancellation",
        "activity-lifecycle",
        "workflow-shutdown",
        "heartbeat",
        "context-propagation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably cancel running activities when canceling a workflow, causing unexpected continued execution of activity logic.",
      "rootCause": "Activities need to be cancelled from within the workflow context rather than at the workflow level; activities only respond to cancellation if they perform heartbeats to check cancellation status.",
      "proposedFix": "Use the context passed to ExecuteActivity to cancel individual activities from within the workflow; ensure activities perform heartbeats to detect cancellation signals.",
      "workaround": "Cancel activities from within the workflow by passing a context with cancellation capability and ensure the activity implementation includes heartbeat calls to check for cancellation.",
      "resolution": "wontfix",
      "resolutionDetails": "Marked as question/guidance issue. The behavior is by design - workflow cancellation must be handled at the activity level within the workflow context, not externally.",
      "related": [],
      "keyQuote": "Cancel activity from within the workflow via the context that is used to start the activity. If the activity is already started, the activity needs to do heartbeat to be able to notice that it is cancelled.",
      "number": 4507,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:40.623Z"
    },
    {
      "summary": "Support data references to avoid storing the same Payload multiple times in history. When the same value is used as an argument across many activities, it's currently stored redundantly. The proposal is to introduce a mechanism to store values once and reference them by eventId in subsequent commands.",
      "category": "feature",
      "subcategory": "payload-storage",
      "apis": [],
      "components": [
        "history",
        "workflow-execution",
        "activity-scheduling",
        "payload-management"
      ],
      "concepts": [
        "data-deduplication",
        "payload-references",
        "storage-efficiency",
        "memory-optimization",
        "workflow-arguments",
        "event-references"
      ],
      "severity": "medium",
      "userImpact": "Users working with large payloads or workflows that reuse the same data across many activities experience unnecessary storage overhead and increased history size.",
      "rootCause": "Temporal currently stores complete payload copies in each activity's schedule command, leading to redundant storage when the same value is used multiple times.",
      "proposedFix": "Introduce a special command (or reuse Marker command) to store values as Payloads in history once, then allow activity/child workflow inputs and outputs to reference that value by eventId instead of including it directly.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "In this case, the document is going to be stored 1k+1 times in the history which is wasteful.",
      "number": 4502,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:39.794Z"
    },
    {
      "summary": "The dynamic config `frontend.namespaceCount` currently enforces a concurrent request count limit per frontend pod. This enhancement requests changing it to a global limit across all frontend pods instead.",
      "category": "feature",
      "subcategory": "frontend-configuration",
      "apis": [],
      "components": [
        "frontend",
        "dynamic-config",
        "namespace-limit"
      ],
      "concepts": [
        "concurrent-requests",
        "global-limit",
        "dynamic-configuration",
        "resource-management",
        "load-distribution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot enforce a single global concurrent request limit across their entire Temporal cluster, limiting their ability to manage overall system load and prevent resource exhaustion.",
      "rootCause": "The current implementation of `frontend.namespaceCount` is scoped to individual frontend pods rather than being cluster-wide.",
      "proposedFix": "Change the dynamic config implementation to enforce a global concurrent request limit across all frontend pods.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed in PR #4743 which implements the global limit functionality.",
      "related": [
        4743
      ],
      "keyQuote": "We need a global limit instead of a per frontend pod limit.",
      "number": 4492,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:24.623Z"
    },
    {
      "summary": "Replication task metrics (replication_tasks_lag, replication_tasks_fetched, replication_tasks_returned) are incorrectly configured with millisecond units and histogram boundaries, when they should be dimensionless counts since they record task ID differences, not time values.",
      "category": "bug",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "replication",
        "ack-manager"
      ],
      "concepts": [
        "metrics-units",
        "histogram-boundaries",
        "replication-lag",
        "task-tracking",
        "configuration-mismatch"
      ],
      "severity": "medium",
      "userImpact": "Users receive metrics with incorrect units and histogram boundaries, making replication lag monitoring inaccurate and misleading.",
      "rootCause": "Metrics are defined as timers with millisecond units when they should be dimensionless counters, and task ID differences are being recorded instead of actual time values.",
      "proposedFix": "Change metric definitions from timer-based with millisecond units to dimensionless histogram boundaries, and update the code recording the metrics to use the correct unit configuration.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "what's actually being recorded is the difference between the task IDs, not time, but the metric is defined as a timer",
      "number": 4483,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:25.943Z"
    },
    {
      "summary": "The temporal-cassandra-tool fails with a panic when deployed in the same Kubernetes namespace as Cassandra because the CASSANDRA_PORT environment variable is set to a connection string (e.g., 'tcp://10.43.109.12:9042') instead of a numeric port. The tool expects a valid integer port number and crashes when it receives the Kubernetes-injected environment variable format.",
      "category": "bug",
      "subcategory": "admin-tools",
      "apis": [],
      "components": [
        "admin-tools",
        "cassandra-tool",
        "environment-parsing"
      ],
      "concepts": [
        "kubernetes",
        "environment-variables",
        "docker",
        "cassandra",
        "deployment",
        "port-parsing"
      ],
      "severity": "medium",
      "userImpact": "Users deploying admin-tools in Kubernetes for database migrations cannot run the tool in the same namespace as Cassandra without manually specifying the port or deploying to a different namespace.",
      "rootCause": "The temporal-cassandra-tool's environment parsing expects CASSANDRA_PORT to be a numeric port value, but Kubernetes injects it as a connection string (tcp://host:port) when services are in the same namespace.",
      "proposedFix": "The tool should parse the CASSANDRA_PORT environment variable to handle both numeric port format and Kubernetes connection string format, or provide better error messaging.",
      "workaround": "Pass the port explicitly via the --port command line argument (e.g., --port 9042) or deploy the admin-tools container in a different namespace from Cassandra.",
      "resolution": "wontfix",
      "resolutionDetails": "The maintainer indicated this is expected behavior and users should use the --port argument or specify CASSANDRA_PORT as a numeric value, rather than relying on Kubernetes environment variables.",
      "related": [],
      "keyQuote": "The temporal-cassandra-tool expect CASSANDRA_PORT to be a valid integer port number (like 9042). If you want to avoid this panic, you can pass in port as argument by `--port`.",
      "number": 4467,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:27.798Z"
    },
    {
      "summary": "Query tasks discard the event history after handling since RespondQueryTaskCompletedRequest lacks sticky attributes. This is wasteful for query-heavy workflows with large histories, especially when codecs are involved, as subsequent queries must reprocess the same history.",
      "category": "feature",
      "subcategory": "query-caching",
      "apis": [
        "RespondQueryTaskCompletedRequest"
      ],
      "components": [
        "query-handler",
        "worker",
        "sticky-queue",
        "history-cache"
      ],
      "concepts": [
        "caching",
        "query-performance",
        "sticky-attributes",
        "event-history",
        "latency",
        "codec",
        "reuse"
      ],
      "severity": "medium",
      "userImpact": "Users with query-heavy workflows pay unnecessary latency penalties and waste computational resources by repeatedly processing large event histories.",
      "rootCause": "RespondQueryTaskCompletedRequest does not carry sticky attributes, so event history is discarded after query handling and must be rehydrated for subsequent queries.",
      "proposedFix": "Add sticky attributes to RespondQueryTaskCompletedRequest or implement another mechanism to cache event history on the worker for efficient handling of subsequent queries.",
      "workaround": "Set a repeating workflow timer with short intervals to revive the workflow and rehydrate the cache, re-establishing the sticky queue.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "For query heavy workflows with large history, especially when codec is involved, discarding the payload seems to be wasteful, and paying latency penalty unnecessarily.",
      "number": 4463,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:13.901Z"
    },
    {
      "summary": "Task queue user data is now in the critical path for every task queue partition, and only versioning data is stored in user data. A dynamic config is needed to disable this mechanism until the user data mechanism is proven reliable.",
      "category": "feature",
      "subcategory": "task-queue-management",
      "apis": [],
      "components": [
        "task-queue",
        "user-data",
        "configuration"
      ],
      "concepts": [
        "dynamic-config",
        "versioning",
        "critical-path",
        "task-queue-partition"
      ],
      "severity": "medium",
      "userImpact": "Users need the ability to disable task queue user data loading in production if issues emerge before the mechanism is fully confident.",
      "rootCause": "Task queue user data was added to critical path for all partitions and currently only stores versioning data, creating potential risk.",
      "proposedFix": "Add a dynamic config option to disable loading task queue user data",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "At the time of writing only versioning data is stored in user data and until we're confident with the user data mechanism, we need the ability to disable it.",
      "number": 4450,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:15.070Z"
    },
    {
      "summary": "Remove the old workflow archival code path now that durable archival is enabled by default in version 1.20. This is a refactoring task to clean up deprecated logic.",
      "category": "other",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "archival-system",
        "workflow-archival",
        "durable-archival"
      ],
      "concepts": [
        "deprecation",
        "code-cleanup",
        "archival",
        "backwards-compatibility",
        "default-behavior"
      ],
      "severity": "low",
      "userImpact": "Users will experience cleaner codebase maintenance and reduced technical debt, with no direct functional changes.",
      "rootCause": "Old workflow archival code path became redundant after durable archival was made the default in version 1.20.",
      "proposedFix": "Deprecate and remove the old workflow archival code path.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Durable archival is enabled by default in 1.20, we should be able to deprecate the old code path (workflow archival).",
      "number": 4435,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:14.872Z"
    },
    {
      "summary": "Server startup fails intermittently due to concurrent map writes. This race condition occurs during initialization and prevents successful server startup.",
      "category": "bug",
      "subcategory": "startup",
      "apis": [],
      "components": [
        "server",
        "initialization",
        "concurrency"
      ],
      "concepts": [
        "race-condition",
        "concurrent-access",
        "map-writes",
        "startup",
        "synchronization"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably start the Temporal server, experiencing intermittent startup failures that require retries.",
      "rootCause": "Concurrent writes to a map during server initialization without proper synchronization",
      "proposedFix": null,
      "workaround": "Retry the server startup command",
      "resolution": "duplicate",
      "resolutionDetails": "Duplicate of issue #4000 which addresses the same concurrent map write problem",
      "related": [
        4000
      ],
      "keyQuote": "Server startup fail at concurrent map writes",
      "number": 4434,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:02.837Z"
    },
    {
      "summary": "Archived workflow executions are displayed multiple times in the Temporal UI and CLI, despite only a single copy existing in the S3 archival storage. The issue appears to be with the gRPC API's query logic when fetching archived workflows without filters.",
      "category": "bug",
      "subcategory": "archival-visibility",
      "apis": [],
      "components": [
        "grpc-api",
        "archival",
        "visibility-store",
        "ui"
      ],
      "concepts": [
        "archival",
        "visibility",
        "duplication",
        "s3",
        "filtering",
        "query-results"
      ],
      "severity": "medium",
      "userImpact": "Users see duplicate workflow executions in the archived section of the UI and CLI, causing confusion and making it harder to navigate archived workflows.",
      "rootCause": "The gRPC API that fetches archived workflows from S3 appears to be returning duplicate results when querying without filters, despite S3 containing only single copies of each workflow.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was reproduced by maintainer and identified as a gRPC API bug in the query logic for archived workflows.",
      "related": [],
      "keyQuote": "I suspect you're right that this is an issue with the the gRPC API which fetches the actual workflows as there are no duplicates in S3.",
      "number": 4432,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:03.228Z"
    },
    {
      "summary": "Feature request to support absolute deadline timers instead of relative delays. Currently, timers use relative delays (e.g., '10s'), but network transit and persistence lag cause actual delays to exceed the specified time. The proposal is to allow specifying deadlines as absolute timestamps/ISO strings.",
      "category": "feature",
      "subcategory": "timer-deadline",
      "apis": [
        "Workflow.await"
      ],
      "components": [
        "timer",
        "workflow-execution",
        "scheduler"
      ],
      "concepts": [
        "deadline",
        "timer",
        "delay",
        "network-latency",
        "persistence-lag",
        "absolute-timestamp",
        "workflow-resume"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot currently guarantee exact workflow resumption times when using timers due to network and persistence delays, limiting precision for time-sensitive workflows.",
      "rootCause": null,
      "proposedFix": "Add support for specifying timer deadlines as absolute timestamps or ISO strings instead of only relative delays.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I would like to be able to specify the deadline of a timer as an absolute timestamp / ISO string, rather than a relative delay.",
      "number": 4428,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:54:04.080Z"
    },
    {
      "summary": "Build IDs need to be stored in user data when tasks contain unknown build IDs. This is particularly important for replication edge cases and when workflows schedule compatible child/activity/CAN on different task queues that lack the worker's build ID in their versioning data.",
      "category": "feature",
      "subcategory": "build-id-versioning",
      "apis": [],
      "components": [
        "task-queue",
        "versioning-data",
        "replication"
      ],
      "concepts": [
        "build-id",
        "user-data",
        "task-queue-isolation",
        "versioning",
        "compatibility",
        "replication"
      ],
      "severity": "medium",
      "userImpact": "Without storing build IDs in user data, workflows fail when scheduling compatible tasks on different task queues or during replication edge cases.",
      "rootCause": "Build ID information is not persisted in user data for incoming tasks with unknown build IDs, causing compatibility issues across task queues.",
      "proposedFix": "Store build ID in user data when receiving tasks that contain an unknown build ID.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by implementing the feature to store build IDs in user data for unknown build IDs.",
      "related": [],
      "keyQuote": "Store build ID in user data when incoming task contains an unknown build id",
      "number": 4410,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:51.403Z"
    },
    {
      "summary": "Activity inputs are stored in workflow history but consume significant space and are not needed for workflow replay since inputs can be reconstructed deterministically. The proposal is to stop persisting activity inputs and instead reconstruct them through deterministic replay or query mechanisms.",
      "category": "feature",
      "subcategory": "history-optimization",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "history",
        "activity-executor",
        "workflow-engine",
        "persistence"
      ],
      "concepts": [
        "history-space",
        "deterministic-replay",
        "state-recovery",
        "performance-optimization",
        "storage-efficiency"
      ],
      "severity": "medium",
      "userImpact": "Users with workflows containing many activities experience increased database size and higher history cache overhead due to redundant activity input storage.",
      "rootCause": "Activity inputs are persisted in the workflow history even though they can be deterministically reconstructed from workflow code during replay, leading to unnecessary storage consumption.",
      "proposedFix": "Remove activity inputs from history persistence and reconstruct them during workflow replay or through a query mechanism when needed for UI display.",
      "workaround": "Implement a custom query in the workflow to retrieve activity inputs instead of relying on history storage.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Activity inputs consume history space and are not used when recovering workflow state through the replay.",
      "number": 4389,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:51.323Z"
    },
    {
      "summary": "Workflow should support blacklisting and whitelisting of signal and update types to prevent unwanted signals from consuming history and being processed in incorrect workflow states.",
      "category": "feature",
      "subcategory": "signals-updates",
      "apis": [
        "Signal",
        "Update"
      ],
      "components": [
        "workflow-execution",
        "signal-handler",
        "update-handler",
        "history"
      ],
      "concepts": [
        "signal-filtering",
        "update-validation",
        "history-optimization",
        "workflow-state",
        "allow-list",
        "deny-list"
      ],
      "severity": "high",
      "userImpact": "Users cannot efficiently filter signals and updates, causing unnecessary history consumption and forcing workarounds or migration away from Temporal.",
      "rootCause": "Workflow execution currently processes all signals and updates regardless of workflow state, adding them to history even when not relevant.",
      "proposedFix": "Implement mutable allow/denylist at workflow execution start time. Future enhancement: support server-side per-WorkflowType configuration via Worker configuration.",
      "workaround": "Implement Update validation handlers (SDK-specific, not available in PHP and TS initially) that reject invalid updates, though this has limitations with timeouts.",
      "related": [
        1277
      ],
      "keyQuote": "Signals, even ones that workflow doesn't care about, still consume workflow history. Signals received in other states should be rejected.",
      "number": 4387,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:52.616Z"
    },
    {
      "summary": "Request to add a new Workflow ID Reuse Policy that queues duplicate workflow submissions instead of rejecting them. This would allow workflows with the same ID to be processed sequentially without requiring the master-workflow + signal + child-workflow pattern.",
      "category": "feature",
      "subcategory": "workflow-id-reuse-policy",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-engine",
        "duplicate-handling",
        "queue-management",
        "state-machine"
      ],
      "concepts": [
        "workflow-id-reuse",
        "queueing",
        "duplicate-submissions",
        "sequential-processing",
        "observability",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Users processing high-volume sequential workflows by user ID would benefit from simpler design and better observability instead of requiring complex master-workflow patterns.",
      "rootCause": null,
      "proposedFix": "Add new Workflow ID Reuse Policy option (e.g., 'Queue Duplicate') that queues workflows with duplicate IDs in a 'Queued' state instead of rejecting them.",
      "workaround": "Use master-workflow + signal + child-workflow recipe, though this reduces observability and increases design complexity.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would be useful to add a new reuse policy like 'Queue Duplicate' where the workflow would be queued with a new state like 'Queued'.",
      "number": 4386,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:38.463Z"
    },
    {
      "summary": "During graceful shutdown of single-binary Temporal servers, spurious error logs appear (\"Error looking up host for shardID\", \"Unable to call matching.PollActivityTaskQueue\") that make users think something went wrong even though shutdown is proceeding normally.",
      "category": "bug",
      "subcategory": "graceful-shutdown",
      "apis": [],
      "components": [
        "shard-controller",
        "frontend",
        "worker",
        "history-service"
      ],
      "concepts": [
        "graceful-shutdown",
        "logging",
        "error-messages",
        "single-binary-server",
        "connection-handling",
        "EOF-errors"
      ],
      "severity": "medium",
      "userImpact": "Users see alarming ERROR and WARN logs during normal shutdown, creating confusion about whether the shutdown is actually proceeding correctly.",
      "rootCause": "During graceful shutdown, the server closes connections and shards become unavailable, causing lookup operations to fail and generate error logs that are actually expected behavior during shutdown.",
      "proposedFix": "Suppress or downgrade log messages that occur during graceful shutdown, or implement log-less shutdown option. May require coordination with CLI for proper shutdown handling.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Any solution that does not make a user think there is an error when they see logs.",
      "number": 4383,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:40.025Z"
    },
    {
      "summary": "The temporal-sql-tool create-database command fails with 'flag provided but not defined: -database' error when attempting to create MySQL databases. The tool incorrectly rejects the -database flag and requires manual database creation before schema setup.",
      "category": "bug",
      "subcategory": "sql-tool",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "database-setup",
        "mysql-integration"
      ],
      "concepts": [
        "flag-parsing",
        "database-creation",
        "command-line-interface",
        "schema-initialization",
        "helm-deployment"
      ],
      "severity": "medium",
      "userImpact": "Users following helm chart documentation cannot programmatically create MySQL databases, requiring manual database creation before using the tool.",
      "rootCause": "The temporal-sql-tool's flag parser does not recognize the '-database' flag for the create-database subcommand (expects '--defaultdb' instead).",
      "proposedFix": null,
      "workaround": "Manually create the required databases (temporal, temporal_visibility) in MySQL before running the schema creation commands with temporal-sql-tool.",
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of helm-charts issue #364",
      "related": [
        364
      ],
      "keyQuote": "Incorrect Usage: flag provided but not defined: -database",
      "number": 4378,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:37.578Z"
    },
    {
      "summary": "Search attributes with datetime type return inconsistent RFC3339 formats across environments (UTC format like 2023-04-27T22:52:33Z vs timezone-aware format like 2006-01-02T15:04:05-07:00), making parsing difficult without flexible RFC3339 parsing.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "search-attributes",
        "datetime-handling",
        "sdk-client"
      ],
      "concepts": [
        "datetime-format",
        "RFC3339",
        "timezone-handling",
        "parsing",
        "consistency",
        "serialization"
      ],
      "severity": "low",
      "userImpact": "Users must handle multiple RFC3339 datetime formats when parsing search attribute values, requiring flexible parsing logic instead of fixed format strings.",
      "rootCause": "System search attributes are stored in UTC, while custom search attributes preserve the timezone as set by the user, resulting in different RFC3339 representations.",
      "proposedFix": "Use flexible RFC3339 parsing (e.g., time.RFC3339Nano in Go) that accepts both timezone-aware and UTC formats.",
      "workaround": "Use time.Parse(time.RFC3339Nano, dt) for flexible RFC3339 datetime parsing.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed by maintainer after clarifying that system attributes use UTC while custom attributes preserve user-set timezone. The inconsistency is by design, not a bug.",
      "related": [],
      "keyQuote": "For system, we convert to UTC for storage. For custom, we try to keep it with timezone as set by the user so it can be retrieved exactly as the user set.",
      "number": 4377,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:26.595Z"
    },
    {
      "summary": "Sending signals via batch operation causes a payload encoding error in TypeScript SDK workflows. The server re-encodes signal input payloads, resulting in 'Unknown encoding: binary/protobuf' when the workflow tries to deserialize the signal arguments.",
      "category": "bug",
      "subcategory": "signal-delivery",
      "apis": [
        "StartBatchOperation",
        "signalWorkflow"
      ],
      "components": [
        "payload-converter",
        "batch-operation",
        "signal-processing",
        "data-converter"
      ],
      "concepts": [
        "encoding",
        "serialization",
        "batch-operations",
        "signal-handling",
        "protobuf",
        "payload-conversion"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably send signals with arguments to multiple workflows using batch operations without encountering deserialization errors.",
      "rootCause": "The server re-encodes the signal input's payloads, causing a mismatch where the payload encoder is set to 'binary/protobuf' but the DefaultPayloadConverter on the client side does not recognize this encoding.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was identified as a server-side re-encoding problem that occurs when using raw gRPC calls for batch signal operations.",
      "related": [],
      "keyQuote": "the server seems to reencode the signal input's payloads",
      "number": 4373,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:27.662Z"
    },
    {
      "summary": "Feature request to persist and modify worker activity rate limits across restarts and migrations. Currently WorkerActivitiesPerSecond resets when a worker restarts or moves, causing rate limiting strategy problems especially when observing third-party downstream rate limits.",
      "category": "feature",
      "subcategory": "rate-limiting",
      "apis": [
        "WorkerActivitiesPerSecond"
      ],
      "components": [
        "worker",
        "rate-limiter",
        "activity-executor"
      ],
      "concepts": [
        "rate-limiting",
        "worker-persistence",
        "state-management",
        "downstream-limits",
        "worker-restart",
        "configuration-modification"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot maintain consistent rate limits across worker restarts or migrations, forcing them to implement workarounds like external rate limiting systems.",
      "rootCause": "Rate limit state is not persisted and is tied to worker instance lifecycle rather than worker identity, causing loss of settings on restart or migration.",
      "proposedFix": "Tie rate limit state to worker ID for persistence across restarts/migrations and provide ability to modify WorkerActivitiesPerSecond after worker creation without requiring restart.",
      "workaround": "Use external rate limiting systems like Gubernator with additional activities to check and register hits, or add sleep delays in worker's main method during token replenishment period.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue author concluded the concerns were not significant after reviewing rate limiter implementation, noting burst value of one means minimal overage on restart and impact depends on restart frequency relative to token replenishment.",
      "related": [],
      "keyQuote": "Allow the WorkerActivitiesPerSecond limit to stick around when a worker restarts or moves to a new machine and make it possible to change the limit after the worker is created.",
      "number": 4361,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:27.536Z"
    },
    {
      "summary": "User seeks firewall-friendly solutions for orchestrating durable tasks across multiple clusters/VNets with microservices behind firewalls, questioning Temporal's suitability given its gRPC-only protocol and requesting guidance on deployment patterns.",
      "category": "question",
      "subcategory": "deployment-architecture",
      "apis": [],
      "components": [
        "worker",
        "frontend",
        "grpc-endpoint"
      ],
      "concepts": [
        "firewall-friendly",
        "cross-cluster",
        "network-connectivity",
        "deployment-patterns",
        "orchestration",
        "decoupling"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal across multiple secure networks need clear guidance on firewall-friendly architectures and supported protocols.",
      "rootCause": "User misconception that Temporal's gRPC protocol makes it unsuitable for cross-firewall deployments with microservices.",
      "proposedFix": "Workers can be deployed locally within each service cluster, requiring only outbound gRPC connections; HTTP API support available as alternative.",
      "workaround": "Deploy Temporal workers inside each service cluster/VNet to minimize firewall impact and enable local communication.",
      "resolution": "fixed",
      "resolutionDetails": "Community response clarified that workers only need outbound gRPC connections to Temporal service, and HTTP API support was added via PR #4543.",
      "related": [
        4543
      ],
      "keyQuote": "Temporal workers which are hosted inside each service only need an outbound gRPC connection to the Temporal service.",
      "number": 4360,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:15.488Z"
    },
    {
      "summary": "User asks whether Temporal supports automatic rollback/compensation when workflow steps fail or when a workflow is cancelled, in the context of cloud resource provisioning (VMs, IPs) that need cleanup on failure.",
      "category": "question",
      "subcategory": "saga-pattern",
      "apis": [],
      "components": [
        "workflow-execution",
        "cancellation-handling"
      ],
      "concepts": [
        "rollback",
        "saga-pattern",
        "compensation",
        "workflow-cancellation",
        "resource-cleanup",
        "error-handling"
      ],
      "severity": "low",
      "userImpact": "Users need to understand how to implement cleanup/rollback logic when provisioning cloud resources with Temporal workflows.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Implement explicit rollback steps in the workflow to handle failures and cancellations manually, using the saga pattern as shown in provided examples.",
      "resolution": "invalid",
      "resolutionDetails": "Closed as the question was answered - Temporal does not do automatic rollback/compensation but provides patterns (saga) for implementing it explicitly.",
      "related": [],
      "keyQuote": "Temporal will not do automatically rollback (or saga compenstaion), but your workflow would have to handle that explicitly.",
      "number": 4358,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:14.098Z"
    },
    {
      "summary": "Replication tasks referencing archived workflow executions block all replication when a secondary cluster is offline for longer than the namespace retention window. When the cluster returns online, the replication system becomes deadlocked trying to process replication tasks for workflows that have been archived, preventing both historical and new workflow replication.",
      "category": "bug",
      "subcategory": "multi-cluster-replication",
      "apis": [],
      "components": [
        "history-service",
        "replication-manager",
        "persistence-layer",
        "ack-manager"
      ],
      "concepts": [
        "multi-cluster-replication",
        "workflow-archival",
        "retention-window",
        "replication-tasks",
        "deadlock",
        "cluster-recovery"
      ],
      "severity": "critical",
      "userImpact": "When a secondary cluster is offline for extended periods and returns online, all workflow history replication stops indefinitely, including newly created workflows, blocking cross-cluster synchronization.",
      "rootCause": "The replication task reader cannot process replication tasks for archived workflow executions that no longer exist in the database. The system attempts to load mutable state for workflows that have been archived during the offline period, causing context cancellation errors that block the entire replication queue.",
      "proposedFix": "Modify shard.Context.GetWorkflowExecution() to return serviceerror.NotFound for archived workflows instead of failing, allowing the replication system to skip archived workflows and continue processing.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The replication task reader cannot process replication tasks for archived workflow executions, causing context canceled errors that block the entire replication system from functioning.",
      "number": 4348,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:14.472Z"
    },
    {
      "summary": "Request to allow zero port in config to let the OS automatically assign a free port for testing scenarios. This simplifies testing with multiple local servers and avoids port conflicts.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "config",
        "rpc",
        "server-startup"
      ],
      "concepts": [
        "port-binding",
        "testing",
        "os-ephemeral-ports",
        "server-initialization",
        "configuration"
      ],
      "severity": "low",
      "userImpact": "Simplifies testing workflows by allowing automatic port assignment instead of manual configuration for each test run.",
      "rootCause": null,
      "proposedFix": "Allow specifying port 0 in config.Config to let OS choose a port, then update the config with the actual bound port after binding.",
      "workaround": "Use temporalite's freeport.go approach, though it has race conditions with port rebinding.",
      "resolution": "wontfix",
      "resolutionDetails": "Decision made not to implement due to OS-level port reservation behavior - all major operating systems reserve ports for 60 seconds after closing before making them available again.",
      "related": [],
      "keyQuote": "We decided to not do this as all major operating systems will reserve ports for 60 seconds before making them available again.",
      "number": 4329,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:02.223Z"
    },
    {
      "summary": "Workflow with CRON schedule \"0 0 29 2 1\" (intended to run on Feb 29 of leap years on Mondays) executes at an unexpected date in 2024 instead of 2032. The issue stems from cron's \"or\" logic for day-of-month and day-of-week fields, which Temporal's workflow CronSchedule implementation follows differently than standard cron.",
      "category": "bug",
      "subcategory": "workflow-scheduling",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-execution",
        "cron-scheduler",
        "schedule-parser"
      ],
      "concepts": [
        "cron-syntax",
        "scheduling",
        "leap-year",
        "date-calculation",
        "field-logic"
      ],
      "severity": "medium",
      "userImpact": "Users relying on complex CRON expressions with both day-of-month and day-of-week constraints get unexpected schedule execution times, making it difficult to effectively disable workflows or control their execution timing.",
      "rootCause": "Temporal's CronSchedule implementation treats day-of-month and day-of-week as an \"or\" condition (standard cron behavior) rather than an \"and\" condition, causing the schedule to execute on Feb 5, 2024 (Monday) instead of waiting for Feb 29, 2032 (Monday of a leap year).",
      "proposedFix": null,
      "workaround": "Do not specify both day-of-month and day-of-week fields simultaneously; use only one constraint or accept the \"or\" semantics.",
      "resolution": "wontfix",
      "resolutionDetails": "Determined to be working as designed per cron standard behavior (day-of-month and day-of-week use \"or\" logic). The behavior is consistent with crontab.guru and standard cron implementations.",
      "related": [],
      "keyQuote": "Classic cron syntax has the \"feature\" than when both day-of-month and day-of-week are specified, then they're treated as \"or\" instead of \"and\".",
      "number": 4288,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:01.737Z"
    },
    {
      "summary": "Temporal UI cannot be displayed in an iframe when CORS is set to '*', with content being blocked. The user wants to embed the UI in a dashboard application to avoid separate authentication.",
      "category": "bug",
      "subcategory": "ui-cors",
      "apis": [],
      "components": [
        "temporal-ui",
        "cors"
      ],
      "concepts": [
        "cors",
        "iframe",
        "cross-origin",
        "embedding",
        "authentication",
        "dashboard"
      ],
      "severity": "medium",
      "userImpact": "Users cannot embed Temporal UI in external applications via iframe, limiting integration capabilities and forcing separate authentication flows.",
      "rootCause": "CORS configuration does not properly handle iframe embedding even when set to '*'",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was determined to belong in the dedicated UI repository rather than the core temporal repository and was redirected there.",
      "related": [],
      "keyQuote": "I want to use the UI as part of a dashboard because here I have the benefit of not having the user authenticate in temporal, but beforehand in the dashboard.",
      "number": 4285,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:53:00.506Z"
    },
    {
      "summary": "After upgrading from Temporal 1.19.1 to 1.20.0, workflows created by schedules fail during archival with 'invalid search attribute type: Unspecified' errors. This affects the archival process for visibility and task handling, causing errors when attempting to archive workflows or process tasks.",
      "category": "bug",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "archiver",
        "visibility",
        "scheduler",
        "search-attributes"
      ],
      "concepts": [
        "archival",
        "search-attributes",
        "migration",
        "schedule",
        "visibility-store"
      ],
      "severity": "high",
      "userImpact": "Users upgrading to 1.20.0+ encounter repeated archival failures for schedule-generated workflows, preventing proper workflow archival and causing error logs.",
      "rootCause": "Bug in archiving workflows created with schedules where search attribute type handling fails, specifically with KeywordList and Unspecified type attributes",
      "proposedFix": "PR #4304 provides a fix for the long-term solution",
      "workaround": "Set `history.durableArchivalEnabled` to false as a temporary workaround",
      "resolution": "fixed",
      "resolutionDetails": "Fixed via PR #4304 which addresses the search attribute type handling for schedule-generated workflows",
      "related": [
        4304
      ],
      "keyQuote": "failed to archive target -> invalid search attribute type : Unspecified",
      "number": 4270,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:49.362Z"
    },
    {
      "summary": "User asking for guidance on deploying a Temporal production cluster, including minimum server count and resource requirements. Redirected to community forums as this is a support question.",
      "category": "question",
      "subcategory": "deployment-configuration",
      "apis": [],
      "components": [
        "cluster",
        "deployment",
        "infrastructure"
      ],
      "concepts": [
        "production deployment",
        "cluster sizing",
        "server requirements",
        "infrastructure planning"
      ],
      "severity": "low",
      "userImpact": "Users seeking production deployment guidance are directed to appropriate community support channels.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Closed as this is a support question better handled by community forums and Slack, not GitHub issues.",
      "related": [],
      "keyQuote": "Please ask this type of question in our community forum https://community.temporal.io/ or community slack",
      "number": 4269,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:46.963Z"
    },
    {
      "summary": "User inquired about production statistics and performance metrics for Temporal, specifically asking about event throughput per second and machine capacity requirements.",
      "category": "question",
      "subcategory": "operations-and-monitoring",
      "apis": [],
      "components": [],
      "concepts": [
        "production-deployment",
        "performance-metrics",
        "capacity-planning",
        "throughput",
        "scalability"
      ],
      "severity": "low",
      "userImpact": "Users lack public information about production performance benchmarks and capacity planning guidance for Temporal deployments.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue redirected to community forum as it is a support question rather than a bug or feature request.",
      "related": [],
      "keyQuote": "Please ask this type of question in our community forum https://community.temporal.io/ or community slack",
      "number": 4268,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:47.039Z"
    },
    {
      "summary": "gocql library fails to refresh Cassandra node IP addresses after cluster/node restarts, requiring explicit service restarts. A bug in gocql v1.3.1 prevents the client from discovering new node IPs, addressed in gocql 1.4.0+.",
      "category": "bug",
      "subcategory": "cassandra-connectivity",
      "apis": [],
      "components": [
        "cassandra-client",
        "gocql-driver",
        "service-initialization"
      ],
      "concepts": [
        "node-discovery",
        "ip-refresh",
        "cluster-restart",
        "connection-management",
        "dependency-update"
      ],
      "severity": "high",
      "userImpact": "Users must manually restart Temporal services after Cassandra cluster maintenance to re-establish connectivity with new node IPs.",
      "rootCause": "Bug in gocql v1.3.1 preventing IP address refresh for Cassandra nodes after cluster restarts",
      "proposedFix": "Update gocql dependency to version 1.4.0 or later where the issue is resolved",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by updating gocql dependency to version 1.4.0+",
      "related": [
        1582
      ],
      "keyQuote": "new IP addresses of Cassandra are not getting refreshed by the gocql and after the cluster maintenance each temporal service with a dependency on gocql have to be restarted explicitly",
      "number": 4265,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:34.484Z"
    },
    {
      "summary": "Add firstExecutionRunId as a system search attribute to help users query workflow results across multiple executions when using IdReusePolicy. Currently, runId changes on ContinueAsNew, Reset, Cron, and WorkflowRetry operations, making it difficult to track the final result of a workflow that has been reset or retried.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "search-attributes",
        "workflow-execution",
        "visibility"
      ],
      "concepts": [
        "IdReusePolicy",
        "ContinueAsNew",
        "reset",
        "workflow-retry",
        "cron",
        "run-tracking",
        "query"
      ],
      "severity": "medium",
      "userImpact": "Users can more easily track and query workflow results across multiple executions without manually managing search attributes themselves.",
      "rootCause": null,
      "proposedFix": "Add firstExecutionRunId as a system search attribute that users can filter on to find the first execution's run ID, then use it to locate the final runId by filtering on completion or running status.",
      "workaround": "Users can implement their own search attributes to track the first execution run ID manually.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        351
      ],
      "keyQuote": "User can provide the firstExecutionRunId to find out the last runId (filter by completed or running status) to find out the last runId.",
      "number": 4247,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:35.913Z"
    },
    {
      "summary": "Request to add heartbeat details to ActivityFailure for other failure modes like Canceled and ApplicationFailure, not just TimeoutFailure. This would help users resume activities from where a previous attempt got stuck.",
      "category": "feature",
      "subcategory": "activity-heartbeat",
      "apis": [
        "ActivityFailure"
      ],
      "components": [
        "activity-executor",
        "failure-handling",
        "api-proto"
      ],
      "concepts": [
        "heartbeat",
        "activity-resume",
        "failure-details",
        "timeout",
        "retry-strategy"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily resume activities after non-timeout failures because heartbeat details are only available in TimeoutFailure.",
      "rootCause": "Heartbeat details are currently only included in TimeoutFailure, not in the generic ActivityFailure type used for other failure modes.",
      "proposedFix": "Add heartbeat details field to ActivityFailure to make it available for all failure modes (Canceled, ApplicationFailure, etc.)",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Would be great, if this can be surfaced on other items (such as Canceled and ApplicationFailure) as that can make it easier to start a new activity and resume from when a potentially previous attempt got stuck",
      "number": 4246,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:36.198Z"
    },
    {
      "summary": "Add rate limiting to the UpdateWorkerBuildIdCompatibility API to prevent spam and keep the namespace replication queue at low bandwidth for efficient cluster failover messaging.",
      "category": "feature",
      "subcategory": "worker-versioning",
      "apis": [
        "UpdateWorkerBuildIdCompatibility"
      ],
      "components": [
        "worker-versioning",
        "replication",
        "namespace-queue"
      ],
      "concepts": [
        "rate-limiting",
        "api-protection",
        "bandwidth-optimization",
        "replication",
        "cluster-failover",
        "queue-management"
      ],
      "severity": "medium",
      "userImpact": "Without rate limiting, users could accidentally or maliciously spam the UpdateWorkerBuildIdCompatibility API, causing excessive replication traffic and impacting cluster failover performance.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Need to ensure the API isn't spammed since we want the namespace replication queue to be relatively low bandwidth for cluster failover messages.",
      "number": 4240,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:22.250Z"
    },
    {
      "summary": "History service exhibits excessive memory consumption in production environments, with heap analysis showing significant memory bloat even with conservative cache settings (max 50 items). Multiple users report the same issue affecting pod resource usage in Kubernetes deployments.",
      "category": "bug",
      "subcategory": "history-service",
      "apis": [],
      "components": [
        "history-service",
        "cache",
        "heap-management"
      ],
      "concepts": [
        "memory-usage",
        "caching",
        "resource-limits",
        "performance",
        "scaling",
        "garbage-collection"
      ],
      "severity": "high",
      "userImpact": "Excessive memory consumption forces users to allocate more pod resources than expected, increasing infrastructure costs and preventing efficient cluster utilization.",
      "rootCause": "Cache implementation or garbage collection inefficiency in the history service is causing memory to not be released properly despite conservative cache size limits.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Could you help us understand if the history service detects available memory(available to the pod) and automatically set max cache size to available value?",
      "number": 4233,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:24.280Z"
    },
    {
      "summary": "Add configuration flags to toggle build ID based versioning behavior, including disabling build ID matching and CRUD endpoints for backward compatibility and rollback scenarios.",
      "category": "feature",
      "subcategory": "versioning",
      "apis": [],
      "components": [
        "versioning",
        "matching",
        "build-id",
        "crud-endpoints"
      ],
      "concepts": [
        "build-id versioning",
        "backward compatibility",
        "feature toggle",
        "rollback",
        "matching behavior"
      ],
      "severity": "medium",
      "userImpact": "Users can now disable build ID based versioning when needed for compatibility or rolling back to previous behavior.",
      "rootCause": null,
      "proposedFix": "Implement flags to disable build ID matching and disable BuildID CRUD endpoints",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented with flags to disable build ID versioning in matching and CRUD endpoints",
      "related": [],
      "keyQuote": "Use flag to disable in matching, define rollback behavior; Use flag to disable BuildID CRUD endpoints",
      "number": 4226,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:22.814Z"
    },
    {
      "summary": "Implement a background process to periodically clean up unused build IDs from the database, as they accumulate in a size-limited column and need to be scavenged to prevent storage exhaustion.",
      "category": "feature",
      "subcategory": "database-maintenance",
      "apis": [],
      "components": [
        "database",
        "build-id-management",
        "maintenance-worker"
      ],
      "concepts": [
        "build-id",
        "database-cleanup",
        "scavenging",
        "storage-limits",
        "retention-policy",
        "periodic-tasks"
      ],
      "severity": "medium",
      "userImpact": "Without a cleanup mechanism, accumulated build IDs will eventually fill the limited database column, potentially affecting system functionality.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Once we start accumulating build IDs in the DB, we'll need a process that will periodically clean up unused build IDs.",
      "number": 4225,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:10.705Z"
    },
    {
      "summary": "Implement base64 or hex encoding of hashes for task queue versioning data set IDs. This is a technical improvement to how version identifiers are encoded in the task queue versioning system.",
      "category": "feature",
      "subcategory": "task-queue-versioning",
      "apis": [],
      "components": [
        "task-queue",
        "versioning",
        "set-id"
      ],
      "concepts": [
        "hashing",
        "encoding",
        "data-serialization",
        "task-queue-versioning",
        "identifier-format"
      ],
      "severity": "low",
      "userImpact": "Improves the encoding format of version set identifiers for better compatibility and representation in task queue versioning.",
      "rootCause": null,
      "proposedFix": "Use a base64 or hex encoded hash of the version for the set ID",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation completed and integrated into the task queue versioning system",
      "related": [
        4194
      ],
      "keyQuote": "use a base64 or hex encoded hash of the version here",
      "number": 4203,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:12.006Z"
    },
    {
      "summary": "Request to use gRPC GracefulStop instead of Stop for closing frontend connections, allowing the server to drain active connections while refusing new ones during shutdown. This would reduce disruption to workers during upgrades by giving clients time to gracefully disconnect before forcefully closing connections.",
      "category": "feature",
      "subcategory": "graceful-shutdown",
      "apis": [],
      "components": [
        "grpc",
        "frontend",
        "connection-management"
      ],
      "concepts": [
        "graceful-shutdown",
        "connection-draining",
        "upgrade-resilience",
        "long-polling",
        "client-notification"
      ],
      "severity": "medium",
      "userImpact": "Enables graceful server shutdown that reduces connection disruption for workers, especially those sensitive to sudden connection termination during upgrades.",
      "rootCause": null,
      "proposedFix": "Use gRPC GracefulStop with shutdownDrainDuration as a timeout to drain active connections before forcefully closing them. Stop accepting new connections, wait the drain duration, then send GOAWAY before finishing shutdown.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented graceful stop for frontend connections",
      "related": [],
      "keyQuote": "using grpc GracefulStop with the shutdownDrainDuration as a timeout before forcefully closing connections seems like a good option",
      "number": 4202,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:10.215Z"
    },
    {
      "summary": "Feature request to add native support for controlling workflow concurrency based on workflow ID. Users want to ensure only one workflow with the same ID runs at a time, with subsequent runs queued serially, to avoid the cumbersome workaround of using SignalWithStartWorkflow.",
      "category": "feature",
      "subcategory": "workflow-concurrency-control",
      "apis": [
        "StartWorkflow",
        "ExecuteWorkflow",
        "SignalWithStartWorkflow"
      ],
      "components": [
        "workflow-engine",
        "task-queue",
        "workflow-scheduler",
        "client"
      ],
      "concepts": [
        "concurrency-control",
        "workflow-id-deduplication",
        "serial-execution",
        "queueing",
        "workflow-history",
        "traceability"
      ],
      "severity": "medium",
      "userImpact": "Users must implement complex custom queueing logic using SignalWithStartWorkflow instead of having native concurrency control, limiting workflow history traceability.",
      "rootCause": null,
      "proposedFix": "Extend StartWorkflowOptions with a WorkflowIDConcurrencyPolicy parameter supporting UNLIMITED and SERIAL modes to control concurrency per workflow ID.",
      "workaround": "Use SignalWithStartWorkflow with custom signal channel draining, though this extends history rather than creating separate workflow runs.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4386
      ],
      "keyQuote": "We want to ensure that only one workflow with the same ID runs at the same time and that subsequent workflow runs are queued with a concurrency of 1.",
      "number": 4201,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:58.972Z"
    },
    {
      "summary": "API calls timeout when workflows are locked for extended periods, preventing proper error reporting. The issue requests early return with a specific error when workflow lock cannot be acquired, and suggests lower timeouts for background operations.",
      "category": "feature",
      "subcategory": "workflow-locking",
      "apis": [],
      "components": [
        "workflow-lock",
        "task-scheduler",
        "api-handler"
      ],
      "concepts": [
        "timeout",
        "context-deadline",
        "workflow-lock",
        "resource-exhaustion",
        "background-operations",
        "caller-type"
      ],
      "severity": "high",
      "userImpact": "Users receive misleading context deadline exceeded errors when API calls fail due to workflow lock contention, making it difficult to diagnose the actual cause.",
      "rootCause": "API calls consume entire context timeout waiting for workflow lock, masking the real reason for failure and blocking goroutines in task scheduler for background operations.",
      "proposedFix": "Return early with a special error type (e.g., ResourceExhausted with workflow busy cause) when workflow lock cannot be acquired. Use lower timeout (e.g., 500ms) for background calls identified by caller-type context.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "if workflow is super busy and workflow can't be locked within the given context timeout, caller side will see a context deadline exceeded error and has no clue why",
      "number": 4196,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:52:00.001Z"
    },
    {
      "summary": "Temporal server is making excessive MySQL queries (2000+ per 30 seconds) under no-load conditions, causing potential performance and database pressure issues. The issue appears related to frequent task pump polling across multiple database shards.",
      "category": "bug",
      "subcategory": "database-polling",
      "apis": [],
      "components": [
        "task-pump",
        "persistence-layer",
        "history-shard"
      ],
      "concepts": [
        "database-polling",
        "connection-frequency",
        "query-overhead",
        "shard-scaling",
        "performance-degradation"
      ],
      "severity": "high",
      "userImpact": "Users experience excessive database load and potential performance degradation even when Temporal server is idle, impacting database resource utilization and costs.",
      "rootCause": "Each history shard has task pumps that read the database periodically for tasks. With 512 shards configured, the cumulative polling frequency becomes excessive, causing 36,000+ queries in 30 seconds under no load.",
      "proposedFix": null,
      "workaround": null,
      "related": [],
      "resolution": null,
      "resolutionDetails": null,
      "keyQuote": "Each shard will have some task pump to read DB peroidically for tasks to be processed. So if your shard count is too large it would have considerable overhead cost.",
      "number": 4193,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:59.323Z"
    },
    {
      "summary": "Server exits prematurely when shutdownDrainDuration and shutdownFailHeathCheckDuration are set to over 1 minute due to a default stop function timeout in fx.go that limits the shutdown grace period to 2 minutes total.",
      "category": "bug",
      "subcategory": "server-shutdown",
      "apis": [],
      "components": [
        "frontend",
        "fx",
        "StopService"
      ],
      "concepts": [
        "shutdown",
        "graceful-shutdown",
        "drain-duration",
        "health-check",
        "timeout",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot configure shutdown grace periods longer than 1 minute due to hardcoded timeout constraints, preventing proper draining of in-flight requests.",
      "rootCause": "Default stop function StopService in fx.go has a built-in timeout that exits the server after 2 minutes regardless of configured shutdownDrainDuration and shutdownFailHeathCheckDuration values.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal server exists after 2 minutes due to the behavior of the default stop function `StopService` set in fx.go",
      "number": 4192,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:44.699Z"
    },
    {
      "summary": "Signal dedup map in mutable state grows unbounded because request_id entries from external SignalWorkflow API calls are never cleaned up. A memory leak occurs where this map accumulates entries indefinitely without any removal mechanism.",
      "category": "bug",
      "subcategory": "signal-dedup",
      "apis": [
        "SignalWorkflow"
      ],
      "components": [
        "mutable-state",
        "signal-handling",
        "dedup-mechanism"
      ],
      "concepts": [
        "memory-leak",
        "cleanup",
        "deduplication",
        "request-tracking",
        "unbounded-growth",
        "state-management"
      ],
      "severity": "high",
      "userImpact": "Long-running workflows accumulate signal request IDs indefinitely, causing memory pressure and potential out-of-memory errors.",
      "rootCause": "Signal dedup map lacks a removal mechanism for external signal request IDs, causing unbounded accumulation.",
      "proposedFix": "Implement a limit on the map size (count-based) or use TTL-based expiration for signal dedup entries.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When the signal is coming from external (like received from SignalWorkflow API), those request_id stay in the map forever and never get cleaned up.",
      "number": 4191,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:46.916Z"
    },
    {
      "summary": "The persistenceGlobalMaxQPS dynamic config parameter is not being used in the codebase. The issue requests either enabling this configuration or removing/commenting it to avoid confusion about whether it has any effect.",
      "category": "other",
      "subcategory": "dynamic-config",
      "apis": [],
      "components": [
        "dynamic-config",
        "persistence",
        "service"
      ],
      "concepts": [
        "configuration",
        "performance",
        "qps-limiting",
        "dynamic-config",
        "unused-parameters"
      ],
      "severity": "low",
      "userImpact": "Users may attempt to configure persistenceGlobalMaxQPS without realizing it has no effect on the system.",
      "rootCause": "The persistenceGlobalMaxQPS configuration parameter is defined but not actually referenced or used in the service initialization code.",
      "proposedFix": "Either enable the persistenceGlobalMaxQPS configuration or remove/comment the related dynamic config knobs to clarify they are not functional.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved via PR #4803 which addressed the unused dynamic config parameter.",
      "related": [
        4803
      ],
      "keyQuote": "Looks as currently persistenceGlobalMaxQPS dynamic config is not used",
      "number": 4185,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:45.932Z"
    },
    {
      "summary": "Temporal 1.20.0+ fails to read JSONB fields in PostgreSQL visibility queue processor with pgbouncer in transaction pooling mode, causing the error 'pq: unsupported jsonb version number 123'. This prevents workflow completion status from updating in the UI.",
      "category": "bug",
      "subcategory": "visibility-queue",
      "apis": [],
      "components": [
        "visibility-queue-processor",
        "sql-plugin",
        "persistence-layer",
        "postgres-driver"
      ],
      "concepts": [
        "jsonb-serialization",
        "pgbouncer-pooling",
        "prepared-statements",
        "binary-parameters",
        "visibility-index"
      ],
      "severity": "high",
      "userImpact": "Workflows show as running in the list UI when they are actually completed, requiring users to view individual workflow details to see accurate status.",
      "rootCause": "pgbouncer in transaction pooling mode combined with binary_parameters flag causes the pq driver to fail parsing JSONB data with version number 123, occurring when visibility queue processor reads rows from the database.",
      "proposedFix": "Modify VisibilitySearchAttributes.Value() to return string(ret) instead of raw bytes, and add support for pgx driver as an alternative to pq.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by adding pgx driver support in PR #4913, which handles transaction mode pooling correctly.",
      "related": [
        888,
        4913
      ],
      "keyQuote": "The combination of Temporal 1.20.0+, visibility schema version 1.2+, pgbouncer run in transaction pooling mode, and temporal configured to use postgres with the binary_parameters flag set is apparently causing temporal-history to fail",
      "number": 4184,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:33.999Z"
    },
    {
      "summary": "ExecuteWorkflow with certain invalid CronSchedule values (e.g., '* * 31 2 *') causes client timeout instead of returning an immediate error. The issue appears to be caused by an infinite loop or hang in server-side cron schedule evaluation when the next scheduled time resolves to an unreachable date.",
      "category": "bug",
      "subcategory": "cron-schedule",
      "apis": [
        "ExecuteWorkflow",
        "StartWorkflowOptions"
      ],
      "components": [
        "cron-scheduler",
        "workflow-execution",
        "client-timeout"
      ],
      "concepts": [
        "timeout",
        "cron-schedule",
        "validation",
        "edge-case",
        "infinite-loop",
        "date-calculation"
      ],
      "severity": "high",
      "userImpact": "Users experience client timeouts when starting workflows with certain invalid cron schedules, with no clear error message indicating the root cause.",
      "rootCause": "Server-side cron schedule evaluation creates an infinite loop or indefinite wait when evaluating schedules that result in unreachable next execution times (e.g., February 31st).",
      "proposedFix": "Add validation to detect when cron.Next() resolves to an unreachable date (like year 0001) and return an immediate error instead of attempting to schedule.",
      "workaround": "Client-side validation using robfig/cron library: check if s.Next(time.Now()) returns a time after year 0001 and reject invalid schedules before sending to Temporal.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed via pull request #4206 which adds proper validation for cron schedules that cannot produce valid next execution times.",
      "related": [
        4206
      ],
      "keyQuote": "seems to make request timeout as is not a valid date even though it is valid cron syntax",
      "number": 4182,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:33.014Z"
    },
    {
      "summary": "Request for official CockroachDB (CRDB) support in Temporal. Currently works with the PostgreSQL v9.6 backend but lacks official support, requiring either compatible SQL scripts or alternative scripts and CI test coverage.",
      "category": "feature",
      "subcategory": "database-support",
      "apis": [],
      "components": [
        "database",
        "sql-scripts",
        "ci-testing"
      ],
      "concepts": [
        "database-compatibility",
        "postgresql",
        "cockroachdb",
        "sql-migration",
        "testing-infrastructure"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably use CockroachDB with Temporal as there is no official support or testing guarantee.",
      "rootCause": null,
      "proposedFix": "Provide SQL scripts compatible with CRDB or alternative scripts, and run a subset of tests against CRDB in CI pipeline.",
      "workaround": "Use PostgreSQL v9.6 backend instead, as CRDB currently works with it despite lacking official support.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4136
      ],
      "keyQuote": "We'd like to see CRDB becoming officially supported. The least I'd consider required for this is SQL scripts either work for CRDB too or alternative scripts are provided",
      "number": 4180,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:33.213Z"
    },
    {
      "summary": "Users need the ability to modify context propagator variables within activities or workflows and have those changes propagated back in the result when returned to the calling workflow. Currently context propagation only works one-way across the temporal stack.",
      "category": "feature",
      "subcategory": "context-propagation",
      "apis": [],
      "components": [
        "context-propagator",
        "activity-executor",
        "workflow-executor"
      ],
      "concepts": [
        "context-propagation",
        "header-propagation",
        "cross-cutting-concerns",
        "activity-results",
        "workflow-results",
        "context-modification"
      ],
      "severity": "medium",
      "userImpact": "Users cannot modify context variables in activities and have those changes reflected back to the calling workflow, limiting the use of context propagators for dynamic cross-cutting parameters.",
      "rootCause": null,
      "proposedFix": "Encode extra header info into activity/workflow results that can be decoded and applied to workflow context when results are returned.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We would like to be able to change context propagated vars within activity/workflow and have it propagated when the result is returned to the calling workflow.",
      "number": 4179,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:20.263Z"
    },
    {
      "summary": "Dev server hangs for 10 seconds when shutdown signal is sent shortly after startup. The Worker service fails to connect to Frontend and waits for its context deadline (10 seconds) before exiting. This was fixed in server 1.24.0/CLI 0.13.0.",
      "category": "bug",
      "subcategory": "dev-server-shutdown",
      "apis": [],
      "components": [
        "worker",
        "frontend",
        "dev-server"
      ],
      "concepts": [
        "shutdown",
        "context-deadline",
        "connection-timeout",
        "startup-delay",
        "service-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Integration tests and users shutting down the dev server quickly experience unnecessary 10-second delays before process termination.",
      "rootCause": "Worker service fails to connect to Frontend during early shutdown, causing it to wait until expiration of its context deadline (10 seconds) before exiting.",
      "proposedFix": "Gracefully handle early Frontend connection failures or reduce context deadline on shutdown signal.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in server 1.24.0 / CLI 0.13.0, presumably through PR #5459. Tested with various sleep durations (0.01-2 seconds) and shutdown completes within 1000-1400ms.",
      "related": [
        5459
      ],
      "keyQuote": "This delay is caused by the Worker node failing to connect to Frontend, and thus waiting until expiration of its context deadline delay (thus 10 seconds).",
      "number": 4174,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:20.125Z"
    },
    {
      "summary": "Update requests are lost when reset point occurs before them, unlike signal requests which are preserved. The feature requests that update requests should be preserved after reset by default to re-apply to the new run, similar to how signals are handled.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "reset",
        "update-request-handling",
        "signal-parity"
      ],
      "concepts": [
        "reset",
        "update-request",
        "signal",
        "preservation",
        "idempotency",
        "external-actions"
      ],
      "severity": "medium",
      "userImpact": "Users lose update requests after reset operations, requiring manual resubmission and breaking use cases dependent on update preservation like signal requests.",
      "rootCause": null,
      "proposedFix": "Preserve all update requests after reset points by default to re-apply to the new run, with an option to skip re-application.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Similar to signal, update requests are trigger from external actions like human actions, and not recoverable like activity (with idempotency).",
      "number": 4173,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:20.880Z"
    },
    {
      "summary": "Request to add a counter metric in the history service when activity task failed events are written due to retries being exhausted. This would help users monitor how often the service fails activities for this specific reason.",
      "category": "feature",
      "subcategory": "metrics-monitoring",
      "apis": [],
      "components": [
        "history-service",
        "activity-task-failed",
        "event-writing"
      ],
      "concepts": [
        "metrics",
        "monitoring",
        "activity-failures",
        "retry-exhaustion",
        "observability",
        "counters"
      ],
      "severity": "low",
      "userImpact": "Users would gain visibility into activity task failure rates due to exhausted retries, enabling better monitoring and debugging of workflow execution issues.",
      "rootCause": null,
      "proposedFix": "Add counter metric in the respondActivityTaskFailed API endpoint to track activity task failed events written to history",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would be imo useful to add counter metric here so that user can track how often service has failed activity for retries exhausted",
      "number": 4171,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:06.773Z"
    },
    {
      "summary": "Request for an API that allows UI and CLI tools to determine which workers can be safely retired as part of worker versioning in Temporal. This addresses operational needs for managing versioned worker deployments.",
      "category": "feature",
      "subcategory": "worker-versioning",
      "apis": [],
      "components": [
        "worker",
        "versioning",
        "deployment-management",
        "cli"
      ],
      "concepts": [
        "worker-versioning",
        "deployment",
        "retirement",
        "operational-safety",
        "version-compatibility",
        "worker-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Users need visibility into worker compatibility and retirement safety to manage versioned deployments effectively.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Worker versioning API was implemented to provide the requested functionality for determining which workers can be retired.",
      "related": [],
      "keyQuote": null,
      "number": 4162,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:05.900Z"
    },
    {
      "summary": "Search attributes registered successfully via CLI/API are sometimes lost after the operation reports success. This occurs occasionally when adding multiple search attributes of the same type, with missing attributes varying randomly between attempts.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [
        "AddSearchAttributes",
        "OperatorService"
      ],
      "components": [
        "frontend",
        "namespace-registration",
        "search-attributes-persistence"
      ],
      "concepts": [
        "data-loss",
        "race-condition",
        "bulk-operations",
        "namespace-setup",
        "registration-sync",
        "multi-type-handling"
      ],
      "severity": "high",
      "userImpact": "Users lose search attributes they registered without error notification, causing runtime failures when workflows attempt to use the missing attributes.",
      "rootCause": "Race condition when adding multiple search attributes of the same type in a single request; tctl code path was out of sync with Temporal CLI implementation.",
      "proposedFix": "PR #4273 fixes the issue by updating the search attributes registration logic to properly handle bulk requests with same-type attributes.",
      "workaround": "Add search attributes one at a time instead of in bulk, or use Temporal CLI instead of tctl.",
      "resolution": "fixed",
      "resolutionDetails": "PR #4273 was merged to fix the race condition in handling multiple same-type search attributes. tctl code path was also updated to sync with Temporal CLI.",
      "related": [
        4273
      ],
      "keyQuote": "It happens when trying to add multiple search attributes of same type (eg: adding 2 keyword type search attributes). It might happen that one of them might be lost.",
      "number": 4160,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:51:09.212Z"
    },
    {
      "summary": "Temporal server panics with a slice bounds out of range error during protobuf marshaling operations when processing workflow task responses. The panic occurs in OpenTelemetry's span end handler and is triggered by an UnhandledCommand error in the workflow task.",
      "category": "bug",
      "subcategory": "protobuf-marshaling",
      "apis": [],
      "components": [
        "history-engine",
        "protobuf-marshaling",
        "otel-instrumentation",
        "grpc-interceptor"
      ],
      "concepts": [
        "panic",
        "slice-bounds",
        "protobuf",
        "opentelemetry",
        "marshaling",
        "workflow-task"
      ],
      "severity": "critical",
      "userImpact": "Server crashes unexpectedly, causing service unavailability and potential data consistency issues during workflow task processing.",
      "rootCause": "OpenTelemetry SDK v1.10.0 bug in span.go:393 causing incorrect slice bounds when marshaling protobuf messages, particularly WorkflowExecutionInfo structures.",
      "proposedFix": null,
      "workaround": "Upgrade OpenTelemetry to a newer version (1.20 or later) which contains the fix.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by upgrading OpenTelemetry version. The user confirmed no further occurrences after upgrading to version 1.20.",
      "related": [],
      "keyQuote": "Looks like a otel bug. We have recently upgraded otel version, could you try with 1.20",
      "number": 4159,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:55.235Z"
    },
    {
      "summary": "WorkflowExecutionSignaledEvent lacks information about which workflow sent the signal when using SignalExternalWorkflowExecution. The event only contains generic identity field (\"history-service\") instead of the sender's workflowID and runID.",
      "category": "feature",
      "subcategory": "signals",
      "apis": [
        "SignalExternalWorkflowExecution",
        "WorkflowExecutionSignaledEvent"
      ],
      "components": [
        "history-service",
        "event-attributes",
        "signal-handling"
      ],
      "concepts": [
        "signal-sender",
        "workflow-identification",
        "event-context",
        "external-signals",
        "traceability",
        "event-metadata"
      ],
      "severity": "medium",
      "userImpact": "Users cannot identify which workflow initiated an external signal, making it difficult to track signal origins in complex multi-workflow systems.",
      "rootCause": "WorkflowExecutionSignaledEvent only contains a generic identity field instead of structured sender information (workflowID and runID).",
      "proposedFix": "Add a field to WorkflowExecutionSignaledEvent containing the workflowID and runID of the workflow that sent the signal.",
      "workaround": "Populate the identity field with sender workflow information (not ideal as it's unstructured text).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "WorkflowExecutionSignaled has a field that contains `workflowID` and `runID` of the workflow that sent the signal.",
      "number": 4155,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:55.623Z"
    },
    {
      "summary": "Request to add API for updating timeout and retry configuration of started activities dynamically. This would enable more reliable async activity completion and allow different backoff strategies for different error types without requiring short timeouts and frequent retries.",
      "category": "feature",
      "subcategory": "activity-retry",
      "apis": [
        "GetActivityInfo",
        "ErrResultPending"
      ],
      "components": [
        "activity-executor",
        "retry-policy",
        "task-pollers",
        "mutable-state"
      ],
      "concepts": [
        "async-completion",
        "timeout",
        "retry-policy",
        "backoff-strategy",
        "heartbeat",
        "error-handling",
        "activity-token"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently handle async activity completion or apply different retry strategies for different error types, forcing inefficient workarounds with short timeouts and excessive retries.",
      "rootCause": "Activity timeout and retry configuration is immutable after activity start, preventing dynamic adjustments based on runtime conditions or error types.",
      "proposedFix": "Introduce an API to update timeout/retry config for started activities with retry, leveraging the existing mutable state storage mechanism.",
      "workaround": "Set very short start-to-close timeouts with frequent retries, but this causes inefficiency and complications (duplicate messages, additional database logic) in external systems.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "One solution to this is to let activity set a smaller heartbeat timeout initially, and then update the heartbeat timeout to infinite() after successfully sending the token",
      "number": 4151,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:55.573Z"
    },
    {
      "summary": "Server version upgrades currently require manual sequential steps (v1.n.x  v1.n+1.x  ...). The request is to allow direct upgrades between non-sequential versions using automated migration logic that decouples migrations from service-specific constraints.",
      "category": "feature",
      "subcategory": "server-upgrade",
      "apis": [],
      "components": [
        "server",
        "cluster-metadata",
        "history-service",
        "migration-system"
      ],
      "concepts": [
        "version-upgrade",
        "migration",
        "metadata-management",
        "database-schema",
        "proto-encoding"
      ],
      "severity": "medium",
      "userImpact": "Users must perform time-consuming sequential version upgrades instead of upgrading directly to their target version.",
      "rootCause": "Metadata upgrades (cluster metadata, history shard metadata, queue metadata) are encoded in protos and cannot be handled by DDL-level database migrations; they require service-specific logic with proper locking rules.",
      "proposedFix": "Decouple migration logic from the service by implementing Up & Down migrations that can handle metadata and other non-schema migrations independently of service constraints.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by implementing migration system that decouples migration logic from service-specific constraints, enabling non-sequential version upgrades.",
      "related": [],
      "keyQuote": "Those upgrades are not limited to only running schema migrations. They can run metadata and other migrations too. In other words, decouple migration logic from the service.",
      "number": 4150,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:41.902Z"
    },
    {
      "summary": "After upgrading to v1.20.1, queries to temporal_visibility table cause context deadline exceeded errors and MySQL slow logs. The query optimizer incorrectly chooses the PRIMARY key index instead of the default_idx index, resulting in poor performance on large datasets (3M+ rows).",
      "category": "bug",
      "subcategory": "visibility-query-performance",
      "apis": [],
      "components": [
        "visibility-manager",
        "persistence-layer",
        "query-optimizer",
        "mysql-adapter"
      ],
      "concepts": [
        "index-selection",
        "query-performance",
        "query-timeout",
        "database-optimization",
        "execution-visibility"
      ],
      "severity": "high",
      "userImpact": "Users upgrading to v1.20.1 experience frequent context deadline exceeded errors and MySQL slow queries when the server queries the visibility table, potentially causing operational issues and performance degradation.",
      "rootCause": "MySQL query optimizer incorrectly selects PRIMARY key index instead of the more efficient default_idx index for namespace_id lookups, causing full scans on large visibility tables.",
      "proposedFix": "Force or hint the query planner to use the default_idx index, or restructure the query to encourage proper index selection.",
      "workaround": "Use USE INDEX(default_idx) hint in the SQL query to force the optimizer to use the correct index.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "MySQL did not use the \"default_idx\" index as I forced, instead, it used the PrimaryKey as index",
      "number": 4149,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:42.658Z"
    },
    {
      "summary": "Creating a search attribute via CLI produces an error message despite successfully creating the attribute in the server. The error states the attribute already exists when it actually doesn't, indicating a response/error handling issue in the server.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "cli",
        "operator",
        "search-attributes",
        "advanced-visibility"
      ],
      "concepts": [
        "error-handling",
        "search-attributes",
        "operator-commands",
        "sql-visibility",
        "namespace-operations"
      ],
      "severity": "medium",
      "userImpact": "Users encounter confusing error messages when creating search attributes, making it unclear whether the operation succeeded or failed despite the attribute being created successfully.",
      "rootCause": "The Temporal server responds with an error message even though the search attribute is successfully created, suggesting an issue with response handling or error state checking in the server.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Server was returning an error response even when the search attribute was successfully created. The issue was fixed to return proper success responses.",
      "related": [],
      "keyQuote": "A new custom search attribute is still created while producing an error",
      "number": 4145,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:40.783Z"
    },
    {
      "summary": "The server fails to validate the update state machine and accepts duplicate Accept/Complete messages for the same protocol instance ID, generating duplicate event sets instead of rejecting the violation.",
      "category": "bug",
      "subcategory": "update-protocol",
      "apis": [],
      "components": [
        "server",
        "update-handler",
        "protocol-state-machine"
      ],
      "concepts": [
        "duplicate-detection",
        "state-machine",
        "protocol-validation",
        "idempotency",
        "message-handling",
        "event-generation"
      ],
      "severity": "high",
      "userImpact": "Users may experience duplicate events and corrupted workflow state when SDKs accidentally send multiple update completion messages.",
      "rootCause": "Server does not validate that only one Accept/Complete message is allowed per update protocol instance ID, violating the update state machine contract.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was fixed as confirmed by comment from yiminc.",
      "related": [],
      "keyQuote": "If an SDK sends more then one Accept/Complete message in response to a protocol instance ID the server should reject for violating the update state machine",
      "number": 4143,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:28.475Z"
    },
    {
      "summary": "Updates completed in the last workflow task are not written to history. When a workflow sends an update in the final task before completion, the update accepted and update completed events are missing from the workflow history.",
      "category": "bug",
      "subcategory": "workflow-updates",
      "apis": [
        "SetUpdateHandler"
      ],
      "components": [
        "workflow-engine",
        "history-writer",
        "update-handler"
      ],
      "concepts": [
        "update-handling",
        "history-recording",
        "workflow-completion",
        "event-persistence",
        "final-task"
      ],
      "severity": "high",
      "userImpact": "Users cannot rely on complete update history when updates are processed in the final workflow task, breaking audit trails and update tracking.",
      "rootCause": "The workflow engine does not write update events to history when the update is completed in the last workflow task before execution completion.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Updates completed in the last WF task of a workflow are not written in history.",
      "number": 4142,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:29.346Z"
    },
    {
      "summary": "CNCF TAG-Runtime co-chair requesting a presentation or discussion about the Temporal project for their community meetings. The team acknowledged the request and moved discussion to private communication.",
      "category": "other",
      "subcategory": "community-engagement",
      "apis": [],
      "components": [],
      "concepts": [
        "community",
        "cncf",
        "presentation",
        "adoption",
        "governance"
      ],
      "severity": "low",
      "userImpact": "This enables community awareness and potential adoption of Temporal through CNCF channels.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue closed as not actionable on the repo; discussion moved to private email communication between team and CNCF TAG-Runtime co-chair.",
      "related": [],
      "keyQuote": "We'd love for you to present/discuss the Temporal project in one of our meetings",
      "number": 4139,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:28.074Z"
    },
    {
      "summary": "Workflow tasks fail repeatedly when signals arrive during continueAsNew operations, causing the workflow to either exceed history limits or fail to process signals. The server should either reject fast signals during continueAsNew failures or apply buffered signals to the next run.",
      "category": "feature",
      "subcategory": "signal-handling",
      "apis": [
        "DescribeWorkflow",
        "ContinueAsNew"
      ],
      "components": [
        "workflow-task-handler",
        "signal-processor",
        "history-manager"
      ],
      "concepts": [
        "signal-rejection",
        "continueAsNew",
        "history-limits",
        "signal-buffering",
        "workflow-restart"
      ],
      "severity": "high",
      "userImpact": "Workflows using continueAsNew under high signal load encounter failing workflow tasks in loops, risking history size limits or signal loss.",
      "rootCause": "Server fails workflow tasks when signals are received but not processed during continueAsNew, causing retry loops when signals arrive faster than they can be processed.",
      "proposedFix": "Three options: (1) Return failure reason in DescribeWF response so apps can reject signals; (2) Server automatically rejects signals when continueAsNew fails; (3) Apply buffered signals to next run instead of failing.",
      "workaround": "Application can describe workflow to check pending workflow task attempt and reject signals when attempt > 1.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in pull request #4395",
      "related": [
        1289,
        236
      ],
      "keyQuote": "workflow task won't be able to process, and failing in a loop... workflow will either get terminated because of history length/size limit or can't process the signals",
      "number": 4137,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:14.975Z"
    },
    {
      "summary": "CockroachDB 22.2.7 fails to execute Temporal v1.21 schema migration scripts due to unsupported PostgreSQL extensions and functions (btree_gin, convert_ts, tsvector cast). The issue involves incompatible SQL syntax between PostgreSQL and CockroachDB, preventing schema updates.",
      "category": "bug",
      "subcategory": "database-schema",
      "apis": [],
      "components": [
        "schema-migration",
        "postgresql-driver",
        "cockroachdb-compatibility"
      ],
      "concepts": [
        "database-compatibility",
        "sql-migration",
        "schema-update",
        "postgresql-extensions",
        "cockroachdb-support",
        "timestamp-conversion"
      ],
      "severity": "high",
      "userImpact": "Users attempting to deploy Temporal on CockroachDB encounter migration failures, blocking deployment and preventing database initialization.",
      "rootCause": "CockroachDB does not support certain PostgreSQL-specific features used in Temporal's migration scripts: the btree_gin extension, the convert_ts function, and the ::tsvector cast operator.",
      "proposedFix": "Modify migration scripts to use CockroachDB-compatible SQL syntax: replace convert_ts function with cast or CockroachDB's UDF format, and use native tsvector support (available in v23.1+).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by opening a PR to fix CockroachDB compatibility in the schema migration scripts.",
      "related": [],
      "keyQuote": "the `convert_ts` function: PSQL isn't supported yet (as in many pg-compatible DBs)",
      "number": 4136,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:14.807Z"
    },
    {
      "summary": "Workflows get stuck in RUNNING state after upgrading from MariaDB 10.2.30 to 10.6.9 with Galera clustering. Activities are scheduled but never execute, affecting some workflows non-deterministically. Issue potentially related to outdated MySQL Go driver version.",
      "category": "bug",
      "subcategory": "database-compatibility",
      "apis": [],
      "components": [
        "database",
        "activity-scheduler",
        "workflow-engine"
      ],
      "concepts": [
        "mariadb",
        "galera-cluster",
        "mysql-driver",
        "connection-pooling",
        "activity-execution"
      ],
      "severity": "high",
      "userImpact": "Workflows fail to progress in production when using MariaDB 10.6.9 with Galera clustering, causing stuck activities and workflow hangs.",
      "rootCause": "Potential incompatibility between Temporal's MySQL driver version (1.5) and MariaDB 10.6.9/Galera support, which was added in MySQL driver 1.7.",
      "proposedFix": "Upgrade MySQL Go driver from version 1.5 to the latest version (1.7+) which includes MariaDB 10.6 and Go 1.17 support.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "some workflows stay in status RUNNING forever",
      "number": 4131,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:15.782Z"
    },
    {
      "summary": "A test issue created for testing purposes. The issue contains minimal content and test comments.",
      "category": "other",
      "subcategory": "test",
      "apis": [],
      "components": [],
      "concepts": [
        "testing",
        "validation"
      ],
      "severity": "low",
      "userImpact": "No user impact; this is a test issue intended to be ignored.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Closed as invalid/test issue with no actual functionality or bug being reported.",
      "related": [],
      "keyQuote": "I am testing, please ignore.",
      "number": 4127,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:00.739Z"
    },
    {
      "summary": "APIs return unhelpful error messages like 'sql: no rows in result set' when targeting nonexistent workflows. Users should receive meaningful, user-facing error messages instead of database layer errors.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [
        "WorkflowService.RequestCancelWorkflow",
        "UpdateSchedule"
      ],
      "components": [
        "frontend",
        "error-handling",
        "persistence-layer"
      ],
      "concepts": [
        "error-messages",
        "user-experience",
        "workflow-not-found",
        "schedule-management",
        "api-reliability"
      ],
      "severity": "medium",
      "userImpact": "Users receive confusing database error messages instead of clear, actionable error descriptions when attempting operations on nonexistent workflows or schedules.",
      "rootCause": "Error messages from the persistence layer (SQL/Cassandra) are being propagated directly to API responses instead of being wrapped with context-specific, user-friendly messages.",
      "proposedFix": "Wrap database errors with meaningful error messages that describe the operation and missing resource, e.g., 'Workflow foo-bar was not found when attempting workflow cancellation'.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed by improving error message handling to prevent raw database errors from leaking into API responses.",
      "related": [],
      "keyQuote": "The message in the response should be something meaningful like 'Workflow foo-bar was not found when attempting workflow cancellation', rather than some message straight out of the database.",
      "number": 4118,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:03.743Z"
    },
    {
      "summary": "The server's continue-as-new suggestion mechanism currently only considers history event length and byte size. This enhancement requests expansion to account for additional limits such as signal count and scheduled activity count to better help users manage workflow state.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "server",
        "workflow-engine",
        "history-management"
      ],
      "concepts": [
        "continue-as-new",
        "workflow-limits",
        "history-size",
        "signals",
        "scheduled-activities",
        "resource-limits"
      ],
      "severity": "medium",
      "userImpact": "Users hitting signal or activity limits will not receive server suggestions to continue-as-new, leaving them to discover and manage these limits manually.",
      "rootCause": "The continue-as-new suggestion logic is incomplete, only evaluating history length and byte size rather than all relevant limits.",
      "proposedFix": "Expand the server suggestion mechanism to consider additional factors including number of signals received and scheduled activities.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Users can hit other limits such as number of signals received and scheduled activities, the server suggestion should take those factors into account",
      "number": 4117,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:50:02.801Z"
    },
    {
      "summary": "Request for official ScyllaDB support as a persistence layer in Temporal. User seeks guidance on integration requirements, testing procedures, and potentially coordinating community effort to make ScyllaDB an officially supported persistence layer alongside existing options.",
      "category": "feature",
      "subcategory": "persistence-layer",
      "apis": [],
      "components": [
        "persistence-layer",
        "cassandra-driver",
        "datastore-adapter"
      ],
      "concepts": [
        "persistence",
        "scylladb",
        "cassandra-compatibility",
        "storage-backend",
        "integration",
        "community-support"
      ],
      "severity": "medium",
      "userImpact": "Community members currently using ScyllaDB lack official support and integration guidance, creating uncertainty about compatibility and best practices.",
      "rootCause": null,
      "proposedFix": "Establish ScyllaDB as an officially supported persistence layer with clear feature requirements, integration guidelines, and testing procedures.",
      "workaround": "Community members are already using ScyllaDB unofficially, suggesting basic compatibility exists.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I would like to provide support for temporal community and help to accept scylladb as persistence layer",
      "number": 4105,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:49.983Z"
    },
    {
      "summary": "After a cluster version upgrade, the version info still displays an outdated \"new release available\" alert even though the current version matches the recommended version. The alert should be cleared when the upgrade is complete.",
      "category": "bug",
      "subcategory": "version-management",
      "apis": [],
      "components": [
        "cluster-admin",
        "version-info",
        "alert-system"
      ],
      "concepts": [
        "version-notification",
        "cluster-upgrade",
        "alert-clearing",
        "version-synchronization"
      ],
      "severity": "low",
      "userImpact": "Users see confusing upgrade notifications after their cluster has already been upgraded, creating confusion about the actual cluster state.",
      "rootCause": "The versionInfo alert is not cleared when the cluster version is upgraded; the lastUpdateTime updates but alerts persist despite current and recommended versions matching.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        7650
      ],
      "keyQuote": "Note that the `lastUpdateTime` is after the cluster was upgraded.",
      "number": 4094,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:50.219Z"
    },
    {
      "summary": "CVE-2023-24535 vulnerability in google.golang.org/protobuf-v1.29.0 (CVSS 7.5) affects the Temporal SDK. Parsing invalid text-format messages can cause a panic, impacting availability.",
      "category": "bug",
      "subcategory": "security-dependency",
      "apis": [],
      "components": [
        "protobuf-dependency",
        "message-parsing"
      ],
      "concepts": [
        "vulnerability",
        "denial-of-service",
        "panic",
        "transitive-dependency",
        "protocol-buffers"
      ],
      "severity": "high",
      "userImpact": "Applications using the Temporal SDK are vulnerable to denial-of-service attacks through malformed protobuf messages.",
      "rootCause": "google.golang.org/protobuf-v1.29.0 contains a panic when parsing invalid text-format messages with whitespace-only number sequences",
      "proposedFix": "Upgrade google.golang.org/protobuf to v1.29.1 or later",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Automatically closed by Mend because the vulnerable library was either ignored or no longer part of the inventory",
      "related": [],
      "keyQuote": "Parsing a text-format message which contains a potential number consisting of a minus sign, one or more characters of whitespace, and no further input will cause a panic.",
      "number": 4075,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:51.384Z"
    },
    {
      "summary": "Search attributes fail to be added when the `temporal operator search-attribute create` command is executed back-to-back. Only the last attribute is actually created, though they can all be created in a single command with multiple --name and --type flags.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "temporal-cli",
        "operator-command",
        "search-attributes"
      ],
      "concepts": [
        "search-attributes",
        "cli-command",
        "batch-operations",
        "idempotency",
        "command-execution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently create multiple search attributes sequentially and must use a workaround of combining all attributes into a single command.",
      "rootCause": "The search-attribute create command has a race condition or state management issue when executed back-to-back, causing only the final attribute to persist.",
      "proposedFix": null,
      "workaround": "Execute all search-attribute create commands with multiple --name and --type flag pairs in a single command instead of running separate commands.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #4080",
      "related": [
        4080
      ],
      "keyQuote": "When executing the following command back-to-back only one search attribute was added.",
      "number": 4073,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:37.078Z"
    },
    {
      "summary": "User reports WorkflowTaskScheduled event not retrying after WorkflowTaskTimeout expires when worker is killed. Expected timeout-based retry after 10 seconds, but task hangs in queue until WorkflowRunTimeout is reached.",
      "category": "question",
      "subcategory": "workflow-task-timeout",
      "apis": [
        "StartWorkflowOptions"
      ],
      "components": [
        "task-queue",
        "workflow-task-scheduler",
        "timeout-handler"
      ],
      "concepts": [
        "timeout",
        "retry",
        "task-queue",
        "worker-availability",
        "event-scheduling"
      ],
      "severity": "low",
      "userImpact": "Users may misunderstand WorkflowTaskTimeout behavior, expecting it to trigger retries when tasks are not picked up by workers, potentially causing confusion during worker failures.",
      "rootCause": "WorkflowTaskTimeout only applies to tasks already picked up by a worker and in execution, not tasks waiting in the queue. The task remains in queue without timing out until WorkflowRunTimeout is reached.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was clarified as a misunderstanding of WorkflowTaskTimeout behavior. The timeout applies to executing tasks, not tasks waiting in queue.",
      "related": [],
      "keyQuote": "The task is stilling in the task queue waiting for worker to pick it up. The task did not started, so it won't timeout.",
      "number": 4070,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:36.379Z"
    },
    {
      "summary": "PostgreSQL schema upgrade fails on GCP instances when creating the btree_gin extension due to insufficient permissions. The temporal_visibility user lacks the required superuser privileges to create untrusted extensions, but adding IF NOT EXISTS to the CREATE EXTENSION statement would allow reusability.",
      "category": "bug",
      "subcategory": "schema-upgrade",
      "apis": [],
      "components": [
        "schema-upgrade",
        "postgres-visibility",
        "database-initialization"
      ],
      "concepts": [
        "permissions",
        "extensions",
        "superuser",
        "GCP-postgres",
        "advanced-visibility",
        "schema-migration"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal on GCP Cloud SQL PostgreSQL instances cannot complete schema upgrades for advanced visibility features due to extension permission restrictions.",
      "rootCause": "PostgreSQL btree_gin extension requires superuser permissions to create untrusted language extensions. GCP Cloud SQL restricts true superuser creation and only provides cloudsqlsuperuser role, which is insufficient for untrusted extensions. The CREATE EXTENSION statement lacks IF NOT EXISTS clause, preventing workarounds.",
      "proposedFix": "Add IF NOT EXISTS clause to the CREATE EXTENSION btree_gin statement in schema/postgresql/v12/visibility/versioned/v1.2/advanced_visibility.sql to allow idempotent execution.",
      "workaround": "User can manually execute schema upgrade with postgres user before running temporal schema upgrade, though this requires careful coordination.",
      "resolution": "self_resolved",
      "resolutionDetails": "Issue reporter found workaround with Google Cloud support assistance and closed the issue, noting that IF NOT EXISTS would help but is not critical.",
      "related": [],
      "keyQuote": "is it possible to add \"IF NOT EXISTS\" will be in the CREATE EXTENSION statement... adding \"IF NOT EXISTS\" to the statement could help but not critical",
      "number": 4061,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:39.778Z"
    },
    {
      "summary": "Add system search attributes to help find workflows that are backoff retrying an activity. Currently, finding such workflows is difficult and requires parsing noisy production logs.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "webui",
        "search",
        "activity-executor"
      ],
      "concepts": [
        "retry",
        "backoff",
        "search-attributes",
        "workflow-visibility",
        "activity-state",
        "production-debugging"
      ],
      "severity": "medium",
      "userImpact": "Users struggle to find workflows experiencing activity retry backoff in production without wading through noisy logs.",
      "rootCause": null,
      "proposedFix": "Provide system search attributes (similar to historyLength) such as retryingActivityType to enable searching in WebUI",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Provide system search attributes like historyLength, eg retryingActivityType to help searching in webUI",
      "number": 4059,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:23.187Z"
    },
    {
      "summary": "Calling `client.workflow.list` with the latest CLI (0.6.0 with server 1.20.0) fails with 'unexpected end of JSON input' error, while the same code works with temporalite. The issue only occurs with new SQL DB configs using advanced visibility.",
      "category": "bug",
      "subcategory": "workflow-list-api",
      "apis": [
        "ListWorkflowExecutions"
      ],
      "components": [
        "client",
        "grpc-client",
        "sql-visibility",
        "advanced-visibility"
      ],
      "concepts": [
        "json-parsing",
        "grpc-streaming",
        "empty-token",
        "database-visibility",
        "protocol-buffer"
      ],
      "severity": "high",
      "userImpact": "Users cannot list workflows with the latest CLI when using advanced SQL visibility, blocking a core workflow management operation.",
      "rootCause": "Server returns empty byte array `[]byte{}` for pagination token, but code only checks against `nil`, not empty data, causing JSON parsing failure.",
      "proposedFix": "Add check for empty data in addition to nil check for pagination tokens in the server code.",
      "workaround": "Use temporalite (devel version with server 1.17.3) instead of the latest temporal CLI, or avoid advanced visibility configuration.",
      "resolution": "fixed",
      "resolutionDetails": "PR #4074 fixes the token validation to check both nil and empty data conditions.",
      "related": [
        4074
      ],
      "keyQuote": "The API is making a call with token being `[]byte{}` and the code is only checking against `nil`, not empty data.",
      "number": 4053,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:24.873Z"
    },
    {
      "summary": "Temporal Frontend incorrectly rejects valid JWTs with long expiration times (24+ hours), marking them as expired even when iat, nbf, and exp claims are valid. Tokens with shorter validity periods (1 hour) are accepted without issue.",
      "category": "bug",
      "subcategory": "jwt-validation",
      "apis": [],
      "components": [
        "authorization-interceptor",
        "jwt-validator",
        "frontend-server"
      ],
      "concepts": [
        "jwt-validation",
        "token-expiration",
        "authentication",
        "timestamp-validation",
        "claim-verification"
      ],
      "severity": "high",
      "userImpact": "Users cannot authenticate with JWTs that have longer validity periods (24+ hours), blocking server access even when tokens are technically valid.",
      "rootCause": "The JWT validator appears to have a logic error when processing tokens with extended expiration times, possibly related to timestamp comparison or clock skew handling.",
      "proposedFix": null,
      "workaround": "Use shorter JWT validity periods (under 1 hour) as a temporary workaround, though this conflicts with external auth provider settings.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Apparently, the \"long\" token validity of 24 hours is what confuses the validator. Unfortunately, we do not control validity of the token",
      "number": 4052,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:26.010Z"
    },
    {
      "summary": "When a workflow task includes RequestCancelActivityTask, CancelTimer, and CompleteWorkflowExecution commands together, the server rejects it with an UnhandledCommand error instead of processing all three commands.",
      "category": "bug",
      "subcategory": "command-processing",
      "apis": [
        "ExecuteActivity",
        "WithCancel",
        "NewTimer"
      ],
      "components": [
        "workflow-task-processor",
        "command-handler",
        "timer-manager"
      ],
      "concepts": [
        "command-validation",
        "activity-cancellation",
        "timer-cancellation",
        "workflow-completion",
        "command-ordering"
      ],
      "severity": "high",
      "userImpact": "Workflows that cancel activities and timers while completing fail with UnhandledCommand errors until activities timeout, blocking workflow completion.",
      "rootCause": "Server command handler does not properly handle the combination of RequestCancelActivityTask and CancelTimer commands in the same workflow task completion.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Server accepts a WFT with RequestCancelActivityTask, CancelTimer, and CompleteWorkflowExecution.",
      "number": 4049,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:10.925Z"
    },
    {
      "summary": "User unable to upgrade temporal_visibility PostgreSQL schema to v1.9 with error that 'queue' table does not exist. The root cause was using incorrect schema directory path (temporal instead of visibility) in the update command.",
      "category": "question",
      "subcategory": "schema-migration",
      "apis": [],
      "components": [
        "schema-tool",
        "postgresql-driver",
        "visibility-schema"
      ],
      "concepts": [
        "schema-upgrade",
        "database-migration",
        "version-compatibility",
        "postgresql",
        "configuration-error"
      ],
      "severity": "medium",
      "userImpact": "Users upgrading from v1.18.4 to v1.19.0 cannot complete the temporal_visibility schema migration, blocking their upgrade process.",
      "rootCause": "Incorrect schema directory path specified in the update-schema command: using '/v96/temporal/versioned' instead of '/v96/visibility/versioned' for the temporal_visibility database.",
      "proposedFix": "Use the correct schema path for visibility database: temporal-sql-tool with -d /etc/temporal/schema/postgresql/v96/visibility/versioned",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was user configuration error - incorrect path parameter provided to schema migration tool, not a bug in Temporal itself.",
      "related": [],
      "keyQuote": "Your command to update `temporal_visibility` database seems to be wrong. It should be... Note the path is incorrect: `v96/temporal/versioned` -> `v96/visibility/versioned`.",
      "number": 4046,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:12.726Z"
    },
    {
      "summary": "Request to expose the worker identity information for started pending activities in the server. This would allow developers to know which worker is currently executing long-running tasks.",
      "category": "feature",
      "subcategory": "activity-execution",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "pending-activity"
      ],
      "concepts": [
        "worker-identity",
        "task-assignment",
        "activity-tracking",
        "observability",
        "long-running-tasks"
      ],
      "severity": "low",
      "userImpact": "Developers would gain visibility into which worker is executing their long-running activities, improving observability and debugging capabilities.",
      "rootCause": null,
      "proposedFix": "Expose the StartedIdentity field from server internal pending activity information",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Expose more info from server internal pending activty info (StartedIdentity). For long running task, it's nice to know which worker picked current task.",
      "number": 4045,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:11.972Z"
    },
    {
      "summary": "Request to add PostgreSQL table partitioning support using hash-based partitioning on shard_id to improve scalability for large tables, with discussion of in-database partitioning as an alternative to multi-db sharding.",
      "category": "feature",
      "subcategory": "database-schema",
      "apis": [],
      "components": [
        "database",
        "persistence-layer",
        "postgres-driver"
      ],
      "concepts": [
        "partitioning",
        "scaling",
        "sharding",
        "schema-design",
        "performance",
        "data-distribution"
      ],
      "severity": "medium",
      "userImpact": "Users with large Temporal deployments on PostgreSQL would benefit from native hash partitioning for improved query performance and data distribution.",
      "rootCause": null,
      "proposedFix": "Implement hash-based table partitioning in PostgreSQL schema, using shard_id (and optionally run_id) as partition key, with partition count aligned to number of shards.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Most temporal sql query use shard_id as condition, for postgres database partition by hash is nice for large table.",
      "number": 4044,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:59.376Z"
    },
    {
      "summary": "Request to support OTEL_SERVICE_NAME environment variable to allow custom service name prefixes in OpenTelemetry traces. Currently all clusters in an organization appear under the same service name, making it difficult to distinguish between multiple instances managed by different teams.",
      "category": "feature",
      "subcategory": "observability-otel",
      "apis": [],
      "components": [
        "metrics",
        "otel-exporter",
        "tracing"
      ],
      "concepts": [
        "observability",
        "service-name",
        "environment-variables",
        "otel-configuration",
        "multi-tenant",
        "trace-identification"
      ],
      "severity": "medium",
      "userImpact": "Organizations with multiple teams running separate clusters cannot distinguish between instances in their traces without custom service name configuration.",
      "rootCause": null,
      "proposedFix": "Use OTEL_SERVICE_NAME environment variable to customize service.name: if set, use ${OTEL_SERVICE_NAME}.{history,frontend,matching}, else default to io.temporal.{history,frontend,matching}",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, suggesting the feature request was implemented or addressed",
      "related": [],
      "keyQuote": "Use OTEL_SERVICE_NAME env var to customize the service.name in the trace",
      "number": 4042,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:58.459Z"
    },
    {
      "summary": "OpenTelemetry environment variables specified in the OTEL SDK specification are not being read by Temporal Server, forcing users to configure tracing via YAML config instead.",
      "category": "bug",
      "subcategory": "otel-configuration",
      "apis": [],
      "components": [
        "otel-exporter",
        "configuration",
        "tracing"
      ],
      "concepts": [
        "opentelemetry",
        "environment-variables",
        "configuration-loading",
        "jaeger-integration",
        "tracing-export"
      ],
      "severity": "medium",
      "userImpact": "Users cannot configure OTEL tracing via standard OTEL environment variables as documented, requiring manual YAML configuration as a workaround.",
      "rootCause": "Temporal Server does not read or respect OTEL_* environment variables during initialization, only accepts YAML config.",
      "proposedFix": null,
      "workaround": "Configure tracing via YAML config with otel.exporters section specifying endpoint, protocol, and insecure flags.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved after discussion between reporter and maintainer, with suggested fix applied.",
      "related": [],
      "keyQuote": "OTEL_* env variables aren't taken into account. The only way I managed to make this work is to use this config",
      "number": 4041,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:49:00.671Z"
    },
    {
      "summary": "Request to partition activity task queues based on workflowId to improve caching performance in activity workers. Currently, activity tasks are randomly distributed across workers, preventing effective per-pod caching unlike workflow workers which use sticky execution.",
      "category": "feature",
      "subcategory": "activity-task-queue",
      "apis": [],
      "components": [
        "activity-worker",
        "task-queue",
        "worker-dispatcher"
      ],
      "concepts": [
        "caching",
        "partitioning",
        "task-distribution",
        "sticky-execution",
        "load-balancing",
        "entity-locality"
      ],
      "severity": "medium",
      "userImpact": "Users with entity-specific caches in activity workers must implement custom task queue routing logic, increasing complexity and preventing automatic cache locality optimization.",
      "rootCause": "Activity task queues lack partition-based distribution mechanism that ensures related tasks (same workflowId) route to the same worker instance, unlike workflow workers with sticky execution.",
      "proposedFix": "Implement optional partition-based task queue assignment where activity tasks are routed based on workflowId to specific worker partitions, similar to Kafka consumer group partition assignment.",
      "workaround": "Use custom task queue names in activity options (set dynamically in workflow code based on entity identifier), requiring custom logic in worker setup, workflow code, and activity code as shown in fileprocessing sample.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Partition activity task queues based on workflowId, and assign partitions to specific worker instances (similar to partition assignment within a kafka consumer group).",
      "number": 4038,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:48.016Z"
    },
    {
      "summary": "Add ability to query workflow state at specific WorkflowTaskStarted eventIds for debugging purposes. Enable replaying workflow to a specific event and executing queries against that state, plus a new API to get query results for each WorkflowTaskStarted event.",
      "category": "feature",
      "subcategory": "query-debugging",
      "apis": [
        "Query"
      ],
      "components": [
        "workflow-engine",
        "event-history",
        "query-handler"
      ],
      "concepts": [
        "workflow-replay",
        "debugging",
        "event-id",
        "query-execution",
        "terminated-workflows",
        "troubleshooting"
      ],
      "severity": "medium",
      "userImpact": "Enables developers to debug workflow state at specific points in execution history and safely query terminated workflows.",
      "rootCause": null,
      "proposedFix": "Add 'WorkflowTaskStartedEventId' argument to query request; implement workflow replay up to that event and query execution; create new API returning list of query results per WorkflowTaskStarted eventId.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add 'WorkflowTaskStartedEventId' argument to query request. The workflow should be replayed up to that event id and a query should be executed against it.",
      "number": 4030,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:48.805Z"
    },
    {
      "summary": "Request to clear signal requestID on workflow close since all signal requests are rejected after closure. The signal requestID is currently used for deduplication, but may not be needed after workflow completion.",
      "category": "feature",
      "subcategory": "signal-deduplication",
      "apis": [],
      "components": [
        "signal-handler",
        "workflow-state",
        "deduplication"
      ],
      "concepts": [
        "signal-deduplication",
        "workflow-lifecycle",
        "cleanup",
        "state-management",
        "memory-optimization"
      ],
      "severity": "low",
      "userImpact": "Clearing signal requestIDs on workflow close could reduce memory overhead without affecting functionality since signals are rejected after closure.",
      "rootCause": "Signal requestID map persists after workflow close even though it's no longer needed for deduplication.",
      "proposedFix": "Clear signal requestID on workflow close, with option to persist requestID in signaled event to support workflow reset scenarios.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4028
      ],
      "keyQuote": "Signal requestID is for dedupping signal request. After workflow close, all signal requests will be rejected, so we don't really need signal requestIDs for dedupping.",
      "number": 4029,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:47.829Z"
    },
    {
      "summary": "Signal requestID is not carried over when a workflow is reset, breaking deduplication for signals that were sent before the reset. The signal event is picked up in the new run but its corresponding requestID is lost.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-reset",
        "signal-deduplication",
        "event-history"
      ],
      "concepts": [
        "signal-deduplication",
        "workflow-reset",
        "requestID",
        "event-replay",
        "state-preservation"
      ],
      "severity": "medium",
      "userImpact": "Users lose signal deduplication guarantees after workflow reset, potentially allowing duplicate signal processing when signals are re-executed.",
      "rootCause": "Signal requestID is not being carried forward when the workflow history is reset, only the signal event itself is preserved.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "After reset, signal event got picked but not signal request ID.",
      "number": 4028,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:34.965Z"
    },
    {
      "summary": "Workflow task retry handling is inefficient because it relies on long start-to-close timeouts (up to 10min) to prevent duplicate responses. This causes problems for features like Query that must wait for pending workflow tasks to complete, as extended wait times can cause in-memory state eviction and timeout failures.",
      "category": "feature",
      "subcategory": "workflow-task-retry",
      "apis": [],
      "components": [
        "workflow-task-processing",
        "query",
        "mutable-state-cache"
      ],
      "concepts": [
        "retry",
        "timeout",
        "workflow-pause",
        "backoff",
        "pending-workflow",
        "state-eviction"
      ],
      "severity": "high",
      "userImpact": "Queries can fail with Unavailable errors or timeout when workflow tasks are delayed due to retry handling, and users lack efficient retry mechanisms.",
      "rootCause": "Current implementation increases workflow task start-to-close timeout to prevent duplicate responses on retries, but this long wait time causes in-memory state to be evicted from cache and affects dependent features like Query.",
      "proposedFix": "Implement workflow pause feature or schedule backoff time for next workflow task attempt without overriding start-to-close timeout.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Comment indicates that the query problem should fail fast instead of waiting, as it's already implemented. Workflow pause tracking is handled separately.",
      "related": [],
      "keyQuote": "Instead of waiting for next workflow task, it is better to fail fast which is already implemented.",
      "number": 4022,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:37.502Z"
    },
    {
      "summary": "Signal requestIDs are stored indefinitely in workflow mutable state, causing unbounded growth and potential performance issues when workflows receive many signals. The issue requests an expiration policy for these requestIDs while balancing deduplication guarantees and enforcing max signal count limits.",
      "category": "other",
      "subcategory": "signal-deduplication",
      "apis": [
        "Signal"
      ],
      "components": [
        "mutable-state",
        "signal-handler",
        "deduplication"
      ],
      "concepts": [
        "signal-requestID",
        "mutable-state-size",
        "memory-management",
        "expiration-policy",
        "deduplication",
        "performance"
      ],
      "severity": "high",
      "userImpact": "Workflows with high signal volume accumulate unbounded requestIDs in mutable state, causing performance degradation and potential out-of-memory issues.",
      "rootCause": "Signal requestIDs are retained indefinitely for deduplication purposes, with no expiration mechanism despite max signal count limits.",
      "proposedFix": "Implement expiration policy for requestIDs: rely on max signal count limit, expire by time, or expire by count while preserving deduplication within a reasonable window.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        4191
      ],
      "keyQuote": "signal requestID will be store in workflow mutable state forever, which will result in an increase in mutable state size, which could lead to performance issue",
      "number": 4021,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:36.176Z"
    },
    {
      "summary": "When switching a namespace's active cluster in a multi-cluster replication setup, there is an unexpected 10+ minute lag between TimerStarted and TimerFired events when using Workflow.sleep(). Investigation suggests possible timer queue backlog during cluster failover.",
      "category": "bug",
      "subcategory": "timer-execution",
      "apis": [
        "sleep"
      ],
      "components": [
        "timer-queue",
        "multi-cluster-replication",
        "namespace-failover"
      ],
      "concepts": [
        "cluster-switching",
        "timer-latency",
        "queue-backlog",
        "replication",
        "workflow-sleep",
        "active-cluster"
      ],
      "severity": "medium",
      "userImpact": "Workflows using Workflow.sleep() experience significant delays during namespace active cluster changes, causing unexpected workflow execution delays.",
      "rootCause": "Suspected timer queue backlog during active cluster switching based on metrics showing task_latency_queue_bucket staying flat and ReadHistoryBranchReverse persistence requests increasing.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the time between `TimerStarted` and `TimerFired` to be more than 10 minutes",
      "number": 4020,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:25.058Z"
    },
    {
      "summary": "WFT message validation incorrectly rejects valid message sequences with multiple updates because it doesn't consider the update instance ID when validating Response messages, causing false errors when multiple update requests are coalesced into a single WFT.",
      "category": "bug",
      "subcategory": "workflow-task-processing",
      "apis": [],
      "components": [
        "workflow-task-handler",
        "message-validation",
        "update-processing"
      ],
      "concepts": [
        "message-validation",
        "update-requests",
        "workflow-task",
        "coalescing",
        "instance-id",
        "state-machine"
      ],
      "severity": "medium",
      "userImpact": "Workers processing multiple concurrent updates fail validation when those updates are coalesced into a single workflow task, blocking legitimate workflows.",
      "rootCause": "Validation logic checks for Response messages before Acceptance messages without considering that different Response messages may correspond to different updates with different IDs.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Confirmed as already fixed in a later version",
      "related": [],
      "keyQuote": "The validation code will incorrectly give an error indicating that it has seen a Response message before an Acceptance message. While this is true, the Response was for a different update",
      "number": 4018,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:23.074Z"
    },
    {
      "summary": "Namespace-specific search attributes fail to register with a caching delay when `system.forceSearchAttributesCacheRefreshOnRead=true` is enabled. Starting a workflow immediately after registering a search attribute fails with INVALID_ARGUMENT error, indicating the server's cache hasn't been refreshed.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [
        "addSearchAttributes",
        "workflow.execute"
      ],
      "components": [
        "namespace",
        "search-attributes-cache",
        "search-attribute-registration"
      ],
      "concepts": [
        "caching",
        "registration-delay",
        "namespace-isolation",
        "cache-invalidation",
        "search-attributes"
      ],
      "severity": "high",
      "userImpact": "Users cannot immediately use newly registered search attributes in workflows due to cache synchronization delays, blocking workflow execution.",
      "rootCause": "Search attributes cache is not being refreshed immediately when namespace-specific search attributes are registered with `system.forceSearchAttributesCacheRefreshOnRead=true` enabled.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by ensuring cache refresh on read when namespace-specific search attributes are registered.",
      "related": [],
      "keyQuote": "INVALID_ARGUMENT: Namespace default has no mapping defined for search attribute foo",
      "number": 4017,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:23.894Z"
    },
    {
      "summary": "Temporal auto setup fails to connect to PostgreSQL 15 due to pg_hba.conf authentication issues. User upgraded AWS RDS from PostgreSQL 14 to 15 and Temporal ceased working, though the issue was ultimately attributed to misconfiguration rather than Postgres 15 incompatibility.",
      "category": "bug",
      "subcategory": "database-setup",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "schema-setup",
        "postgres-driver"
      ],
      "concepts": [
        "database-connection",
        "authentication",
        "schema-migration",
        "configuration",
        "kubernetes-deployment"
      ],
      "severity": "medium",
      "userImpact": "Users upgrading to PostgreSQL 15 on AWS RDS may encounter connection failures during Temporal auto setup if pg_hba.conf is not properly configured.",
      "rootCause": "Missing or incorrect pg_hba.conf entry for the dbuser connecting from the Temporal pod's IP (10.12.87.28) to the default PostgreSQL databases without encryption",
      "proposedFix": null,
      "workaround": "Downgrade to PostgreSQL 14 or ensure pg_hba.conf is configured to allow connections from the Temporal pod IP",
      "resolution": "invalid",
      "resolutionDetails": "Issue was determined to be a misconfiguration in the Airbyte Helm chart's pg_hba.conf settings rather than a Temporal Server incompatibility with PostgreSQL 15. Testing by maintainer confirmed Temporal works with Postgres 15.",
      "related": [],
      "keyQuote": "pq: no pg_hba.conf entry for host \"10.12.87.28\", user \"dbuser\", database \"postgres\", no encryption",
      "number": 4016,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:12.208Z"
    },
    {
      "summary": "DescribeWorkflow API times out when a workflow has a large number of pending activities (e.g., 1500) because it fetches the scheduled event for each activity to retrieve the ActivityType, causing performance degradation.",
      "category": "bug",
      "subcategory": "workflow-describe",
      "apis": [
        "DescribeWorkflow"
      ],
      "components": [
        "mutable-state",
        "workflow-description",
        "activity-scheduler"
      ],
      "concepts": [
        "timeout",
        "pending-activities",
        "performance",
        "event-loading",
        "scalability"
      ],
      "severity": "high",
      "userImpact": "Users cannot describe workflows with large numbers of pending activities due to API timeouts, blocking visibility into workflow state.",
      "rootCause": "DescribeWorkflow loads scheduled events from history for each pending activity individually to retrieve ActivityType information, resulting in O(n) event lookups where n is the number of pending activities.",
      "proposedFix": "Move ActivityType information to mutable state instead of loading it from event history on demand.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #4717 which optimizes the event loading approach for pending activities.",
      "related": [
        4717
      ],
      "keyQuote": "DescribeWorkflow timeout because it try to fetch scheduled event for each of the pending activity, only because it needs to show ActivityType for each pending activity.",
      "number": 4005,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:12.158Z"
    },
    {
      "summary": "A transitive dependency vulnerability (CVE-2020-28928) was detected in modernc.org/libc-v1.22.3, a dependency of modernc.org/sqlite, with a CVSS score of 5.5. The issue was automatically closed when the vulnerable library was no longer part of the dependency inventory.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependencies",
        "build-system",
        "sqlite-binding"
      ],
      "concepts": [
        "vulnerability",
        "buffer-overflow",
        "dependency-management",
        "security-patch",
        "transitive-dependency"
      ],
      "severity": "medium",
      "userImpact": "Users could potentially experience availability issues due to buffer overflow vulnerabilities in transitive dependencies used by the Temporal server.",
      "rootCause": "Buffer overflow in musl libc's wcsnrtombs function when handling particular combinations of destination buffer size and source character limit",
      "proposedFix": "Upgrade musl libc to version 1.2.2-1 or later",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was automatically closed by Mend when the vulnerable library was marked as ignored or removed from the dependency inventory",
      "related": [],
      "keyQuote": "wcsnrtombs mishandles particular combinations of destination buffer size and source character limit, as demonstrated by an invalid write access (buffer overflow)",
      "number": 4002,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:48:10.102Z"
    },
    {
      "summary": "Concurrent map read and write panic occurring during Temporal server startup, happening intermittently when starting multiple container instances in quick succession. The error is preventing server initialization in Docker environments.",
      "category": "bug",
      "subcategory": "server-startup",
      "apis": [],
      "components": [
        "server",
        "initialization",
        "persistence"
      ],
      "concepts": [
        "race-condition",
        "concurrency",
        "panic",
        "startup",
        "docker",
        "postgres"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal in Docker environments experience random startup failures (~5% of runs), requiring manual restarts and causing CI/CD pipeline instability.",
      "rootCause": "Concurrent map read and write operation occurring during server initialization, likely in initialization or persistence layer code.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        212
      ],
      "keyQuote": "Regularly crashing in our CI environments. Every ~1/20 pipeline executions.",
      "number": 4000,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:59.602Z"
    },
    {
      "summary": "S3 archival fails in global namespaces after failover to a different region because the S3 client is initialized with the original region's configuration and attempts to access buckets in the wrong region.",
      "category": "bug",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "archival",
        "s3-archiver",
        "history-service"
      ],
      "concepts": [
        "global-namespace",
        "failover",
        "multi-region",
        "s3",
        "configuration",
        "cluster-deployment"
      ],
      "severity": "high",
      "userImpact": "Users with global namespaces spanning multiple regions cannot successfully archive workflows after failover, causing archive operations to fail with 400 Bad Request errors.",
      "rootCause": "The S3 archiver client is configured at cluster startup with the cluster's default region and does not dynamically adjust the region when accessing S3 buckets after failover to a different region.",
      "proposedFix": null,
      "workaround": "Configure both clusters to use the same S3 bucket instead of region-specific buckets. Alternatively, use AWS S3 Multi-Region Access Points (MRAP) if compatible with the archival implementation.",
      "resolution": "wontfix",
      "resolutionDetails": "The issue was clarified as expected behavior. Users must configure both clusters to use the same S3 bucket configuration, or use S3 MRAP. The workaround is documented in comments.",
      "related": [],
      "keyQuote": "You have to config your clusters to use same S3 bucket. Basically the same config for both cluster A and B.",
      "number": 3991,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:59.762Z"
    },
    {
      "summary": "Request for operator API to blacklist workers by version and identity to prevent misconfigured old workers from connecting to production clusters. Should also consider IP address-based blacklisting.",
      "category": "feature",
      "subcategory": "worker-management",
      "apis": [],
      "components": [
        "worker",
        "server",
        "operator-api"
      ],
      "concepts": [
        "worker-blacklist",
        "version-control",
        "deployment-safety",
        "worker-identity",
        "access-control"
      ],
      "severity": "high",
      "userImpact": "Allows operators to prevent problematic worker versions from connecting to production, reducing deployment mistakes and their cascading effects.",
      "rootCause": null,
      "proposedFix": "Provide operator API to blacklist workers by version, identity string, and optionally IP address ranges",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Provide an operator API to blacklist workers that have a specific version from connecting, as well as blacklist specific workers by their identity string.",
      "number": 3990,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:59.930Z"
    },
    {
      "summary": "User connects Temporal to a read-only MySQL slave database and expects it to work, but Temporal server fails to start because it requires write access to the database.",
      "category": "question",
      "subcategory": "database-configuration",
      "apis": [],
      "components": [
        "frontend",
        "history",
        "matching",
        "worker",
        "persistence"
      ],
      "concepts": [
        "read-only database",
        "database connectivity",
        "MySQL replication",
        "startup failure",
        "write access"
      ],
      "severity": "medium",
      "userImpact": "Users cannot start Temporal server when the database is configured as read-only, blocking deployment in read-only database setups.",
      "rootCause": "Temporal server requires write access to the database for initialization and operation, but MySQL master-slave replication configured to read-only slaves prevents this.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Temporal server fundamentally requires write access to the database. Using a read-only replica is not a supported configuration.",
      "related": [],
      "keyQuote": "Temporal server needs write access to the MySQL db. With read-only, it is expected not working.",
      "number": 3989,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:46.496Z"
    },
    {
      "summary": "Request to implement caching of query results in the Temporal Server to reduce redundant calculations when multiple users frequently query the same workflow state. The cache should only be invalidated when the actual query result changes.",
      "category": "feature",
      "subcategory": "query-caching",
      "apis": [],
      "components": [
        "query-engine",
        "workflow-state",
        "cache-layer"
      ],
      "concepts": [
        "caching",
        "query-optimization",
        "state-polling",
        "performance",
        "resource-efficiency"
      ],
      "severity": "medium",
      "userImpact": "Users can reduce server load and improve query response times when multiple clients frequently poll the same workflow state.",
      "rootCause": null,
      "proposedFix": "Add caching of query results with cache invalidation only when the query result is updated as part of the last workflow task.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Most of the query results are the same, so there is no need to recalculate them unless workflow returns a different value.",
      "number": 3988,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:47.011Z"
    },
    {
      "summary": "Feature request to support named timers in workflow testing to make unit tests more robust. Currently timers are identified by creation order (1, 2, 3...), making tests brittle when timer ordering changes. The proposal is to allow named timers similar to named goroutines.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [
        "SetOnTimerScheduledListener"
      ],
      "components": [
        "workflow-testing",
        "timer-management",
        "test-framework"
      ],
      "concepts": [
        "named-identifiers",
        "unit-testing",
        "timer-identification",
        "test-brittleness",
        "test-robustness"
      ],
      "severity": "medium",
      "userImpact": "Users must maintain brittle unit tests where timer placement order determines test expectations, making it difficult to refactor workflow timer code.",
      "rootCause": "Timers are identified by creation order rather than meaningful names, causing test dependencies on code structure.",
      "proposedFix": "Add SetOnNamedTimerScheduledListener callback that passes both timer ID and name, allowing named timer identification similar to goroutines.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This means unit tests are brittle because the placement of a timer in the code will mean the position expectations will break.",
      "number": 3987,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:46.340Z"
    },
    {
      "summary": "GetHistoryTask persistence API is unused in the codebase. The author proposes removing it since the batch read version (GetHistoryTasks) can handle the same use cases for admin tools.",
      "category": "other",
      "subcategory": "persistence-api",
      "apis": [],
      "components": [
        "persistence",
        "history-task",
        "admin-api"
      ],
      "concepts": [
        "api-cleanup",
        "unused-code",
        "batch-operations",
        "task-visibility"
      ],
      "severity": "low",
      "userImpact": "Removal of unused API simplifies the codebase with no impact on users as GetHistoryTasks provides equivalent functionality.",
      "rootCause": "GetHistoryTask was never utilized in the codebase; GetHistoryTasks (batch version) provides sufficient functionality for all use cases.",
      "proposedFix": "Remove GetHistoryTask API from the codebase.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Will remove this API. The only use case I can see is use it as an admin tool to dump information for a certain task, which GetHistoryTasks (the batch read version) can also do.",
      "number": 3983,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:32.646Z"
    },
    {
      "summary": "Deprecate the `QueueAckLevel` field in shardInfo as it's replaced by `QueueStates` in the multi-cursor implementation. However, replication queue still requires per-cluster ack level tracking, necessitating a new field for replication queue state management.",
      "category": "other",
      "subcategory": "queue-management",
      "apis": [],
      "components": [
        "shardInfo",
        "queue-states",
        "replication-queue",
        "dlq"
      ],
      "concepts": [
        "deprecation",
        "queue-ack-level",
        "multi-cursor",
        "replication",
        "cluster-ack-tracking",
        "dlq-management"
      ],
      "severity": "medium",
      "userImpact": "Internal refactoring to consolidate queue state representation; no direct user impact but affects internal architecture.",
      "rootCause": "Multi-cursor implementation made `QueueAckLevel` redundant for most queues, but replication queue requires per-cluster ack level tracking that cannot be represented in `QueueStates`.",
      "proposedFix": "Create a new field for replication that combines normal ack level and DLQ ack level for each source cluster, similar to existing separate DLQ ack level field.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Field was deprecated and new field structure was created for replication queue to handle per-cluster ack level tracking.",
      "related": [],
      "keyQuote": "replication queue is still using QueueAckLevel, which has per (remote) cluster ack level...create a new field for replication that combine normal ack level and dlq ack level.",
      "number": 3981,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:35.015Z"
    },
    {
      "summary": "Dependency security vulnerability in modernc.org/sqlite v1.20.4 with a transitive vulnerability (CVE-2020-28928) in modernc.org/libc v1.22.2 causing a buffer overflow in wcsnrtombs function.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "modernc.org/sqlite",
        "modernc.org/libc"
      ],
      "concepts": [
        "buffer-overflow",
        "dependency-vulnerability",
        "security",
        "cvss-5.5",
        "transitive-dependency"
      ],
      "severity": "medium",
      "userImpact": "Users of the Temporal Server may be exposed to potential availability attacks through the vulnerable SQLite dependency.",
      "rootCause": "The modernc.org/libc library (v1.22.2) contains a buffer overflow vulnerability in the wcsnrtombs function that mishandles specific combinations of destination buffer size and source character limit.",
      "proposedFix": "Upgrade modernc.org/libc to a version where the vulnerability is fixed (musl 1.2.2-1 or later).",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "The issue was automatically closed because the vulnerable library was either marked as ignored or is no longer part of the Mend inventory.",
      "related": [],
      "keyQuote": "wcsnrtombs mishandles particular combinations of destination buffer size and source character limit, as demonstrated by an invalid write access (buffer overflow).",
      "number": 3980,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:34.475Z"
    },
    {
      "summary": "Dependency security vulnerability in uber-go/tally v4.1.6 which includes Apache Thrift with two high-severity CVEs (CVE-2019-0205 and CVE-2019-0210) affecting availability. Issue was automatically closed when the vulnerable library was removed from Mend inventory.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "tally",
        "thrift",
        "dependency-management"
      ],
      "concepts": [
        "security-vulnerability",
        "availability",
        "denial-of-service",
        "dependency-update",
        "thrift-protocol"
      ],
      "severity": "high",
      "userImpact": "Servers could experience denial of service attacks through endless loops or panics when processing malformed input from untrusted sources.",
      "rootCause": "Apache Thrift versions up to 0.12.0 have infinite loop and panic vulnerabilities in JSON protocol handlers when processing specific or invalid input data.",
      "proposedFix": "Upgrade Apache Thrift dependency to version 0.13.0 or later.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was automatically closed by Mend bot when the vulnerable library was either marked as ignored or removed from the Mend inventory, indicating the dependency was likely already updated or replaced.",
      "related": [],
      "keyQuote": "This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.",
      "number": 3979,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:18.856Z"
    },
    {
      "summary": "Task latency metric includes workflow lock delays, causing false alerts when many workflows signal one target workflow. The request is to exclude latency from workflow lock timeouts from the task_latency metric or create a separate metric.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "task-processor",
        "workflow-lock",
        "metrics"
      ],
      "concepts": [
        "latency-measurement",
        "workflow-lock",
        "timeout",
        "retry-backoff",
        "concurrent-signals",
        "metric-accuracy"
      ],
      "severity": "medium",
      "userImpact": "False latency alerts are triggered when workflows contend for locks, misleading operators about actual system performance.",
      "rootCause": "task_latency metric includes all retry backoffs and workflow lock wait times, not just actual task processing latency.",
      "proposedFix": "Exclude latency related to workflow lock timeouts from task_latency metric, or create a separate metric. Return a special error type from workflow lock acquisition to distinguish lock timeouts from other timeout errors.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented via PR #4408",
      "related": [
        4408
      ],
      "keyQuote": "task_latency will be very high and may trigger alerts when a large # of workflow trying to signal one workflow",
      "number": 3978,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:19.001Z"
    },
    {
      "summary": "Parent workflows can get stuck when child workflows timeout or are terminated before the child started event is recorded in the parent's history. The parent fails to record the child completion event, preventing progress.",
      "category": "bug",
      "subcategory": "child-workflow-handling",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-engine",
        "history-processing",
        "child-workflow-executor"
      ],
      "concepts": [
        "child-workflow",
        "event-ordering",
        "timeout",
        "workflow-completion",
        "history-consistency"
      ],
      "severity": "high",
      "userImpact": "Parent workflows can become stuck and unable to progress when child workflows timeout or terminate before their started event is recorded.",
      "rootCause": "The record child completed logic does not handle the case where the child workflow completion event arrives before the child started event has been recorded in the parent's history.",
      "proposedFix": "Create the child started event on child completion when it hasn't been recorded yet.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Parent won't record the completion event and can get stuck if child timedout or get terminated before child started event is recorded in parent's history.",
      "number": 3977,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:17.318Z"
    },
    {
      "summary": "Add TLS support for Elasticsearch client connections in Temporal server, similar to the Cadence implementation. TLS support has been added to resolve this request.",
      "category": "feature",
      "subcategory": "elasticsearch-tls",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "server-config",
        "tls"
      ],
      "concepts": [
        "tls",
        "encryption",
        "elasticsearch",
        "security",
        "client-connection",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users can now securely connect to Elasticsearch clusters using TLS encryption through server configuration.",
      "rootCause": null,
      "proposedFix": "Implement TLS support for ES client connections following the pattern used in Cadence PR #4154",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "TLS support was added to the Elasticsearch client",
      "related": [],
      "keyQuote": "TLS support has been added.",
      "number": 3939,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:06.623Z"
    },
    {
      "summary": "The default Docker configuration template does not expose the secondary_visibility configuration option, which is needed for proper visibility handling in Temporal deployments.",
      "category": "feature",
      "subcategory": "docker-configuration",
      "apis": [],
      "components": [
        "docker",
        "config-template",
        "visibility"
      ],
      "concepts": [
        "visibility",
        "secondary-visibility",
        "configuration",
        "docker-deployment",
        "infrastructure"
      ],
      "severity": "medium",
      "userImpact": "Users cannot configure secondary visibility settings through the default Docker configuration template.",
      "rootCause": "The secondary_visibility configuration option is not included in the config_template.yaml file.",
      "proposedFix": "Add secondary_visibility configuration to the docker/config_template.yaml",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Community user submitted PR #3917 to expose secondary_visibility in the default Docker config template",
      "related": [
        3917
      ],
      "keyQuote": "does not expose secondary config",
      "number": 3916,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:05.541Z"
    },
    {
      "summary": "Worker fails to find the default namespace immediately after server startup due to a caching issue, requiring a 20-second delay to work around the problem. The issue was resolved through namespace cache changes that allow reading through the cache.",
      "category": "bug",
      "subcategory": "namespace-management",
      "apis": [],
      "components": [
        "worker",
        "namespace-cache",
        "server-startup"
      ],
      "concepts": [
        "namespace-registration",
        "caching",
        "server-initialization",
        "timing-issue",
        "default-namespace"
      ],
      "severity": "high",
      "userImpact": "Workers cannot connect to the default namespace immediately after server startup and require manual delays or registration workarounds.",
      "rootCause": "Namespace cache did not allow reading through the cache, causing workers to fail namespace lookups immediately after server startup.",
      "proposedFix": "Implement namespace read through cache to allow workers to access the default namespace even during server initialization.",
      "workaround": "Wait 20 seconds after server startup before starting workers, or automatically register the default namespace.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by namespace read through cache changes in PR #3908.",
      "related": [
        3908
      ],
      "keyQuote": "I have to do a wait for 20s after startup iWF server... Otherwise the worker will not working correct (GoSDK) complaining namespace default not found",
      "number": 3914,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:47:05.676Z"
    },
    {
      "summary": "Cassandra-backed Temporal Server experiences timeout errors during workflow execution due to excessive tombstone scanning when querying expired tasks. Users encounter 'Operation timed out' errors on StartWorkflowExecution with only partial responses from the database.",
      "category": "bug",
      "subcategory": "cassandra-persistence",
      "apis": [
        "StartWorkflowExecution"
      ],
      "components": [
        "frontend",
        "cassandra-persistence",
        "workflow-execution"
      ],
      "concepts": [
        "tombstone-scanning",
        "query-timeout",
        "expired-tasks",
        "cassandra-performance",
        "database-queries"
      ],
      "severity": "high",
      "userImpact": "Users experience timeout failures when starting workflows on Cassandra-backed servers due to inefficient expired task cleanup scanning.",
      "rootCause": "Cassandra encounters excessive tombstones when querying for expired tasks, causing query timeouts and incomplete responses.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "operation CreateWorkflowExecution encounter Operation timed out - received only 1 responses",
      "number": 3912,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:51.981Z"
    },
    {
      "summary": "Request to extend namespace delete functionality to work across multiple clusters in a multi-cluster environment, similar to how RegisterNamespaceRequest handles ClusterReplicationConfig. Currently, deletion only affects the current cluster and requires separate deletion calls to each cluster.",
      "category": "feature",
      "subcategory": "namespace-management",
      "apis": [
        "DeleteNamespace",
        "RegisterNamespace"
      ],
      "components": [
        "namespace-service",
        "multi-cluster-replication",
        "cluster-management"
      ],
      "concepts": [
        "multi-cluster",
        "namespace-deletion",
        "cluster-replication",
        "cross-cluster-operations",
        "namespace-lifecycle"
      ],
      "severity": "low",
      "userImpact": "Multi-cluster environment operators must manually delete namespaces in each cluster separately rather than having a single delete operation propagate across the replication topology.",
      "rootCause": "Namespace deletion does not leverage the same cluster replication configuration mechanism available for namespace registration, limiting cross-cluster consistency.",
      "proposedFix": "Add capability to pass ClusterReplicationConfig (or similar) with DeleteNamespaceRequest to enable cross-cluster namespace deletion, analogous to RegisterNamespaceRequest behavior.",
      "workaround": "Connect to each cluster individually and issue separate delete namespace requests to each cluster.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers decided against automatic cross-cluster namespace deletion due to namespace migration concerns. Deletion is a rare operation and users can safely issue separate requests to each cluster. Current behavior is safer as it prevents accidental cascade deletion during cluster migration scenarios.",
      "related": [],
      "keyQuote": "Delete namespace is a rare operation so is is reasonable to issue delete requests separately to all clusters.",
      "number": 3888,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:55.000Z"
    },
    {
      "summary": "LIKE operator with wildcard characters causes a URI malformed error in the frontend and doesn't work as expected when searching visibility attributes. The issue is compounded by unclear documentation about which field types support wildcards.",
      "category": "bug",
      "subcategory": "visibility-search",
      "apis": [],
      "components": [
        "visibility-search",
        "ui-frontend",
        "query-parser"
      ],
      "concepts": [
        "wildcards",
        "visibility-query",
        "string-matching",
        "search-attributes",
        "uri-encoding"
      ],
      "severity": "medium",
      "userImpact": "Users experience crashes (URIError) when trying to use LIKE with wildcards in visibility search, and the feature doesn't work as documented.",
      "rootCause": "Percent characters in wildcard queries aren't being properly URI escaped, causing frontend errors. Additionally, LIKE operator support for various field types is unclear or not fully implemented.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "LIKE does not appear to be working as expected. I tried with WorkflowType and no results were returned.",
      "number": 3885,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:53.302Z"
    },
    {
      "summary": "Implement CallerTypePreemptable priority for replication and non-critical queues (excluding transfer/timer/visibility) to mitigate resource contention during namespace migration.",
      "category": "feature",
      "subcategory": "task-scheduling",
      "apis": [],
      "components": [
        "task-executor",
        "queue-management",
        "replication",
        "task-loading"
      ],
      "concepts": [
        "priority-scheduling",
        "resource-contention",
        "caller-type",
        "namespace-migration",
        "queue-management",
        "task-categorization"
      ],
      "severity": "medium",
      "userImpact": "Users performing namespace migrations will experience improved resource contention and stability through prioritization of critical queue operations.",
      "rootCause": "Task executor assumes all tasks are background priority, lacking differentiated caller type specification for different task categories.",
      "proposedFix": "Implement mechanism to specify different caller types for different task categories in task executor and task loading implementation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation completed, confirmed by maintainer.",
      "related": [
        3870
      ],
      "keyQuote": "The new CallerTypePreemptable should be used for all replication and all queues other than transfer/timer/visibility to resolve issues of resource contention.",
      "number": 3874,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:42.491Z"
    },
    {
      "summary": "SQLite search attribute persistence fails when a Datetime value has a +00:00 timezone offset, though the same format passes Go's RFC3339Nano parser. The issue could not be reproduced by other contributors.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [
        "UpsertSearchAttributes"
      ],
      "components": [
        "search-attributes",
        "sqlite",
        "datetime-parsing"
      ],
      "concepts": [
        "timezone-handling",
        "datetime-persistence",
        "rfc3339-parsing",
        "search-attributes",
        "validation"
      ],
      "severity": "low",
      "userImpact": "Users setting Datetime search attributes in SQLite with +00:00 timezone receive validation errors despite using the documented RFC3339Nano format.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Could not be reproduced by contributors; likely user error or environment-specific issue",
      "related": [],
      "keyQuote": "Could not reproduce. Tried executing this code snippet in a workflow",
      "number": 3864,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:41.506Z"
    },
    {
      "summary": "Users need a way to pin/preserve specific closed workflow runs to prevent them from being automatically purged, allowing developers to investigate rare or difficult-to-reproduce issues without racing against retention deadlines.",
      "category": "feature",
      "subcategory": "workflow-retention",
      "apis": [],
      "components": [
        "retention-policy",
        "workflow-history",
        "ui"
      ],
      "concepts": [
        "data-retention",
        "workflow-preservation",
        "purging",
        "lifecycle-management",
        "debugging",
        "investigation"
      ],
      "severity": "medium",
      "userImpact": "Users lose the ability to investigate rare or difficult-to-reproduce workflow issues if they don't preserve them before automatic purging occurs.",
      "rootCause": null,
      "proposedFix": "Implement a pin/mark feature in the Temporal UI that allows users to explicitly prevent specific closed workflow runs from being purged by the retention policy.",
      "workaround": "Users can manually download workflow history JSON for local storage as a temporary workaround.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal can provide a button to mark/pin/save a run and this makes sure the run would not get purged.",
      "number": 3863,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:40.521Z"
    },
    {
      "summary": "InvalidArgument gRPC error returned for poll requests with deadlines within 2 seconds of the server clock is non-deterministic, making it unreliable for callers to determine retryability. This rare condition occurs when a proxy holds up a request during internal server outages.",
      "category": "bug",
      "subcategory": "grpc-error-handling",
      "apis": [],
      "components": [
        "grpc-server",
        "request-deadline",
        "error-handling"
      ],
      "concepts": [
        "deadline",
        "timeout",
        "gRPC-error-codes",
        "retryability",
        "determinism",
        "InvalidArgument"
      ],
      "severity": "medium",
      "userImpact": "SDK callers cannot reliably distinguish between retryable and non-retryable InvalidArgument errors when deadline validation fails.",
      "rootCause": "Poll requests with deadlines within 2 seconds of server system clock return InvalidArgument status, creating non-deterministic behavior in rare scenarios with proxy delays during server outages.",
      "proposedFix": "Either return a different gRPC status code for deadline validation failures, or reconsider treating InvalidArgument as non-retryable on the SDK side.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed via PR #3856",
      "related": [
        218
      ],
      "keyQuote": "If a poll request is made with a deadline time within 2s of the server system clock, an InvalidArgument is returned. This only happens when there is a proxy that would hold up a request during internal server outage.",
      "number": 3846,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:28.794Z"
    },
    {
      "summary": "Worker fails to start temporal-sys-history-scanner-workflow with 'context deadline exceeded' error after upgrading to 1.19.0, preventing workflow creation and persisting intermittently. Issue occurs during startup when services are restarted quickly, particularly in Kubernetes environments.",
      "category": "bug",
      "subcategory": "server-startup",
      "apis": [],
      "components": [
        "worker",
        "scanner",
        "startup"
      ],
      "concepts": [
        "context-deadline",
        "startup-timeout",
        "service-restart",
        "cluster-membership",
        "workflow-initialization"
      ],
      "severity": "high",
      "userImpact": "Users upgrading to 1.19.0 experience workflow creation failures and service startup issues that persist intermittently, requiring workarounds like clearing cluster_membership tables.",
      "rootCause": "The startup timeout for the history scanner workflow was too short, causing context deadline exceeded errors when services were restarted in quick succession or when cluster services started at different times.",
      "proposedFix": "Increase the startup timeout for the history scanner workflow initialization, as implemented in PR #3911.",
      "workaround": "Clear the cluster_membership table and restart all services, or wait at least 20 seconds between stopping and starting services.",
      "resolution": "fixed",
      "resolutionDetails": "PR #3911 was merged to increase the startup timeout, resolving the issue. User testing confirmed the fix eliminated the problem across multiple pod restarts.",
      "related": [
        3911
      ],
      "keyQuote": "The startup timeout is too short. I added a fix https://github.com/temporalio/temporal/pull/3911.",
      "number": 3826,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:29.992Z"
    },
    {
      "summary": "Query operations on terminated workflows that lack a WorkflowTaskStarted event currently timeout with a generic DEADLINE_EXCEEDED error. The request is to return a specific, descriptive error message that immediately explains why the query failed.",
      "category": "feature",
      "subcategory": "query-workflow",
      "apis": [
        "QueryWorkflow"
      ],
      "components": [
        "query-handler",
        "workflow-execution",
        "event-history"
      ],
      "concepts": [
        "terminated-workflow",
        "event-history",
        "error-messaging",
        "developer-experience",
        "deadline"
      ],
      "severity": "medium",
      "userImpact": "Developers spend time debugging confusing timeout errors when querying terminated workflows, slowing down troubleshooting and understanding of system behavior.",
      "rootCause": "Queries on terminated workflows without WorkflowTaskStarted event lack immediate validation, causing the query to timeout rather than fail fast with an explanatory error.",
      "proposedFix": "Return a specific error message when attempting to query a workflow execution that was terminated before the WorkflowTaskStarted event, explaining that queries cannot be performed in this state.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed in PR #4867 which implements descriptive error handling for queries on terminated workflows.",
      "related": [
        4867
      ],
      "keyQuote": "This Workflow Execution was terminated before the WorkflowTaskStarted event, which means it can't be Queried.",
      "number": 3821,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:27.351Z"
    },
    {
      "summary": "Workflow task timeout is not capped when using continue as new, allowing users to set arbitrary long timeouts that persist in the new run without being capped.",
      "category": "bug",
      "subcategory": "workflow-task-timeout",
      "apis": [
        "WithWorkflowTaskTimeout"
      ],
      "components": [
        "workflow",
        "task-timeout",
        "continue-as-new"
      ],
      "concepts": [
        "timeout",
        "capping",
        "workflow-task",
        "continue-as-new",
        "task-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users can bypass workflow task timeout limits using continue as new, potentially causing unexpected behavior in workflow executions.",
      "rootCause": "Workflow task timeout capping logic is not applied when continuing as new with user-specified timeout values.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "User can set arbitrary long workflow task timeout via `workflow.WithWorkflowTaskTimeout()` and do a continue as new. And in the new run, the workflow task timeout is not capped",
      "number": 3811,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:13.762Z"
    },
    {
      "summary": "SignalWithStart called after workflow execution creation immediately schedules a workflow task, bypassing the firstWorkflowTaskBackoff timer in cron executions. This causes workflows to execute immediately rather than respecting their cron schedule definition.",
      "category": "bug",
      "subcategory": "cron-scheduling",
      "apis": [
        "SignalWithStart"
      ],
      "components": [
        "cron-engine",
        "workflow-task-scheduler",
        "signal-handling"
      ],
      "concepts": [
        "scheduling",
        "timing",
        "cron",
        "backoff",
        "workflow-initialization",
        "signal-interaction"
      ],
      "severity": "medium",
      "userImpact": "Users with cron workflows may experience unexpected immediate execution when using SignalWithStart, circumventing their defined schedule.",
      "rootCause": "SignalWithStart scheduling a workflow task immediately disregards the firstWorkflowTaskBackoff timer that cron executions should respect.",
      "proposedFix": "Modify SignalWithStart to honor the firstWorkflowTaskBackoff timer when scheduling workflow tasks for cron executions.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Acknowledged as a bug but deemed unlikely to be fixed due to potential user dependency on current behavior and concern about breaking changes in legacy cron feature.",
      "related": [],
      "keyQuote": "It's definitely a bug, but it's in an old feature that no one is actively working on. And as I said, I'd be worried about the fix breaking someone who happened to be depending on it.",
      "number": 3805,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:16.923Z"
    },
    {
      "summary": "Client receives unhelpful 'Unknown' error when server cache is exhausted under high load, with excessive error logging instead of proper back-pressure. Should return 'ResourceExhausted' error and warn instead of error-level logging.",
      "category": "bug",
      "subcategory": "cache-management",
      "apis": [
        "PollMutableState"
      ],
      "components": [
        "cache",
        "history-service",
        "error-handling",
        "telemetry"
      ],
      "concepts": [
        "backpressure",
        "resource-exhaustion",
        "cache-capacity",
        "error-codes",
        "logging"
      ],
      "severity": "high",
      "userImpact": "Clients receive cryptic error messages and server logs are flooded with errors when workflow cache is exhausted under load, making debugging difficult.",
      "rootCause": "Cache exhaustion handling returns 'Unknown' error code instead of 'ResourceExhausted' and logs errors instead of warnings.",
      "proposedFix": "Return ResourceExhausted error code with appropriate message and reduce logging to warning level for cache exhaustion scenarios.",
      "workaround": "Increase history.cacheMaxSize configuration value or upgrade to v1.21.4 if running v1.21.3 which had cache calculation issues.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #4796",
      "related": [
        4796
      ],
      "keyQuote": "Cache capacity is fully occupied with pinned elements - which, while perhaps a true message, isn't terribly useful.",
      "number": 3802,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:16.077Z"
    },
    {
      "summary": "User seeking proto files and documentation to implement a custom C# SDK without Rust interop. Question about whether gRPC or REST APIs are available for SDK development.",
      "category": "question",
      "subcategory": "sdk-development",
      "apis": [],
      "components": [
        "grpc-api",
        "proto-definitions"
      ],
      "concepts": [
        "sdk-implementation",
        "interoperability",
        "grpc",
        "multi-language-support",
        "dotnet"
      ],
      "severity": "low",
      "userImpact": "User looking to build alternative SDK implementation but lacks documentation on protocol contracts and gRPC definitions.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Team clarified that official .NET SDK uses Rust Core and gRPC definitions are available in api repo, but building independent SDK is infeasible. Closed as the request doesn't align with SDK architecture strategy.",
      "related": [],
      "keyQuote": "We have TypeScript, Python, and Ruby SDKs all being built on top of the same library",
      "number": 3794,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:03.005Z"
    },
    {
      "summary": "SQLite database tables disappear at runtime in development mode, causing HTTP 503 errors and operation failures. The issue occurs after running temporal server start-dev for extended periods, requiring a restart to recover.",
      "category": "bug",
      "subcategory": "sqlite-persistence",
      "apis": [],
      "components": [
        "sqlite-persistence",
        "metadata-client",
        "task-queue-persistence",
        "namespace-registry"
      ],
      "concepts": [
        "database-corruption",
        "table-missing",
        "sqlite-driver",
        "persistence-layer",
        "dev-server",
        "schema-integrity"
      ],
      "severity": "high",
      "userImpact": "Developers using temporal server start-dev experience unexpected server crashes with missing database tables, requiring manual restarts and losing work state.",
      "rootCause": "SQLite driver bug causing table metadata loss at runtime, fixed in sqlite driver merge request 74",
      "proposedFix": "Upgrade to Temporal v1.26 or later which includes the SQLite driver fix from https://gitlab.com/cznic/sqlite/-/merge_requests/74",
      "workaround": "Restart the temporal server process to recover from the error state",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in Temporal v1.26 by updating the SQLite driver with a bug fix from the upstream sqlite project",
      "related": [
        124
      ],
      "keyQuote": "SQL logic error: no such table: cluster_metadata_info (1)",
      "number": 3784,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:02.900Z"
    },
    {
      "summary": "Feature request to add SignalWithStartExternalWorkflow capability to workflow code, mirroring the existing SignalWithStartWorkflow client API functionality.",
      "category": "feature",
      "subcategory": "external-workflows",
      "apis": [
        "SignalWithStartWorkflow"
      ],
      "components": [
        "workflow-client",
        "external-workflow",
        "signal-api"
      ],
      "concepts": [
        "signaling",
        "workflow-lifecycle",
        "external-workflows",
        "workflow-control",
        "async-operations"
      ],
      "severity": "low",
      "userImpact": "Users currently cannot signal and start external workflows from within workflow code, requiring workarounds or client-side implementation.",
      "rootCause": null,
      "proposedFix": "Implement SignalWithStartExternalWorkflow method in workflow code API with equivalent behavior to the client-side SignalWithStartWorkflow.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "This issue was marked as a duplicate of issue #537 and transferred to the server repository as it requires server-side implementation.",
      "related": [
        537
      ],
      "keyQuote": "Feature request is to add 'SignalWithStartExternalWorkflow' (name tbd) which can be called in workflow code.",
      "number": 3773,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:46:01.088Z"
    },
    {
      "summary": "User requests MS SQL Server support as a persistence backend for Temporal Server, noting that some on-premise customers require it. There's discussion about the architectural challenges and potential solutions including plugin refactoring and gRPC-based persistence alternatives.",
      "category": "feature",
      "subcategory": "persistence-backend",
      "apis": [],
      "components": [
        "persistence-plugin",
        "storage-abstraction",
        "configuration-system"
      ],
      "concepts": [
        "persistence",
        "database-backend",
        "MS-SQL-Server",
        "plugin-architecture",
        "distributed-transaction",
        "data-storage",
        "extensibility"
      ],
      "severity": "medium",
      "userImpact": "Organizations using MS SQL Server are unable to use Temporal Server without finding third-party solutions or workarounds, limiting adoption in enterprise environments.",
      "rootCause": "Current architecture tightly couples persistence implementations; adding new database backends requires significant maintenance overhead and lacks external contributor support infrastructure.",
      "proposedFix": "Refactor persistence layer to allow plugins to reside in separate repositories and be injected at runtime, with generic testing infrastructure for plugin validation.",
      "workaround": "Third-party implementation available at https://gitlab.com/lercher/temporal-sqls (unofficial).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Ideally we would first refactor the persistence which allows these plugins to reside in separate repos and injected at runtime.",
      "number": 3769,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:50.628Z"
    },
    {
      "summary": "User encounters 'schema_version table doesn't exist' error when deploying Temporal Server on Kubernetes with Cloud SQL. The error indicates the database schema was not properly initialized before the server startup.",
      "category": "question",
      "subcategory": "deployment-setup",
      "apis": [],
      "components": [
        "sql-schema",
        "database-initialization",
        "server-startup"
      ],
      "concepts": [
        "database-migration",
        "schema-version",
        "gcp-cloudsql",
        "kubernetes-deployment",
        "initialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy Temporal Server on Cloud SQL without proper schema initialization, preventing server startup.",
      "rootCause": "Database schema was not created/initialized before server startup. The schema_version table is missing from the temporal database.",
      "proposedFix": null,
      "workaround": "Ensure database schema is initialized before starting the server (typically via schema setup/migration scripts).",
      "resolution": "invalid",
      "resolutionDetails": "Issue was closed as it was determined to be a user configuration/setup issue rather than a bug. Responder redirected user to community support.",
      "related": [],
      "keyQuote": "temporal.schema_version' doesn't exist usually means db was not created.",
      "number": 3765,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:51.046Z"
    },
    {
      "summary": "WorkflowRunTimeout does not trigger defer functions or throw errors when reached - the workflow is silently terminated. User expects to catch timeout errors and run cleanup code in defer blocks.",
      "category": "bug",
      "subcategory": "workflow-timeout",
      "apis": [
        "ExecuteWorkflow",
        "StartWorkflowOptions"
      ],
      "components": [
        "workflow-runtime",
        "timeout-handling",
        "context-cancellation"
      ],
      "concepts": [
        "timeout",
        "error-handling",
        "defer",
        "workflow-termination",
        "context-cancellation"
      ],
      "severity": "high",
      "userImpact": "Users cannot detect workflow timeouts or execute cleanup code when WorkflowRunTimeout is reached, causing silent workflow termination without proper error handling.",
      "rootCause": "WorkflowRunTimeout termination does not properly propagate as a context cancellation error or trigger defer functions in the workflow.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Issue was closed and reposted to temporalio/sdk-go#984 for proper tracking in the Go SDK repository.",
      "related": [
        984
      ],
      "keyQuote": "The fact is the temporal will terminate the workflow without entering the the defer function.",
      "number": 3746,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:49.585Z"
    },
    {
      "summary": "CRON scheduled workflows that use ContinueAsNew do not schedule subsequent CRON runs after the workflow completes. The workflow ends without triggering the next scheduled execution.",
      "category": "bug",
      "subcategory": "workflow-scheduling",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "workflow-scheduler",
        "cron-service",
        "temporal-server"
      ],
      "concepts": [
        "cron-scheduling",
        "continue-as-new",
        "workflow-continuation",
        "scheduled-execution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably combine CRON scheduling with ContinueAsNew, breaking workflows that need periodic execution with internal state resets.",
      "rootCause": "The CRON scheduling mechanism does not properly handle workflow continuation when ContinueAsNew is invoked, likely due to incomplete implementation as referenced in issue #2146.",
      "proposedFix": "Use the newer Schedules feature instead of CRON with ContinueAsNew.",
      "workaround": "Switch to the Schedules feature for periodic workflow execution.",
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of #746. The issue was not fixed but rather superseded by the newer Schedules feature.",
      "related": [
        2146,
        746
      ],
      "keyQuote": "#2146 wasn't fixed, it was just marked as a dup. You should try the new Schedules feature instead.",
      "number": 3745,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:36.896Z"
    },
    {
      "summary": "ES connection errors lack debugging information about the configured connection settings. Users need to see the scheme and host from their configuration to quickly identify typos or misconfigurations versus actual Elasticsearch issues.",
      "category": "other",
      "subcategory": "elasticsearch-logging",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "error-logging",
        "config-validation"
      ],
      "concepts": [
        "debugging",
        "connection-errors",
        "configuration",
        "error-messages",
        "logging"
      ],
      "severity": "medium",
      "userImpact": "Users debugging Elasticsearch connection failures cannot easily trace errors back to their configuration, making it difficult to identify typos or misconfigurations.",
      "rootCause": "Error messages from ES connection failures do not include the configured connection parameters (scheme, host), making it hard to distinguish config issues from actual Elasticsearch problems.",
      "proposedFix": "Add ES config values (scheme, host at minimum) to the error message to help users debug connection issues.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "PR #4811 added more useful debugging information for ES connection issues while being mindful of not exposing sensitive configuration data.",
      "related": [
        4811
      ],
      "keyQuote": "Currently small typos in config can cause this error and its hard to debug / trace it back to values user configured vs some ES specific problem.",
      "number": 3710,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:38.075Z"
    },
    {
      "summary": "Request for a listener/observer pattern to capture Temporal workflow and activity lifecycle events (start, completion, etc.) at the worker level, with ability to filter, transform, and persist to external databases. Must distinguish between first execution and replay/retry events.",
      "category": "feature",
      "subcategory": "event-listening",
      "apis": [],
      "components": [
        "worker",
        "workflow-execution",
        "activity-execution",
        "event-handling"
      ],
      "concepts": [
        "event-listener",
        "lifecycle-events",
        "replay-detection",
        "event-persistence",
        "worker-interceptor",
        "event-filtering"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently capture and persist workflow/activity lifecycle events for integration with external systems without querying and transforming Temporal data at runtime.",
      "rootCause": "Current interceptor implementation only fires on subset of events (start/finish) and cannot distinguish between first execution and replay events.",
      "proposedFix": "Implement worker-level listener/observer that captures all lifecycle events with replay/retry detection capability, similar to existing interceptor pattern but more comprehensive.",
      "workaround": "Use existing interceptors (limited to start/finish events) or gRPC proxy approach (not available in Java SDK and incomplete for server-side completions like timeout/termination).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "ability to distinguish workflow / activity replay / retry events from \"first execution\" events",
      "number": 3709,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:39.261Z"
    },
    {
      "summary": "SQL persistence layer throws a NullPointerException when accessed after being closed instead of returning a proper error. The NPE occurs in the factory.go file when attempting to use a closed persistence connection.",
      "category": "bug",
      "subcategory": "sql-persistence",
      "apis": [],
      "components": [
        "sql-persistence",
        "factory",
        "persistence-layer"
      ],
      "concepts": [
        "connection-lifecycle",
        "null-pointer-exception",
        "error-handling",
        "resource-cleanup",
        "closed-state"
      ],
      "severity": "high",
      "userImpact": "Users experience crashes with unhelpful NPE errors instead of clear error messages when the SQL persistence layer is closed.",
      "rootCause": "Missing null check or proper state validation before accessing persistence resources after closure",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Return error from sql persistence if persistence layer is already closed.",
      "number": 3708,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:26.913Z"
    },
    {
      "summary": "Users need a way to replay and reset workflows from saved historical data after workflow history has been purged from Temporal, enabling recovery from issues that take longer to fix than the retention period.",
      "category": "feature",
      "subcategory": "workflow-replay",
      "apis": [
        "Replayer"
      ],
      "components": [
        "workflow-engine",
        "history-management",
        "replay-mechanism"
      ],
      "concepts": [
        "workflow-reset",
        "history-import",
        "replay",
        "retention",
        "workflow-recovery"
      ],
      "severity": "medium",
      "userImpact": "Users cannot recover failed workflows after history retention expires, even with saved history snapshots.",
      "rootCause": "Current Replayer only supports in-memory replay and cannot spawn new workflow runs from historical JSON data.",
      "proposedFix": "Implement workflow import functionality to allow resetting and continuing workflows from serialized history JSON, with support for resuming from specific event IDs.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Related PR #4853 addresses workflow import functionality.",
      "related": [
        4853
      ],
      "keyQuote": "Workflow authors or support teams should be able to save the temporal workflow & replay the stateful history even if the history has been purged from temporal.",
      "number": 3703,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:26.469Z"
    },
    {
      "summary": "Request to add ExecutionStatus field to archival visibility syntax for S3 and Google Cloud Storage archivers, enabling users to query archived workflow status without scanning workflow history.",
      "category": "feature",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "archival",
        "visibility",
        "s3-archiver",
        "gcloud-archiver"
      ],
      "concepts": [
        "workflow-status",
        "archived-workflows",
        "visibility-query",
        "workflow-search",
        "archival-storage"
      ],
      "severity": "medium",
      "userImpact": "Users must currently scan entire workflow history to determine archived workflow status instead of querying it directly.",
      "rootCause": null,
      "proposedFix": "Add ExecutionStatus as a queryable field in archival visibility syntax alongside existing fields (WorkflowType, WorkflowID, StartTime, CloseTime, SearchPrecision).",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Please add ExecutionStatus to this so finding archived wf status does not have to be done by scanning workflow history.",
      "number": 3700,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:25.026Z"
    },
    {
      "summary": "Request for a read-only workflow worker mode that can process and respond to workflow queries but will not execute workflows, activities, or process signals. This would enable maintenance periods where systems need to remain queryable without performing work.",
      "category": "feature",
      "subcategory": "worker-configuration",
      "apis": [],
      "components": [
        "worker",
        "workflow-processor",
        "query-handler"
      ],
      "concepts": [
        "maintenance-mode",
        "read-only",
        "query-processing",
        "workflow-execution",
        "signal-handling",
        "activity-execution"
      ],
      "severity": "low",
      "userImpact": "Allows operators to pause workflow/activity/signal processing during maintenance while keeping the system responsive to queries.",
      "rootCause": null,
      "proposedFix": "Implement a read-only worker mode that processes queries but rejects or queues workflow starts, activity executions, and signals.",
      "workaround": "Custom code in workflows using timer events with workflow.await() to block processing during maintenance periods.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "a read only worker which can process /respond to queryes but will not process activity/siginals/workflows",
      "number": 3688,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:15.362Z"
    },
    {
      "summary": "Transitive dependency vulnerability in modernc.org/libc-v1.21.5 (CVE-2020-28928) affecting modernc.org/sqlite-v1.20.0. This is a buffer overflow in the wcsnrtombs function with CVSS score 5.5 (Medium severity), automatically resolved when the vulnerable library was removed from the dependency tree.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "modernc.org/sqlite",
        "modernc.org/libc"
      ],
      "concepts": [
        "security-vulnerability",
        "buffer-overflow",
        "transitive-dependency",
        "cve",
        "cvss"
      ],
      "severity": "medium",
      "userImpact": "Potential availability impact through buffer overflow in character encoding functions, though practical exploitation requires local access and low privileges.",
      "rootCause": "Buffer overflow in musl libc wcsnrtombs function when handling particular combinations of destination buffer size and source character limit.",
      "proposedFix": "Upgrade musl to version 1.2.2-1 or 1.1.16-3+deb9u1, or update the modernc.org/sqlite and modernc.org/libc dependencies to versions with the fix applied.",
      "workaround": null,
      "resolution": "stale",
      "resolutionDetails": "Automatically closed when the vulnerable library was no longer part of the Mend inventory or was marked as ignored.",
      "related": [],
      "keyQuote": "wcsnrtombs mishandles particular combinations of destination buffer size and source character limit, as demonstrated by an invalid write access (buffer overflow)",
      "number": 3685,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:15.271Z"
    },
    {
      "summary": "Two high-severity vulnerabilities (CVE-2019-0205 and CVE-2019-0210) were detected in the uber-go/tally dependency version 4.1.3 related to Apache Thrift. Both vulnerabilities involve denial of service risks with CVSS score 7.5 and were automatically resolved when the dependency was no longer part of the inventory.",
      "category": "bug",
      "subcategory": "dependency-security-vulnerability",
      "apis": [],
      "components": [
        "dependency-management",
        "uber-go-tally",
        "apache-thrift"
      ],
      "concepts": [
        "security-vulnerability",
        "denial-of-service",
        "dependency-upgrade",
        "cvss-score",
        "apache-thrift"
      ],
      "severity": "high",
      "userImpact": "Users could be affected by potential denial of service attacks through vulnerable Thrift protocol handling in the metrics library.",
      "rootCause": "Apache Thrift versions 0.9.3 to 0.12.0 contain vulnerabilities where servers implemented in Go using TJSONProtocol or TSimpleJSONProtocol may panic or enter endless loops with invalid input data.",
      "proposedFix": "Upgrade org.apache.thrift:libthrift to version 0.13.0 or later to resolve both CVE-2019-0205 and CVE-2019-0210.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Automatically closed by Mend because the vulnerable library was no longer part of the Mend inventory or was marked as ignored in the dependency management.",
      "related": [],
      "keyQuote": "a server or client may run into an endless loop when feed with specific input data",
      "number": 3684,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:14.257Z"
    },
    {
      "summary": "Task queue scavenger feature is not functional for Cassandra persistence because the ListTaskQueue API is not implemented in the Cassandra persistence layer. This prevents stale task queues from being cleaned up.",
      "category": "bug",
      "subcategory": "persistence-cassandra",
      "apis": [],
      "components": [
        "persistence",
        "cassandra",
        "matching-task-store",
        "task-queue-scavenger"
      ],
      "concepts": [
        "task-queue",
        "cleanup",
        "scavenger",
        "persistence-api",
        "stale-data"
      ],
      "severity": "medium",
      "userImpact": "Users with Cassandra persistence will accumulate stale task queues that cannot be cleaned up, potentially affecting system resource management and maintenance.",
      "rootCause": "ListTaskQueue persistence API is not implemented in Cassandra matching_task_store.go",
      "proposedFix": "Implement the ListTaskQueue API for Cassandra persistence to enable task queue scavenger functionality",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1021
      ],
      "keyQuote": "Task queue scavenger is not working for cassandra. This is mostly because the ListTaskQueue persistence API is not implemented by Cassandra persistence.",
      "number": 3682,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:01.756Z"
    },
    {
      "summary": "Temporal Server returns incorrect RetryState (NonRetryableError instead of Timeout) when activity ScheduleToClose timeout occurs in speculative mode (before next retry attempt). This inconsistency with factual mode complicates user error handling and violates expected semantics for timeout failures.",
      "category": "bug",
      "subcategory": "activity-timeout",
      "apis": [
        "ActivityFailure",
        "TimeoutFailureInfo"
      ],
      "components": [
        "activity-executor",
        "retry-logic",
        "timeout-handler"
      ],
      "concepts": [
        "timeout",
        "retry",
        "retryState",
        "scheduleToClose",
        "activity-failure",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably distinguish between explicit non-retryable errors and timeout-enforced failures, complicating retry logic and error handling in workflows.",
      "rootCause": "Server sets retryState=NonRetryableError in speculative timeout mode instead of retryState=Timeout, creating inconsistency between speculative (proactive) and factual (actual) timeout failures.",
      "proposedFix": "Return ActivityFailure with retryState=Timeout and TimeoutFailureInfo(timeoutType=ScheduleToClose) consistently in both speculative and factual modes. Include last activity failure as cause and only return TimeoutFailureInfo for StartToClose timeout.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "RetryState=NonRetryableError should happen only when an activity attempt is failed with a type explicitly set as non-retryable by the user and only in this situation.",
      "number": 3667,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:03.678Z"
    },
    {
      "summary": "Request to add a flag controlling when ParentClosePolicy applies: either on individual run completion or on entire workflow chain completion. Currently, keeping children alive during continue-as-new requires ABANDON policy, which prevents automatic child termination when parent is terminated.",
      "category": "feature",
      "subcategory": "parent-close-policy",
      "apis": [
        "ContinueAsNew",
        "ParentClosePolicy"
      ],
      "components": [
        "workflow-engine",
        "parent-child-coordination",
        "retention-handler"
      ],
      "concepts": [
        "parent-close-policy",
        "continue-as-new",
        "workflow-chain",
        "child-termination",
        "retention"
      ],
      "severity": "medium",
      "userImpact": "Users cannot keep children running across a chain of parent continue-as-new calls while maintaining the ability to terminate all children when the final parent run is terminated.",
      "rootCause": "ParentClosePolicy currently only applies at individual run completion, not at workflow chain completion level.",
      "proposedFix": "Add a `parentClosePolicyScope` flag with options RUN or WORKFLOW_CHAIN that determines the scope of policy application. When set to WORKFLOW_CHAIN, terminating the last run terminates all children from the entire chain, respecting parent retention intervals.",
      "workaround": "Set ParentClosePolicy to ABANDON to keep children running during continue-as-new, but this disables automatic child termination on parent termination.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add a flag that specifies if ParentClosePolicy applies when the chain is done or the run is done.",
      "number": 3666,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:45:01.724Z"
    },
    {
      "summary": "User requests the ability to update workflow search attributes after a workflow has started, from outside the workflow context. This would enable external systems to tag workflows with metadata and update multiple workflows based on queries (e.g., all failed workflows).",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "search-attributes",
        "workflow-execution",
        "visibility"
      ],
      "concepts": [
        "metadata-tagging",
        "external-updates",
        "batch-operations",
        "query-filtering",
        "workflow-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently add or update search attributes for workflows from outside the workflow context, limiting their ability to enrich workflow metadata with external system data.",
      "rootCause": null,
      "proposedFix": "Add API capability to update search attributes on workflows outside of workflow context, with support for bulk updates based on query filters (e.g., ExecutionStatus='Failed').",
      "workaround": "Store workflow IDs and search attributes in a separate datastore, or manually update the Elasticsearch index using the Elasticsearch API directly.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I would like the ability to update a workflows search attributes outside of the workflow context. Preferably with the ability to be able to update a set of workflows based on a query.",
      "number": 3665,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:51.375Z"
    },
    {
      "summary": "Temporal services experience extremely long startup times (up to an hour) when the Cassandra cluster is degraded due to gocql's token-aware host selection policy waiting for unavailable nodes during connection initialization.",
      "category": "bug",
      "subcategory": "persistence-cassandra",
      "apis": [],
      "components": [
        "persistence",
        "cassandra-client",
        "gocql",
        "host-selection-policy"
      ],
      "concepts": [
        "connection-timeout",
        "cluster-degradation",
        "startup-performance",
        "fault-tolerance",
        "host-discovery",
        "connection-pooling"
      ],
      "severity": "high",
      "userImpact": "Temporal pods fail to start and service requests for extended periods when any Cassandra node in the cluster becomes unavailable, impacting availability in containerized deployments.",
      "rootCause": "Gocql's token-aware host selection policy doesn't gracefully handle degraded clusters by falling back to available nodes quickly during startup, causing long connection timeouts.",
      "proposedFix": "Wrap the token-aware host selection policy with a single host ready policy to allow faster fallback to available nodes during startup.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Upgraded to Gocql 1.4.0 which improved connection handling for degraded clusters, included in Temporal Server v1.21.5.",
      "related": [
        1365
      ],
      "keyQuote": "When one of the hosts of our Cassandra cluster is down, newly booted temporal pods take nearly an hour to begin to service user requests successfully.",
      "number": 3638,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:49.156Z"
    },
    {
      "summary": "Cassandra schema uses Level Tiered Compaction Strategy (LCS) for all tables, but should use the default Size Tiered Compaction Strategy (STCS) instead. LCS causes significant write amplification (up to 13x) and is not suitable for general-purpose workloads.",
      "category": "other",
      "subcategory": "cassandra-schema",
      "apis": [],
      "components": [
        "cassandra",
        "schema",
        "persistence"
      ],
      "concepts": [
        "compaction-strategy",
        "write-amplification",
        "database-tuning",
        "performance-optimization"
      ],
      "severity": "medium",
      "userImpact": "Users experience unnecessary write amplification and reduced performance with Temporal's current Cassandra configuration.",
      "rootCause": "All Cassandra tables are configured with LCS, which causes significant write magnification unsuitable for general-purpose workloads.",
      "proposedFix": "Change all Cassandra tables from LevelTieredCompactionStrategy (LCS) to the default SizeTieredCompactionStrategy (STCS).",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Level compaction (LCS) can involve significant write magnification, where data is re-written multiple times. In performance testing, reports show up to 13-fold write amplification with LCS.",
      "number": 3633,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:49.422Z"
    },
    {
      "summary": "Temporal UI workflow listing fails in Kubernetes deployment with Elasticsearch error about text field data requirements. The UI cannot list workflows because Elasticsearch is misconfigured - RunId field is a text field instead of keyword field, which doesn't support the required aggregations and sorting operations.",
      "category": "bug",
      "subcategory": "ui-elasticsearch-integration",
      "apis": [],
      "components": [
        "temporal-ui",
        "elasticsearch",
        "visibility-store"
      ],
      "concepts": [
        "field-configuration",
        "elasticsearch-mapping",
        "workflow-listing",
        "kubernetes-deployment",
        "aggregation",
        "sorting"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal on Kubernetes cannot access the UI to list workflows due to Elasticsearch misconfiguration.",
      "rootCause": "RunId field is configured as a text field in Elasticsearch instead of a keyword field, which prevents field data aggregations and sorting required for workflow listing.",
      "proposedFix": "Change RunId field from text to keyword field in Elasticsearch mapping, or set fielddata=true on the RunId field.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Issue was marked as user misconfiguration rather than a product bug. Responder indicated this is an Elasticsearch configuration issue and directed user to community forum for help.",
      "related": [],
      "keyQuote": "This looks like a misconfiguration of the elastic search for visibility. Please seek help in community forum.",
      "number": 3631,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:39.179Z"
    },
    {
      "summary": "Retry logic with context timeout returns a generic context deadline exceeded error instead of the actual last error that caused the retry to fail. This masks the real error from users and makes debugging difficult.",
      "category": "bug",
      "subcategory": "retry-mechanism",
      "apis": [],
      "components": [
        "backoff",
        "retry",
        "error-handling"
      ],
      "concepts": [
        "retry",
        "timeout",
        "error-propagation",
        "context-deadline",
        "error-masking",
        "debugging"
      ],
      "severity": "medium",
      "userImpact": "Users cannot see the actual error that caused retries to fail, only a generic timeout error, making it harder to diagnose and fix issues.",
      "rootCause": "The retry logic prioritizes context deadline exceeded error over the actual last error encountered during retries.",
      "proposedFix": "Return the last error (or most prominent error) instead of timeout error when giving up on retries.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Retry with context should return the last error (or most prominent error?) instead of timeout error when giving up",
      "number": 3630,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:37.623Z"
    },
    {
      "summary": "The auto-setup container fails to connect to the Temporal server when binding to IPv6 addresses because the IPv6 address format (with brackets) is not preserved during connection setup. The brackets are needed in IPv6 addresses with ports but are being stripped, causing \"too many colons in address\" errors.",
      "category": "bug",
      "subcategory": "network-configuration",
      "apis": [],
      "components": [
        "auto-setup",
        "client-factory",
        "network-configuration"
      ],
      "concepts": [
        "ipv6",
        "address-binding",
        "connection",
        "network-setup",
        "configuration"
      ],
      "severity": "high",
      "userImpact": "Users cannot run Temporal on IPv6-enabled clusters (like AWS EKS with IPv6) because the auto-setup container cannot connect to itself.",
      "rootCause": "The auto-setup process strips bracket notation from IPv6 addresses when establishing SDK client connections, resulting in malformed addresses with too many colons for the TCP dialer.",
      "proposedFix": "Fix the client factory to properly handle IPv6 addresses by preserving or adding bracket notation when constructing connection addresses.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in pull request #4766, to be available in the next release.",
      "related": [
        3612
      ],
      "keyQuote": "For some reason when autosetup is trying to connect the '[ ]' are disappearing... address 2a05:d014:6e0:c883:1d45::11:7233: too many colons in address",
      "number": 3629,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:38.406Z"
    },
    {
      "summary": "Workflow reset is blocked when a parent workflow is closed with pending child workflows in its mutable state, even though the child workflows may have closed independently. This creates a deadlock condition preventing legitimate resets after parent termination, timeout, or failure.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [
        "ResetWorkflow"
      ],
      "components": [
        "workflow-resetter",
        "mutable-state",
        "child-workflow-management"
      ],
      "concepts": [
        "workflow-reset",
        "child-workflows",
        "deadlock",
        "workflow-closure",
        "pending-state"
      ],
      "severity": "high",
      "userImpact": "Users cannot reset closed parent workflows to any point, blocking recovery operations when legitimate resets should be allowed.",
      "rootCause": "Child workflows can close independently after parent closure, but the parent's mutable state retains them as pending indefinitely, triggering the pending child workflow check in the resetter.",
      "proposedFix": "Allow reset if the current run is already closed, as child workflows can close asynchronously after parent closure without sending close signals back to the parent.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        705,
        5030
      ],
      "keyQuote": "after parent is closed for any reasons(terminate/timeout/fail/etc), this parent is never allowed to reset to anywhere because it has a pending childWFs",
      "number": 3624,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:25.021Z"
    },
    {
      "summary": "The schedule action trigger mechanism silently fails to run multiple actions when triggered within the same second due to a 1-workflow-per-second limitation and second-resolution workflow ID generation. Users need reliable support for triggering multiple schedule actions immediately.",
      "category": "bug",
      "subcategory": "schedules",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "schedule-engine",
        "workflow-id-generation",
        "trigger-handler"
      ],
      "concepts": [
        "uniqueness",
        "workflow-id-collision",
        "schedule-actions",
        "timestamp-resolution",
        "duplicate-detection"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably trigger multiple schedule actions in quick succession, as some triggers fail silently without error indication.",
      "rootCause": "Workflow IDs are generated at second-resolution timestamps, limiting only one workflow per second. When multiple triggers occur within the same second, subsequent ones fail silently because Temporal cannot have duplicate workflow IDs running simultaneously.",
      "proposedFix": "Modify server-side workflow ID generation to append a unique counter suffix (starting at -2) when duplicate IDs are detected, allowing multiple actions in the same second without requiring external uniqueness tracking.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We only allow 1 workflow per second and we can't fail a trigger is within that. Ideally we should/could change workflow ID to have a unique value put on the end if it duplicates",
      "number": 3614,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:26.892Z"
    },
    {
      "summary": "The Slack invite link provided in the repository documentation or community resources was not active or functional, preventing users from joining the Temporal community Slack workspace.",
      "category": "other",
      "subcategory": "community-infrastructure",
      "apis": [],
      "components": [
        "community-resources",
        "documentation",
        "slack-integration"
      ],
      "concepts": [
        "community-engagement",
        "onboarding",
        "communication-channels",
        "external-links",
        "slack-workspace"
      ],
      "severity": "low",
      "userImpact": "Users attempting to join the Temporal Slack community were unable to access the workspace through outdated or inactive invite links.",
      "rootCause": "Slack invite link had expired or was incorrectly configured",
      "proposedFix": "Provide an updated working Slack invite link: https://temporalio.slack.com/join/shared_invite/zt-1i173003n-E2J69y3nlXEJIMpkAb54bg#/shared-invite/email",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Updated Slack invite link was provided in the response comment",
      "related": [],
      "keyQuote": "go to https://temporal.io/ and click join Slack or click below link: https://temporalio.slack.com/join/shared_invite/zt-1i173003n-E2J69y3nlXEJIMpkAb54bg#/shared-invite/email",
      "number": 3613,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:25.490Z"
    },
    {
      "summary": "The Temporal server fails to bind to IPv6 addresses when using the BIND_ON_IP environment variable. The auto-setup Docker image does not properly format IPv6 addresses with brackets, causing connection errors with 'too many colons in address'.",
      "category": "bug",
      "subcategory": "ipv6-support",
      "apis": [],
      "components": [
        "server",
        "auto-setup",
        "network-binding"
      ],
      "concepts": [
        "ipv6",
        "address-formatting",
        "socket-binding",
        "docker-configuration",
        "environment-variable"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy Temporal server on IPv6-only Kubernetes clusters (AWS EKS IPv6) because the auto-setup Docker image fails to properly format and bind to IPv6 addresses.",
      "rootCause": "The auto-setup Docker entrypoint does not wrap IPv6 addresses in brackets when setting the BIND_ON_IP environment variable. Go's net.SplitHostPort requires IPv6 addresses to be formatted as '[address]:port' to parse correctly.",
      "proposedFix": "Modify the auto-setup Docker entrypoint to detect IPv6 addresses and automatically wrap them in brackets before passing to the server configuration.",
      "workaround": "Users must manually wrap the IPv6 address in brackets and ensure the auto-setup script doesn't strip them during configuration processing.",
      "resolution": "fixed",
      "resolutionDetails": "The issue was resolved by updating the server configuration and auto-setup Docker image to properly handle IPv6 address formatting with bracket notation as required by Go's standard library.",
      "related": [],
      "keyQuote": "You need to surround host with brackets: '[2a05:d014:6e0:c883:1d45::11]:7233' works just fine.",
      "number": 3612,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:14.223Z"
    },
    {
      "summary": "Cancelling a workflow is non-deterministic and can take a long time or never complete. Users need a way to check if a cancel request has been created for a workflow via the DescribeWorkflowExecution API.",
      "category": "feature",
      "subcategory": "workflow-cancellation",
      "apis": [
        "DescribeWorkflowExecution"
      ],
      "components": [
        "api",
        "workflow-execution",
        "status"
      ],
      "concepts": [
        "cancellation",
        "non-determinism",
        "status-tracking",
        "workflow-lifecycle"
      ],
      "severity": "high",
      "userImpact": "Users cannot track whether a cancellation request was accepted when cancelling a workflow, making it difficult to implement reliable UI feedback or monitoring.",
      "rootCause": null,
      "proposedFix": "Add a new value to the workflowExecutionInfo.status field that reflects whether a cancel request has been created",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Cancelling a workflow is a non-deterministic operation that could take quite a while to complete - or possibly never complete at all",
      "number": 3597,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:13.366Z"
    },
    {
      "summary": "The GRPC_SSL_CIPHER_SUITES environment variable is not being respected by the Temporal server. When set, the server should use only the specified ciphers, but instead it continues to use default ciphers including weak and vulnerable ones.",
      "category": "bug",
      "subcategory": "grpc-tls-configuration",
      "apis": [],
      "components": [
        "grpc-server",
        "tls-configuration",
        "cipher-suite-handling"
      ],
      "concepts": [
        "ssl-tls",
        "cipher-suite",
        "security",
        "environment-variable",
        "configuration",
        "cryptography"
      ],
      "severity": "high",
      "userImpact": "Users cannot restrict the server to use only secure cipher suites, leaving their deployments vulnerable to weak cipher attacks.",
      "rootCause": "The GRPC_SSL_CIPHER_SUITES environment variable is not being parsed or applied to the gRPC server's TLS configuration.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When `GRPC_SSL_CIPHER_SUITES` environment variable is set, those are the only ciphers the server will use. The default ciphers are still used",
      "number": 3590,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:14.687Z"
    },
    {
      "summary": "Admin DeleteWorkflowExecution fails to remove visibility store entries when mutable state is missing, even though visibility data still exists. The feature request asks for visibility deletion to be attempted regardless of mutable state availability.",
      "category": "bug",
      "subcategory": "admin-operations",
      "apis": [
        "DeleteWorkflowExecution"
      ],
      "components": [
        "admin-handler",
        "visibility-store",
        "mutable-state"
      ],
      "concepts": [
        "data-consistency",
        "error-recovery",
        "cleanup",
        "visibility-persistence",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Users cannot fully clean up workflow execution data via Admin API when internal state becomes corrupted, leaving orphaned visibility entries.",
      "rootCause": "Admin delete logic skips visibility deletion when mutable state cannot be loaded, treating it as a failure condition rather than attempting partial cleanup.",
      "proposedFix": "Allow visibility deletion to proceed independently of mutable state availability, with fallback behavior for visibility stores that require mutable state data.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation improved to attempt visibility deletion even when mutable state is missing, with appropriate handling for visibility stores like Cassandra that require StartTime or CloseTime.",
      "related": [],
      "keyQuote": "If visibility store is Cassandra, it is not possible to delete visibility without mutable state due to it rely on StartTime or Close time",
      "number": 3583,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:00.817Z"
    },
    {
      "summary": "Request to extend the batch API to accept a list of workflow executions as input instead of just a visibility query, allowing users to apply bulk actions to a pre-determined set of workflows with a configurable size limit.",
      "category": "feature",
      "subcategory": "batch-api",
      "apis": [],
      "components": [
        "batch-api",
        "visibility-query"
      ],
      "concepts": [
        "batch-operations",
        "workflow-selection",
        "api-flexibility",
        "input-options"
      ],
      "severity": "low",
      "userImpact": "Users with pre-identified workflow execution lists could apply bulk actions directly without needing to construct visibility queries.",
      "rootCause": null,
      "proposedFix": "Add alternative input mode to batch API accepting a list of workflow executions (e.g., max 1000 items) alongside existing query-based approach.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to allow batch API to accept workflow execution lists as input.",
      "related": [],
      "keyQuote": "User should be able to specify either a query OR a list of workflow executions, but not both.",
      "number": 3572,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:02.955Z"
    },
    {
      "summary": "User requests upgrade of OpenTelemetry Go dependencies from v0.31.0 to v0.32.0+ to access the new MeterProvider API and resolve dependency conflicts with projects using newer OTel versions.",
      "category": "feature",
      "subcategory": "observability",
      "apis": [],
      "components": [
        "otel-integration",
        "metrics",
        "dependencies"
      ],
      "concepts": [
        "opentelemetry",
        "dependency-management",
        "metrics",
        "sdk-compatibility",
        "deprecation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use newer OpenTelemetry SDK versions in their projects when importing Temporal due to dependency conflicts.",
      "rootCause": "Temporal server uses outdated OpenTelemetry Go SDK v0.31.0 which conflicts with newer versions that deprecate certain APIs and folders.",
      "proposedFix": "Upgrade OTel dependencies to v0.32.0 or v0.33.0, or keep them in sync with upstream OTel releases.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Temporal server v1.20 upgraded OpenTelemetry to v0.34 in its release.",
      "related": [
        2321
      ],
      "keyQuote": "I will try to have the upgrade in the next server release 1.20... Temporal server upgrade otel to 0.34 with the latest release 1.20.",
      "number": 3556,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:44:01.604Z"
    },
    {
      "summary": "Helm chart installation for Temporal fails when deployed via Terraform, with pods stuck in Init:2/4 state during initialization. Manual Helm installation works, but Terraform deployment does not complete successfully.",
      "category": "bug",
      "subcategory": "deployment-kubernetes",
      "apis": [],
      "components": [
        "helm-chart",
        "kubernetes",
        "cassandra",
        "init-containers"
      ],
      "concepts": [
        "deployment",
        "terraform",
        "initialization",
        "job-timeout",
        "helm-release"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy Temporal via Terraform and Helm automation, forcing manual workarounds or disabling safety checks.",
      "rootCause": "Helm chart initialization jobs fail to complete when deployed through Terraform, possibly due to timing or job configuration differences between manual and automated deployments.",
      "proposedFix": "Configure helm_release resource with wait_for_jobs = true and appropriate timeout values (e.g., 300 seconds)",
      "workaround": "Set wait = false in the helm_release Terraform resource to skip waiting for pod initialization and avoid blocking failures",
      "resolution": "invalid",
      "resolutionDetails": "Issue was resolved by user through workaround. Root cause was determined to be Terraform/Helm configuration expectations rather than Temporal chart defect.",
      "related": [],
      "keyQuote": "use \"wait = false\" in the helm_release resource to avoid blocking and failing temporal instalation",
      "number": 3550,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:49.476Z"
    },
    {
      "summary": "Workflow execution updates fail with DBRecordVersion mismatches when workflow history cache is disabled, causing workflows to stall. This occurs at scale (5k workflows) when using disabled history cache, with error messages showing version conflicts in lock checking.",
      "category": "bug",
      "subcategory": "workflow-cache",
      "apis": [],
      "components": [
        "history-service",
        "workflow-execution-store",
        "lock-mechanism",
        "transaction-handling"
      ],
      "concepts": [
        "caching",
        "concurrency-control",
        "version-mismatch",
        "database-locking",
        "workflow-stalling",
        "memory-optimization"
      ],
      "severity": "high",
      "userImpact": "Users cannot run workflows with disabled history cache as execution updates fail with version conflicts, causing complete workflow stalling at scale.",
      "rootCause": "DBRecordVersion check fails when history cache is disabled, likely due to concurrent updates not being properly serialized or version tracking being incorrect without cache buffer.",
      "proposedFix": null,
      "workaround": "Reduce cache size to a small number (e.g., 32) instead of completely disabling it (set to 0).",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by recommending cache size reduction rather than complete disabling, avoiding the version conflict scenario entirely.",
      "related": [],
      "keyQuote": "lockAndCheckExecution failed. DBRecordVersion expected: 7045, actually 7046",
      "number": 3545,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:48.777Z"
    },
    {
      "summary": "The zap logging framework can consume unbounded memory due to naive sync.Pool usage with variable-sized buffers. In Temporal, this causes excessive memory consumption (up to 2.5GB) when logging patterns alternate between small and large lines, following a known anti-pattern in Go's sync.Pool design.",
      "category": "bug",
      "subcategory": "logging-memory-management",
      "apis": [],
      "components": [
        "zap-logger",
        "memory-pool",
        "logging-framework"
      ],
      "concepts": [
        "memory-management",
        "sync.Pool",
        "buffer-pooling",
        "logging-overhead",
        "resource-efficiency"
      ],
      "severity": "high",
      "userImpact": "Users experience unexpectedly high memory consumption in Temporal deployments due to zap logger buffer pools not being released properly.",
      "rootCause": "Zap follows anti-pattern of storing variable-sized buffers in sync.Pool without size bounds, causing large buffers allocated for occasional large logs to persist in the pool indefinitely.",
      "proposedFix": "Implement one of several solutions: deduplicate logs with long lines like stack traces, commit suggestions from Go issue #23199 to zap (don't return large buffers to pool, use tiered pools for small vs large, implement upper bound on pool size), or implement pool size limits.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This pattern leads to an unnecessarily large zap buffer pool (in practice, I've seen up to 2.5gb).",
      "number": 3543,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:50.716Z"
    },
    {
      "summary": "Request to export ScopeDefinition factory methods or make the type exportable so external projects (like a Temporal Cloud proxy) can extend the metrics library with custom scope definitions. The issue was closed as the metrics.Client was deprecated in favor of the simpler metrics.MetricsHandler interface.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [
        "MetricsHandler"
      ],
      "components": [
        "metrics",
        "metrics-library",
        "scope-definition"
      ],
      "concepts": [
        "metrics",
        "extension",
        "factory-methods",
        "scope-definitions",
        "metrics-handler",
        "backwards-compatibility"
      ],
      "severity": "low",
      "userImpact": "External projects cannot extend the metrics library with custom scope definitions due to unexported types, limiting metrics customization and consistency.",
      "rootCause": "scopeDefinition was unexported (private) in the metrics package, preventing external extension while factory methods existed for other aspects.",
      "proposedFix": "Create scopeDefinition factory methods or export ScopeDefinition type to allow external extension of scope definitions.",
      "workaround": "Use a separate metrics client instead of extending the built-in metrics library.",
      "resolution": "fixed",
      "resolutionDetails": "metrics.Client was deprecated in favor of metrics.MetricsHandler which has a much simpler interface that addresses the need.",
      "related": [],
      "keyQuote": "Create `scopeDefinition` factory methods or export `ScopeDefinition` for extension. This will allow me to extend the existing scope definitions and metrics for new uses.",
      "number": 3538,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:37.242Z"
    },
    {
      "summary": "Admin DeleteWorkflowExecution API should support deletion by namespace ID in addition to namespace name, to allow cleanup of workflow data after a namespace has been deleted.",
      "category": "feature",
      "subcategory": "admin-api",
      "apis": [
        "DeleteWorkflowExecution"
      ],
      "components": [
        "admin-api",
        "namespace-management",
        "data-cleanup"
      ],
      "concepts": [
        "namespace-deletion",
        "orphaned-data",
        "cleanup",
        "workflow-execution",
        "data-retention"
      ],
      "severity": "medium",
      "userImpact": "Users cannot clean up workflow execution data after a namespace is deleted, leaving orphaned data in the system.",
      "rootCause": null,
      "proposedFix": "Modify Admin DeleteWorkflowExecution to accept namespace ID as an alternative parameter to namespace name",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Admin DeleteWorkflowExecution currently takes in namespace. After a namespace is deleted, we don't have a way to delete data if there is leftover data.",
      "number": 3536,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:37.003Z"
    },
    {
      "summary": "Remove unused methods from ClientBean and add GetAdminClient method to align with Frontend Client API. This is a code cleanup and API consistency improvement for the temporal server's dependency injection layer.",
      "category": "other",
      "subcategory": "code-cleanup",
      "apis": [],
      "components": [
        "ClientBean",
        "fx-injection",
        "admin-client"
      ],
      "concepts": [
        "dependency-injection",
        "code-cleanup",
        "api-alignment",
        "unused-methods",
        "refactoring"
      ],
      "severity": "low",
      "userImpact": "This change improves code maintainability and API consistency for developers using the Temporal server's dependency injection framework.",
      "rootCause": null,
      "proposedFix": "Remove unused methods from ClientBean and add GetAdminClient method. One commenter suggested replacing the entire ClientBean with proper fx injections.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        5000
      ],
      "keyQuote": "I would remove entire `ClientBean` and replace it with proper `fx` injections.",
      "number": 3532,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:37.995Z"
    },
    {
      "summary": "System worker fails to connect to Frontend service in TLS-enabled clusters after removing publicClient configuration. The issue appears to be caused by service discovery returning an IP address while TLS certificate validation fails because the certificate's DNS name doesn't match the IP.",
      "category": "bug",
      "subcategory": "tls-connectivity",
      "apis": [],
      "components": [
        "worker",
        "service-discovery",
        "tls-client"
      ],
      "concepts": [
        "tls",
        "certificate-validation",
        "service-discovery",
        "dns",
        "connection",
        "cluster-configuration"
      ],
      "severity": "high",
      "userImpact": "Users cannot run system workers in TLS-enabled clusters after removing publicClient settings due to connection failures.",
      "rootCause": "Service discovery returns IP addresses while TLS certificate validation expects DNS names that match the certificate's Subject Alternative Names (SANs), causing validation to fail.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "System worker should be able to connect to a Frontend service resolved by a new service discovery when a publicClient configuration is removed and a Temporal cluster is TLS enabled",
      "number": 3527,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:24.057Z"
    },
    {
      "summary": "User requests ability to use a single MySQL database for both default and visibility schemas instead of requiring two separate databases. Response clarifies that two configurations are needed but can point to the same database instance.",
      "category": "question",
      "subcategory": "database-configuration",
      "apis": [],
      "components": [
        "database",
        "visibility-store",
        "schema-management"
      ],
      "concepts": [
        "database-consolidation",
        "configuration",
        "schema-separation",
        "visibility",
        "elasticsearch"
      ],
      "severity": "low",
      "userImpact": "Users seeking simplified database setup may be confused about whether multiple database instances are required.",
      "rootCause": null,
      "proposedFix": "Configuration documentation clarifying that the same database instance can be used for both default and visibility schemas",
      "workaround": "Point both default and visibility configurations to the same database instance",
      "resolution": "invalid",
      "resolutionDetails": "The issue was based on a misunderstanding. Temporal requires two separate schema configurations but they can reference the same database instance, not two separate instances.",
      "related": [],
      "keyQuote": "You still need 2 configuration for default and visibility, but they can point to the same database instance.",
      "number": 3522,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:23.669Z"
    },
    {
      "summary": "Activity failures in a retry sequence are not accessible - only the last failure can be viewed in the SDK and UI. Users need access to the complete failure history of retried activities for better debugging and error understanding.",
      "category": "feature",
      "subcategory": "activity-retry-failure-history",
      "apis": [
        "PollActivityTaskQueueResponse"
      ],
      "components": [
        "activity-executor",
        "worker",
        "retry-policy",
        "failure-handling"
      ],
      "concepts": [
        "retry",
        "failure-history",
        "debugging",
        "activity-execution",
        "error-tracking",
        "last-failure",
        "failure-context"
      ],
      "severity": "medium",
      "userImpact": "Users cannot debug activity failures effectively when retries occur, as they can only see the final failure and lose context from earlier attempts.",
      "rootCause": "The activity task queue response and activity context do not include previous failure information, only the current/last failure state.",
      "proposedFix": "Return a list of all previous Failures in PollActivityTaskQueueResponse or make failure history available in Activity Context so workers can access it.",
      "workaround": "Activity can use a Client to describe the workflow and check pendingActivities.lastFailure, though this only provides the last failure, not full history.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        92
      ],
      "keyQuote": "When an activity retries more than one time, there is no way right now to access previous failures.",
      "number": 3517,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:26.431Z"
    },
    {
      "summary": "CVE-2022-32149 in golang.org/x/text v0.3.7 allows denial of service through crafted Accept-Language headers. This is a transitive dependency of ringpop-go with CVSS score 7.5 (High severity).",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "ringpop-go",
        "golang.org/x/text",
        "golang.org/x/net"
      ],
      "concepts": [
        "vulnerability",
        "denial-of-service",
        "dependency-management",
        "security",
        "cvss-score",
        "transitive-dependency",
        "header-parsing"
      ],
      "severity": "high",
      "userImpact": "Users of Temporal are potentially vulnerable to denial of service attacks through malicious Accept-Language headers.",
      "rootCause": "Inefficient parsing logic in golang.org/x/text ParseAcceptLanguage function allows attackers to craft headers that consume significant parsing time.",
      "proposedFix": "Upgrade golang.org/x/text from v0.3.7 to v0.3.8 or later.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was automatically closed by Mend as the vulnerable library was either marked as ignored or is no longer part of the Mend inventory.",
      "related": [],
      "keyQuote": "An attacker may cause a denial of service by crafting an Accept-Language header which ParseAcceptLanguage will take significant time to parse.",
      "number": 3514,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:11.204Z"
    },
    {
      "summary": "Request to add native support for Yugabyte YCQL backend as an alternative to Cassandra, allowing users to leverage a single YugabyteDB cluster instead of maintaining separate Cassandra and YB clusters.",
      "category": "feature",
      "subcategory": "database-backend",
      "apis": [],
      "components": [
        "persistence",
        "cassandra-driver",
        "database-layer"
      ],
      "concepts": [
        "database-backend",
        "cassandra-compatibility",
        "ycql",
        "multi-database-support",
        "cluster-management"
      ],
      "severity": "medium",
      "userImpact": "Users currently using both YugabyteDB and Temporal could consolidate to a single database platform without maintaining separate clusters.",
      "rootCause": null,
      "proposedFix": "Implement native support for Yugabyte YCQL variant of its Cassandra driver in the persistence layer",
      "workaround": "Use YugabyteDB YSQL backend (though performance may require custom configuration tuning)",
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed without implementation. While identified as relatively straightforward, no upstream contribution materialized.",
      "related": [
        26322
      ],
      "keyQuote": "We currently leverage both Yugabyte YCQL and Temporal and would love to see native support for Temporal to use the YCQL variant",
      "number": 3511,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:13.326Z"
    },
    {
      "summary": "User requests a feature to group workers and tag workflow executions so that workflows with specific tags run on specific worker groups, enabling geographic distribution, latency optimization, and access control.",
      "category": "feature",
      "subcategory": "worker-routing",
      "apis": [],
      "components": [
        "worker",
        "task-queue",
        "workflow-execution"
      ],
      "concepts": [
        "worker-grouping",
        "task-routing",
        "geographic-distribution",
        "latency-optimization",
        "access-control",
        "worker-tagging"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently route specific workflow executions to specific worker groups based on geographic location, hardware requirements, or access control needs.",
      "rootCause": null,
      "proposedFix": "Add feature to group workers and tag workflow executions that can be matched with worker groups for intelligent routing.",
      "workaround": "Use different task queues for each group of workers.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer suggested using existing task queue feature as a workaround to meet the user's needs.",
      "related": [],
      "keyQuote": "A feature to group or tag workers, so that they're distinguishable from other workers, and a way to set a tag or class id on a specific workflow execution",
      "number": 3506,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:12.045Z"
    },
    {
      "summary": "Request for dynamic rate limiting on task queues to act as a circuit breaker, scaling back task consumption when downstream services are struggling, without requiring worker restarts.",
      "category": "feature",
      "subcategory": "task-queue-rate-limiting",
      "apis": [],
      "components": [
        "task-queue",
        "worker",
        "rate-limiting"
      ],
      "concepts": [
        "circuit-breaker",
        "rate-limiting",
        "dynamic-configuration",
        "backoff",
        "downstream-resilience",
        "retry-strategy"
      ],
      "severity": "medium",
      "userImpact": "Users need the ability to dynamically adjust task queue rate limits to handle downstream service failures without restarting workers.",
      "rootCause": null,
      "proposedFix": "Implement dynamic rate limiting similar to issue #3288, allowing rate limit changes without worker restarts. Consider supporting dynamic backoff adjustments for individual activity retries via custom error metadata.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        3288
      ],
      "keyQuote": "Ability to be able to change the rate limit with changing having to restart the worker",
      "number": 3503,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:00.792Z"
    },
    {
      "summary": "Request for a metric to track the number of open Workflow Executions in Temporal. Users need visibility into workflow backlog to understand system capacity and processing load, especially during traffic bursts.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "visibility",
        "server"
      ],
      "concepts": [
        "monitoring",
        "observability",
        "workflow-backlog",
        "capacity-planning",
        "latency-visibility"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily monitor workflow execution backlog and system capacity pressure without manually querying visibility.",
      "rootCause": null,
      "proposedFix": "Add a server-side metric that emits the count of open workflows, available in both self-hosted and Cloud deployments.",
      "workaround": "Query the visibility layer periodically to count open workflows and emit custom metrics.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We often have bursts of workflows and want to know how far behind we are in processing them.",
      "number": 3502,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:59.286Z"
    },
    {
      "summary": "Upgrade the Temporal server codebase to use Go 1.19. This involves updating the base Docker images used for building and running the server.",
      "category": "feature",
      "subcategory": "build-infrastructure",
      "apis": [],
      "components": [
        "build-system",
        "docker-images",
        "ci-builder"
      ],
      "concepts": [
        "golang-version",
        "build-tools",
        "container-images",
        "dependency-management"
      ],
      "severity": "low",
      "userImpact": "Users will benefit from performance improvements and language features in Go 1.19, with updated base images for deployments.",
      "rootCause": null,
      "proposedFix": "Update Go version to 1.19 and release new base images (base-builder, base-server, base-admin-tools, base-ci-builder)",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "New base images were released (base-builder:1.10.0, base-server:1.11.0, base-admin-tools:1.7.0, base-ci-builder:1.7.0) with Go 1.19 support",
      "related": [],
      "keyQuote": "Released new base images: temporalio/base-builder:1.10.0, temporalio/base-server:1.11.0",
      "number": 3498,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:43:01.199Z"
    },
    {
      "summary": "Request for clearer error messages when List Filters contain invalid syntax, specifically when string values are missing quotes. Currently, unquoted values produce opaque errors instead of helpful guidance about required quoting.",
      "category": "other",
      "subcategory": "list-filters",
      "apis": [],
      "components": [
        "list-filters",
        "error-messages",
        "query-parser"
      ],
      "concepts": [
        "error-handling",
        "user-experience",
        "input-validation",
        "string-formatting",
        "filter-syntax"
      ],
      "severity": "low",
      "userImpact": "Users receive confusing error messages when forgetting quotes in List Filter values, making it difficult to diagnose and fix filter syntax errors.",
      "rootCause": "Error message does not clearly indicate that unquoted values are interpreted differently or explain the syntax requirement",
      "proposedFix": "Return specific error message like 'Invalid List Filter: the right side of `ExecutionStatus = Running` should be a string (needs quotes around it).'",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Invalid List Filter: the right side of `ExecutionStatus = Running` should be a string (needs quotes around it).",
      "number": 3494,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:47.040Z"
    },
    {
      "summary": "Request to add batch reset functionality to the service batch API, allowing users to reset multiple workflows in a single batch operation.",
      "category": "feature",
      "subcategory": "batch-operations",
      "apis": [
        "ResetWorkflowExecution"
      ],
      "components": [
        "batch-api",
        "workflow-reset",
        "service"
      ],
      "concepts": [
        "batch-processing",
        "workflow-reset",
        "api-extension",
        "bulk-operations"
      ],
      "severity": "medium",
      "userImpact": "Users need a more efficient way to reset multiple workflows without making individual API calls.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Batch reset functionality was implemented in the service batch API",
      "related": [],
      "keyQuote": "Support batch reset in service batch API",
      "number": 3481,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:46.748Z"
    },
    {
      "summary": "Request to add a ListTaskQueues API to list all active task queues in a given namespace for diagnosing misconfigured or incorrect task queue assignments that result in workflows being routed to queues without workers.",
      "category": "feature",
      "subcategory": "task-queue-management",
      "apis": [],
      "components": [
        "matching-service",
        "task-queue",
        "namespace"
      ],
      "concepts": [
        "task-queue",
        "diagnostics",
        "monitoring",
        "active-queues",
        "workers",
        "fault-finding"
      ],
      "severity": "medium",
      "userImpact": "Users cannot diagnose task queue misconfiguration issues or identify workflows being sent to queues without active workers listening.",
      "rootCause": null,
      "proposedFix": "Add a ListTaskQueues API to list active task queues (those with new tasks or pollers in the last 5 minutes) loaded in the matching service.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1797
      ],
      "keyQuote": "The ability to list all active task queues could be very useful to help diagnose these problems.",
      "number": 3468,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:48.182Z"
    },
    {
      "summary": "Request for a batch operation API to delete workflows that have exceeded their retention period. This is needed when retention policies are reduced after workflows have already closed, requiring bulk deletion of workflows that should no longer be retained.",
      "category": "feature",
      "subcategory": "workflow-deletion",
      "apis": [],
      "components": [
        "workflow-store",
        "retention-policy",
        "batch-operations"
      ],
      "concepts": [
        "retention",
        "deletion",
        "batch-operations",
        "workflow-lifecycle",
        "data-cleanup"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently delete workflows that exceed updated (shorter) retention policies, requiring manual cleanup or waiting for natural expiration.",
      "rootCause": null,
      "proposedFix": "Implement a batch operation API to support deleting workflows based on retention criteria.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Batch operation to support delete would be ideal.",
      "number": 3461,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:34.611Z"
    },
    {
      "summary": "CVE-2022-40674 (critical vulnerability in expat library) detected in admin-tools container images v1.18.0 and v1.18.1. The vulnerability requires upgrading the expat package from version 2.4.8 to 2.4.9, which necessitates a new server release.",
      "category": "bug",
      "subcategory": "security-vulnerability",
      "apis": [],
      "components": [
        "admin-tools",
        "docker-image",
        "dependencies"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "cve",
        "container",
        "package-management",
        "dependencies",
        "supply-chain"
      ],
      "severity": "critical",
      "userImpact": "Users running admin-tools containers in CVE-scanning environments face critical security alerts and potential deployment blocks due to unpatched vulnerabilities.",
      "rootCause": "expat library version 2.4.8 contains CVE-2022-40674; fixed in 2.4.9",
      "proposedFix": "Upgrade expat package to version 2.4.9 and release new admin-tools versions",
      "workaround": "Run `apk upgrade` to pull in the fixed package",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by releasing new admin-tools versions with updated expat dependency",
      "related": [],
      "keyQuote": "admin-tools image requires a new server release",
      "number": 3459,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:35.974Z"
    },
    {
      "summary": "Range queries (using >= and < operators) fail with SQLite driver in Temporalite, returning an error that these operators are not allowed in comparison expressions. The issue is resolved once advanced visibility API support is added to SQL databases.",
      "category": "bug",
      "subcategory": "visibility-queries",
      "apis": [],
      "components": [
        "temporalite",
        "sqlite-driver",
        "visibility-api",
        "query-engine"
      ],
      "concepts": [
        "range-queries",
        "filtering",
        "visibility",
        "sql-support",
        "operator-support"
      ],
      "severity": "medium",
      "userImpact": "Users cannot perform range-based workflow queries with Temporalite using SQLite backend, limiting ability to filter workflows by ID ranges.",
      "rootCause": "Advanced visibility API support was not implemented for SQL databases at the time of the issue; the SQLite driver did not support >= and < operators in comparison expressions.",
      "proposedFix": "Implement advanced visibility API support on SQL databases to enable range query operators.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by implementing advanced visibility API support on SQL databases, which added support for range query operators like >= and <.",
      "related": [],
      "keyQuote": "This will be fixed when the advanced visibility API is supported on SQL. We are actively working on that.",
      "number": 3454,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:35.872Z"
    },
    {
      "summary": "Docker images use numeric UIDs/GIDs that vary between containers, making Kubernetes security policies harder to configure. Users must explicitly specify runAsUser/runAsGroup instead of relying on image metadata.",
      "category": "bug",
      "subcategory": "docker-security",
      "apis": [],
      "components": [
        "docker-images",
        "kubernetes-deployment",
        "security-context"
      ],
      "concepts": [
        "container-security",
        "uid-gid",
        "kubernetes-integration",
        "privilege-escalation",
        "security-policies",
        "non-root-user"
      ],
      "severity": "medium",
      "userImpact": "Users deploying in restricted Kubernetes environments must manually configure additional security context settings instead of relying on consistent numeric UIDs/GIDs in images.",
      "rootCause": "Docker images use inconsistent numeric UIDs/GIDs (web container uses 5000, others use different values) or username-based UIDs that Kubernetes cannot automatically detect.",
      "proposedFix": "Use the same integer UID/GID across all Temporal docker images for consistent Kubernetes security policy detection.",
      "workaround": "Manually define securityContext in Kubernetes manifests with explicit runAsUser and runAsGroup values (e.g., 1000).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If the UIDs were not 0, I could leave off at least the last two items. Also, the web container uses 5000 for the temporal user... which is likely an unneeded difference.",
      "number": 3453,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:23.859Z"
    },
    {
      "summary": "User encountered a context deadline exceeded error with command ID generation in the Go SDK. The error appears related to attempting to generate a command before processing WorkflowTaskStarted event, possibly triggered by querying cron workflows.",
      "category": "bug",
      "subcategory": "command-generation",
      "apis": [],
      "components": [
        "commandsHelper",
        "workflow-task-processor",
        "cron-workflow"
      ],
      "concepts": [
        "context-deadline",
        "command-generation",
        "workflow-events",
        "error-handling",
        "query-execution"
      ],
      "severity": "medium",
      "userImpact": "Users may experience runtime errors when querying cron workflows, preventing normal workflow execution.",
      "rootCause": "Known bug triggered when querying cron workflows, caused by attempting to generate commands before the WorkflowTaskStarted event is processed.",
      "proposedFix": "Fixed in PR #2826 and released since v1.17.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Bug was fixed in PR #2826 and released in v1.17 or later.",
      "related": [
        2826
      ],
      "keyQuote": "Error: Attempt to generate a command before processing WorkflowTaskStarted event. Is the error triggered by a query? There is a known bug that could cause this error when query cron workflow.",
      "number": 3446,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:22.924Z"
    },
    {
      "summary": "User requests ability to reclaim and reuse workflow IDs after explicitly marking them as no longer needed, to support workflows that create and destroy related resources while maintaining idempotence guarantees.",
      "category": "feature",
      "subcategory": "workflow-id-reuse-policy",
      "apis": [
        "StartWorkflowOptions",
        "WorkflowIDReusePolicy"
      ],
      "components": [
        "workflow-client",
        "workflow-id-manager",
        "reuse-policy-engine"
      ],
      "concepts": [
        "idempotence",
        "workflow-id-reuse",
        "resource-lifecycle",
        "duplicate-detection",
        "state-management"
      ],
      "severity": "low",
      "userImpact": "Users cannot safely reuse workflow IDs for new operations after completing resource deletion without risking conflicts or losing idempotence protection.",
      "rootCause": null,
      "proposedFix": "Add a mechanism to explicitly mark/taint workflow IDs as available for reuse after their associated resources are deleted.",
      "workaround": "Implement idempotence protection in application layers outside of Temporal.",
      "resolution": "invalid",
      "resolutionDetails": "Maintainer suggested using WORKFLOW_ID_REUSE_POLICY_ALLOW_DUPLICATE combined with idempotent RequestID as the proper solution.",
      "related": [],
      "keyQuote": "You should be able to achieve what you want with WORKFLOW_ID_REUSE_POLICY_ALLOW_DUPLICATE and an idempotent RequestID.",
      "number": 3441,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:22.159Z"
    },
    {
      "summary": "When a child workflow is reset, the parent workflow incorrectly registers a ChildWorkflowExecutionTerminated event instead of understanding the reset occurred. This causes the parent workflow to exit prematurely, leaving the child workflow orphaned.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-engine",
        "child-workflow-execution",
        "event-history"
      ],
      "concepts": [
        "workflow-reset",
        "parent-child-relationship",
        "event-handling",
        "workflow-termination",
        "orphaned-workflows"
      ],
      "severity": "high",
      "userImpact": "Users cannot safely reset child workflows without causing parent workflows to terminate unexpectedly and abandon running child workflows.",
      "rootCause": "The parent workflow's event processing does not distinguish between actual child workflow termination and reset operations, treating resets as terminations.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in PR #2913, available in v1.17",
      "related": [
        2913
      ],
      "keyQuote": "When I reset a child workflow, the parent workflow should understand that the child workflow was reset to a certain event instead of registering a ChildWorkflowExecutionTerminated event.",
      "number": 3437,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:11.209Z"
    },
    {
      "summary": "Integration tests currently cannot verify that specific metrics are emitted as expected. The task is to build infrastructure for metric verification and add integration tests for key metrics to prevent regressions like the disappearance of 'action' metrics.",
      "category": "feature",
      "subcategory": "testing-metrics",
      "apis": [],
      "components": [
        "integration-tests",
        "metrics",
        "observability"
      ],
      "concepts": [
        "metrics-emission",
        "regression-prevention",
        "test-coverage",
        "observability",
        "integration-testing",
        "metric-verification"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably verify that expected metrics are emitted, making it harder to monitor system behavior and detect regressions in metric collection.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We need to build that ability and add integration test to cover some key metrics. For example, prevent regression that cause \"action\" metrics to disappear.",
      "number": 3436,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:11.849Z"
    },
    {
      "summary": "Request for a new worker API to signal shutdown and reset sticky task queues. This would allow workers to clean up state during graceful shutdown, particularly by clearing task queue affinity.",
      "category": "feature",
      "subcategory": "worker-lifecycle",
      "apis": [],
      "components": [
        "worker",
        "task-queue",
        "sticky-queue"
      ],
      "concepts": [
        "graceful-shutdown",
        "sticky-task-queue",
        "cleanup",
        "worker-lifecycle",
        "task-affinity",
        "state-management"
      ],
      "severity": "low",
      "userImpact": "Workers cannot properly reset sticky task queue assignments during shutdown, potentially causing task routing issues after worker restart.",
      "rootCause": null,
      "proposedFix": "Add an API method on the worker to signal shutdown and reset sticky task queue state.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Currently the main thing workers would use this API for is to reset the sticky task queue for any tasks on that queue.",
      "number": 3435,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:10.475Z"
    },
    {
      "summary": "Data race in TestWorkflowEnvironment.SignalExternalWorkflow when signal payload is modified after being sent. The test framework spawns a goroutine that concurrently accesses signal data while the workflow modifies it, causing race conditions that don't occur in production where data is serialized first.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [
        "SignalExternalWorkflow"
      ],
      "components": [
        "test-workflow-environment",
        "signal-handling",
        "mock-framework"
      ],
      "concepts": [
        "data-race",
        "concurrency",
        "signal-serialization",
        "test-suite",
        "goroutine-safety",
        "mock-verification"
      ],
      "severity": "high",
      "userImpact": "Users testing workflows that signal external workflows and modify signal payload data encounter unreliable data races in the test framework.",
      "rootCause": "TestWorkflowEnvironmentImpl.SignalExternalWorkflow spawns a goroutine to lookup the correct mock signal receiver concurrently with workflow execution, allowing concurrent access to signal data that the workflow is still modifying.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Issue was filed in wrong repository (temporalio/temporal). Correct issue is temporalio/sdk-go#922.",
      "related": [
        922
      ],
      "keyQuote": "a new go-routine is spawned, which tries to look up the correct mock of the signal receiver _concurrently_ with the rest of the workflow proceeding (and modifying the data of the signal currently being processed)",
      "number": 3430,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:59.740Z"
    },
    {
      "summary": "History scavenger functionality needs to be implemented for SQL-based persistence to clean up dangling history data, similar to the existing Cassandra-based implementation. The main blocker is implementing GetAllHistoryTreeBranches for SQL persistence.",
      "category": "feature",
      "subcategory": "history-scavenging",
      "apis": [],
      "components": [
        "persistence",
        "history-management",
        "sql-persistence"
      ],
      "concepts": [
        "data-cleanup",
        "garbage-collection",
        "dangling-data",
        "history-tree",
        "branch-management"
      ],
      "severity": "medium",
      "userImpact": "SQL-based persistence deployments cannot clean up dangling history data, leading to potential data bloat and storage inefficiency.",
      "rootCause": "GetAllHistoryTreeBranches function is not implemented for SQL persistence backend",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "History scavenger was implemented for SQL-based persistence with the missing GetAllHistoryTreeBranches functionality",
      "related": [],
      "keyQuote": "The major missing function is GetAllHistoryTreeBranches which is not implemented on SQL persistence.",
      "number": 3419,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:58.793Z"
    },
    {
      "summary": "DeleteWorkflowExecution API error messages leak internal implementation details by referencing GetWorkflowExecution, confusing users about which operation actually failed. This occurs when attempting to delete an already-deleted workflow execution.",
      "category": "bug",
      "subcategory": "api-error-handling",
      "apis": [
        "DeleteWorkflowExecution"
      ],
      "components": [
        "delete-workflow",
        "error-response",
        "grpc-service"
      ],
      "concepts": [
        "error-messages",
        "api-contract",
        "information-leakage",
        "implementation-details",
        "user-experience"
      ],
      "severity": "medium",
      "userImpact": "Users receive confusing error messages that reference internal operations they didn't call, making it harder to understand and debug workflow deletion issues.",
      "rootCause": "DeleteWorkflowExecution internally calls GetWorkflowExecution, and when that call fails, the error message exposes the internal operation name instead of the user-facing operation.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "operation GetWorkflowExecution encountered not found while we didn't call GetWorkflowExecution, we called DeleteWorkflowExecution",
      "number": 3417,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:42:00.243Z"
    },
    {
      "summary": "Security vulnerability in modernc.org/sqlite dependency (CVE-2020-28928) affecting musl libc with a buffer overflow in wcsnrtombs function. Issue was automatically closed as the vulnerable library was marked as ignored or removed from inventory.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependencies",
        "sqlite",
        "libc"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "buffer-overflow",
        "dependency-management",
        "cve",
        "transitive-dependency"
      ],
      "severity": "medium",
      "userImpact": "Users are exposed to a potential denial-of-service vulnerability through an indirect dependency, though the actual risk depends on how the vulnerable code path is used.",
      "rootCause": "musl libc wcsnrtombs function mishandles particular combinations of destination buffer size and source character limit, causing invalid memory writes.",
      "proposedFix": "Upgrade musl libc to version 1.2.2-1 or 1.1.16-3+deb9u1 to resolve the vulnerability.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was automatically closed by Mend because the vulnerable library was marked as ignored or is no longer part of the inventory.",
      "related": [
        108
      ],
      "keyQuote": "wcsnrtombs mishandles particular combinations of destination buffer size and source character limit, as demonstrated by an invalid write access (buffer overflow).",
      "number": 3410,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:48.605Z"
    },
    {
      "summary": "Docker is not running error message is confusing. When docker-compose script fails due to Docker not being started, the error message should clearly indicate 'Docker is not running' instead of a low-level FileNotFoundError about missing socket files.",
      "category": "bug",
      "subcategory": "docker-setup",
      "apis": [],
      "components": [
        "docker-compose",
        "error-handling"
      ],
      "concepts": [
        "docker",
        "setup",
        "error-messaging",
        "developer-experience",
        "dependency-check"
      ],
      "severity": "low",
      "userImpact": "Developers debugging docker-compose failures get a cryptic error message that doesn't clearly indicate Docker needs to be started.",
      "rootCause": "Error message from Docker client is not user-friendly; missing high-level check for Docker daemon availability.",
      "proposedFix": "Catch the Docker connection error and display a clear message: 'Docker is not running'",
      "workaround": "Start docker before running docker-compose script",
      "resolution": "wontfix",
      "resolutionDetails": "Marked as external tool issue - Docker error messaging is outside Temporal's control",
      "related": [],
      "keyQuote": "Error message should be docker is not running",
      "number": 3407,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:46.922Z"
    },
    {
      "summary": "Workflow signals currently trigger immediate workflow task scheduling even when not needed. Users request an optional flag to skip workflow task creation when receiving signals, allowing batching of multiple signals before processing to improve resource efficiency.",
      "category": "feature",
      "subcategory": "signal-handling",
      "apis": [
        "SignalWorkflow"
      ],
      "components": [
        "workflow-task-scheduler",
        "signal-processor",
        "server"
      ],
      "concepts": [
        "signal-batching",
        "resource-efficiency",
        "workflow-scheduling",
        "delayed-processing",
        "task-optimization"
      ],
      "severity": "medium",
      "userImpact": "Applications that receive multiple signals in quick succession waste server resources by creating unnecessary workflow tasks instead of batching signals together.",
      "rootCause": null,
      "proposedFix": "Add an optional SkipNewWorkflowTask flag to signal requests to prevent automatic workflow task scheduling",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "trigger a new workflow task on each received signal is a significant waste of resource",
      "number": 3406,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:47.838Z"
    },
    {
      "summary": "GetWorkflowExecutionHistoryReverse API was missing from readOnlyNamespaceAPI in the default authorizer, causing Temporal UI to fail when navigating workflow details with read-only permissions. This prevented users with system:read claims from accessing workflow history.",
      "category": "bug",
      "subcategory": "authorization",
      "apis": [
        "GetWorkflowExecutionHistoryReverse"
      ],
      "components": [
        "frontend-api",
        "authorizer",
        "default-authorizer"
      ],
      "concepts": [
        "authorization",
        "permissions",
        "read-only-access",
        "namespace-api",
        "workflow-history"
      ],
      "severity": "high",
      "userImpact": "Users with read-only permissions could not view workflow details in Temporal UI due to authorization failures.",
      "rootCause": "GetWorkflowExecutionHistoryReverse API was not included in the readOnlyNamespaceAPI map used by the default authorizer.",
      "proposedFix": "Add GetWorkflowExecutionHistoryReverse to the readOnlyNamespaceAPI configuration in frontend_api.go.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by commit c5ccbd5963bc280163eff2aed798b2e060b8334c, available in later server versions.",
      "related": [],
      "keyQuote": "GetWorkflowExecutionHistoryReverse is not included in the readonlyNamespaceAPI map and the permissions claim is system:read",
      "number": 3387,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:34.724Z"
    },
    {
      "summary": "Users need the ability to specify and use custom PostgreSQL schemas with Temporal Server, rather than being forced to use the default public schema. Currently, sql-tool lacks support for schema creation and configuration.",
      "category": "feature",
      "subcategory": "postgresql-schema-management",
      "apis": [],
      "components": [
        "sql-tool",
        "postgres-driver",
        "database-initialization",
        "schema-management"
      ],
      "concepts": [
        "schema-isolation",
        "search-path",
        "database-permissions",
        "multitenancy",
        "postgres-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot isolate Temporal data using PostgreSQL schemas, limiting database organization and preventing proper schema-based access control in multi-tenant or restricted-permission environments.",
      "rootCause": "The sql-tool does not support defining or creating custom schemas; it relies on default public schema and lacks schema name configuration as a property.",
      "proposedFix": "Add schema name as a configurable property in sql-tool, support CREATE SCHEMA IF NOT EXISTS statement during initialization, and optionally set search_path automatically based on configuration.",
      "workaround": "Manually create schema and set search_path via ALTER ROLE, provided Temporal doesn't hardcode references to public schema.",
      "related": [],
      "keyQuote": "Define the schema name as a property (for templating the above query) and put the CREATE statement after CREATE DATABASE with IF NOT EXISTS checks.",
      "resolution": null,
      "resolutionDetails": null,
      "number": 3383,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:36.859Z"
    },
    {
      "summary": "Current shard-level replication processes each shard independently, which can result in causal inconsistency issues after failover. The enhancement proposes making replication aware of cross-shard events and guaranteeing their ordering before applying events.",
      "category": "feature",
      "subcategory": "replication",
      "apis": [],
      "components": [
        "replication",
        "shard",
        "failover"
      ],
      "concepts": [
        "causal consistency",
        "cross-shard ordering",
        "event ordering",
        "failover",
        "replication",
        "data consistency"
      ],
      "severity": "high",
      "userImpact": "Without causal consistency guarantees in replication, users may experience data inconsistencies after failover events in distributed Temporal deployments.",
      "rootCause": "Shard-level replication operates independently without awareness of cross-shard event dependencies, leading to potential ordering violations during failover.",
      "proposedFix": "Make replication aware of all cross-shard events and guarantee their order before applying events to ensure causal consistency.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "replication need to be aware of all cross shard events, and guarantee their order before apply events",
      "number": 3381,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:36.375Z"
    },
    {
      "summary": "Request to support delayed start of workflows to enable one-off timer functionality without requiring a separate sleep workflow step. Users could start a workflow with a delay instead of the current two-step process.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-engine",
        "scheduler",
        "workflow-runtime"
      ],
      "concepts": [
        "delayed-start",
        "timer",
        "workflow-initialization",
        "scheduling",
        "one-off-timers",
        "workflow-control"
      ],
      "severity": "low",
      "userImpact": "Users could simplify timer workflows by starting workflows with built-in delays instead of combining workflow start with sleep operations.",
      "rootCause": null,
      "proposedFix": "Support a delay parameter or mechanism in StartWorkflow API to schedule workflow execution after a specified duration.",
      "workaround": "Current workaround is two-step process: start workflow and then use Sleep within the workflow logic.",
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "users who want just one off timers who instead of 2 step 1)start WF 2) Sleep workflow could achieve same thing with one action",
      "number": 3378,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:24.268Z"
    },
    {
      "summary": "Server should log a warning if task queue kind is not set when polling, as sticky task queues without kind specification could be mishandled. Additionally, server should perform exhaustive enum value checks for task queue kind to ensure all cases are handled.",
      "category": "bug",
      "subcategory": "task-queue",
      "apis": [],
      "components": [
        "matching",
        "loadbalancer",
        "task-queue-handler"
      ],
      "concepts": [
        "task-queue-kind",
        "sticky-task-queue",
        "enum-validation",
        "logging",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Sticky task queues without explicit kind specification may be incorrectly processed as normal task queues, leading to potential routing issues.",
      "rootCause": "Missing validation and logging for task queue kind parameter in the matching service loadbalancer",
      "proposedFix": "Add warn/error logging when task queue kind is not set, and implement exhaustive enum value checking with switch/case statements that handle all possible enum values",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "SDK when polling a task queue, must provide task queue kind, or sticky task queue can be treated as normal task queue",
      "number": 3372,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:23.074Z"
    },
    {
      "summary": "Dependency security vulnerability in modernc.org/sqlite-v1.17.3 with transitive dependency modernc.org/libc-v1.16.10 containing CVE-2020-28928 (buffer overflow in wcsnrtombs). Issue was automatically closed after the vulnerable library was removed from inventory.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "go-modules",
        "sqlite"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "buffer-overflow",
        "dependency",
        "cve",
        "version-management"
      ],
      "severity": "medium",
      "userImpact": "Users could be affected by a buffer overflow vulnerability in the SQLite C library dependency that could lead to availability issues.",
      "rootCause": "wcsnrtombs function in musl libc mishandles particular combinations of destination buffer size and source character limit, causing invalid write access",
      "proposedFix": "Upgrade modernc.org/libc to version 1.2.2-1 or later to fix CVE-2020-28928",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Automatically closed by Mend because the vulnerable library was either marked as ignored or is no longer part of the dependency inventory",
      "related": [],
      "keyQuote": "wcsnrtombs mishandles particular combinations of destination buffer size and source character limit, as demonstrated by an invalid write access (buffer overflow)",
      "number": 3371,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:25.456Z"
    },
    {
      "summary": "Dependency vulnerability in tally library (v4.1.2) with 2 high-severity CVEs affecting Apache Thrift integration causing potential denial of service through endless loops or panics when processing malformed input.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "tally",
        "thrift",
        "dependency-management"
      ],
      "concepts": [
        "vulnerability",
        "denial-of-service",
        "dependency-security",
        "input-validation",
        "protocol-handling"
      ],
      "severity": "high",
      "userImpact": "Temporal Server instances using vulnerable tally version could experience service disruption through malformed protocol input.",
      "rootCause": "Apache Thrift library versions 0.9.3-0.12.0 have vulnerabilities in TJSONProtocol/TSimpleJSONProtocol that cause endless loops or panics on invalid input.",
      "proposedFix": "Upgrade Apache Thrift dependency to version 0.13.0 or later.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was automatically closed by Mend as the vulnerable library was either marked as ignored or is no longer part of the Mend inventory in the specific branch.",
      "related": [],
      "keyQuote": "A server or client may run into an endless loop when feed with specific input data... or panic when feed with invalid input data.",
      "number": 3370,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:13.776Z"
    },
    {
      "summary": "Task queue state validation logic in the delete workflow execution is tightly coupled with task processing, making the codebase harder to understand. The proposed solution moves this validation logic from the delete manager to task processing, checking transfer and visibility queue ack levels to determine if deletion tasks can be safely processed.",
      "category": "feature",
      "subcategory": "task-queue-deletion",
      "apis": [],
      "components": [
        "delete-manager",
        "task-processing",
        "transfer-queue",
        "visibility-queue",
        "mutable-state"
      ],
      "concepts": [
        "task-queue-state",
        "validation",
        "coupling",
        "ack-levels",
        "workflow-deletion",
        "queue-processing"
      ],
      "severity": "medium",
      "userImpact": "This refactoring improves code organization and maintainability by decoupling delete workflow execution logic from task processing, making it easier for developers to understand and modify deletion behavior.",
      "rootCause": "Delete workflow execution behavior is tightly coupled with task processing state validation, creating unnecessary interdependencies between the delete manager and task processing components.",
      "proposedFix": "Move task queue state validation from delete manager to task processing. For transfer queue: load mutable state to get close transfer task id when processing delete task. For visibility queue: store closed visibility task id in mutable state since mutable state may be deleted before visibility task processing.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We should move this part into task processing.",
      "number": 3368,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:13.136Z"
    },
    {
      "summary": "Consolidate multiple timer tasks into a single active timer per workflow. Currently, separate timer tasks are created for user timers, activities, workflow tasks, and workflow execution. The enhancement proposes creating only one next timer at a time and managing subsequent timers after the current one fires.",
      "category": "feature",
      "subcategory": "timer-management",
      "apis": [],
      "components": [
        "timer-manager",
        "workflow-execution",
        "activity-execution"
      ],
      "concepts": [
        "timer-consolidation",
        "timer-scheduling",
        "event-ordering",
        "resource-optimization",
        "workflow-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Reduces system resource usage and complexity by consolidating multiple timer tasks into a single managed timer per workflow.",
      "rootCause": null,
      "proposedFix": "Create only one next timer for any kind and create the next in line after the current one fires. Add ability to delete a timer if a new timer created is earlier than the current one.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "we should only create one next timer for any kind, and create next in line after current one fires",
      "number": 3367,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:11.317Z"
    },
    {
      "summary": "Request to officially support SQLite in production for Temporal Server, enabling low-cost self-hosting for small projects and edge deployments without requiring PostgreSQL or other enterprise databases.",
      "category": "feature",
      "subcategory": "database-support",
      "apis": [],
      "components": [
        "persistence-layer",
        "sql-driver",
        "setup-scripts",
        "visibility"
      ],
      "concepts": [
        "sqlite",
        "production-readiness",
        "cost-efficiency",
        "self-hosting",
        "edge-deployment",
        "database-durability",
        "single-node"
      ],
      "severity": "medium",
      "userImpact": "Users with limited budgets cannot deploy Temporal to production without expensive managed databases or complex infrastructure.",
      "rootCause": null,
      "proposedFix": "Add sqlite as an option to the sql tool, update auto-setup shell scripts to support sqlite, and add sqlite visibility support (potentially using fulltext search extension).",
      "workaround": "Use Temporalite with litestream for replication to S3-compatible storage, or use temporal CLI dev-server with on-disk SQLite database.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        23,
        110
      ],
      "keyQuote": "Officially support sqlite in production... enabling deployment on micro budgets (think 5USD to 100USD a month at most).",
      "number": 3366,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:00.292Z"
    },
    {
      "summary": "Feature request to allow the auth interceptor to pull authentication information from custom headers instead of being limited to TLS or the standard authorization header. Users need flexibility to define where auth info is sourced from, including support for multiple headers (e.g., JWT in one header and client certificate in another).",
      "category": "feature",
      "subcategory": "auth-interceptor",
      "apis": [],
      "components": [
        "auth-interceptor",
        "claim-mapper",
        "authorization"
      ],
      "concepts": [
        "authentication",
        "custom-headers",
        "authorization",
        "jwt",
        "client-certificate",
        "header-mapping"
      ],
      "severity": "medium",
      "userImpact": "Users can now source authentication information from custom header names instead of being restricted to standard headers, enabling more flexible auth architectures.",
      "rootCause": null,
      "proposedFix": "PR #4935 added support for configurable header names (authHeaderName, authExtraHeaderName) in static config. Additionally, AuthInfoRequired method in ClaimMapper can return false to process all requests regardless of header presence.",
      "workaround": "Implement custom ClaimMapper that returns false from AuthInfoRequired to handle custom headers.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved via PR #4935 which added configurable header name support and enhanced ClaimMapper with AuthInfoRequired feature for custom auth scenarios.",
      "related": [
        4935
      ],
      "keyQuote": "You can implement `AuthInfoRequired` and return `false` and the ClaimMapper will be called in all cases.",
      "number": 3362,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:02.157Z"
    },
    {
      "summary": "When describing a workflow, activities that never sent a heartbeat incorrectly report their LastHeartbeatUpdateTime as equal to StartTime instead of being nil. The issue has been identified in the mutable state implementation.",
      "category": "bug",
      "subcategory": "activity-heartbeat",
      "apis": [
        "DescribeWorkflow"
      ],
      "components": [
        "mutable_state",
        "activity",
        "workflow_state"
      ],
      "concepts": [
        "heartbeat",
        "activity_state",
        "timestamp",
        "nil_check",
        "state_tracking"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably determine whether an activity has sent heartbeats, as the LastHeartbeatUpdateTime field is misleading for activities that never heartbeat.",
      "rootCause": "In mutable_state_impl.go around line 2049, LastHeartbeatUpdateTime is being set to StartTime instead of remaining nil for activities that have never heartbeat.",
      "proposedFix": "Fix the code in mutable_state_impl.go to properly handle the case where an activity has never received a heartbeat, ensuring LastHeartbeatUpdateTime remains nil.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The bug was identified in mutable_state_impl.go and likely fixed after the issue report.",
      "related": [],
      "keyQuote": "For activity that never heartbeat, if you DescribeWorkflow the PendingActivity will have LastHeartbeatUpdateTime set to same value as StartTime. It should be nil if it never received a heartbeat.",
      "number": 3358,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:41:00.415Z"
    },
    {
      "summary": "Move memo and search attribute data from mutable state to history events to reduce memory pressure. The history event needs to store both the upsert value and the merged value, with callers loading them from history or storing as part of task data.",
      "category": "feature",
      "subcategory": "mutable-state-optimization",
      "apis": [],
      "components": [
        "mutable-state",
        "history-event",
        "task-data"
      ],
      "concepts": [
        "memory-optimization",
        "data-storage",
        "state-management",
        "event-sourcing",
        "payload-handling"
      ],
      "severity": "medium",
      "userImpact": "Reduces memory consumption in Temporal Server, improving performance and scalability for workflows with large memo and search attribute values.",
      "rootCause": "Memo and search attribute payloads stored in mutable state create unnecessary memory pressure on the server.",
      "proposedFix": "Store memo and search attribute data in history events instead of mutable state; callers can retrieve from history events or store in task data (transfer/timer tasks).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed as an enhancement, indicating the proposed approach was implemented.",
      "related": [],
      "keyQuote": "Store payload of memo and search attribute put a lot of pressure on mutable state. We should move them to history event.",
      "number": 3357,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:48.882Z"
    },
    {
      "summary": "The batcher workflow should use UpsertMemo to record the final state instead of relying on heartbeat for progress tracking, enabling more efficient state monitoring without needing to wait for completion events.",
      "category": "feature",
      "subcategory": "workflow-state-management",
      "apis": [
        "UpsertMemo"
      ],
      "components": [
        "batcher-workflow",
        "workflow-engine",
        "state-recording"
      ],
      "concepts": [
        "progress-tracking",
        "workflow-state",
        "memo",
        "heartbeat",
        "async-state-update"
      ],
      "severity": "medium",
      "userImpact": "Users can better track batcher workflow progress and final state without waiting for completion events or implementing workarounds.",
      "rootCause": "Current heartbeat-based progress tracking requires completion events to determine final state, which is inefficient.",
      "proposedFix": "Replace heartbeat-based progress recording with UpsertMemo to record the final state of the batcher workflow.",
      "workaround": "Use workflow query as an alternative approach.",
      "resolution": "fixed",
      "resolutionDetails": "Implemented UpsertMemo feature to record job progress in batcher workflow instead of heartbeat.",
      "related": [],
      "keyQuote": "We can use upsert memo feature to record the final state.",
      "number": 3319,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:47.704Z"
    },
    {
      "summary": "Request for ability to broadcast commands or metadata updates to all workers listening on the same task queue, enabling coordinated setup/cleanup and data synchronization across multiple workers without requiring per-workflow API calls.",
      "category": "feature",
      "subcategory": "worker-coordination",
      "apis": [],
      "components": [
        "worker",
        "task-queue",
        "workflow-execution",
        "metadata-management"
      ],
      "concepts": [
        "worker-broadcast",
        "task-queue",
        "high-throughput",
        "data-synchronization",
        "worker-notification",
        "metadata-updates",
        "coordination"
      ],
      "severity": "medium",
      "userImpact": "Users running multiple workers on the same task queue cannot efficiently broadcast setup/cleanup commands or data updates to all workers, requiring inefficient per-workflow API calls.",
      "rootCause": null,
      "proposedFix": "Add a metadata map returned as part of every workflow or activity task, with an admin API to update this map. SDKs should watch for map changes and notify workers appropriately.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the business client can send a workflow or a command, which can be picked up and executed by all workers, instead of only one of them.",
      "number": 3309,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:49.278Z"
    },
    {
      "summary": "User requests database sharding capability for the task results table when data exceeds 5 million rows, as queries timeout under high data volume. The issue lacks specific context about database type and query conditions.",
      "category": "question",
      "subcategory": "database-scaling",
      "apis": [],
      "components": [
        "database",
        "task-results-storage",
        "query-execution"
      ],
      "concepts": [
        "sharding",
        "database-scaling",
        "performance",
        "large-datasets",
        "query-optimization"
      ],
      "severity": "medium",
      "userImpact": "Users experience query timeouts when the task results table grows beyond 5 million rows, impacting system usability at scale.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was closed as invalid due to lack of sufficient context. Maintainers requested clarification about database type, specific table, and query conditions, but the user did not provide further details.",
      "related": [],
      "keyQuote": "In a certain field of business, the data in a single table has exceeded 5 million, and a timeout occurs when filtering data for a certain query condition.",
      "number": 3308,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:34.782Z"
    },
    {
      "summary": "When an activity fails repeatedly and then hits the ScheduleToClose timeout, the timeout failure event only includes the timeout information and loses the original activity failure details, making debugging difficult. The request is to include the last activity failure as a cause in the timeout failure.",
      "category": "bug",
      "subcategory": "activity-timeout",
      "apis": [],
      "components": [
        "activity-executor",
        "failure-handling",
        "event-history"
      ],
      "concepts": [
        "timeout",
        "failure-chaining",
        "error-context",
        "activity-lifecycle",
        "debugging",
        "cause-chain"
      ],
      "severity": "medium",
      "userImpact": "Users lose visibility into the root cause of activities when they fail multiple times before hitting a timeout, making troubleshooting and debugging significantly harder.",
      "rootCause": "ScheduleToClose timeout failure event does not include the cause chain from previous activity failures, truncating the error context.",
      "proposedFix": "Include the last activity failure into the timeout failure as a cause field in the ActivityTaskTimedOut event.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Already fixed by the time of the comment. MichaelSnowden confirmed that the latest server version includes the failure cause of the latest failed activity attempt in the event.",
      "related": [],
      "keyQuote": "Include the last activity failure into the timeout failure as a cause.",
      "number": 3304,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:37.594Z"
    },
    {
      "summary": "Workers need visibility into when they can be retired by understanding which workflow versions are still in use. The issue proposes adding BuildIDs search attributes and possibly APIs to enable automatic worker retirement discovery and notifications.",
      "category": "feature",
      "subcategory": "worker-versioning",
      "apis": [
        "GetWorkerBuildIDOrdering"
      ],
      "components": [
        "worker",
        "versioning",
        "visibility-api",
        "search-attributes"
      ],
      "concepts": [
        "worker-retirement",
        "build-ids",
        "version-graph",
        "workflow-compatibility",
        "backwards-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users lack visibility into when workers supporting deprecated versions can be safely retired, making it difficult to manage worker lifecycle and automate cleanup.",
      "rootCause": "Missing visibility mechanism to determine which workflow versions are still actively running and require worker support.",
      "proposedFix": "Add BuildIDs search attribute (preferably with ES alias support), combine with version graph data, and potentially create new API or augment GetWorkerBuildIDOrdering to determine worker retirement eligibility.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Worker versioning feature with visibility support was implemented to address this requirement.",
      "related": [],
      "keyQuote": "We need a way for users to be able to understand when a set of workers can be retired because no more workflows require the version those workers support.",
      "number": 3303,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:36.324Z"
    },
    {
      "summary": "The system lacks guarantees on the ordering of child-to-parent signals and child workflow completion events, potentially causing race conditions where completion events arrive before signals from the child workflow.",
      "category": "feature",
      "subcategory": "child-workflow-ordering",
      "apis": [
        "StartChildWorkflow"
      ],
      "components": [
        "workflow-engine",
        "event-processing",
        "signal-handling"
      ],
      "concepts": [
        "ordering",
        "child-workflow",
        "signals",
        "event-ordering",
        "completion",
        "synchronization"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably depend on the ordering of signals sent from child workflows relative to their completion, leading to potential race conditions in parent workflow logic.",
      "rootCause": "No guarantee mechanism exists in the workflow execution engine to ensure signal events from child workflows are ordered before their completion events.",
      "proposedFix": "Provide a guarantee on the ordering of child-to-parent signals before child workflow completion events.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We don't have any guarantees that a signal sent from a child to its parent happens before child workflow completion event.",
      "number": 3296,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:24.715Z"
    },
    {
      "summary": "Request to add TLS configuration support for Elasticsearch visibility in Temporal. Currently only username/password authentication is supported, but TLS is required for production deployments with self-signed certificates.",
      "category": "feature",
      "subcategory": "elasticsearch-visibility",
      "apis": [],
      "components": [
        "elasticsearch-visibility",
        "tls-configuration",
        "authentication"
      ],
      "concepts": [
        "tls",
        "ssl",
        "elasticsearch",
        "visibility",
        "infrastructure",
        "production",
        "security"
      ],
      "severity": "high",
      "userImpact": "Users cannot connect Temporal to Elasticsearch in production environments that require TLS/SSL authentication, blocking enterprise deployments.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "TLS support has been added to Elasticsearch visibility configuration as confirmed in comments.",
      "related": [],
      "keyQuote": "TLS is a basic requirement in production and currently we cannot connect to Elasticsearch if it has TLS with self signed certs.",
      "number": 3292,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:24.327Z"
    },
    {
      "summary": "User needs a gRPC API to dynamically set task queue rate limiting (maxTaskQueueActivitiesPerSecond) without redeploying workers. Currently requires full worker redeployment to change this setting.",
      "category": "feature",
      "subcategory": "task-queue-rate-limiting",
      "apis": [],
      "components": [
        "task-queue",
        "rate-limiting",
        "gRPC",
        "worker"
      ],
      "concepts": [
        "dynamic-configuration",
        "rate-limiting",
        "task-queue-management",
        "deployment-avoidance",
        "operational-efficiency"
      ],
      "severity": "medium",
      "userImpact": "Users must redeploy all workers to change task queue rate limits, preventing dynamic operational adjustments without downtime.",
      "rootCause": null,
      "proposedFix": "Add a gRPC method that accepts task queue name and rate limit value, allowing configuration via gRPC/tctl/SDK without worker redeployment.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A gRPC method for setting a new value (takes task queue name and a number). Then a user can not set it on the Workers, and only use gRPC/tctl/SDK.",
      "number": 3288,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:22.838Z"
    },
    {
      "summary": "Server returns overly large error messages that exceed gRPC metadata limits, causing connection drops with cryptic errors instead of readable error messages when workers send invalid command sequences.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "server",
        "error-messaging",
        "grpc-communication",
        "workflow-execution"
      ],
      "concepts": [
        "metadata-limits",
        "error-messages",
        "grpc-headers",
        "http2-settings",
        "connection-management",
        "command-validation"
      ],
      "severity": "high",
      "userImpact": "Developers receive cryptic connection errors instead of clear validation messages when sending invalid command sequences, making debugging extremely difficult.",
      "rootCause": "Server includes raw command sequences in error messages without preprocessing to ensure they fit within gRPC metadata limits (8KB default) or announced client limits.",
      "proposedFix": "Preprocess variable-sized portions of error messages to respect the counterpart's announced metadata limit or enforce a reasonable default max length lower than 8KB.",
      "workaround": "Client-side can increase maxInboundMetadataSize to handle larger error messages.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The Server should preprocess all the variable-sized portions of error messages to make sure that it fits into a header limit announced by the counterpart",
      "number": 3284,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:11.775Z"
    },
    {
      "summary": "Frontend handler retries during workflow start requests can cause InvalidArgument errors due to missing search attributes, preventing new workflows from starting successfully.",
      "category": "bug",
      "subcategory": "workflow-start",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "frontend-handler",
        "workflow-start",
        "search-attributes",
        "retry-logic"
      ],
      "concepts": [
        "retry",
        "idempotency",
        "search-attributes",
        "error-handling",
        "workflow-initialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot start new workflows due to InvalidArgument errors caused by frontend handler retry logic.",
      "rootCause": "Frontend handler retries may lead to InvalidArgument errors when search attributes are missing during workflow start requests.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the retry logic for frontend handlers was corrected to prevent InvalidArgument errors.",
      "related": [],
      "keyQuote": "New workflow fails to start, and `InvalidArgument` error with message indicating that search attribute is not found returned.",
      "number": 3282,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:09.980Z"
    },
    {
      "summary": "User wants to control which workflow execution processes scheduled tasks when multiple workers are available, rather than having all workers receive and process the same task. Mistakenly believes multiple workflow executions are needed for high availability.",
      "category": "question",
      "subcategory": "workflow-routing",
      "apis": [],
      "components": [
        "worker",
        "workflow-execution",
        "task-dispatch"
      ],
      "concepts": [
        "high-availability",
        "load-distribution",
        "task-routing",
        "cron-scheduling",
        "multi-worker-coordination"
      ],
      "severity": "low",
      "userImpact": "User unable to control task distribution across multiple workers and workflow executions, leading to unnecessary redundant processing.",
      "rootCause": "Misunderstanding of Temporal's distributed architecture. User expects to control which workflow execution processes tasks, but Temporal's design handles high availability through worker-to-task assignment and timeout-based reassignment.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #3309, which addresses the same requirement for selective worker task assignment.",
      "related": [
        3309
      ],
      "keyQuote": "I just want to send to multiple workflows to get relevant information and execute tasks preemptively, not all workflows get this information and process.",
      "number": 3281,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:40:09.037Z"
    },
    {
      "summary": "Flaky test in the archival worker integration test suite that intermittently fails. The test was disabled because archival functionality is being rewritten.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "archival",
        "test-suite",
        "worker"
      ],
      "concepts": [
        "flaky-test",
        "archival",
        "integration-testing",
        "test-stability"
      ],
      "severity": "medium",
      "userImpact": "Test instability affects development workflow and CI/CD reliability, though archival is being rewritten.",
      "rootCause": "Underlying archival implementation issues causing intermittent test failures",
      "proposedFix": null,
      "workaround": "Test was disabled pending archival rewrite",
      "resolution": "wontfix",
      "resolutionDetails": "Archival functionality is being rewritten, so the flaky test was disabled rather than fixed",
      "related": [],
      "keyQuote": "Won't fix because archival is being rewritten, and we have simply disabled this test for now.",
      "number": 3277,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:55.347Z"
    },
    {
      "summary": "User requested an option to allow activities to fail gracefully without failing the entire workflow. After exhausting retries or timing out, activities should be skipped rather than propagating the failure to the workflow.",
      "category": "question",
      "subcategory": "activity-error-handling",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "activity-executor",
        "workflow-engine",
        "retry-policy"
      ],
      "concepts": [
        "retry",
        "timeout",
        "error-handling",
        "best-effort",
        "activity-failure",
        "graceful-degradation"
      ],
      "severity": "low",
      "userImpact": "Users wanted a built-in way to mark activities as optional without manually ignoring errors in workflow code.",
      "rootCause": null,
      "proposedFix": "Implement an ActivityOptions flag to skip activity failure after retries exhausted, similar to optional activity behavior.",
      "workaround": "Manually ignore activity errors in workflow code and handle them with conditional logic or return nil on failure.",
      "resolution": "invalid",
      "resolutionDetails": "The requested functionality already exists - workflows can catch activity errors and continue execution. The user realized this capability was already available through existing patterns.",
      "related": [],
      "keyQuote": "You are able to do so already. Here is a sample code: f1 := workflow.ExecuteActivity(...); err1 := f1.Get(ctx, nil); // ignore err1 and keep going",
      "number": 3272,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:58.165Z"
    },
    {
      "summary": "System Worker spawns repeated Prometheus metric registration errors when multiple workers register the same metric descriptor with variable labels in different orders, causing warning logs to flood the output.",
      "category": "bug",
      "subcategory": "prometheus-metrics",
      "apis": [],
      "components": [
        "system-worker",
        "prometheus-reporter",
        "metrics"
      ],
      "concepts": [
        "metric-registration",
        "label-ordering",
        "prometheus-descriptors",
        "concurrency",
        "worker-startup"
      ],
      "severity": "medium",
      "userImpact": "Users experience warning spam in logs that obscures actual issues and indicates potential problems with metric collection across multiple workers.",
      "rootCause": "Prometheus descriptor registration fails when the same metric (temporal_worker_task_slots_available) is registered multiple times with the same label set but in different orders, which Prometheus treats as different descriptors.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was not reproducible in versions 1.20 and 1.21, indicating it was fixed in a later release.",
      "related": [],
      "keyQuote": "a previously registered descriptor with the same fully-qualified name as Desc{fqName: \"temporal_worker_task_slots_available\"...} has different label names or a different help string",
      "number": 3265,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:57.061Z"
    },
    {
      "summary": "Add a command to describe external workflows from within a workflow, eliminating the need to use a client-based workaround in activities.",
      "category": "feature",
      "subcategory": "workflow-query",
      "apis": [
        "DescribeExternalWorkflowExecution"
      ],
      "components": [
        "workflow-engine",
        "command-processor",
        "external-workflow"
      ],
      "concepts": [
        "external-workflow",
        "workflow-description",
        "introspection",
        "client-operations",
        "workflow-context"
      ],
      "severity": "medium",
      "userImpact": "Users currently must use workarounds through activity clients to describe external workflows, which is inconvenient and limits workflow expressiveness.",
      "rootCause": null,
      "proposedFix": "Implement a new DescribeExternalWorkflowExecution command that can be called directly from within workflows.",
      "workaround": "Call workflow.describe() using a client invoked from within an activity.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        590
      ],
      "keyQuote": "Being able to describe an external workflow from inside a workflow.",
      "number": 3261,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:43.767Z"
    },
    {
      "summary": "Reset workflow execution fails when there are pending child workflows. The feature request asks for a flag option that would allow terminating pending child workflows to enable the reset command to proceed.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-execution",
        "reset-handler",
        "child-workflow-manager"
      ],
      "concepts": [
        "reset",
        "child-workflows",
        "workflow-termination",
        "pending-executions",
        "execution-control",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reset workflows that have pending child workflows, limiting their ability to recover from failed states or correct workflow logic.",
      "rootCause": null,
      "proposedFix": "Add a flag or option to the reset command that automatically terminates any pending child workflow executions before performing the reset.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Allow a flag to terminate the child workflow execution allowing reset command to go through.",
      "number": 3258,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:44.745Z"
    },
    {
      "summary": "Request to expose an HTTP healthcheck endpoint on the Temporal Frontend service to support deployment engines that cannot make gRPC calls due to infrastructure limitations.",
      "category": "feature",
      "subcategory": "healthcheck",
      "apis": [],
      "components": [
        "frontend",
        "healthcheck"
      ],
      "concepts": [
        "healthcheck",
        "http",
        "grpc",
        "deployment",
        "monitoring",
        "service-health"
      ],
      "severity": "medium",
      "userImpact": "Users with infrastructure constraints that only support HTTP healthchecks cannot easily deploy and monitor self-hosted Temporal servers.",
      "rootCause": "Frontend service only exposes gRPC healthcheck, not HTTP.",
      "proposedFix": "Expose a \"/healthcheck\" endpoint on the Frontend service.",
      "workaround": "Use TCP calls instead of HTTP healthchecks for deployment engine health monitoring.",
      "resolution": "fixed",
      "resolutionDetails": "HTTP API implementation was undertaken as work in progress to provide HTTP endpoint support.",
      "related": [],
      "keyQuote": "Exposing a \"/healthcheck\" endpoint on the Frontend service so service deployment engines can utilize this endpoint if they can't support gRPC calls.",
      "number": 3247,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:43.625Z"
    },
    {
      "summary": "WorkflowExecutionCancelRequested event from parent workflow is not processed when a child workflow returns ContinueAsNew concurrently. The cancellation event appears to be leaked due to a race condition during the workflow transition.",
      "category": "bug",
      "subcategory": "workflow-continuation",
      "apis": [
        "ContinueAsNew",
        "Sleep"
      ],
      "components": [
        "workflow-engine",
        "event-processor",
        "continuation-handler"
      ],
      "concepts": [
        "cancellation",
        "concurrent-processing",
        "continue-as-new",
        "event-leak",
        "race-condition",
        "parent-child-workflow"
      ],
      "severity": "medium",
      "userImpact": "Workflows that respond to cancellation requests by performing ContinueAsNew may lose the cancellation event, leading to unexpected workflow state transitions.",
      "rootCause": "Race condition between cancellation event processing and ContinueAsNew execution when a workflow receives a cancel request but decides to continue as new instead of honoring the cancellation.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Determined to be expected behavior - workflows can choose to ignore cancellation requests and perform ContinueAsNew; this is not a bug but a design choice by the workflow author.",
      "related": [],
      "keyQuote": "The workflow can decide to honor the cancellation request by return a CancelledError, or it can ignore that request and do something else.",
      "number": 3241,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:31.820Z"
    },
    {
      "summary": "When a schedule is created with UNSPECIFIED overlap policy, the server should store it as the current default (SKIP) instead of UNSPECIFIED. This prevents breaking changes if the server's default policy changes in the future.",
      "category": "feature",
      "subcategory": "schedule-overlap-policy",
      "apis": [
        "DescribeSchedule"
      ],
      "components": [
        "schedule-service",
        "overlap-policy",
        "api-enums"
      ],
      "concepts": [
        "default-behavior",
        "backward-compatibility",
        "api-versioning",
        "schedule-configuration",
        "state-persistence"
      ],
      "severity": "medium",
      "userImpact": "Prevents unexpected schedule behavior changes if the server's default overlap policy changes after schedule creation.",
      "rootCause": "Server stores UNSPECIFIED instead of resolving it to the current default value, making schedules fragile to future default changes.",
      "proposedFix": "When server receives UNSPECIFIED schedule overlap policy on create or update, store it as the current default (SKIP).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue has been implemented and released.",
      "related": [],
      "keyQuote": "If server stores UNSPECIFIED and later changes its default, the schedule I created earlier thinking it would be default/SKIP will now behave differently",
      "number": 3240,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:30.773Z"
    },
    {
      "summary": "Add a server capability flag to indicate worker versioning support. This is needed as part of the worker versioning feature implementation, with API changes tracked separately.",
      "category": "feature",
      "subcategory": "worker-versioning",
      "apis": [],
      "components": [
        "frontend",
        "server-capabilities",
        "workflowHandler"
      ],
      "concepts": [
        "worker-versioning",
        "server-capability",
        "feature-detection",
        "api-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Enables servers to advertise worker versioning support, allowing SDKs to conditionally enable the feature based on server capability.",
      "rootCause": null,
      "proposedFix": "Add a server-side capability flag for worker versioning in the workflowHandler, referencing the capabilities structure in the Temporal codebase.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented as part of worker versioning feature with corresponding API changes in temporalio/api#219.",
      "related": [
        3127
      ],
      "keyQuote": "We need to add a server-side capability flag for worker versioning",
      "number": 3230,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:32.187Z"
    },
    {
      "summary": "User asks how to query workflows based on custom object properties added to the workflow interface. The solution involves using Temporal's search attributes feature, which requires registering search attributes and tagging workflows with values during workflow start.",
      "category": "question",
      "subcategory": "workflow-querying",
      "apis": [
        "ListWorkflowExecutions",
        "UpsertSearchAttribute",
        "StartWorkflow"
      ],
      "components": [
        "workflow-execution",
        "list-workflows",
        "search-attributes"
      ],
      "concepts": [
        "search-attributes",
        "workflow-filtering",
        "query-language",
        "metadata-indexing",
        "workflow-execution-info"
      ],
      "severity": "low",
      "userImpact": "Users need to understand the search attributes mechanism to filter workflows by custom properties, requiring setup and explicit tagging.",
      "rootCause": null,
      "proposedFix": "Register a search attribute via the TCLD API, then use UpsertSearchAttribute in workflow code to tag workflows, and filter using the search attribute name in ListWorkflow queries.",
      "workaround": "Set SearchAttributes when starting the workflow via the StartWorkflow request to avoid needing UpsertSearchAttribute calls during execution.",
      "resolution": "fixed",
      "resolutionDetails": "Question answered with clear solution: use search attributes feature with registration and tagging, optionally set during workflow start.",
      "related": [],
      "keyQuote": "You need to 1) register your search attribute (this is a one time job); 2) tag your workflow with your search attribute.",
      "number": 3229,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:20.898Z"
    },
    {
      "summary": "Users need a way to add metadata (names/labels) to timers to make them distinguishable in the Temporal UI. Currently timers are displayed generically (e.g., 'Timer 37()'), making it difficult to identify which timer corresponds to which part of the workflow when multiple timers exist.",
      "category": "feature",
      "subcategory": "timer-metadata",
      "apis": [],
      "components": [
        "timer",
        "ui",
        "workflow-execution"
      ],
      "concepts": [
        "metadata",
        "naming",
        "observability",
        "workflow-debugging",
        "timer-identification",
        "ui-readability"
      ],
      "severity": "medium",
      "userImpact": "Users struggle to identify and debug multiple timers in workflows, reducing visibility into workflow execution and making it harder to troubleshoot timer-related issues.",
      "rootCause": null,
      "proposedFix": "Add metadata field (such as a name or label) to Timer that displays in the UI alongside the timer ID to provide human-readable context.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A way to add metadata to the Timer. Say adding a name or something that would help readability in the UI.",
      "number": 3228,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:20.824Z"
    },
    {
      "summary": "A panic occurs in loadClusterInformationFromStore due to assignment to a nil map when starting the Temporal server with a MySQL configuration. This should return a well-formatted error instead of panicking.",
      "category": "bug",
      "subcategory": "cluster-metadata",
      "apis": [],
      "components": [
        "fx",
        "cluster-metadata",
        "configuration"
      ],
      "concepts": [
        "nil-map",
        "panic",
        "initialization",
        "error-handling",
        "startup"
      ],
      "severity": "high",
      "userImpact": "Users cannot start the Temporal server with certain configurations as the application crashes with a panic instead of providing a helpful error message.",
      "rootCause": "The loadClusterInformationFromStore function attempts to assign values to a map that was never initialized (nil map), causing a panic at line 733 in temporal/fx.go.",
      "proposedFix": null,
      "workaround": null,
      "related": [],
      "resolution": null,
      "resolutionDetails": null,
      "keyQuote": "panic: assignment to entry in nil map in `loadClusterInformationFromStore`",
      "number": 3223,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:19.986Z"
    },
    {
      "summary": "PostgreSQL instance experiencing heavy database load on history_node table during stress testing. User proposes adding database indexes on (shard_id, tree_id, branch_id, node_id, txn_id) to optimize INSERT, DELETE, and SELECT query performance, though it may add slight overhead to insertions.",
      "category": "feature",
      "subcategory": "database-indexes",
      "apis": [],
      "components": [
        "history-node",
        "postgresql-persistence",
        "database-schema"
      ],
      "concepts": [
        "database-performance",
        "indexing",
        "query-optimization",
        "io-efficiency",
        "stress-testing"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal with PostgreSQL experience database performance degradation under heavy loads due to unoptimized queries on the history_node table.",
      "rootCause": "Missing or suboptimal database indexes on frequently executed queries (INSERT, DELETE, SELECT) on the history_node table, causing excessive disk I/O during stress testing.",
      "proposedFix": "Create composite database index on (shard_id, tree_id, branch_id, node_id, txn_id) to optimize the three identified query patterns. Alternative considered: table partitioning based on shard_id.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "The proposed index already exists as the table's primary key. The issue was resolved by clarifying that the suggested optimization was already implemented in the schema.",
      "related": [],
      "keyQuote": "The index you proposed is already the primary key. PRIMARY KEY (shard_id, tree_id, branch_id, node_id, txn_id)",
      "number": 3222,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:07.128Z"
    },
    {
      "summary": "Include the worker's build ID in PollerInfo so that it appears in describe task queue responses, enabling better visibility into worker build information.",
      "category": "feature",
      "subcategory": "task-queue-metadata",
      "apis": [
        "PollerInfo"
      ],
      "components": [
        "worker",
        "task-queue",
        "poller"
      ],
      "concepts": [
        "build-id",
        "worker-metadata",
        "task-queue-description",
        "observability"
      ],
      "severity": "low",
      "userImpact": "Enables users to view worker build IDs in task queue descriptions for better debugging and monitoring.",
      "rootCause": null,
      "proposedFix": "Add worker build ID field to PollerInfo structure and propagate it from worker configuration.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Worker build ID was added to PollerInfo to be included in task queue descriptions.",
      "related": [
        217
      ],
      "keyQuote": "We should include the worker's build id in `PollerInfo` so it shows up in describe tq responses",
      "number": 3214,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:04.836Z"
    },
    {
      "summary": "User requests adding a query parameter to ListNamespacesRequest to enable filtering namespaces by name substring. Proposed implementation would support simple name matching like `{ query: 'foo' }` to match namespaces like `foobar`, with potential for fuzzy matching or additional filter fields in the future.",
      "category": "feature",
      "subcategory": "namespace-filtering",
      "apis": [
        "ListNamespacesRequest"
      ],
      "components": [
        "namespace-api",
        "list-namespaces",
        "query-filtering"
      ],
      "concepts": [
        "filtering",
        "search",
        "namespace-management",
        "query-parameter",
        "substring-matching",
        "api-enhancement"
      ],
      "severity": "low",
      "userImpact": "Users would be able to filter namespaces by name when listing them, reducing the need to retrieve and filter all namespaces client-side.",
      "rootCause": null,
      "proposedFix": "Add a query parameter to ListNamespacesRequest that accepts either a simple string `{ query: 'foo' }` or an object `{ query: { name: 'foo' } }` for substring matching against namespace names.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "My guess is just `name` substring would be good enough for most: `{ query: 'foo' }` or `{ query: { name: 'foo' } }`",
      "number": 3212,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:39:04.695Z"
    },
    {
      "summary": "Admin API currently queries the database directly for history tasks instead of using the history service as the source of truth. This should be changed to query the history service, which may cache tasks in memory for better performance.",
      "category": "feature",
      "subcategory": "admin-api",
      "apis": [],
      "components": [
        "admin-handler",
        "history-service",
        "frontend-service"
      ],
      "concepts": [
        "caching",
        "source-of-truth",
        "api-design",
        "history-queries",
        "performance"
      ],
      "severity": "medium",
      "userImpact": "Users querying history tasks through the Admin API may not see up-to-date cached data that the history service maintains in memory.",
      "rootCause": "Admin API queries the database directly instead of routing through the history service abstraction layer",
      "proposedFix": "Refactor Admin API to query history tasks from the history service instead of querying the database directly",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #5426 which implemented the change to query history service",
      "related": [
        5426
      ],
      "keyQuote": "Admin API should query history tasks from source of truth, i.e. history service, instead of query DB directly",
      "number": 3211,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:54.272Z"
    },
    {
      "summary": "Request to add Sentry integration for error and panic reporting in the Temporal Go SDK. This would provide centralized error tracking to prevent errors from being lost in logs.",
      "category": "feature",
      "subcategory": "observability-error-tracking",
      "apis": [],
      "components": [
        "error-handling",
        "logging",
        "sdk-core"
      ],
      "concepts": [
        "error-tracking",
        "observability",
        "panic-reporting",
        "centralized-logging",
        "sentry-integration"
      ],
      "severity": "medium",
      "userImpact": "Users currently lose error context when errors occur, making debugging production issues difficult without centralized error tracking.",
      "rootCause": null,
      "proposedFix": "Integrate Sentry using the Go SDK (https://docs.sentry.io/platforms/go/) for error and panic reporting.",
      "workaround": "Community sentry interceptor available at https://github.com/sdcxtech/sentrytemporal",
      "resolution": "wontfix",
      "resolutionDetails": "No plan to implement Sentry support in the near future.",
      "related": [],
      "keyQuote": "No plan to do so in near future.",
      "number": 3203,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:53.420Z"
    },
    {
      "summary": "Frontend service fails to handle PollActivityTaskQueue requests with protobuf deserialization error due to version mismatch between frontend and matching services. Root cause was running multiple Temporal releases in the same Kubernetes namespace with identical labels but different versions.",
      "category": "bug",
      "subcategory": "deployment-configuration",
      "apis": [
        "PollActivityTaskQueue"
      ],
      "components": [
        "frontend",
        "matching",
        "grpc",
        "protobuf"
      ],
      "concepts": [
        "version-mismatch",
        "deployment",
        "namespace-isolation",
        "message-deserialization",
        "kubernetes"
      ],
      "severity": "high",
      "userImpact": "Users cannot submit or poll activity tasks when Temporal server instances with different versions run in the same Kubernetes namespace.",
      "rootCause": "Frontend service running older version that does not recognize NamespaceNotFoundFailure protobuf message type from newer matching service. Multiple deployments with same label but different versions caused this mismatch.",
      "proposedFix": "Register namespace using tctl tool and ensure all Temporal server components in the same Kubernetes namespace run the same version.",
      "workaround": "Move conflicting deployments to separate Kubernetes namespaces to isolate versions.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by separating deployments to different namespaces, ensuring version consistency within each namespace.",
      "related": [],
      "keyQuote": "it happens because we have 2 different release in one k8s namespace that have same label with different version",
      "number": 3196,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:53.607Z"
    },
    {
      "summary": "Create a stress test for buildID-based task dispatch using versioned task queues to validate the dynamic dispatch mechanism under load.",
      "category": "other",
      "subcategory": "testing",
      "apis": [],
      "components": [
        "task-dispatch",
        "versioning",
        "worker"
      ],
      "concepts": [
        "stress-testing",
        "buildID",
        "versioned-task-queues",
        "performance",
        "load-testing"
      ],
      "severity": "low",
      "userImpact": "Helps validate the reliability and performance of buildID-based dispatch under stress conditions before production use.",
      "rootCause": null,
      "proposedFix": "Write a benchmarking tool similar to 'bench' that uses versioned task queues, or leverage the OMES framework for testing.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by implementing stress testing using the OMES framework or creating a versioned task queue benchmark.",
      "related": [],
      "keyQuote": "Consider writing a version of \"bench\" that uses versioned task queues.",
      "number": 3193,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:40.287Z"
    },
    {
      "summary": "Ensure versioning data is properly propagated during replication across data centers. This involves creating infrastructure to store task queue metadata, updating internal structures, and implementing version set merging to support cross-dc scenarios.",
      "category": "feature",
      "subcategory": "versioning-replication",
      "apis": [],
      "components": [
        "versioning",
        "task-queue",
        "replication",
        "cluster-coordination"
      ],
      "concepts": [
        "cross-dc",
        "data-replication",
        "versioning-data",
        "task-queue-metadata",
        "version-sets",
        "build-ids",
        "cluster-seeding"
      ],
      "severity": "high",
      "userImpact": "Users relying on multi-data-center deployments need versioning data to be properly synchronized across clusters for consistent build ID and compatibility information.",
      "rootCause": null,
      "proposedFix": "Implement comprehensive versioning replication system including new metadata table, updated data structures, version set merging, user data replication, idempotent build ID updates, request redirection, and cluster seeding mechanisms.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "All checklist items completed including table definition, structure updates, API backend changes, version set merge, user data replication, idempotent updates, request redirection, cluster seeding mechanism, and comprehensive tests.",
      "related": [],
      "keyQuote": "We need to make sure this all works in cross-dc situations",
      "number": 3192,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:42.663Z"
    },
    {
      "summary": "Replace code-generated metrics and retryable client wrappers with gRPC client interceptors to reduce complexity and maintenance cost, aligning client-side implementation with existing server-side approach.",
      "category": "feature",
      "subcategory": "client-interceptors",
      "apis": [],
      "components": [
        "client",
        "metrics",
        "interceptors",
        "grpc"
      ],
      "concepts": [
        "code-generation",
        "metrics",
        "retry",
        "interceptors",
        "client-side",
        "maintenance"
      ],
      "severity": "medium",
      "userImpact": "Users will benefit from simpler client implementation with reduced maintenance burden and consistent interceptor-based approach across client and server.",
      "rootCause": "Current metrics and retryable client implementation uses code generation; server-side uses gRPC interceptors, creating inconsistent architectural approaches.",
      "proposedFix": "Replace code-generated metrics/retryable client with gRPC client interceptors, mirroring the server-side implementation approach.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        3172
      ],
      "keyQuote": "same approach can be applied to client side as well, and avoid the complexity of code generation and the cost for maintaining it",
      "number": 3183,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:41.452Z"
    },
    {
      "summary": "History shard's context override logic for timeout enforcement doesn't prevent caller from cancelling the context, potentially causing shard instability through unexpected rangeID updates when persistence operations are interrupted.",
      "category": "bug",
      "subcategory": "history-shard-context",
      "apis": [],
      "components": [
        "history-shard",
        "context-management",
        "persistence"
      ],
      "concepts": [
        "context-cancellation",
        "timeout-override",
        "shard-stability",
        "rangeID-management",
        "persistence-operations"
      ],
      "severity": "high",
      "userImpact": "Badly behaved clients can trigger unexpected shard behavior and rangeID updates, leading to potential shard instability.",
      "rootCause": "The context override logic that sets minimal timeout does not prevent the caller from cancelling the context, allowing cancellation during persistence operations.",
      "proposedFix": "Use a separate context for IO operations that is isolated from caller cancellation signals.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "History shard will override the context to ensure minimal timeout, however, this checking / overriding logic does not prevent caller from cancelling the call, potentially causing shard instability",
      "number": 3178,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:30.418Z"
    },
    {
      "summary": "Add input validation in the business logic layer to ensure request ID size does not exceed the configured threshold. Currently validation is only done at the database layer.",
      "category": "feature",
      "subcategory": "input-validation",
      "apis": [],
      "components": [
        "persistence",
        "request-handler",
        "business-logic"
      ],
      "concepts": [
        "input-validation",
        "request-id",
        "size-limit",
        "schema-compliance",
        "database-constraints"
      ],
      "severity": "low",
      "userImpact": "Prevents invalid requests from reaching the database layer by validating request ID size earlier in the processing pipeline.",
      "rootCause": "Input validation for request ID size is currently only enforced at the database schema level rather than in business logic.",
      "proposedFix": "Move request ID size validation from database schema constraints to the business logic layer to validate earlier in the request processing flow.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Input validation for request_ID has to be uuid as it is required by persistence schema.",
      "number": 3177,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:29.605Z"
    },
    {
      "summary": "Add a --no-auth flag to explicitly indicate when no authorizer is configured, improving security by requiring explicit opt-in for running without authentication. This will be optional in 1.18 but required in future releases.",
      "category": "feature",
      "subcategory": "security",
      "apis": [],
      "components": [
        "server",
        "authorizer",
        "configuration",
        "security"
      ],
      "concepts": [
        "authentication",
        "authorization",
        "security",
        "configuration",
        "default-behavior",
        "opt-in"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal server must explicitly acknowledge when no authorizer is configured, preventing accidental deployments without authentication.",
      "rootCause": null,
      "proposedFix": "Add a --no-auth flag to the server that must be present if no authorizer is specified. Make it optional in 1.18, required in following releases.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented as proposed with phased rollout approach",
      "related": [],
      "keyQuote": "If no authorizer is provided, temporal server will still start with a noop authorizer. For security reason, we should make it more explicitly that there is no authorizer configured.",
      "number": 3171,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:28.276Z"
    },
    {
      "summary": "Request for a new metric worker_task_slots_total to expose the configured number of task slots a worker allows. This would enable better tuning assessment by comparing against available slots and facilitate percentage-based alerting on task slot availability.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "worker",
        "metrics"
      ],
      "concepts": [
        "task-slots",
        "monitoring",
        "alerting",
        "worker-configuration",
        "tuning"
      ],
      "severity": "low",
      "userImpact": "Users can better monitor worker health and configure alerting based on task slot utilization percentage rather than absolute numbers.",
      "rootCause": null,
      "proposedFix": "Emit a metric worker_task_slots_total indicating the number of slots the worker is configured for, or alternatively worker_task_slots_usage showing the ratio of configured versus available slots.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Issue was moved to temporalio/sdk-features repository (issue #98).",
      "related": [
        98
      ],
      "keyQuote": "A metric worker_task_slots_total which emits the number of slots the worker is configured for.",
      "number": 3169,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:17.034Z"
    },
    {
      "summary": "Sticky workflow tasks should not be persisted in the database when they cannot be sync-matched, as this wastes IOPS when the SDK is unavailable and the history service will timeout the task anyway. Proposes optimization strategies to avoid unnecessary database operations.",
      "category": "feature",
      "subcategory": "sticky-task-matching",
      "apis": [],
      "components": [
        "matching-service",
        "history-service",
        "task-dispatch"
      ],
      "concepts": [
        "sticky-tasks",
        "sync-matching",
        "database-persistence",
        "timeout",
        "iops-optimization",
        "task-queue"
      ],
      "severity": "medium",
      "userImpact": "Reduces unnecessary database overhead and improves system efficiency by preventing persistent storage of sticky workflow tasks that cannot be immediately matched.",
      "rootCause": "Matching service persists sticky workflow tasks to the database even when they cannot be sync-matched, but these tasks are likely to be timed out by history service shortly after, wasting database I/O operations.",
      "proposedFix": "Three options proposed: (1) return error on failed sync-match and let history service retry before giving up, (2) use longer sync-match timeout and give up immediately if unable to sync-match, or (3) change history service to dispatch tasks preferentially to SDK queue with local cache, falling back to normal queue on sync-match timeout.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If sticky workflow task cannot be sync-matched, it (sometimes? usually?) means SDK is unavailable and history service will timeout the task few seconds later. If above case happen, DB IOPS are wasted.",
      "number": 3168,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:16.909Z"
    },
    {
      "summary": "When Elasticsearch index is deleted and rebuilt using `make install-schema-es`, existing search attributes are not automatically synced to the ES mappings. Users need a command to sync search attributes with ES schema, and the schema installation should auto-sync.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "elasticsearch",
        "search-attributes",
        "visibility",
        "schema"
      ],
      "concepts": [
        "elasticsearch-mapping",
        "schema-synchronization",
        "search-attributes",
        "index-management",
        "data-consistency"
      ],
      "severity": "medium",
      "userImpact": "Users rebuilding Elasticsearch indexes lose search attribute mappings and must manually reconfigure them, complicating disaster recovery and schema management.",
      "rootCause": "No automatic sync mechanism exists between Temporal search attributes and Elasticsearch schema during index rebuild operations.",
      "proposedFix": "Add a `tctl` command to sync search attributes with ES schema, and auto-sync when running `make install-schema-es`. Alternatively, add validation checks in visibility when adding documents.",
      "workaround": "Manually recreate Elasticsearch mappings after index deletion, or use Elasticsearch Dynamic Mapping (though this doesn't guarantee correct data types).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the temporal attribute to elasticsearch mapping sync tool is needed",
      "number": 3165,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:18.358Z"
    },
    {
      "summary": "The tdbg and tctl admin commands require direct database access but should instead rely exclusively on frontend admin APIs. This issue tracks removing direct DB connections and replacing them with API-based equivalents.",
      "category": "bug",
      "subcategory": "admin-tools",
      "apis": [],
      "components": [
        "tctl",
        "admin-api",
        "database-access"
      ],
      "concepts": [
        "database-abstraction",
        "api-migration",
        "admin-commands",
        "workflow-deletion",
        "frontend-apis"
      ],
      "severity": "medium",
      "userImpact": "Users need to have database access to run admin commands, which increases operational complexity and security surface area.",
      "rootCause": "Admin commands like tctl are directly accessing the database instead of going through frontend admin APIs.",
      "proposedFix": "Replace direct database connections with admin API calls. Implement workflow deletion through a new force workflow deletion API instead of direct DB connection.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed by implementing admin workflow delete command to use the new force workflow deletion API via PR #2841, removing direct database access.",
      "related": [
        2841
      ],
      "keyQuote": "We should remove any db access from tctl and rely only on frontend admin APIs.",
      "number": 3156,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:04.553Z"
    },
    {
      "summary": "The tdbg workflow show command ignores the global namespace flag, causing it to not respect the specified namespace parameter when displaying workflow details.",
      "category": "bug",
      "subcategory": "cli-namespace-handling",
      "apis": [],
      "components": [
        "tctl",
        "admin-cli",
        "workflow-show-command"
      ],
      "concepts": [
        "namespace",
        "flag-parsing",
        "command-execution",
        "cli-parameter-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot view workflow details in non-default namespaces using the global namespace flag, requiring workarounds to query the correct namespace.",
      "rootCause": "The workflow show command implementation does not properly read or apply the global namespace flag passed via tctl.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was verified as fixed by the reporter",
      "related": [],
      "keyQuote": "verified that this is fixed",
      "number": 3155,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:04.769Z"
    },
    {
      "summary": "The tdbg workflow show command does not support pagination, which limits the ability to view large workflow histories.",
      "category": "bug",
      "subcategory": "cli-tools",
      "apis": [],
      "components": [
        "tdbg",
        "cli",
        "workflow-show"
      ],
      "concepts": [
        "pagination",
        "cli-tools",
        "workflow-inspection",
        "data-retrieval",
        "usability"
      ],
      "severity": "medium",
      "userImpact": "Users cannot paginate through large workflow execution histories in the tdbg tool, making it difficult to inspect workflows with many events.",
      "rootCause": "The workflow show command implementation does not include pagination logic in the CLI code.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "tdbg workflow show does not support pagination",
      "number": 3154,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:38:05.564Z"
    },
    {
      "summary": "Expired tasks sitting in the matching task queue manager's in-memory queue can still be matched to pollers even after expiration, causing recordActivityTaskStarted to fail since the history layer has already timed out the task.",
      "category": "bug",
      "subcategory": "task-matching",
      "apis": [],
      "components": [
        "matching-task-queue-manager",
        "task-dispatcher",
        "history-service"
      ],
      "concepts": [
        "task-expiration",
        "task-ttl",
        "task-queue",
        "rate-limiting",
        "task-dispatch"
      ],
      "severity": "high",
      "userImpact": "Tasks can be dispatched to pollers after expiration, causing failures when attempting to record task start in history, leading to corrupted activity tracking.",
      "rootCause": "Tasks are not validated for expiration before being matched to a poller; once loaded into the in-memory queue, they bypass the TTL checks that occur at the persistence layer.",
      "proposedFix": "Check if a task is already expired before matching it to a poller to prevent dispatch of expired tasks.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented expiration validation before task dispatch to pollers to prevent matching of expired tasks.",
      "related": [],
      "keyQuote": "We need to check if task is already expired before match it to a poller.",
      "number": 3150,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:53.548Z"
    },
    {
      "summary": "Per-task-queue rate limiter has a 1-minute initialization delay, causing a throughput spike immediately after service restart before settling to the configured limit. The rate limiter should apply the configured rate immediately without waiting for its first update cycle.",
      "category": "bug",
      "subcategory": "rate-limiting",
      "apis": [],
      "components": [
        "matching",
        "rate-limiter",
        "task-queue"
      ],
      "concepts": [
        "rate-limiting",
        "throughput",
        "initialization",
        "service-restart",
        "backpressure"
      ],
      "severity": "medium",
      "userImpact": "After restarting the matching service, users experience a brief period of uncontrolled throughput spike (up to 1 minute) before rate limits take effect.",
      "rootCause": "The rate limiter is initialized with a large limit value and only updates its rate every 1 minute, causing a delay before the configured per-task-queue rate limit takes effect.",
      "proposedFix": "Allow the rate limiter to update its rate for the first time immediately without the 1-minute delay.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by allowing initial rate update without 1-minute delay (PR #3335)",
      "related": [
        3335
      ],
      "keyQuote": "We should allow the rate limiter to update its rate for the first time without that 1m delay.",
      "number": 3149,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:51.991Z"
    },
    {
      "summary": "Per-task-queue rate limiting counts all tasks including expired and failed ones, causing invalid tasks to consume rate limiter tokens and artificially throttle the backlog drain rate. Only successfully started tasks should count toward the TaskQueueActivitiesPerSecond limit.",
      "category": "bug",
      "subcategory": "rate-limiting",
      "apis": [
        "TaskQueueActivitiesPerSecond"
      ],
      "components": [
        "matching",
        "rate-limiter",
        "task-queue"
      ],
      "concepts": [
        "rate-limiting",
        "task-queue",
        "backpressure",
        "throughput",
        "task-expiration",
        "token-consumption"
      ],
      "severity": "high",
      "userImpact": "Users experience artificially reduced activity task throughput when expired or failed tasks are in the queue, even though no actual work is being dispatched.",
      "rootCause": "Rate limiting is applied at the match attempt stage (service/matching/matcher.go#L228) before validating whether a task successfully started, causing all task attempts to consume rate limit tokens regardless of outcome.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "All tasks, regardless of whether its expired or failed to start or successfully started will consume token from the rate limiter",
      "number": 3145,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:53.891Z"
    },
    {
      "summary": "Add metrics to the matching service to detect when workflows or activities are scheduled on task queues with no active pollers, helping workers identify misconfiguration issues.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "matching-service",
        "task-queue-manager",
        "worker",
        "metrics"
      ],
      "concepts": [
        "task-queue",
        "poller-detection",
        "monitoring",
        "misconfiguration",
        "observability",
        "diagnostic"
      ],
      "severity": "medium",
      "userImpact": "Workers will be unable to detect when workflows or activities are scheduled on wrong task queues, leading to silent failures and difficult debugging.",
      "rootCause": "SDK worker has no visibility into whether they are listening on the correct task queues or if tasks are being scheduled on queues with no active pollers.",
      "proposedFix": "Emit metrics from the matching service before adding tasks to the task queue manager when there are no recent pollers (e.g., within the last couple of minutes).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Metrics were implemented to detect no poller scenarios in the matching service.",
      "related": [],
      "keyQuote": "emit metric if there is NO recent pollers (for example in last couple of minutes)",
      "number": 3144,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:41.012Z"
    },
    {
      "summary": "Request to add new per-taskqueue metrics with improved naming and labeling. Current metrics use deprecated 'per_tl' naming and inconsistent taskqueue labels, making it difficult to aggregate metrics across partitions. Proposal is to add '_per_taskqueue' metrics with root taskqueue labels and partition indicators.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "task-queue",
        "partitions"
      ],
      "concepts": [
        "metrics-naming",
        "task-queue-partitions",
        "aggregation",
        "monitoring",
        "backwards-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily aggregate per-partition metrics to monitor task queue health, and the deprecated naming convention makes metric discovery difficult.",
      "rootCause": null,
      "proposedFix": "Add new metrics with '_per_taskqueue' suffix, use root taskqueue name for taskqueue labels, add partition label to indicate partition number (0 for root), deprecate old '_per_tl' metrics after several releases.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The metrics should have their root taskqueue for the taskqueue labels and a partition label which records their task queue partition",
      "number": 3143,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:41.226Z"
    },
    {
      "summary": "Enable workflow reset to any point even when there are pending ChildWorkflows. Currently, workflows with ChildWorkflows cannot be reset, making disaster recovery difficult and limiting the natural use of this feature.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-engine",
        "child-workflow",
        "reset-handler"
      ],
      "concepts": [
        "workflow-reset",
        "child-workflow",
        "disaster-recovery",
        "outage-recovery",
        "workflow-state-management"
      ],
      "severity": "high",
      "userImpact": "Users cannot safely reset workflows containing ChildWorkflows during outages, limiting their ability to recover from failures and constraining how they can design workflows.",
      "rootCause": null,
      "proposedFix": "Mirror Cadence feature to support reset to any point with pending ChildWorkflows, as described in referenced Cadence issue #3914.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        3914
      ],
      "keyQuote": "a workflow with childWorkflow can be not resettable which will be hard to recover from outage",
      "number": 3141,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:40.410Z"
    },
    {
      "summary": "Update gocql (Cassandra driver) dependency to v1.2.0 to incorporate necessary fixes required for PR #3076.",
      "category": "other",
      "subcategory": "dependencies",
      "apis": [],
      "components": [
        "cassandra-driver",
        "dependencies"
      ],
      "concepts": [
        "dependency-upgrade",
        "cassandra",
        "client-library",
        "database-driver"
      ],
      "severity": "medium",
      "userImpact": "Updates the Cassandra client library to include bug fixes that improve stability and compatibility with Temporal's Cassandra persistence layer.",
      "rootCause": null,
      "proposedFix": "Upgrade gocql to v1.2.0 as specified in PR #3226",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Updated to gocql v1.2.0 via PR #3226",
      "related": [
        3076,
        3226
      ],
      "keyQuote": "There are fixes necessary for PR https://github.com/temporalio/temporal/pull/3076",
      "number": 3138,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:29.186Z"
    },
    {
      "summary": "Add validation to Assert shard ownership against the database (source of truth) when acquireShards is invoked, in addition to existing in-memory data structure validation.",
      "category": "other",
      "subcategory": "shard-management",
      "apis": [],
      "components": [
        "shard-controller",
        "persistence",
        "history-service"
      ],
      "concepts": [
        "shard-ownership",
        "data-consistency",
        "source-of-truth",
        "validation",
        "database-state"
      ],
      "severity": "medium",
      "userImpact": "Improved shard ownership validation ensures consistency between in-memory state and database, preventing potential data inconsistencies.",
      "rootCause": "acquireShards only validates in-memory shard data structure without verifying against the authoritative database state via AssertShardOwnership API",
      "proposedFix": "Use the AssertShardOwnership persistence API to validate shard ownership against the database source of truth during acquireShards invocation",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "acquireShards function right now only assert shard (in mem) data structure is in valid state, additional assertion should also be performed against DB (source of truth)",
      "number": 3135,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:27.868Z"
    },
    {
      "summary": "Shard ownership assertion is blocked if the acquireShards operation is blocked, preventing periodic shard ownership checks from running. The issue proposes decoupling these operations using separate goroutines and channels so that periodic assertions aren't delayed by long-running acquireShards calls.",
      "category": "bug",
      "subcategory": "shard-controller",
      "apis": [],
      "components": [
        "shard-controller",
        "shard-acquisition",
        "event-loop"
      ],
      "concepts": [
        "concurrency",
        "non-blocking",
        "event-loop",
        "goroutine",
        "shard-ownership",
        "periodic-tasks"
      ],
      "severity": "medium",
      "userImpact": "Delayed or missed shard ownership assertions can lead to inconsistent shard allocation and potential data loss if ownership is not properly maintained.",
      "rootCause": "The event loop's select statement blocks on acquireShards completion before checking the acquire ticker again, preventing timely execution of periodic shard ownership assertions.",
      "proposedFix": "Decouple shard acquisition from the event loop using a separate goroutine and channel-based communication with a map to track in-progress assertions, allowing periodic tickers to trigger new assertions without waiting for previous ones to complete.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        3108
      ],
      "keyQuote": "Do not block shard ownership assertion if `acquireShards` is blocked",
      "number": 3134,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:28.799Z"
    },
    {
      "summary": "Request to publish the size and number of history events for closed workflows, along with mutable state size metrics. Currently these metrics are available during workflow updates but are needed at workflow closure for investigation and analysis purposes.",
      "category": "feature",
      "subcategory": "history-events",
      "apis": [],
      "components": [
        "history",
        "workflow-execution",
        "metrics"
      ],
      "concepts": [
        "history-events",
        "mutable-state",
        "metrics",
        "workflow-closure",
        "observability"
      ],
      "severity": "medium",
      "userImpact": "Users cannot measure history event count and mutable state size at workflow closure, limiting visibility into workflow performance and resource consumption.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We have size/number of history events whenever workflow update happens. For my current and future investigations we need those number at the time of closing workflow.",
      "number": 3133,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:15.468Z"
    },
    {
      "summary": "Spike test scenario on AWS RDS PostgreSQL causes database corruption, resulting in 'context deadline exceeded' errors that persist until the database is destroyed and rebuilt. After investigation, the issue was traced to Kubernetes infrastructure configuration rather than database corruption.",
      "category": "bug",
      "subcategory": "testing-infrastructure",
      "apis": [],
      "components": [
        "database",
        "history-service",
        "worker",
        "kubernetes"
      ],
      "concepts": [
        "timeout",
        "spike-testing",
        "database-stability",
        "infrastructure",
        "resource-exhaustion",
        "connection-handling"
      ],
      "severity": "high",
      "userImpact": "Users running benchmark/spike tests on AWS RDS PostgreSQL encounter persistent database failures requiring database recreation.",
      "rootCause": "Kubernetes EKS Fargate profile, CNI plugin configuration, and node capacity settings caused resource exhaustion and inter-node communication failures rather than actual database corruption.",
      "proposedFix": "Remove EKS Fargate profile, adjust CNI plugin configuration, change node capacity type to 'on-demand', and update security profile to allow traffic between nodes.",
      "workaround": "Destroy and rebuild the database to recover from the error state.",
      "resolution": "invalid",
      "resolutionDetails": "Issue was determined to be infrastructure/Kubernetes configuration problem, not database corruption. Resolved by adjusting cluster configuration: removing Fargate profile, updating CNI plugin, switching to on-demand nodes, and fixing security policies.",
      "related": [],
      "keyQuote": "this is not a database corruption issue but a problem with Kubernetes setup",
      "number": 3131,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:15.980Z"
    },
    {
      "summary": "Implement task dispatch to versioned queues for worker build ID versioning. Task queues with build ID versioning must route tasks according to the versioning proposal, including handling version set management, task readers, and sticky queue behavior.",
      "category": "feature",
      "subcategory": "worker-versioning",
      "apis": [],
      "components": [
        "task-queue",
        "matching",
        "versioning",
        "worker-build-id"
      ],
      "concepts": [
        "versioning",
        "task-dispatch",
        "build-id",
        "queue-routing",
        "sticky-queue",
        "version-set",
        "capability-flag"
      ],
      "severity": "high",
      "userImpact": "Enables proper task routing for versioned workers, allowing users to manage multiple worker versions simultaneously with correct task assignment.",
      "rootCause": null,
      "proposedFix": "Dispatch tasks to versioned queues according to the worker-versions proposal, including TQM startup for all version sets, task readers for all IDs in a set, and respecting use_latest_build_id flags.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented task dispatch to versioned queues with version set management, task reader startup, and dynamic configuration support.",
      "related": [],
      "keyQuote": "TQs which have any build id versioning info set using the API must dispatch tasks to versioned queues according to the proposal.",
      "number": 3127,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:17.179Z"
    },
    {
      "summary": "Request to propagate versioning data between partitions in the Temporal system. The issue lacks detailed description but appears to be related to ensuring version information is properly shared across partition boundaries.",
      "category": "feature",
      "subcategory": "versioning",
      "apis": [],
      "components": [
        "partition",
        "versioning",
        "metadata-propagation"
      ],
      "concepts": [
        "versioning",
        "data-propagation",
        "partitions",
        "cross-partition-communication",
        "version-tracking"
      ],
      "severity": "medium",
      "userImpact": "Without proper version propagation across partitions, systems may have inconsistent version information across distributed components.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, suggesting the feature was implemented or the request was addressed",
      "related": [],
      "keyQuote": null,
      "number": 3126,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:03.503Z"
    },
    {
      "summary": "User asked about cleaning up history_node and history_tree database tables that grow indefinitely in MySQL. Issue resolved by explaining that data is automatically removed after the workflow retention period.",
      "category": "question",
      "subcategory": "database-cleanup",
      "apis": [],
      "components": [
        "history-tables",
        "persistence",
        "database"
      ],
      "concepts": [
        "data-retention",
        "storage-management",
        "database-cleanup",
        "mysql"
      ],
      "severity": "low",
      "userImpact": "Users may be concerned about unbounded database growth and need to understand the retention mechanism for automatic cleanup.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Reduce the workflow retention time to a shorter duration if storage becomes a concern.",
      "resolution": "invalid",
      "resolutionDetails": "Resolved by clarifying that data in history tables is automatically removed after workflow retention time expires.",
      "related": [],
      "keyQuote": "Data in history tables will be removed after workflow retention time. You may want to reduce your retention time to a shorter duration if the storage become a concern.",
      "number": 3122,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:02.995Z"
    },
    {
      "summary": "Metrics for Operator API or Admin API incorrectly report operation=\"Unknown\" instead of the actual operation name (e.g., \"AdminGetShard\"). The namespace is correctly marked as unknown when not provided in the request.",
      "category": "bug",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "admin-api",
        "operator-api",
        "frontend-service"
      ],
      "concepts": [
        "metrics",
        "operation-tracking",
        "observability",
        "diagnostic-data"
      ],
      "severity": "medium",
      "userImpact": "Users monitoring admin API operations cannot properly identify which operations are failing due to incorrect operation names in metrics.",
      "rootCause": "Operation name is not being populated correctly in metrics collection for Admin API/Operator API calls.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The operation name is now correctly populated as \"AdminGetShard\" instead of \"Unknown\" in metrics.",
      "related": [],
      "keyQuote": "The namespace _unknown_ is expected as the request does not contain a namespace filed. But the operation should be \"AdminGetShard\".",
      "number": 3120,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:37:05.060Z"
    },
    {
      "summary": "Prometheus reporter generates a warning about conflicting metric descriptor for client_errors counter metric with mismatched label names or help string. This metric conflict appears intermittently after upgrading to version 1.17.",
      "category": "bug",
      "subcategory": "metrics-prometheus",
      "apis": [],
      "components": [
        "prometheus-reporter",
        "metrics",
        "config"
      ],
      "concepts": [
        "metric-conflict",
        "prometheus",
        "label-mismatch",
        "descriptor-registration",
        "client-errors"
      ],
      "severity": "medium",
      "userImpact": "Users see warning messages in logs when Prometheus reporter encounters conflicting metric descriptors, potentially causing metrics to be unreliable or not registered correctly.",
      "rootCause": "The client_errors counter metric descriptor is being registered multiple times with different label names or help strings, likely due to changes in the 1.17 release.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "error in prometheus reporter: a previously registered descriptor with the same fully-qualified name as Desc{fqName: \"client_errors\", help: \"client_errors counter\"} has different label names",
      "number": 3117,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:52.575Z"
    },
    {
      "summary": "Dependency security vulnerability in stretchr/testify v1.8.0 due to transitive dependency on stretchr/objx v0.4.0 with CVE-2022-28948. The vulnerability causes denial of service when deserializing invalid YAML input (CVSS 7.5).",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "build-system",
        "testing-framework"
      ],
      "concepts": [
        "vulnerability",
        "dependency",
        "security",
        "denial-of-service",
        "yaml-deserialization",
        "transitive-dependency"
      ],
      "severity": "high",
      "userImpact": "Applications using Temporal with this dependency version are vulnerable to denial of service attacks through malformed YAML input.",
      "rootCause": "stretchr/objx v0.4.0 contains CVE-2022-28948 which crashes when deserializing invalid YAML in the Unmarshal function.",
      "proposedFix": "Upgrade stretchr/objx to v3.0.0 or later.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was automatically closed by Mend as the vulnerable library was either marked as ignored or no longer part of the inventory in the specific branch.",
      "related": [],
      "keyQuote": "An issue in the Unmarshal function in Go-Yaml v3 causes the program to crash when attempting to deserialize invalid input.",
      "number": 3105,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:51.199Z"
    },
    {
      "summary": "Static clusterMetadata configuration for non-current clusters is silently ignored and deleted from the database after initialization, despite being specified in the config. The deprecation warning for ClusterInformation doesn't clearly indicate that non-current cluster configurations will be ignored.",
      "category": "bug",
      "subcategory": "cluster-configuration",
      "apis": [],
      "components": [
        "cluster-metadata",
        "metadata-initializer",
        "configuration",
        "database-initialization"
      ],
      "concepts": [
        "cluster-configuration",
        "static-config",
        "deprecation",
        "silent-failure",
        "global-namespace",
        "failover",
        "configuration-persistence"
      ],
      "severity": "medium",
      "userImpact": "Users who configure standby cluster information in static config are confused when that configuration is silently deleted from the database without clear notification.",
      "rootCause": "ClusterInformation configuration for non-current clusters is deprecated and ignored during initialization, but the deprecation warning doesn't clearly communicate that the configuration will be removed.",
      "proposedFix": "Either fully deprecate clusterMetadata with a new structure for cluster initialization, or improve logging to clearly indicate which cluster configurations will be ignored and why.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Silent failure is misleading, either complete deprecate the cluster metadata by using a new structure or improve the logging",
      "number": 3104,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:53.355Z"
    },
    {
      "summary": "Docker Hub images for Temporal Server, admin-tools, and auto-setup were incorrectly tagged as 1.7.1 instead of 1.17.1. This was a tagging error that prevented users from accessing the correct latest version of the software.",
      "category": "bug",
      "subcategory": "docker-image-release",
      "apis": [],
      "components": [
        "docker-image",
        "release-process",
        "docker-hub"
      ],
      "concepts": [
        "versioning",
        "container-image",
        "release-tagging",
        "docker-registry"
      ],
      "severity": "high",
      "userImpact": "Users pulling latest images from Docker Hub would receive version 1.7.1 instead of the intended 1.17.1, causing version mismatch issues.",
      "rootCause": "Incorrect tagging in the Docker image release process resulted in wrong semantic version labels being pushed to Docker Hub.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Correct tags (1.17.1) were pushed to Docker Hub to replace the incorrect 1.7.1 tags.",
      "related": [],
      "keyQuote": "Latest images for server, admin-tools & auto-setup should be tagged 1.17.1 but all are tagged 1.7.1 on docker hub.",
      "number": 3093,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:39.195Z"
    },
    {
      "summary": "The Docker Hub auto-setup container image is showing version 1.7.1 as latest when it should be 1.17.1, indicating a versioning or tagging issue with the published container image.",
      "category": "bug",
      "subcategory": "docker-image",
      "apis": [],
      "components": [
        "docker-image",
        "auto-setup",
        "release-process"
      ],
      "concepts": [
        "versioning",
        "docker-hub",
        "image-tagging",
        "release-management",
        "container-distribution"
      ],
      "severity": "medium",
      "userImpact": "Users pulling the latest auto-setup container image may get an outdated version instead of the current stable release.",
      "rootCause": "Docker Hub image tagging or release process issue where version 1.7.1 is marked as latest instead of 1.17.1.",
      "proposedFix": null,
      "workaround": "Users can explicitly specify the correct version 1.17.1 when pulling the image.",
      "resolution": "fixed",
      "resolutionDetails": "Docker Hub image tagging was corrected to properly reflect version 1.17.1 as latest.",
      "related": [],
      "keyQuote": "The current latest version should be 1.17.1, but it is listed... The latest version in is 1.7.1",
      "number": 3092,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:41.091Z"
    },
    {
      "summary": "CPU profiling reveals that emitting metrics during shard read/write lock acquisition causes performance overhead. The request is to either eliminate metrics emission during lock acquisition or implement sampling to reduce the impact.",
      "category": "other",
      "subcategory": "shard-lock-metrics",
      "apis": [],
      "components": [
        "shard-lock",
        "metrics-emission",
        "lock-acquisition"
      ],
      "concepts": [
        "cpu-profiling",
        "performance-overhead",
        "metrics-sampling",
        "lock-contention",
        "metric-filtering"
      ],
      "severity": "medium",
      "userImpact": "High CPU usage during shard lock acquisition degrades overall system performance and scalability.",
      "rootCause": "Metrics are being emitted during every shard r/w lock acquisition, creating excessive overhead.",
      "proposedFix": "Do not emit metrics when acquiring shard r/w lock, or implement sampling to reduce metric emission frequency.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "CPU profiling shows that logic probably should not emitting metrics during / before acquisition of shard r/w lock.",
      "number": 3086,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:39.734Z"
    },
    {
      "summary": "Temporal server crashes when AWS RDS IAM authentication token expires due to missing token refresh mechanism. The server cannot reconnect to the database after the IAM role session expires (default 1 hour).",
      "category": "bug",
      "subcategory": "database-authentication",
      "apis": [],
      "components": [
        "database-connection",
        "rds-iam-auth-plugin",
        "session-management"
      ],
      "concepts": [
        "token-expiration",
        "connection-pooling",
        "authentication-refresh",
        "aws-iam",
        "database-resilience",
        "session-management"
      ],
      "severity": "high",
      "userImpact": "Temporal servers running with AWS RDS IAM authentication crash and become unavailable when IAM tokens expire, requiring manual intervention to restart.",
      "rootCause": "The rds-iam-auth plugin lacks a mechanism to re-hydrate or refresh expired AWS IAM tokens. The underlying database drivers do not automatically fetch new tokens on reconnection.",
      "proposedFix": "Extend each database driver to override the Connect method to always fetch a new token, or force maxConns to 1 and recreate the session on connection failure.",
      "workaround": "Force maxConns to 1 and recreate the session for each store on connection failure.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2830
      ],
      "keyQuote": "With an IAM role with 1h of duration session, once the session is expired Temporal crashes and can't reach the DB.",
      "number": 3077,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:29.037Z"
    },
    {
      "summary": "User couldn't call multi-goroutine ExecuteActivity or ExecuteChildWorkflow inside a workflow due to illegal access from outside of workflow context. The issue was resolved by explaining that workflow code must be deterministic and requires using workflow.Go and workflow.Sleep instead of standard Go goroutines and time.Sleep.",
      "category": "question",
      "subcategory": "workflow-execution",
      "apis": [
        "ExecuteActivity",
        "ExecuteChildWorkflow",
        "workflow.Go",
        "workflow.Sleep"
      ],
      "components": [
        "workflow-executor",
        "activity-executor",
        "determinism-enforcement"
      ],
      "concepts": [
        "determinism",
        "goroutines",
        "concurrency",
        "context-isolation",
        "parallel-execution"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to use standard Go concurrency patterns inside workflows encounter runtime panics and need to understand Temporal's deterministic execution model.",
      "rootCause": "Attempting to use standard Go goroutines (go statement) and time.Sleep inside workflow code violates the determinism requirement, as the workflow context can only be accessed from within the workflow goroutine.",
      "proposedFix": "Use workflow.Go instead of standard goroutines and workflow.Sleep instead of time.Sleep. Alternatively, execute all activities sequentially without goroutines, then call Get on all futures to achieve parallel execution.",
      "workaround": "Execute all activities in sequence first, then call Get on futures in a separate loop to achieve parallel execution without needing goroutines.",
      "resolution": "fixed",
      "resolutionDetails": "User confirmed the solution worked after implementing the suggested changes using workflow.Go and workflow.Sleep.",
      "related": [],
      "keyQuote": "Workflow code must be deterministic, so `go` can't be used inside workflow definition. Please use the `workflow.Go` instead.",
      "number": 3071,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:28.854Z"
    },
    {
      "summary": "MySQL database initialization fails when using temporal-sql-tool due to timezone handling. The schema update process encounters errors related to timezone configuration, which was fixed in a later version but not included in v1.17.0.",
      "category": "bug",
      "subcategory": "database-setup",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "mysql-schema",
        "database-initialization"
      ],
      "concepts": [
        "timezone",
        "database-schema",
        "initialization",
        "sql-migration",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to initialize MySQL databases with temporal-sql-tool in v1.17.0 encounter failures requiring workarounds or upgrades.",
      "rootCause": "MySQL timezone configuration not properly handled during schema setup operations.",
      "proposedFix": "Set UTC timezone for MySQL as a workaround; fix included in v1.18+.",
      "workaround": "Set UTC time zone for MySQL before running temporal-sql-tool.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in PR #3012, included in version 1.18 (not backported to 1.17).",
      "related": [
        3012
      ],
      "keyQuote": "This is due to your timezone. Workaround is to set UTC time zone for MySQL. I fixed it in #3012 but it is not in 1.17 (will go to 1.18).",
      "number": 3070,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:27.115Z"
    },
    {
      "summary": "TaskQueue history metrics endpoint returns an excessive number of metrics (80,000+ activity metrics) that exceeds Prometheus's capacity to scrape, causing performance issues for monitoring systems.",
      "category": "bug",
      "subcategory": "metrics-taskqueue",
      "apis": [],
      "components": [
        "metrics",
        "taskqueue",
        "monitoring"
      ],
      "concepts": [
        "prometheus",
        "metrics-cardinality",
        "performance",
        "scraping",
        "observability",
        "activity-metrics"
      ],
      "severity": "high",
      "userImpact": "Users cannot effectively monitor their Temporal deployments because the metrics endpoint returns too much data for Prometheus to handle.",
      "rootCause": "TaskQueue history metrics include individual entries for all activities, creating unbounded cardinality that scales linearly with activity count.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "i get taskqueue history metics,it has abort all 80000 activities metrics.it is to large for promethues to pull",
      "number": 3065,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:16.018Z"
    },
    {
      "summary": "Request for a new error type to distinguish between workflow not found and workflow already completed cases. Currently both return NotFound error, preventing callers from implementing different error handling logic for these distinct scenarios.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "error-handling",
        "workflow-state-machine",
        "workflow-service"
      ],
      "concepts": [
        "error-types",
        "workflow-lifecycle",
        "completion-state",
        "error-differentiation",
        "state-tracking"
      ],
      "severity": "medium",
      "userImpact": "Users cannot distinguish between workflow not found and already completed scenarios, making it difficult to implement appropriate error recovery strategies.",
      "rootCause": "API returns the same NotFound error type for two distinct workflow states, preventing proper error differentiation at the caller level.",
      "proposedFix": "Introduce a new error type specifically for workflow already completed case, separate from NotFound error.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "For both workflow not found and workflow already completed case, we return a NotFound error. And caller can't tell the which case it is just by checking the error type",
      "number": 3062,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:16.415Z"
    },
    {
      "summary": "Add validation to ensure task queue partition count can only be decreased following the proper sequence (decrease write partitions first, wait for DB drain, then decrease read partitions) to prevent operational errors.",
      "category": "feature",
      "subcategory": "task-queue-partitioning",
      "apis": [],
      "components": [
        "matching",
        "task-queue",
        "dynamic-config"
      ],
      "concepts": [
        "partitioning",
        "scaling",
        "operational-safety",
        "configuration-validation",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Operators can safely decrease task queue partitions without risk of misconfiguration or data loss.",
      "rootCause": null,
      "proposedFix": "Implement validation logic to enforce the required sequence: decrease write partitions first, verify all DB tasks drain, then allow decrease of read partitions",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If operator need to decrease number of task queue partition, operator need to first decrease num of write partition, wait for all DB tasks to drain, then decrease num of read partitions",
      "number": 3060,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:15.289Z"
    },
    {
      "summary": "Dependency security vulnerability in stretchr/testify v1.7.4 due to transitive dependency on stretchr/objx v0.4.0 with CVE-2022-28948. The vulnerability causes denial of service through crashes when deserializing invalid YAML input.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependencies",
        "test-framework",
        "yaml-deserialization"
      ],
      "concepts": [
        "security-vulnerability",
        "denial-of-service",
        "dependency-management",
        "cvss-score",
        "transitive-dependency"
      ],
      "severity": "high",
      "userImpact": "Users of Temporal with testify v1.7.4 are exposed to a high-severity DoS vulnerability (CVSS 7.5) in transitive dependencies that could crash applications on malformed input.",
      "rootCause": "stretchr/objx v0.4.0 transitive dependency has unpatched vulnerability CVE-2022-28948 in YAML unmarshaling that causes crashes on invalid input",
      "proposedFix": "Upgrade stretchr/objx to v3.0.0 or higher where the vulnerability is fixed",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Auto-closed by Mend bot because the vulnerable library was removed from the dependency tree or marked as ignored in the Mend inventory",
      "related": [],
      "keyQuote": "An issue in the Unmarshal function in Go-Yaml v3 causes the program to crash when attempting to deserialize invalid input.",
      "number": 3057,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:04.372Z"
    },
    {
      "summary": "Request to support wildcard and regex search capabilities for workflow attributes like WorkflowId. Currently users must create custom search attributes to perform partial matches on machine-generated identifiers like product-prefixed IDs.",
      "category": "feature",
      "subcategory": "workflow-search",
      "apis": [],
      "components": [
        "workflow-search",
        "elasticsearch",
        "search-attributes",
        "list-filters"
      ],
      "concepts": [
        "wildcard-search",
        "regex-matching",
        "keyword-queries",
        "search-filters",
        "pattern-matching",
        "machine-generated-ids"
      ],
      "severity": "medium",
      "userImpact": "Users must currently create custom search attributes to search for workflows by partial identifiers (e.g., by product prefix), adding complexity to the developer experience.",
      "rootCause": "Built-in keyword search attributes do not support wildcard or regex patterns, limiting search capabilities for machine-generated identifiers.",
      "proposedFix": "Enable wildcard and/or regex support in List Filters for keyword search attributes using Elasticsearch' native wildcard and regexp query capabilities.",
      "workaround": "Users can create custom Search Attributes as an alternative, though this requires additional configuration.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        3962
      ],
      "keyQuote": "Wildcard search for built-in Keyword search attributes is a recurring request/need (like with a WorkflowId that is <productId>-<uuid>)",
      "number": 3056,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:03.155Z"
    },
    {
      "summary": "Issue reporting Temporal Cassandra failing in Kubernetes, referenced from community forum discussion. Marked as duplicate of issue #3010.",
      "category": "bug",
      "subcategory": "cassandra-kubernetes",
      "apis": [],
      "components": [
        "cassandra",
        "kubernetes",
        "deployment"
      ],
      "concepts": [
        "cassandra",
        "kubernetes",
        "deployment",
        "failure",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal with Cassandra backend in Kubernetes environments experience failures.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Duplicate of issue #3010 which addresses the same Cassandra-Kubernetes failure issue.",
      "related": [
        3010
      ],
      "keyQuote": "duplicate of https://github.com/temporalio/temporal/issues/3010",
      "number": 3054,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:36:01.436Z"
    },
    {
      "summary": "Advanced search with StartTime filter in the future returns incorrect results - UI shows workflows from the previous search instead of clearing them when the query returns no results. tctl correctly returns 0 results for the same query.",
      "category": "bug",
      "subcategory": "advanced-search",
      "apis": [],
      "components": [
        "search",
        "ui",
        "workflow-list"
      ],
      "concepts": [
        "search-filtering",
        "start-time",
        "query-results",
        "ui-state",
        "result-clearing"
      ],
      "severity": "medium",
      "userImpact": "Users get incorrect search results displayed when their advanced search query returns empty results, causing confusion about actual workflow state.",
      "rootCause": "After clicking search, if query results are empty, the currently displayed workflows in the list are not cleared/updated to reflect the empty result set.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It seems after clicking search, if the results of query are empty, the currnetly shown workflows in list are not updated (not removed)",
      "number": 3045,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:50.465Z"
    },
    {
      "summary": "User interface confusion in the advanced search feature where date range controls display icons that operate on workflow type instead of dates, and there's no way to copy the start/end date values for use in search queries.",
      "category": "bug",
      "subcategory": "ui-search",
      "apis": [],
      "components": [
        "web-ui",
        "search-interface"
      ],
      "concepts": [
        "user-interface",
        "usability",
        "search-filters",
        "date-range",
        "copy-functionality"
      ],
      "severity": "low",
      "userImpact": "Users cannot easily copy date values for search queries and are confused by misleading UI controls that don't match their visual proximity.",
      "rootCause": "UI design issue where date control icons are grouped with or misaligned relative to the workflow type controls, causing confusion about which element they operate on.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Author noted issue was posted to wrong repository and is not applicable to temporalio-temporal.",
      "related": [],
      "keyQuote": "all of these operate on the workflow type, not the start date. I got confused thinking the second set of icons would copy my start date but they copied the workflow type only.",
      "number": 3043,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:51.626Z"
    },
    {
      "summary": "User requests that Temporal provide guaranteed ordering for asynchronously invoked activities within a workflow. Currently, async started activities have no relative ordering guarantee, which is needed for certain use cases.",
      "category": "feature",
      "subcategory": "activity-execution",
      "apis": [
        "StartActivity"
      ],
      "components": [
        "workflow-executor",
        "activity-scheduler",
        "async-invocation"
      ],
      "concepts": [
        "ordering",
        "async-execution",
        "activity-sequencing",
        "execution-guarantee",
        "workflow-semantics"
      ],
      "severity": "medium",
      "userImpact": "Users requiring deterministic ordering of concurrently executed activities cannot rely on Temporal's current async execution model.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal does not guarantee relative ordering of async started activities",
      "number": 3025,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:52.202Z"
    },
    {
      "summary": "Aggressive retry logic for resource exhausted errors amplifies server load and makes resource contention worse. The system should throttle or disable retries for resource exhausted errors to allow the server to stabilize.",
      "category": "bug",
      "subcategory": "retry-logic",
      "apis": [],
      "components": [
        "client",
        "server",
        "history-service"
      ],
      "concepts": [
        "retry",
        "backoff",
        "resource-exhaustion",
        "service-busy",
        "load-amplification",
        "stability"
      ],
      "severity": "high",
      "userImpact": "Aggressive retries during resource exhaustion events worsen server overload and delay system recovery.",
      "rootCause": "Retry mechanisms don't account for resource exhausted errors, causing exponential load amplification during periods of high resource contention.",
      "proposedFix": "Slow down or stop retrying resource exhausted errors to allow the system to stabilize naturally.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed through PR #3069 which implements more intelligent retry handling for resource exhausted errors.",
      "related": [
        3069
      ],
      "keyQuote": "When there's resource exhausted error, the retry between client/server and within history service is too aggressive. This amplifies the load on the server and often makes the situation worse.",
      "number": 3023,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:39.980Z"
    },
    {
      "summary": "PostgreSQL password exposed in plain text in auto-setup logging output. The -x flag in docker/auto-setup.sh script causes sensitive credentials including database passwords to be logged without masking.",
      "category": "bug",
      "subcategory": "security-logging",
      "apis": [],
      "components": [
        "auto-setup",
        "docker",
        "logging",
        "credential-handling"
      ],
      "concepts": [
        "password-exposure",
        "credential-leakage",
        "shell-script-debugging",
        "security-vulnerability",
        "sensitive-data-logging"
      ],
      "severity": "critical",
      "userImpact": "Users deploying Temporal via docker-compose with auto-setup may have database passwords exposed in container logs, creating a significant security vulnerability.",
      "rootCause": "The -x flag (set -x) in auto-setup.sh enables shell script debugging which echoes all executed commands including variable assignments with passwords to stdout/logs without redaction.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "unknown",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "last line is plain text ^ [password exposed in logs]",
      "number": 3022,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:38.887Z"
    },
    {
      "summary": "Three security vulnerabilities (CVE-2019-0205, CVE-2019-0210, CVE-2018-11798) detected in transitive dependency Apache Thrift 0.10.0 through the tchannel-go library. The highest severity is 7.5 (high), involving potential denial of service and file access vulnerabilities.",
      "category": "bug",
      "subcategory": "security-dependency-vulnerability",
      "apis": [],
      "components": [
        "dependency-management",
        "tchannel-go",
        "apache-thrift"
      ],
      "concepts": [
        "security-vulnerability",
        "dependency-update",
        "cve",
        "denial-of-service",
        "file-access",
        "transitive-dependency"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal Server are exposed to denial of service attacks and potential unauthorized file access through vulnerable transitive dependencies.",
      "rootCause": "Outdated Apache Thrift library (0.10.0) with known vulnerabilities included transitively through tchannel-go v1.22.3 dependency.",
      "proposedFix": "Upgrade Apache Thrift to version 0.13.0 or later and update tchannel-go to a version that uses a patched Thrift dependency.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The vulnerable library was removed or ignored in the Mend inventory, automatically closing the issue as the dependency is no longer part of the codebase.",
      "related": [],
      "keyQuote": "This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.",
      "number": 3019,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:39.929Z"
    },
    {
      "summary": "The tagsToMap function is called on every counter/timer emission, creating objects repeatedly. Request to move tag exclusion logic to the metrics export phase (e.g., Prometheus) instead of during generation to reduce object allocation overhead.",
      "category": "other",
      "subcategory": "metrics-optimization",
      "apis": [],
      "components": [
        "tally-metric-provider",
        "metrics-exporter",
        "prometheus-integration"
      ],
      "concepts": [
        "metrics-tags",
        "object-allocation",
        "performance-optimization",
        "tag-filtering",
        "export-pipeline"
      ],
      "severity": "medium",
      "userImpact": "High-frequency metrics emission creates unnecessary memory overhead due to repeated tag mapping operations.",
      "rootCause": "Tag filtering (tagsToMap) executed during metric generation instead of during export phase, causing redundant object allocations.",
      "proposedFix": "Move tag exclusion logic to the metrics export phase (e.g., when exporting to Prometheus) rather than executing it on every counter/timer increment.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Can we execute the exclusion logic when exporting the metrics to e.g. prometheus? instead of doing this logic every time when increasing a counter.",
      "number": 3015,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:25.725Z"
    },
    {
      "summary": "Dependency security vulnerability in uber/tchannel-go v1.31.0 with 2 high-severity CVEs (CVE-2019-0205 and CVE-2019-0210) affecting Apache Thrift versions up to 0.12.0. The vulnerabilities can cause endless loops or panics in server/client implementations.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "dependency-management",
        "tchannel-go",
        "apache-thrift"
      ],
      "concepts": [
        "security-vulnerability",
        "dos-availability",
        "dependency-upgrade",
        "input-validation"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal with the vulnerable tchannel-go dependency are exposed to denial-of-service attacks that can cause server/client hangs or panics.",
      "rootCause": "Apache Thrift versions up to 0.12.0 have improper input validation that causes endless loops when processing specific data or panics on invalid JSON protocol input.",
      "proposedFix": "Upgrade Apache Thrift to version 0.13.0 or later.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was automatically closed by Mend because the vulnerable library was either marked as ignored or is no longer part of the Mend inventory.",
      "related": [],
      "keyQuote": "This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.",
      "number": 3011,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:27.622Z"
    },
    {
      "summary": "Temporal Cassandra failing to start properly in Kubernetes environment when using the temporaltest Helm chart with a single Cassandra node. Issue appears to be an environment setup problem rather than a bug in Temporal itself.",
      "category": "question",
      "subcategory": "deployment-kubernetes",
      "apis": [],
      "components": [
        "cassandra",
        "helm-chart",
        "kubernetes-deployment"
      ],
      "concepts": [
        "deployment",
        "database-initialization",
        "environment-setup",
        "containerization",
        "cluster-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to deploy Temporal with Cassandra in Kubernetes may encounter startup failures when using single-node configurations.",
      "rootCause": "Cassandra node configuration issue in temporaltest Helm chart environment, likely due to single-node setup and inadequate resource allocation or configuration.",
      "proposedFix": "Use a properly configured multi-node Cassandra setup for production environments rather than the temporaltest Helm chart which is only meant for testing.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was determined to be an environment setup problem, not a product bug. The temporaltest Helm chart is not production-ready and the single Cassandra node configuration was insufficient.",
      "related": [],
      "keyQuote": "The helm chart, as the name `temporaltest` suggest, is not meant to be used as production environment.",
      "number": 3010,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:25.357Z"
    },
    {
      "summary": "Request to add a new error type that allows continuing a workflow as new and signaling it in a single transaction, similar to SignalWithStartWorkflow but for the continue-as-new pattern. This is needed for workflows that iterate with input updates signaled during execution.",
      "category": "feature",
      "subcategory": "workflow-continuation",
      "apis": [
        "NewContinueAsNewError",
        "SignalWithStartWorkflow"
      ],
      "components": [
        "workflow-execution",
        "signal-handling",
        "workflow-state-management"
      ],
      "concepts": [
        "continue-as-new",
        "signal-workflow",
        "transaction-atomicity",
        "input-update",
        "workflow-iteration",
        "state-persistence"
      ],
      "severity": "medium",
      "userImpact": "Users implementing iterative workflows with dynamic input updates via signals must duplicate input handling across workflow arguments and signals, making code awkward and error-prone.",
      "rootCause": null,
      "proposedFix": "Add a new error type that combines NewContinueAsNewError functionality with the ability to signal the new workflow instance in a single atomic transaction.",
      "workaround": "Accept the same input struct through both workflow function argument and signal handlers, duplicating input handling logic.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add new error type that is like `NewContinueAsNewError` but allows signalling the new workflow in a single transaction like `SignalWithStartWorkflow`.",
      "number": 3008,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:13.990Z"
    },
    {
      "summary": "Temporal server fails to start with MySQL when using a remote database server, encountering an 'invalid default value for session_start' error during cluster_membership table initialization.",
      "category": "bug",
      "subcategory": "database-setup",
      "apis": [],
      "components": [
        "database-schema",
        "mysql-driver",
        "cluster-membership"
      ],
      "concepts": [
        "database-initialization",
        "schema-setup",
        "remote-database",
        "cluster-configuration",
        "mysql-compatibility"
      ],
      "severity": "high",
      "userImpact": "Users cannot set up Temporal server with MySQL running on a separate server, blocking deployment in multi-server configurations.",
      "rootCause": "Invalid default value for session_start column in cluster_membership table during schema setup with MySQL 8.0.29",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in issue #3012",
      "related": [
        3012
      ],
      "keyQuote": "Temporal server says \"invalid default value for session_start (a column in cluster_membership table created during schema setup)\"",
      "number": 3007,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:12.352Z"
    },
    {
      "summary": "Request for workflow pause/unpause functionality to prevent task scheduling during pauses and support batch operations. The feature would help handle workflow failures due to bugs and enable conditional pausing on activity failures.",
      "category": "feature",
      "subcategory": "workflow-control",
      "apis": [],
      "components": [
        "workflow-task-scheduler",
        "visibility-api",
        "activity-executor"
      ],
      "concepts": [
        "pause",
        "workflow-state",
        "retry-policy",
        "failure-handling",
        "batch-operations",
        "task-scheduling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot pause failing workflows to prevent further task scheduling, forcing them to either let failed tasks retry indefinitely or manually intervene.",
      "rootCause": null,
      "proposedFix": "Implement pause/unpause workflow commands with state replication to passive side, add default pause policy for continued workflow task failures, enable pausing on activity failures, and support batch unpause operations through visibility API.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Pause a workflow would mean no more workflow task would be scheduled for that workflow. The unpause would mean create new workflow task if one is needed.",
      "number": 3006,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:14.542Z"
    },
    {
      "summary": "Request to add a way to bypass autoforwarding with a special header to allow debugging and investigation of standby cluster and workflow issues without all requests being forwarded to the active cluster.",
      "category": "feature",
      "subcategory": "cluster-forwarding",
      "apis": [],
      "components": [
        "cluster-router",
        "request-handler",
        "autoforwarding"
      ],
      "concepts": [
        "debugging",
        "cluster-failover",
        "request-routing",
        "investigation",
        "standby-cluster",
        "header-override"
      ],
      "severity": "medium",
      "userImpact": "Users need a way to debug and investigate standby cluster issues when autoforwarding is enabled globally, currently impossible since all requests are routed to the active cluster.",
      "rootCause": null,
      "proposedFix": "Implement a special header mechanism to bypass autoforwarding on a per-request basis for debugging purposes.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Some way to bypass autoforwarding. This is mostly need for debugging/investigation purpose",
      "number": 3004,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:00.958Z"
    },
    {
      "summary": "Implement true mathematical equivalence checking for task predicates to enable better queue slice merging. Current equality check only compares predicate form and value, producing false negatives when logically equivalent predicates have different arrangements (especially with boolean operations).",
      "category": "feature",
      "subcategory": "predicate-optimization",
      "apis": [],
      "components": [
        "predicate-engine",
        "queue-management",
        "predicate-equals"
      ],
      "concepts": [
        "equivalence-checking",
        "boolean-logic",
        "queue-merging",
        "predicate-simplification",
        "performance-optimization",
        "mathematical-equivalence",
        "storage-efficiency"
      ],
      "severity": "low",
      "userImpact": "Users may experience suboptimal queue slice merging due to false negatives in predicate equivalence checks, resulting in increased storage overhead for queue slice metadata.",
      "rootCause": "The current Predicate.Equals() implementation only performs form-based comparison without normalizing or re-arranging logical operations (and/or/not), missing mathematically equivalent predicates that differ in structure.",
      "proposedFix": "Implement a real equivalence check that can normalize predicates and recognize mathematically equivalent forms, allowing more queue slices to be merged together.",
      "workaround": "Accept false negatives in predicate merging; the performance impact is minimal as unmerged queue slices only incur negligible additional storage costs.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2988
      ],
      "keyQuote": "implement a real equivalence check for predicates so that more queue slices can be merged together",
      "number": 2995,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:02.985Z"
    },
    {
      "summary": "Refactor metrics collection to dynamically generate task type tag values instead of using static lookup tables for queue processors (active/standby transfer/timer and visibility queues).",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "queue-processor",
        "timer-queue",
        "transfer-queue",
        "visibility-queue",
        "metrics"
      ],
      "concepts": [
        "metrics-tagging",
        "task-type-enumeration",
        "code-generation",
        "dashboard-compatibility",
        "refactoring"
      ],
      "severity": "low",
      "userImpact": "This is primarily an internal refactoring that maintains existing metrics output while reducing code duplication in task type tagging.",
      "rootCause": "Static lookup tables for task type tag values are duplicated across multiple queue processors and could be replaced with dynamic generation from proto enum definitions.",
      "proposedFix": "Dynamically generate task type tag values based on the proto enum definition for task types, with options including string manipulation, proto definition updates, or introducing separate active/standby tags.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2930
      ],
      "keyQuote": "Is it possible to get rid of the look up table and dynamically generate the value based on task type?",
      "number": 2991,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:35:01.335Z"
    },
    {
      "summary": "DescribeTaskQueue and other RPC calls return incomplete data when called on a task queue that hasn't finished loading. The issue is that matchingEngine calls RPCs on newly created taskQueueManager instances before the taskWriter has acquired the initial lease.",
      "category": "bug",
      "subcategory": "task-queue-manager",
      "apis": [
        "DescribeTaskQueue"
      ],
      "components": [
        "matchingEngine",
        "taskQueueManager",
        "taskWriter",
        "taskReader"
      ],
      "concepts": [
        "lease-acquisition",
        "async-initialization",
        "blocking",
        "task-queue-loading",
        "data-consistency"
      ],
      "severity": "medium",
      "userImpact": "Users calling DescribeTaskQueue on newly accessed task queues may receive incomplete or missing metadata.",
      "rootCause": "taskQueueManager returns before taskWriter has finished acquiring the initial lease, causing downstream RPC calls to operate on uninitialized state.",
      "proposedFix": "Add a blocking method to taskQueueManager that waits until taskWriter has acquired the lease (or failed), called from matchingEngine methods. Alternatively, make newTaskQueueManager not return until lease acquisition is complete using a context for timeout/cancellation.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add a method to tqm that blocks (up to the context timeout) until taskWriter has acquired the lease (or failed to), and call that from most of the matchingEngine methods.",
      "number": 2969,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:34:49.102Z"
    },
    {
      "summary": "Feature request to support updating system workflow cron schedules dynamically. Currently cron schedules are hardcoded and don't update when code is redeployed if a workflow is already running.",
      "category": "feature",
      "subcategory": "system-workflows",
      "apis": [],
      "components": [
        "system-workflows",
        "cron-scheduler",
        "workflow-deployment"
      ],
      "concepts": [
        "cron-schedule",
        "dynamic-configuration",
        "workflow-lifecycle",
        "deployment",
        "system-workflows",
        "schedule-update"
      ],
      "severity": "medium",
      "userImpact": "Users cannot update cron schedules for system workflows without manual intervention or workflow restart during deployments.",
      "rootCause": "Cron schedules are hardcoded with no mechanism to detect and apply schedule changes to already-running system workflows.",
      "proposedFix": "Detect when an existing system workflow is running with a different cron schedule and terminateAndStart a new workflow with the updated schedule.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Support changing system workflow cron schedule. Currently the cron schedule is hardcoded and even if the value in code is updated, when deploying the change, the cron schedule for the workflow won't be updated if there's already a workflow running.",
      "number": 2968,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:34:47.484Z"
    },
    {
      "summary": "History cache is currently per-shard and inefficient. Proposal is to move to a host-level cache shared among all shards, change dynamic config from entry count to byte size, and add metrics for cache size monitoring.",
      "category": "feature",
      "subcategory": "history-cache",
      "apis": [],
      "components": [
        "history",
        "cache",
        "shard",
        "metrics"
      ],
      "concepts": [
        "cache",
        "memory",
        "configuration",
        "shared-state",
        "performance",
        "metrics"
      ],
      "severity": "medium",
      "userImpact": "Users struggle to configure per-shard caches and lack visibility into cache memory usage, limiting performance optimization and resource management.",
      "rootCause": "Current architecture uses per-shard mutable state and event caches with entry-count-based configuration, making it difficult to estimate memory usage and configure effectively.",
      "proposedFix": "Implement host-level cache shared among all shards, change dynamic config to byte-size-based instead of entry-count-based, and emit gauge metrics for cache size monitoring.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        5894
      ],
      "keyQuote": "History uses mutable state cache and history event cache, and the caches are per shard, this is not efficient and hard to config.",
      "number": 2941,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:34:46.475Z"
    },
    {
      "summary": "Namespace replication task cleanup incorrectly calculates minimum ack level by considering disconnected clusters. Should only consider connected clusters to determine which replication tasks can be safely deleted.",
      "category": "bug",
      "subcategory": "namespace-replication",
      "apis": [],
      "components": [
        "namespace-replication",
        "cluster-management",
        "task-cleanup"
      ],
      "concepts": [
        "replication",
        "ack-level",
        "cluster-connection",
        "task-deletion",
        "namespace-metadata",
        "connected-clusters"
      ],
      "severity": "high",
      "userImpact": "Users with multi-cluster setups experience unnecessary retention of replication tasks, wasting storage and potentially causing operational issues.",
      "rootCause": "Replication ack level calculation includes disconnected clusters instead of only considering active/connected cluster connections.",
      "proposedFix": "Update namespace replication task cleanup logic to calculate minimum ack level based only on connected clusters, filtering out disconnected ones.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "In 1.16, we allow remove clusters from namespace cluster list. We should update this logic accordingly.",
      "number": 2929,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:34:36.383Z"
    },
    {
      "summary": "DescribeWorkflowExecution API does not work with archived workflows from filestore, S3, or GCloud providers, unlike GetWorkflowExecutionRawHistoryV2 which can display archived workflow histories.",
      "category": "bug",
      "subcategory": "archival",
      "apis": [
        "DescribeWorkflowExecution",
        "GetWorkflowExecutionRawHistoryV2"
      ],
      "components": [
        "archival",
        "mutable-state",
        "workflow-execution"
      ],
      "concepts": [
        "archival",
        "workflow-state",
        "history-retrieval",
        "storage-providers"
      ],
      "severity": "medium",
      "userImpact": "Users cannot retrieve workflow execution details for archived workflows, limiting visibility into completed workflow executions stored in external archives.",
      "rootCause": "DescribeWorkflowExecution reads from mutable state, which no longer exists for archived workflows since archival only preserves history events.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Determined to be expected behavior - archival only stores history events, not mutable state required by DescribeWorkflowExecution.",
      "related": [],
      "keyQuote": "Archival only saved history events, not mutable states. DescribeWorkflow reads info from mutable state. For archived workflow, there is no mutable state anymore.",
      "number": 2919,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:34:18.465Z"
    },
    {
      "summary": "Proposes an alternative persistence approach using coroutine serialization instead of Event Sourcing, arguing it could simplify workflow versioning and improve efficiency by avoiding full log replay.",
      "category": "feature",
      "subcategory": "workflow-persistence",
      "apis": [],
      "components": [
        "workflow-engine",
        "persistence",
        "serialization"
      ],
      "concepts": [
        "coroutines",
        "event-sourcing",
        "persistence",
        "serialization",
        "versioning",
        "idempotency",
        "continuation-points"
      ],
      "severity": "low",
      "userImpact": "Users could potentially benefit from simpler workflow versioning and better performance if this alternative persistence model were implemented.",
      "rootCause": null,
      "proposedFix": "Implement coroutine suspension and serialization at continuation points with explicit schema (e.g., protobuf) and persistence annotations for each point and field, requiring language-level coroutine serialization support.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "instead of Event Sourcing the framework could persist the Workflows' coroutines states into the persistent storage",
      "number": 2918,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:34:16.607Z"
    },
    {
      "summary": "Feature request to add a push model listener for dynamic config changes instead of relying on polling. Currently, infrequently accessed configs require either ignoring updates or implementing redundant polling code in each component.",
      "category": "feature",
      "subcategory": "dynamic-config",
      "apis": [],
      "components": [
        "dynamic-config",
        "config-listener",
        "worker-pool"
      ],
      "concepts": [
        "dynamic-config",
        "push-model",
        "event-listener",
        "configuration-management",
        "polling-overhead"
      ],
      "severity": "medium",
      "userImpact": "Users must either ignore config updates at runtime or implement duplicated polling logic in each component that needs infrequently updated configurations.",
      "rootCause": null,
      "proposedFix": "Add support for a push model in dynamic config where listeners can be registered for specific configuration keys to receive notifications on changes.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2911
      ],
      "keyQuote": "Add support for push model in dynamic config so that a listener can be registered for a certain key.",
      "number": 2915,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:34:06.849Z"
    },
    {
      "summary": "Upon namespace failover, each shard creates failover timer and transfer queues that load tasks simultaneously, causing a spike in persistence requests. The request proposes adding a host-level rate limiter for failover queue loading as a short-term solution.",
      "category": "feature",
      "subcategory": "failover-queue-management",
      "apis": [],
      "components": [
        "failover-queue",
        "shard-management",
        "persistence-layer",
        "rate-limiter",
        "task-loading"
      ],
      "concepts": [
        "failover",
        "rate-limiting",
        "task-loading",
        "queue-management",
        "namespace-failover",
        "persistence-spike",
        "load-balancing"
      ],
      "severity": "medium",
      "userImpact": "Namespace failover causes service availability issues due to spikes in persistence requests from simultaneous queue task loading across shards.",
      "rootCause": "Each shard creates failover timer queue and failover transfer queue that start task loading at nearly the same time, causing a persistence request spike.",
      "proposedFix": "Add a host level rate limiter for failover queue as a short-term solution. Long-term: task guarantee execution work and multicursor will eliminate standby and failover queues.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2864
      ],
      "keyQuote": "Each shard will create a failover timer queue and a failover transfer queue...start task load at almost the same time and cause a spike in persistence requests",
      "number": 2912,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:34:06.479Z"
    },
    {
      "summary": "Core Temporal components need access to configure their underlying gRPC connections to enable custom tracing, credentials, and service identification. Currently, server components cannot intercept or configure gRPC connection details during initialization.",
      "category": "feature",
      "subcategory": "grpc-configuration",
      "apis": [],
      "components": [
        "grpc-connection",
        "server-components",
        "frontend"
      ],
      "concepts": [
        "grpc-configuration",
        "connection-management",
        "interceptors",
        "credentials",
        "tracing",
        "service-identification"
      ],
      "severity": "medium",
      "userImpact": "Users cannot add custom tracing or gRPC credentials to inter-service Temporal server connections, limiting service identification and observability.",
      "rootCause": "Server components lack exposed access to their underlying gRPC connection configuration, preventing user customization of dial options and interceptors.",
      "proposedFix": "Expose gRPC connection access during component initialization to allow users to set custom dial options and/or interceptors on the underlying connection.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal server components would expose access to their underlying gRPC connection during initialization. A user can set custom dial options and/or interceptors on the underlying connection.",
      "number": 2905,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:34:05.722Z"
    },
    {
      "summary": "User reported that workflows weren't being picked up by workers unless the WorkflowId was prefixed with the TaskQueue name. Investigation revealed this was a user errorthe workflow had been originally started on a different task queue (cron) and needed to be terminated before a new workflow with the same ID could be created on the target queue (aura).",
      "category": "question",
      "subcategory": "workflow-execution",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-scheduler",
        "task-queue-dispatcher",
        "execution-model"
      ],
      "concepts": [
        "workflow-id",
        "task-queue",
        "execution-isolation",
        "workflow-identity"
      ],
      "severity": "low",
      "userImpact": "Users may be confused about workflow identity and task queue relationships when attempting to reuse workflow IDs across different task queues.",
      "rootCause": "User attempted to start a workflow with the same ID on a different task queue while the original workflow was still running on the first queue, causing execution conflicts.",
      "proposedFix": null,
      "workaround": "Terminate the existing workflow with the same ID on the original task queue before starting a new workflow with that ID on a different task queue.",
      "resolution": "invalid",
      "resolutionDetails": "Reporter identified the issue as user error and closed it. The behavior is correctworkflows with the same ID cannot execute on different task queues simultaneously.",
      "related": [],
      "keyQuote": "I had originally started a workflow with my ID as a cron... Once I terminated this old workflow, everything worked.",
      "number": 2902,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:55.314Z"
    },
    {
      "summary": "Request to publish Temporal Docker images to Amazon ECR Public Gallery to provide an alternative to Docker Hub, reducing rate-limiting issues for users who don't have paid Docker Hub subscriptions.",
      "category": "feature",
      "subcategory": "docker-distribution",
      "apis": [],
      "components": [
        "docker-images",
        "image-registry",
        "distribution"
      ],
      "concepts": [
        "rate-limiting",
        "docker-hub",
        "ecr",
        "public-registry",
        "accessibility"
      ],
      "severity": "medium",
      "userImpact": "Users without paid Docker Hub subscriptions experience rate-limiting when pulling Temporal images and need alternative sources.",
      "rootCause": null,
      "proposedFix": "Publish Temporal Docker images to Amazon ECR Public Gallery, following the example of organizations like HashiCorp, Elastic, and Ubuntu.",
      "workaround": "Users can pay for Docker Hub or manually copy Temporal images from Docker Hub to a private registry.",
      "resolution": "fixed",
      "resolutionDetails": "Temporal images are now available in Amazon ECR Public Gallery.",
      "related": [],
      "keyQuote": "The Amazon ECR Public Gallery has high public quotas...This would allow more users who are unwilling to buy into the paid Docker ecosystem use Temporal.",
      "number": 2900,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:53.774Z"
    },
    {
      "summary": "Temporal server panics if shutdown is signaled during startup, specifically when the stop signal arrives while services are still initializing. The issue occurs because the worker service only checks if start() was invoked, not if it completed.",
      "category": "bug",
      "subcategory": "server-lifecycle",
      "apis": [],
      "components": [
        "worker",
        "history",
        "frontend",
        "matching",
        "service-lifecycle"
      ],
      "concepts": [
        "shutdown",
        "signal-handling",
        "race-condition",
        "concurrent-startup",
        "panic-recovery",
        "graceful-shutdown"
      ],
      "severity": "medium",
      "userImpact": "Users triggering rapid start-stop cycles may encounter server panics, though this is unlikely in normal operation.",
      "rootCause": "Worker service checks if start() was invoked but does not verify if start() completed before accepting shutdown signals, creating a race condition during service initialization.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Could not be reproduced in latest version; reporter may have been using incorrect version (v0.11.0). Issue closed pending panic log reproduction.",
      "related": [],
      "keyQuote": "it seems that you just checked whether start() is invoked but did check whether it is complete",
      "number": 2899,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:52.930Z"
    },
    {
      "summary": "Feature request to allow historyArchiver and visibilityArchiver to read AWS credentials from a file path instead of relying solely on global environment variables, improving security and supporting Kubernetes workloads with mounted secrets.",
      "category": "feature",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "historyArchiver",
        "visibilityArchiver",
        "s3-archiving"
      ],
      "concepts": [
        "aws-credentials",
        "security",
        "kubernetes",
        "secret-management",
        "iam-role"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal on Kubernetes or shared EC2 instances cannot securely pass AWS credentials to archiving components without exposing them globally.",
      "rootCause": "Archivers only support global AWS credentials via environment variables or instance profiles, not credential files, limiting deployment options.",
      "proposedFix": "Add first lookup on AWS secret from a file path before falling back to global environment variables, AWS profiles, or instance profiles.",
      "workaround": "Set the AWS_SHARED_CREDENTIALS_FILE environment variable to specify a credential file location, following AWS SDK best practices.",
      "resolution": "wontfix",
      "resolutionDetails": "Closed as the requested functionality is already available through standard AWS SDK environment variables (AWS_SHARED_CREDENTIALS_FILE), following AWS best practices.",
      "related": [],
      "keyQuote": "I think you can just set the AWS_SHARED_CREDENTIALS_FILE environment variable if you want to specify a specific file.",
      "number": 2894,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:40.839Z"
    },
    {
      "summary": "Documentation on development.yaml and dynamicconfig is severely lacking, particularly guidance on configuration for Docker deployments and clarity on how to configure Temporal using pre-built Docker images versus building from source.",
      "category": "docs",
      "subcategory": "configuration-documentation",
      "apis": [],
      "components": [
        "docker",
        "configuration",
        "development-yaml",
        "dynamicconfig"
      ],
      "concepts": [
        "configuration",
        "deployment",
        "docker-setup",
        "environment-variables",
        "out-of-the-box-setup",
        "production-deployment"
      ],
      "severity": "medium",
      "userImpact": "New Temporal users struggle to understand configuration options and how to properly set up Temporal using Docker images, requiring them to dig through repositories to find answers.",
      "rootCause": "Documentation lacks practical examples for Docker-based deployments and doesn't explain the relationship between development.yaml, dynamicconfig, environment variables, and command-line arguments.",
      "proposedFix": "Create documentation covering: (1) how to configure Temporal using pre-built Docker images, (2) high-level explanation of development.yaml and dynamicconfig relationship, (3) blog post contrasting binary setup with Docker image setup, (4) clarification on production-readiness of Docker images.",
      "workaround": "Users can examine docker/config_template.yaml and docker-builds repository to understand Docker configuration patterns.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Most out-of-the-box docker images can be configured via command arguments or environment variables...but I didn't find this documented anywhere",
      "number": 2891,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:39.515Z"
    },
    {
      "summary": "The temporal-sql-tool cannot connect to PostgreSQL databases unless the database name is one of the default names (postgres or defaultdb) because cfg.DatabaseName is unconditionally blanked out before connection, preventing use with custom database names.",
      "category": "bug",
      "subcategory": "sql-tool",
      "apis": [],
      "components": [
        "sql-tool",
        "database-handler",
        "postgres-driver"
      ],
      "concepts": [
        "database-connection",
        "configuration",
        "postgres",
        "sql-setup",
        "connection-pooling"
      ],
      "severity": "high",
      "userImpact": "Users cannot use temporal-sql-tool to set up schemas on PostgreSQL instances with custom database names, forcing manual database creation or schema setup.",
      "rootCause": "The cfg.DatabaseName value is unconditionally blanked out in tools/sql/handler.go line 107 before creating a database connection, regardless of database type.",
      "proposedFix": "Add an if-clause to only blank out cfg.DatabaseName for MySQL use cases instead of unconditionally for all databases.",
      "workaround": "Manually create a defaultdb or postgres database instance first, or manually create the temporal database and use the tool only for schema setup.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved; the reporter closed it after discovering the root cause and finding that creation of default database instances allowed the tool to work.",
      "related": [],
      "keyQuote": "a connection to a postgres db instance is not possible unless the database name is one of the default names (postgres or defaultdb)",
      "number": 2890,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:42.038Z"
    },
    {
      "summary": "Workers filling up with activities stop polling and disappear from the task-queue endpoint and UI, even though they continue functioning. The task-queue endpoint only shows workers that are actively polling.",
      "category": "bug",
      "subcategory": "task-queue-visibility",
      "apis": [],
      "components": [
        "task-queue",
        "worker-registry",
        "ui"
      ],
      "concepts": [
        "polling",
        "visibility",
        "worker-status",
        "activity-capacity",
        "heartbeat"
      ],
      "severity": "medium",
      "userImpact": "Users cannot see workers in the UI when they are busy processing long-running activities, causing confusion about worker availability and status.",
      "rootCause": "Task-queue endpoint only returns workers that have actively polled within the last 5 minutes. Workers blocked by activity backlog stop polling and become invisible.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Behavior is by design - task-queue only tracks active pollers for 5 minutes. Workers disappear when not polling and reappear when polling resumes.",
      "related": [],
      "keyQuote": "The workers list from task queue is collected when worker poll from task queue. The info is keep for 5m. If worker stops polling for that task queue for more than 5m, that worker will disappear.",
      "number": 2882,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:27.346Z"
    },
    {
      "summary": "RequestCancelWorkflowExecution returns success when called on a completed workflow instead of returning NotFound error as documented. The API documentation promised NotFound would be raised, but the actual behavior is to silently succeed.",
      "category": "docs",
      "subcategory": "workflow-cancellation",
      "apis": [
        "RequestCancelWorkflowExecution"
      ],
      "components": [
        "workflow-execution",
        "cancellation-handler",
        "api-documentation"
      ],
      "concepts": [
        "workflow-state",
        "error-handling",
        "api-contract",
        "request-semantics",
        "state-validation"
      ],
      "severity": "medium",
      "userImpact": "Users relying on the documented behavior of getting NotFound error for completed workflows receive no error instead, potentially hiding logic errors in their code.",
      "rootCause": "The implementation treats cancellation as a 'best-effort request' that always succeeds, while documentation claimed it would fail with NotFound for completed workflows.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "API documentation was updated to reflect the actual behavior: cancellation requests are best-effort and return success even for completed workflows, consistent with the request-based semantics of cancel vs terminate.",
      "related": [],
      "keyQuote": "This is expected behavior. There is major difference between terminate and cancel. For terminate, it is a forceful operation, but for cancel it is a request thus the name RequestCancelWorkflowExecution.",
      "number": 2860,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:28.593Z"
    },
    {
      "summary": "ListWorkflow API for basic visibility requires BETWEEN filter syntax instead of supporting simpler comparison operators (>, >=, <, <=) like StartTime > SOME_TIME. This limitation is confusing for users and differs from expected SQL-like behavior.",
      "category": "feature",
      "subcategory": "visibility-api",
      "apis": [
        "ListWorkflow"
      ],
      "components": [
        "visibility",
        "basic-visibility",
        "query-api"
      ],
      "concepts": [
        "filtering",
        "time-range",
        "visibility-store",
        "query-syntax",
        "user-experience"
      ],
      "severity": "low",
      "userImpact": "Users find the BETWEEN-only filter syntax unintuitive and confusing when they expect standard comparison operators to work with the ListWorkflow API.",
      "rootCause": "Basic visibility API implementation only supports BETWEEN filter syntax, not individual comparison operators.",
      "proposedFix": "Add support for >, >=, <, <= comparison operators in basic visibility API filters.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Basic visibility APIs are being deprecated in favor of advanced visibility APIs. SQL visibility store now supports advanced visibility with full filter capabilities.",
      "related": [],
      "keyQuote": "No plan to improve basic visibility APIs. We decided to deprecate basic visibility API and unify on advanced visibility APIs.",
      "number": 2851,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:28.617Z"
    },
    {
      "summary": "The server loads all persistence libraries at startup regardless of which persistence backend is configured, causing unnecessary dependencies (like AWS libraries for RDS IAM auth) to be pulled in. The issue requests a mechanism to load only the required persistence libraries dynamically or at build time.",
      "category": "other",
      "subcategory": "persistence-plugin-loading",
      "apis": [],
      "components": [
        "persistence-layer",
        "plugin-system",
        "build-configuration"
      ],
      "concepts": [
        "lazy-loading",
        "dynamic-linking",
        "dependency-management",
        "plugin-architecture",
        "build-optimization",
        "configuration-driven"
      ],
      "severity": "medium",
      "userImpact": "Users are forced to include unnecessary dependencies in their deployments, increasing binary size, build time, and potential security surface area.",
      "rootCause": "Persistence libraries are loaded statically at startup rather than being conditionally loaded based on configured persistence backend.",
      "proposedFix": "Two options suggested: (1) Build persistence plugins as libraries and load dynamically at runtime based on persistence config, or (2) Use build flags to include only needed libraries at build time, resulting in different binaries per persistence type.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We need a better way to be able to load only needed libraries.",
      "number": 2849,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:16.042Z"
    },
    {
      "summary": "Child workflows started with abandon close policy are not started after parent workflow closes if the child start event hasn't been recorded yet. Users must explicitly wait for the start event before completing the parent to work around this limitation.",
      "category": "bug",
      "subcategory": "child-workflow-lifecycle",
      "apis": [
        "StartChildWorkflow",
        "SignalExternalWorkflow"
      ],
      "components": [
        "history-service",
        "transfer-queue-executor",
        "workflow-close-policy"
      ],
      "concepts": [
        "child-workflow",
        "abandon-policy",
        "workflow-closure",
        "event-recording",
        "async-start"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably start child workflows with abandon policy after parent closure, forcing workarounds to wait for start events.",
      "rootCause": "Child start is skipped when parent workflow is closed if the child started event has not been recorded, regardless of abandon close policy setting.",
      "proposedFix": "Modify transferQueueActiveTaskExecutor and transferQueueStandbyTaskExecutor to allow child start even after parent closure when abandon policy is set.",
      "workaround": "Explicitly wait for the child workflow started event before completing the parent workflow.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "if child is not started yet (meaning child started event not recorded), child start will be skipped even if the parent close policy is abandon",
      "number": 2829,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:15.068Z"
    },
    {
      "summary": "Direct queries are failing with 'Unknown query type' error after Continue-as-New, suggesting a race condition where queries arrive before the first workflow task is scheduled. This is a regression introduced between versions 1.14 and 1.15.2.",
      "category": "bug",
      "subcategory": "workflow-query",
      "apis": [],
      "components": [
        "query-dispatcher",
        "workflow-task-handler",
        "replay-executor"
      ],
      "concepts": [
        "race-condition",
        "direct-query",
        "continue-as-new",
        "workflow-task-scheduling",
        "query-buffering",
        "timing-issue"
      ],
      "severity": "high",
      "userImpact": "Users experience query failures immediately after workflow Continue-as-New, causing application errors and degraded functionality.",
      "rootCause": "Query is dispatched to worker before the first workflow task is scheduled after Continue-as-New, violating the guarantee that queries should be buffered as part of the first workflow task.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Closed by PR #2826",
      "related": [
        2300,
        2826
      ],
      "keyQuote": "the query is getting dispatched to the worker before scheduling a first Workflow Task after Continue-as-New. But this should never happen.",
      "number": 2824,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:16.712Z"
    },
    {
      "summary": "Prometheus histogram metrics are being registered with conflicting label sets, causing reporter errors. The issue stems from a tally scope using the root scope instead of a properly tagged child scope when reporting histogram metrics.",
      "category": "bug",
      "subcategory": "metrics-prometheus",
      "apis": [],
      "components": [
        "metrics",
        "prometheus-reporter",
        "tally-scope",
        "frontend-handler"
      ],
      "concepts": [
        "histogram",
        "metrics-registration",
        "label-conflict",
        "prometheus",
        "tally-scope"
      ],
      "severity": "medium",
      "userImpact": "Users running the Temporal server from the master branch encounter warning errors in the prometheus reporter due to conflicting histogram metric descriptors.",
      "rootCause": "The tally scope in workflowHandler.go line 3116 is using the root scope instead of the tagged child scope (with stats_type tag) to report the histogram, causing the metric to be registered with different label sets than previously registered metrics with the same name.",
      "proposedFix": "Use the properly tagged child scope instead of rootScope when reporting the histogram metric in the frontend workflowHandler.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The error from SizeStatsTypeTagValue has been fixed, likely by correcting the tally scope reference to use the tagged child scope.",
      "related": [],
      "keyQuote": "The root cause is tally scope is using rootScope instead of that tagged child scope to report the histogram.",
      "number": 2817,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:02.857Z"
    },
    {
      "summary": "The Slack channel invitation URL in the README is outdated and no longer active. Users clicking the link encounter a Slack error message.",
      "category": "docs",
      "subcategory": "readme",
      "apis": [],
      "components": [
        "documentation",
        "readme"
      ],
      "concepts": [
        "community",
        "communication",
        "links",
        "onboarding"
      ],
      "severity": "low",
      "userImpact": "Users cannot join the Slack community channel through the link provided in the README.",
      "rootCause": "Slack invitation link expired or was revoked",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The invalid Slack link was updated with a valid one",
      "related": [],
      "keyQuote": "Following the link leads to a Slack error message saying \"This link is no longer active\".",
      "number": 2812,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:03.477Z"
    },
    {
      "summary": "Worker in the Go tutorial times out with 'context deadline exceeded' error when running the sample application. The issue was caused by the tutorial using an outdated SDK version (v1.0.0) instead of a newer compatible version (1.14.0+).",
      "category": "bug",
      "subcategory": "tutorial-setup",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "task-queue"
      ],
      "concepts": [
        "timeout",
        "context-deadline",
        "version-compatibility",
        "sdk-upgrade",
        "task-processing"
      ],
      "severity": "medium",
      "userImpact": "Users following the official Go tutorial cannot complete the introductory sample due to SDK version incompatibility causing activity timeouts.",
      "rootCause": "Tutorial sample uses outdated SDK version (v1.0.0) instead of the minimum compatible version (1.14.0)",
      "proposedFix": "Upgrade temporal SDK-Go version to 1.14.0 or later; pull latest code from https://github.com/temporalio/money-transfer-project-template-go",
      "workaround": "Manually upgrade the SDK dependency to version 1.14.0 or later",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved by updating tutorial to use latest SDK version; repository updated to include correct dependencies",
      "related": [],
      "keyQuote": "After upgrade temporal sdk-go version to 1.14.0, it worked. Please pull latest code from https://github.com/temporalio/money-transfer-project-template-go",
      "number": 2805,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:33:04.457Z"
    },
    {
      "summary": "Optimize workflow task history by preventing repeated WorkflowTaskCompleted/Scheduled/Started events when a long-running local activity has no new commands or events to write.",
      "category": "feature",
      "subcategory": "workflow-task-heartbeat",
      "apis": [],
      "components": [
        "workflow-task-handler",
        "history-writer",
        "local-activity"
      ],
      "concepts": [
        "deduplication",
        "history-optimization",
        "event-batching",
        "long-running-activity",
        "heartbeat"
      ],
      "severity": "medium",
      "userImpact": "Users with very long-running local activities experience bloated workflow history due to repeated empty task events, impacting performance and storage.",
      "rootCause": "Repeated WorkflowTaskCompleted/Scheduled/Started events are written to history even when there are no commands or new events to include.",
      "proposedFix": "Skip writing these events if the WorkflowTaskCompleted has no commands and there are no new events, similar to existing deduplication for failing/timing out workflow tasks.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Do not write these events if there are no commands included in WorkflowTaskCompleted and there are no new events.",
      "number": 2800,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:51.354Z"
    },
    {
      "summary": "A test for deleting executions is failing intermittently with an activity execution error. The test expects specific byte output but receives nil, and the activity is calling runtime.Goexit, causing the workflow execution to fail.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "workflow-testsuite",
        "activity-executor",
        "internal-worker"
      ],
      "concepts": [
        "test-flakiness",
        "activity-execution",
        "runtime-error",
        "nil-value",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Developers experience intermittent test failures when testing deletion workflows with many executions, making test suites unreliable.",
      "rootCause": "Activity is calling runtime.Goexit during execution, causing unexpected nil return values and workflow execution failures in the test suite.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating a fix was applied to address the activity execution error and test flakiness.",
      "related": [],
      "keyQuote": "activity called runtime.Goexit (type: wrapError, retryable: true): unable to execute activity",
      "number": 2792,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:50.463Z"
    },
    {
      "summary": "Temporal Server enforces a hard 4MB limit on inbound gRPC messages, which is insufficient for large workflow task completions with many parallel activities and metadata markers. The request is to either lift this limit to match the maximum history size or make it configurable.",
      "category": "feature",
      "subcategory": "grpc-limits",
      "apis": [],
      "components": [
        "grpc-server",
        "workflow-task-execution",
        "history-persistence"
      ],
      "concepts": [
        "message-size-limit",
        "configuration",
        "scalability",
        "workflow-completion",
        "history-batch"
      ],
      "severity": "medium",
      "userImpact": "Users with large-scale workflows that spawn many parallel activities cannot complete workflow tasks if the serialized markers and metadata exceed 4MB, forcing workarounds or workflow restructuring.",
      "rootCause": "Fixed 4MB inbound gRPC message size limit without configurability or alignment with other system limits like max history size and transaction size.",
      "proposedFix": "Lift the incoming gRPC message limit to match max history size, or expose it as a configurable option. Ideally, coordinate this with transaction size limit and history batch size to derive all limits from a single max history size parameter.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The limit was relaxed to be configurable and aligned with other system limits.",
      "related": [],
      "keyQuote": "Limit for an incoming gRPC message to be lifted to a max history size. Or some percentage of max history size.",
      "number": 2786,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:52.428Z"
    },
    {
      "summary": "Race condition detected in shard state transitions during test execution, causing unexpected panics. The issue appears to stem from concurrent access to shard state during engine creation and gomock printing operations.",
      "category": "bug",
      "subcategory": "shard-controller",
      "apis": [],
      "components": [
        "shard-context",
        "engine-factory",
        "shard-controller"
      ],
      "concepts": [
        "race-condition",
        "concurrency",
        "locking",
        "state-transition",
        "test-artifacts",
        "synchronization"
      ],
      "severity": "medium",
      "userImpact": "Tests fail with panic on race condition detection; potential production issue if race condition exists in actual service code.",
      "rootCause": "Race between `s.state = contextStateStopped` and gomock attempting to print object values, likely because `createEngine` is called outside the read-write lock but callbacks from created components may access shard state requiring locks.",
      "proposedFix": "Fixed in PR #2879, which likely involved proper locking around engine creation or refactoring engine initialization to avoid concurrent state access.",
      "workaround": "Commenting out the unlock/lock sequence around engine creation eliminates the race in tests, though this may have unintended consequences for production behavior.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by #2879 according to final comment",
      "related": [
        2879
      ],
      "keyQuote": "The shard context explicitly calls `s.engineFactory.createEngine` outside of the rwlock, but there's no reason that should touch `s.state` directly.",
      "number": 2777,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:34.990Z"
    },
    {
      "summary": "User reports that ExecuteActivity fails with BadScheduleActivityAttributes error indicating missing StartToClose or ScheduleToCloseTimeout. Issue was resolved by the user, suggesting it was a configuration or usage problem rather than a framework bug.",
      "category": "question",
      "subcategory": "activity-timeouts",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "activity-executor",
        "workflow-engine"
      ],
      "concepts": [
        "timeout",
        "activity-attributes",
        "scheduling",
        "configuration",
        "error-handling"
      ],
      "severity": "low",
      "userImpact": "Activity execution fails with unclear error message when timeout attributes are not properly configured, making debugging difficult.",
      "rootCause": "Missing or invalid StartToClose or ScheduleToCloseTimeout configuration on activity command",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "User marked as resolved with comment 'solve, no bug', indicating they found the solution themselves, likely a configuration or usage issue.",
      "related": [],
      "keyQuote": "BadScheduleActivityAttributes: A valid StartToClose or ScheduleToCloseTimeout is not set on command.",
      "number": 2772,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:33.036Z"
    },
    {
      "summary": "Temporal fails to start when run inside a VSCode development container, unable to find dynamic config files due to path binding issues between the host and container environments.",
      "category": "bug",
      "subcategory": "docker-devcontainers",
      "apis": [],
      "components": [
        "dynamic-config",
        "docker-compose",
        "configuration-loader"
      ],
      "concepts": [
        "development-containers",
        "docker-in-docker",
        "path-binding",
        "configuration-resolution",
        "volume-mounting"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot use Temporal with VSCode development containers, blocking adoption of this development workflow.",
      "rootCause": "Path binding issue where docker-compose commands run from inside the dev container reference paths that don't exist in the docker daemon's context, typically '/workspace' is not accessible to the docker daemon.",
      "proposedFix": "Use fully qualified paths accessible to the docker daemon instead of relative paths, or use docker compose profiles to control service initialization.",
      "workaround": "Run docker-compose on the host instead of inside the dev container, or use Docker extension to manually manage container lifecycle.",
      "resolution": "stale",
      "resolutionDetails": "Closed with 'close-after-30-days' label, indicating it was closed due to inactivity rather than being fixed. This is a configuration/setup issue rather than a Temporal bug.",
      "related": [],
      "keyQuote": "Unable to create dynamic config client. Error: unable to validate dynamic config: config/dynamicconfig/development_sql_es.yaml: no such file or directory",
      "number": 2755,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:38.384Z"
    },
    {
      "summary": "Consistent query buffer overflow returns incorrect Unavailable error when workers fall behind on processing queries. The error was replaced with ResourceExhaustedError to better indicate worker capacity issues.",
      "category": "bug",
      "subcategory": "consistent-query",
      "apis": [],
      "components": [
        "worker",
        "query-processor",
        "error-handling"
      ],
      "concepts": [
        "query-processing",
        "buffer-overflow",
        "backpressure",
        "worker-capacity",
        "error-semantics"
      ],
      "severity": "medium",
      "userImpact": "Users receive incorrect error messages when the query buffer is full, making it difficult to diagnose whether the issue is worker slowness, worker being stuck, or concurrent query load.",
      "rootCause": "Worker not processing queries fast enough or being stuck causes the consistent query buffer to fill up, but the returned Unavailable error doesn't accurately represent this capacity issue.",
      "proposedFix": "Return ResourceExhaustedError instead of Unavailable error to better indicate that workers cannot keep up with query processing.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Replaced Unavailable error with ResourceExhaustedError to properly indicate worker capacity exhaustion when consistent query buffer is full.",
      "related": [],
      "keyQuote": "We now returns ResourceExhaustedError.",
      "number": 2749,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:20.686Z"
    },
    {
      "summary": "Proposal to use broadcast shutdown channel for shard lifecycle management. This is a refactoring to improve the shutdown mechanism for shard lifecycle management using a broadcast pattern instead of the current approach.",
      "category": "feature",
      "subcategory": "shard-lifecycle",
      "apis": [],
      "components": [
        "shard-lifecycle",
        "shutdown-channel",
        "broadcast-mechanism"
      ],
      "concepts": [
        "shutdown",
        "broadcast",
        "lifecycle-management",
        "shard",
        "channel",
        "graceful-shutdown"
      ],
      "severity": "medium",
      "userImpact": "Improves the reliability and efficiency of shard shutdown operations, potentially reducing resource contention and improving system stability during shutdown.",
      "rootCause": null,
      "proposedFix": "Use broadcast shutdown channel for shard lifecycle management as referenced in common/channel/shutdown_once.go",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Use broadcast shutdown channel for shard lifecycle management",
      "number": 2730,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:20.411Z"
    },
    {
      "summary": "Temporal server fails to connect to Cassandra cluster when a single node is down, despite having multiple nodes configured. The gocql driver appears unable to fall back to remaining healthy nodes even when multiple hosts are specified in the configuration.",
      "category": "bug",
      "subcategory": "cassandra-connectivity",
      "apis": [],
      "components": [
        "cassandra-client",
        "gocql-driver",
        "persistence-layer",
        "session-management"
      ],
      "concepts": [
        "cluster-failover",
        "connection-resilience",
        "multi-node-connectivity",
        "timeout-handling",
        "host-discovery",
        "circuit-breaker"
      ],
      "severity": "high",
      "userImpact": "Temporal server becomes unavailable when any Cassandra cluster node goes down, preventing normal operation and requiring manual intervention to recover.",
      "rootCause": "The gocql driver fails to properly implement connection fallback to remaining healthy nodes in a multi-node Cassandra cluster, possibly due to configuration parsing or reconnection logic issues.",
      "proposedFix": "Upgrade to gocql 1.4.0 or later (included in Temporal v1.21.5), which may address the reconnection handling between cluster nodes.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal is not able to connect to Cassandra even when ONE node is down. The temporal pods are not able to connect to remaining other two nodes.",
      "number": 2729,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:22.890Z"
    },
    {
      "summary": "Users cannot establish mTLS connections between Temporal and PostgreSQL through an HAProxy proxy. While plain TCP connections work and mTLS works with curl, temporal-sql-tool fails with a generic TLS handshake error without detailed diagnostic information.",
      "category": "bug",
      "subcategory": "database-connectivity",
      "apis": [],
      "components": [
        "sql-tool",
        "tls-client",
        "database-driver"
      ],
      "concepts": [
        "mTLS",
        "proxy",
        "database-connection",
        "tls-handshake",
        "certificate-validation",
        "connection-pooling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot securely connect Temporal to PostgreSQL through HAProxy proxies with mTLS, limiting deployment options in production environments.",
      "rootCause": "The temporal-sql-tool or underlying database driver may not properly handle TLS handshakes through intermediate proxies, or certificate validation issues specific to proxy scenarios.",
      "proposedFix": null,
      "workaround": "Use plain TCP connections from Temporal to HAProxy and configure HAProxy to handle the mTLS connection to PostgreSQL.",
      "resolution": "wontfix",
      "resolutionDetails": "Forum discussion indicated limitations in proxy TLS support; no fix was implemented.",
      "related": [],
      "keyQuote": "Is Temporal designed to communicate with a postgresql server through haproxy in mTLS?",
      "number": 2727,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:09.600Z"
    },
    {
      "summary": "Request to communicate to Workflows when they should continue as new to manage event history size. SDKs should provide both eventHistoryLength and a shouldContinueAsNew indicator, either computed by the SDK or sent by the Server based on history size and payload information.",
      "category": "feature",
      "subcategory": "workflow-lifecycle",
      "apis": [
        "isContinueAsNewNeeded"
      ],
      "components": [
        "workflow-runtime",
        "event-history",
        "sdk-core"
      ],
      "concepts": [
        "continue-as-new",
        "event-history-management",
        "workflow-continuation",
        "history-size-limits",
        "long-running-workflows",
        "performance-optimization"
      ],
      "severity": "medium",
      "userImpact": "Users writing long-running workflows lack guidance on when to continue as new, potentially leading to performance issues from unbounded event history growth.",
      "rootCause": null,
      "proposedFix": "SDKs should expose eventHistoryLength and shouldContinueAsNew flag to workflows, with the latter either computed by SDK (e.g., when history > 10,000 events) or communicated by the Server which has better visibility into history size and payload sizes.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented to communicate continue as new information to workers via SDK APIs",
      "related": [
        16
      ],
      "keyQuote": "We should give `Workflow.isContinueAsNewNeeded` flag to the SDKs. And it should be coming from the service.",
      "number": 2726,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:08.524Z"
    },
    {
      "summary": "Apache Thrift dependency (0.10.0) in ringpop-go contains 3 vulnerabilities (2 high severity with CVSS 7.5, 1 medium with CVSS 6.5) affecting availability and confidentiality. Issue was automatically managed by Mend security scanner through repeated closure and reopening based on dependency inventory status.",
      "category": "bug",
      "subcategory": "dependency-security-vulnerability",
      "apis": [],
      "components": [
        "ringpop-go",
        "dependency-management",
        "thrift-serialization"
      ],
      "concepts": [
        "security-vulnerability",
        "dependency-upgrade",
        "availability-impact",
        "dos-attack",
        "path-traversal",
        "protocol-handling"
      ],
      "severity": "high",
      "userImpact": "Applications using the vulnerable ringpop-go dependency are exposed to denial-of-service attacks and potential unauthorized file access via Apache Thrift vulnerabilities.",
      "rootCause": "Apache Thrift 0.10.0 contains three unpatched CVEs: CVE-2019-0205 (endless loop in protocol handling), CVE-2019-0210 (panic in Go JSON protocol with invalid input), and CVE-2018-11798 (directory traversal in Node.js web server).",
      "proposedFix": "Upgrade Apache Thrift to version 0.13.0 or later to resolve all three vulnerabilities.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Automatically closed by Mend when vulnerable library was marked as ignored or removed from inventory. The final closure indicates the vulnerability was either remediated by dependency updates or intentionally ignored.",
      "related": [],
      "keyQuote": "This issue was automatically closed by Mend because the vulnerable library in the specific branch(es) was either marked as ignored or it is no longer part of the Mend inventory.",
      "number": 2719,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:32:08.837Z"
    },
    {
      "summary": "Workflow.sleep(0) does not add an event to the workflow history in the Java SDK, which can cause nondeterminism errors when sleep duration is controlled by dynamic properties that change between 0 and non-zero values.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [
        "Workflow.sleep"
      ],
      "components": [
        "workflow-executor",
        "history-management",
        "replay-engine"
      ],
      "concepts": [
        "determinism",
        "history-events",
        "dynamic-configuration",
        "workflow-replay",
        "backwards-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users who use dynamic properties to control Workflow.sleep duration risk nondeterminism errors when the property value changes to or from 0.",
      "rootCause": "Workflow.sleep(0) is a no-op that does not generate a history event, unlike non-zero sleep durations, creating an inconsistency in replay behavior.",
      "proposedFix": "Add a history event for Workflow.sleep(0) to maintain determinism when sleep duration is controlled dynamically.",
      "workaround": "Use SideEffect or MutableSideEffect to fetch dynamic configuration to ensure replay safety instead of directly controlling sleep duration with dynamic values.",
      "resolution": "wontfix",
      "resolutionDetails": "The issue was closed as the team determined that adding an event for Workflow.sleep(0) would be a breaking change. The recommended approach is to use SideEffect/MutableSideEffect for dynamic configuration.",
      "related": [],
      "keyQuote": "If the property value is changed to or from 0, the workflow history will have changed and this causes nondeterminism errors.",
      "number": 2714,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:56.550Z"
    },
    {
      "summary": "User requested the ability to download workflow history from production and import it into a dev/test namespace for debugging and manual import scenarios.",
      "category": "feature",
      "subcategory": "workflow-history-management",
      "apis": [
        "ImportWorkflowExecution"
      ],
      "components": [
        "admin-api",
        "workflow-history",
        "database",
        "tdbg-tool"
      ],
      "concepts": [
        "workflow-debugging",
        "history-export",
        "history-import",
        "namespace-migration",
        "dev-test-workflow",
        "offline-analysis"
      ],
      "severity": "medium",
      "userImpact": "Developers can more easily debug production workflows and manually manage workflow history across different environments.",
      "rootCause": null,
      "proposedFix": "Use AdminAPI ImportWorkflowExecution or tdbg workflow import command",
      "workaround": "tdbg tool provides workflow import functionality with --input-filename option",
      "resolution": "fixed",
      "resolutionDetails": "Feature was already supported through AdminAPI ImportWorkflowExecution and tdbg tool's workflow import command",
      "related": [],
      "keyQuote": "This is supported with AdminAPI ImportWorkflowExecution, or tdbg tool",
      "number": 2708,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:54.114Z"
    },
    {
      "summary": "Request for an Admin CLI tool to update mutable state directly, such as workflow task timeout and task queue settings.",
      "category": "feature",
      "subcategory": "admin-cli",
      "apis": [],
      "components": [
        "admin-cli",
        "workflow-task",
        "mutable-state"
      ],
      "concepts": [
        "workflow-management",
        "state-mutation",
        "task-configuration",
        "timeout",
        "task-queue",
        "operational-tools"
      ],
      "severity": "medium",
      "userImpact": "Enables operators to directly modify workflow task settings via CLI without requiring code changes or manual intervention.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "e.g. update workflow task timeout\ne.g. update workflow task queue",
      "number": 2707,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:54.277Z"
    },
    {
      "summary": "FreeBSD build fails due to missing getSendQueueLen function in tchannel-go dependency. The tchannel-go library's Unix socket implementation doesn't support FreeBSD, causing compilation to fail when targeting FreeBSD.",
      "category": "bug",
      "subcategory": "build-system",
      "apis": [],
      "components": [
        "tchannel-go",
        "build-system",
        "platform-support"
      ],
      "concepts": [
        "cross-platform",
        "build-failure",
        "dependency-issue",
        "platform-specific-code",
        "FreeBSD-support"
      ],
      "severity": "low",
      "userImpact": "Users cannot build Temporal server for FreeBSD deployment, preventing FreeBSD adoption of the project.",
      "rootCause": "tchannel-go dependency has platform-specific Unix socket code (getSendQueueLen) that is not implemented for FreeBSD, only for Linux and other Unix-like systems.",
      "proposedFix": "Remove tchannel dependency (noted as planned removal by maintainers).",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers indicated plans to remove tchannel-go dependency entirely, making this a non-issue for future versions rather than fixing it in the current codebase.",
      "related": [],
      "keyQuote": "We plan to remove tchannel anyway.",
      "number": 2703,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:42.208Z"
    },
    {
      "summary": "Temporal Server logs every error response from the history client as an error-level message, causing excessive noise in logs with irrelevant stacktraces. These are typically client-side validation errors, not server problems, and should be logged at a lower level.",
      "category": "other",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "history-client",
        "metric-client",
        "logging"
      ],
      "concepts": [
        "log-level",
        "noise-reduction",
        "error-handling",
        "client-errors",
        "logging-configuration"
      ],
      "severity": "medium",
      "userImpact": "Operators see excessive error-level log messages from valid client-side errors, making it difficult to identify real server problems and reducing log clarity.",
      "rootCause": "History client metrics recording wrapper logs all errors at error level regardless of whether they are server-side or client-side validation errors.",
      "proposedFix": "Change the log level from 'error' to 'info' or 'warn' for history client errors in server logs, particularly for validation errors like BadSearchAttributes.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This should be a warn on error in the client/sdk logs, but I propose that the level of these messages in the server log should be decreased. It probably should be an info, not an error in server logs.",
      "number": 2700,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:44.267Z"
    },
    {
      "summary": "Internode TLS configuration uses a single certificate for both client and server authentication, and applies the same SAN across all services, creating security risks. This prevents using properly scoped certificates with specific key usage restrictions.",
      "category": "bug",
      "subcategory": "tls-configuration",
      "apis": [],
      "components": [
        "tls-configuration",
        "internode-communication",
        "frontend",
        "matching",
        "history"
      ],
      "concepts": [
        "mTLS",
        "certificate-validation",
        "key-usage",
        "SAN",
        "service-authentication",
        "security"
      ],
      "severity": "high",
      "userImpact": "Users cannot use properly scoped TLS certificates with specific key usage restrictions, forcing them to use overly permissive certificates that violate security best practices.",
      "rootCause": "Code assumes a single certificate for dual purpose (client and server) and uses the same SAN for all services, rather than supporting separate certificates and SANs per component.",
      "proposedFix": "Add separate CertFile/CertData and KeyFile/KeyData fields to ClientTLS configuration to allow distinct client and server certificates.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Ability to feed separate certificates for clients and their respective server certificates for each service with different server names for each component",
      "number": 2698,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:43.238Z"
    },
    {
      "summary": "Mismatch between workflow ID size limits: frontend service allows 1000 characters but MySQL/PostgreSQL database schemas only support 255 characters, causing insertion failures when the limit is configured higher than the database constraint.",
      "category": "bug",
      "subcategory": "database-schema",
      "apis": [],
      "components": [
        "frontend-service",
        "database-schema",
        "mysql",
        "postgresql"
      ],
      "concepts": [
        "workflow-id",
        "size-limit",
        "configuration",
        "database-constraint",
        "schema-mismatch"
      ],
      "severity": "high",
      "userImpact": "Users setting the workflow ID length limit to the default 1000 characters encounter database insertion errors because the underlying database tables are constrained to 255 characters.",
      "rootCause": "The frontend service business logic enforces a 1000 character maximum for workflow IDs, but the database schema only allocates 255 characters for the field, causing a constraint violation.",
      "proposedFix": "The fix must be applied via dynamic config to align the default configuration with the actual database constraint, as mentioned in a previous fix for issue #2122.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2122,
        2568
      ],
      "keyQuote": "I've left the default setting of `limit.maxIDLength = 1000` but observing the error `createOrUpdateCurrentExecution failed. Failed to insert into current_executions table. Error: pq: value too long for type character varying(255)`",
      "number": 2695,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:30.009Z"
    },
    {
      "summary": "Race condition when signaling current workflow during continue-as-new: signal caller gets stale run ID from DB, then sees the old workflow as completed while the actual current workflow has moved to a new run ID.",
      "category": "bug",
      "subcategory": "workflow-signaling",
      "apis": [],
      "components": [
        "history-engine",
        "workflow-execution",
        "signal-processing",
        "mutable-state"
      ],
      "concepts": [
        "race-condition",
        "continue-as-new",
        "signal-delivery",
        "run-id",
        "workflow-lock",
        "eventual-consistency"
      ],
      "severity": "high",
      "userImpact": "Signals intended for the current workflow may fail to deliver if the workflow performs a continue-as-new operation between the signal request lookup and delivery.",
      "rootCause": "Signal caller retrieves current run ID from DB, then acquires workflow lock using that run ID. If continue-as-new occurs between these steps, the caller sees the old run as completed and returns error without retrying with the new run ID.",
      "proposedFix": "Swap the if-block logic to check if workflow is continued-as-new and run ID is empty, returning Unavailable error to trigger retry rather than immediately returning WorkflowCompleted error.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed by query logic in history engine that gets current run ID from DB, locks execution, checks if workflow is running, and if not, queries current ID again to ensure it hasn't changed before returning error.",
      "related": [],
      "keyQuote": "If it's not running, the logic will query current runID again and make sure the current runID doesn't change before returning.",
      "number": 2694,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:31.554Z"
    },
    {
      "summary": "Search attribute validation incorrectly uses Payload metadata type instead of the actual Elasticsearch index type, causing validation to pass when it should fail and leading to internal server errors during indexing.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "search-attribute-validator",
        "payload-decoder",
        "elasticsearch-indexer"
      ],
      "concepts": [
        "type-validation",
        "metadata-handling",
        "search-attributes",
        "payload-encoding",
        "type-mismatch"
      ],
      "severity": "high",
      "userImpact": "Users receive false success responses when upserting search attributes with incorrect types, only to encounter internal Elasticsearch failures later.",
      "rootCause": "The DecodeValue function prioritizes the type metadata field in the Payload over the externally supplied type parameter, causing validation to use the wrong type target.",
      "proposedFix": "Either strip type metadata from Payload before validation or add a decoder mode that prefers the externally supplied type for validation purposes.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "MetadataType field has priority over passed type. Validator should cleanup type metadata from the Payload or use another mode for decoder.",
      "number": 2693,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:29.050Z"
    },
    {
      "summary": "Request to extend Temporal server APIs to support 'chain_start_run_id' as an alternative to 'run_id' for addressing the latest run in a workflow execution chain. This allows users to continue interacting with runs in the same logical workflow chain without accidentally switching to a new chain when the same workflow ID is reused.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "QueryWorkflow",
        "TerminateWorkflowExecution",
        "SignalWorkflowExecution"
      ],
      "components": [
        "server-api",
        "workflow-execution",
        "workflow-chain"
      ],
      "concepts": [
        "workflow-chain",
        "run-id",
        "workflow-id-reuse",
        "execution-chain",
        "latest-run",
        "chain-continuation"
      ],
      "severity": "medium",
      "userImpact": "Users can safely interact with a specific workflow chain without accidentally switching to a new chain when the same workflow ID is reused, preventing unintended operations on wrong workflow instances.",
      "rootCause": null,
      "proposedFix": "Extend all workflow APIs that address 'latest run' to accept optional 'chain_start_run_id' parameter, and return the 'chain_start_run_id' in response payloads so SDKs can track which chain a workflow belongs to.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        151,
        2608
      ],
      "keyQuote": "A chain represents a workflow with one or more runs. Once such a chain finishes, the workflow logically concludes. A new chain is a completely new workflow with the same workflow id.",
      "number": 2691,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:18.629Z"
    },
    {
      "summary": "User seeks guidance on deploying Temporal in production across 8 instances and encounters a ringpop bootstrap timeout error. The configuration attempts to set up a distributed system with separate frontend, matching, history, and worker services using MySQL persistence.",
      "category": "question",
      "subcategory": "deployment-configuration",
      "apis": [],
      "components": [
        "frontend",
        "matching",
        "history",
        "worker",
        "ringpop",
        "membership",
        "persistence"
      ],
      "concepts": [
        "production-deployment",
        "cluster-configuration",
        "ringpop-bootstrap",
        "membership",
        "distributed-system",
        "network-topology"
      ],
      "severity": "medium",
      "userImpact": "Users attempting production deployments of Temporal receive ringpop bootstrap timeout errors when configuring multi-instance clusters, preventing successful cluster formation.",
      "rootCause": "The broadcastAddress is set to 0.0.0.0 instead of the actual IP address, preventing proper ringpop peer discovery and joining.",
      "proposedFix": "Set global.membership.broadcastAddress to the actual IP address of the server instance (e.g., 10.114.1.1 for frontend instances) instead of 0.0.0.0.",
      "workaround": "Use 127.0.0.1 as the broadcastAddress, though this may not work for distributed deployments.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue was closed as it was identified as a documentation/configuration question rather than a bug. Community provided the solution (use actual IP for broadcastAddress) and redirected user to community forums.",
      "related": [],
      "keyQuote": "the broadcastAddress should be actual IP address",
      "number": 2690,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:17.271Z"
    },
    {
      "summary": "Request to add a SignalExternalWorkflowExecutionWithStart command that combines signaling an external workflow with starting it if it doesn't exist, eliminating the need for separate start and signal operations.",
      "category": "feature",
      "subcategory": "workflow-signaling",
      "apis": [
        "SignalExternalWorkflowExecution"
      ],
      "components": [
        "workflow-engine",
        "signal-dispatcher",
        "workflow-command-handler"
      ],
      "concepts": [
        "external-workflow-signaling",
        "workflow-lifecycle",
        "command-composition",
        "workflow-coordination",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Developers must write additional logic to ensure external workflows exist before signaling them, increasing complexity in cross-workflow communication patterns.",
      "rootCause": null,
      "proposedFix": "Add a new SignalExternalWorkflowExecutionWithStart command that atomically signals an external workflow and starts it if not already running.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "SignalExternalExecutionWithStart would make this experience a lot easier",
      "number": 2688,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:15.306Z"
    },
    {
      "summary": "Child workflow execution history does not populate the identity field in WorkflowExecutionStartedEventAttributes, unlike top-level workflows started from a client. The feature request asks to use the RespondWorkflowTaskCompletedRequest.identity value for child workflows to ensure consistency.",
      "category": "feature",
      "subcategory": "child-workflows",
      "apis": [
        "WorkflowExecutionStartedEventAttributes",
        "RespondWorkflowTaskCompletedRequest",
        "StartWorkflowExecutionRequest"
      ],
      "components": [
        "history",
        "workflow-service",
        "child-workflow-execution"
      ],
      "concepts": [
        "identity",
        "child-workflow",
        "workflow-history",
        "event-attributes",
        "client-identity",
        "consistency"
      ],
      "severity": "low",
      "userImpact": "Users cannot identify which SDK client initiated child workflows because the identity field remains empty in child workflow history events.",
      "rootCause": "The server does not populate the identity field from RespondWorkflowTaskCompletedRequest when creating WorkflowExecutionStartedEventAttributes for child workflows.",
      "proposedFix": "Use temporal.api.workflowservice.v1.RespondWorkflowTaskCompletedRequest.identity for the child workflow identity in WorkflowExecutionStartedEventAttributes, matching the identity set by SDKs on StartWorkflowExecutionRequest.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "identity is populated with the client identity when started from a client, but not populated with a value when it's a child workflow",
      "number": 2687,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:03.407Z"
    },
    {
      "summary": "User unable to enable S3 archival for history and visibility through tctl commands and namespace updates. Configuration appears correct but results in DeadlineExceeded and BadRequest errors when attempting to activate archival with S3 URIs.",
      "category": "bug",
      "subcategory": "archival-s3",
      "apis": [],
      "components": [
        "archival",
        "s3-store",
        "namespace-manager",
        "tctl"
      ],
      "concepts": [
        "archival",
        "s3-configuration",
        "namespace-activation",
        "timeout",
        "uri-parsing",
        "credentials"
      ],
      "severity": "medium",
      "userImpact": "Users cannot enable S3-based archival for history and visibility, blocking the ability to archive workflow data to cloud storage.",
      "rootCause": "Likely S3 configuration parsing issue or timeout during archival setup; user identified bad S3 config and timeouts as contributing factors",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "User identified the issue as bad S3 configuration and timeouts on their end rather than a product bug",
      "related": [],
      "keyQuote": "bad s3 config + some timeouts.",
      "number": 2686,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:03.077Z"
    },
    {
      "summary": "Proposal for a new way to write Temporal workflows using WorkflowState definitions instead of traditional replay-safe workflow code, inspired by AWS StepFunctions but built on Temporal to reduce developer complexity and eliminate non-deterministic errors.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "workflow",
        "activity",
        "signal",
        "timer"
      ],
      "concepts": [
        "replayability",
        "state-machine",
        "workflow-abstraction",
        "non-determinism",
        "developer-experience",
        "code-generation"
      ],
      "severity": "medium",
      "userImpact": "Provides developers with a simpler, safer way to write Temporal workflows without needing to understand replay semantics, reducing cognitive load and common errors.",
      "rootCause": "Current Temporal workflow development requires understanding replay safety and determinism, which is counter to typical programming patterns and causes difficulty for engineers.",
      "proposedFix": "Implement a WorkflowState-based DSL where users define workflow logic through state definitions with requestPrecondition and decideNextStates callbacks, executed as activities within a single framework-provided workflow.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Instead of writing the Temporal workflow, we let users write workflow in code by defining a set of **WorkflowState**.",
      "number": 2685,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:31:04.518Z"
    },
    {
      "summary": "Temporal fails to create workflows with the same ID when using ScyllaDB as persistence. The server enters a high CPU loop (>200%) and becomes unresponsive. Root cause: ScyllaDB returns null-filled rows in batch results (unlike Cassandra), which are incorrectly interpreted as shard ownership errors.",
      "category": "bug",
      "subcategory": "persistence-scylla",
      "apis": [],
      "components": [
        "persistence",
        "cassandra-driver",
        "history-engine",
        "workflow-execution"
      ],
      "concepts": [
        "persistence",
        "scylla",
        "batch-operations",
        "error-handling",
        "shard-ownership",
        "row-scanning"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal with ScyllaDB cannot execute workflows with repeated IDs, causing server hangs and making the system unusable.",
      "rootCause": "ScyllaDB batch result iterator returns rows for all queries in batch (including null-filled rows), whereas Cassandra only returns rows with values. Error detection helpers scan the 'type' column as non-nullable int, defaulting nulls to 0 (rowTypeShard) and falsely detecting shard ownership loss.",
      "proposedFix": "Scan 'type' column as nullable (*int) and check if it is actually set before processing the row as an error result.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by handling nullable columns in batch result scanning. PR #3027 was submitted to address the issue.",
      "related": [
        3027
      ],
      "keyQuote": "Scylla returns 5 rows (as we have 5 queries in batch) with only 2 containing values and others with all columns nulls",
      "number": 2683,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:49.938Z"
    },
    {
      "summary": "Child workflow events at ID=1 show an empty identity value even when ClientOptions.Identity is configured, while other event IDs display the identity correctly.",
      "category": "bug",
      "subcategory": "child-workflows",
      "apis": [
        "StartChildWorkflowExecution"
      ],
      "components": [
        "child-workflow",
        "event-history",
        "client-options"
      ],
      "concepts": [
        "identity",
        "workflow-history",
        "event-metadata",
        "child-workflow-execution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot track the identity of child workflow initiators in workflow history because the first event in child workflow logs shows empty identity despite configuring it in ClientOptions.",
      "rootCause": "Child workflow event ID=1 does not properly propagate the identity parameter from ClientOptions, while subsequent events handle it correctly.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed as duplicate of issue #2687 which provides clearer reproduction steps for the same identity parameter bug in child workflows.",
      "related": [
        2687
      ],
      "keyQuote": "for each child workflow event id=1 in that identity is shown empty(identity=\"\") . but for other event id it's showing correct identity",
      "number": 2682,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:49.797Z"
    },
    {
      "summary": "Fresh Temporal installation with Elasticsearch fails to list workflows due to mismatched Elasticsearch index template. The index is created with default schema instead of the required template, causing RunId field to be Text type instead of Keyword, breaking sorting operations.",
      "category": "bug",
      "subcategory": "elasticsearch-visibility",
      "apis": [],
      "components": [
        "elasticsearch",
        "visibility",
        "cli"
      ],
      "concepts": [
        "index-template",
        "schema-mismatch",
        "elasticsearch-configuration",
        "visibility-store",
        "workflow-listing"
      ],
      "severity": "high",
      "userImpact": "Users cannot list or manage workflows on fresh installations when using Elasticsearch, blocking basic operational tasks.",
      "rootCause": "Elasticsearch index name does not match the index_patterns in the visibility index template, causing Elasticsearch to create the index with default schema where RunId is Text instead of Keyword type, breaking sort operations required for ListClosedWorkflowExecutions.",
      "proposedFix": "Upload modified index schema template matching the custom index name or change index name to match the pattern in the template. Delete existing index to allow Elasticsearch to recreate it with proper schema.",
      "workaround": "Rename Elasticsearch index to match the default pattern or manually upload a modified index template before creating the index.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was caused by incorrect Elasticsearch index configuration. Solution provided in comments by maintainer to use proper index naming or upload modified template.",
      "related": [
        2649
      ],
      "keyQuote": "Index name should match one of `index_patterns` specified in index_template_v7.json. You index name doesn't match it and therefore it was created without applying proper template.",
      "number": 2671,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:50.238Z"
    },
    {
      "summary": "Environment variable POSTGRES_SEEDS is poorly named and undocumented, causing confusion about PostgreSQL database connection configuration. The issue requests either better naming or comprehensive documentation of Docker Compose configuration.",
      "category": "docs",
      "subcategory": "configuration-documentation",
      "apis": [],
      "components": [
        "postgres-connector",
        "docker-compose",
        "environment-configuration",
        "server-setup"
      ],
      "concepts": [
        "environment-variables",
        "database-connection",
        "naming-clarity",
        "documentation",
        "docker-setup",
        "configuration-management",
        "developer-experience"
      ],
      "severity": "medium",
      "userImpact": "Users struggle to understand and configure PostgreSQL database connections due to unclear environment variable names and missing documentation, leading to significant setup frustration.",
      "rootCause": "Environment variable POSTGRES_SEEDS has a misleading name that doesn't clearly indicate it's for PostgreSQL connection host configuration, combined with lack of centralized documentation.",
      "proposedFix": "Either rename the environment variable to something more descriptive (e.g., POSTGRES_HOST or POSTGRES_CONNECTION_HOST) or comprehensively document all Docker environment variables in a single location.",
      "workaround": "Search the codebase for POSTGRES_SEEDS usage to understand the configuration options, or refer to Docker Hub server image documentation.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I have finally realized after searching this codebase for `POSTGRES_SEEDS` that it is actually being used as \"postgres connection host\".",
      "number": 2668,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:35.726Z"
    },
    {
      "summary": "SQLite build dependency breaks CGO-disabled builds and test execution. Tests fail to compile when CGO_ENABLED=0 due to sqlite package build constraints, despite the main binary building successfully.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "persistence",
        "sql-plugin",
        "sqlite",
        "build-system"
      ],
      "concepts": [
        "cgo-dependency",
        "build-constraints",
        "cross-platform-build",
        "windows-compatibility",
        "test-infrastructure"
      ],
      "severity": "medium",
      "userImpact": "Users unable to build and run tests on Windows without a CGO-compatible C/C++ compiler, blocking pure Go builds and cross-compilation scenarios.",
      "rootCause": "SQLite driver package imported by test code is guarded by CGO build constraints, making it unavailable when CGO_ENABLED=0, but persistence-tests package unconditionally imports it.",
      "proposedFix": "Apply build constraints to test components that depend on sqlite, or migrate to a CGO-free SQLite implementation like modernc.org/sqlite.",
      "workaround": "Enable CGO and install a compatible C/C++ compiler for Windows builds.",
      "resolution": "fixed",
      "resolutionDetails": "Moved to modernc.org/sqlite library as mentioned in comment; race detector requirement no longer blocks CGO-free builds.",
      "related": [],
      "keyQuote": "Opting out from CGO intentionally turns off sqlite support, so there is probably no personal need to test the sqlite driver package.",
      "number": 2657,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:37.194Z"
    },
    {
      "summary": "Minimal test issue with no description. The issue and single comment both contain only the word 'test', providing no context about what is being tested or requested.",
      "category": "other",
      "subcategory": "test",
      "apis": [],
      "components": [],
      "concepts": [
        "testing",
        "validation",
        "verification"
      ],
      "severity": "low",
      "userImpact": "Unclear due to lack of description; appears to be a placeholder or incomplete issue submission.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue closed as invalid due to insufficient information and no actionable content.",
      "related": [],
      "keyQuote": "test",
      "number": 2656,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:34.230Z"
    },
    {
      "summary": "The sendArchiveSignal timeout in the archiver client is hard-coded to 300ms, causing archival failures for users with slower networks. The request is to increase the default timeout and make it configurable through dynamic config.",
      "category": "feature",
      "subcategory": "archiver",
      "apis": [],
      "components": [
        "archiver",
        "archiver-client",
        "worker",
        "dynamic-config"
      ],
      "concepts": [
        "timeout",
        "archival",
        "signal",
        "configurability",
        "network-latency"
      ],
      "severity": "medium",
      "userImpact": "Users experiencing archival failures due to timeout are forced to rebuild from source to increase the timeout value.",
      "rootCause": "Hard-coded 300ms timeout in sendArchiveSignal is insufficient for networks with higher latency or under load.",
      "proposedFix": "Increase default sendArchiveSignal timeout and make it configurable via dynamic config.",
      "workaround": "Rebuild temporal from source with modified timeout value.",
      "resolution": "fixed",
      "resolutionDetails": "Default timeout was increased and made configurable through dynamic config.",
      "related": [],
      "keyQuote": "Users run into issues with it being too short and have to increase it and rebuild from source",
      "number": 2655,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:24.346Z"
    },
    {
      "summary": "Fresh Temporal install with Elasticsearch fails on workflow list operations with 'context deadline exceeded' and 'sql: no rows in result set' errors. The Elasticsearch schema appears properly created but queries still fail.",
      "category": "bug",
      "subcategory": "visibility-store",
      "apis": [],
      "components": [
        "visibility-store",
        "elasticsearch",
        "workflow-list"
      ],
      "concepts": [
        "schema-initialization",
        "elasticsearch-query",
        "timeout",
        "workflow-visibility",
        "persistence"
      ],
      "severity": "high",
      "userImpact": "Users cannot list workflows or terminate workflows after fresh installation, blocking basic workflow operations.",
      "rootCause": "Likely Elasticsearch index schema mismatch or query timeout despite index existing, possibly related to timing of index creation versus query execution.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue appears to be resolved through schema initialization process; marked as user configuration/setup issue rather than code defect.",
      "related": [],
      "keyQuote": "You didn't create schema in Elasticsearch. How did you deploy your cluster? If you don't use `auto-setup` image or helm charts, you need to run similar steps manually",
      "number": 2649,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:24.055Z"
    },
    {
      "summary": "Cron workflow encounters cascading failures after worker crash, resulting in \"unable to fetch tasks: too many workflows found\" errors on subsequent executions. The error count increments with each retry attempt.",
      "category": "bug",
      "subcategory": "cron-workflows",
      "apis": [],
      "components": [
        "cron-scheduler",
        "worker",
        "task-fetcher",
        "workflow-execution"
      ],
      "concepts": [
        "cron-schedule",
        "worker-crash",
        "task-fetching",
        "error-escalation",
        "workflow-retry",
        "memory-error"
      ],
      "severity": "high",
      "userImpact": "Cron workflows permanently fail after any worker crash, preventing scheduled task execution from recovering.",
      "rootCause": "Worker crash (nil pointer dereference) during cron workflow execution leaves orphaned or corrupted workflow executions, causing task fetching to fail with increasing counts on retry attempts.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue author determined the problem was on their side, not a Temporal bug.",
      "related": [],
      "keyQuote": "bad request - unable to fetch tasks: bad request - too many (2) workflows found",
      "number": 2631,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:23.022Z"
    },
    {
      "summary": "User requests support for DNS names in BroadcastAddress configuration for Docker Swarm deployments. Currently only IP addresses are supported due to validation in ringpop.go, which limits flexibility in containerized environments where DNS-based service discovery would be beneficial.",
      "category": "feature",
      "subcategory": "docker-deployment",
      "apis": [],
      "components": [
        "ringpop",
        "membership-coordination",
        "network-configuration"
      ],
      "concepts": [
        "dns-resolution",
        "docker-swarm",
        "service-discovery",
        "broadcast-address",
        "network-binding",
        "containerization"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to deploy Temporal in Docker Swarm mode face limitations requiring either complex static IP allocation, Kubernetes adoption, or DNS workarounds at the container level.",
      "rootCause": "Validation in ringpop.go:75 restricts BroadcastAddress to IP addresses only, not supporting DNS names which would simplify containerized deployments.",
      "proposedFix": "Allow BroadcastAddress to accept DNS names in addition to IP addresses; ensure BIND_ON_IP defaults to 0.0.0.0 for all interfaces.",
      "workaround": "Resolve DNS names to IP addresses in entrypoint.sh using `hostname -i` command before setting TEMPORAL_BROADCAST_ADDRESS environment variable.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "TEMPORAL_BROADCAST_ADDRESS can be dns name... can not understand reason for this restriction",
      "number": 2630,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:11.306Z"
    },
    {
      "summary": "Service errors are currently reported as separate metrics for each error type, making it difficult to query all service errors together for dashboards. The proposal is to create a single metric with error type as a tag instead.",
      "category": "other",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "service-error-handling"
      ],
      "concepts": [
        "monitoring",
        "observability",
        "metrics-aggregation",
        "error-tracking",
        "dashboard-queries",
        "metric-tagging"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily create dashboards that aggregate all service errors across different error types without writing complex queries for each metric individually.",
      "rootCause": "Metrics are structured as separate metrics per error type rather than using tags for error type differentiation.",
      "proposedFix": "Create a new metric (service_errors_with_type) that reports all service errors with error_type as a tag, deprecate old error-specific metrics in v1.17, and remove them in v1.18.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "New metric created with error_type tag and deprecation plan established for v1.17 with removal in v1.18.",
      "related": [],
      "keyQuote": "Should consider having one metric for all service_errors with error type tag for each error type.",
      "number": 2628,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:11.955Z"
    },
    {
      "summary": "The TLS enable environment variable names are inconsistent between temporal-sql-tool (SQL_TLS) and auto-setup config template (SQL_TLS_ENABLED), causing user confusion. The issue requests aligning these two environment variable names.",
      "category": "other",
      "subcategory": "configuration",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "config-template",
        "auto-setup"
      ],
      "concepts": [
        "TLS",
        "environment-variables",
        "configuration",
        "consistency",
        "naming"
      ],
      "severity": "low",
      "userImpact": "Users are confused by inconsistent TLS environment variable names when configuring Temporal with SQL.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2293
      ],
      "keyQuote": "This causes some confusion for our users. Would be nice to align these two.",
      "number": 2621,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:10.505Z"
    },
    {
      "summary": "Feature request to allow retention period to be configured per workflow completion type (e.g., different retention for completed vs failed workflows) rather than applying a single retention period to all workflows.",
      "category": "feature",
      "subcategory": "retention-policy",
      "apis": [],
      "components": [
        "namespace-management",
        "tctl",
        "retention-service"
      ],
      "concepts": [
        "retention-period",
        "workflow-completion",
        "data-governance",
        "namespace-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot optimize data retention strategies for different workflow outcomes, leading to inefficient storage utilization or compliance challenges.",
      "rootCause": null,
      "proposedFix": "Add workflow completion type parameter to tctl namespace update --retention command to allow fine-grained retention configuration per completion state.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Allow to set workflow completion type when updating retention for a namespace in tctl namespace update --retention X",
      "number": 2617,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:30:00.040Z"
    },
    {
      "summary": "The auto-setup.sh script fails to pass the DB_PORT parameter to temporal-sql-tool when setting up MySQL and Cassandra databases, causing connection failures when a non-default port is specified.",
      "category": "bug",
      "subcategory": "database-setup",
      "apis": [],
      "components": [
        "auto-setup",
        "temporal-sql-tool",
        "mysql-setup",
        "cassandra-setup"
      ],
      "concepts": [
        "database-configuration",
        "connection-parameters",
        "port-configuration",
        "setup-script",
        "database-initialization"
      ],
      "severity": "medium",
      "userImpact": "Users cannot start the Temporal server with a custom database port, blocking server initialization in environments that don't use default ports.",
      "rootCause": "The temporal-sql-tool invocations in auto-setup.sh do not include the DB_PORT parameter in the endpoint specification.",
      "proposedFix": "Add DB_PORT parameter to all temporal-sql-tool commands in the auto-setup.sh script for MySQL and Cassandra database setup.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "auto-setup.sh misses the DB_PORT parameter in mysql and cassandra connection",
      "number": 2615,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:57.956Z"
    },
    {
      "summary": "Request to transfer child workflows to a new parent run when the parent workflow calls continue as new. Currently, child workflows are not transferred and instead follow the parentClosePolicy, limiting use cases like buildQueue patterns where child workflows need to be managed across parent workflow continuations.",
      "category": "feature",
      "subcategory": "workflow-continuation",
      "apis": [
        "ContinueAsNew",
        "ExecuteChildWorkflow",
        "StartChildWorkflow"
      ],
      "components": [
        "workflow-executor",
        "parent-child-relationship",
        "continue-as-new-handler"
      ],
      "concepts": [
        "workflow-continuation",
        "child-workflow-transfer",
        "parent-close-policy",
        "workflow-lifecycle",
        "state-preservation",
        "dynamic-workflow-patterns"
      ],
      "severity": "medium",
      "userImpact": "Users cannot maintain child workflow handles when a parent workflow calls continue as new, forcing them to implement workarounds like polling or bidirectional signaling.",
      "rootCause": "The continue as new mechanism does not preserve or transfer child workflow relationships to the new parent run, following standard parentClosePolicy instead.",
      "proposedFix": null,
      "workaround": "Implement bidirectional signaling between workflows or use polling patterns to track child workflow status, though this can lead to recursive signaling issues.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        373
      ],
      "keyQuote": "We can't move the handle over (aka just Promise.any await on the number of builds), the top level buildQueue just kinda has to poll the child workflows on an interval",
      "number": 2609,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:57.786Z"
    },
    {
      "summary": "Workflow execution history was not displaying in the Temporal UI after restarting a Cassandra-backed setup with Persistence Volume Claims, despite the data being retained in the database. The issue was resolved by correcting the volume mount path from 'var/lib/elasticsearch' to '/usr/share/elasticsearch/data'.",
      "category": "bug",
      "subcategory": "persistence-visibility",
      "apis": [],
      "components": [
        "cassandra",
        "persistence",
        "visibility-store",
        "ui"
      ],
      "concepts": [
        "volume-mounting",
        "data-persistence",
        "visibility-indexing",
        "kubernetes-statefulset",
        "elasticsearch-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users lose visibility of workflow executions in the UI after restarting their Temporal setup with persistent storage, even though the data is preserved in the database.",
      "rootCause": "Incorrect volume mount path configuration for Elasticsearch (/var/lib/elasticsearch instead of /usr/share/elasticsearch/data) prevented the visibility store from accessing persisted data.",
      "proposedFix": "Change the volume mount path in the Kubernetes StatefulSet from 'var/lib/elasticsearch' to '/usr/share/elasticsearch/data'.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "User resolved the issue by correcting the Elasticsearch volume mount path in their Kubernetes configuration.",
      "related": [],
      "keyQuote": "The volume mount path needs to be changed from \"var/lib/elasticsearch\" to \"/usr/share/elasticsearch/data\".",
      "number": 2603,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:46.728Z"
    },
    {
      "summary": "Sticky queue assignment should be reset to 'Normal' kind after a configurable timeout period without workflow task activity. Currently, workflow tasks scheduled after extended inactivity still use sticky queue designation intended for recent worker assignments.",
      "category": "feature",
      "subcategory": "workflow-task-scheduling",
      "apis": [],
      "components": [
        "sticky-queue",
        "workflow-task-scheduler",
        "worker-assignment"
      ],
      "concepts": [
        "sticky-queue",
        "worker-affinity",
        "timeout",
        "inactivity",
        "task-scheduling"
      ],
      "severity": "medium",
      "userImpact": "Workflow tasks may be routed to stale or outdated workers during periods of inactivity, potentially causing task failures or delays.",
      "rootCause": "Sticky queue designation persists indefinitely without considering inactivity periods, leading to task routing to potentially unavailable workers.",
      "proposedFix": "Implement a configurable timeout mechanism to automatically reset sticky queue to 'Normal' kind when the time since the last workflow task completion exceeds a threshold.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #2363, which addresses the same sticky queue timeout problem.",
      "related": [
        2363
      ],
      "keyQuote": "If a workflow task is scheduled after a specified (configurable?) time after a previous workflow task then sticky queue shouldn't be used (it should reset to \"Normal\" kind).",
      "number": 2600,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:46.839Z"
    },
    {
      "summary": "Users need the ability to configure specific times when history and visibility data archival should occur, rather than happening unpredictably at workflow retention time. This would allow scheduling archival during low-production periods to minimize resource impact.",
      "category": "feature",
      "subcategory": "archival-configuration",
      "apis": [],
      "components": [
        "archival",
        "visibility",
        "history",
        "retention"
      ],
      "concepts": [
        "archival-scheduling",
        "resource-management",
        "production-load",
        "configuration",
        "data-retention",
        "performance-optimization"
      ],
      "severity": "medium",
      "userImpact": "Users cannot control when resource-intensive archival operations occur, limiting their ability to optimize performance during production hours.",
      "rootCause": null,
      "proposedFix": "Add configuration options to specify specific times or time durations when archival should be performed.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would be helpful if users had the ability to configure a specific time or a time duration when archival should be performed",
      "number": 2598,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:44.524Z"
    },
    {
      "summary": "Remove the DisableHealthCheck option from Go SDK client.ConnectionOptions as the SDK is replacing health checks with server-info calls, making eager connectivity unavoidable. The client should not instantiate until it can reach a server.",
      "category": "feature",
      "subcategory": "client-connection",
      "apis": [],
      "components": [
        "client",
        "connection",
        "health-check"
      ],
      "concepts": [
        "connectivity",
        "health-check",
        "server-info",
        "client-initialization",
        "eager-connectivity"
      ],
      "severity": "medium",
      "userImpact": "Users need to update their Go SDK integration to remove the deprecated DisableHealthCheck option and adapt to new eager connectivity behavior.",
      "rootCause": "Go SDK replaced health check mechanism with get-server-info call, eliminating the ability to defer connectivity checks",
      "proposedFix": "Remove the disable health check option and delay client instantiation until server connectivity can be established with retry logic",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "DisableHealthCheck option was removed from Go SDK client.ConnectionOptions as planned, aligning with the new server-info based connectivity model",
      "related": [],
      "keyQuote": "Remove the disable health check option in use and do not instantiate a client until it can reach a server (and/or retry as needed)",
      "number": 2597,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:34.246Z"
    },
    {
      "summary": "Add a new WorkflowIDReusePolicyTerminateIfRunning policy that allows starting a workflow while automatically terminating any existing workflow with the same ID. This is useful during development and for cron workflows.",
      "category": "feature",
      "subcategory": "workflow-reuse-policy",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-engine",
        "reuse-policy",
        "workflow-termination"
      ],
      "concepts": [
        "workflow-id-reuse",
        "policy",
        "termination",
        "cron-workflows",
        "development"
      ],
      "severity": "medium",
      "userImpact": "Users can now automatically terminate existing workflows when starting new ones with the same ID, enabling cleaner development workflows and cron job patterns.",
      "rootCause": null,
      "proposedFix": "Implement WorkflowIDReusePolicyTerminateIfRunning as a new reuse policy option similar to Cadence's implementation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented and merged as requested.",
      "related": [],
      "keyQuote": "Sometimes it is necessary to start workflow terminating any workflow which is already running. This is especially useful during development and for corn workflows.",
      "number": 2593,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:34.809Z"
    },
    {
      "summary": "Change the GetTasks range matching from [Inclusive, Inclusive] to [Inclusive, Exclusive) to align with common programming conventions and improve API consistency.",
      "category": "feature",
      "subcategory": "api-design",
      "apis": [
        "GetTasks"
      ],
      "components": [
        "task-matching",
        "api-design",
        "range-semantics"
      ],
      "concepts": [
        "range-bounds",
        "inclusive-exclusive",
        "api-consistency",
        "semantic-clarity"
      ],
      "severity": "medium",
      "userImpact": "Users would experience more intuitive and consistent range behavior when querying tasks, aligning with standard programming conventions.",
      "rootCause": null,
      "proposedFix": "Modify GetTasks range matching to use [Inclusive, Exclusive) bounds instead of [Inclusive, Inclusive]",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The GetTasks API range matching was updated to use [Inclusive, Exclusive) semantics",
      "related": [],
      "keyQuote": "Change matching GetTasks range to be [Inclusive, Exclusive)",
      "number": 2587,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:33.097Z"
    },
    {
      "summary": "Request to add a gRPC health check handler to the worker service, similar to matching and history services. Discussion reveals design challenges around what \"healthy\" means for application workers with context-specific requirements.",
      "category": "feature",
      "subcategory": "worker-health-check",
      "apis": [],
      "components": [
        "worker",
        "health-check",
        "grpc-handler"
      ],
      "concepts": [
        "health-check",
        "service-readiness",
        "worker-connectivity",
        "application-health",
        "resource-availability",
        "monitoring"
      ],
      "severity": "low",
      "userImpact": "Users must manually implement custom health check endpoints for worker services, while other Temporal services have built-in handlers.",
      "rootCause": "Health status is application-specific and context-dependent; no universal definition exists for what constitutes a healthy worker.",
      "proposedFix": "Add a flag during worker initialization to enable a built-in health check server, providing sensible defaults that users can customize.",
      "workaround": "Implement a custom HTTP health check server that spins up after the worker connects to Temporal, similar to existing SDK examples.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I suspect lots of users would be happy if there was a flag we could set during worker initiation that would handle this for us.",
      "number": 2582,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:23.579Z"
    },
    {
      "summary": "Add per-namespace rate limiting for Visibility APIs to protect shared backend resources from being overwhelmed by requests across multiple namespaces. The feature should include both RPS limits and concurrent request limits.",
      "category": "feature",
      "subcategory": "visibility-api",
      "apis": [],
      "components": [
        "visibility-api",
        "namespace",
        "rate-limiter"
      ],
      "concepts": [
        "rate-limiting",
        "resource-protection",
        "concurrency",
        "namespace-isolation",
        "backend-resources"
      ],
      "severity": "high",
      "userImpact": "Without rate limiting, visibility API requests from one namespace can impact the stability and performance of the entire backend system.",
      "rootCause": "Visibility APIs share backend resources among all namespaces without per-namespace rate limiting protection.",
      "proposedFix": "Implement per-namespace RPS limit and concurrent request limit for Visibility APIs.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Completed via PR #2585",
      "related": [
        2585
      ],
      "keyQuote": "We need per namespace rate limiting for Visibility APIs to protect backend resources as they are shared among all namespaces.",
      "number": 2577,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:21.131Z"
    },
    {
      "summary": "TerminatedFailureInfo is missing a `details` field that exists in the corresponding WorkflowExecutionTerminatedEventAttributes proto message. Request to add this field for consistency.",
      "category": "feature",
      "subcategory": "api-consistency",
      "apis": [
        "TerminatedFailureInfo",
        "WorkflowExecutionTerminatedEventAttributes"
      ],
      "components": [
        "api",
        "proto-definitions",
        "failure-handling"
      ],
      "concepts": [
        "proto-consistency",
        "terminated-workflows",
        "failure-details",
        "event-attributes"
      ],
      "severity": "low",
      "userImpact": "Users cannot access termination details through TerminatedFailureInfo, limiting visibility into workflow termination reasons.",
      "rootCause": null,
      "proposedFix": "Add `details` field to TerminatedFailureInfo proto message to match WorkflowExecutionTerminatedEventAttributes.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Field was added to TerminatedFailureInfo to match the proto structure.",
      "related": [
        150
      ],
      "keyQuote": "TerminatedFailureInfo.details doesn't. Is that intentional?",
      "number": 2566,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:22.145Z"
    },
    {
      "summary": "Request to add an option for bidirectional gRPC streaming instead of unary RPC for worker-frontend communication. This would provide flexibility for traffic management solutions and connection policies that may benefit from streaming instead of long polling.",
      "category": "feature",
      "subcategory": "grpc-streaming",
      "apis": [],
      "components": [
        "worker",
        "frontend",
        "grpc",
        "connection-management"
      ],
      "concepts": [
        "streaming",
        "bidirectional-communication",
        "traffic-management",
        "load-balancing",
        "connection-policies",
        "long-polling"
      ],
      "severity": "low",
      "userImpact": "Users with specific traffic management or connection policy requirements could optimize worker-frontend communication by choosing bidirectional streaming.",
      "rootCause": null,
      "proposedFix": "Add an option to use gRPC bidirectional streaming for sending/receiving multiple requests as an alternative to unary RPC with long polling",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Ideally an option to use gRPC bidirectional streaming for sending / receiving multiple requests rather than using long polling with unary RPC",
      "number": 2565,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:10.638Z"
    },
    {
      "summary": "Continue as new operations copy over prev_auto_reset_points, potentially causing unbounded growth. The issue was closed as by-design since a MaxAutoResetPoints limit of 20 prevents actual growth, but the reason for copying these points during continue-as-new operations remains unclear.",
      "category": "feature",
      "subcategory": "workflow-continuation",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "history-service",
        "workflow-execution",
        "auto-reset"
      ],
      "concepts": [
        "continue-as-new",
        "auto-reset-points",
        "memory-growth",
        "workflow-replay",
        "state-management"
      ],
      "severity": "low",
      "userImpact": "Long-running workflows using continue-as-new could accumulate excessive auto-reset-point data, though a safety limit prevents unbounded growth.",
      "rootCause": "Continue-as-new implementation copies prev_auto_reset_points from previous runs, potentially accumulating historical data unnecessarily.",
      "proposedFix": "Possible solutions mentioned: add an option to purge auto-reset-points, or only retain the previous run's values instead of all historical values.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Closed as by-design. MaxAutoResetPoints is already capped at 20, preventing unbounded growth. The copying behavior is intentional.",
      "related": [],
      "keyQuote": "I'm gonna go ahead and close this issue as by-design.",
      "number": 2558,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:10.583Z"
    },
    {
      "summary": "SQL GetTransfer/VisibilityTasks implementation ignores the page size parameter and returns all tasks instead of respecting pagination limits.",
      "category": "bug",
      "subcategory": "pagination",
      "apis": [
        "GetTransfer",
        "VisibilityTasks"
      ],
      "components": [
        "sql-persistence",
        "visibility-store",
        "task-store"
      ],
      "concepts": [
        "pagination",
        "performance",
        "query-optimization",
        "result-limiting",
        "api-compliance"
      ],
      "severity": "medium",
      "userImpact": "Queries return excessive data instead of respecting requested page sizes, causing performance degradation with large result sets.",
      "rootCause": "SQL GetTransfer/VisibilityTasks implementation does not apply the page size parameter from the request when fetching tasks.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Author was working on fix dependent on PR #2547 landing.",
      "related": [
        2547
      ],
      "keyQuote": "SQL GetTransfer/VisibilityTasks implementation ignores the page size parameter in the request and always return all tasks",
      "number": 2553,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:11.672Z"
    },
    {
      "summary": "Cassandra schema migration fails due to a trailing comma in the CREATE TYPE statement for serialized_event_batch, which violates CQL syntax. This prevents proper schema bootstrap, particularly affecting Azure CosmosDB Cassandra API.",
      "category": "bug",
      "subcategory": "schema-migration",
      "apis": [],
      "components": [
        "schema-manager",
        "cassandra-schema",
        "migration-executor"
      ],
      "concepts": [
        "schema-bootstrap",
        "syntax-error",
        "database-compatibility",
        "cql"
      ],
      "severity": "high",
      "userImpact": "Users cannot bootstrap Cassandra schema during initial setup, blocking deployment and cluster initialization.",
      "rootCause": "Trailing comma in CREATE TYPE serialized_event_batch field list violates CQL syntax specification.",
      "proposedFix": "Remove the trailing comma from the serialized_event_batch CREATE TYPE statement in the v1.0 schema migration.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The trailing comma was removed from the schema definition to comply with CQL syntax standards.",
      "related": [],
      "keyQuote": "CREATE TYPE serialized_event_batch (encoding_type text,version int,data blob,); has one comma at the end while the syntax should be without trailing comma",
      "number": 2551,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:58.197Z"
    },
    {
      "summary": "Remote sync match failed error logged in matching service when scheduling parallel activities through Ruby SDK. Activities execute successfully in parallel but error is logged almost every time with a canceled service error.",
      "category": "bug",
      "subcategory": "activity-matching",
      "apis": [],
      "components": [
        "matching-service",
        "task-queue-manager",
        "sync-match"
      ],
      "concepts": [
        "parallel-execution",
        "error-handling",
        "activity-scheduling",
        "gRPC",
        "context-cancellation"
      ],
      "severity": "medium",
      "userImpact": "Users see error logs in the Temporal server even though parallel activities execute successfully, causing concern and confusion about system reliability.",
      "rootCause": "Remote sync match operation in the matching service is being canceled during normal parallel activity execution, likely due to timing issues with task matching or context cancellation.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed via PR #2549",
      "related": [
        2549
      ],
      "keyQuote": "Both actions do run in parallel and finish successfully, but (edit: almost) every time I'm seeing an error logged on the temporal server",
      "number": 2543,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:00.510Z"
    },
    {
      "summary": "Docker-compose deployment of Temporal Server fails to auto-setup in Gitpod between v1.14 and v1.15, with DNS resolution errors for the 'temporal' hostname. The same docker-compose file works in v1.13 and on local desktop, suggesting an undocumented networking change in the server.",
      "category": "bug",
      "subcategory": "docker-networking",
      "apis": [],
      "components": [
        "docker-compose",
        "auto-setup",
        "network-dns"
      ],
      "concepts": [
        "dns-resolution",
        "container-networking",
        "docker-compose",
        "gitpod-environment",
        "host-lookup"
      ],
      "severity": "high",
      "userImpact": "Users relying on docker-compose for local Temporal setup in Gitpod or similar environments cannot upgrade beyond v1.13 without deployment failures.",
      "rootCause": "Undocumented networking dependency or docker-compose configuration change introduced between v1.13 and v1.14 that breaks DNS resolution of the 'temporal' hostname in certain network environments.",
      "proposedFix": null,
      "workaround": "Remain on Temporal v1.13, or investigate container network connectivity in Gitpod environment to ensure temporal service DNS resolution.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue was not resolved; user decided to stay on v1.13 indefinitely as a practical workaround rather than debug further.",
      "related": [],
      "keyQuote": "the change between v1.13 and v1.14 (which i can consistently replicate) is behavior that is being broken somehow",
      "number": 2539,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:29:00.133Z"
    },
    {
      "summary": "When attempting to complete an activity that is already completed, the server throws an INVALID_ARGUMENT error instead of a more semantically appropriate error code like NOT_FOUND or FAILED_PRECONDITION, making it difficult for users to diagnose the root cause.",
      "category": "bug",
      "subcategory": "activity-completion",
      "apis": [
        "respondActivityTaskCompletedById",
        "ManualActivityCompletionClientImpl"
      ],
      "components": [
        "activity-completion",
        "error-handling",
        "grpc-client"
      ],
      "concepts": [
        "error-codes",
        "activity-lifecycle",
        "activity-state",
        "manual-completion",
        "error-messages"
      ],
      "severity": "medium",
      "userImpact": "Users receive an unclear INVALID_ARGUMENT error when trying to complete an already-completed activity, making it difficult to understand whether the issue is a timeout, duplicate completion attempt, or something else.",
      "rootCause": "Server returns INVALID_ARGUMENT instead of a more descriptive error code (NOT_FOUND or FAILED_PRECONDITION) when an activity cannot be found in mutable state, which could be due to timeout or already being completed.",
      "proposedFix": "Change the gRPC error code from INVALID_ARGUMENT to NOT_FOUND or FAILED_PRECONDITION, and improve the error message to guide users to check workflow execution history to determine activity state.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Server error code and message were updated to provide clearer feedback about activity state, suggesting users review workflow execution history when an activity cannot be found.",
      "related": [],
      "keyQuote": "Shouldn't another exception be thrown vs. `INVALID_ARGUMENT` when the activity is already completed?",
      "number": 2538,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:48.258Z"
    },
    {
      "summary": "Deprecate queue-type-specific ack level fields (timer, transfer, visibility) in shard info in favor of a generalized QueueAckLevel field. The migration will occur over two releases with deprecation in 1.16 and removal in 1.17.",
      "category": "feature",
      "subcategory": "shard-management",
      "apis": [],
      "components": [
        "shard-info",
        "task-queues",
        "ack-level-management"
      ],
      "concepts": [
        "deprecation",
        "api-modernization",
        "backwards-compatibility",
        "schema-evolution",
        "queue-abstraction"
      ],
      "severity": "medium",
      "userImpact": "Users will need to migrate code using old ack level fields to use the new generalized QueueAckLevel field across two release cycles.",
      "rootCause": "Multiple queue-type-specific ack level fields (timer, transfer, visibility) are redundant and can be replaced with a single generalized QueueAckLevel field.",
      "proposedFix": "Deprecate old ack level fields in 1.16 release and remove them in 1.17 release, with migration guidance provided.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented via PR #2884 with deprecation and migration path across versions 1.16 and 1.17.",
      "related": [
        2511,
        2884
      ],
      "keyQuote": "we should deprecate/stop using other ack level fields that are specific to one queue type (e.g. timer, transfer, visibility, etc.)",
      "number": 2535,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:46.667Z"
    },
    {
      "summary": "SQL delete workflow execution does not delete all expired data. The deletion only targets the execution table, leaving other related data orphaned in the database.",
      "category": "bug",
      "subcategory": "sql-persistence",
      "apis": [],
      "components": [
        "sql-plugin",
        "persistence",
        "data-retention"
      ],
      "concepts": [
        "data-cleanup",
        "expired-workflows",
        "database-consistency",
        "orphaned-data",
        "sql-deletion"
      ],
      "severity": "high",
      "userImpact": "Expired workflow execution data accumulates in the database, causing storage bloat and potential performance degradation in SQL-backed Temporal deployments.",
      "rootCause": "The SQL plugin's deletion logic only targets the execution table and does not cascade or delete related data from other tables.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2525
      ],
      "keyQuote": "Only delete the data in execution table.",
      "number": 2528,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:47.666Z"
    },
    {
      "summary": "Request for server-side metrics to detect workflows stuck due to repetitive Workflow Task Timeouts or failures. Currently, only logging exists; metrics would enable easier detection and monitoring of this problematic condition.",
      "category": "feature",
      "subcategory": "workflow-task-execution",
      "apis": [],
      "components": [
        "history-service",
        "workflow-task-state-machine",
        "metrics"
      ],
      "concepts": [
        "workflow-task-timeout",
        "failure-detection",
        "observability",
        "stuck-workflows",
        "metric-reporting"
      ],
      "severity": "high",
      "userImpact": "Users cannot easily detect when workflows are stuck due to repetitive task timeouts, making it difficult to diagnose and respond to production issues.",
      "rootCause": "Lack of metrics exposure for workflow task timeout conditions; currently only logging exists which is insufficient for automated detection.",
      "proposedFix": "Expose as a metric reporting number of workflows exceeding N failed workflow tasks in a row, or as a histogram with failed task count as x-axis and workflow count as y-axis.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "when Workflow Task Timeout happens, something went wrong on the SDK side and SDK shouldn't be trusted in this situation. Temporal Server should be responsible for producing the metric.",
      "number": 2526,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:33.848Z"
    },
    {
      "summary": "Task queue dispatch prioritizes sync matches over backlog tasks, causing fairness issues where backlogged tasks can be starved. The issue requests improved FIFO ordering to ensure backlog tasks are processed fairly alongside incoming tasks.",
      "category": "other",
      "subcategory": "task-queue-dispatch",
      "apis": [],
      "components": [
        "task-queue",
        "matching",
        "dispatcher",
        "backlog-processing"
      ],
      "concepts": [
        "fairness",
        "FIFO-ordering",
        "task-queue-partitioning",
        "sync-match",
        "backlog",
        "starvation",
        "load-balancing"
      ],
      "severity": "medium",
      "userImpact": "Users experience unfair task delivery where backlogged tasks wait indefinitely while new tasks are processed, reducing system efficiency during high load.",
      "rootCause": "Task queue partitioning causes asymmetric backlog distribution between root and leaf partitions, with root partitions processing backlog faster and leaf partitions accumulating heavier/older backlogs. Sync match requests bypass the backlog queue.",
      "proposedFix": "Deliver tasks from backlog first with optional configuration to control the behavior. For users with low/medium traffic, reducing partitions to 1 enables approximate FIFO ordering.",
      "workaround": "Reduce task queue partitions to 1 using dynamic config matching.numTaskqueueWritePartitions and matching.numTaskqueueReadPartitions set to value 1, which provides approximate FIFO ordering for low/medium load task queues.",
      "resolution": "completed",
      "resolutionDetails": "Issue closed as completed. Solution determined: exact FIFO guaranteed for single-partition task queues; larger task queues achieve FIFOish behavior through balanced partition backlogs by design.",
      "related": [],
      "keyQuote": "For task queues with low/medium load you can get approximate FIFO ordering by reducing the number of partitions to 1",
      "number": 2517,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:34.447Z"
    },
    {
      "summary": "User needs to use Elasticsearch over HTTPS with self-signed certificates in auto-setup, but curl validation fails. Requests ability to pass custom curl parameters like --insecure or --cacert to make auto-setup work with private certificate infrastructure.",
      "category": "feature",
      "subcategory": "elasticsearch-setup",
      "apis": [],
      "components": [
        "auto-setup",
        "elasticsearch",
        "docker-setup"
      ],
      "concepts": [
        "https",
        "self-signed-certificate",
        "curl-parameters",
        "ssl-validation",
        "docker-setup",
        "elasticsearch-connection"
      ],
      "severity": "medium",
      "userImpact": "Users with HTTPS-only Elasticsearch clusters using self-signed certificates cannot use the provided auto-setup Docker image without creating custom scripts.",
      "rootCause": "auto-setup.sh uses curl without options to accept self-signed certificates, and the auto-setup Docker container doesn't run as root so it cannot install custom CA certificates.",
      "proposedFix": "Add ES_CURL_PARAMS environment variable option to allow users to pass custom curl parameters like -k or --cacert to the auto-setup script.",
      "workaround": "Create a custom Docker image based on temporal/auto-setup or temporal-admin-tools that includes the custom certificate and a modified auto-setup script, or run temporal as a binary with pre-installed certificates instead of using Docker.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed after 30 days without implementation. Workaround via custom Docker images or temporal-admin-tools was suggested as alternative.",
      "related": [
        82
      ],
      "keyQuote": "Perhaps a more universal approach would be just to add the ES_CURL_PARAMS option.",
      "number": 2502,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:36.179Z"
    },
    {
      "summary": "Request to allow customization of activity retry behavior for timeout errors. Currently the server has hardcoded retry logic where heartbeat and start-to-close timeouts are retryable, but other timeout types are not. The proposed solution is to use the non-retryable errors field in activity retry policy to customize this behavior.",
      "category": "feature",
      "subcategory": "activity-retry",
      "apis": [],
      "components": [
        "activity-executor",
        "retry-policy",
        "server"
      ],
      "concepts": [
        "timeout",
        "retry",
        "activity",
        "error-handling",
        "customization"
      ],
      "severity": "medium",
      "userImpact": "Users need more granular control over which timeout errors trigger activity retries, enabling use cases like session creation that require selective error handling.",
      "rootCause": null,
      "proposedFix": "Allow customizing activity retry behavior via the non-retryable errors field in activity retry policy",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [
        722
      ],
      "keyQuote": "Allow customizing this behavior via the non-retryable errors field in activity retry policy",
      "number": 2496,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:22.213Z"
    },
    {
      "summary": "The current throttled logger rate-limits all log lines equally, which can cause important error messages that occur infrequently to be missed while the logger is handling high-frequency repeated log lines. The proposal is to implement smarter throttling that only limits frequently repeated log lines while allowing rare errors through.",
      "category": "other",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "throttle_logger",
        "log_system"
      ],
      "concepts": [
        "rate_limiting",
        "error_handling",
        "log_filtering",
        "throttling",
        "frequency_detection"
      ],
      "severity": "medium",
      "userImpact": "Important rare errors may be dropped from logs due to rate limiting, making it harder to diagnose and troubleshoot critical issues.",
      "rootCause": "The throttled logger applies uniform rate limiting to all log lines regardless of frequency, causing rare errors to be lost when the rate limit is reached by high-frequency messages.",
      "proposedFix": "Implement frequency-aware throttling that tracks how often each unique log line appears and only applies rate limiting to frequently repeated messages while allowing rare messages through.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We can do much better job to throttle only frequently repeat log lines. Current throttled logger likely will miss important error log lines if they happen rarely while hit by rps limit.",
      "number": 2491,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:20.696Z"
    },
    {
      "summary": "Auto-setup Docker image fails to start when DYNAMIC_CONFIG_FILE_PATH environment variable is not set, attempting to read a directory as a file. The dynamic config client initialization should be conditionally wrapped to allow server startup without dynamic configuration.",
      "category": "bug",
      "subcategory": "docker-setup",
      "apis": [],
      "components": [
        "docker-image",
        "dynamic-config-client",
        "config-template"
      ],
      "concepts": [
        "configuration",
        "environment-variables",
        "docker-setup",
        "conditional-initialization",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot launch the auto-setup Docker image without providing a dynamic config file path, preventing simple deployment scenarios.",
      "rootCause": "The config_template.yaml does not conditionally wrap the dynamicConfigClient segment, causing it to attempt reading a directory as a configuration file when the DYNAMIC_CONFIG_FILE_PATH environment variable is absent.",
      "proposedFix": "Wrap the dynamicConfigClient segment in config_template.yaml with {{- if .Env.DYNAMIC_CONFIG_FILE_PATH }} conditional logic.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Server startup failed with `Unable to create dynamic config client. Error: unable to read dynamic config: dynamic config file: /etc/temporal/config/dynamicconfig: read /etc/temporal/config/dynamicconfig: is a directory`",
      "number": 2482,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:22.575Z"
    },
    {
      "summary": "Temporal caches the archiver object with AWS credentials at startup, preventing credential refresh when using time-limited AWS keys that rotate every 8 hours. This causes archival failures after credentials expire.",
      "category": "bug",
      "subcategory": "archival-credentials",
      "apis": [],
      "components": [
        "archiver",
        "storage-service",
        "aws-s3",
        "credential-management"
      ],
      "concepts": [
        "credential-refresh",
        "session-caching",
        "authentication",
        "archival",
        "aws-integration",
        "credential-rotation"
      ],
      "severity": "high",
      "userImpact": "Users with rotating AWS credentials experience archival failures after credential expiration, blocking archived workflow access.",
      "rootCause": "Archiver object is created and cached once at application startup, using initial credentials for the entire application lifecycle without refreshing from credential file.",
      "proposedFix": null,
      "workaround": "The AWS Go SDK may support credential refresh; see aws/aws-sdk-go#1993 for possible workaround details.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Archiver object which does the archiving is created only once for the first call and is cached. There after for subsequent calls, the same archiver object is used.",
      "number": 2470,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:08.745Z"
    },
    {
      "summary": "Race condition in workflow archival where signaling the archival workflow succeeds despite caller's context deadline exceeded, causing deletion of history that wasn't confirmed archived. Subsequent archival attempts fail because history is already deleted.",
      "category": "bug",
      "subcategory": "archival",
      "apis": [],
      "components": [
        "archival-system",
        "history-archiver",
        "transfer-task",
        "signal-handling"
      ],
      "concepts": [
        "archival",
        "context-deadline",
        "race-condition",
        "retry",
        "data-loss",
        "history-deletion",
        "signal-delivery"
      ],
      "severity": "high",
      "userImpact": "Users risk losing workflow history data if archival signaling times out but is actually processed, causing history deletion before confirming archival success.",
      "rootCause": "300ms context deadline timeout can expire from caller's perspective while the archival workflow still receives and processes the signal, causing asynchronous history deletion that conflicts with retry logic.",
      "proposedFix": "Handle 'history not found' as an expected error rather than fatal non-retryable error; verify workflow has been archived before deleting history, or implement more resilient signal delivery confirmation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "PR modified how workflow history not found is handled during archival - returns nil error instead of non-retryable error since history not found is expected when archival already succeeded. Transfer task retries until archival request is delivered to system workflow.",
      "related": [],
      "keyQuote": "If the signal fails, we can check if the workflow has been archived, if yes, we can ignore the signal failure and proceed to delete the wf history.",
      "number": 2464,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:10.554Z"
    },
    {
      "summary": "Request for a metric to detect when task queues have pending tasks but no workers listening, which can indicate misconfiguration or versioning schema issues. A new metric 'no_poller_tasks' was added to address this.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "matching-engine",
        "task-queue",
        "metrics"
      ],
      "concepts": [
        "task-queue",
        "worker-availability",
        "monitoring",
        "misconfiguration",
        "versioning",
        "task-backlog"
      ],
      "severity": "medium",
      "userImpact": "Users can now detect when tasks are stuck in a queue due to missing workers, improving operational visibility and debugging capabilities.",
      "rootCause": "Lack of observability metric to distinguish between slow workers and absent workers on a task queue.",
      "proposedFix": "Add a metric to detect non-empty queues with no active workers listening on them.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Metrics 'no_poller_tasks' was added to support detection of task queues with tasks but no workers.",
      "related": [],
      "keyQuote": "If for any reason there is a task queue with tasks and just no workers listening on this queue, there is no way to detect it.",
      "number": 2463,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:28:08.049Z"
    },
    {
      "summary": "Improve the error message for invalid command sequences in workflow task handlers. Currently, the message is unclear; it should explicitly state which commands are invalid and explain that terminal commands must be the last command.",
      "category": "other",
      "subcategory": "error-messages",
      "apis": [],
      "components": [
        "commandChecker",
        "workflow-task-handler"
      ],
      "concepts": [
        "command-sequence",
        "validation",
        "error-messaging",
        "terminal-commands",
        "user-experience"
      ],
      "severity": "low",
      "userImpact": "Third-party SDK developers receive clearer error messages when they submit invalid command sequences, making it easier to debug workflow execution issues.",
      "rootCause": "The current error message lacks clarity about which commands are invalid and why the command sequence is problematic.",
      "proposedFix": "Reword the error message to explicitly list the commands that appeared after a terminal command and explain that the terminal command must be last.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Pull request #2468 was created to implement the improved error message.",
      "related": [
        2468
      ],
      "keyQuote": "Invalid command sequence. Encountered commands ScheduleActivityTask, RecordMarker after command ContinueAsNewWorkflowExecution, which must be the last command in a workflow execution.",
      "number": 2460,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:56.776Z"
    },
    {
      "summary": "listArchivedWorkflowExecutions returns both archived and unarchived workflows instead of only archived ones. The archival feature archives visibility data immediately upon workflow completion, not when the retention period expires, causing confusion in the UI and incorrect filtering behavior.",
      "category": "bug",
      "subcategory": "archival",
      "apis": [
        "listArchivedWorkflowExecutions",
        "describeWorkflowExecution"
      ],
      "components": [
        "archival",
        "visibility",
        "search-attributes"
      ],
      "concepts": [
        "archival",
        "retention",
        "workflow-visibility",
        "data-lifecycle",
        "filtering"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably filter for only archived workflows, leading to confusion in the archival UI tab and incorrect application logic when retrieving archived workflow data.",
      "rootCause": "Visibility data is archived immediately upon workflow completion rather than when the retention period expires, causing unarchived workflows to appear in archived results.",
      "proposedFix": "Fix archival behavior to archive both workflow history and visibility data together when the retention period expires, ensuring listArchivedWorkflowExecutions only returns truly archived workflows.",
      "workaround": "Users can filter results by closed time window to limit results to workflows within a specific time range.",
      "resolution": "wontfix",
      "resolutionDetails": "Archival feature remains experimental. The team indicated they would revisit archival holistically when ready to productionize it, deferring this fix.",
      "related": [],
      "keyQuote": "The archival feature is still experimental. We will revisit it holistically when we are ready to productionize it.",
      "number": 2457,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:57.748Z"
    },
    {
      "summary": "CVE-2021-44716, a Go runtime vulnerability, is present in Temporal Server container images. This security vulnerability affects binaries compiled with vulnerable Go versions prior to v1.17.5.",
      "category": "bug",
      "subcategory": "security-vulnerability",
      "apis": [],
      "components": [
        "base-builder",
        "container-image",
        "temporal-server"
      ],
      "concepts": [
        "security",
        "vulnerability",
        "golang",
        "cve",
        "container-image",
        "dependency"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal Server containers are exposed to the CVE-2021-44716 vulnerability until they upgrade to a version built with Go v1.17.5 or later.",
      "rootCause": "Temporal Server binaries were compiled with a vulnerable version of Go (prior to v1.17.5) that contains CVE-2021-44716.",
      "proposedFix": "Rebuild Temporal Server binaries and container images with Go v1.17.5 or later.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by rebuilding container images with Go v1.17.5 or later, which patches the CVE-2021-44716 vulnerability.",
      "related": [
        2184
      ],
      "keyQuote": "Golang Vulnerability CVE-2021-44716 is present in Temporal binaries such as temporal-server. Apparently this is fixed in Golang v1.17.5.",
      "number": 2451,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:55.986Z"
    },
    {
      "summary": "Service crashes with a nil pointer dereference during startup when invalid TLS configuration is provided. The crash occurs because TLS initialization and validation check different configuration sources, causing a mismatch that leads to a panic instead of a proper error.",
      "category": "bug",
      "subcategory": "tls-configuration",
      "apis": [],
      "components": [
        "rpc-factory",
        "encryption-provider",
        "tls-config"
      ],
      "concepts": [
        "tls",
        "configuration-validation",
        "error-handling",
        "service-startup",
        "nil-pointer",
        "panic"
      ],
      "severity": "high",
      "userImpact": "Users cannot start the Temporal server with invalid TLS configurations; the service crashes with a segmentation fault instead of providing a helpful error message.",
      "rootCause": "Mismatch between TLS enablement check in Frontend TLS provider (uses config.Frontend) and TLS initialization logic (uses different config path), causing nil pointer dereference when accessing certificate fields.",
      "proposedFix": "Align TLS configuration validation between localStoreTlsProvider.GetFrontendServerConfig and the TLS initialization logic to use the same configuration source and validate before attempting to access certificate fields.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed by ensuring consistent TLS configuration validation and proper error handling before accessing certificate fields.",
      "related": [],
      "keyQuote": "The Frontend TLS uses config.Frontend to determine if TLS is enable but the TLS initialization uses a different config path, causing a panic when accessing nil certificate fields.",
      "number": 2448,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:44.002Z"
    },
    {
      "summary": "User requests an option to skip the database creation step in auto-setup.sh when using managed cloud databases that don't support SQL-based database creation, while still allowing schema setup to proceed.",
      "category": "feature",
      "subcategory": "setup-automation",
      "apis": [],
      "components": [
        "auto-setup.sh",
        "sql-tool",
        "schema-setup"
      ],
      "concepts": [
        "database-creation",
        "managed-database",
        "schema-migration",
        "setup-automation",
        "cloud-integration"
      ],
      "severity": "medium",
      "userImpact": "Users with managed cloud databases cannot use the auto-setup script without modifying it, forcing them to maintain custom setup scripts.",
      "rootCause": "auto-setup.sh unconditionally attempts to create the database via sql-tool, which fails on managed database services that handle database creation separately.",
      "proposedFix": "Add SKIP_CREATE_DB option to auto-setup.sh that skips temporal-sql-tool create commands while allowing schema setup commands to run.",
      "workaround": "Write a custom setup script that skips the database creation step.",
      "resolution": "fixed",
      "resolutionDetails": "The SKIP_CREATE_DB option was implemented, allowing users with pre-existing databases to skip creation while running schema setup.",
      "related": [],
      "keyQuote": "Add option SKIP_CREATE_DB to auto-setup.sh witch will skipp `temporal-sql-tool create` commands",
      "number": 2447,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:45.119Z"
    },
    {
      "summary": "Support including heartbeat details in activity task failure completions to avoid race conditions and unnecessary heartbeat RPCs. Currently, developers must delay completion until pending heartbeat info is flushed; allowing details in the failure response would eliminate this timing issue.",
      "category": "feature",
      "subcategory": "activity-heartbeat",
      "apis": [
        "RespondActivityTaskFailedRequest"
      ],
      "components": [
        "activity-executor",
        "heartbeat-buffer",
        "rpc-handler"
      ],
      "concepts": [
        "heartbeat",
        "activity-failure",
        "race-condition",
        "buffering",
        "completion-details",
        "rpc-optimization"
      ],
      "severity": "medium",
      "userImpact": "Developers will be able to record progress and fail activities atomically without managing complex heartbeat flush timing.",
      "rootCause": "RespondActivityTaskFailedRequest lacks a details field for heartbeat information, forcing completion to wait for pending heartbeat buffer flush.",
      "proposedFix": "Upgrade RespondActivityTaskFailedRequest to include a details field that acts like heartbeat details.",
      "workaround": "Delay activity completion until pending heartbeat info is flushed from the buffer.",
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "if I could send the details with the completion I can just avoid sending another heartbeat RPC alltogether",
      "number": 2443,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:43.798Z"
    },
    {
      "summary": "Need detailed server-internal resource exhaustion metrics to improve detection of approaching resource limits. These metrics could be exposed to workers as simplified errors or accessed through a resource monitor object.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "server",
        "resource-monitor"
      ],
      "concepts": [
        "resource-exhaustion",
        "headroom",
        "monitoring",
        "capacity-detection",
        "performance-degradation"
      ],
      "severity": "medium",
      "userImpact": "Users lack detailed visibility into server resource exhaustion, making it difficult to detect and prevent service degradation.",
      "rootCause": null,
      "proposedFix": "Provide detailed resource headroom metrics that collapse into simpler errors exposed to workers, or implement a resource monitor object for on-demand access.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2412
      ],
      "keyQuote": "we need more detailed resource headroom metrics to improve our ability to detect approaching exhaustion",
      "number": 2435,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:32.449Z"
    },
    {
      "summary": "Request to add a temporal_cluster label to exported metrics to enable consistent filtering across multiple clusters regardless of deployment environment (Kubernetes or VM).",
      "category": "feature",
      "subcategory": "metrics-observability",
      "apis": [],
      "components": [
        "metrics-exporter",
        "server-config",
        "prometheus-integration"
      ],
      "concepts": [
        "observability",
        "multi-tenant",
        "metrics-labeling",
        "cluster-identification",
        "configuration"
      ],
      "severity": "low",
      "userImpact": "Users managing multiple Temporal clusters need cluster-aware metric filtering without relying on external labeling mechanisms.",
      "rootCause": null,
      "proposedFix": "Add temporal_cluster label to the metrics exporter, or ensure server config allows injecting cluster-identifying tags via metrics.tags configuration.",
      "workaround": "Add cluster label via Kubernetes labels or configure temporal server with metrics.tags in global config to inject custom labels.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by pointing user to existing config option: global.metrics.tags can be used to inject cluster labels into all metrics reported by server.",
      "related": [],
      "keyQuote": "You can also add label into temporal server config. This label will be injected into all the metrics reported by server.",
      "number": 2430,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:32.215Z"
    },
    {
      "summary": "Request to add a 'reason' field to RequestCancelWorkflowExecutionRequest for API consistency, matching the existing 'reason' fields in TerminateWorkflowExecutionRequest and ResetWorkflowExecutionRequest.",
      "category": "feature",
      "subcategory": "workflow-cancellation",
      "apis": [
        "RequestCancelWorkflowExecutionRequest",
        "TerminateWorkflowExecutionRequest",
        "ResetWorkflowExecutionRequest"
      ],
      "components": [
        "workflow-execution",
        "api",
        "proto-definitions"
      ],
      "concepts": [
        "workflow-cancellation",
        "api-consistency",
        "metadata",
        "audit-trail",
        "reason-tracking"
      ],
      "severity": "medium",
      "userImpact": "Users cannot provide a reason when canceling workflow executions, limiting visibility into cancellation decisions and audit trails.",
      "rootCause": null,
      "proposedFix": "Add a 'reason' field to RequestCancelWorkflowExecutionRequest matching the implementation in TerminateWorkflowExecutionRequest and ResetWorkflowExecutionRequest.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Currently both TerminateWorkflowExecutionRequest and ResetWorkflowExecutionRequest have a \"reason\" field. RequestCancelWorkflowExecutionRequest does not.",
      "number": 2426,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:33.233Z"
    },
    {
      "summary": "User attempted to use DataStax Astra (managed Cassandra) as Temporal's backend but encountered x509 certificate validation errors due to IP SAN mismatches. The issue was resolved with proper configuration settings for TLS hostname verification and initial host lookup bypass.",
      "category": "bug",
      "subcategory": "cassandra-astra-integration",
      "apis": [],
      "components": [
        "temporal-cassandra-tool",
        "auto-setup",
        "cassandra-client",
        "tls-configuration"
      ],
      "concepts": [
        "tls-certificate-validation",
        "cassandra-backend",
        "astra-database",
        "hostname-verification",
        "ip-sans",
        "docker-compose"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily set up Temporal with DataStax Astra as a backend without encountering TLS certificate validation failures.",
      "rootCause": "Astra's certificates are generated without IP SANs (Subject Alternative Names), but the gocql Cassandra driver's host lookup process stores only IP addresses, causing certificate validation to fail when connecting by IP address.",
      "proposedFix": "Use CASSANDRA_TLS_SERVER_NAME environment variable set to the hostname and CASSANDRA_DISABLE_INITIAL_HOST_LOOKUP=true to prevent IP-based lookups that trigger certificate validation failures.",
      "workaround": "A TLS proxy (e.g., ghostunnel) can be used as an intermediary to handle the TLS connection before forwarding to the application.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by confirming that the proper environment variables (CASSANDRA_TLS_SERVER_NAME, CASSANDRA_DISABLE_INITIAL_HOST_LOOKUP, SKIP_DB_CREATE) enable successful Astra connections. The temporal-cassandra-tool and auto-setup image work as expected with correct configuration.",
      "related": [
        1611
      ],
      "keyQuote": "The initial lookup must be skipped and the server name must be set (to the same value as seeds). This is due to the way Astra's certificates are generated without IP SANS versus Cassandra's host lookup process which stores only IP addresses.",
      "number": 2419,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:21.977Z"
    },
    {
      "summary": "Request to add ARM64 support to the temporalio/web Docker image to achieve parity with other Temporal images like temporalio/auto-setup which already support ARM64.",
      "category": "feature",
      "subcategory": "docker-image",
      "apis": [],
      "components": [
        "docker-image",
        "web-ui"
      ],
      "concepts": [
        "arm64",
        "multi-architecture",
        "docker-hub",
        "platform-support"
      ],
      "severity": "low",
      "userImpact": "Users with ARM64 architecture systems cannot use the temporalio/web Docker image, limiting their ability to run the web UI on ARM-based infrastructure.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed in favor of temporalio/web repository issue #420",
      "related": [
        420
      ],
      "keyQuote": "I was hoping to see if the temporalio/web docker hub image could get support for arm64?",
      "number": 2415,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:18.699Z"
    },
    {
      "summary": "Add resource_type field to resource_exhausted errors to distinguish between RPS limit and concurrent poller limit errors, and emit metrics with this type tag for better observability.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "error-handling",
        "metrics",
        "resource-management"
      ],
      "concepts": [
        "resource-exhausted",
        "error-tagging",
        "metrics-emission",
        "rate-limiting",
        "observability",
        "error-classification"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently distinguish different causes of resource_exhausted errors or track them separately in metrics, limiting debugging and monitoring capabilities.",
      "rootCause": "resource_exhausted error lacks type information to differentiate between RPS limit and concurrent poller limit scenarios.",
      "proposedFix": "Include resource_type field in resource_exhausted error and emit metrics with type tag when the error is returned.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation completed in PR #2423 to add resource_type field and metrics tagging.",
      "related": [
        2423
      ],
      "keyQuote": "include resource_type to resource_exhausted error and emit metrics with tag of type when resource_exhausted error is returned",
      "number": 2412,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:18.588Z"
    },
    {
      "summary": "MySQL INSERT statement with ON DUPLICATE KEY UPDATE returns 2 rows affected instead of 1, causing workflow creation failures. The persistence layer incorrectly validates that exactly 1 row must be affected, but duplicate key updates can legitimately affect 2 rows.",
      "category": "bug",
      "subcategory": "persistence-mysql",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "persistence",
        "history-store",
        "mysql-plugin"
      ],
      "concepts": [
        "duplicate-key-update",
        "row-affected-count",
        "workflow-creation",
        "child-workflows"
      ],
      "severity": "high",
      "userImpact": "Users experience significant delays in creating child workflows due to false error conditions in the MySQL persistence layer.",
      "rootCause": "The AppendHistoryNodes validation in history_store.go checks for exactly 1 row affected, but MySQL's ON DUPLICATE KEY UPDATE can affect up to 2 rows (insert + update), causing the validation to fail incorrectly.",
      "proposedFix": "Change the validation from `if rowsAffected != 1` to `if rowsAffected > 2` to allow for the legitimate 2-row case from duplicate key updates.",
      "workaround": "Modify the persistence validation check to accept rowsAffected up to 2.",
      "resolution": "fixed",
      "resolutionDetails": "The validation was corrected to handle the case where ON DUPLICATE KEY UPDATE affects 2 rows.",
      "related": [],
      "keyQuote": "This particular statement can affect 2 rows at the max. But the check above was if rowsAffected != 1 which seems to be causing the issue",
      "number": 2408,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:08.474Z"
    },
    {
      "summary": "Transaction.CreateWorkflowExecution() API is defined but not actually used in the codebase. Shard context directly calls createWorkflowExecutionWithRetry() instead of going through the transaction abstraction, indicating incomplete API implementation.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [
        "CreateWorkflowExecution"
      ],
      "components": [
        "transaction",
        "shard-context",
        "workflow-execution"
      ],
      "concepts": [
        "transaction-abstraction",
        "api-usage",
        "code-consistency",
        "refactoring"
      ],
      "severity": "medium",
      "userImpact": "The transaction abstraction layer is incomplete, potentially limiting transaction functionality and creating maintenance burden through inconsistent API usage patterns.",
      "rootCause": "Transaction.CreateWorkflowExecution() method exists but is bypassed by direct calls to createWorkflowExecutionWithRetry() in shard context code.",
      "proposedFix": "Update shard context to use Transaction.CreateWorkflowExecution() instead of directly calling createWorkflowExecutionWithRetry().",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The Transaction.CreateWorkflowExecution() is not used anywhere. Shard context directly calls into createWorkflowExecutionWithRetry().",
      "number": 2406,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:07.014Z"
    },
    {
      "summary": "Preserve sticky task queue affinity when querying closed workflows to allow queries to be dispatched to the correct sticky worker. Currently, sticky workflows are evicted from cache when closed, but the server-side sticky flag remains, creating a mismatch in query routing.",
      "category": "feature",
      "subcategory": "workflow-query",
      "apis": [
        "QueryWorkflow"
      ],
      "components": [
        "sticky-cache",
        "query-dispatcher",
        "workflow-eviction",
        "worker-dispatch"
      ],
      "concepts": [
        "sticky-task-queue",
        "query-routing",
        "cache-eviction",
        "worker-affinity",
        "LRU-eviction",
        "workflow-state"
      ],
      "severity": "medium",
      "userImpact": "Queries on closed workflows may be dispatched to incorrect workers instead of the originally sticky worker, potentially causing query failures or performance issues.",
      "rootCause": "SDK evicts closed workflows from sticky cache immediately, but server retains sticky flag and attempts to route queries to that sticky worker, creating a mismatch.",
      "proposedFix": "SDK should stop immediately evicting closed workflows and instead rely on LRU eviction policy to naturally remove them from cache over time, maintaining sticky routing consistency.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "From server side, there is no change needed, the sticky flag is still there and so server could dispatch the query task to the right sticky worker.",
      "number": 2405,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:27:06.138Z"
    },
    {
      "summary": "Importing github.com/cactus/go-statsd-client creates an ambiguous import error in go.temporal.io/server v1.14.3 due to multiple versions of the package being loaded, causing go mod tidy to fail with Go 1.16+.",
      "category": "bug",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "metrics",
        "authorization",
        "dependency-resolution"
      ],
      "concepts": [
        "module-dependency",
        "import-ambiguity",
        "go-modules",
        "version-conflict",
        "backwards-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users cannot upgrade to go.temporal.io/server v1.14.3+ without encountering go mod tidy failures, blocking adoption of newer server versions.",
      "rootCause": "Multiple versions of github.com/cactus/go-statsd-client are loaded simultaneously (v3.1.1+incompatible and v0.0.0-20200423205355), creating ambiguous import paths that go 1.16+ cannot resolve.",
      "proposedFix": null,
      "workaround": "Use go mod tidy -compat=1.17 instead of standard go mod tidy",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "ambiguous import: found package github.com/cactus/go-statsd-client/statsd in multiple modules",
      "number": 2403,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:54.426Z"
    },
    {
      "summary": "Deprecate ListOpen/CloseWorkflowExecutions APIs in favor of the new ListWorkflowExecutions which handles basic visibility functionality. These deprecated methods remain available in the frontend for backwards compatibility but should be phased out.",
      "category": "feature",
      "subcategory": "visibility",
      "apis": [
        "ListWorkflowExecutions",
        "ListOpenWorkflowExecutions",
        "ListCloseWorkflowExecutions"
      ],
      "components": [
        "visibility",
        "frontend",
        "api"
      ],
      "concepts": [
        "deprecation",
        "backwards-compatibility",
        "api-evolution",
        "visibility-query"
      ],
      "severity": "low",
      "userImpact": "Users are encouraged to migrate from ListOpen/CloseWorkflowExecutions to the unified ListWorkflowExecutions API for visibility queries.",
      "rootCause": "ListWorkflowExecutions now provides the visibility functionality that previously required separate List Open/Close APIs.",
      "proposedFix": "Deprecate ListOpen/CloseWorkflowExecutions and guide users to ListWorkflowExecutions.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Visibility no longer exposes ListOpen/CloseWorkflowExecutions; they remain available in frontend for backwards compatibility, calling ListWorkflowExecutions internally.",
      "related": [],
      "keyQuote": "Visibility no longer has `ListOpen/CloseWorkflowExecutions`. However, they are still available in the frontend for backwards compatibility",
      "number": 2389,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:53.493Z"
    },
    {
      "summary": "Request to integrate migration tool functionality into tctl to simplify database schema setup and upgrades. Currently requires running separate migration tools from Docker or source, but embedding migration capabilities in tctl would provide a unified tool for all migration operations.",
      "category": "feature",
      "subcategory": "tooling-migration",
      "apis": [],
      "components": [
        "tctl",
        "migration-tool",
        "schema-setup"
      ],
      "concepts": [
        "database-migration",
        "schema-upgrade",
        "bootstrap",
        "embedded-tools",
        "binary-distribution"
      ],
      "severity": "low",
      "userImpact": "Users must currently use multiple tools or enter Docker containers to perform database migrations, making production setup and upgrades cumbersome.",
      "rootCause": null,
      "proposedFix": "Integrate temporal-*-tool migration functions into tctl and embed migration SQL files and artifacts directly in the tctl binary.",
      "workaround": "Run migration tools from Docker image or compile from source directly.",
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #2059",
      "related": [
        2059
      ],
      "keyQuote": "tctl can integrate temporal-*-tool's function, reduce the setup work. tctl contain all these tool's functions, one tool for all job.",
      "number": 2383,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:54.565Z"
    },
    {
      "summary": "Request to make TokenKeyProvider more flexible by passing the JWT token to GetKey() method, enabling dynamic key fetching based on token claims (like issuer). This allows providers to handle multi-tenant scenarios where JWK URLs are issuer-specific without prefetching all keys.",
      "category": "feature",
      "subcategory": "jwt-authentication",
      "apis": [],
      "components": [
        "authorization",
        "token-validation",
        "jwt-handler"
      ],
      "concepts": [
        "jwt-token",
        "issuer",
        "key-provider",
        "dynamic-lookup",
        "multi-tenant",
        "openid-connect"
      ],
      "severity": "medium",
      "userImpact": "Users with dynamic JWT issuers (e.g., multi-tenant SaaS services) currently cannot implement custom TokenKeyProviders that fetch keys based on token issuer, forcing them to prefetch keys for all possible issuers.",
      "rootCause": "Current TokenKeyProvider interface's GetKey(alg, kid) signature lacks access to token context (like issuer claim), preventing dynamic JWK URL resolution based on token metadata.",
      "proposedFix": "Create a new RawTokenKeyProvider interface with GetKey(token *jwt.Token) signature, and add NewDefaultJWTClaimMapperWithRawKeyProvider() to support both interfaces without breaking changes.",
      "workaround": "Users can implement custom token parsing that calls their key provider with full token context before passing to validation, but this breaks the standard flow.",
      "resolution": "fixed",
      "resolutionDetails": "Proposal accepted to create a separate RawTokenKeyProvider interface alongside existing TokenKeyProvider to support both use cases without breaking changes.",
      "related": [],
      "keyQuote": "It's impossible to prefetch pubkey, issuer like `*.example.com` is all valid issuer, they point to different keycloak like service.",
      "number": 2382,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:42.592Z"
    },
    {
      "summary": "DescribeTaskQueueRequest with TaskQueueType.TASK_QUEUE_TYPE_UNSPECIFIED returns no results, but users expect it to match all task queue types. The server should default to TASK_QUEUE_TYPE_WORKFLOW when type is unspecified.",
      "category": "bug",
      "subcategory": "task-queue-api",
      "apis": [
        "DescribeTaskQueueRequest"
      ],
      "components": [
        "server",
        "task-queue",
        "grpc-api"
      ],
      "concepts": [
        "task-queue-type",
        "default-behavior",
        "api-usability",
        "unspecified-enum"
      ],
      "severity": "medium",
      "userImpact": "Users cannot query task queues without explicitly specifying a type, making the API less intuitive and preventing discovery of all task queues.",
      "rootCause": "Server does not apply a default TaskQueueType when TASK_QUEUE_TYPE_UNSPECIFIED is provided, resulting in no matches instead of matching the default type.",
      "proposedFix": "Server should use TASK_QUEUE_TYPE_WORKFLOW as the default when TaskQueueType is unspecified.",
      "workaround": "Explicitly set TaskQueueType to TASK_QUEUE_TYPE_WORKFLOW or TASK_QUEUE_TYPE_ACTIVITY",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #4815 to make server use default type (TASK_QUEUE_TYPE_WORKFLOW) when unspecified",
      "related": [
        4815
      ],
      "keyQuote": "When the TaskQueueType is not specified, server should use a default type (TASK_QUEUE_TYPE_WORKFLOW), and this needs to be documented.",
      "number": 2381,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:42.613Z"
    },
    {
      "summary": "Simplify data encoding architecture by removing per-attribute encoding flexibility and standardizing on proto3. Replace DataBlob with []byte in three core workflow state structs to reduce unnecessary complexity and duplication.",
      "category": "feature",
      "subcategory": "persistence-encoding",
      "apis": [],
      "components": [
        "persistence",
        "workflow-state",
        "data-encoding"
      ],
      "concepts": [
        "encoding",
        "data-serialization",
        "proto3",
        "simplification",
        "persistence-layer"
      ],
      "severity": "medium",
      "userImpact": "Reduces complexity in the persistence layer and improves performance by eliminating unnecessary per-attribute encoding operations.",
      "rootCause": "Current implementation allows per-attribute encoding (activity, child workflow) but only proto3 is used in practice, creating unnecessary abstraction and duplication.",
      "proposedFix": "Replace DataBlob with []byte in InternalWorkflowMutableState, InternalWorkflowMutation, and InternalWorkflowSnapshot structs to standardize on proto3 encoding.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "per attribute encoding can in theory allow more flexibility, but in reality, only proto3 is used",
      "number": 2375,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:42.495Z"
    },
    {
      "summary": "WorkflowExecutionInfo.task_queue field is blank in DescribeWorkflowExecutionResponse, though the same information is available in execution_config.task_queue. This is a low-priority bug that doesn't block users due to the workaround.",
      "category": "bug",
      "subcategory": "workflow-introspection",
      "apis": [
        "DescribeWorkflowExecutionResponse",
        "WorkflowExecutionInfo"
      ],
      "components": [
        "workflow-service",
        "api-response",
        "protobuf-definitions"
      ],
      "concepts": [
        "task-queue",
        "workflow-metadata",
        "response-fields",
        "data-completeness",
        "api-consistency"
      ],
      "severity": "low",
      "userImpact": "Users cannot access task_queue information from WorkflowExecutionInfo in DescribeWorkflowExecutionResponse, though they can retrieve it from execution_config as a workaround.",
      "rootCause": "The task_queue field was missing in the response implementation",
      "proposedFix": null,
      "workaround": "Use DescribeWorkflowExecutionResponse.execution_config.task_queue instead of WorkflowExecutionInfo.task_queue",
      "resolution": "fixed",
      "resolutionDetails": "Field was added to the response as confirmed by maintainer",
      "related": [],
      "keyQuote": "DescribeWorkflowExecutionResponse.execution_config.task_queue is set. That unblocks us - this bug is extremely low priority.",
      "number": 2374,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:30.490Z"
    },
    {
      "summary": "Tasks dispatched to sticky task queues fail to find pollers after the original worker is gone, causing a 5-second delay before falling back to non-sticky queues. The system doesn't clear sticky queue flags when workers disconnect.",
      "category": "bug",
      "subcategory": "sticky-task-queue",
      "apis": [],
      "components": [
        "task-dispatcher",
        "sticky-task-queue",
        "worker-matching",
        "mutable-state"
      ],
      "concepts": [
        "sticky-assignment",
        "worker-lifecycle",
        "task-routing",
        "queue-polling",
        "fallback-logic"
      ],
      "severity": "medium",
      "userImpact": "Workflow tasks experience unnecessary 5-second delays when workers disconnect, reducing performance until tasks are routed to available workers.",
      "rootCause": "Sticky task queue flags in mutable state persist after workers disconnect, causing new tasks to be dispatched to queues with no active pollers.",
      "proposedFix": "Check during task matching if a sticky queue has no active pollers in the last X minutes, and if so, add the task to the non-sticky queue instead. When the task is picked by another worker, update the sticky queue assignment.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed via PR #2811 which implements the proposed logic to detect inactive sticky queues and route tasks to non-sticky queues when necessary.",
      "related": [
        2811
      ],
      "keyQuote": "We don't clear that sticky flag when worker is gone. New task will be dispatched to the sticky task queue that no poller is working on.",
      "number": 2363,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:30.173Z"
    },
    {
      "summary": "User requests the ability to register custom dynamicconfig keys beyond those defined in the upstream enum, enabling extensions to Temporal to gate features with their own dynamic configuration flags.",
      "category": "feature",
      "subcategory": "dynamicconfig",
      "apis": [],
      "components": [
        "dynamicconfig",
        "configuration"
      ],
      "concepts": [
        "extensibility",
        "dynamic-configuration",
        "feature-flags",
        "initialization",
        "custom-keys"
      ],
      "severity": "medium",
      "userImpact": "Users building Temporal extensions cannot gate custom features with dynamic configuration without forking the dynamicconfig client.",
      "rootCause": "The dynamicconfig client expects all keys to be predefined in the upstream enum, preventing custom key registration.",
      "proposedFix": "Register new dynamicconfig keys during initialization or via a post-initialization function call.",
      "workaround": "Fork the dynamicconfig client or replace it with a custom implementation that doesn't depend on upstream Keys.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved via PR #2462",
      "related": [
        2462
      ],
      "keyQuote": "I'd like to be able to register new dynamicconfig keys as part of initialization of the dynamicconfig client, or after the fact via a function call.",
      "number": 2360,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:28.542Z"
    },
    {
      "summary": "Request to add a customization API that allows operators to register custom system-level workflows with the Temporal server's worker role, eliminating the need to spin up separate worker applications for operator-specific integrations.",
      "category": "feature",
      "subcategory": "worker-customization",
      "apis": [],
      "components": [
        "worker",
        "server",
        "system-workflows",
        "worker-role"
      ],
      "concepts": [
        "customization",
        "system-workflows",
        "operator-integration",
        "infrastructure-integration",
        "worker-configuration",
        "dependency-injection"
      ],
      "severity": "medium",
      "userImpact": "Operators can more easily integrate custom system workflows with Temporal without requiring separate worker applications.",
      "rootCause": null,
      "proposedFix": "Add a new customization API to ServerOptions that allows specifying custom system workflows, their workflow IDs, and worker configuration options (similar to the Config struct in the worker package).",
      "workaround": "Operators can currently run separate worker applications for custom system workflows.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "allow for a new customization API that would allow the operator to specify system level workflows and corresponding workers that should be run within the worker role",
      "number": 2342,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:17.057Z"
    },
    {
      "summary": "Request for strict configuration validation mode to catch missing or unknown configuration keys during server startup. Currently, the server silently ignores invalid configuration, causing subtle runtime issues that are difficult to debug.",
      "category": "feature",
      "subcategory": "configuration-validation",
      "apis": [],
      "components": [
        "config",
        "bootstrap",
        "yaml-unmarshaling",
        "metrics-config"
      ],
      "concepts": [
        "validation",
        "strict-mode",
        "configuration-parsing",
        "error-handling",
        "yaml-schema",
        "startup-checks"
      ],
      "severity": "medium",
      "userImpact": "Users can misconfigure Temporal Server and not detect the error until runtime when features silently fail, requiring significant debugging effort.",
      "rootCause": "YAML unmarshaling with gopkg.in/yaml.v3 does not reject unknown fields by default, allowing invalid configuration to pass silently through the bootstrap code.",
      "proposedFix": "Option 1: Use yaml.Decoder.KnownFields to reject unknown fields and expand validator.v2 usage in config structs, optionally behind a strict-mode flag. Option 2: Migrate to protobuf for configuration specifications with built-in validation.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I would like the option for strict config validation. If a key is unrecognized, the server should fail to start so it can be corrected.",
      "number": 2341,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:17.959Z"
    },
    {
      "summary": "Request to add SDK version checks through the server version check call, allowing SDKs to verify compatibility with the server.",
      "category": "feature",
      "subcategory": "version-compatibility",
      "apis": [],
      "components": [
        "server",
        "sdk-client",
        "version-check"
      ],
      "concepts": [
        "version-compatibility",
        "sdk-compatibility",
        "server-compatibility",
        "version-negotiation",
        "client-server-protocol"
      ],
      "severity": "medium",
      "userImpact": "Users need a reliable way to verify SDK and server version compatibility to prevent runtime issues from mismatches.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the feature was implemented",
      "related": [],
      "keyQuote": null,
      "number": 2333,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:17.462Z"
    },
    {
      "summary": "Docker Compose PostgreSQL container fails to start because the temporal container cannot resolve the 'postgresql' hostname. The issue occurs with both docker-compose.yml and docker-compose-postgres.yml, with the netcat command failing due to 'bad address' error.",
      "category": "bug",
      "subcategory": "docker-compose-setup",
      "apis": [],
      "components": [
        "docker-compose",
        "postgresql-container",
        "network-connectivity"
      ],
      "concepts": [
        "container-networking",
        "service-discovery",
        "docker-compose-links",
        "port-mapping",
        "hostname-resolution"
      ],
      "severity": "high",
      "userImpact": "Users cannot start Temporal with PostgreSQL using the provided docker-compose configuration, blocking local development and testing workflows.",
      "rootCause": "Missing 'links' property in the temporal container definition prevents Docker from resolving the postgresql service hostname. Additionally, port mapping mismatches between the compose configuration and environment variables can cause connection failures to the wrong PostgreSQL instance.",
      "proposedFix": "Add 'links' property to the temporal container definition to establish service discovery, and ensure DB_PORT environment variable matches the exposed PostgreSQL port in the compose configuration.",
      "workaround": "Manually add 'links: [postgresql]' to the temporal container definition in docker-compose.yml, or adjust POSTGRES_DEFAULT_PORT environment variable to match the container's internal port.",
      "resolution": "fixed",
      "resolutionDetails": "Community identified that adding the 'links' property to the temporal service definition resolves the hostname resolution issue, and proper port configuration prevents port conflicts.",
      "related": [],
      "keyQuote": "I had the same issue but I fixed it by adding the links property in the temporal container definition to allow the containers to communicate with postrgresql",
      "number": 2322,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:04.561Z"
    },
    {
      "summary": "OpenTelemetry dependencies between Temporal server and Go SDK are out of sync (v1.1.0/v0.24.0 vs v1.2.0/v0.25.0), causing conflicts when importing both into a monorepo. User requests keeping these dependencies synchronized across both projects.",
      "category": "feature",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "opentelemetry",
        "metrics",
        "go-sdk",
        "temporal-server"
      ],
      "concepts": [
        "dependency-sync",
        "version-conflict",
        "monorepo",
        "opentelemetry-migration",
        "breaking-changes"
      ],
      "severity": "medium",
      "userImpact": "Users who integrate both Temporal server and Go SDK into a single monorepo face dependency conflicts requiring workarounds like forking releases.",
      "rootCause": "OpenTelemetry has breaking changes in minor versions, and server and SDK independently upgrade to different versions without coordination.",
      "proposedFix": "Keep OpenTelemetry dependencies synchronized across server and Go SDK, or decouple OpenTelemetry by moving related code to its own module with an abstracted metrics interface.",
      "workaround": "Fork the server release and manually update OpenTelemetry versions to match the SDK.",
      "resolution": "fixed",
      "resolutionDetails": "PR #2385 was merged to upgrade OpenTelemetry to v1.2.0, bringing server and SDK dependencies into sync.",
      "related": [
        2385
      ],
      "keyQuote": "Because of this we had to fork the server release and update OpenTelemetry.",
      "number": 2321,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:05.998Z"
    },
    {
      "summary": "Update Thrift library from outdated version 0.10 to v0.13+ (preferably v0.15) to address active CVEs and unify Thrift versions across dependencies. Multiple transitive dependencies on older Thrift versions create security vulnerabilities.",
      "category": "bug",
      "subcategory": "dependency-security",
      "apis": [],
      "components": [
        "thrift-library",
        "ringpop-go",
        "tchannel-go",
        "dependency-management"
      ],
      "concepts": [
        "security",
        "cve",
        "dependency-update",
        "version-unification",
        "breaking-changes"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to known CVEs in outdated Thrift versions, creating potential security vulnerabilities in Temporal Server deployments.",
      "rootCause": "Thrift pinned to old v0.10 version, and ringpop-go depends on multiple outdated Thrift versions (v0.9.3 and older). Latest versions (v0.13+) contain security fixes.",
      "proposedFix": "Update Thrift to minimum v0.13, preferably v0.15. Update tchannel-go and ringpop-go dependencies to support newer Thrift versions to unify across the dependency tree.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        3250
      ],
      "keyQuote": "Latest thrift is 0.15 and beyond 0.13 have some active CVEs. Could you please consider reviewing the thrift versions in those 2 packages?",
      "number": 2320,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:26:05.523Z"
    },
    {
      "summary": "Request to add MongoDB and MongoDB Atlas as a persistent storage backend for Temporal, similar to existing Cassandra and SQL database support. Company heavily uses MongoDB and wants to consolidate their data infrastructure.",
      "category": "feature",
      "subcategory": "persistence-layer",
      "apis": [],
      "components": [
        "persistence-layer",
        "database-adapter",
        "mongodb-driver"
      ],
      "concepts": [
        "persistence",
        "storage-backend",
        "mongodb",
        "data-store",
        "backup-restore",
        "infrastructure-consolidation"
      ],
      "severity": "medium",
      "userImpact": "Users with MongoDB infrastructure could standardize their data persistence layer without requiring multiple databases.",
      "rootCause": null,
      "proposedFix": "Implement MongoDB persistence adapter similar to existing Cassandra and SQL database implementations",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        8908
      ],
      "keyQuote": "it makes sense for us to run everything through MongoDB as our primary data store specially when it comes to backup / restore",
      "number": 2318,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:52.989Z"
    },
    {
      "summary": "Replace the abandoned github.com/dgrijalva/jwt-go library with an actively maintained alternative to mitigate security risks from unpatched exploits. Community fork github.com/golang-jwt/jwt was initially considered but lacks certain validation functions, leading to exploration of alternatives like lestrrat-go/jwx.",
      "category": "other",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "jwt-parser",
        "authentication",
        "token-validation"
      ],
      "concepts": [
        "security",
        "dependency-maintenance",
        "jwt-validation",
        "audience-validation",
        "library-migration"
      ],
      "severity": "high",
      "userImpact": "Users are exposed to potential security vulnerabilities from unpatched exploits in the abandoned jwt-go library.",
      "rootCause": "github.com/dgrijalva/jwt-go is not actively maintained and lacks timely security patches for newly discovered exploits.",
      "proposedFix": "Replace dgrijalva/jwt-go with actively maintained alternative such as golang-jwt/jwt or lestrrat-go/jwx, following the migration guide.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Library was replaced with an actively maintained alternative to address security concerns.",
      "related": [
        142
      ],
      "keyQuote": "JWT token gets parsed with the library github.com/dgrijalva/jwt-go which is not actively maintained and is a subject to the risk of newly discoverable exploits",
      "number": 2313,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:52.697Z"
    },
    {
      "summary": "Add inter-service client side logging for history and matching clients. The issue references existing metric client implementations and asks whether logging should also be added from frontend to matching services.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "matching-client",
        "history-client",
        "frontend-service"
      ],
      "concepts": [
        "logging",
        "inter-service-communication",
        "observability",
        "metrics",
        "client-side-logging"
      ],
      "severity": "low",
      "userImpact": "Better observability of inter-service communication helps developers diagnose issues and monitor system health.",
      "rootCause": null,
      "proposedFix": "Add logging similar to existing metricClient implementation for history and matching clients",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Logging was added for inter-service clients to improve observability",
      "related": [],
      "keyQuote": "Add necessary logging for history / matching client",
      "number": 2308,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:50.939Z"
    },
    {
      "summary": "Multiple retry layers in history service persistence calls cause excessive retries when errors occur. The issue requests consolidating retry logic to specific layers (client, network interceptor, async tasks) rather than having retry logic scattered throughout business logic.",
      "category": "bug",
      "subcategory": "persistence-retry",
      "apis": [],
      "components": [
        "history-service",
        "persistence-layer",
        "network-interceptor",
        "client-layer",
        "workflow-transaction"
      ],
      "concepts": [
        "retry-logic",
        "error-handling",
        "resource-limits",
        "distributed-system",
        "redundant-retries",
        "network-resilience",
        "transaction-management"
      ],
      "severity": "medium",
      "userImpact": "Duplicate retry logic across multiple layers amplifies the number of requests sent during errors, potentially overwhelming services and degrading performance during resource constraint situations.",
      "rootCause": "Retry logic is implemented at multiple independent layers (client, network API, persistence transaction) without coordination, leading to exponential retry attempts when errors occur.",
      "proposedFix": "Consolidate retry logic to specific designated layers: client (2x), service network interceptor (2x), and async task processing, with no retry within business logic.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue author's comment indicates this was resolved, though no specific PR or commit reference is provided.",
      "related": [],
      "keyQuote": "When error actually happen, e.g. resource limit exceeded, retry will be done in multiple layer, causing even more requests sent to e.g. history service",
      "number": 2306,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:39.234Z"
    },
    {
      "summary": "User unable to connect to PostgreSQL database with required SSL/TLS encryption when using temporal-sql-tool and temporalio/auto-setup Docker image. The tool fails because pg_hba.conf rejects unencrypted connections.",
      "category": "question",
      "subcategory": "database-setup",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "auto-setup",
        "database-connection",
        "postgresql-client"
      ],
      "concepts": [
        "encryption",
        "tls",
        "ssl",
        "database-authentication",
        "pg_hba.conf",
        "connection-security"
      ],
      "severity": "medium",
      "userImpact": "Users cannot set up Temporal with PostgreSQL databases that mandate SSL/TLS encryption when using the auto-setup Docker image.",
      "rootCause": "The temporal-sql-tool and auto-setup configuration do not properly handle or default to TLS connections for PostgreSQL, causing authentication failures with strict pg_hba.conf policies requiring encrypted connections.",
      "proposedFix": "Pass --tls flag to temporal-sql-tool or set SQL_TLS environment variables to enable TLS connections. The docker-compose PR #82 provides an example configuration.",
      "workaround": "Set SQL_TLS=true and SQL_TLS_DISABLE_HOST_VERIFICATION=true environment variables, or upgrade auto-setup Docker tag to 1.13.1 or later.",
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #2293 which addresses the same TLS configuration problem with auto-setup.",
      "related": [
        2293
      ],
      "keyQuote": "temporal-sql-tool does allow you to connect with --tls enabled. Is this the problem with auto-setup docker image?",
      "number": 2303,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:38.428Z"
    },
    {
      "summary": "Queries for workflows that are scheduled but not yet started fail in the SDKs because they expect the workflow to have started. The server should eagerly return an error or provide a way for static queries to answer for workflows with zero completed workflow tasks.",
      "category": "feature",
      "subcategory": "query-execution",
      "apis": [],
      "components": [
        "query-handler",
        "workflow-execution",
        "event-loop"
      ],
      "concepts": [
        "query",
        "workflow-execution",
        "scheduling",
        "event-loop",
        "static-queries",
        "workflow-retry"
      ],
      "severity": "medium",
      "userImpact": "Users cannot query workflows during scheduled/retrying states, limiting the ability to inspect workflow state before execution begins.",
      "rootCause": "SDKs expect workflows to be started before handling queries because query handlers are registered during the workflow event loop. For scheduled/retried workflows, this loop hasn't executed yet.",
      "proposedFix": "Add QUERY_RESULT_TYPE_AWAITING_EXECUTION enum to QueryResultType and have server return a typed error. Alternatively, allow static queries to register ahead of time so they can answer for workflows with zero completed workflow tasks.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Duplicate of issue #1310. Discussion evolved to consider allowing static queries for workflows with zero completed tasks rather than just returning an error.",
      "related": [
        1310,
        475,
        2826,
        865
      ],
      "keyQuote": "We do allow a signal to a workflow that didn't start yet. Why should a query be disallowed? We can implement a workaround with the registration of static queues ahead of time.",
      "number": 2300,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:41.455Z"
    },
    {
      "summary": "Nil pointer dereference panic in temporal-sql-tool update command when running schema migrations, causing the tool to crash with a segmentation violation before migrations complete.",
      "category": "bug",
      "subcategory": "sql-tool",
      "apis": [],
      "components": [
        "sql-tool",
        "schema-migration",
        "connection-handler"
      ],
      "concepts": [
        "nil-pointer",
        "panic-recovery",
        "database-connection",
        "error-handling",
        "schema-update"
      ],
      "severity": "critical",
      "userImpact": "Users cannot run temporal-sql-tool migrations due to random panics, blocking deployment and initialization.",
      "rootCause": "Connection object is nil when ReadSchemaVersion is called, causing nil pointer dereference in Connection.Close() method.",
      "proposedFix": "Add nil checks before dereferencing connection object and implement proper error handling in UpdateTask.Run()",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in PR #2250",
      "related": [
        2250
      ],
      "keyQuote": "panic: runtime error: invalid memory address or nil pointer dereference in go.temporal.io/server/tools/sql.(*Connection).ReadSchemaVersion",
      "number": 2294,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:24.062Z"
    },
    {
      "summary": "The auto-setup Docker container fails to connect to PostgreSQL when TLS is required, with SSL connections being rejected despite SQL_TLS_ENABLED environment variable being set. The container reverts to non-TLS connections causing database creation and migration failures.",
      "category": "bug",
      "subcategory": "database-setup",
      "apis": [],
      "components": [
        "auto-setup",
        "docker-entrypoint",
        "sql-tool",
        "config-template"
      ],
      "concepts": [
        "TLS",
        "SSL",
        "PostgreSQL",
        "database-connection",
        "environment-variables",
        "container-startup"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy Temporal with PostgreSQL instances requiring TLS/SSL connections, blocking setup on secured database platforms like Zalando Postgres Operator and Azure PostgreSQL.",
      "rootCause": "Environment variables for TLS configuration (SQL_TLS, SQL_TLS_ENABLED, SQL_HOST_VERIFICATION) are not being properly applied during the config template rendering in the auto-setup entrypoint.",
      "proposedFix": "Update docker config_template.yaml to properly default TLS settings to true and ensure all TLS-related environment variables (SQL_TLS, SQL_TLS_ENABLED, SQL_HOST_VERIFICATION, SQL_HOST_NAME) are correctly mapped in the configuration template.",
      "workaround": "Manually set multiple environment variables: SQL_TLS=true, SQL_TLS_ENABLED=true, SQL_TLS_DISABLE_HOST_VERIFICATION=true, and SQL_HOST_VERIFICATION=true in the container or Kubernetes deployment.",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved through configuration template updates and documentation of proper environment variable settings for TLS-enabled PostgreSQL connections.",
      "related": [],
      "keyQuote": "pq: pg_hba.conf rejects connection for host \"10.0.1.59\", user \"airbyte\", database \"postgres\", SSL off",
      "number": 2293,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:26.428Z"
    },
    {
      "summary": "Feature request to add a consistent indicator, label, or tag to log output from Temporal services to make it easier to identify Temporal-originated log lines in mixed logs. The requester notes this may need implementation in the SDK (specifically sdk-go) rather than just the server.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "logging",
        "output-formatting"
      ],
      "concepts": [
        "log-identification",
        "output-clarity",
        "mixed-logs",
        "service-identification",
        "observability"
      ],
      "severity": "low",
      "userImpact": "Users have difficulty distinguishing Temporal log lines from other services in mixed log environments.",
      "rootCause": null,
      "proposedFix": "Add a well-known indicator, label, or tag to log output from Temporal services",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When looking at mixed logs it's not always easy to clearly see what lines are from temporal services and what isn't.",
      "number": 2283,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:23.444Z"
    },
    {
      "summary": "ActivityTaskStarted events are buffered until activity completion, causing workflow execution histories to not show that an activity was ever started when the workflow times out or closes. This makes debugging difficult as the history appears as if the activity was never picked up by a worker.",
      "category": "bug",
      "subcategory": "workflow-history",
      "apis": [],
      "components": [
        "history",
        "activity-executor",
        "workflow-state-machine"
      ],
      "concepts": [
        "history-fidelity",
        "debugging",
        "event-buffering",
        "activity-lifecycle",
        "timeout",
        "workflow-closure"
      ],
      "severity": "medium",
      "userImpact": "Users cannot see in the execution history that an activity was started when a workflow times out or closes, making it harder to debug activity-related issues.",
      "rootCause": "ActivityTaskStarted events are delayed or buffered until activity completion to simplify state machine processing, but these buffered events are not flushed when the workflow closes.",
      "proposedFix": "Flush buffered ActivityTaskStarted events before closing the workflow (on completion, termination, cancellation, or timeout).",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Flush buffered ActivityTaskStarted events before closing the workflow [completion, termination, cancellation, timeout]",
      "number": 2282,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:10.338Z"
    },
    {
      "summary": "User requests tutorials and learning paths for Temporal, citing difficulty understanding core concepts like activities and workers. Current documentation focuses on reference material and parameters rather than real-world examples and conceptual understanding needed for framework adoption.",
      "category": "docs",
      "subcategory": "learning-resources",
      "apis": [],
      "components": [
        "documentation",
        "learning-materials"
      ],
      "concepts": [
        "activity",
        "worker",
        "retry",
        "learning-path",
        "tutorials",
        "onboarding",
        "conceptual-understanding"
      ],
      "severity": "medium",
      "userImpact": "New users struggle to effectively learn and adopt Temporal due to reference-heavy documentation lacking practical examples and learning structure.",
      "rootCause": null,
      "proposedFix": "Create comprehensive tutorials similar to AWS or Baeldung documentation, or provide a structured learning path with sequential documents and real-life examples (e.g., order processing scenarios).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue addressed through improved documentation and learning resources provided to the community.",
      "related": [],
      "keyQuote": "They just feel like references instead of something that help you understand the framework. There are lack of real-life examples when a concept is explained",
      "number": 2274,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:09.412Z"
    },
    {
      "summary": "User-facing error messages when database constraints are violated are opaque and don't indicate the actual problem. For example, a workflow type name exceeding the 255-character limit shows a raw database error instead of actionable guidance.",
      "category": "feature",
      "subcategory": "error-messages",
      "apis": [],
      "components": [
        "persistence",
        "database",
        "error-handling"
      ],
      "concepts": [
        "error-messages",
        "database-constraints",
        "user-experience",
        "validation",
        "actionable-feedback"
      ],
      "severity": "medium",
      "userImpact": "Users encounter cryptic database error messages that require understanding of internal schema to diagnose and fix the underlying issue.",
      "rootCause": "Database constraint violations are returned as raw database errors without interpretation or mapping to user-understandable problems.",
      "proposedFix": "Create an adapter that interprets database-specific errors and transforms them into actionable user messages that explain the constraint and its limit.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2122
      ],
      "keyQuote": "These messages should be actionable by users without understanding the database schema / temporal internals.",
      "number": 2252,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:25:11.960Z"
    },
    {
      "summary": "When workflow code deploys invalid commands to the server, workers enter a spin loop of immediate task failures and resubmissions. The request is for the Temporal Server to implement exponential backoff before creating new WorkflowTaskScheduled events for previously failed tasks, with optional pausing after multiple attempts.",
      "category": "feature",
      "subcategory": "workflow-task-execution",
      "apis": [],
      "components": [
        "workflow-task-executor",
        "server-task-scheduler",
        "worker-process"
      ],
      "concepts": [
        "exponential-backoff",
        "task-failure-recovery",
        "spin-loop-prevention",
        "server-side-validation",
        "retry-logic"
      ],
      "severity": "high",
      "userImpact": "Malformed workflow commands can cause workers to DDOS the server through rapid task resubmissions, potentially bringing down deployments.",
      "rootCause": "Server immediately resubmits workflow tasks after validation failures without backoff, creating a spin loop when workers produce invalid commands.",
      "proposedFix": "Implement exponential backoff on the server before creating new WorkflowTaskScheduled events for failed tasks. After a number of attempts, consider pausing completely using timers.",
      "workaround": "SDKs can simulate workflow task timeout on subsequent failures (attempt > 1) by dropping tasks without reporting, causing retry after task timeout period (default 10 seconds).",
      "resolution": "fixed",
      "resolutionDetails": "Partially mitigated in #2548 by simulating timeout instead of registering workflow task failure for attempts >= 2. Additional pause functionality requested separately in #3006.",
      "related": [
        1303,
        901,
        2548,
        3006
      ],
      "keyQuote": "User in the original issue actually was able to DDOS their Temporal server deployment by workers deployment that causes all the workers to submit malformed commands",
      "number": 2238,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:59.024Z"
    },
    {
      "summary": "Request to add a configuration flag enabling self-signed certificates for Elasticsearch transport in Temporal server. Currently, connections to Elasticsearch fail when using self-signed TLS certificates.",
      "category": "feature",
      "subcategory": "elasticsearch-tls",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "transport-security",
        "configuration"
      ],
      "concepts": [
        "self-signed-certificates",
        "tls",
        "elasticsearch",
        "configuration-flag",
        "security"
      ],
      "severity": "medium",
      "userImpact": "Users running Elasticsearch with self-signed certificates cannot connect Temporal server to their Elasticsearch instance without workarounds.",
      "rootCause": "Elasticsearch client configuration lacks support for disabling certificate validation or accepting self-signed certificates.",
      "proposedFix": "Add a configuration flag to enable self-signed certificates, similar to the approach in olivere/elastic library.",
      "workaround": "Bundle CA certificate with the image or manually update Temporal startup script to import CAs from mounted folder.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Enhance config options with a flag which enables self-signed certificates",
      "number": 2230,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:58.529Z"
    },
    {
      "summary": "Task queue root partition in standby cluster may persist forwarded tasks without TTL (time-to-live), potentially causing indefinite retention of orphaned tasks.",
      "category": "bug",
      "subcategory": "task-queue-management",
      "apis": [],
      "components": [
        "matching-service",
        "task-queue-manager",
        "standby-cluster"
      ],
      "concepts": [
        "ttl",
        "task-persistence",
        "task-forwarding",
        "standby-cluster",
        "partition",
        "task-cleanup"
      ],
      "severity": "medium",
      "userImpact": "Tasks in standby cluster task queues may persist indefinitely without TTL, potentially consuming storage and causing unexpected behavior.",
      "rootCause": "Task queue root partition forwarding logic in taskQueueManager.go was not setting TTL when persisting forwarded tasks in standby clusters.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #2327",
      "related": [
        2327
      ],
      "keyQuote": "standby cluster's task queue root partition may persist forwarded task without ttl",
      "number": 2211,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:56.696Z"
    },
    {
      "summary": "SQLite persistence in Temporalite is not populating the task_queue field in WorkflowExecutionInfo when calling ListClosedWorkflowExecutions, while Postgres and Elastic implementations correctly return this field.",
      "category": "bug",
      "subcategory": "persistence-sqlite",
      "apis": [
        "ListClosedWorkflowExecutions"
      ],
      "components": [
        "persistence-layer",
        "sql-plugin",
        "visibility-storage"
      ],
      "concepts": [
        "persistence",
        "data-model",
        "field-mapping",
        "sqlite",
        "workflow-metadata"
      ],
      "severity": "medium",
      "userImpact": "Users testing workflows with Temporalite (SQLite backend) cannot access task queue information through ListClosedWorkflowExecutions, making it incompatible with production persistence layers.",
      "rootCause": "TaskQueue field was not wired into the SQL persistence data model (VisibilityRow) for both read and write paths, despite being defined in the database schema.",
      "proposedFix": "Add TaskQueue to the VisibilityRow data model in sqlplugin/visibility.go and update SQL statements for both write and read operations to include this field, following the pattern used for other fields like workflow_type_name.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Field was added to data model and SQL statements for both read and write paths.",
      "related": [],
      "keyQuote": "TaskQueue was not defined in our SQL persistence data model, but fortunately it is defined in db schema.",
      "number": 2207,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:45.589Z"
    },
    {
      "summary": "Users need the ability to mutate workflow metadata attributes like cron schedule, workflow task timeout, and retry policy after workflow creation. The most critical issue is workflow task timeout, which can cause workflows to be permanently stuck when processing long histories.",
      "category": "feature",
      "subcategory": "workflow-metadata",
      "apis": [],
      "components": [
        "workflow-execution",
        "task-processor",
        "metadata-management"
      ],
      "concepts": [
        "workflow-timeout",
        "cron-schedule",
        "retry-policy",
        "mutability",
        "workflow-recovery"
      ],
      "severity": "high",
      "userImpact": "Users cannot update critical workflow parameters like task timeout after creation, causing workflows to become stuck in certain scenarios involving long histories.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #1499",
      "related": [
        1499
      ],
      "keyQuote": "The most critical one is workflow task timeout at the moment because it is impossible to recover from in some cases.",
      "number": 2191,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:44.130Z"
    },
    {
      "summary": "Implement a GetServerInfo RPC on the workflow service to expose server capabilities, version information, and metadata. This enables clients to discover server capabilities like signal/query headers and error differentiation support for better compatibility.",
      "category": "feature",
      "subcategory": "api-design",
      "apis": [],
      "components": [
        "workflow-service",
        "api-definitions",
        "capabilities-discovery"
      ],
      "concepts": [
        "server-metadata",
        "capability-negotiation",
        "version-discovery",
        "backwards-compatibility",
        "client-server-communication"
      ],
      "severity": "medium",
      "userImpact": "Clients can discover server capabilities at runtime, enabling better error handling and feature adaptation without hardcoding assumptions.",
      "rootCause": null,
      "proposedFix": "Add GetServerInfo RPC to WorkflowService that returns server version, version info, metadata, and a Capabilities message containing boolean flags for supported features.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented with GetServerInfo RPC returning server capabilities including signal/query headers and internal error differentiation support.",
      "related": [],
      "keyQuote": "All capabilities the server supports. It is clearer and more forward compatible as a message than a repeated enum or other form.",
      "number": 2190,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:46.953Z"
    },
    {
      "summary": "Go vulnerability CVE-2021-39293 is present in Temporal server binaries and container images. While Temporal doesn't use the vulnerable code directly, the base Docker image uses an older Go version that includes the vulnerability.",
      "category": "bug",
      "subcategory": "build-and-deployment",
      "apis": [],
      "components": [
        "docker-image",
        "base-builder",
        "build-process"
      ],
      "concepts": [
        "vulnerability",
        "golang-version",
        "supply-chain",
        "container-security",
        "cve"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal using official Docker images are exposed to a known Go vulnerability (CVE-2021-39293) in the container environment.",
      "rootCause": "Official Docker images built with Go versions older than 1.16.8 or 1.17.1, which contain the vulnerability.",
      "proposedFix": "Rebuild Docker images with patched Go version (1.16.12+ or 1.17.6+) to eliminate the vulnerability.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by updating Go version to 1.17.6 for Temporal server versions 1.13, 1.14, and 1.15.",
      "related": [],
      "keyQuote": "golang 1.17.6 is used for temporal server 1.13, 1.14 and 1.15.",
      "number": 2184,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:33.910Z"
    },
    {
      "summary": "User questioning whether execution DB and history tree/node DB are separate databases, noting different shard DB algorithms. The clarification confirms they are in the same DB, but user still seeks understanding of how range_id field is processed in history service.",
      "category": "question",
      "subcategory": "database-architecture",
      "apis": [],
      "components": [
        "execution-db",
        "history-service",
        "database-sharding"
      ],
      "concepts": [
        "database-sharding",
        "shard-algorithm",
        "range-id",
        "database-architecture",
        "history-management"
      ],
      "severity": "low",
      "userImpact": "Users may be confused about Temporal's internal database structure and how data is partitioned across shards.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "stale",
      "resolutionDetails": "Closed after 30 days (auto-close label indicates no follow-up or resolution provided)",
      "related": [],
      "keyQuote": "They are in same DB. What are the different shard db algorithm?",
      "number": 2172,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:31.362Z"
    },
    {
      "summary": "User asks how to inspect BLOB field details in MySQL database used by Temporal Server. The issue was answered with information about proto encoding and the tctl admin shard command.",
      "category": "question",
      "subcategory": "database-inspection",
      "apis": [],
      "components": [
        "persistence",
        "database",
        "mysql"
      ],
      "concepts": [
        "blob-inspection",
        "proto-encoding",
        "data-visibility",
        "database-debugging",
        "admin-tools"
      ],
      "severity": "low",
      "userImpact": "Users need guidance on inspecting binary data stored in MySQL BLOB fields for debugging and administrative purposes.",
      "rootCause": null,
      "proposedFix": "Use tctl admin shard command to display shard info data which contains the proto-encoded blob content.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was a support question redirected to community forum and Slack; marked for closure as it's not a bug or feature request.",
      "related": [],
      "keyQuote": "The blob data is proto encoded. You can use tctl admin shard command to show shard info data.",
      "number": 2157,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:32.398Z"
    },
    {
      "summary": "User asks how to support DSL (Domain-Specific Language) workflows in Temporal. The discussion clarifies that DSLs require translation layers to map to Temporal workflows/activities, with examples provided from existing samples.",
      "category": "question",
      "subcategory": "dsl-support",
      "apis": [],
      "components": [
        "workflow-execution",
        "activity-execution",
        "dsl-engine"
      ],
      "concepts": [
        "dsl-translation",
        "workflow-definition",
        "activity-mapping",
        "lambda-functions",
        "rest-invocation"
      ],
      "severity": "low",
      "userImpact": "Users seeking guidance on implementing DSL-based workflows in Temporal are directed to samples and community resources.",
      "rootCause": null,
      "proposedFix": "Refer to existing DSL samples in samples-go and samples-java repositories, and use community forum for detailed guidance.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Closed as this is a question better suited for community forum/Slack rather than issue tracker.",
      "related": [],
      "keyQuote": "Any dsl has an extra translation step from its markup to some object model representation that defines execution instructions",
      "number": 2153,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:19.930Z"
    },
    {
      "summary": "Cron workflows that use continue as new lose their cron schedule, causing the workflow to no longer be triggered for the next scheduled run. The history limit is reduced but scheduled execution is lost.",
      "category": "bug",
      "subcategory": "workflow-continuation",
      "apis": [
        "ContinueAsNew",
        "CronSchedule"
      ],
      "components": [
        "workflow-engine",
        "cron-scheduler",
        "history-management"
      ],
      "concepts": [
        "cron-scheduling",
        "continue-as-new",
        "history-limit",
        "workflow-execution",
        "state-preservation"
      ],
      "severity": "high",
      "userImpact": "Users cannot reduce workflow history in cron workflows using continue as new without losing the cron schedule, forcing them to choose between managing history or maintaining scheduled execution.",
      "rootCause": "When user code executes continue as new, the CronSchedule option is lost on the new workflow run.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #746, which was a known issue with the same root cause of losing CronSchedule during continue as new operations.",
      "related": [
        148,
        746
      ],
      "keyQuote": "Continue as new solves the history limit problem but creates none scheduled workflows",
      "number": 2146,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:19.465Z"
    },
    {
      "summary": "ResetWorkflowExecution treats requests with empty requestId as duplicates and ignores subsequent calls, causing a deduplication issue where only the first reset succeeds. Users expect idempotent behavior but instead see silent failures on retries.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [
        "ResetWorkflowExecution",
        "ResetWorkflowExecutionRequest"
      ],
      "components": [
        "server-reset",
        "request-deduplication",
        "api-semantics"
      ],
      "concepts": [
        "idempotency",
        "deduplication",
        "request-id",
        "retry",
        "workflow-state"
      ],
      "severity": "high",
      "userImpact": "Users cannot reset workflows multiple times as subsequent reset requests silently fail due to empty requestId deduplication, breaking expected idempotent behavior.",
      "rootCause": "Server deduplicates requests based on requestId, treating empty/null requestId as a valid deduplication key rather than disabling deduplication or requiring a unique ID.",
      "proposedFix": "Either make requestId mandatory, generate random IDs for empty requestId, skip deduplication for empty requestId, or error on empty requestId requests.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was identified as a bug in the reset workflow execution deduplication logic and addressed in the codebase.",
      "related": [],
      "keyQuote": "resetWorkflowExecution is idempotent and duplicate requests with the same requestId is getting ignored, including an empty one. Such a request will work right at the first invocation, which makes the problem even hard to catch.",
      "number": 2140,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:20.572Z"
    },
    {
      "summary": "JWT authentication fails when only one JWKS provider URI is configured via TEMPORAL_JWT_KEY_SOURCE1 because the second environment variable (TEMPORAL_JWT_KEY_SOURCE2) remains empty, causing the key provider to attempt fetching from an empty string and fail.",
      "category": "bug",
      "subcategory": "authentication-jwt",
      "apis": [],
      "components": [
        "default-token-key-provider",
        "authorization",
        "jwt-configuration"
      ],
      "concepts": [
        "jwt",
        "authentication",
        "jwks",
        "environment-variables",
        "configuration",
        "tls"
      ],
      "severity": "high",
      "userImpact": "Users cannot use JWT authentication with only a single JWKS provider URI in the configuration template, as the server fails to refresh token keys.",
      "rootCause": "The default token key provider iterates through all configured JWKS URIs without checking if they are empty before attempting to fetch from them, causing HTTP errors when it encounters empty string URIs.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The code was updated to handle empty JWKS URIs properly in the token key provider.",
      "related": [],
      "keyQuote": "error while refreshing token keys: Get \"\": unsupported protocol scheme \"\"",
      "number": 2134,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:06.914Z"
    },
    {
      "summary": "User requests support for Cassandra 4 upgrade from Cassandra 3, citing the ability to set custom connection ports per host in a cluster. The issue was resolved by confirming Cassandra 4.0 is already supported in Temporal.",
      "category": "feature",
      "subcategory": "cassandra-upgrade",
      "apis": [],
      "components": [
        "persistence",
        "cassandra",
        "database-configuration"
      ],
      "concepts": [
        "cassandra",
        "upgrade",
        "port-configuration",
        "database-dependency",
        "cluster"
      ],
      "severity": "low",
      "userImpact": "Users wanting to use Cassandra 4 with custom port configurations per host can now do so with Temporal.",
      "rootCause": null,
      "proposedFix": "Upgrade to Cassandra 4.0, which provides the flexibility to set different connection ports on different hosts in the same cluster.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Cassandra 4.0 is already supported as documented in Temporal's dependency versions documentation.",
      "related": [],
      "keyQuote": "Cassandra 4.0 is supported: https://docs.temporal.io/clusters#dependency-versions",
      "number": 2132,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:05.406Z"
    },
    {
      "summary": "getWorkflowExecutionHistory API returns INVALID_ARGUMENT error instead of NOT_FOUND when querying archival namespace history that doesn't exist. The status code is mismatched and should return NOT_FOUND for this situation.",
      "category": "bug",
      "subcategory": "workflow-history",
      "apis": [
        "getWorkflowExecutionHistory"
      ],
      "components": [
        "history-service",
        "archival",
        "api-handler"
      ],
      "concepts": [
        "error-handling",
        "status-code",
        "archival",
        "workflow-history",
        "namespace"
      ],
      "severity": "medium",
      "userImpact": "Users querying workflow history in archival namespaces receive incorrect error codes, making error handling and debugging more difficult.",
      "rootCause": "Status code mapping logic incorrectly returns INVALID_ARGUMENT instead of NOT_FOUND when workflow history doesn't exist in archival namespace",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #3340",
      "related": [
        3340
      ],
      "keyQuote": "getWorkflowExecutionHistory for archival namespace returns \"INVALID_ARGUMENT: requested workflow history does not exist\" error. It looks like the status code is mismatched and NOT_FOUND should be returned",
      "number": 2130,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:24:04.466Z"
    },
    {
      "summary": "ErrShardClosed error is returned directly to clients instead of being converted to ShardOwnershipLost and retried on a different history node. When a history node shuts down during deployment, clients receive a serviceerror.Unknown instead of proper error handling.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "history-shard",
        "frontend",
        "error-handling"
      ],
      "concepts": [
        "error-conversion",
        "retry-logic",
        "node-shutdown",
        "deployment",
        "shard-ownership"
      ],
      "severity": "high",
      "userImpact": "Clients receive unknown errors during history node restarts and cannot properly retry requests, potentially causing workflow execution failures during deployments.",
      "rootCause": "ErrShardClosed is not being converted to serviceerrors.ShardOwnershipLost before being returned to the client, preventing the frontend from retrying on a different node.",
      "proposedFix": "Convert ErrShardClosed to serviceerrors.ShardOwnershipLost so the frontend can handle the error properly and retry on a different history node.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Error handling was implemented to properly convert ErrShardClosed to ShardOwnershipLost before returning to client.",
      "related": [],
      "keyQuote": "ErrShardClosed is returned to the client if request reached history node when node is shutting down (during deployment, for instance).",
      "number": 2128,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:50.362Z"
    },
    {
      "summary": "Add header propagation support for signals and queries, similar to existing support for workflow and activity calls. This feature was already added to the API protos and now needs implementation in the SDKs.",
      "category": "feature",
      "subcategory": "signal-query-headers",
      "apis": [
        "Signal",
        "Query"
      ],
      "components": [
        "signal-handler",
        "query-handler",
        "header-propagation"
      ],
      "concepts": [
        "header-propagation",
        "signals",
        "queries",
        "metadata-propagation",
        "distributed-tracing"
      ],
      "severity": "medium",
      "userImpact": "Users cannot propagate headers (metadata like trace context) through signal and query calls, limiting observability and context passing in these communication patterns.",
      "rootCause": "Header propagation logic was implemented for workflows and activities but not extended to signals and queries despite API proto support.",
      "proposedFix": "Implement header propagation between signal/query calls and their corresponding events, following the same pattern used for workflows and activities.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation completed as indicated by the 'Done' comment from the issue author.",
      "related": [
        124
      ],
      "keyQuote": "Need to implement the propagation of the header values between the signal/query calls and the events.",
      "number": 2125,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:51.827Z"
    },
    {
      "summary": "SQL schema limits workflow_id column to 255 characters, but the system allows configuration up to 1000 characters (maxIDLength), creating a mismatch that causes rejection of longer workflow IDs on SQL persistence backends.",
      "category": "bug",
      "subcategory": "persistence-schema",
      "apis": [],
      "components": [
        "persistence",
        "sql-schema",
        "workflow-id"
      ],
      "concepts": [
        "schema-limits",
        "configuration-mismatch",
        "id-length",
        "database-constraints",
        "sql-persistence"
      ],
      "severity": "high",
      "userImpact": "Users cannot create workflows with IDs longer than 255 characters despite configuring maxIDLength up to 1000, causing unexpected rejections in production.",
      "rootCause": "SQL schema VARCHAR(255) column limit for workflow_id is inconsistent with the dynamic configuration value (default 1000) that claims to allow longer IDs.",
      "proposedFix": "Either expand database schema to support 1000 character IDs (or 1024) and hard-code the workflow ID limit to 1000, or reduce maxIDLength dynamic config to 255. Also consider applying the same fix to all other ID fields governed by maxIDLength (Namespace, TaskQueue, ActivityID, TimerID, WorkflowType, ActivityType, SignalName, MarkerName, ErrorReason/FailureReason/CancelCause, Identity, RequestID).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Schema was updated to support longer IDs and maxIDLength limit was standardized across the system.",
      "related": [],
      "keyQuote": "Workflow ids longer than 255 chars are rejected on SQL persistence because of schema limitation",
      "number": 2122,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:54.204Z"
    },
    {
      "summary": "User cannot access the CronSchedule of a running workflow through DescribeWorkflowExecution, but needs this information to compare workflow options and determine if a workflow needs to be rescheduled.",
      "category": "feature",
      "subcategory": "workflow-description",
      "apis": [
        "DescribeWorkflowExecution",
        "GetWorkflowHistory"
      ],
      "components": [
        "workflow-execution",
        "api-server",
        "history-service"
      ],
      "concepts": [
        "cron-schedule",
        "workflow-options",
        "workflow-comparison",
        "execution-metadata",
        "configuration-retrieval"
      ],
      "severity": "medium",
      "userImpact": "Users cannot retrieve cron schedule information from existing workflows, preventing them from making informed decisions about workflow updates without accessing raw history events.",
      "rootCause": "CronSchedule is not exposed in the DescribeWorkflowExecution response, though it is available in the WorkflowExecutionStartedEventAttributes in the execution history.",
      "proposedFix": "Add support for returning cron schedule information in the DescribeWorkflowExecution API response.",
      "workaround": "Use client.GetWorkflowHistory to access the execution-started history event and extract CronSchedule from WorkflowExecutionStartedEventAttributes.",
      "resolution": "fixed",
      "resolutionDetails": "Feature request was acknowledged and transferred to the server project for implementation.",
      "related": [],
      "keyQuote": "I cannot access CronSchedule, which is a value I expect to change. It's the one affecting me but I would like a general way to accessing the options of a previous workflow",
      "number": 2119,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:37.316Z"
    },
    {
      "summary": "Docker Compose setup fails on M1 Mac with Temporal v1.13 due to a segmentation fault in temporal-sql-tool during schema initialization, causing DNS resolution failures in Temporal Web. The issue was resolved in v1.14.2.",
      "category": "bug",
      "subcategory": "docker-setup",
      "apis": [],
      "components": [
        "docker-compose",
        "temporal-sql-tool",
        "schema-initialization",
        "postgresql"
      ],
      "concepts": [
        "segmentation-fault",
        "dns-resolution",
        "arm-architecture",
        "docker-setup",
        "schema-migration",
        "m1-mac"
      ],
      "severity": "high",
      "userImpact": "Users on M1 Mac cannot run Temporal locally using docker-compose due to crashes during database schema setup.",
      "rootCause": "Segmentation fault in temporal-sql-tool during PostgreSQL schema initialization on ARM architecture (M1 Mac), likely related to CGO bindings as noted in comments.",
      "proposedFix": "Provide ARM-compatible Docker images to mitigate architecture-related issues.",
      "workaround": "Run `docker system prune -a && git reset 0e68621 --hard && docker-compose up` to clear Docker state and use an earlier commit.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved in version 1.14.2",
      "related": [],
      "keyQuote": "qemu: uncaught target signal 11 (Segmentation fault) - core dumped",
      "number": 2117,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:39.354Z"
    },
    {
      "summary": "Add a metric tag to differentiate between two types of rate limiting issues: RPS (requests per second) limit exceeded versus poller concurrent limit exceeded. This enables better observability and troubleshooting of rate limiting problems.",
      "category": "feature",
      "subcategory": "metrics-observability",
      "apis": [],
      "components": [
        "metrics",
        "rate-limiter",
        "poller"
      ],
      "concepts": [
        "rate-limiting",
        "metrics-tagging",
        "observability",
        "throttling",
        "RPS",
        "concurrency-limits"
      ],
      "severity": "medium",
      "userImpact": "Users cannot distinguish between different rate limiting causes in metrics, making it harder to diagnose and resolve throttling issues.",
      "rootCause": "Lack of metric tags to differentiate RPS limit exceeded from poller concurrent limit exceeded.",
      "proposedFix": "Add metric tags that indicate the cause of throttling (RPS vs poller concurrent limit).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented via PR #2423 with metric tags to differentiate rate limiting causes.",
      "related": [
        2423
      ],
      "keyQuote": "Currently, throttling could be due to RPS exceed or poller concurrent limit exceed. We need the ability to differentiate them from metrics.",
      "number": 2108,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:37.804Z"
    },
    {
      "summary": "Add per-namespace gauge metrics to report how many task queues are loaded on each matching service. This helps diagnose issues with excessive task queue loading.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "matching-service",
        "metrics",
        "task-queue-manager"
      ],
      "concepts": [
        "monitoring",
        "diagnostics",
        "task-queues",
        "namespace-isolation",
        "performance-metrics",
        "resource-usage"
      ],
      "severity": "medium",
      "userImpact": "Users can better diagnose and troubleshoot issues related to too many task queues being loaded on matching services through new observability metrics.",
      "rootCause": null,
      "proposedFix": "Implement new per-namespace gauge metrics tracking the number of loaded task queues on each matching service.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation completed as indicated by the author's comment 'this is done'.",
      "related": [],
      "keyQuote": "Need new metrics to report per-namespace gauge on how many task queues are loaded on each matching service.",
      "number": 2107,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:25.582Z"
    },
    {
      "summary": "Request for the history server to emit metrics about cache size and capacity for both mutable cache and history event cache, allowing operators to monitor cache utilization.",
      "category": "feature",
      "subcategory": "monitoring-metrics",
      "apis": [],
      "components": [
        "history-server",
        "cache",
        "metrics"
      ],
      "concepts": [
        "observability",
        "monitoring",
        "cache-metrics",
        "capacity-tracking",
        "performance-tuning"
      ],
      "severity": "low",
      "userImpact": "Users cannot currently monitor history server cache utilization, making it difficult to optimize cache configuration and detect capacity issues.",
      "rootCause": null,
      "proposedFix": "Emit metrics for mutable cache current size and capacity, and history event cache current size and capacity.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "for mutable cache and history event cache. current size and cache capacity.",
      "number": 2101,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:24.460Z"
    },
    {
      "summary": "DNS resolution error appears in Temporal Web UI when typing in text input fields when running the docker-compose example on Mac M1. The error is triggered during text input interactions but doesn't prevent basic functionality like accessing localhost:8088.",
      "category": "bug",
      "subcategory": "web-ui-docker",
      "apis": [],
      "components": [
        "web-ui",
        "docker-compose",
        "dns-resolver"
      ],
      "concepts": [
        "dns-resolution",
        "docker-networking",
        "web-ui-interaction",
        "error-handling",
        "m1-compatibility"
      ],
      "severity": "low",
      "userImpact": "Users encounter DNS resolution error messages in the Web UI during text input, creating a poor experience despite core functionality working.",
      "rootCause": "DNS resolution failure triggered during text input interactions in the Web UI, likely related to docker networking configuration on M1 Macs.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Verified resolved in v1.24 or later; issue no longer reproducible on current versions.",
      "related": [],
      "keyQuote": "Verified that this is no longer repro on v1.24 at least.",
      "number": 2097,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:26.186Z"
    },
    {
      "summary": "The Temporal server's gRPC health check implementation doesn't support empty service queries and requires separate probes for each service (workflow, history, matching). The request is to use the standard gRPC health package which provides better service discovery and supports both empty queries and per-service health checks.",
      "category": "feature",
      "subcategory": "grpc-health-checks",
      "apis": [],
      "components": [
        "frontend",
        "matching-service",
        "history-service",
        "workflow-service"
      ],
      "concepts": [
        "health-checks",
        "grpc",
        "service-discovery",
        "monitoring",
        "server-health",
        "probes"
      ],
      "severity": "medium",
      "userImpact": "Users must craft separate health probe commands for each Temporal service instead of having a single unified health check endpoint.",
      "rootCause": "Custom gRPC health check implementation doesn't handle empty service queries and doesn't support the standard grpc.health.v1.Health service interface.",
      "proposedFix": "Replace the custom health check implementation with the standard gRPC health package from google.golang.org/grpc/health, which supports empty queries, multiple services, and the Watch API.",
      "workaround": "Users can specify the full service name explicitly: grpc_health_probe -addr localhost:7233 -service temporal.api.workflowservice.v1.WorkflowService",
      "resolution": "fixed",
      "resolutionDetails": "The standard gRPC health check implementation was adopted, allowing both empty service queries and per-service health checks to work across all components.",
      "related": [],
      "keyQuote": "Health checking without specifying a service should respond with the overall health of the server.",
      "number": 2096,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:13.411Z"
    },
    {
      "summary": "Custom search attributes cache takes 1 minute to invalidate after updates, causing failures in automated tests and confusion for new users. Frontend cache invalidates immediately but history cache doesn't, creating inconsistency where tctl shows updated attributes but workflow requests fail.",
      "category": "other",
      "subcategory": "search-attributes-cache",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "search-attributes",
        "cache-invalidation",
        "history-service",
        "gossip-protocol"
      ],
      "concepts": [
        "cache-coherence",
        "cache-invalidation",
        "dynamic-configuration",
        "cluster-consistency",
        "testing-support"
      ],
      "severity": "medium",
      "userImpact": "Users experience test failures and inconsistent behavior when using custom search attributes immediately after creation, and must wait for cache expiration.",
      "rootCause": "Search attributes cache has a fixed 1-minute invalidation interval without cluster-wide invalidation mechanism, causing delayed propagation of updates across nodes.",
      "proposedFix": "Add dynamic config to bypass cache for search attributes reads (especially for testing), allowing read-through cache strategy with database fallback when fresh data is required.",
      "workaround": "Reduce cache invalidation interval to 1 second using environment variable for test environments (e.g., docker-compose), or wait for the 1-minute cache expiration.",
      "resolution": "fixed",
      "resolutionDetails": "Implemented read-through cache strategy with dynamic config flag to bypass cache when enabled, supporting testing scenarios without requiring gossip protocol or cache invalidation interval changes.",
      "related": [],
      "keyQuote": "make the cache read through (when a search attribute is not in the cache, reload from db), and only enable that read through if a dynamic config is enabled",
      "number": 2094,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:14.008Z"
    },
    {
      "summary": "Auto-setup Docker image fails to migrate databases when the PostgreSQL user name differs from the database name, as it attempts to recreate an existing database. This occurs because the script uses user existence as a proxy for database existence, which doesn't account for mismatched naming conventions.",
      "category": "bug",
      "subcategory": "database-setup",
      "apis": [],
      "components": [
        "auto-setup",
        "docker",
        "database-migration",
        "postgresql"
      ],
      "concepts": [
        "database-migration",
        "user-database-mapping",
        "postgresql-setup",
        "container-initialization",
        "version-upgrade"
      ],
      "severity": "medium",
      "userImpact": "Users cannot migrate Temporal instances when their PostgreSQL user and database names don't match, blocking upgrades to newer versions.",
      "rootCause": "The auto-setup script checks if a PostgreSQL user exists as an indicator that the database already exists, but this assumption fails when user and database names differ. PostgreSQL 9.6 creates databases automatically with user creation, but the script doesn't account for non-standard naming.",
      "proposedFix": null,
      "workaround": "Use matching PostgreSQL user and database names during setup.",
      "resolution": "wontfix",
      "resolutionDetails": "Auto-setup image is documented as development-only and not intended for production migrations. Users should use docker-compose instead. Sequential version updates are required, not major jumps.",
      "related": [
        1613
      ],
      "keyQuote": "auto-setup image is for local development only and was never tested for any migrations",
      "number": 2084,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:11.150Z"
    },
    {
      "summary": "tctl admin commands that interface with the database directly fail for users with custom transaction isolation levels. The issue requests support for setting transaction isolation levels in tctl to enable these commands to work with custom database configurations.",
      "category": "feature",
      "subcategory": "tctl",
      "apis": [],
      "components": [
        "tctl",
        "shard-manager",
        "database-connection"
      ],
      "concepts": [
        "transaction-isolation",
        "database-connection",
        "admin-commands",
        "sql-attributes"
      ],
      "severity": "medium",
      "userImpact": "Users with custom database transaction isolation levels cannot use tctl admin commands that require direct database access.",
      "rootCause": "tctl does not support setting SQL connection attributes like transaction isolation levels, causing it to fail when the database uses non-default isolation levels.",
      "proposedFix": "Add support to tctl for configuring transaction isolation levels and other SQL connection attributes.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "tctl is being redesigned to move away from direct database connections; all requests will be routed through the admin API instead, eliminating the need for this feature.",
      "related": [
        2075
      ],
      "keyQuote": "Tctl is moving away from direct connection to database. We will be removing all direct database dependencies from tctl.",
      "number": 2081,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:23:00.611Z"
    },
    {
      "summary": "Request to officially support PostgreSQL v10+ as v9.6 reaches end-of-life. Users report that newer versions work but lack official support, and documentation needs updating.",
      "category": "feature",
      "subcategory": "database-support",
      "apis": [],
      "components": [
        "persistence",
        "postgresql-connector",
        "database-layer"
      ],
      "concepts": [
        "database-version-support",
        "postgresql",
        "dependency-management",
        "eol",
        "backwards-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users running PostgreSQL v10+ cannot use officially supported database versions, forcing them to use unsupported versions or rely on undocumented compatibility.",
      "rootCause": "Temporal's official documentation and support matrix had not been updated to reflect PostgreSQL v10+ compatibility despite evidence of real-world usage.",
      "proposedFix": "Update official support to PostgreSQL v10+, document supported versions in official documentation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Temporal now officially supports PostgreSQL v10-13 (and v9.6 until EOL), with documentation updated to reflect v10.18 and v13.4 as supported versions.",
      "related": [],
      "keyQuote": "We now officially support PostgreSQL versions 1013 (and 9.6 until EOL).",
      "number": 2080,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:59.197Z"
    },
    {
      "summary": "Request for batch operation to restart or reset failed workflows in tctl. Users want to fail workflows under certain conditions and later rerun them in bulk, rather than manually rerunning each one individually.",
      "category": "feature",
      "subcategory": "workflow-management",
      "apis": [],
      "components": [
        "tctl",
        "workflow-execution",
        "batch-operations"
      ],
      "concepts": [
        "batch-processing",
        "workflow-failure",
        "workflow-reset",
        "workflow-restart",
        "automation"
      ],
      "severity": "medium",
      "userImpact": "Users currently must manually query and rerun failed workflows one-by-one, which is inefficient for bulk operations.",
      "rootCause": null,
      "proposedFix": "Add batch reset/rerun action to tctl batch command to restart failed workflows to their first task",
      "workaround": "Users can build their own solution to query failed workflows and rerun them individually",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by implementing batch reset functionality in tctl to reset failed workflows to first task",
      "related": [],
      "keyQuote": "Batch reset failed workflow to its first task will do the same.",
      "number": 2079,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:59.929Z"
    },
    {
      "summary": "Batch terminate operation fails with connection refused error due to hardcoded 127.0.0.1 address in config_template.yaml, preventing batch operations from working with default docker-compose setup.",
      "category": "bug",
      "subcategory": "batch-operations",
      "apis": [],
      "components": [
        "batcher",
        "activity-executor",
        "config-management"
      ],
      "concepts": [
        "connection",
        "docker-compose",
        "batch-operation",
        "hardcoded-address",
        "networking"
      ],
      "severity": "high",
      "userImpact": "Users cannot execute batch terminate operations when using default docker-compose configuration.",
      "rootCause": "127.0.0.1 is hardcoded in config_template.yaml line 283, causing connection refused errors when the batcher activity tries to connect to the gRPC port.",
      "proposedFix": "Replace hardcoded 127.0.0.1 with a configurable address that works in docker-compose environments.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The hardcoded address was replaced with a proper configuration that supports docker-compose networking.",
      "related": [],
      "keyQuote": "This happens because `127.0.0.1` is hardcoded in config_template.yaml.",
      "number": 2077,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:47.166Z"
    },
    {
      "summary": "Request to make linux/arm64 Docker images available on Docker Hub to support running Temporal Server on Mac M1 devices. The issue was marked as a duplicate of #1305 and resolved by making arm64 images available starting from version 1.14.",
      "category": "feature",
      "subcategory": "docker-images",
      "apis": [],
      "components": [
        "docker",
        "container-image",
        "distribution"
      ],
      "concepts": [
        "arm64",
        "multi-arch",
        "docker-hub",
        "m1-support",
        "platform-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal Server on Mac M1 devices can now use native arm64 Docker images instead of relying on emulated amd64 images.",
      "rootCause": null,
      "proposedFix": "Push multi-arch docker images to Docker Hub",
      "workaround": "Run linux/amd64 image",
      "resolution": "fixed",
      "resolutionDetails": "arm64 image is available in docker hub since 1.14",
      "related": [
        1305
      ],
      "keyQuote": "arm64 image is available in docker hub since 1.14",
      "number": 2073,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:45.797Z"
    },
    {
      "summary": "Race condition in namespace registry where newly created namespaces can be directly loaded without waiting for all failover notifications to be processed, causing the history shard to skip failover notifications from other namespaces.",
      "category": "bug",
      "subcategory": "namespace-failover",
      "apis": [],
      "components": [
        "namespace-registry",
        "history-engine",
        "replication-task-executor",
        "namespace-cache"
      ],
      "concepts": [
        "race-condition",
        "failover",
        "notification-ordering",
        "namespace-loading",
        "state-synchronization",
        "cache-consistency"
      ],
      "severity": "high",
      "userImpact": "Failover notifications for existing namespaces may be skipped when new namespaces are created concurrently, leading to incorrect state in the history shard.",
      "rootCause": "The current implementation allows direct loading of newly created namespaces without ensuring all previous failover notifications have been processed, breaking the ordering guarantee needed for correct failover handling.",
      "proposedFix": "Revert to the v1.12.x behavior of checking if namespace exists and forcing a reload of all namespaces to ensure notification ordering is maintained.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by implementing proper namespace loading behavior to prevent race conditions during failover notification processing.",
      "related": [],
      "keyQuote": "history shard will record all notifications <= 101 are seen, and skip the failover notification of namespace A later",
      "number": 2060,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:48.403Z"
    },
    {
      "summary": "Use Go's embed feature to include schema content directly within the schema tool instead of requiring external file references.",
      "category": "other",
      "subcategory": "schema-management",
      "apis": [],
      "components": [
        "schema-tool",
        "setup"
      ],
      "concepts": [
        "code-embedding",
        "file-embedding",
        "schema-initialization",
        "go-tooling"
      ],
      "severity": "low",
      "userImpact": "Simplifies schema tool deployment and reduces external dependencies by embedding schema content directly in the binary.",
      "rootCause": null,
      "proposedFix": "Use Go's go:embed feature to embed schema content within the schema tool",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We should include the schema content within schema tool using `go:embed`",
      "number": 2059,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:34.636Z"
    },
    {
      "summary": "User asks if Temporal Server's database persistence layer can be replaced with alternatives like Redis or PostgreSQL instead of Cassandra, citing resource consumption concerns. The issue was self-resolved when the author discovered PostgreSQL support documentation.",
      "category": "question",
      "subcategory": "database-backend",
      "apis": [],
      "components": [
        "database",
        "persistence-layer",
        "server-deployment"
      ],
      "concepts": [
        "database-abstraction",
        "cassandra-alternative",
        "postgresql-support",
        "redis",
        "resource-consumption",
        "persistence-backend"
      ],
      "severity": "low",
      "userImpact": "Users concerned about Cassandra resource usage want to know if alternative databases are supported for Temporal Server deployment.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Use PostgreSQL as documented in Temporal's official documentation for versions and dependencies.",
      "resolution": "fixed",
      "resolutionDetails": "User self-resolved by discovering that PostgreSQL is officially supported as a database backend for Temporal Server, as documented in the server configuration guide.",
      "related": [],
      "keyQuote": "I have noticed that Cassandra may consume a lot of resources... please check this document. https://docs.temporal.io/docs/server/versions-and-dependencies/#postgresql",
      "number": 2057,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:35.174Z"
    },
    {
      "summary": "Request to expose the disableInitialHostLookup configuration flag in the temporal-cassandra-tool, similar to how it's already exposed in the server config. This would help users avoid problematic connections and host-based auto-discovery issues.",
      "category": "feature",
      "subcategory": "cassandra-tool",
      "apis": [],
      "components": [
        "cassandra-tool",
        "cassandra-config",
        "host-lookup"
      ],
      "concepts": [
        "cassandra-connection",
        "host-discovery",
        "configuration",
        "connection-management",
        "database-tool"
      ],
      "severity": "low",
      "userImpact": "Users cannot configure host lookup behavior in the cassandra tool, limiting their ability to handle connection issues in certain deployment environments.",
      "rootCause": null,
      "proposedFix": "Add disableInitialHostLookup flag to temporal-cassandra-tool configuration, and potentially also disable host event based auto-discovery as an additional mitigation.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Would be great to expose a similar flag for the temporal-cassandra-tool as well",
      "number": 2054,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:36.066Z"
    },
    {
      "summary": "ParentClosePolicy is ignored when a parent workflow in one namespace starts a child workflow in a different namespace, even though the parent info is correctly recorded. The policy works correctly when both workflows are in the same namespace.",
      "category": "bug",
      "subcategory": "parent-close-policy",
      "apis": [
        "ChildWorkflowOptions",
        "StartWorkflow"
      ],
      "components": [
        "parentclosepolicy",
        "worker",
        "cross-namespace"
      ],
      "concepts": [
        "namespace",
        "parent-child-workflows",
        "policy-enforcement",
        "task-routing",
        "workflow-lifecycle"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably control child workflow termination behavior when children are in different namespaces than their parents, breaking cross-namespace workflow coordination patterns.",
      "rootCause": "The parentclosepolicy workflow service uses request.Namespace instead of execution.Namespace when checking cross-namespace child workflows, causing it to fail the namespace comparison check.",
      "proposedFix": "Change request.Namespace to execution.Namespace in the parentclosepolicy/workflow.go logic to correctly match the child's namespace.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #2224",
      "related": [
        2224
      ],
      "keyQuote": "The ParentClosePolicy setting however seems to be ignored completely in this case (if set in ChildWorkflowOptions by parent).",
      "number": 2043,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:24.626Z"
    },
    {
      "summary": "Java SDK needs the ability to parse and test SearchAttributes, requiring exposure of the cluster operator API as part of the server API to enable SDK-level unit tests and user testing capabilities.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [
        "SearchAttributes"
      ],
      "components": [
        "server-api",
        "operator-api",
        "java-sdk",
        "search-attributes"
      ],
      "concepts": [
        "testing",
        "search-attributes",
        "operator-api",
        "test-framework",
        "sdk-integration"
      ],
      "severity": "medium",
      "userImpact": "Users and SDK developers can now properly test SearchAttributes functionality in their applications.",
      "rootCause": null,
      "proposedFix": "Expose cluster operator API as part of the server API to include calls for registering search attributes",
      "workaround": "Enable SearchAttribute in-memory testing only",
      "resolution": "fixed",
      "resolutionDetails": "Addressed by the Operator APIs",
      "related": [],
      "keyQuote": "Expose cluster operator API as a part of server API. This API should include the calls to register search attributes",
      "number": 2040,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:22.428Z"
    },
    {
      "summary": "Worker client cannot establish TLS connections when TLS is terminated by an upstream load balancer rather than being directly configured in the frontend server. The worker fails to start with 'error starting scanner' and 'context deadline exceeded' errors.",
      "category": "bug",
      "subcategory": "tls-configuration",
      "apis": [],
      "components": [
        "worker",
        "client",
        "tls",
        "frontend"
      ],
      "concepts": [
        "tls-termination",
        "load-balancer",
        "connection-security",
        "configuration",
        "grpc"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal with upstream TLS termination (e.g., AWS ALB) cannot start workers without manual workarounds.",
      "rootCause": "The SdkClient only attempts TLS connection if TLS is configured in tls.frontend.server block, not when TLS is provided by upstream load balancer.",
      "proposedFix": "Update client TLS connection logic to respect TLS attributes specified in tls.frontend.client even when tls.frontend.server is not configured.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed via PR #2036 which was merged after discussion with the team.",
      "related": [
        2036
      ],
      "keyQuote": "The SdkClient will only attempt to connect via TLS if TLS is configured in the tls.frontend.server block. This is not always true, for example, if TLS is being terminated by an upstream load balancer.",
      "number": 2035,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:25.074Z"
    },
    {
      "summary": "The lastWorkerIdentity field in describeWorkflow's pendingActivity is empty when an activity is picked up by a worker on its first attempt. The server captures the worker identity from the long poll request but stores it in StartedIdentity instead of RetryLastWorkerIdentity, which describeWorkflow uses to report the value.",
      "category": "bug",
      "subcategory": "activity-worker-identity",
      "apis": [
        "describeWorkflow"
      ],
      "components": [
        "history-engine",
        "mutable-state",
        "activity-info"
      ],
      "concepts": [
        "worker-identity",
        "activity-execution",
        "pending-activity",
        "retry-policy",
        "first-attempt"
      ],
      "severity": "medium",
      "userImpact": "Users cannot identify which worker is executing a pending activity on its first attempt, making it difficult to debug distributed workflow execution and monitor activity assignment.",
      "rootCause": "The server persists worker identity to ActivityInfo#StartedIdentity on first attempt but describeWorkflow only reads from ActivityInfo#RetryLastWorkerIdentity, which is only populated on retries.",
      "proposedFix": "Write the worker identity to ActivityInfo#RetryLastWorkerIdentity immediately after StartedIdentity on the first attempt, or modify describeWorkflow to check StartedIdentity when RetryLastWorkerIdentity is empty.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #2007 which ensured worker identity is properly persisted to RetryLastWorkerIdentity on first attempt.",
      "related": [
        2007,
        784
      ],
      "keyQuote": "If the activity is picked up by the worker the first time and is in-progress, describeWorkflow returns an empty pendingActivity#lastWorkerIdentity.",
      "number": 1999,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:12.499Z"
    },
    {
      "summary": "Request for a ServerOption to support custom gRPC request rate limiting logic on the frontend service. This would enable multi-tenant deployments to enforce namespace-specific RPS limits on various Temporal operations (workflows, activities, signals, timers).",
      "category": "feature",
      "subcategory": "rate-limiting",
      "apis": [],
      "components": [
        "frontend-service",
        "grpc-interceptor",
        "server-options"
      ],
      "concepts": [
        "rate-limiting",
        "multi-tenant",
        "custom-interceptor",
        "gRPC",
        "extensibility",
        "request-rate"
      ],
      "severity": "medium",
      "userImpact": "Operators running multi-tenant Temporal services can now implement custom rate limiting logic per namespace to guarantee consistent QoS across customers.",
      "rootCause": null,
      "proposedFix": "Add WithChainedFrontendGrpcInterceptors ServerOption allowing operators to supply multiple ordered gRPC UnaryServerInterceptors that are appended after internal interceptors.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented via PR #2156 with a general-purpose custom interceptor extension point allowing multiple ordered gRPC interceptors to be chained on the frontend service.",
      "related": [
        2156
      ],
      "keyQuote": "We need to enforce RPS limits on various aspects of namespace-specific customer workload in order to guarantee a good experience for all customers.",
      "number": 1996,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:10.859Z"
    },
    {
      "summary": "Request for a workflow scheduling feature that allows cron workflows to sleep for a fixed interval after completion before the next run, rather than immediately triggering at the cron schedule. This enables patterns like polling data at consistent intervals regardless of execution time.",
      "category": "feature",
      "subcategory": "cron-scheduling",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-engine",
        "cron-scheduler",
        "worker"
      ],
      "concepts": [
        "cron-scheduling",
        "workflow-intervals",
        "polling",
        "scheduling-patterns",
        "sleep-between-runs"
      ],
      "severity": "medium",
      "userImpact": "Users cannot schedule cron workflows to run at fixed intervals from completion time rather than from a fixed schedule, limiting use cases like periodic data polling.",
      "rootCause": "The cron feature currently adheres strictly to the cron schedule rather than supporting delay-after-completion patterns, which can cause back-to-back executions if the workflow completes before the next scheduled time.",
      "proposedFix": "Implement a new cron feature variant that sleeps for a fixed period after each run completes before triggering the next execution, possibly as part of the new experience.",
      "workaround": "Define an outer workflow that wraps a child workflow with a sleep/loop pattern to achieve the desired interval-based scheduling while maintaining visibility into individual run statuses.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Cron has been changed since 1.14 and the workflow is now complete with it actual state and not as continue-as-new.",
      "number": 1994,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:22:10.383Z"
    },
    {
      "summary": "ListClosedWorkflowExecutions query fails during upgrade from Temporal 1.9 to 1.12 with error about parsing 'Running' as integer. The query was changed to use string format for ExecutionStatus instead of integer, breaking compatibility with existing Elasticsearch indices that expect integer values.",
      "category": "bug",
      "subcategory": "elasticsearch-visibility",
      "apis": [
        "ListClosedWorkflowExecutions"
      ],
      "components": [
        "visibility-store",
        "elasticsearch",
        "persistence",
        "search-attributes"
      ],
      "concepts": [
        "upgrade-compatibility",
        "schema-mismatch",
        "query-format",
        "index-mapping",
        "elasticsearch-schema"
      ],
      "severity": "high",
      "userImpact": "Users upgrading from 1.9 to 1.12 cannot use workflow search functionality until they manually update their Elasticsearch index mapping.",
      "rootCause": "Breaking change in query format from integer to string for ExecutionStatus field between versions, causing mismatches with existing Elasticsearch indices that expect integer values.",
      "proposedFix": "Update Elasticsearch index mapping template to handle the new string format for ExecutionStatus field, or provide migration tooling.",
      "workaround": "Manually update the Elasticsearch index template and recreate the visibility index using the new schema template file before upgrading.",
      "resolution": "fixed",
      "resolutionDetails": "User resolved by manually updating their Elasticsearch index mapping with the correct schema template before completing the upgrade, and referenced the v1.11.2 release notes for upgrade path documentation.",
      "related": [],
      "keyQuote": "For any version below 1.11, you need to go through this manual process to upgrade to 1.11 first.",
      "number": 1992,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:57.605Z"
    },
    {
      "summary": "Request to implement dynamic task queue routing that allows forwarding tasks to different queues after scheduling. This feature would support use cases like development/troubleshooting, local dev environments, and traffic routing without requiring SDK-level workarounds.",
      "category": "feature",
      "subcategory": "task-queue-routing",
      "apis": [],
      "components": [
        "task-queue-manager",
        "worker",
        "scheduler",
        "routing-engine"
      ],
      "concepts": [
        "task-routing",
        "dynamic-configuration",
        "traffic-routing",
        "local-development",
        "worker-topology",
        "request-scoped-rules"
      ],
      "severity": "medium",
      "userImpact": "Developers struggle with local development and traffic routing in shared Temporal clusters; dynamic routing would eliminate the need for workarounds and custom SDK libraries.",
      "rootCause": null,
      "proposedFix": "Add API to manage routing rules that forward tasks to other queues, with potential support for request-scoped routing rules inspired by Envoy's HTTP routing architecture.",
      "workaround": "Use SDK library implementations or custom client-side routing logic to intercept and redirect tasks.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add API to manage routing rules that forward tasks to other queues. This feature could solve that to some degree, at least for the things running inside Temporal.",
      "number": 1988,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:57.655Z"
    },
    {
      "summary": "The temporal-cassandra-tool schema update command fails with an error when re-executed after a previous execution was aborted, because it attempts to CREATE TYPE without checking if the type already exists. The tool needs idempotent schema update queries.",
      "category": "bug",
      "subcategory": "cassandra-schema-management",
      "apis": [],
      "components": [
        "temporal-cassandra-tool",
        "schema-updater",
        "cassandra-driver"
      ],
      "concepts": [
        "idempotency",
        "schema-versioning",
        "database-migration",
        "error-recovery",
        "cassandra-types"
      ],
      "severity": "medium",
      "userImpact": "Users cannot safely retry schema updates if a previous attempt was interrupted, forcing them to manually clean up the database before retrying.",
      "rootCause": "CREATE TYPE statements in the schema update process are not idempotent; they fail if the type already exists from a partial previous execution.",
      "proposedFix": "Change CREATE TYPE to CREATE TYPE IF NOT EXISTS in the schema update statements.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed as part of pull request #4761",
      "related": [
        4761
      ],
      "keyQuote": "Change `CREATE TYPE` to `CREATE TYPE IF NOT EXISTS`",
      "number": 1981,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:55.679Z"
    },
    {
      "summary": "Temporal server frontend closes connections with nginx ingress, causing 502 bad-gateway errors. The issue was caused by the default 5-minute keepAliveMaxConnectionAge setting that triggered connection closures during active gRPC polling, which conflicted with nginx's connection management.",
      "category": "bug",
      "subcategory": "grpc-connection-management",
      "apis": [],
      "components": [
        "frontend",
        "grpc-server",
        "connection-management",
        "keepalive"
      ],
      "concepts": [
        "connection-timeout",
        "keepalive",
        "grpc",
        "nginx-ingress",
        "kubernetes",
        "http2",
        "connection-lifecycle"
      ],
      "severity": "high",
      "userImpact": "Users experience intermittent 502 bad-gateway errors when accessing Temporal frontend through nginx ingress due to unexpected connection closures.",
      "rootCause": "The default frontend.keepAliveMaxConnectionAge setting of 5 minutes causes the server to close connections via GoAway, which conflicts with long-lived gRPC polling requests and nginx's connection handling.",
      "proposedFix": "Set dynamic config 'frontend.keepAliveMaxConnectionAge' to 0 to disable automatic connection closure.",
      "workaround": "Disable the MaxConnectionAge by setting the dynamic config 'frontend.keepAliveMaxConnectionAge' to 0.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by disabling the keepAliveMaxConnectionAge setting, which prevents premature connection closures during long-lived gRPC connections.",
      "related": [],
      "keyQuote": "Please try to disable the MaxConnectionAge by set dynamic config of 'frontend.keepAliveMaxConnectionAge` to 0, and see if that fixed the problem.",
      "number": 1980,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:40.852Z"
    },
    {
      "summary": "Worker identity from activity heartbeats is not exposed in the describeExecution API, even though the heartbeat API accepts an identity field. The issue proposes either overriding the lastWorkerIdentity field with heartbeat identity or exposing it as a separate field to handle cases where different workers pick up and heartbeat the same activity.",
      "category": "feature",
      "subcategory": "activity-heartbeat",
      "apis": [
        "RecordActivityTaskHeartbeatRequest",
        "RecordActivityTaskHeartbeatByIdRequest",
        "PendingActivityInfo"
      ],
      "components": [
        "worker",
        "activity-executor",
        "history-engine",
        "activity-heartbeat"
      ],
      "concepts": [
        "worker-identity",
        "heartbeat",
        "activity-tracking",
        "async-completion",
        "worker-rotation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot track which worker is currently processing an activity when heartbeats are sent by a different worker than the one that originally picked up the task.",
      "rootCause": "The identity field in heartbeat requests is accepted but not stored or exposed through the PendingActivityInfo API, creating a gap in worker identity tracking.",
      "proposedFix": "Either override PendingActivityInfo#lastWorkerIdentity with heartbeat identity when present, or expose heartbeat identity in a new PendingActivityInfo field.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Worker 1 picked the task, Worker 2 heartbeats for this task. Especially relevant with async completion of activities.",
      "number": 1979,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:41.699Z"
    },
    {
      "summary": "Split resource limit exceeded errors into user-facing and internal-facing error types. User-facing errors should not be retried, while internal errors should be retried and eventually converted to unavailable errors.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "error-handling",
        "resource-limits",
        "retry-logic"
      ],
      "concepts": [
        "resource-limits",
        "error-classification",
        "retry-behavior",
        "internal-errors",
        "user-facing-errors",
        "unavailable-error"
      ],
      "severity": "medium",
      "userImpact": "Users will receive clearer error messages that distinguish between user-caused resource limit errors and transient internal errors, improving error handling and retry logic.",
      "rootCause": null,
      "proposedFix": "Separate resource limit exceed errors into two types: user-facing (non-retriable) and internal-facing (retriable, eventually becomes unavailable)",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Split resource limit exceed into user facing & internal facing error types",
      "number": 1966,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:44.694Z"
    },
    {
      "summary": "Feature request to add a setting or environment variable to suppress validation errors for Custom Searchable Attributes (CSA), enabling lightweight CI/CD testing with Temporal configurations that lack Elasticsearch support.",
      "category": "feature",
      "subcategory": "custom-searchable-attributes",
      "apis": [],
      "components": [
        "searchable-attributes",
        "validation",
        "elasticsearch"
      ],
      "concepts": [
        "custom-searchable-attributes",
        "validation",
        "testing",
        "ci-cd",
        "elasticsearch",
        "temporalite"
      ],
      "severity": "medium",
      "userImpact": "Users testing with Temporal configurations without Elasticsearch (like temporalite) experience errors when using CSA APIs, hindering CI/CD workflows.",
      "rootCause": "Validation for Custom Searchable Attributes requires Elasticsearch, which is not available in lightweight testing setups.",
      "proposedFix": "Add SKIP_CSA_VALIDATION environment variable or setting to bypass CSA validation.",
      "workaround": "Use SQLite with temporal CLI which now supports custom search attributes.",
      "resolution": "fixed",
      "resolutionDetails": "SQLite support for custom search attributes was added to temporal CLI, providing an alternative solution.",
      "related": [],
      "keyQuote": "have a SKIP_CSA_VALIDATION environment variable or setting that does it? alex has ideas",
      "number": 1963,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:27.080Z"
    },
    {
      "summary": "User unable to connect to remote Temporal server on GCP, receiving HTTP2 exception. Issue resolved by using correct port (7233 instead of 8088).",
      "category": "question",
      "subcategory": "client-connection",
      "apis": [
        "WorkflowServiceStubs"
      ],
      "components": [
        "client",
        "grpc",
        "connection",
        "health-check"
      ],
      "concepts": [
        "remote-connection",
        "port-configuration",
        "http2",
        "grpc-protocol",
        "network-connectivity",
        "server-communication"
      ],
      "severity": "low",
      "userImpact": "Users attempting to connect to remote Temporal servers may encounter connection failures if using incorrect port numbers.",
      "rootCause": "Incorrect port number (8088) used instead of the standard Temporal gRPC port (7233), causing HTTP/2 framing errors.",
      "proposedFix": "Use port 7233 for Temporal server connections instead of 8088.",
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was not a bug but user error - incorrect port configuration. Resolved when user corrected port to 7233.",
      "related": [],
      "keyQuote": "My fault, should be to port 7233",
      "number": 1952,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:29.698Z"
    },
    {
      "summary": "Workflow retry tests in JavaSDK fail on the latest Temporal server image (temporaliotest/auto-setup:latest) but pass on version 1.12.1, indicating a regression in workflow retry functionality between server versions.",
      "category": "bug",
      "subcategory": "workflow-retry",
      "apis": [
        "WorkflowRetry"
      ],
      "components": [
        "workflow-executor",
        "retry-handler",
        "server-compatibility"
      ],
      "concepts": [
        "workflow-retry",
        "compatibility",
        "regression",
        "server-version",
        "test-failure"
      ],
      "severity": "high",
      "userImpact": "Workflow retry functionality is broken on the latest server version, preventing workflows from properly retrying on failure.",
      "rootCause": "Regression introduced in commit 8b71d20d94001ab86b368beb0eeefb900e633f5c or related changes in PR #1866 between server versions 1.12.1 and latest.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #1949",
      "related": [
        1866,
        1949
      ],
      "keyQuote": "The tests pass on temporalio/auto-setup:1.12.1 but not on temporaliotest/auto-setup:latest",
      "number": 1944,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:29.581Z"
    },
    {
      "summary": "Newly registered namespaces are not immediately available to workers polling for tasks because the namespace cache takes ~10 seconds to update, causing NOT_FOUND errors. The namespace database write completes but workers use a separate cache that lags behind.",
      "category": "bug",
      "subcategory": "namespace-caching",
      "apis": [
        "RegisterNamespace",
        "PollActivityTaskQueue",
        "PollWorkflowTaskQueue"
      ],
      "components": [
        "namespace-cache",
        "worker-poller",
        "grpc-service"
      ],
      "concepts": [
        "caching",
        "namespace-propagation",
        "eventual-consistency",
        "timing-race-condition",
        "database-cache-sync"
      ],
      "severity": "high",
      "userImpact": "Users cannot immediately use newly registered namespaces with workers, requiring artificial delays or polling logic as a workaround.",
      "rootCause": "Namespace-centric APIs (registerNamespace, listNamespaces) write directly to the database while worker task polling APIs use a separate namespace cache that is neither read-through nor write-through and takes approximately 10 seconds to update. Cross-namespace replication failover notification complexity prevents a simple cache invalidation solution.",
      "proposedFix": "Restructure background task processing logic to remove the requirement for reliable namespace failover notifications, then make the namespace cache read-through to ensure immediate consistency.",
      "workaround": "Add a 10-second delay after registering a namespace before starting workers, or implement polling logic to retry until the namespace is available.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by pull request #3908 which restructured the background task processing logic and made the namespace cache read-through.",
      "related": [
        3908
      ],
      "keyQuote": "This cache is not a read-thought or write-through cache and takes about 10s to be updated.",
      "number": 1941,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:13.746Z"
    },
    {
      "summary": "Request to upstream the SQLite persistence plugin from the temporalite project to make lightweight Temporal deployments available for local development and testing without Docker containers.",
      "category": "feature",
      "subcategory": "persistence-plugin",
      "apis": [],
      "components": [
        "persistence",
        "sql-plugin",
        "test-suite"
      ],
      "concepts": [
        "sqlite",
        "lightweight",
        "local-development",
        "testing",
        "persistence",
        "plugin-architecture"
      ],
      "severity": "low",
      "userImpact": "Enables developers to use Temporal for local development and testing environments without requiring Docker or full server deployments.",
      "rootCause": null,
      "proposedFix": "Upstream the SQL plugin from temporalite (https://github.com/DataDog/temporalite/tree/main/internal/common/persistence/sql/sqlplugin) to the main Temporal repository",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Closed by issue #1968, indicating the upstreaming effort was completed",
      "related": [
        1968
      ],
      "keyQuote": "We want to upstream temporalite's sql plugin so we can run our test suite on them and make sqlite persistence available to others",
      "number": 1940,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:13.380Z"
    },
    {
      "summary": "Expand SDK version tracking and reporting to be included in server version reports for usage tracking. Currently only server version info is collected; the system should track which SDK types, versions, and how many connections each pair establishes.",
      "category": "feature",
      "subcategory": "versioning",
      "apis": [],
      "components": [
        "version-reporting",
        "metrics-collection",
        "usage-tracking",
        "connection-tracking"
      ],
      "concepts": [
        "sdk-version",
        "tracking",
        "usage-metrics",
        "version-reporting",
        "connection-counting",
        "telemetry"
      ],
      "severity": "low",
      "userImpact": "Enables the Temporal team to better understand SDK adoption, version distribution, and deployment patterns across their user base.",
      "rootCause": null,
      "proposedFix": "Expand the implementation from PR #823 to collect SDK version metrics on the server side and report them in version reports. Consider adding a connection_count field to track the number of connections per SDK type/version pair.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was addressed by implementing SDK version collection and reporting as part of the metrics infrastructure.",
      "related": [
        823
      ],
      "keyQuote": "We should collect and report that info so we can track what SDKs are being used and at what versions.",
      "number": 1929,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:21:13.185Z"
    },
    {
      "summary": "When a workflow is reset to an earlier point after the ExecutionTimeout has expired, the new run immediately times out instead of starting a fresh timeout calculation from the reset point. This prevents recovery of timed-out workflows.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-reset",
        "execution-timeout",
        "timeout-calculation"
      ],
      "concepts": [
        "timeout",
        "workflow-reset",
        "execution-timeout",
        "workflow-recovery",
        "time-calculation"
      ],
      "severity": "high",
      "userImpact": "Users cannot recover workflows that have timed out using reset, as the reset operation immediately times out the workflow again instead of allowing it to execute with a fresh timeout.",
      "rootCause": "The execution timeout is not recalculated from the time of reset; instead it uses the original timeout expiration time, causing the new run to be immediately timed out.",
      "proposedFix": "Recalculate the execution timeout from the time the workflow is reset, not from the original workflow start time.",
      "workaround": "Use WorkflowRunTimeout instead of ExecutionTimeout, as the issue does not occur when only RunTimeout is specified.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #6473 which properly handles timeout recalculation on workflow reset.",
      "related": [
        6473
      ],
      "keyQuote": "The execution timeout is calculated from the time the workflow is reset.",
      "number": 1913,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:59.297Z"
    },
    {
      "summary": "The visibility database can become out-of-sync with the main database due to various reasons. This feature request proposes creating a system workflow that can reconstruct the entire visibility database from the main database, started via tctl command.",
      "category": "feature",
      "subcategory": "visibility-reconciliation",
      "apis": [],
      "components": [
        "visibility-database",
        "main-database",
        "workflow-system",
        "tctl"
      ],
      "concepts": [
        "database-synchronization",
        "data-consistency",
        "reconciliation",
        "reconstruction",
        "system-workflow",
        "visibility-records"
      ],
      "severity": "medium",
      "userImpact": "Users can recover from visibility database inconsistencies by running a reconciliation workflow to rebuild visibility records from the authoritative main database.",
      "rootCause": "Various unspecified reasons can cause visibility records to get out of sync with the main database.",
      "proposedFix": "Create a system workflow that reconstructs the entire visibility database from the main database, started via tctl command.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It is possible to reconstruct entire visibility database from the main database.",
      "number": 1904,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:57.467Z"
    },
    {
      "summary": "Request to add WorkflowSizeInBytes as a visibility attribute for completed workflows, similar to the recently added StateTransition count. This would allow users to query and monitor the final size of workflow executions.",
      "category": "feature",
      "subcategory": "visibility-attributes",
      "apis": [],
      "components": [
        "visibility",
        "workflow-metadata",
        "search-attributes"
      ],
      "concepts": [
        "workflow-size",
        "visibility-attributes",
        "completed-workflow",
        "metadata",
        "monitoring"
      ],
      "severity": "low",
      "userImpact": "Users would gain ability to track and analyze workflow execution sizes for optimization and monitoring purposes.",
      "rootCause": null,
      "proposedFix": "Add WorkflowSizeInBytes as a searchable visibility attribute alongside existing attributes like StateTransition count.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We recently added StateTransition count as a visibility attribute for a Completed Workflow. Can we also add the final size of the Workflow itself?",
      "number": 1903,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:58.950Z"
    },
    {
      "summary": "User encountered an error when starting a workflow with a custom search attribute in Temporal 1.12.0, though the same code worked in 1.11.2. The issue was resolved after discovering that custom search attributes now require explicit registration, a change introduced in the 1.12 release.",
      "category": "docs",
      "subcategory": "custom-search-attributes",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "search-attributes",
        "workflow-start",
        "elasticsearch-integration"
      ],
      "concepts": [
        "custom-search-attributes",
        "registration",
        "breaking-change",
        "error-handling",
        "version-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users upgrading from 1.11.2 to 1.12.0 may experience workflow startup failures with custom search attributes until they register the attributes, with an unclear error message.",
      "rootCause": "Custom search attributes registration requirement was introduced in version 1.12.0 as a breaking change from 1.11.2, but the change was not clearly communicated in release notes.",
      "proposedFix": null,
      "workaround": "Register custom search attributes before starting workflows with them.",
      "resolution": "invalid",
      "resolutionDetails": "User discovered that custom search attributes require explicit registration as of version 1.12.0. This was not a bug but a behavioral change that needed better documentation.",
      "related": [
        1764
      ],
      "keyQuote": "I missed this in the 1.12 release. So our upgrade to 1.11.2 was working fine and our issues started once we went to 1.12.",
      "number": 1899,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:46.250Z"
    },
    {
      "summary": "Feature request to add a PID file configuration option to the Temporal server, allowing it to write its process ID to a file on startup and clean it up on shutdown for easier daemon management in local development.",
      "category": "feature",
      "subcategory": "server-configuration",
      "apis": [],
      "components": [
        "server",
        "configuration",
        "process-management"
      ],
      "concepts": [
        "daemon",
        "process-id",
        "local-development",
        "configuration",
        "shutdown"
      ],
      "severity": "low",
      "userImpact": "Users running Temporal server as a local daemon without Docker would have built-in PID file support for process management instead of implementing custom solutions.",
      "rootCause": null,
      "proposedFix": "Add a PID file path configuration option that the server writes to on start and deletes on shutdown.",
      "workaround": "Use an external tool or custom daemonization process; alternatively use systemd or another process manager.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I'd like to run the Temporal server process as a daemon locally and it would be useful if the server could write its PID to a file.",
      "number": 1898,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:46.785Z"
    },
    {
      "summary": "User asks whether calling CompleteActivity multiple times via a message broker with at-least-once delivery is idempotent, and what happens on the second call.",
      "category": "question",
      "subcategory": "activity-completion",
      "apis": [
        "CompleteActivity"
      ],
      "components": [
        "activity-executor",
        "client"
      ],
      "concepts": [
        "idempotency",
        "at-least-once-delivery",
        "message-broker",
        "duplicate-requests"
      ],
      "severity": "low",
      "userImpact": "Users integrating with message brokers need to understand whether repeated CompleteActivity calls are safe or will fail.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Clarified that second call returns NotFound error, establishing that CompleteActivity is not idempotent",
      "related": [],
      "keyQuote": "Second call would received a NotFound error.",
      "number": 1897,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:47.985Z"
    },
    {
      "summary": "Request to use a non-root user with configurable UID/GID for Docker containers instead of hardcoded 1000:1000, to accommodate security scanning requirements and Docker implementations that require specific user IDs.",
      "category": "feature",
      "subcategory": "docker-security",
      "apis": [],
      "components": [
        "docker",
        "container-runtime",
        "security"
      ],
      "concepts": [
        "uid-gid",
        "non-root-user",
        "docker-security",
        "container-permissions",
        "security-scanning"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal in Docker with security scanning or custom UID/GID requirements cannot use the current hardcoded 1000:1000 user configuration.",
      "rootCause": "Docker implementation hardcodes uid:gid to 1000:1000, which conflicts with security scanning requirements (UID/GID > 10000) and Docker setups requiring custom user IDs.",
      "proposedFix": "Use a configurable user account instead of hardcoded 1000:1000, or allow passing UID/GID as parameters.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We have a similar requirement for passing security scans that require UID/GID > 10000",
      "number": 1895,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:33.595Z"
    },
    {
      "summary": "User requests that released versions of Temporal Server include bundled versioned database schemas. Currently, schemas must be manually downloaded from GitHub, making manual server setup difficult.",
      "category": "feature",
      "subcategory": "release-distribution",
      "apis": [],
      "components": [
        "release-artifacts",
        "schema-management",
        "server-setup",
        "database-initialization"
      ],
      "concepts": [
        "release-packaging",
        "schema-versioning",
        "deployment",
        "installation",
        "distribution"
      ],
      "severity": "medium",
      "userImpact": "Users manually deploying Temporal Server must separately source and manage database schemas, adding complexity to the installation process.",
      "rootCause": null,
      "proposedFix": "Include schema files tied to each release version in the release artifacts alongside compiled binaries.",
      "workaround": "Download schemas directly from the GitHub repository tag.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "In addition to the compiled binaries, include the schemas that are tied to a version.",
      "number": 1893,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:35.095Z"
    },
    {
      "summary": "Request for automatic worker-level heartbeats to enable activities to failover to a new worker if the current executing worker goes down, reducing the need for manual per-activity heartbeat implementations.",
      "category": "feature",
      "subcategory": "activity-heartbeat",
      "apis": [
        "activity.Context.current",
        "heartbeat"
      ],
      "components": [
        "worker",
        "activity-executor",
        "heartbeat-mechanism"
      ],
      "concepts": [
        "heartbeat",
        "failover",
        "worker-availability",
        "activity-lifecycle",
        "timeout",
        "external-heartbeat"
      ],
      "severity": "medium",
      "userImpact": "Users currently must manually implement auto-heartbeat wrappers or add SDK support within activities, adding complexity and boilerplate code.",
      "rootCause": null,
      "proposedFix": "Add support for worker-level automatic heartbeats so activities failover automatically if the executing worker goes down, rather than requiring per-activity implementations.",
      "workaround": "Users can wrap activity functions in an 'autoheartbeat' wrapper that sends periodic heartbeats at intervals based on the heartbeat timeout.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        126
      ],
      "keyQuote": "Add support for Worker heartbeat so activities automatically failover to a new Worker if the current executing Worker goes down.",
      "number": 1891,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:31.703Z"
    },
    {
      "summary": "Temporal history service crashes with nil pointer dereference in statsComputer.computeMutableStateStats when acquiring a shard, causing workflows to become stuck and unable to create or cancel.",
      "category": "bug",
      "subcategory": "persistence-stats",
      "apis": [],
      "components": [
        "history-service",
        "persistence-layer",
        "shard-manager",
        "statsComputer"
      ],
      "concepts": [
        "nil-pointer-dereference",
        "crash-loop",
        "memory-safety",
        "workflow-execution",
        "shard-acquisition"
      ],
      "severity": "critical",
      "userImpact": "Users experience complete service unavailability and inability to create or cancel workflows due to history service crashes.",
      "rootCause": "Nil pointer dereference in statsComputer.computeMutableStateStats at line 70 of statsComputer.go when computing mutable state statistics during workflow execution retrieval.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "stale",
      "resolutionDetails": "Issue was closed due to no activity after asking user to try the latest server version (1.12.0), suggesting it may have been fixed in a later release.",
      "related": [],
      "keyQuote": "panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation]",
      "number": 1885,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:20.072Z"
    },
    {
      "summary": "User requested the ability to manually start activities instead of automatically, to support use cases like staged production deployments where user intervention is required before an activity begins execution.",
      "category": "feature",
      "subcategory": "activity-execution",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "workflow-engine",
        "activity-executor",
        "signal-handler"
      ],
      "concepts": [
        "manual-control",
        "workflow-coordination",
        "signals",
        "user-intervention",
        "deployment"
      ],
      "severity": "low",
      "userImpact": "Users requiring manual control over activity execution (e.g., production deployment gates) would need to use signals as a workaround.",
      "rootCause": null,
      "proposedFix": "Signals can be used as a workaround: workflow waits on a signal channel, and users trigger activity execution by sending a signal via API.",
      "workaround": "Use signal channels to implement manual control: workflows wait on signals, and users send signals via the Temporal API or tctl when ready to proceed.",
      "resolution": "wontfix",
      "resolutionDetails": "The issue was addressed through guidance that the existing signals mechanism already provides the requested functionality, making a dedicated feature unnecessary.",
      "related": [
        1882
      ],
      "keyQuote": "You shall be able to do this via signals. In your workflow, you can wait on a signal channel. When your user is ready to start the next activity, you can send signal to the target workflow",
      "number": 1883,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:18.383Z"
    },
    {
      "summary": "User requests a ManualRetryPolicy to give users explicit control over when and how activities are retried, rather than relying on automatic retry policies.",
      "category": "feature",
      "subcategory": "activity-retries",
      "apis": [
        "ExecuteActivity",
        "ActivityTimeoutEvent",
        "ActivityFailedEvent"
      ],
      "components": [
        "activity-executor",
        "retry-policy",
        "workflow-engine"
      ],
      "concepts": [
        "retry",
        "manual-control",
        "activity-execution",
        "failure-handling",
        "heartbeat",
        "signals"
      ],
      "severity": "low",
      "userImpact": "Users cannot manually control retry timing and conditions, requiring workarounds via signals to implement custom retry logic.",
      "rootCause": null,
      "proposedFix": "Implement a ManualRetryPolicy that allows users to determine when to retry activities based on custom conditions.",
      "workaround": "Use signal channels to implement manual retry logic - define signals for 'continue retry' and 'abort activity' that users can send based on their own conditions.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "You can achieve this by using signals. In your workflow, you have a signal channel, and depending on when and what signal it received, the workflow react on the signal.",
      "number": 1882,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:18.197Z"
    },
    {
      "summary": "MySQL users cannot use tctl admin workflow delete commands to clean up malformed workflows, only Cassandra is supported. This prevents emergency remediation of database bloat and performance degradation caused by unintended workflow cascades.",
      "category": "feature",
      "subcategory": "database-operations",
      "apis": [],
      "components": [
        "tctl",
        "persistence-layer",
        "mysql-adapter",
        "workflow-visibility"
      ],
      "concepts": [
        "workflow-cleanup",
        "database-maintenance",
        "emergency-recovery",
        "persistence-consistency",
        "namespace-management",
        "operational-tooling"
      ],
      "severity": "high",
      "userImpact": "MySQL users cannot remediate database bloat from cascading workflows, forcing degraded service or manual database manipulation with risk of inconsistency.",
      "rootCause": "The tctl admin workflow delete command set only supports Cassandra persistence, not MySQL, creating an operational gap for MySQL deployments.",
      "proposedFix": "Support MySQL in tctl admin workflow commands, or add ability to delete workflows by namespace and time window, or add namespace deletion capability for MySQL.",
      "workaround": "Manually delete rows from executions_visibility table (risky), or reduce namespace retention to accelerate cleanup (causes degraded service for 24+ hours).",
      "resolution": "fixed",
      "resolutionDetails": "tctl was refactored to connect via temporal frontend instead of directly to database, making it agnostic to persistence layer.",
      "related": [
        598
      ],
      "keyQuote": "tctl now does not connect to database directly and all operation is via temporal frontend. In other word, tctl now supports all temporal's persistence layer.",
      "number": 1881,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:05.309Z"
    },
    {
      "summary": "User unable to query a completed workflow after restarting the worker and creating a new workflow stub with the same workflow ID. The issue was resolved when the user discovered they should use newWorkflowStub(Class, workflowId) instead of newWorkflowStub(Class, WorkflowOptions) for querying existing workflows.",
      "category": "question",
      "subcategory": "workflow-query",
      "apis": [
        "newWorkflowStub",
        "query"
      ],
      "components": [
        "workflow-stub",
        "client",
        "worker"
      ],
      "concepts": [
        "workflow-query",
        "completed-workflow",
        "workflow-stub-instantiation",
        "state-recovery",
        "client-initialization"
      ],
      "severity": "low",
      "userImpact": "Users may fail to query completed workflows if they use incorrect WorkflowStub instantiation patterns, resulting in IllegalStateException.",
      "rootCause": "User instantiated WorkflowStub with WorkflowOptions.newBuilder() for querying existing workflows, which is intended only for starting new workflows. The correct method is newWorkflowStub(Class, workflowId).",
      "proposedFix": "Use client.newWorkflowStub(WorkflowClass.class, workflowId) for querying existing workflows instead of client.newWorkflowStub(WorkflowClass.class, WorkflowOptions.newBuilder().setWorkflowId(workflowId)...build())",
      "workaround": "Use the same workflowStub instance that started the workflow without restarting the worker or client",
      "resolution": "invalid",
      "resolutionDetails": "User error - incorrect API usage. User should instantiate WorkflowStub with workflowId directly for existing workflows, not with WorkflowOptions.",
      "related": [],
      "keyQuote": "For an existing workflow, I should instantiate the workflowStub with: client.newWorkflowStub(DSLWorkflow.class, workflowId);",
      "number": 1869,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:06.064Z"
    },
    {
      "summary": "Temporal Java SDK fails on JDK 16 due to illegal reflection access to Throwable.cause field. The CheckedExceptionWrapper class initializer crashes during exception handling because JDK 16 restricts reflection access to internal fields by default.",
      "category": "bug",
      "subcategory": "jdk-compatibility",
      "apis": [],
      "components": [
        "CheckedExceptionWrapper",
        "exception-handling",
        "workflow-worker"
      ],
      "concepts": [
        "reflection",
        "jdk-compatibility",
        "illegal-access",
        "exception-handling",
        "internal-fields"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal Java SDK on JDK 16 experience runtime crashes when handling workflow exceptions, breaking workflow execution.",
      "rootCause": "Temporal uses reflection to access the private Throwable.cause field, which is now denied by default in JDK 16 per JDK Enhancement Proposal 8256299.",
      "proposedFix": "Update to Temporal Java SDK version 1.2.0 or later which resolves this compatibility issue.",
      "workaround": "Run Java with --illegal-access=permit flag to allow reflection access, though this option will be removed in future JDK releases.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by upgrading to Temporal Java SDK 1.2.0, which implements proper JDK 16 compatible exception handling without illegal reflection.",
      "related": [],
      "keyQuote": "Temporal accesses a private member Throwable.cause by means of reflection which is now denied by default (new behaviour in JDK 16)",
      "number": 1868,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:20:05.844Z"
    },
    {
      "summary": "Temporal server hangs during SIGTERM shutdown after losing Cassandra database connection. The server fails to reconnect to the database when using disableInitialHostLookup=true configuration and becomes unresponsive to termination signals, requiring SIGKILL to stop.",
      "category": "bug",
      "subcategory": "database-connection-handling",
      "apis": [],
      "components": [
        "cassandra-client",
        "membership-ringpop",
        "persistence-layer",
        "shutdown-handler"
      ],
      "concepts": [
        "connection-reset",
        "database-reconnection",
        "graceful-shutdown",
        "signal-handling",
        "network-failure",
        "connection-pooling"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal server with Cassandra become unable to cleanly shut down the server after network connectivity loss, forcing them to use SIGKILL and risking data corruption.",
      "rootCause": "After a connection reset error, the Cassandra node is marked unhealthy and the client fails to retry the known connection when disableInitialHostLookup=true is configured, blocking graceful shutdown.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It looks as though after hitting \"read: connection reset by peer\", the node is marked unhealthy. When there is the config disableInitialHostLookup=true it should just retry the known connection after a short while.",
      "number": 1867,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:53.310Z"
    },
    {
      "summary": "RoleWorker lost write permissions after PR #1711 introduced fine-grained authorization in defaultAuthorizer. The worker role, which was previously treated as a writer by default, now has no permissions, creating a breaking change during upgrade from 1.9.2.",
      "category": "bug",
      "subcategory": "authorization",
      "apis": [],
      "components": [
        "defaultAuthorizer",
        "authorization",
        "role-based-access-control"
      ],
      "concepts": [
        "permissions",
        "role-worker",
        "access-control",
        "backward-compatibility",
        "fine-grained-authorization"
      ],
      "severity": "high",
      "userImpact": "Users upgrading to versions after PR #1711 experience broken worker permissions, requiring manual configuration changes.",
      "rootCause": "PR #1711 added read-only API support to defaultAuthorizer but did not implement corresponding worker-only API list, causing RoleWorker to lose all permissions when previously it was treated as a writer.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was clarified as expected behavior - defaultAuthorizer is incomplete and meant as a sample. No side effect fix was implemented.",
      "related": [
        1711
      ],
      "keyQuote": "defaultAuthorizer is currently incomplete in its support for fine-grain authorization, and is meant more as a sample for custom authorizers.",
      "number": 1862,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:50.958Z"
    },
    {
      "summary": "User unable to add custom search attributes via tctl command - interactive prompt fails on first attempt and doesn't work on retry. Community provided workarounds including using shell access directly or the --auto_confirm flag.",
      "category": "bug",
      "subcategory": "tctl-admin",
      "apis": [],
      "components": [
        "tctl",
        "admin-tools",
        "search-attributes"
      ],
      "concepts": [
        "custom-search-attributes",
        "interactive-prompt",
        "docker-exec",
        "tctl-command"
      ],
      "severity": "medium",
      "userImpact": "Users following documentation are unable to add custom search attributes through tctl, requiring workarounds or alternative methods.",
      "rootCause": "Interactive prompt handling issue when running tctl command via docker exec without -it flags",
      "proposedFix": "Use --auto_confirm flag or run tctl from within docker shell using 'docker exec -it temporal-admin-tools sh'",
      "workaround": "Either use --auto_confirm flag or access docker shell first with 'docker exec -it temporal-admin-tools sh' then run tctl command",
      "resolution": "stale",
      "resolutionDetails": "Issue closed with close-after-30-days label, suggesting it was closed due to inactivity rather than being fixed",
      "related": [],
      "keyQuote": "Also you can use `--auto_confirm` flag to bypass this question.",
      "number": 1851,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:51.561Z"
    },
    {
      "summary": "Parent workflow keeps retrying child workflows indefinitely when WorkflowTaskTimeout occurs, ignoring the workflow retry policy's MaximumAttempts setting. The user expected the retry policy to limit retries but found that workflow tasks are always retried by design regardless of the policy configuration.",
      "category": "bug",
      "subcategory": "workflow-task-timeout",
      "apis": [],
      "components": [
        "workflow-task-executor",
        "retry-policy",
        "workflow-execution"
      ],
      "concepts": [
        "timeout",
        "retry",
        "workflow-task",
        "retry-policy",
        "resource-exhaustion",
        "infinite-retry"
      ],
      "severity": "high",
      "userImpact": "Users experience infinite retry loops and resource exhaustion when workflow tasks timeout on parent workflows with multiple child workflows, blocking other workflows from execution.",
      "rootCause": "Workflow tasks are by design always retried by the system until they succeed or until the workflow run times out, independent of the workflow-level retry policy which only applies to workflow-level failures.",
      "proposedFix": "Set a shorter workflow run timeout to allow the workflow to time out and stop retrying after the timeout is reached, rather than relying on the workflow retry policy.",
      "workaround": "Configure a shorter workflow run timeout instead of relying on retry policy MaximumAttempts to limit retries, since workflow task retries are handled separately from workflow-level retries.",
      "resolution": "wontfix",
      "resolutionDetails": "Determined to be by design. Workflow tasks are always retried by Temporal; retry policies apply only at the workflow level when the entire workflow fails/times out. User resolved by redesigning their workflow to use shorter run timeouts.",
      "related": [],
      "keyQuote": "Workflow task failure does not mean the workflow run is failed. Temporal keeps retrying the workflow task until it succeed or until workflow run timed out.",
      "number": 1848,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:37.355Z"
    },
    {
      "summary": "Workers connected to a standby cluster in XDC (Cross-Datacenter) setup fail to pick up tasks after failover to active cluster. The issue is non-deterministic: sometimes workers never pick up tasks, sometimes they only process new workflows but not in-progress ones from the previously active cluster.",
      "category": "bug",
      "subcategory": "xdc-failover",
      "apis": [],
      "components": [
        "worker",
        "xdc-replication",
        "task-dispatcher",
        "namespace-failover"
      ],
      "concepts": [
        "failover",
        "cross-datacenter",
        "task-dispatch",
        "cluster-promotion",
        "replication",
        "active-standby"
      ],
      "severity": "high",
      "userImpact": "Users experience workflow interruptions during failover events as workers in the promoted standby cluster cannot pick up existing or new tasks.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The workers (java sdk) connected to the standby cluster is still not getting any workflows/tasks dispatched.",
      "number": 1847,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:39.742Z"
    },
    {
      "summary": "The taskqueue scavenger operation emits excessive persistence errors and metrics for operations like gettasks, deletetaskqueue, listtaskqueue, and completetaskslessthan, causing dips in persistence availability metrics even though the scavenger appears to be functioning correctly. The root cause is unclear, potentially related to unbounded task queue batch sizes or lack of persistence QPS throttling during scavenger runs.",
      "category": "bug",
      "subcategory": "taskqueue-scavenger",
      "apis": [],
      "components": [
        "worker",
        "taskqueue-scavenger",
        "persistence-layer",
        "metrics"
      ],
      "concepts": [
        "persistence-errors",
        "batch-processing",
        "connection-pooling",
        "rate-limiting",
        "database-load"
      ],
      "severity": "medium",
      "userImpact": "Users experience false dips in persistence availability metrics during taskqueue scavenger runs, making it difficult to monitor actual persistence health and potentially causing unnecessary alerting.",
      "rootCause": "The taskqueue scavenger does not limit batch sizes or apply persistence QPS throttling when scanning and deleting queues, causing excessive database reads and connection overhead that manifest as persistence errors without proper error logging.",
      "proposedFix": "Make the taskqueue scavenger more configurable by allowing users to set task queue batch sizes, and apply the max persistence QPS setting similar to how it's used in the history scavenger to throttle operations.",
      "workaround": "Exclude worker persistence metrics from overall persistence availability metric calculations to avoid false positives during scavenger runs.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the scavenger does appear to be making progress and successfully deleting queues, so it is unclear what all these errors are about",
      "number": 1844,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:39.712Z"
    },
    {
      "summary": "Request to support multiple custom Elasticsearch indices in Temporal with per-index search attribute configuration. Currently, all custom search attributes are stored in a single index, limiting flexibility for users who want to organize attributes across multiple indices.",
      "category": "feature",
      "subcategory": "elasticsearch-visibility",
      "apis": [],
      "components": [
        "visibility",
        "elasticsearch",
        "search-attributes"
      ],
      "concepts": [
        "custom-indices",
        "search-attributes",
        "elasticsearch-mapping",
        "attribute-organization",
        "multi-index-support"
      ],
      "severity": "medium",
      "userImpact": "Users with multiple custom search attributes cannot organize them across different Elasticsearch indices, forcing all attributes into a single index and limiting search flexibility.",
      "rootCause": null,
      "proposedFix": "Allow users to configure multiple custom Elasticsearch indices, each with its own customized search field mapping, enabling per-index search attribute configuration.",
      "workaround": "Custom Elasticsearch indices can be updated directly via workflow activities, but Temporal native rollback will not work with this approach.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "User should be able to setup multiple, in this example 3 custom indices with customized custom search field mapping and update.",
      "number": 1840,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:23.422Z"
    },
    {
      "summary": "Request to expose healthCheck and idleConnectionCloseInterval as configuration options on Elasticsearch configuration to enable Temporal to work with AWS Elasticsearch service.",
      "category": "feature",
      "subcategory": "elasticsearch-configuration",
      "apis": [],
      "components": [
        "elasticsearch",
        "persistence",
        "configuration"
      ],
      "concepts": [
        "health-check",
        "connection-pooling",
        "idle-connections",
        "aws-elasticsearch",
        "configuration-options"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal against AWS Elasticsearch cannot configure health checks and idle connection behavior, limiting compatibility with AWS Elasticsearch.",
      "rootCause": null,
      "proposedFix": "Add healthCheck and idleConnectionCloseInterval as configuration settings in the Elasticsearch configuration",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Configuration options were added to the Elasticsearch configuration to expose health check and idle connection close interval settings",
      "related": [],
      "keyQuote": "The ability to disable/enable health check and to periodically close idle connections enables Temporal to work against AWS Elasticsearch.",
      "number": 1830,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:19.284Z"
    },
    {
      "summary": "CronSchedule workflows stopped receiving tasks after running for a period of time. The issue was caused by read-write separation in the MySQL database configuration where writes weren't immediately readable, resulting in task scheduling failures for cron workflows.",
      "category": "bug",
      "subcategory": "cron-scheduling",
      "apis": [],
      "components": [
        "workflow-scheduler",
        "task-queue",
        "database",
        "cron-engine"
      ],
      "concepts": [
        "cron-schedule",
        "task-scheduling",
        "database-consistency",
        "read-write-separation",
        "workflow-continuation"
      ],
      "severity": "high",
      "userImpact": "Cron workflows stop executing after running for a period of time, causing scheduled jobs to fail silently.",
      "rootCause": "Database read-write separation configuration caused writes to the tasks table to succeed but not be immediately readable on subsequent queries, preventing task retrieval.",
      "proposedFix": null,
      "workaround": "Change the database configuration to remove read-write separation to ensure immediate consistency between writes and reads.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was identified as a database configuration problem (read-write separation), not a Temporal server bug. Resolved by adjusting the database setup.",
      "related": [],
      "keyQuote": "When the task was inserted into the tasks table, the return was successful, but the data was not written successfully, which resulted in the subsequent read failure.",
      "number": 1829,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:24.302Z"
    },
    {
      "summary": "MySQL 8.0 fails to initialize Temporal schema due to TIMESTAMP DEFAULT value compatibility issue. Error occurs during schema setup where version mismatch is detected (Expected 1.6 vs Actual 0.0) caused by invalid default TIMESTAMP values.",
      "category": "bug",
      "subcategory": "database-schema",
      "apis": [],
      "components": [
        "sql-schema",
        "mysql-driver",
        "schema-setup"
      ],
      "concepts": [
        "timestamp",
        "timezone",
        "mysql-compatibility",
        "schema-versioning",
        "default-values",
        "database-initialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot initialize or start Temporal server with MySQL 8.0+ due to schema setup failures.",
      "rootCause": "MySQL 8.0+ requires timezone information in TIMESTAMP DEFAULT values. The schema uses TIMESTAMP DEFAULT '1970-01-01 00:00:01' without UTC indicator (+00:00), causing compatibility issues.",
      "proposedFix": "Add +00:00 to all default TIMESTAMP values in schema files to explicitly indicate UTC timezone, or use TIMESTAMP with timezone specification.",
      "workaround": "Use MySQL version 8.0.19 or earlier (as mentioned in comments), or manually modify schema to add timezone indicators.",
      "resolution": "fixed",
      "resolutionDetails": "Existing schema files updated to work with MySQL 8.0.19+ by adding UTC timezone indicators to TIMESTAMP DEFAULT values.",
      "related": [],
      "keyQuote": "It seems that `+00:00` needs to be added to all default `TIMESTAMP` values to indicate UTC time zone.",
      "number": 1826,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:08.903Z"
    },
    {
      "summary": "Request for documentation on the specific Elasticsearch permissions required by Temporal. Users encountered 403 errors when ES had restrictive permissions and need clear guidance on required access levels.",
      "category": "docs",
      "subcategory": "elasticsearch-permissions",
      "apis": [
        "ListWorkflowExecutionsRequest"
      ],
      "components": [
        "visibility-store",
        "elasticsearch",
        "persistence"
      ],
      "concepts": [
        "permissions",
        "access-control",
        "elasticsearch",
        "visibility",
        "documentation",
        "security"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal with Elasticsearch must guess at required permissions, leading to debugging efforts and security risks when they over-provision access.",
      "rootCause": "Lack of documentation specifying which Elasticsearch permissions Temporal requires for operations like ListWorkflowExecutionsRequest.",
      "proposedFix": "Create documentation detailing the specific Elasticsearch permissions and roles required by Temporal for all operations.",
      "workaround": "Temporarily open all permissions between Temporal and Elasticsearch cluster (not recommended for production).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "a documentation update as to which permissions are required by Temporal would be amazing",
      "number": 1821,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:06.841Z"
    },
    {
      "summary": "CronSchedule workflows were executing twice within the scheduled interval (e.g., twice in two minutes for */2 * * * *). The issue was caused by incorrect logic in PR #560 that set the next schedule time to a value in the past, triggering immediate re-execution.",
      "category": "bug",
      "subcategory": "cron-scheduling",
      "apis": [],
      "components": [
        "cron-scheduler",
        "workflow-scheduling",
        "schedule-calculation"
      ],
      "concepts": [
        "cron-expression",
        "scheduling",
        "timing",
        "execution-frequency",
        "backoff"
      ],
      "severity": "high",
      "userImpact": "Cron-scheduled workflows execute more frequently than specified, causing unexpected duplicate executions and potential resource waste.",
      "rootCause": "The condition `if nowUTC.Before(scheduledUTCTime)` in PR #560 caused nextScheduleTime to be set incorrectly, leading to immediate re-dispatch when scheduledUTCTime was set to a future value that was actually in the past relative to the next scheduled execution.",
      "proposedFix": "Remove the `if nowUTC.Before(scheduledUTCTime)` judgment and directly set `nextScheduleTime = schedule.Next(scheduledUTCTime)`, as implemented in the Cadence codebase.",
      "workaround": "Downgrade to a version before PR #560 or manually remove the problematic conditional check.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in patch release 1.11.4 which included two fixes to cron scheduling logic.",
      "related": [
        560,
        1828
      ],
      "keyQuote": "After I deleted the if judgment, directly set nextScheduleTime = schedule.Next(scheduledUTCTime), there is no repeated execution",
      "number": 1818,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:19:07.219Z"
    },
    {
      "summary": "CronSchedule workflow executes twice within the scheduled interval instead of once per interval. A change in PR #560 that checks if the current time is before the scheduled time appears to cause duplicate executions for cron workflows with intervals like */2 * * * *.",
      "category": "bug",
      "subcategory": "workflow-scheduling",
      "apis": [],
      "components": [
        "cron-scheduler",
        "schedule-calculator",
        "workflow-engine"
      ],
      "concepts": [
        "cron-expression",
        "scheduling",
        "timing",
        "interval",
        "duplicate-execution"
      ],
      "severity": "high",
      "userImpact": "Cron-scheduled workflows execute multiple times per interval, violating scheduling expectations and potentially causing data duplication or unintended side effects.",
      "rootCause": "PR #560 introduced a condition that checks if current time is before scheduled time, but the subsequent logic for advancing the schedule may cause multiple executions within the same interval.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was closed by the reporter without resolution details provided.",
      "related": [
        560
      ],
      "keyQuote": "Sometimes it was executed twice in two minutes",
      "number": 1817,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:53.309Z"
    },
    {
      "summary": "Certificate hot-reloading configuration parameters (RefreshInterval and ExpirationChecks) exist in the code but cannot be set via environment variables, requiring manual vendoring of the configuration file as a workaround.",
      "category": "bug",
      "subcategory": "configuration",
      "apis": [],
      "components": [
        "config",
        "certificate-management",
        "docker"
      ],
      "concepts": [
        "certificate-reloading",
        "environment-variables",
        "configuration",
        "tls"
      ],
      "severity": "medium",
      "userImpact": "Users cannot configure certificate hot-reloading behavior without vendoring and manually editing the configuration file.",
      "rootCause": "Configuration parameters RefreshInterval and ExpirationChecks are defined in the code but not exposed as environment variable options in the configuration template.",
      "proposedFix": "Add environment variable support for RefreshInterval and ExpirationChecks parameters in the docker configuration template.",
      "workaround": "Vendor the configuration file and manually update the certificate hot-reloading parameters.",
      "resolution": "fixed",
      "resolutionDetails": "Configuration parameters were made settable via environment variables.",
      "related": [],
      "keyQuote": "These parameters do exist... But are not settable via environment variable",
      "number": 1799,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:55.343Z"
    },
    {
      "summary": "Feature request to expose a public API for listing all task queues in a namespace for maintenance and debugging purposes. Currently only individual task queue queries are available, though a persistence layer API exists internally but is inefficient and doesn't properly support pagination across all namespaces.",
      "category": "feature",
      "subcategory": "task-queue-management",
      "apis": [],
      "components": [
        "persistence-layer",
        "task-queue-service",
        "namespace-management"
      ],
      "concepts": [
        "pagination",
        "maintenance",
        "debugging",
        "task-queue-discovery",
        "namespace-isolation",
        "persistence-schema"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently discover and list all task queues within a namespace, limiting visibility for maintenance tools and operational dashboards.",
      "rootCause": "Persistence layer API exists but lacks proper public exposure, efficient pagination, and namespace-aware implementations across different database backends (Cassandra, MySQL, PostgreSQL).",
      "proposedFix": "Expose a public gRPC API endpoint to list all task queues per namespace with proper pagination support that works efficiently across all supported persistence backends.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Currently public API only allow query a specific task queue. Temporal should consider adding functionality to list all task queue per namespace for maintenance & debugging purpose.",
      "number": 1797,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:54.639Z"
    },
    {
      "summary": "Flaky integration test TestVisibility occasionally fails with visibility count mismatch (expecting 1, getting 2), passing on retry attempts. The test has been fixed.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "workflow-visibility",
        "integration-tests",
        "test-suite"
      ],
      "concepts": [
        "flakiness",
        "test-reliability",
        "race-condition",
        "visibility-index",
        "determinism"
      ],
      "severity": "medium",
      "userImpact": "Flaky tests reduce confidence in test reliability and can block CI/CD pipelines intermittently.",
      "rootCause": "Race condition or timing issue in visibility index updates during workflow execution",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The flakiness was fixed, as indicated by the comment 'this is fixed'",
      "related": [],
      "keyQuote": "flakey integration test, failed twice, succeed on 3rd retry",
      "number": 1783,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:40.261Z"
    },
    {
      "summary": "Go struct receiver is nil in matching service forwarder, causing potential nil pointer dereference. Fixed by PR #1779.",
      "category": "bug",
      "subcategory": "matching-service",
      "apis": [],
      "components": [
        "matching-service",
        "forwarder"
      ],
      "concepts": [
        "nil-check",
        "receiver",
        "struct",
        "pointer-safety"
      ],
      "severity": "medium",
      "userImpact": "Could cause runtime crashes in the matching service forwarder when receiver is nil.",
      "rootCause": "Missing nil check on receiver/this pointer in matching service forwarder",
      "proposedFix": "Add nil check for receiver before use",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #1779",
      "related": [
        1708,
        1779
      ],
      "keyQuote": "Go struct `this` / receiver should not be nil",
      "number": 1778,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:42.239Z"
    },
    {
      "summary": "Feature request to migrate from lib/pq to jackc/pgx as the PostgreSQL driver to enable client-side failover support for HA PostgreSQL clusters. lib/pq is in maintenance mode and lacks necessary features, while pgx is actively maintained and compatible with sqlx.",
      "category": "feature",
      "subcategory": "database-driver",
      "apis": [],
      "components": [
        "postgresql-driver",
        "database-connection",
        "persistence-layer"
      ],
      "concepts": [
        "high-availability",
        "failover",
        "database-driver",
        "client-side-routing",
        "cluster-management",
        "connection-pooling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot implement client-side HA PostgreSQL failover with Temporal's current lib/pq driver, limiting deployment options for highly available setups.",
      "rootCause": "lib/pq is in maintenance mode and lacks support for client-side failover and multiple-host connection strategies required for HA PostgreSQL clusters.",
      "proposedFix": "Migrate from lib/pq to jackc/pgx as the PostgreSQL driver, which is actively maintained, supports necessary HA features, and is compatible with sqlx.",
      "workaround": "Deploy and use a proxy to handle failover and load balancing, though this adds complexity to the deployment scheme.",
      "resolution": "wontfix",
      "resolutionDetails": "No resources available at the time. Contributors were welcome to submit patches.",
      "related": [
        4184
      ],
      "keyQuote": "We don't have resource to work on this at the moment. Contribution on this would be awesome.",
      "number": 1775,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:42.392Z"
    },
    {
      "summary": "Request to support loading TLS CA certificates from URLs in tctl CLI, eliminating the need to manually download and save certificates to files before use.",
      "category": "feature",
      "subcategory": "tctl-cli",
      "apis": [],
      "components": [
        "tctl",
        "tls",
        "cli"
      ],
      "concepts": [
        "certificate-management",
        "tls-configuration",
        "url-loading",
        "environment-variables",
        "ca-certificate"
      ],
      "severity": "low",
      "userImpact": "Users can simplify their TLS setup workflow by directly referencing certificate URLs instead of manually downloading and saving files.",
      "rootCause": null,
      "proposedFix": "Modify TEMPORAL_CLI_TLS_CA handling to support both file paths and HTTP/HTTPS URLs for loading CA certificates.",
      "workaround": "Download certificate to file using curl or similar tool, then reference the local file path in TEMPORAL_CLI_TLS_CA.",
      "resolution": "fixed",
      "resolutionDetails": "Author indicated they were working on implementing the feature.",
      "related": [],
      "keyQuote": "Be able to load the CA certificate from a URL, in addition to from a file.",
      "number": 1768,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:25.698Z"
    },
    {
      "summary": "Request to audit and improve all dynamic configuration options in Temporal Server to make them truly dynamic without requiring server restarts. The issue aims to reduce the number of dynamic configs that still require restarts.",
      "category": "feature",
      "subcategory": "dynamic-configuration",
      "apis": [],
      "components": [
        "configuration",
        "server",
        "config-management"
      ],
      "concepts": [
        "dynamic-configuration",
        "hot-reload",
        "server-restart",
        "configuration-management",
        "operational-efficiency"
      ],
      "severity": "medium",
      "userImpact": "Users currently need to restart Temporal Server to apply changes to certain configuration parameters, reducing operational flexibility.",
      "rootCause": null,
      "proposedFix": "Audit all dynamic config options and refactor those that still require restarts to support true dynamic updates.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Many configs have since been changed to be truly dynamic and no longer require restarts, addressing the enhancement request on a case-by-case basis.",
      "related": [
        1757
      ],
      "keyQuote": "We have since changed many configs to be truly dynamic and not require restarts.",
      "number": 1758,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:29.479Z"
    },
    {
      "summary": "Request to document the two categories of dynamic configuration: truly dynamic configs (checked at runtime) and static configs (loaded at startup). This documentation would help developers understand which dynamic config values are actually dynamic and which are only evaluated once.",
      "category": "docs",
      "subcategory": "configuration",
      "apis": [],
      "components": [
        "configuration",
        "server"
      ],
      "concepts": [
        "dynamic-config",
        "configuration-loading",
        "runtime-behavior",
        "server-startup",
        "business-logic"
      ],
      "severity": "low",
      "userImpact": "Developers are unclear about which dynamic configuration values are truly dynamic versus which are only evaluated at server startup, leading to potential misunderstandings in their implementations.",
      "rootCause": null,
      "proposedFix": "Add documentation that clearly categorizes dynamic configs into two groups: truly dynamic (business logic checks newest value each time) and static (only checked at server startup).",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Document above 2 categories for easy reference.",
      "number": 1757,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:27.910Z"
    },
    {
      "summary": "Add a dedicated WORKFLOW_TASK_FAILED_CAUSE for non-deterministic errors to provide clear distinction for SDKs and enable server-side metrics logging.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "workflow-task-executor",
        "error-handling",
        "metrics"
      ],
      "concepts": [
        "non-deterministic-errors",
        "error-classification",
        "observability",
        "server-metrics",
        "workflow-execution"
      ],
      "severity": "medium",
      "userImpact": "SDKs can now properly report and classify non-deterministic errors, and servers can track these errors with dedicated metrics.",
      "rootCause": null,
      "proposedFix": "Introduce a distinct WORKFLOW_TASK_FAILED_CAUSE enum value for non-deterministic errors in the API.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Change was implemented in the API repository (PR #118) to add the dedicated cause enum value.",
      "related": [
        118
      ],
      "keyQuote": "We need a clear indication for non-deterministic error for SDK to report this specific error.",
      "number": 1756,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:15.723Z"
    },
    {
      "summary": "Request to add ScheduleToStartLatency metric for activity and workflow tasks to help monitor SDK worker backlog accumulation. The metric should be tracked per namespace, task queue, and task type.",
      "category": "feature",
      "subcategory": "metrics-observability",
      "apis": [],
      "components": [
        "metrics",
        "activity-task",
        "workflow-task",
        "task-queue"
      ],
      "concepts": [
        "latency",
        "scheduling",
        "backlog",
        "worker-capacity",
        "observability",
        "performance-monitoring"
      ],
      "severity": "medium",
      "userImpact": "Operators lack visibility into whether SDK workers are falling behind, making it difficult to detect and respond to backlog accumulation.",
      "rootCause": null,
      "proposedFix": "Implement ScheduleToStartLatency metric tracking per namespace, task queue, and task type",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Metric was implemented as requested, providing per-namespace, per-task-queue, and per-task-type tracking",
      "related": [],
      "keyQuote": "This metrics could be used as a proxy to estimate if SDK worker is falling behind and accumulate backlogs.",
      "number": 1754,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:14.481Z"
    },
    {
      "summary": "Temporal services use non-reentrant locks which can cause deadlocks when the same coroutine attempts to acquire the same lock twice. The proposal is to investigate and potentially replace non-reentrant locks with reentrant alternatives across frontend, matching, history, and worker services.",
      "category": "feature",
      "subcategory": "concurrency-primitives",
      "apis": [],
      "components": [
        "frontend",
        "matching",
        "history",
        "worker",
        "locks"
      ],
      "concepts": [
        "deadlock",
        "reentrancy",
        "concurrency",
        "locking",
        "coroutine",
        "mutex"
      ],
      "severity": "medium",
      "userImpact": "Users may experience deadlocks when Temporal services internally attempt to acquire locks multiple times from the same coroutine context.",
      "rootCause": "Non-reentrant locks in Temporal services cannot be safely acquired twice by the same coroutine, causing deadlock conditions.",
      "proposedFix": "Investigate existing reentrant lock libraries and substitute all non-reentrant locks with reentrant alternatives. Alternative approaches include reusing PriorityMutex variant with timeout or adopting channels instead of locks.",
      "workaround": null,
      "resolution": "unknown",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "most locks used within Temporal are non reentrant lock, meaning when the same coroutine trying to acquire the same lock twice, deadlock will occur",
      "number": 1753,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:13.912Z"
    },
    {
      "summary": "Version was not bumped in v1.11.0 release, discovered via the versionChecker code. A patch release v1.11.1 was subsequently published to address the issue.",
      "category": "bug",
      "subcategory": "release-management",
      "apis": [],
      "components": [
        "version-checker",
        "release-process"
      ],
      "concepts": [
        "version-management",
        "release-versioning",
        "semantic-versioning"
      ],
      "severity": "low",
      "userImpact": "Users running v1.11.0 may have version detection issues due to the missing version bump.",
      "rootCause": "Release process did not properly bump the version string in versionChecker.go",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "A patch release v1.11.1 was created with the correct version bump.",
      "related": [],
      "keyQuote": "Release v1.11.0 did not bump version.",
      "number": 1743,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:02.528Z"
    },
    {
      "summary": "The `tctl adm namespace list` command displays raw Go object references instead of formatted output. The table format is printing the entire struct instead of extracting and displaying specific fields like namespace name, global namespace status, and notification version.",
      "category": "bug",
      "subcategory": "tctl-namespace-formatting",
      "apis": [],
      "components": [
        "tctl",
        "namespace-commands",
        "table-formatting"
      ],
      "concepts": [
        "output-formatting",
        "namespace-management",
        "cli-ux",
        "object-serialization",
        "table-display"
      ],
      "severity": "medium",
      "userImpact": "Users running tctl namespace list commands see unreadable Go object references instead of formatted data, making the command unusable for namespace inspection.",
      "rootCause": "The printTable function is printing the entire struct instead of extracting and formatting individual fields from the NamespaceDetail object.",
      "proposedFix": "Implement proper field extraction and formatting for the table output, similar to the approach used in the new tctl repository's workflow field printing mechanism.",
      "workaround": "Use JSON output format instead of table format, which correctly marshals the entire response.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in the new temporal/CLI repository (https://github.com/temporalio/cli) which replaces tctl from the temporal repo with improved output formatting.",
      "related": [],
      "keyQuote": "The output should be outputting correct value and not golang object reference",
      "number": 1741,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:03.022Z"
    },
    {
      "summary": "Many Temporal APIs return imprecise error codes (e.g., NOT_FOUND instead of FAILED_PRECONDITION) making it impossible to distinguish between workflows that don't exist versus workflows that have already completed. All public APIs need holistic review and adjustment to use more accurate gRPC error codes.",
      "category": "feature",
      "subcategory": "error-codes",
      "apis": [],
      "components": [
        "grpc-api",
        "error-handling",
        "workflow-execution"
      ],
      "concepts": [
        "error-codes",
        "grpc-status",
        "error-differentiation",
        "precondition-checking",
        "workflow-state"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably determine the root cause of API failures, forcing them to implement workarounds to interpret error messages.",
      "rootCause": "APIs use generic error codes (NOT_FOUND) for multiple failure scenarios that should have distinct error codes (FAILED_PRECONDITION for already-completed workflows).",
      "proposedFix": "Conduct holistic review of all public APIs and adjust error codes to follow gRPC standard conventions (googleapis/rpc/code.proto), using FAILED_PRECONDITION for state-related errors.",
      "workaround": "Parse error message details (e.g., check for 'workflow execution already completed' in status description) to differentiate error scenarios.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "NOT_FOUND error code is returned when requesting a termination and workflow is already completed. It makes it impossible to differentiate this from the case when workflow actually doesn't exist.",
      "number": 1739,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:18:03.263Z"
    },
    {
      "summary": "PostgreSQL password environment variable containing special characters is not URL-encoded when building the connection string, causing the server to fail with a parse error. The issue was identified and fixed in the codebase.",
      "category": "bug",
      "subcategory": "postgres-connection",
      "apis": [],
      "components": [
        "persistence",
        "sql-plugin",
        "postgresql-session",
        "url-builder"
      ],
      "concepts": [
        "url-encoding",
        "special-characters",
        "connection-string",
        "environment-variables",
        "postgres",
        "credentials"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal Server with PostgreSQL passwords containing special characters cannot start the server.",
      "rootCause": "PostgreSQL password is not URL-encoded when combined into the connection URL string, causing invalid characters to break URL parsing.",
      "proposedFix": "URL-encode the password component before including it in the PostgreSQL connection string.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by properly encoding password in postgresql/session/session.go line 110.",
      "related": [],
      "keyQuote": "parse \"postgres://temporal:31VKrnpbtrYW3y9rP:j7}N(VD=v_3vNw@...\": net/url: invalid userinfo",
      "number": 1719,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:48.572Z"
    },
    {
      "summary": "Optimize activity dispatching by attempting synchronous matching with the matching service before persisting transfer tasks to the database, allowing successful sync matches to omit the corresponding transfer task write.",
      "category": "feature",
      "subcategory": "activity-dispatching",
      "apis": [],
      "components": [
        "activity-dispatcher",
        "matching-service",
        "transfer-task",
        "activity-executor"
      ],
      "concepts": [
        "sync-matching",
        "optimization",
        "task-persistence",
        "retry-policy",
        "at-least-once-execution"
      ],
      "severity": "medium",
      "userImpact": "Reduces database writes and improves activity dispatching performance by eliminating unnecessary transfer task persistence when activities are immediately matched.",
      "rootCause": null,
      "proposedFix": "Before writing un-started activity tasks with retry policy to the database, attempt pushing to matching service for sync match with a 1-2 second timeout window; omit the transfer task if sync matching succeeds.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2965
      ],
      "keyQuote": "try pushing these task to matching service for sync match (e.g. wait for 1 to 2 second), if sync matching is a success, the corresponding transfer task can be omitted",
      "number": 1714,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:50.616Z"
    },
    {
      "summary": "Temporal services fail to reconnect to a new MySQL/Aurora master after failover, requiring manual service restart. The request is for automatic detection and reconnection to the new master instance.",
      "category": "feature",
      "subcategory": "database-failover",
      "apis": [],
      "components": [
        "database-connection",
        "persistence-layer",
        "mysql-driver"
      ],
      "concepts": [
        "failover",
        "high-availability",
        "connection-recovery",
        "read-only-detection",
        "auto-reconnect"
      ],
      "severity": "high",
      "userImpact": "Aurora/MySQL failovers cause Temporal cluster outages until services are manually restarted, disrupting availability.",
      "rootCause": "Temporal services don't detect when they're connected to a read-only replica after failover and fail to reconnect to the new master.",
      "proposedFix": "Set rejectReadOnly=true in the go-sql-driver/mysql configuration to enable Aurora MySQL failover support.",
      "workaround": "Manually restart all Temporal services after failover to force reconnection to the new master.",
      "resolution": "fixed",
      "resolutionDetails": "The fix was implemented via #1705 and included in v1.11.2, enabling automatic failover support through rejectReadOnly configuration.",
      "related": [
        1705
      ],
      "keyQuote": "Set rejectReadOnly=true to support Aurora MySQL failover",
      "number": 1703,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:49.791Z"
    },
    {
      "summary": "Docker images from Dockerhub have Elasticsearch schemas at an incorrect directory structure (`schema/elasticsearch/{version}/visibility/`) compared to the expected location (`schema/elasticsearch/visibility/index_template_{version}.json`), causing deployment failures with Helm charts.",
      "category": "bug",
      "subcategory": "deployment-elasticsearch",
      "apis": [],
      "components": [
        "admin-tools",
        "docker-image",
        "elasticsearch-setup",
        "helm-integration"
      ],
      "concepts": [
        "deployment",
        "docker",
        "elasticsearch",
        "schema-versioning",
        "configuration-mismatch",
        "helm-charts"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal clusters with Elasticsearch via Helm charts encounter setup job failures and index creation errors.",
      "rootCause": "Directory structure was recently changed in the server repository but Dockerhub images were built from a different version than the latest master, causing a mismatch between image layout and expected paths in Helm charts and deployment scripts.",
      "proposedFix": "Ensure Dockerhub-published images have the correct directory structure matching the current master branch, and maintain version alignment between server releases and corresponding Helm chart releases.",
      "workaround": "Use the specific Helm chart version that matches your server release version (e.g., Helm chart 1.10.1 for server 1.10.5).",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by clarifying version alignment between server releases and Helm chart releases; users needed to match specific versions rather than assume latest-to-latest compatibility.",
      "related": [],
      "keyQuote": "Temporal server image 1.10.5 requires Helm chart 1.10.1 to deploy, and latest Helm chart targets latest master of server",
      "number": 1702,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:35.727Z"
    },
    {
      "summary": "Request for a simple API to limit concurrent workflow executions of a specific type across all workers to prevent multiple simultaneous instances. User needs this for workflows accessing shared database resources that require exclusive access.",
      "category": "feature",
      "subcategory": "workflow-concurrency-limits",
      "apis": [
        "RegisterWorkflowWithOptions"
      ],
      "components": [
        "worker",
        "workflow-executor",
        "concurrency-control"
      ],
      "concepts": [
        "concurrency-limit",
        "workflow-type",
        "cross-worker-coordination",
        "resource-locking",
        "serialization",
        "distributed-limit"
      ],
      "severity": "medium",
      "userImpact": "Users needing to serialize workflow executions of the same type must implement complex workarounds instead of using a simple built-in limit mechanism.",
      "rootCause": "Lack of built-in workflow-type level concurrency limiting across the worker pool; current design doesn't provide this feature at the API level.",
      "proposedFix": "Add MaxConcurrentWorkflowsAcrossWorkers option to RegisterWorkflowWithOptions to limit concurrent executions of a workflow type across all workers.",
      "workaround": "Use constant workflow IDs to ensure only one execution per ID, or implement complex mutex-based workflow patterns as described in linked samples and community discussions.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "An ability to limit the number of parallel workflow executions (of a given type or by some other group like batchId) across all workers is a very frequently requested feature.",
      "number": 1700,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:31.100Z"
    },
    {
      "summary": "Feature request to add support for NATS Streaming, Lifebridge, and JetStream as lighter-weight alternatives to Kafka for Temporal's internal messaging needs in cloud deployments.",
      "category": "feature",
      "subcategory": "messaging-infrastructure",
      "apis": [],
      "components": [
        "messaging-layer",
        "kafka-replacement",
        "cloud-deployment"
      ],
      "concepts": [
        "streaming",
        "message-broker",
        "lightweight-alternative",
        "cloud-native",
        "golang-native",
        "infrastructure"
      ],
      "severity": "low",
      "userImpact": "Users deploying Temporal in cloud environments could benefit from lighter-weight messaging alternatives to reduce deployment overhead.",
      "rootCause": null,
      "proposedFix": "Integrate NATS Streaming, Lifebridge, or JetStream as alternatives to Kafka for internal Temporal messaging",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed due to inactivity. Additionally, Kafka is no longer required for Temporal Cloud deployments, reducing the relevance of this feature request.",
      "related": [],
      "keyQuote": "Kafka cluster is too heavy when deploying in cloud. Using NATS Streaming, Lifebridge or Jetstream is more lightweight and golang native",
      "number": 1698,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:31.420Z"
    },
    {
      "summary": "User requests documentation for running Temporal Server tutorial with MySQL instead of Cassandra, which consumes excessive memory on developer laptops. Existing MySQL docker-compose file exists but isn't promoted as the default option.",
      "category": "docs",
      "subcategory": "setup-documentation",
      "apis": [],
      "components": [
        "docker-compose",
        "server-setup",
        "documentation"
      ],
      "concepts": [
        "database-configuration",
        "memory-efficiency",
        "development-environment",
        "tutorial",
        "docker",
        "local-setup"
      ],
      "severity": "low",
      "userImpact": "Developers experience system performance issues when following the default tutorial setup, making it difficult to get started with Temporal Server.",
      "rootCause": "Cassandra requires significant system resources and the documentation doesn't highlight the lightweight MySQL alternative for development.",
      "proposedFix": "Promote the existing docker-compose-mysql.yml (or docker-compose-mysql-es.yml) as the default or recommended option for tutorial setup in the documentation.",
      "workaround": "Users can manually use the docker-compose-mysql.yml file from the docker-compose repository instead of the default Cassandra setup.",
      "resolution": "stale",
      "resolutionDetails": "Issue was closed due to no activity. The underlying need (MySQL documentation) was addressed by pointing to the existing docker-compose repository with MySQL options.",
      "related": [],
      "keyQuote": "Devs like me would like a small and neat Mysql version. docker-compose -f docker-compose-mysql.yml up",
      "number": 1693,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:18.225Z"
    },
    {
      "summary": "Operators need namespace-level rate limiting controls that align with SDK-centric metrics (workflows/s, activities/s, signals/s, timers/s) rather than low-level system metrics like persistence QPS or frontend RPS. Current rate limiting options lack the granularity needed for per-namespace enforcement based on projected user loads.",
      "category": "feature",
      "subcategory": "rate-limiting",
      "apis": [],
      "components": [
        "frontend",
        "namespace-manager",
        "rate-limiter"
      ],
      "concepts": [
        "rate-limiting",
        "namespace-isolation",
        "load-projection",
        "quota-management",
        "operational-control"
      ],
      "severity": "medium",
      "userImpact": "Operators cannot effectively enforce user-defined load limits at the namespace level, forcing them to translate SDK-centric load projections into low-level system metrics that don't directly correspond to user requirements.",
      "rootCause": "Rate limiting controls are implemented at low system levels (persistence, frontend RPS) rather than at the namespace level with SDK-centric metrics abstraction.",
      "proposedFix": "Implement namespace-level rate limiting controls that directly support SDK-centric metrics: workflows/s, activities/s, signals/s, and timers/s per namespace.",
      "workaround": "Use existing controls like frontend.namespaceRPS or frontend.globalNamespaceRPS, though these lack the granularity needed for SDK-centric load projections.",
      "resolution": "fixed",
      "resolutionDetails": "Comments indicate frontend.globalNamespaceRPS was identified as a per-namespace rate limit option that addresses the core need, though with acknowledged limitations on granularity.",
      "related": [],
      "keyQuote": "It would be ideal if the system offered direct control on the rates of these SDK-centric events, at the namespace level.",
      "number": 1690,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:15.017Z"
    },
    {
      "summary": "Users experience transient errors when adding new activities to existing workflows during rolling deployments. The system routes activity tasks to workers that don't have implementations of the new activities, causing confusion and requiring error handling workarounds.",
      "category": "feature",
      "subcategory": "worker-versioning",
      "apis": [],
      "components": [
        "worker",
        "activity-router",
        "task-queue"
      ],
      "concepts": [
        "deployment",
        "rolling-upgrade",
        "activity-routing",
        "worker-versioning",
        "backwards-compatibility",
        "canary-deployment"
      ],
      "severity": "medium",
      "userImpact": "Users must add error handling and distinguish transient errors from real deployment issues when rolling out new activities, complicating operational procedures.",
      "rootCause": "Activity task router lacks awareness of which workers implement specific activities, causing tasks to be routed to workers without the required implementation.",
      "proposedFix": "Implement worker versioning that allows the system to track which workers implement which activities and route tasks accordingly based on worker capabilities.",
      "workaround": "Use separate task queues for old and new code versions, though this requires complex operational setup and configuration.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by implementing worker versioning feature that allows automatic routing based on worker capabilities.",
      "related": [],
      "keyQuote": "It would be a cleaner experience all around if the system could somehow understand which workers implement which activities and routed the activities accordingly.",
      "number": 1689,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:16.153Z"
    },
    {
      "summary": "Add built-in support for read-only user roles in Temporal, including API categorization and UI enforcement. Currently, operators must write custom authorizers to enforce read-only access, but the framework should provide out-of-the-box support with API categorization into ReadOnly/ReadWrite/Admin categories.",
      "category": "feature",
      "subcategory": "authorization",
      "apis": [],
      "components": [
        "authorization",
        "frontend-api",
        "ui",
        "claim-mapper",
        "authorizer"
      ],
      "concepts": [
        "read-only-role",
        "access-control",
        "authorization",
        "rbac",
        "api-categorization",
        "claim-mapping",
        "enforcement"
      ],
      "severity": "medium",
      "userImpact": "Operators can enforce read-only access without writing custom authorizers, and the UI properly reflects read-only permissions.",
      "rootCause": null,
      "proposedFix": "Implement built-in API categorization into ReadOnly/ReadWrite/Admin, update the default authorizer to enforce ReadOnly roles, and provide UI with access to authorization.Claims for the logged-in user.",
      "workaround": "Write custom claim mappers and authorizers that understand the universe of APIName values.",
      "resolution": "fixed",
      "resolutionDetails": "Built-in read-only role support was implemented with API categorization and enforcement.",
      "related": [],
      "keyQuote": "The authorization.ClaimMapper interface does currently support the notion of a ReadOnly role, and so operators can write custom claim mappers that assign ReadOnly roles to namespaces",
      "number": 1688,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:01.086Z"
    },
    {
      "summary": "Request to enhance RespondWorkflowTaskFailedResponse to include an indication that non-determinism caused the failure, enabling server-side tracking and metrics for workflow non-determinism incidents.",
      "category": "feature",
      "subcategory": "workflow-non-determinism",
      "apis": [],
      "components": [
        "workflow-task-handler",
        "resend-response",
        "metrics"
      ],
      "concepts": [
        "non-determinism",
        "workflow-replay",
        "error-tracking",
        "observability",
        "metrics",
        "failure-detection"
      ],
      "severity": "medium",
      "userImpact": "Operators can proactively track and notify customers of workflow non-determinism issues before they reach production.",
      "rootCause": "Lack of explicit non-determinism indication in RespondWorkflowTaskFailedResponse prevents server-side tracking of these failures.",
      "proposedFix": "Add a field to RespondWorkflowTaskFailedResponse indicating failure was caused by non-determinism; emit metrics based on this server-side.",
      "workaround": "Monitor service_errors_nondeterministic metrics, though these may not be comprehensive for all scenarios.",
      "resolution": "fixed",
      "resolutionDetails": "Added non-determinism indication to RespondWorkflowTaskFailedResponse, implemented in workflowHandler.go to enable server-side tracking.",
      "related": [],
      "keyQuote": "It would be extremely useful to be able to do server side tracking of instances of workflow non-determinism.",
      "number": 1685,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:02.133Z"
    },
    {
      "summary": "Temporal Server fails to start with TiDB due to differences in transaction handling. TiDB's optimistic transaction model returns duplicate key errors during COMMIT rather than during INSERT, which the Server cannot handle.",
      "category": "bug",
      "subcategory": "database-compatibility",
      "apis": [],
      "components": [
        "database",
        "persistence",
        "sql",
        "transaction-handling"
      ],
      "concepts": [
        "transaction-model",
        "error-handling",
        "database-compatibility",
        "TiDB",
        "optimistic-transactions",
        "pessimistic-transactions"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to use TiDB as a Temporal Server database backend experience startup failures.",
      "rootCause": "TiDB uses optimistic transaction model by default, returning duplicate key errors at COMMIT time rather than INSERT time like MySQL, and Temporal Server's error handling doesn't account for this difference.",
      "proposedFix": null,
      "workaround": "Set TiDB global variables: SET GLOBAL tidb_enable_noop_functions=1; and SET GLOBAL tidb_txn_mode=\"pessimistic\";",
      "resolution": "wontfix",
      "resolutionDetails": "Temporal team lacks bandwidth to support TiDB's optimistic transaction model natively. Workaround of switching TiDB to pessimistic mode allows server to operate successfully.",
      "related": [],
      "keyQuote": "We currently do not have the bandwidth to work on supporting TiDB's optimistic transaction model, but contribution is welcomed",
      "number": 1684,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:17:01.283Z"
    },
    {
      "summary": "Support multiple stages of namespace deletion to manage abandoned or misspelled namespaces. Proposed stages include disabling new submissions, hiding from namespace lists, soft delete with undelete capability, and hard delete from the database.",
      "category": "feature",
      "subcategory": "namespace-management",
      "apis": [],
      "components": [
        "namespace-service",
        "database",
        "ui",
        "api"
      ],
      "concepts": [
        "namespace-deletion",
        "namespace-disabling",
        "data-cleanup",
        "soft-delete",
        "hard-delete",
        "namespace-visibility"
      ],
      "severity": "medium",
      "userImpact": "Users need a way to clean up abandoned or misspelled namespaces to reduce UI clutter and confusion about active namespaces.",
      "rootCause": null,
      "proposedFix": "Implement staged namespace deletion with four levels: (1) disable new submissions, (2) disable and hide from list API, (3) soft delete with undelete capability, (4) hard delete from database after checking for no references.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Delete namespace functionality was released in version 1.17.0. Users can delete namespaces using the 'temporal operator namespace delete' command.",
      "related": [],
      "keyQuote": "Delete namespace functionality was released in 1.17.0",
      "number": 1679,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:16:43.729Z"
    },
    {
      "summary": "Activity retry policy with MaximumInterval cap is not respected - all retries occur at the maximum interval instead of respecting the initial interval and backoff coefficient until the cap is reached.",
      "category": "bug",
      "subcategory": "activity-retry-policy",
      "apis": [
        "RetryPolicy"
      ],
      "components": [
        "activity-executor",
        "retry-engine",
        "scheduler"
      ],
      "concepts": [
        "retry",
        "backoff",
        "interval",
        "timeout",
        "exponential-backoff",
        "maximum-interval"
      ],
      "severity": "high",
      "userImpact": "Activities with retry policies are not retrying with expected exponential backoff, causing incorrect retry timing and potentially excessive waiting between attempts.",
      "rootCause": "The MaximumInterval cap is being applied incorrectly, causing all retries to use the maximum interval from the start instead of using exponential backoff until reaching the cap.",
      "proposedFix": null,
      "workaround": "Remove the MaximumInterval configuration to allow retries to behave as expected with normal exponential backoff.",
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "All retries are 1 hour apart... Now try again without the `MaximimInterval` - the retries go back to normal and behave as expected",
      "number": 1675,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:16:46.197Z"
    },
    {
      "summary": "User requests the ability to pass header parameters through tctl when starting/stopping workflows to carry metadata like authentication information. The issue was resolved by using ContextPropagator in the client instead of passing headers explicitly through tctl.",
      "category": "feature",
      "subcategory": "tctl-cli",
      "apis": [],
      "components": [
        "tctl",
        "context-propagator",
        "headers"
      ],
      "concepts": [
        "metadata",
        "authentication",
        "headers",
        "context-propagation",
        "workflow-execution",
        "client-configuration"
      ],
      "severity": "low",
      "userImpact": "Users seeking to pass header parameters and metadata through tctl have an alternative solution using ContextPropagator in the client.",
      "rootCause": null,
      "proposedFix": "Use tctl plugins to support pluggable ContextPropagators, or leverage the existing --headers_provider_plugin flag.",
      "workaround": "Use a ContextPropagator in the client to carry metadata to workflows instead of passing headers explicitly through tctl.",
      "resolution": "fixed",
      "resolutionDetails": "User discovered the --headers_provider_plugin flag and switched to using ContextPropagator approach which met their needs.",
      "related": [],
      "keyQuote": "I've switched to using a `ContextPropagator` to carry metadata to workflows. They seem sufficient to my needs.",
      "number": 1672,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:16:46.677Z"
    },
    {
      "summary": "Default JWT claims mapper incorrectly lowercases namespace names, causing authorization failures when namespaces contain uppercase characters. The converted namespace is compared against the original request namespace, resulting in a mismatch and request rejection.",
      "category": "bug",
      "subcategory": "authorization",
      "apis": [],
      "components": [
        "default_jwt_claim_mapper",
        "default_authorizer",
        "authorization"
      ],
      "concepts": [
        "namespace",
        "case-sensitivity",
        "jwt-claims",
        "authorization",
        "token-validation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use namespaces with uppercase characters in their names when relying on default JWT authorization, limiting namespace naming flexibility.",
      "rootCause": "Default claims mapper lowercases namespace names (line 112) and stores the converted value (line 121), but the authorizer compares this lowercase version against the original namespace name from the request (line 60), causing a lookup failure.",
      "proposedFix": "Fix the case-sensitivity handling in either the claims mapper or authorizer to maintain consistent namespace name casing throughout the authorization flow.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by PR #1671 submitted by sergeybykov to address the namespace case-sensitivity issue.",
      "related": [
        1671
      ],
      "keyQuote": "The default claims mapper converts all uppercase characters in namespace name to lowercase and passes them further to authorizer.",
      "number": 1670,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:16:30.430Z"
    },
    {
      "summary": "MySQL 5.7 compatibility issue during schema initialization where the auto-setup container fails to create the temporal database due to an invalid default value for the 'session_start' column. The schema version compatibility check reports a version mismatch (Expected 1.5, Actual 0.0) preventing server startup.",
      "category": "bug",
      "subcategory": "database-schema",
      "apis": [],
      "components": [
        "database",
        "schema-migration",
        "auto-setup",
        "mysql"
      ],
      "concepts": [
        "schema-initialization",
        "version-compatibility",
        "database-startup",
        "mysql-5.7",
        "docker-setup"
      ],
      "severity": "high",
      "userImpact": "Users attempting to run Temporal server with MySQL 5.7 using docker-compose experience startup failures, blocking server deployment.",
      "rootCause": "MySQL 5.7 does not support certain default value syntax used in the schema initialization scripts, causing the 'session_start' column creation to fail with 'Invalid default value' error.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Error 1067: Invalid default value for 'session_start'",
      "number": 1669,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:16:33.426Z"
    },
    {
      "summary": "The default token key provider incorrectly rejects JWT tokens signed with the ES256 algorithm due to a typo in the algorithm name check (checking for 'ec256' instead of 'es256').",
      "category": "bug",
      "subcategory": "authorization",
      "apis": [],
      "components": [
        "default-token-key-provider",
        "authorization-interceptor"
      ],
      "concepts": [
        "JWT",
        "token-signing",
        "EC-algorithm",
        "authentication",
        "key-provider"
      ],
      "severity": "high",
      "userImpact": "Users cannot authenticate to Temporal using JWT tokens signed with EC public keys, preventing access to the server.",
      "rootCause": "Typo in the algorithm name comparison: checking for 'ec256' instead of the correct 'es256' (RFC 7518 standard).",
      "proposedFix": "Correct the typo in the if statement to check for 'es256' instead of 'ec256'.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed via PR #1664 which corrected the algorithm name from 'ec256' to 'es256'.",
      "related": [
        1664
      ],
      "keyQuote": "It looks like there's a typo in this `if` statement as it's referring to `ec256` instead of `es256`",
      "number": 1662,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:16:33.132Z"
    },
    {
      "summary": "The PollWorkflowTaskQueueResponse message contains redundant query attributes (singular 'query' and plural 'queries' map) that should be unified into a single attribute for consistency and cleaner API design.",
      "category": "feature",
      "subcategory": "api-design",
      "apis": [
        "PollWorkflowTaskQueueResponse"
      ],
      "components": [
        "api",
        "proto",
        "query"
      ],
      "concepts": [
        "api-consolidation",
        "schema-unification",
        "backwards-compatibility",
        "refactoring",
        "message-design"
      ],
      "severity": "low",
      "userImpact": "Users would benefit from a simpler, more consistent API interface without redundant query response attributes.",
      "rootCause": "Legacy API design resulted in both singular and plural query attributes being exposed in the same response message.",
      "proposedFix": "Merge the singular 'query' attribute and 'queries' map into a single unified attribute in PollWorkflowTaskQueueResponse.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Won't fix for now, will reopen if situation changes.",
      "related": [],
      "keyQuote": "Won't fix for now, will reopen if situation changes.",
      "number": 1658,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:16:15.242Z"
    },
    {
      "summary": "tctl workflow signal command fails to pass multiple parameters to workflows correctly - only the first parameter is received while subsequent parameters arrive as null. This issue affects users sending signals with 2+ parameters from tctl.",
      "category": "bug",
      "subcategory": "signal-handling",
      "apis": [],
      "components": [
        "tctl",
        "signal-handler",
        "parameter-marshalling"
      ],
      "concepts": [
        "signal",
        "parameters",
        "multi-parameter",
        "marshalling",
        "CLI"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably send signals with multiple parameters via tctl, breaking workflows that depend on multi-parameter signals.",
      "rootCause": "Parameter marshalling or deserialization in tctl signal command fails to handle multiple parameters correctly",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Issue resolved by deprecation - tctl was deprecated in favor of temporal/CLI which should handle multi-parameter signals correctly",
      "related": [],
      "keyQuote": "TCTL is deprecated. Please switch to temporal/CLI https://github.com/temporalio/cli",
      "number": 1655,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:16:19.624Z"
    },
    {
      "summary": "MySQL InnoDB's trx_rseg_history_len metric grows excessively during heavy temporal workload processing, indicating long-running transactions that prevent undo log cleanup and can cause CPU spikes.",
      "category": "bug",
      "subcategory": "database-performance",
      "apis": [],
      "components": [
        "sql-persistence-layer",
        "mysql-backend",
        "transaction-management"
      ],
      "concepts": [
        "transaction-lifecycle",
        "undo-log",
        "database-cleanup",
        "performance-degradation",
        "resource-leak"
      ],
      "severity": "medium",
      "userImpact": "Users with large workflow backlogs may experience database performance degradation and CPU spikes as InnoDB attempts to clean up accumulated transaction history.",
      "rootCause": "Long-running transactions (possibly read-only) in the SQL persistence layer prevent InnoDB from cleaning up the undo log, causing trx_rseg_history_len to grow unbounded.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "stale",
      "resolutionDetails": "Issue closed due to lack of evidence and activity. Maintainers noted that Temporal transactions complete within milliseconds and suspected the issue was specific to a misbehaving cluster configuration.",
      "related": [],
      "keyQuote": "any transaction should be done within within few ms meaning there is no acquire lock on DB, do something that can take minutes, then release lock operation.",
      "number": 1646,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:16:15.903Z"
    },
    {
      "summary": "Request to make tctl admin commands use the same environment variables as temporal-sql-tool for database connection parameters instead of requiring command-line arguments.",
      "category": "feature",
      "subcategory": "cli-configuration",
      "apis": [],
      "components": [
        "tctl",
        "database-connection"
      ],
      "concepts": [
        "environment-variables",
        "cli-arguments",
        "database-credentials",
        "usability",
        "configuration"
      ],
      "severity": "low",
      "userImpact": "Users must pass database credentials on the command line for each tctl admin command instead of using environment variables for convenience.",
      "rootCause": null,
      "proposedFix": "Align tctl admin commands to accept the same environment variables as temporal-sql-tool for database connection parameters.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "tctl is deprecated in favor of temporal CLI, which no longer supports direct database connections.",
      "related": [],
      "keyQuote": "tctl is deprecated, should switch to use temporal CLI. Also, tctl or temporal CLI no longer support directly connect to backend databases.",
      "number": 1644,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:15:33.936Z"
    },
    {
      "summary": "Upgrade to go-mock 1.6.0, which introduces a breaking change that impacts the blocking matching service tests.",
      "category": "other",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-framework",
        "mock-service",
        "blocking-matching-service"
      ],
      "concepts": [
        "dependency-upgrade",
        "breaking-change",
        "test-compatibility",
        "mocking",
        "compatibility"
      ],
      "severity": "medium",
      "userImpact": "Tests fail due to breaking changes in the go-mock library, requiring code updates to maintain test compatibility.",
      "rootCause": "go-mock 1.6.0 introduces breaking changes that affect blocking matching service tests",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It seems that go mock 1.6.0 comes with breaking change blocking matching service tests",
      "number": 1638,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:15:35.473Z"
    },
    {
      "summary": "Request to add a tctl command for removing/deleting namespaces. Currently, users cannot delete namespaces and must drop the entire temporal database to start fresh, which hinders experimentation and learning.",
      "category": "feature",
      "subcategory": "namespace-management",
      "apis": [],
      "components": [
        "tctl",
        "namespace-service",
        "database"
      ],
      "concepts": [
        "namespace-deletion",
        "namespace-deprecation",
        "experimentation",
        "learning-curve",
        "database-management"
      ],
      "severity": "medium",
      "userImpact": "Users learning Temporal cannot easily delete namespaces to experiment with naming strategies without dropping the entire database.",
      "rootCause": null,
      "proposedFix": "Add a tctl command to remove/delete namespaces",
      "workaround": "Drop the temporal database and recreate it using schema migration tools",
      "resolution": "fixed",
      "resolutionDetails": "Namespace deletion capability was implemented and tracked alongside namespace deprecation features in issue #1679",
      "related": [
        1679
      ],
      "keyQuote": "the ability to rename or delete namespaces is pretty important in that sense to make experimentation cheaper",
      "number": 1626,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:15:35.797Z"
    },
    {
      "summary": "Remove the deprecated 'sampled retention days' configuration option from namespace config, which was a workaround created before long-term archival was available.",
      "category": "feature",
      "subcategory": "namespace-configuration",
      "apis": [],
      "components": [
        "namespace-config",
        "archival-system"
      ],
      "concepts": [
        "retention",
        "archival",
        "configuration",
        "cleanup",
        "deprecated-feature"
      ],
      "severity": "low",
      "userImpact": "Users will no longer be able to configure sampled retention, but can use long-term archival as the modern alternative.",
      "rootCause": "Sampled retention was a temporary workaround created before long-term archival feature was available.",
      "proposedFix": "Remove the sampled retention days configuration option from namespace config.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The deprecated sampled retention configuration has been removed from the namespace configuration.",
      "related": [],
      "keyQuote": "We should get rid of the sampled retention.",
      "number": 1622,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:15:22.253Z"
    },
    {
      "summary": "Incomplete issue report with placeholder 'test' content in title, description, and comment. Lacks actual problem statement, reproduction steps, or specifications.",
      "category": "other",
      "subcategory": "invalid-report",
      "apis": [],
      "components": [],
      "concepts": [
        "issue-reporting",
        "placeholder-content",
        "incomplete-submission"
      ],
      "severity": "low",
      "userImpact": "Unable to assess impact due to lack of substantive issue details.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was closed as invalid - it contains only placeholder text with no actual problem or feature request details.",
      "related": [],
      "keyQuote": "test",
      "number": 1619,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:15:23.579Z"
    },
    {
      "summary": "Incomplete test issue with minimal content - appears to be a template or placeholder submission with no actual problem description or steps to reproduce.",
      "category": "other",
      "subcategory": "test-framework",
      "apis": [],
      "components": [],
      "concepts": [
        "testing",
        "validation",
        "issue-tracking"
      ],
      "severity": "low",
      "userImpact": "Unable to determine - issue lacks sufficient information to assess impact.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was closed likely due to insufficient information and missing problem description.",
      "related": [],
      "keyQuote": "test",
      "number": 1617,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:15:24.937Z"
    },
    {
      "summary": "PostgreSQL docker-compose fails to start because the temporal database is auto-created by PostgreSQL initialization before the schema setup script runs, causing the schema version check to fail when the server starts.",
      "category": "bug",
      "subcategory": "docker-setup",
      "apis": [],
      "components": [
        "docker-compose",
        "postgres-sql-tool",
        "auto-setup",
        "schema-initialization"
      ],
      "concepts": [
        "database-initialization",
        "schema-version",
        "docker-entrypoint",
        "postgres-superuser",
        "startup-order"
      ],
      "severity": "high",
      "userImpact": "Users cannot start a local Temporal development environment with the PostgreSQL docker-compose file without manual workarounds.",
      "rootCause": "The POSTGRES_USER environment variable creates a superuser and database with the same name before the auto-setup.sh initialization script runs, causing the temporal database to exist when schema setup is attempted. The set -e flag in auto-setup.sh causes the script to exit on the create database error, leaving the schema uninitialized.",
      "proposedFix": "Modify auto-setup.sh to handle the case where the database already exists or use a different initialization approach that accounts for pre-created databases.",
      "workaround": "Specify a different name for the Temporal database through the DBNAME variable, or use docker-compose-postgres12.yml instead of the default compose file.",
      "resolution": "fixed",
      "resolutionDetails": "The issue was resolved by adjusting the PostgreSQL initialization process in the docker-compose setup to properly handle database creation sequencing.",
      "related": [],
      "keyQuote": "This variable will create the specified user with superuser power and a database with the same name... This means that temporal database is created before initialization script is run.",
      "number": 1613,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:15:10.401Z"
    },
    {
      "summary": "tctl wf start/run command incorrectly requires execution_timeout parameter even though the Temporal service defaults it to infinity. The error indicates the parameter format is invalid when not provided.",
      "category": "bug",
      "subcategory": "tctl-cli",
      "apis": [],
      "components": [
        "tctl",
        "workflow-start",
        "parameter-parsing"
      ],
      "concepts": [
        "execution-timeout",
        "optional-parameters",
        "cli-validation",
        "service-defaults"
      ],
      "severity": "medium",
      "userImpact": "Users cannot start workflows with tctl without explicitly providing an execution_timeout parameter, even though it should be optional with a service-side default.",
      "rootCause": "tctl is treating execution_timeout as a required parameter or failing to properly handle its absence, despite the service supporting infinite timeouts as default.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The execution_timeout parameter was made optional in tctl to align with service behavior that defaults to infinity.",
      "related": [],
      "keyQuote": "tctl wf start/run should not require execution timeout parameter as the service defaults it to infinity.",
      "number": 1601,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:15:06.989Z"
    },
    {
      "summary": "Reducing namespace retention period only affects new workflows; users must wait for the original retention period to expire before old workflows are cleaned up. Users requested automatic cleanup of completed workflows exceeding the new retention period.",
      "category": "feature",
      "subcategory": "namespace-retention",
      "apis": [],
      "components": [
        "namespace-manager",
        "workflow-retention",
        "scavenger"
      ],
      "concepts": [
        "retention-period",
        "workflow-cleanup",
        "data-management",
        "namespace-configuration",
        "batch-operations"
      ],
      "severity": "medium",
      "userImpact": "Users cannot immediately clean up workflows when reducing retention periods, requiring manual batch operations and causing unexpected storage growth.",
      "rootCause": "Retention period changes only apply to newly created workflows; existing workflows use their original retention period.",
      "proposedFix": "Provide automatic cleanup capability or at minimum clearer UI guidance on batch operations for handling workflows exceeding reduced retention periods.",
      "workaround": "Users can manually trigger batch delete operations with query filters to remove workflows closed for more than X days.",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved by implementing batch operations capability. Users can now trigger batch delete with query filters, though auto-cleanup was not implemented.",
      "related": [],
      "keyQuote": "This is now possible to do via batch operation. We decided to not auto trigger a batch delete but user has the ability to trigger a batch delete with query filter",
      "number": 1598,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:15:10.753Z"
    },
    {
      "summary": "tctl command to update namespace retention days to 0 succeeds but doesn't actually change the retention value. The issue discusses whether to reject such requests with clear error messages or properly support zero retention, noting that proto doesn't differentiate between 0 and unset values.",
      "category": "bug",
      "subcategory": "retention-policy",
      "apis": [],
      "components": [
        "tctl",
        "namespace",
        "retention"
      ],
      "concepts": [
        "retention-policy",
        "namespace-configuration",
        "proto-serialization",
        "replication",
        "archival"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to set namespace retention to 0 receive a success message but the retention is not actually updated, causing confusion and potential data retention issues.",
      "rootCause": "Proto definition does not differentiate between 0 and unset values for retention days, making it impossible to properly set retention to 0 through tctl.",
      "proposedFix": "Either disallow 0 as retention value with clear error message, or allow tctl to specify durations (e.g., 12h, 30m) with minimum retention constraints (>0 for non-global namespaces, 24h for global namespaces).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed with target behavior defined: disallow 0 retention, support time duration specifications, and enforce minimum retention thresholds.",
      "related": [],
      "keyQuote": "disallow 0 as retention; allow tctl to specify > 0 retention time, e.g. 12h / 30m, etc for non global namespace; for global namespace, min retention should be 24h",
      "number": 1592,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:10:54.291Z"
    },
    {
      "summary": "Admin tool's workflow delete command fails with an unclear error when RunID is not specified. The tool should either make RunID a required argument or automatically determine the latest RunID if omitted.",
      "category": "bug",
      "subcategory": "admin-tool",
      "apis": [],
      "components": [
        "tctl",
        "admin-tool",
        "workflow-deletion",
        "mutableState"
      ],
      "concepts": [
        "workflow-deletion",
        "runid-resolution",
        "error-handling",
        "admin-operations",
        "usability"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to delete workflows without specifying RunID encounter cryptic error messages instead of clear guidance or automatic RunID detection.",
      "rootCause": "Admin tool does not validate or resolve RunID parameter before attempting deletion, resulting in invalid UUID error when deleting mutableState.",
      "proposedFix": "Either make RunID a required argument with validation, or implement logic to automatically determine and use the latest/current RunID if not specified.",
      "workaround": "Use the tdbg tool instead: `tdbg workflow delete --wid <workflow_id>` which prompts for confirmation and handles RunID resolution.",
      "resolution": "fixed",
      "resolutionDetails": "Functionality moved to tdbg tool which properly handles RunID resolution and provides better user experience with confirmation prompts.",
      "related": [],
      "keyQuote": "Either reject the command and make RunID required argument, OR figure out latest / current RunID if RunID is not specified. (preferred option)",
      "number": 1589,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:10:55.323Z"
    },
    {
      "summary": "tctl rejects primitive values (strings) passed as workflow input parameters, requiring them to be valid JSON objects. The issue was resolved by clarifying that string values must be double-quoted to be parsed correctly as JSON.",
      "category": "bug",
      "subcategory": "cli-input-handling",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "tctl",
        "cli",
        "input-parser"
      ],
      "concepts": [
        "JSON-parsing",
        "input-parameters",
        "primitive-types",
        "string-handling",
        "type-coercion"
      ],
      "severity": "low",
      "userImpact": "Users attempting to pass primitive string values as workflow input parameters via tctl receive an error and must work around it by properly formatting input as valid JSON.",
      "rootCause": "tctl's input parsing expects valid JSON format and does not automatically wrap primitive values in JSON-compatible syntax.",
      "proposedFix": null,
      "workaround": "Double-quote string values so they are parsed correctly as JSON strings.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved as user error - string values must be double-quoted to be valid JSON. Issue was closed after clarification from the author.",
      "related": [],
      "keyQuote": "Works as desired -- string values must be double-quoted to be parsed correctly.",
      "number": 1585,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:10:54.204Z"
    },
    {
      "summary": "The tctl describe workflow command was not returning search attributes in the response, even though they should be included according to the API proto definition. The issue was determined to be on the user's side, not a server bug.",
      "category": "bug",
      "subcategory": "tctl-command",
      "apis": [
        "DescribeWorkflowExecution"
      ],
      "components": [
        "tctl",
        "workflow-execution",
        "api-response"
      ],
      "concepts": [
        "search-attributes",
        "workflow-metadata",
        "describe-workflow",
        "tctl-output"
      ],
      "severity": "low",
      "userImpact": "Users attempting to retrieve search attributes via tctl describe workflow would not see them in the output.",
      "rootCause": "User-side configuration issue, not a server-side bug.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "The issue reporter confirmed after investigation that this was a problem on their side, not a bug in the product.",
      "related": [],
      "keyQuote": "Confirmed this was an issue on our side. Sorry for the noise!",
      "number": 1584,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:10:42.980Z"
    },
    {
      "summary": "Orphaned task queues created by failed tests can cause Cassandra tombstone scanning performance issues when a cron workflow continues submitting tasks indefinitely to queues with no workers, forcing the task reader to repeatedly scan expired TTL'd tasks.",
      "category": "bug",
      "subcategory": "task-queue-management",
      "apis": [],
      "components": [
        "task-manager",
        "task-reader",
        "tasks-table",
        "cron-workflow"
      ],
      "concepts": [
        "orphaned-task-queue",
        "tombstone-scanning",
        "ttl-expiration",
        "cursor-management",
        "cassandra-performance"
      ],
      "severity": "high",
      "userImpact": "Users with failed test infrastructure or poorly managed workflows can inadvertently create performance degradation in Cassandra due to unbounded tombstone scanning.",
      "rootCause": "TTL-based task deletion prevents the task reader cursor from advancing when tasks are orphaned, causing repeated scans over expired tombstones in Cassandra.",
      "proposedFix": "Replace TTL-based task expiration with explicit deletion to allow the task reader to move the cursor forward when no active tasks remain beyond a certain point.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "but since new tasks were being submitted by the orphaned cron every minute, the task manager for the task queue remained active and the taskReader would scan over all the tombstones",
      "number": 1579,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:10:39.433Z"
    },
    {
      "summary": "Request to add a workflow_id-level option to tctl batch start command to enable operations on workflow IDs without requiring specific run IDs. This would provide a more systematic way to handle cron workflow cancellations.",
      "category": "feature",
      "subcategory": "batch-operations",
      "apis": [
        "ScanWorkflowExecutions",
        "CountWorkflow",
        "ExecuteWorkflow",
        "SearchAttributes"
      ],
      "components": [
        "tctl",
        "batch-operations",
        "visibility-api",
        "search-attributes"
      ],
      "concepts": [
        "cron-workflows",
        "workflow-cancellation",
        "batch-operations",
        "search-attributes",
        "workflow-id-filtering",
        "visibility-query"
      ],
      "severity": "low",
      "userImpact": "Users would have a more convenient and systematic way to perform batch operations on workflow IDs, particularly for canceling cron workflows, without needing to construct manual queries.",
      "rootCause": null,
      "proposedFix": "Generate a query from the workflow_id flag (e.g., 'batch start --workflow_id test_workflow_id' translates to query like \"WorkflowId='default.test_workflow_id'\") to avoid backend API changes. Alternatively, add WorkflowId field to CountWorkflow and ExecuteWorkflow APIs.",
      "workaround": "Run visibility query after batch operation and rerun if additional running cron workflows are found. Use WorkflowId as a search attribute in the existing query flag.",
      "resolution": "wontfix",
      "resolutionDetails": "No plan to implement. Batch Operation APIs have inherent limitations where workflow state can change during batch execution, making reliable cron workflow termination challenging. Workaround recommended instead.",
      "related": [],
      "keyQuote": "The challenge is the workflow state could change while that batch operation is running, and a cron workflow may have completed and started a new run.",
      "number": 1578,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:10:42.488Z"
    },
    {
      "summary": "SQL connection errors during autosetup with Postgres show unhelpful generic messages that hide the actual root cause (e.g., misconfigured TLS certificates), making debugging difficult. The error handling in temporal-sql-tool should include the underlying connection errors instead of a static fallback message.",
      "category": "bug",
      "subcategory": "sql-setup",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "sqlplugin",
        "postgresql-plugin",
        "database-initialization"
      ],
      "concepts": [
        "error-handling",
        "debugging",
        "tls-configuration",
        "connection-pooling",
        "database-setup"
      ],
      "severity": "medium",
      "userImpact": "Users struggle to diagnose SQL configuration problems during Temporal setup because error messages mask the real underlying issue, leading to wasted debugging time.",
      "rootCause": "The tryConnect/sqlx.Connect function ignores individual connection errors when trying default database names and returns only a generic static error message instead of including the actual connection failure details.",
      "proposedFix": "Include the last error (or all errors) from the connection attempt loop in the returned error message to provide more diagnostic information.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #1413 which improved error message handling in the SQL plugin connection logic.",
      "related": [
        1413
      ],
      "keyQuote": "Errors from the loop of trying different default DB names are ignored - instead a static string error is returned. Ideally this message would also include the Last error",
      "number": 1577,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:10:27.560Z"
    },
    {
      "summary": "User requested Java ScheduledExecutor-like functionality for scheduling Temporal workflows, including support for initial delay, fixed delay/rate scheduling, and one-time executions. This was resolved by the introduction of the Temporal Schedules feature.",
      "category": "feature",
      "subcategory": "workflow-scheduling",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-scheduler",
        "cron-engine",
        "schedule-service"
      ],
      "concepts": [
        "scheduling",
        "periodic-execution",
        "initial-delay",
        "fixed-rate",
        "fixed-delay",
        "cron-expression"
      ],
      "severity": "medium",
      "userImpact": "Users can now schedule workflows with more flexibility including initial delays and fixed-rate/fixed-delay options without relying solely on cron expressions.",
      "rootCause": null,
      "proposedFix": "Implement Java ScheduledExecutor-like API for workflow scheduling with support for initial delay, period-based scheduling, and one-time scheduling.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The Temporal Schedules feature was implemented to provide the requested scheduling capabilities including start time configuration.",
      "related": [],
      "keyQuote": "It would be nice to have a Java Scheduled Executor like functionality for Temporal workflows with all of same options.",
      "number": 1566,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:10:27.561Z"
    },
    {
      "summary": "User inquires about polyglot programming support in Temporal, specifically whether activities written in different languages (Go, Java, PHP) can be used together in a single workflow, and requests documentation on how to handle language-specific differences like error handling.",
      "category": "docs",
      "subcategory": "polyglot-programming",
      "apis": [],
      "components": [
        "activity-executor",
        "sdk-interoperability"
      ],
      "concepts": [
        "polyglot-programming",
        "cross-language-communication",
        "activity-composition",
        "error-handling",
        "language-differences"
      ],
      "severity": "low",
      "userImpact": "Users need clear documentation on how to build workflows with activities implemented in different programming languages.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Community sample repository (temporal-polyglot) and documentation pull request (#443) were provided as solutions showing communication between all 4 SDKs.",
      "related": [
        443
      ],
      "keyQuote": "An Example would be Activity A Written using Golang Activity B Written In Java Activity C Written in PHP Workflow 1 - Written in Java/Golang, but has to use all the Above activities.",
      "number": 1559,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T02:10:28.682Z"
    },
    {
      "summary": "Two bugs in namespace replication: the get replication messages API doesn't include the caller cluster name, and enqueue message incorrectly uses the last message ID which can be deleted after successful replication.",
      "category": "bug",
      "subcategory": "namespace-replication",
      "apis": [],
      "components": [
        "replicator",
        "namespace-replication",
        "message-processor"
      ],
      "concepts": [
        "replication",
        "message-handling",
        "cluster-communication",
        "state-management"
      ],
      "severity": "high",
      "userImpact": "Multi-cluster replication may fail or produce incorrect results due to missing cluster identification and message ID tracking issues.",
      "rootCause": "API not returning caller cluster name and message enqueue logic depending on potentially deleted message IDs",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed through related issues #1568 and #1569",
      "related": [
        1568,
        1569
      ],
      "keyQuote": "get replication messages API should contain caller cluster name... enqueue message should not use the last message ID since this message can be deleted",
      "number": 1555,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:54:47.330Z"
    },
    {
      "summary": "Temporal server should include the type of throttling limit and throttling QPS in resource limit exceeded error messages to help users better understand and debug rate limiting issues.",
      "category": "other",
      "subcategory": "error-messages",
      "apis": [],
      "components": [
        "server",
        "rate-limiter",
        "error-handling"
      ],
      "concepts": [
        "throttling",
        "resource-limits",
        "rate-limiting",
        "error-messages",
        "observability",
        "operations"
      ],
      "severity": "medium",
      "userImpact": "Users receive vague resource limit exceeded errors without understanding which limit was hit or the actual QPS, making it harder to diagnose and resolve rate limiting issues.",
      "rootCause": null,
      "proposedFix": "Include throttling limit type and current QPS in the error message details",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal server should include type of throttling limit & throttling QPS within resource limit exceeded error message",
      "number": 1552,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:54:45.239Z"
    },
    {
      "summary": "DescribeWorkflowExecution was calculating execution time based on start time and cron schedule/retry policy at call time. The TODO requested adding execution time to mutable state to avoid recalculating on each call.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [
        "DescribeWorkflowExecution"
      ],
      "components": [
        "history-engine",
        "mutable-state",
        "workflow-execution"
      ],
      "concepts": [
        "execution-time",
        "workflow-state",
        "performance-optimization",
        "cron-schedule",
        "retry-policy"
      ],
      "severity": "low",
      "userImpact": "Avoids unnecessary recalculation of execution time on each DescribeWorkflowExecution call, reducing computational overhead.",
      "rootCause": "Execution time was calculated dynamically on each call instead of being persisted in mutable state.",
      "proposedFix": "Add execution time to mutable state to avoid recalculation based on start time and cron schedule/retry policy.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by commit 00394e9d14ac479e5bb65eed4b70161b006aadeb which implemented the proposed fix.",
      "related": [],
      "keyQuote": "we need to consider adding execution time to mutable state. For now execution time will be calculated based on start time and cron schedule/retry policy",
      "number": 1549,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:54:43.997Z"
    },
    {
      "summary": "When a worker is down, queries return a cryptic 'context deadline exceeded' error instead of a more informative message. The improvement implements clearer error messaging like 'no poller seen for task queue recently, worker may be down' to help users understand the actual cause of query failures.",
      "category": "feature",
      "subcategory": "query-error-messages",
      "apis": [],
      "components": [
        "query-handler",
        "task-queue",
        "worker-poller",
        "error-messaging"
      ],
      "concepts": [
        "worker-down",
        "timeout",
        "error-messaging",
        "task-queue-polling",
        "precondition-check",
        "user-experience"
      ],
      "severity": "low",
      "userImpact": "Users receive clearer error messages when queries fail due to workers being down, making it easier to diagnose and troubleshoot workflow issues.",
      "rootCause": "Query handler returns generic context deadline error instead of detecting and reporting the specific condition when no worker is polling the task queue.",
      "proposedFix": "Return a FailedPrecondition error with message 'no poller seen for task queue recently, worker may be down' when appropriate conditions are detected.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #5252 which implements proper error detection and messaging when no worker is polling the task queue.",
      "related": [
        5252
      ],
      "keyQuote": "no poller seen for task queue recently, worker may be down",
      "number": 1546,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:39:08.642Z"
    },
    {
      "summary": "Cron workflows that have child workflow failures currently show as ContinuedAsNew status instead of surfacing the failure, making it difficult for users to see errors on the WebUI. The request is to allow parent workflows to end with a Failure event instead of continuing as new.",
      "category": "feature",
      "subcategory": "cron-workflows",
      "apis": [],
      "components": [
        "cron-workflow",
        "workflow-status",
        "webui"
      ],
      "concepts": [
        "cron-scheduling",
        "failure-handling",
        "workflow-visibility",
        "child-workflow",
        "status-reporting"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily see when cron workflows with child workflow failures have occurred, requiring manual navigation to find failure details.",
      "rootCause": null,
      "proposedFix": "End parent workflows with a Failure event rather than ContinuedAsNew when child workflows fail",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "If parent cron workflow starts child workflow and child workflow fails, then the status of parent workflow will remain as ContinuedAsNew",
      "number": 1545,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:39:08.527Z"
    },
    {
      "summary": "Elasticsearch auto-setup scripts in Docker do not support username and password authentication, making them unsuitable for production use and requiring workarounds in Kubernetes deployments.",
      "category": "bug",
      "subcategory": "elasticsearch-setup",
      "apis": [],
      "components": [
        "docker-scripts",
        "elasticsearch-setup",
        "auto-setup",
        "frontend-container"
      ],
      "concepts": [
        "authentication",
        "elasticsearch",
        "docker-setup",
        "kubernetes-deployment",
        "auto-configuration",
        "production-readiness"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use Elasticsearch with username/password authentication in Docker auto-setup, forcing them to implement custom workarounds for Kubernetes deployments.",
      "rootCause": "The curl commands in docker/start.sh scripts do not include username and password parameters when connecting to Elasticsearch, and the frontend container polling logic also lacks authentication support.",
      "proposedFix": "Update the curl commands in docker/start.sh to support username and password parameters for Elasticsearch authentication.",
      "workaround": "Mount a modified Elasticsearch setup script in a Kubernetes init container and change the frontend container command to use ./start-temporal.sh instead of the default startup command.",
      "resolution": "fixed",
      "resolutionDetails": "Elasticsearch setup was refactored to run only in the auto-setup image, and a Kubernetes server job was created for index creation that supports authentication in production environments.",
      "related": [
        1501
      ],
      "keyQuote": "the docker start.sh scripts, do not use username or password with curl. This means that you cannot use a username/password for elasticsearch",
      "number": 1544,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:54:31.746Z"
    },
    {
      "summary": "Dynamic config setting for frontend.keepAliveMaxConnectionAge was not working because the YAML value needed to be specified with a time duration suffix (e.g., '600s' instead of '600'). This was a configuration syntax issue rather than a code bug.",
      "category": "bug",
      "subcategory": "dynamic-config",
      "apis": [],
      "components": [
        "frontend",
        "dynamic-config",
        "config-parser"
      ],
      "concepts": [
        "keepalive",
        "connection-management",
        "configuration",
        "duration-parsing",
        "gRPC"
      ],
      "severity": "medium",
      "userImpact": "Users unable to configure frontend keepalive settings dynamically, unable to tune connection timeout behavior without restarting.",
      "rootCause": "Dynamic config YAML parser requires Go duration format (e.g., '600s') but documentation or examples showed plain numeric values (e.g., '600').",
      "proposedFix": "Use '600s' instead of '600' when setting frontend.keepAliveMaxConnectionAge in dynamic config YAML.",
      "workaround": "Append 's' suffix to the duration value to specify seconds explicitly.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by identifying that the duration value requires Go time.Duration format with unit suffix.",
      "related": [],
      "keyQuote": "600 -> 600s notice the s ref: https://golang.org/pkg/time/#ParseDuration",
      "number": 1538,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:38:54.563Z"
    },
    {
      "summary": "Dynamic configuration keys are case-sensitive, making it easy for users to misconfigure settings due to typos. The request is to make these keys case-insensitive for better usability.",
      "category": "feature",
      "subcategory": "dynamic-configuration",
      "apis": [],
      "components": [
        "configuration",
        "dynamic-config",
        "config-keys"
      ],
      "concepts": [
        "case-sensitivity",
        "configuration",
        "usability",
        "error-prevention",
        "dynamic-settings"
      ],
      "severity": "low",
      "userImpact": "Users may fail to apply dynamic configuration changes due to case-sensitive key matching, leading to unexpected behavior.",
      "rootCause": "Configuration key matching is implemented with case-sensitive comparison.",
      "proposedFix": "Make dynamic configuration key matching case-insensitive.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Configuration key matching was updated to be case-insensitive.",
      "related": [],
      "keyQuote": "We should make these case insensitive.",
      "number": 1536,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:38:52.803Z"
    },
    {
      "summary": "Request for server-side encryption of data at rest in the database to meet compliance requirements. Currently only client-side encryption via DataConverter is available, which requires reimplementation across multiple SDKs and adds complexity for data that doesn't need encryption in transit.",
      "category": "feature",
      "subcategory": "encryption",
      "apis": [],
      "components": [
        "persistence-layer",
        "cassandra-storage",
        "server-options",
        "payload-serializer"
      ],
      "concepts": [
        "encryption",
        "compliance",
        "data-at-rest",
        "multi-sdk",
        "database-security",
        "regulatory-requirements"
      ],
      "severity": "high",
      "userImpact": "Organizations with compliance and regulatory requirements cannot easily encrypt data at rest without implementing custom client-side encryption across all SDK languages.",
      "rootCause": "Temporal server lacks built-in server-side encryption capability; encryption is only available through client-side DataConverter implementation.",
      "proposedFix": "Refactor persistence layer to use pluggable serializer/deserializer, optionally refactor Cassandra layer, and expose PayloadSerializer in temporal/server_options.go",
      "workaround": "Use a proxy in front of Temporal server to perform server-side encryption, works for both self-hosted and Temporal Cloud deployments",
      "resolution": "wontfix",
      "resolutionDetails": "No plan to add direct server-side encryption support; users advised to use proxy solution instead",
      "related": [],
      "keyQuote": "it's clear that the only way to currently achieve this is through client/SDK side encryption...we do _not_ need the data to be encrypted while in transit/memory",
      "number": 1531,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:38:56.920Z"
    },
    {
      "summary": "MutableState cache is currently created per shard, making it difficult to configure cache size based on cluster topology. The request is to refactor it to use a common cache per host shared by all shards, similar to the EventsCache pattern.",
      "category": "feature",
      "subcategory": "caching",
      "apis": [],
      "components": [
        "history-cache",
        "mutable-state",
        "shard-management"
      ],
      "concepts": [
        "caching",
        "cache-configuration",
        "cluster-topology",
        "per-host-cache",
        "memory-management"
      ],
      "severity": "medium",
      "userImpact": "Operators have difficulty configuring MutableState cache size appropriately for their cluster topology and shard distribution.",
      "rootCause": "MutableState cache is instantiated per shard rather than per host, creating a coupling between cache size and cluster sharding strategy.",
      "proposedFix": "Create a common cache per host that is shared by all shards in the cluster, following the EventsCache pattern.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by pull request #4833 which refactored the caching strategy.",
      "related": [
        4833
      ],
      "keyQuote": "We should have a common cache per host which is shared by all shards in the cluster. Same as EventsCache.",
      "number": 1530,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:38:40.931Z"
    },
    {
      "summary": "The sql-tool fails when database or schema already exists, preventing idempotent runs. Users need the ability to ignore these 'already exists' errors to safely re-run the schema creation tool multiple times.",
      "category": "feature",
      "subcategory": "sql-tool",
      "apis": [],
      "components": [
        "sql-tool",
        "schema-setup",
        "database-initialization"
      ],
      "concepts": [
        "idempotency",
        "error-handling",
        "database-setup",
        "schema-creation",
        "operations"
      ],
      "severity": "medium",
      "userImpact": "Users cannot safely re-run sql-tool for schema creation without manual intervention, breaking infrastructure-as-code patterns.",
      "rootCause": "The sql-tool does not gracefully handle 'already exists' errors during database and schema creation.",
      "proposedFix": "Add support for ignoring 'already exists' errors, similar to the implementation in docker/start.sh#L77.",
      "workaround": "Users can manually handle these errors outside of sql-tool or modify their database setup scripts.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the feature was likely implemented.",
      "related": [
        1087
      ],
      "keyQuote": "It would be nice to be able to ignore those errors.",
      "number": 1523,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:38:40.145Z"
    },
    {
      "summary": "User requests bulk termination of workflows by submitted time range to replace slow one-by-one termination. Discussion explores performance considerations and alternative approaches like namespace deletion.",
      "category": "feature",
      "subcategory": "workflow-termination",
      "apis": [
        "TerminateWorkflow"
      ],
      "components": [
        "workflow-termination",
        "batch-operations",
        "temporal-worker"
      ],
      "concepts": [
        "bulk-operations",
        "time-range-filtering",
        "performance",
        "scalability",
        "namespace-management"
      ],
      "severity": "medium",
      "userImpact": "Users must manually terminate workflows one-by-one instead of performing bulk operations, resulting in significant time overhead for large-scale termination tasks.",
      "rootCause": null,
      "proposedFix": "Implement bulk workflow termination feature that filters by submitted time range, executed as a batch operation via Temporal worker to minimize resource consumption.",
      "workaround": "Use tctl batch command or write custom scripts to query workflows by time range and terminate via API, though this requires more work and consumes more resources.",
      "resolution": "wontfix",
      "resolutionDetails": "Discussion concluded without feature implementation; labeled for closure after 30 days. User acknowledged the limitations and workarounds available.",
      "related": [],
      "keyQuote": "We need to bulk terminated workflow based on its submitted time range. Currently, we have to terminate workflow one by one, which is too slow.",
      "number": 1522,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:38:40.758Z"
    },
    {
      "summary": "Feature request to wrap Java's high-fidelity test service as an out-of-process server executable that can be used as a lightweight sidecar by all SDK languages. This would provide reliable testing capabilities (replays, sleep skipping) across Ruby, Go, and other SDKs instead of maintaining separate, lower-fidelity test implementations.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-service",
        "test-framework",
        "sdk-integration"
      ],
      "concepts": [
        "testing",
        "test-fidelity",
        "multi-language-support",
        "replay",
        "sidecar",
        "service-wrapper",
        "production-emulation"
      ],
      "severity": "high",
      "userImpact": "Enables all SDK users to write trustworthy tests with high-fidelity testing capabilities like replays and sleep skipping, reducing risk of untestable code patterns in production.",
      "rootCause": "Current SDK test implementations vary in fidelity; Ruby and Go use local synchronous testing that doesn't emulate production and doesn't support replays, sleeps, and versioning testing.",
      "proposedFix": "Wrap the Java test service in a server executable that can be deployed as a lightweight sidecar and used by any SDK language.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Out-of-process test server was implemented and merged in sdk-java#470.",
      "related": [
        470
      ],
      "keyQuote": "Java is the only SDK to have a high-fidelity test service (i.e. one does replays, skips sleeps). If we wrap the java test service in a server executable, it can be used by all users of all SDKs.",
      "number": 1521,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:34:32.240Z"
    },
    {
      "summary": "Request to add a configurable 'RetryIn' field to retryable errors that allows specifying when an error should be retried, enabling better control over retry timing with optional jitter support for rate-limiting scenarios.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "error-handling",
        "retry-policy",
        "task-queue"
      ],
      "concepts": [
        "retry",
        "rate-limiting",
        "timeout",
        "jitter",
        "backoff",
        "semaphore",
        "concurrency-control"
      ],
      "severity": "medium",
      "userImpact": "Users implementing custom rate-limiting based on metadata (like request semaphores) would be able to specify exactly when retries should occur, rather than relying on fixed retry policies.",
      "rootCause": null,
      "proposedFix": "Add a 'RetryIn' field to retryable errors that specifies when the error should be retried, with SDK-side changes to honor this timing. Optionally support jitter to prevent thundering herd.",
      "workaround": "Configure retry policy with a fixed timeframe (without jitter) based on the semaphore timeout period, though this lacks flexibility and doesn't prevent stampeding herd issues.",
      "resolution": "fixed",
      "resolutionDetails": "Implemented via pull request #5182 as indicated by the follow-up comment linking the solution.",
      "related": [
        1507
      ],
      "keyQuote": "Add a 'RetryIn' field to the retryable errors. Just like whether a retryable or nonretryable error can be returned, this can be used to set when this would be retried.",
      "number": 1515,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:34:30.088Z"
    },
    {
      "summary": "Team exploring Temporal for large-scale workflow system inquires about feasibility of BigTable-based persistence as alternative to Cassandra, given their Google Cloud infrastructure preference and scale requirements (100s of executions/sec).",
      "category": "question",
      "subcategory": "persistence-layer",
      "apis": [],
      "components": [
        "persistence",
        "cassandra-driver",
        "database-abstraction"
      ],
      "concepts": [
        "consistency-guarantees",
        "atomic-updates",
        "row-level-transactions",
        "cloud-infrastructure",
        "scalability",
        "workflow-execution-state"
      ],
      "severity": "low",
      "userImpact": "Users running Temporal on Google Cloud seeking alternative persistence backends face limited options and must choose between unfamiliar Cassandra operations or expensive Cloud Spanner.",
      "rootCause": "Temporal's persistence layer is tightly coupled to Cassandra's consistency model; BigTable's single-row atomicity and lack of multi-row transactions are insufficient for UpdateCurrentWorkflowExecutionForNewQuery operations.",
      "proposedFix": "Implement conditional read-then-write operations over BigTable to simulate multi-row consistency for critical queries like UpdateCurrentWorkflowExecutionForNewQuery.",
      "workaround": "Use Google Cloud Spanner (more expensive but supports necessary transactions) or invest in operating Cassandra despite unfamiliarity.",
      "resolution": "wontfix",
      "resolutionDetails": "Community confirmed that BigTable's lack of multi-row transactions and complex consistency mechanisms make it incompatible with Temporal's requirements. Spanner is cost-prohibitive, leaving Cassandra as the practical choice for Google Cloud deployments.",
      "related": [],
      "keyQuote": "big table is not targeted as a OLTP database, so if your target is to use a google product, consider spanner or cloud sql",
      "number": 1514,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:34:28.812Z"
    },
    {
      "summary": "Version banner incorrectly displays on the latest version (1.9.0), suggesting a newer version is available when there isn't one. The issue was posted to the wrong project.",
      "category": "bug",
      "subcategory": "ui-version-banner",
      "apis": [],
      "components": [
        "ui",
        "version-banner"
      ],
      "concepts": [
        "version-detection",
        "release-notification",
        "ui-display"
      ],
      "severity": "low",
      "userImpact": "Users on the latest version see a misleading banner suggesting they upgrade, creating confusion about their current version status.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was reported against the wrong project by the author themselves.",
      "related": [],
      "keyQuote": "Wrong project!",
      "number": 1510,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:34:15.097Z"
    },
    {
      "summary": "Request for priority task queues to allow prioritizing high-priority tasks while maintaining fair scheduling across workers. Current implementation in v1.28.0 only supports priority within a single queue but doesn't address prioritizing in-progress workflows over new ones when worker capacity is limited.",
      "category": "feature",
      "subcategory": "task-queue-priority",
      "apis": [],
      "components": [
        "task-queue",
        "worker",
        "scheduler"
      ],
      "concepts": [
        "fairness",
        "scheduling",
        "priority",
        "resource-allocation",
        "backlog-management",
        "worker-capacity"
      ],
      "severity": "high",
      "userImpact": "Users running high-volume workflows with limited worker capacity struggle to ensure fair scheduling and prevent resource monopolization by large workflow batches, resulting in cascading workflow stagnation.",
      "rootCause": "Task queues lack sophisticated priority scheduling that balances task prioritization with fairness guarantees across different workflow sources or execution states.",
      "proposedFix": "Implement priority task queues that order tasks by priority while ensuring fair worker allocation; consider mechanisms to prioritize in-progress workflows over new ones.",
      "workaround": "Manually throttling workflow starts using external queues and monitoring capacity, though this is difficult to implement correctly due to variable data size and load.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Once a workflow has started (executed some activities), it should be prioritized to finish before beginning new workflows from the backlog.",
      "number": 1507,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:34:15.451Z"
    },
    {
      "summary": "Users request the ability to update workflow configuration properties like cron schedule and search attributes while a workflow is running, without requiring cancellation and restart.",
      "category": "feature",
      "subcategory": "workflow-configuration",
      "apis": [],
      "components": [
        "workflow-config",
        "cron-scheduler",
        "search-attributes"
      ],
      "concepts": [
        "workflow-update",
        "configuration-management",
        "cron-schedule",
        "retry-policy",
        "dynamic-updates"
      ],
      "severity": "medium",
      "userImpact": "Users must cancel and restart workflows to modify configuration, limiting flexibility for production workloads.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2651
      ],
      "keyQuote": "The ability to update a workflow config, including but not limited to CronSchedule, SearchAttributes, etc. without having to cancel and restart the workflow would be nice.",
      "number": 1499,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:34:16.162Z"
    },
    {
      "summary": "Users need a way to inject custom Authorizer implementations into Temporal Server without building their own Docker image from source. Currently, namespace-level authorization requires modifying the Temporal repository and building a custom image.",
      "category": "feature",
      "subcategory": "authorization",
      "apis": [],
      "components": [
        "authorizer",
        "docker-image",
        "server-startup"
      ],
      "concepts": [
        "authorization",
        "customization",
        "extensibility",
        "plugin",
        "namespace-level-security"
      ],
      "severity": "medium",
      "userImpact": "Users can now create custom Authorizer implementations and inject them into Temporal Server without needing to fork the repository or build custom Docker images.",
      "rootCause": "The Authorizer is hardcoded in the Temporal Server main.go, requiring users to modify the source code and rebuild the image.",
      "proposedFix": "Provide a way to plug in custom Authorizer implementations, such as through Docker layers that replace the server binary with a custom-built version that includes the custom Authorizer.",
      "workaround": "Build a Docker layer on top of the official image, compile a custom Temporal server binary with the custom Authorizer, and replace the binary in the container. Use base-builder and auto-setup images to accomplish this.",
      "resolution": "fixed",
      "resolutionDetails": "Users can now create custom Authorizers and inject them by building a custom server binary with their Authorizer implementation and replacing the temporal-server binary in the Docker image using a multi-stage build.",
      "related": [],
      "keyQuote": "Ideally, as a customer, we don't have to build our own image. We can just use images like temporalio/auto-setup:1.8.2 and inject or plug in our own implementation of the authorizer somehow.",
      "number": 1496,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:34:04.124Z"
    },
    {
      "summary": "Large payloads passed to many activities or child workflows are repeated in history, causing excessive history size. The request is for a payload deduplication mechanism, such as storing payloads once and referencing them elsewhere, or using content hashing to avoid duplication.",
      "category": "feature",
      "subcategory": "payload-deduplication",
      "apis": [],
      "components": [
        "history",
        "workflow-engine",
        "activity-scheduler"
      ],
      "concepts": [
        "payload-deduplication",
        "history-optimization",
        "memory-efficiency",
        "content-hashing",
        "large-payloads",
        "fan-out"
      ],
      "severity": "medium",
      "userImpact": "Users with workflows that fan out large payloads to many activities or child workflows experience excessive history size that can exceed limits.",
      "rootCause": "Temporal's current model repeats the entire payload in history for each invocation rather than deduplicating shared payloads.",
      "proposedFix": "Payload deduplication mechanism, either via special commands to save and reference payloads, or by storing large payload hashes in mutable state similar to Plan 9's Venti approach.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Recording the same payload many times is clearly nonoptimal.",
      "number": 1492,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:34:04.613Z"
    },
    {
      "summary": "Request for a default mapping between authorization package roles and API names that can be passed through CallTarget, with the flexibility for users to customize the mapping.",
      "category": "feature",
      "subcategory": "authorization",
      "apis": [
        "CallTarget"
      ],
      "components": [
        "authorizer",
        "authorization-package",
        "api-mapping"
      ],
      "concepts": [
        "role-mapping",
        "authorization",
        "api-names",
        "custom-authorizer",
        "configuration"
      ],
      "severity": "low",
      "userImpact": "Users implementing custom authorizers need to manually map authorization roles to API names, which could be simplified with a sensible default mapping.",
      "rootCause": null,
      "proposedFix": "Provide a built-in default mapping from authorization package roles to API names while allowing customization.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "In defining a custom authorizer implementation, it would be nice if there was a default mapping available of the roles found in the authorization package to API names",
      "number": 1485,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:33:59.781Z"
    },
    {
      "summary": "The temporal-sql-tool fails during schema updates when attempting to modify the `timer_tasks` table, causing the update process to fail partway through version 1.2 schema migrations.",
      "category": "bug",
      "subcategory": "schema-migration",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "schema-migration",
        "database-setup",
        "aurora-mysql"
      ],
      "concepts": [
        "schema-versioning",
        "database-migration",
        "table-creation",
        "data-persistence"
      ],
      "severity": "high",
      "userImpact": "Users cannot successfully update their Temporal database schema, blocking initial setup and schema upgrades on AWS Aurora MySQL.",
      "rootCause": "Aurora MySQL fails to create or modify the `timer_tasks` table, returning error 1146 (table doesn't exist) even though the creation statement appears to execute.",
      "proposedFix": "Potential fix mentioned in PR #2104, though specifics not detailed in the issue.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Likely resolved by PR #2104 based on comment suggesting testing against that PR.",
      "related": [
        1087,
        2104
      ],
      "keyQuote": "error executing statement:Error 1146: Table 'temporal.timer_tasks' doesn't exist",
      "number": 1477,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:33:48.024Z"
    },
    {
      "summary": "After adding a custom search attribute via tctl, workflows fail to complete due to a type conversion error in the Temporal server. The panic occurs when attempting to convert an interface value expected to be float64 but received as int, specifically in the retry policy validation code.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "history-service",
        "command-validator",
        "retry-policy"
      ],
      "concepts": [
        "search-attributes",
        "type-conversion",
        "retry-policy",
        "elasticsearch",
        "configuration"
      ],
      "severity": "critical",
      "userImpact": "Workflows cannot complete execution after adding custom search attributes, preventing normal workflow operation.",
      "rootCause": "Type mismatch in configuration parsing: retry policy coefficients are being parsed as integers but the code expects float64 values, causing a panic in FromConfigToDefaultRetrySettings",
      "proposedFix": null,
      "workaround": "Manually convert all retry policy coefficients in development_es.yaml configuration file from integers to floats",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by updating configuration to use proper float types for retry coefficients, related to PR #1464",
      "related": [
        1464
      ],
      "keyQuote": "After adding a new search attribute, workflows won't end because of a conversion error: \"interface conversion: interface {} is int, not float64\"",
      "number": 1474,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:33:48.235Z"
    },
    {
      "summary": "ActivityTaskStarted event is only added to history after activity completion to avoid polluting history on retries, which is confusing when retries are disabled (maximumAttempts=1). The request is to add ActivityTaskStarted immediately when retries are disabled, or even for the first attempt regardless of retry configuration.",
      "category": "other",
      "subcategory": "activity-history",
      "apis": [
        "RetryOptions"
      ],
      "components": [
        "history-service",
        "activity-executor",
        "workflow-history"
      ],
      "concepts": [
        "retry",
        "activity-lifecycle",
        "history-pollution",
        "event-ordering",
        "attempt-tracking"
      ],
      "severity": "medium",
      "userImpact": "Users with retry disabled find it confusing that ActivityTaskStarted doesn't appear in history immediately, making it harder to understand activity execution flow.",
      "rootCause": "ActivityTaskStarted is deferred until completion to avoid history pollution on retries, but this behavior is not intuitive when retries are disabled.",
      "proposedFix": "Add ActivityTaskStarted immediately to history when RetryOptions.maximumAttempts=1, and consider adding it for first attempt regardless of retry config with additional field to ActivityTaskFailed/TimedOut indicating retries will happen.",
      "workaround": "UI-only solution to surface pending activities as ActivityTaskStarted history events (implemented in Web PR #318).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        318
      ],
      "keyQuote": "ActivityTaskStarted event is added to the history only after an activity is completed to avoid polluting history on activity retries",
      "number": 1468,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:33:48.025Z"
    },
    {
      "summary": "Request to add ability to flush events to history for server-side activity retries, enabling detailed visibility of all retry attempts in workflow execution history for better visualization and debugging.",
      "category": "feature",
      "subcategory": "activity-retries",
      "apis": [],
      "components": [
        "history",
        "activity-executor",
        "server"
      ],
      "concepts": [
        "retry",
        "activity",
        "history",
        "visualization",
        "server-side-handling",
        "event-flushing"
      ],
      "severity": "low",
      "userImpact": "Users cannot easily visualize all activity retry attempts in workflow execution history, limiting visibility into retry behavior.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Disable retry (set maxAttempt=1) and have workflow handle retries manually.",
      "resolution": "wontfix",
      "resolutionDetails": "Won't fix; maintainer suggested disabling server-side retries and handling retries in workflow logic instead. May re-open in future if situation changes.",
      "related": [
        1468
      ],
      "keyQuote": "Currently solution is to disable retry (set maxAttempt=1) and have workflow handle retries.",
      "number": 1461,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:17:02.590Z"
    },
    {
      "summary": "Request to add task queue query capability to enable workers to respond to queries about their configuration, registered activities/workflows, and support synchronous activity execution without being tied to a specific workflow execution.",
      "category": "feature",
      "subcategory": "task-queue-query",
      "apis": [
        "RespondTaskQueueQueryCompleted"
      ],
      "components": [
        "worker",
        "task-queue",
        "query-handler"
      ],
      "concepts": [
        "task-queue",
        "worker-configuration",
        "activity-types",
        "workflow-types",
        "service-endpoint",
        "query"
      ],
      "severity": "medium",
      "userImpact": "Workers cannot currently expose information about their capabilities or configuration, limiting introspection and operational insights.",
      "rootCause": "Queries are currently workflow-execution-scoped only, with no mechanism for task-queue-level queries that workers can respond to.",
      "proposedFix": "Implement task queue query concept with new RespondTaskQueueQueryCompleted API and worker mechanism to handle such queries.",
      "workaround": "Run a per-process Nexus service endpoint as an alternative solution.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1927
      ],
      "keyQuote": "Such a query specifies a task queue name to send the query to. Then a worker picks it up and replies using a new API like RespondTaskQueueQueryCompleted.",
      "number": 1460,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:17:00.106Z"
    },
    {
      "summary": "Local activities appear in workflow history as generic MarkerRecorded events without identifying which activity was run. The feature request seeks to make the activity name visible in the event history for better debuggability.",
      "category": "feature",
      "subcategory": "local-activity-history",
      "apis": [],
      "components": [
        "local-activity",
        "workflow-history",
        "event-recording"
      ],
      "concepts": [
        "activity-visibility",
        "event-history",
        "debugging",
        "local-execution",
        "marker-events"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily identify which local activity was executed when reviewing workflow history, making debugging and auditing more difficult.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of sdk-java issue #135",
      "related": [
        135
      ],
      "keyQuote": "Running a local activity manifests itself in the workflow history as a \"MarkerRecorded\" event and does not give any detail about what local activity was run.",
      "number": 1455,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:17:01.254Z"
    },
    {
      "summary": "Matching service experiences continuous stream of ~10qps MatchingClientAddActivityTask client errors with no corresponding logs. These appear to be internal matching-to-matching service requests that are generating misleading error metrics.",
      "category": "bug",
      "subcategory": "matching-service",
      "apis": [],
      "components": [
        "matching-service",
        "metrics",
        "client-error-handling"
      ],
      "concepts": [
        "error-metrics",
        "service-communication",
        "client-errors",
        "matching-service",
        "observability"
      ],
      "severity": "medium",
      "userImpact": "Operators see continuous non-actionable error metrics from the matching service, making it difficult to distinguish real errors from internal communication artifacts.",
      "rootCause": "Potential issue with how matching service client errors are being tracked or reported in metrics, possibly related to internal matching-to-matching requests being incorrectly counted as errors.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "User reported upgrade to version 1.7.2 would resolve the issue after being informed of related metrics fixes in 1.7.x branch.",
      "related": [],
      "keyQuote": "i remember there are some metrics fix related to matching service in 1.7.x, so can you try upgrade to 1.7.2?",
      "number": 1454,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:16:44.844Z"
    },
    {
      "summary": "Custom errors returned from activities cannot be checked using `errors.Is()` because they are wrapped in multiple layers of Temporal error types. The underlying error exists in the chain but `errors.Is()` fails to match it.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "activity-executor",
        "error-wrapping",
        "test-framework"
      ],
      "concepts": [
        "error-chain",
        "error-wrapping",
        "error-matching",
        "activity-errors",
        "retry-policy",
        "test-assertions"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably check for specific custom errors returned from activities in tests or error handling code, making error classification difficult.",
      "rootCause": "Activity errors are wrapped in multiple layers (workflow error -> activity error -> wrapped error) but the error chain structure doesn't properly expose the underlying error for `errors.Is()` matching.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Issue was determined to belong to sdk-go and was duplicated to https://github.com/temporalio/sdk-go/issues/403",
      "related": [
        403
      ],
      "keyQuote": "My error message is wrapped there, but I can't compare/check it.",
      "number": 1453,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:16:47.113Z"
    },
    {
      "summary": "Temporal server hangs indefinitely during startup when using GCP Cloud SQL Proxy sidecar in Kubernetes because the startup script uses an incorrect host parameter that doesn't resolve properly with the proxy.",
      "category": "bug",
      "subcategory": "deployment-configuration",
      "apis": [],
      "components": [
        "docker-setup",
        "database-connectivity",
        "startup-script"
      ],
      "concepts": [
        "kubernetes",
        "cloud-sql-proxy",
        "network-connectivity",
        "environment-variables",
        "configuration"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal to GKE with Cloud SQL Proxy experience indefinite hangs during server startup, requiring configuration workarounds to resolve.",
      "rootCause": "The startup script checks database connectivity using 'nc -z 5432' (hostname only) instead of 'nc -z localhost:5432' or 'nc -z 127.0.0.1:5432', which doesn't work with the Cloud SQL Proxy configuration. DB_ADDRESS is used by CLI, not the server; the server uses POSTGRES_SEEDS.",
      "proposedFix": "Update startup script to use 'localhost' or '127.0.0.1' for database connectivity checks, and clarify documentation that POSTGRES_SEEDS (not DB_ADDRESS) controls server database configuration.",
      "workaround": "Set POSTGRES_SEEDS environment variable to 'localhost' instead of relying on DB_ADDRESS.",
      "resolution": "invalid",
      "resolutionDetails": "Issue was determined to be a user configuration problem - DB_ADDRESS is for CLI only, not the server. Server requires POSTGRES_SEEDS configuration. Closed as the issue was resolved through user education rather than code changes.",
      "related": [],
      "keyQuote": "Setting the postgres seed fixed this. It's confusing that DB_ADDRESS doesn't work.",
      "number": 1451,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:16:37.211Z"
    },
    {
      "summary": "FromConfigToDefaultRetrySettings function does not correctly handle the distinction between int and float64 types, causing potential panics during configuration conversion.",
      "category": "bug",
      "subcategory": "retry-policy",
      "apis": [],
      "components": [
        "config-parsing",
        "retry-settings",
        "type-conversion"
      ],
      "concepts": [
        "type-handling",
        "configuration",
        "numeric-types",
        "panic",
        "error-handling"
      ],
      "severity": "high",
      "userImpact": "Users with certain retry configuration formats may experience runtime panics when the Temporal server processes their configuration.",
      "rootCause": "The FromConfigToDefaultRetrySettings function at common/util.go:483 does not properly distinguish between int and float64 numeric types during configuration conversion.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was addressed by properly handling both int and float64 numeric types in the configuration conversion logic.",
      "related": [],
      "keyQuote": "FromConfigToDefaultRetrySettings does not handle int vs float64 correctly, which can cause panic",
      "number": 1444,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:15:32.070Z"
    },
    {
      "summary": "Custom search attributes cannot be searched in Elasticsearch because the visibility index mapping has dynamic set to false, preventing new attributes from being indexed. This blocks users from using the documented workflow to add custom searchable attributes.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "elasticsearch",
        "visibility-index",
        "search-attributes",
        "dynamic-config"
      ],
      "concepts": [
        "elasticsearch-mapping",
        "search-indexing",
        "custom-attributes",
        "dynamic-fields",
        "schema-management"
      ],
      "severity": "high",
      "userImpact": "Users cannot add custom search attributes as documented because newly added attributes are not indexed and therefore not searchable in the Advanced search interface.",
      "rootCause": "The Elasticsearch visibility index mapping has dynamic set to false at the root level, which prevents dynamic field addition to the Attr object even though the documentation suggests this should be supported.",
      "proposedFix": "Either set dynamic to true on the Attr object specifically, or include all dynamic search attribute definitions in the initial mapping schema.",
      "workaround": "Manually update the mapping with PUT /temporal-visibility-{namespace}/_mapping to set dynamic: true on the Attr object.",
      "resolution": "fixed",
      "resolutionDetails": "Starting from v1.10.0, dynamic config is no longer used for custom search attributes. The tctl admin cluster add-search-attributes command now updates both internal metadata and Elasticsearch schema directly, with dynamic always set to false to control index field count.",
      "related": [],
      "keyQuote": "The elasticsearch mapping has dynamic: false which means any added attributes are not indexed, and therefore not searchable in the Advanced search.",
      "number": 1442,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:15:36.216Z"
    },
    {
      "summary": "The temporal-sql-tool setup-schema command fails when the schema already exists, but users need an idempotent operation for k8s deployments that can safely run repeatedly to create or update the schema.",
      "category": "feature",
      "subcategory": "schema-management",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "schema-setup",
        "database-initialization"
      ],
      "concepts": [
        "idempotency",
        "schema-versioning",
        "kubernetes-deployment",
        "database-setup",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot safely run schema setup in automated deployment pipelines without manual intervention or error handling.",
      "rootCause": "The temporal-sql-tool does not check for existing schema before attempting to create it, causing an error when idempotent operations are needed.",
      "proposedFix": "Make the CLI tool exit gracefully if the table already exists instead of raising an error.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed as duplicate of issue #1087 which addresses the same idempotency concern.",
      "related": [
        1087
      ],
      "keyQuote": "if the schema already exists, this command will raise an error... It would be awesome if instead the CLI tool would exit gracefully",
      "number": 1433,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:15:33.743Z"
    },
    {
      "summary": "Rolling bounces and upgrades of Temporal generate excessive error messages related to shard stealing and state updates, making it difficult to identify real issues during deployments. The solution should be compatible with Kubernetes' default RollingUpdate strategy.",
      "category": "feature",
      "subcategory": "deployment-operations",
      "apis": [],
      "components": [
        "shard-management",
        "state-updates",
        "rebalancing",
        "error-logging"
      ],
      "concepts": [
        "rolling-deployment",
        "shard-stealing",
        "error-suppression",
        "graceful-shutdown",
        "kubernetes-compatibility",
        "monitoring",
        "deployment-noise"
      ],
      "severity": "high",
      "userImpact": "Operators experience noisy logs during routine deployments that obscure real issues, making it difficult to distinguish deployment errors from operational problems.",
      "rootCause": "During rolling bounces/upgrades, shard rebalancing causes predictable transient errors (shard stealing, ack level updates, queue state updates) that are not indicative of actual problems.",
      "proposedFix": "Implement throttling of expected transient errors during ring changes or shard rebalancing, either by: (1) suppressing/downgrading messages to warnings with a counter, or (2) detecting rebalance events and temporarily reducing error log levels.",
      "workaround": "Manual monitoring systems that ignore a baseline level of these errors during deployments based on historical deployment patterns.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Ideally, we should be able to do a rolling bounce or rolling upgrade of Temporal without encountering these errors on a routine basis.",
      "number": 1428,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:15:16.705Z"
    },
    {
      "summary": "Request to enhance the gRPC protocol to allow servers to track client language, version, and namespace information. This would enable operators to identify and manage client versions with known issues and restrict access if needed.",
      "category": "feature",
      "subcategory": "observability-client-tracking",
      "apis": [],
      "components": [
        "frontend",
        "gRPC-protocol",
        "interceptor",
        "client-metadata"
      ],
      "concepts": [
        "client-tracking",
        "version-management",
        "namespace-isolation",
        "client-identification",
        "security",
        "operator-tooling"
      ],
      "severity": "medium",
      "userImpact": "Operators gain the ability to track and manage clients by language and version across namespaces, enabling better control over problematic client versions.",
      "rootCause": null,
      "proposedFix": "Enhance the frontend gRPC protocol to collect and expose client metadata (language, version, IP, identifier) tracked per namespace.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The functionality was already implemented in the codebase via the sdk_version interceptor, making this a duplicate or already-resolved request.",
      "related": [],
      "keyQuote": "This information should be collected in such a way that it is relatively easy for operators to track down all users of version X of source language client Y",
      "number": 1427,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:15:19.483Z"
    },
    {
      "summary": "User requests support for cron schedules faster than one minute per workflow, such as every 15 seconds. The standard cron format only supports minute-level precision.",
      "category": "feature",
      "subcategory": "workflow-scheduling",
      "apis": [
        "StartWorkflowOptions"
      ],
      "components": [
        "workflow-scheduler",
        "cron-scheduler"
      ],
      "concepts": [
        "cron-schedule",
        "timing-precision",
        "workflow-execution",
        "scheduling"
      ],
      "severity": "low",
      "userImpact": "Users who need sub-minute scheduling intervals for workflows are unable to achieve this with standard cron syntax.",
      "rootCause": null,
      "proposedFix": "Extend cron format to support seconds using 6 tokens instead of 5 (e.g., '*/15 * * * * *' for every 15 seconds)",
      "workaround": "Use '@every 30s' syntax as an alternative to cron schedules",
      "resolution": "fixed",
      "resolutionDetails": "User discovered that the '@every' syntax already provides sub-minute scheduling capability as a workaround",
      "related": [],
      "keyQuote": "try using `@every 30s`",
      "number": 1426,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:15:16.919Z"
    },
    {
      "summary": "GCP's log collection treats stderr as errors and stdout as non-errors, causing Temporal's logs to be misclassified regardless of actual log level. Users need configurable output streams to properly separate error and non-error logs.",
      "category": "feature",
      "subcategory": "logging-configuration",
      "apis": [],
      "components": [
        "logger",
        "log-config",
        "zap-logger"
      ],
      "concepts": [
        "logging",
        "output-streams",
        "stderr",
        "stdout",
        "log-levels",
        "cloud-platform-integration"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal on GCP cannot properly segregate error and non-error logs due to mixed output streams, affecting observability and alerting.",
      "rootCause": "Temporal outputs all logs (regardless of level) to the same stream, which cloud platforms like GCP interpret based on the stream rather than the log level.",
      "proposedFix": "Add OutputPaths and ErrorOutputPaths configuration fields to the logger config, similar to Zap's configuration options, while maintaining backward compatibility with existing Stdout and OutputFile fields.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Google Cloud Platform collects logs output to stderr as error and those output to stdout as non-error...everything gets mixed up.",
      "number": 1423,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:15:03.451Z"
    },
    {
      "summary": "Feature request to add a soft workflow timeout mechanism that sends cancellation requests instead of hard termination, allowing workflows to perform cleanup operations. Currently RunTimeout terminates workflows abruptly, forcing users to implement custom timer-based business logic for cleanup.",
      "category": "feature",
      "subcategory": "workflow-timeout",
      "apis": [],
      "components": [
        "workflow-runtime",
        "timeout-handler",
        "cancellation-handler"
      ],
      "concepts": [
        "timeout",
        "cancellation",
        "cleanup",
        "graceful-shutdown",
        "workflow-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Users currently cannot perform cleanup on workflow timeouts without complex custom timer logic, requiring workarounds to ensure proper resource cleanup.",
      "rootCause": null,
      "proposedFix": "Add WorkflowRunCancellationTimeout that sends a cancellation request to the workflow when exceeded instead of immediate termination",
      "workaround": "Implement custom timer-based business logic within workflows for cleanup operations",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add `WorkflowRunCancellationTimeout` (name is strawman) wich sends a cancellation request to the workflow when exceeded",
      "number": 1412,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:59.930Z"
    },
    {
      "summary": "User reports excessive database queries (3000/minute) causing high load and requests configuration options to reduce query intervals. The issue appears to be related to Temporal's task scanning behavior.",
      "category": "question",
      "subcategory": "database-performance",
      "apis": [],
      "components": [
        "database",
        "task-scanner",
        "persistence"
      ],
      "concepts": [
        "query-frequency",
        "database-load",
        "performance-tuning",
        "configuration",
        "task-polling"
      ],
      "severity": "medium",
      "userImpact": "Excessive database queries create performance bottlenecks and high load on user databases.",
      "rootCause": "Temporal's task scanning mechanism queries the database at a fixed interval, causing high query volume under certain workloads.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "stale",
      "resolutionDetails": "Issue closed with close-after-30-days label, suggesting it was closed due to inactivity rather than resolution.",
      "related": [],
      "keyQuote": "Temporal fires 3000 queries in a single minute? Is there a way to reduce db queries? It causes too much load on my db.",
      "number": 1406,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:15:01.868Z"
    },
    {
      "summary": "tctl admin workflow show command currently filters history events using transaction ID logic, hiding uncommitted events. The request is to dump all history events without any filtering.",
      "category": "feature",
      "subcategory": "cli-tools",
      "apis": [],
      "components": [
        "tctl",
        "admin-command",
        "workflow-history"
      ],
      "concepts": [
        "history-events",
        "transaction-filtering",
        "uncommitted-events",
        "diagnostic-tools"
      ],
      "severity": "medium",
      "userImpact": "Users cannot inspect uncommitted workflow history events when debugging workflows using tctl, limiting diagnostic capabilities.",
      "rootCause": "tctl admin workflow show applies transaction ID filtering logic that was designed for other use cases, preventing visibility into all history events.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "tctl admin workflow show should dump all history events without filtering",
      "number": 1403,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:43.209Z"
    },
    {
      "summary": "Optimize mutable state transaction handling for workflow execution by eliminating unnecessary database updates within the same batch. Currently, activity updates are performed even when they will be immediately deleted in the same transaction, causing inefficient queries.",
      "category": "feature",
      "subcategory": "mutable-state-optimization",
      "apis": [],
      "components": [
        "mutable-state",
        "transaction-handler",
        "activity-executor",
        "workflow-execution"
      ],
      "concepts": [
        "batch-optimization",
        "database-efficiency",
        "transaction-batching",
        "state-mutation",
        "query-elimination",
        "performance"
      ],
      "severity": "medium",
      "userImpact": "Users experience slower workflow execution and database operations due to unnecessary state updates and queries being performed in the same batch transaction.",
      "rootCause": "Post mutable state transaction optimization is insufficient - activity task updates are performed even when the activity will be deleted within the same batch update.",
      "proposedFix": "Implement post mutable state transaction optimization to eliminate unnecessary queries and state updates for activities that will be deleted in the same batch.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the update of target activity to mutable state can be omitted since within the same batch the target activity will be deleted",
      "number": 1399,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:46.794Z"
    },
    {
      "summary": "temporal-sql-tool hardcodes 'postgres' as the admin database name, preventing use with PostgreSQL providers like Aiven that use a different default database. Users cannot override this via command-line flags.",
      "category": "bug",
      "subcategory": "database-setup",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "postgresql-plugin",
        "database-initialization"
      ],
      "concepts": [
        "database-connection",
        "admin-database",
        "postgresql-configuration",
        "deployment-setup",
        "cloud-provider-compatibility"
      ],
      "severity": "medium",
      "userImpact": "Users with PostgreSQL instances that don't have a 'postgres' default database (e.g., Aiven) cannot initialize Temporal databases using temporal-sql-tool.",
      "rootCause": "The PostgreSQL plugin hardcodes 'postgres' as the admin database name in the connection logic, and the --database flag does not override this value.",
      "proposedFix": "Add a configuration option to temporal-sql-tool to allow users to specify the admin database name when connecting to PostgreSQL, or try a set of default database names as suggested in issue #1413.",
      "workaround": "Modify and rebuild temporal-sql-tool from source, changing the hardcoded 'postgres' database name to the desired default.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainers decided not to add special behavior for PostgreSQL in temporal-sql-tool. Issue #1413 was opened instead to try multiple default database names.",
      "related": [
        1413
      ],
      "keyQuote": "Aiven clusters have an admin db named `defaultdb`. We tried setting the database name but it seems to be ignored",
      "number": 1389,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:45.896Z"
    },
    {
      "summary": "Docker configuration template should support different database addresses for the main database and visibility database, as well as separate credentials for each.",
      "category": "feature",
      "subcategory": "docker-config",
      "apis": [],
      "components": [
        "docker",
        "config-template",
        "database-configuration"
      ],
      "concepts": [
        "database-separation",
        "configuration-flexibility",
        "visibility-store",
        "multi-database-setup"
      ],
      "severity": "low",
      "userImpact": "Users running Temporal in Docker cannot configure separate database instances for main and visibility databases, limiting deployment flexibility.",
      "rootCause": null,
      "proposedFix": "Modify docker/config_template.yaml to allow independent database address, username, and password configuration for main and visibility databases.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Configuration template was updated to support different database addresses and credentials for main and visibility stores.",
      "related": [],
      "keyQuote": "Docker config template should allow different DB address for main and visibility database",
      "number": 1388,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:29.215Z"
    },
    {
      "summary": "The tctl admin workflow delete command only removes workflow records from the execution database but not from the visibility storage. This inconsistency means deleted workflows may still appear in visibility queries and leave orphaned records.",
      "category": "bug",
      "subcategory": "visibility-storage",
      "apis": [],
      "components": [
        "tctl",
        "visibility-store",
        "execution-db"
      ],
      "concepts": [
        "data-consistency",
        "cleanup",
        "orphaned-records",
        "visibility-queries",
        "admin-operations",
        "database-synchronization"
      ],
      "severity": "medium",
      "userImpact": "Users performing administrative workflow deletions experience inconsistent state where deleted workflows remain visible in queries and searches.",
      "rootCause": "The tctl admin workflow delete implementation does not issue corresponding delete operations to the visibility storage system.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "the tctl admin workflow delete should also delete corresponding record from visibility",
      "number": 1378,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:27.014Z"
    },
    {
      "summary": "User requests exposing the dyamicconfig.Client as a ServerOption extension point to allow direct integration with custom dynamic config services instead of using a filesystem-based sidecar approach.",
      "category": "feature",
      "subcategory": "dynamic-config",
      "apis": [],
      "components": [
        "dynamic-config",
        "server-options",
        "configuration"
      ],
      "concepts": [
        "extension-point",
        "integration",
        "configuration-service",
        "sidecar",
        "customization"
      ],
      "severity": "low",
      "userImpact": "Users can integrate their own dynamic config services directly with Temporal Server instead of building a sidecar.",
      "rootCause": null,
      "proposedFix": "Expose Client from dyamicconfig package as a ServerOption extension point",
      "workaround": "Write a sidecar that integrates with dynamic config service and writes to filesystem",
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It looks like everything is in place for the Client to be provided by end-users, but just has not been exposed as a ServerOption.",
      "number": 1376,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:29.501Z"
    },
    {
      "summary": "temporal-sql-tool returns exit code 0 even when given invalid arguments, making it impossible to detect failures in wrapper scripts. The tool should return a non-zero exit status on argument validation errors.",
      "category": "bug",
      "subcategory": "cli-tool",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "cli",
        "argument-parsing"
      ],
      "concepts": [
        "exit-code",
        "error-handling",
        "scripting",
        "integration",
        "validation",
        "devops"
      ],
      "severity": "medium",
      "userImpact": "Wrapper scripts cannot detect failures when temporal-sql-tool receives bad arguments, breaking error handling in automation.",
      "rootCause": "The temporal-sql-tool does not properly set exit code to non-zero when argument validation fails.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in PR #5204 to return non-zero exit status on invalid arguments.",
      "related": [
        5204
      ],
      "keyQuote": "Bad arguments to temporal-sql-tool should return non-zero status... This makes it impossible to integrate into a wrapping script that needs to react to whether it succeeds or fails.",
      "number": 1370,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:13.846Z"
    },
    {
      "summary": "Temporal crashes with 'unknown protocol version 80877102' error when using CockroachDB as backend with Archival enabled. The issue appears related to CockroachDB's incomplete PostgreSQL protocol compatibility, specifically around cancel query operations.",
      "category": "bug",
      "subcategory": "database-compatibility",
      "apis": [],
      "components": [
        "persistence",
        "archival",
        "database-driver"
      ],
      "concepts": [
        "database-compatibility",
        "postgresql-protocol",
        "cockroachdb",
        "archival",
        "connection-handling"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal on CockroachDB with Archival enabled encounter crashes that prevent the system from running.",
      "rootCause": "CockroachDB claims PostgreSQL 9.5 wire compatibility but does not fully support the PostgreSQL protocol, particularly around cancel query operations used by Temporal's archival feature.",
      "proposedFix": null,
      "workaround": "Disable Archival feature when using CockroachDB, or switch to PostgreSQL backend.",
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed as CockroachDB support is not officially prioritized due to lack of CICD setup, limited user demand, and archival being experimental. PostgreSQL 9.6+ is the officially supported database.",
      "related": [],
      "keyQuote": "CockroachDB claims to be wire compatible with PostgreSQL 9.5... the only reason that CockroachDB is not officially supported is we do not have CICD setup for CockroachDB.",
      "number": 1369,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:14.465Z"
    },
    {
      "summary": "Add Workflow and Signal information to the Authorizer's CallTarget struct to enable fine-grained authorization based on workflow type, task queue, and signal names in addition to namespace and API name.",
      "category": "feature",
      "subcategory": "authorization",
      "apis": [],
      "components": [
        "authorizer",
        "call-target",
        "api"
      ],
      "concepts": [
        "authorization",
        "access-control",
        "granularity",
        "workflow-execution",
        "signal-handling"
      ],
      "severity": "medium",
      "userImpact": "Users can now implement more specific authorization policies that control access to particular workflow types and signals rather than just entire namespaces.",
      "rootCause": null,
      "proposedFix": "Add fields to CallTarget struct to include workflow type, task queue, signal name, and arguments for both workflow execution and signal operations.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented in PR #1432 by adding workflow and signal information fields to the CallTarget struct.",
      "related": [
        1432
      ],
      "keyQuote": "they may execute _this_ workflow type with _these_ arguments on _this_ task queue",
      "number": 1368,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:14.578Z"
    },
    {
      "summary": "SQL client used by Temporal is incompatible with MySQL 8+ when multiStatements flag is enabled. A user reported issues connecting to a Vitess cluster (MySQL 8.0) and found that disabling multiStatements support in the MySQL driver plugin resolves the problem.",
      "category": "bug",
      "subcategory": "mysql-compatibility",
      "apis": [],
      "components": [
        "sql-persistence",
        "mysql-plugin",
        "database-driver"
      ],
      "concepts": [
        "mysql-compatibility",
        "database-connection",
        "vitess-cluster",
        "connection-flags",
        "driver-configuration"
      ],
      "severity": "high",
      "userImpact": "Users cannot use Temporal with MySQL 8+ Vitess clusters when multiStatements flag is enabled, forcing them to either use older MySQL versions or disable the feature.",
      "rootCause": "The SQL client driver used in the mysql plugin has incompatibility with MySQL 8+ when multiStatements flag is enabled in the connection configuration.",
      "proposedFix": "Investigate and update the MySQL driver or its configuration to support multiStatements flag with MySQL 8+ versions, or provide a configuration option to disable multiStatements.",
      "workaround": "Comment out or disable the multiStatements flag in the MySQL plugin configuration (common/persistence/sql/sqlplugin/mysql/plugin.go line 56).",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by updating the MySQL driver dependency or configuration to support MySQL 8+ with multiStatements enabled.",
      "related": [],
      "keyQuote": "the SQL client Temporal is using is not compatible with our vitess cluster when have 'multiStatements' flag enabled",
      "number": 1364,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:59.576Z"
    },
    {
      "summary": "Workflow panics with a state machine error when a cancel request occurs during a workflow sleep with a cleanup activity deferred. The error indicates a lookup failure for scheduledEventID to activityID mapping, causing the workflow to remain stuck in Running state instead of transitioning to Canceled.",
      "category": "bug",
      "subcategory": "workflow-cancellation",
      "apis": [
        "ExecuteActivity",
        "Sleep",
        "NewDisconnectedContext"
      ],
      "components": [
        "workflow-execution",
        "decision-state-machine",
        "event-handler",
        "activity-scheduler"
      ],
      "concepts": [
        "cancellation",
        "cleanup",
        "state-machine",
        "event-processing",
        "deferred-activity",
        "timer-cancellation"
      ],
      "severity": "high",
      "userImpact": "Workflows panic and hang in Running state when cancelled during sleep with a deferred cleanup activity, requiring manual intervention to resolve.",
      "rootCause": "Event handler fails to map scheduledEventID to activityID when processing activity task scheduled events during concurrent cancellation, causing illegal state panic in decision state machine.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Author closed as duplicate and resubmitted in the correct sdk-go repository.",
      "related": [],
      "keyQuote": "lookup failed for scheduledEventID to activityID: scheduleEventID: 11, activityID: 10",
      "number": 1359,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:14:01.004Z"
    },
    {
      "summary": "Feature request to allow resetting workflows to the workflow task scheduled event. Requires refactoring the history builder to support multiple history batches and modifying the workflow reset logic.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "history-builder",
        "workflow-reset",
        "event-replay"
      ],
      "concepts": [
        "reset",
        "workflow-task",
        "history-batch",
        "event-sequence",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Users can reset workflows to a more granular state (workflow task scheduled) for better control over workflow recovery.",
      "rootCause": null,
      "proposedFix": "Refactor history builder to allow multiple history batches and modify workflow reset logic to support resetting to workflow task scheduled event.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The feature was implemented with refactoring of history builder and workflow reset logic to support multiple history batches.",
      "related": [],
      "keyQuote": "Allow reset to workflow task scheduled event",
      "number": 1358,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:59.784Z"
    },
    {
      "summary": "GetReplicationTasks QPS is unexpectedly high (5000 QPS for 16k shards) on newly provisioned XDC clusters with no connected workers, suggesting unnecessary database calls in the replication mechanism.",
      "category": "bug",
      "subcategory": "xdc-replication",
      "apis": [],
      "components": [
        "history-service",
        "replication",
        "persistence"
      ],
      "concepts": [
        "cross-datacenter-replication",
        "qps",
        "database-optimization",
        "baseline-performance",
        "sharding"
      ],
      "severity": "medium",
      "userImpact": "Users with XDC deployments experience higher than expected database load and operational costs even when no workflows are running.",
      "rootCause": "Unnecessary GetReplicationTasks database calls in the replication mechanism, even when there are no connected clients or activities.",
      "proposedFix": "Optimize the replication mechanism to eliminate unnecessary DB calls when there are no connected workers.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was acknowledged as a performance optimization opportunity for the replication mechanism.",
      "related": [],
      "keyQuote": "there should be some optimization which eliminates some unnecessary DB calls",
      "number": 1354,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:44.757Z"
    },
    {
      "summary": "Workflow execution complexity metrics: request to report the number of state transitions used by a workflow to the Temporal server, which helps evaluate the load burden on the server and is included in visibility records on close.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "history-service",
        "workflow-mutable-state",
        "advanced-visibility"
      ],
      "concepts": [
        "state-transitions",
        "complexity-metrics",
        "server-load",
        "visibility-records",
        "workflow-execution",
        "performance-monitoring"
      ],
      "severity": "low",
      "userImpact": "Allows users to track workflow complexity metrics and understand the load burden their workflows place on the Temporal server.",
      "rootCause": null,
      "proposedFix": "Add state transition counting feature to the history service that reports the number of state transitions used by each workflow and includes it in the visibility record on close.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The field was added to workflow mutable state and also to advanced visibility records.",
      "related": [],
      "keyQuote": "Temporal server should add this counting feature to the history service. So every workflow should return the number of state transitions it used and include it into the visibility record on close.",
      "number": 1352,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:46.878Z"
    },
    {
      "summary": "Clicking on a runId link from the event history in the Temporal Web UI incorrectly navigates to the list executions view instead of the execution details page for that specific run.",
      "category": "bug",
      "subcategory": "web-ui-navigation",
      "apis": [],
      "components": [
        "web-ui",
        "event-history",
        "workflow-execution-links"
      ],
      "concepts": [
        "navigation",
        "execution-details",
        "runId-linking",
        "ui-routing"
      ],
      "severity": "medium",
      "userImpact": "Users cannot navigate to execution details by clicking runId links in the event history, requiring manual navigation instead.",
      "rootCause": "Event history API (getWorkflowExecutionHistoryAsync) returns child workflow events with blank namespace field, preventing proper construction of execution detail links.",
      "proposedFix": "Fix the API to include namespace in child workflow execution completed events so the web UI can construct proper execution detail links.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "PR #1350 merged to fix the issue by ensuring namespace is properly populated in event history responses.",
      "related": [
        1350
      ],
      "keyQuote": "To generate the workflow/summary link, you need the namespace, workflow id, and run id. The getWorkflowExecutionHistoryAsync event is returning events with blank namespaces.",
      "number": 1351,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:45.685Z"
    },
    {
      "summary": "Ensure activity start and last timeout/fail events are flushed when a workflow finishes, to prevent loss of critical activity state information.",
      "category": "feature",
      "subcategory": "activity-events",
      "apis": [],
      "components": [
        "workflow-execution",
        "activity-executor",
        "event-handling"
      ],
      "concepts": [
        "event-flushing",
        "activity-lifecycle",
        "timeout-handling",
        "workflow-completion",
        "state-consistency"
      ],
      "severity": "medium",
      "userImpact": "Without proper flushing, users may lose visibility into activity state changes at workflow completion.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed in favor of issue #503, which addresses the same concern.",
      "related": [
        503
      ],
      "keyQuote": "close in favor of #503",
      "number": 1346,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:28.484Z"
    },
    {
      "summary": "Make it easier to export workflow history in JSON format from tctl by adding an explicit --output_format flag instead of relying on undiscovered --output_filename parameter. The current approach is not user-discoverable and lacks support for human-readable formatting of base64-encoded payload data.",
      "category": "feature",
      "subcategory": "tctl-cli",
      "apis": [],
      "components": [
        "tctl",
        "workflow-history",
        "json-export"
      ],
      "concepts": [
        "output-format",
        "user-discoverability",
        "json-serialization",
        "payload-encoding",
        "cli-usability"
      ],
      "severity": "low",
      "userImpact": "Users struggle to discover and use JSON export functionality for workflow history, making it difficult to analyze workflows and use tctl in containerized environments.",
      "rootCause": "The --output_filename parameter is not documented in tctl help, and exported JSON contains base64-encoded payload data that is not human-readable.",
      "proposedFix": "Replace --print_full flag with --output_format flag supporting values like 'short', 'full', 'json', and potentially add decoding of base64-encoded payload data for human readability.",
      "workaround": "Use --output_filename parameter with workflow show command to save history as JSON file.",
      "resolution": "fixed",
      "resolutionDetails": "A unified --output flag was added in temporalio/proposals#31 and implemented in tctl repo to apply consistently across all commands that print golang structs.",
      "related": [],
      "keyQuote": "Explicit flag that specifies output format. I would replace `--print_full` flag with `--outuput_format` that can be `short`, `full`, `json`, etc.",
      "number": 1339,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:32.954Z"
    },
    {
      "summary": "The ResetWorkflowExecution gRPC API currently requires clients to specify a workflow_task_finish_event_id, forcing SDKs and tools to implement their own logic for determining the reset point. This issue requests moving the reset point determination logic to the service by adding a reset_type argument to support more flexible reset strategies.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [
        "ResetWorkflowExecution"
      ],
      "components": [
        "workflow-reset",
        "history-service",
        "tctl",
        "grpc-api"
      ],
      "concepts": [
        "reset-point",
        "workflow-task",
        "history-branch",
        "event-id",
        "reset-type",
        "auto-selection"
      ],
      "severity": "medium",
      "userImpact": "Users must currently inspect workflow history and manually find internal event IDs to reset workflows, making the reset operation brittle and unintuitive across SDKs.",
      "rootCause": "Reset point determination logic is implemented only in tctl, not exposed in the gRPC service API, preventing reuse by SDKs and making resets require low-level event ID knowledge.",
      "proposedFix": "Add reset_type argument to ResetWorkflowExecutionRequest and implement auto-selection logic in the service that finds the last WORKFLOW_TASK_COMPLETED event when WorkflowTaskFinishEventId is omitted.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Move logic of finding reset point to the service by adding reset_type argument to ResetWorkflowExecutionRequest.",
      "number": 1338,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:29.635Z"
    },
    {
      "summary": "Namespace registration in Temporal is asynchronous, causing a 10-15 second delay before a newly registered namespace becomes available for use. Users currently must implement workarounds to detect when a namespace is ready, which is unreliable and impacts developer experience.",
      "category": "feature",
      "subcategory": "namespace-management",
      "apis": [
        "RegisterNamespace",
        "DescribeNamespace",
        "ListNamespaces",
        "GetWorkflowExecutionHistory"
      ],
      "components": [
        "namespace-manager",
        "namespace-cache",
        "grpc-service"
      ],
      "concepts": [
        "namespace-registration",
        "synchronous-operations",
        "propagation-delay",
        "developer-experience",
        "initialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably use newly registered namespaces immediately, requiring error-prone workarounds to detect namespace availability.",
      "rootCause": "Namespace registration is asynchronous with a propagation delay of 10-15 seconds before the namespace becomes available across the system.",
      "proposedFix": "Make namespace registration synchronous so RegisterNamespace returns only after the namespace is immediately available, or reduce the delay to at most one second in locally deployed versions.",
      "workaround": "Call GetWorkflowExecutionHistory with a fake workflow ID and check the error status code to detect when namespace is propagated.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by implementing synchronous namespace registration or reducing propagation delay.",
      "related": [],
      "keyQuote": "Namespace registration is synchronous. I.e. after the RegisterNamespace returned then it can be used without errors.",
      "number": 1336,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:16.184Z"
    },
    {
      "summary": "Request to define searchAttributes on a per-namespace basis to avoid collisions between namespaces. Currently search attributes are scoped at the cluster level with no plans for namespace-level configuration.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "search-attributes",
        "namespace",
        "cluster"
      ],
      "concepts": [
        "namespace-isolation",
        "search-attributes",
        "collision-prevention",
        "cluster-scope",
        "configuration"
      ],
      "severity": "low",
      "userImpact": "Users operating multiple namespaces must prefix search attributes to avoid collisions, adding complexity to their setup.",
      "rootCause": "Search attributes are currently scoped at cluster level rather than namespace level.",
      "proposedFix": "Implement namespace-level search attribute definitions.",
      "workaround": "Prefix each searchAttribute with a per-namespace string.",
      "resolution": "wontfix",
      "resolutionDetails": "Search attributes are scoped at cluster level. The team decided there are no plans for namespace level search attributes.",
      "related": [],
      "keyQuote": "Search attributes are scoped at cluster level. Currently there are no plans for namespace level search attributes.",
      "number": 1332,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:13.596Z"
    },
    {
      "summary": "Organization needs to customize Zap logger configuration for their specific JSON log format requirements and additional values. A request to make Zaplogger configurable through server options was implemented by allowing custom logger instances via WithLogger() server option.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "server",
        "logger",
        "zap-logger"
      ],
      "concepts": [
        "logging",
        "configuration",
        "customization",
        "log-format",
        "dependency-injection"
      ],
      "severity": "low",
      "userImpact": "Users can now provide custom logger implementations to match their organization's specific logging format and requirements instead of being locked into the default Zaplogger configuration.",
      "rootCause": null,
      "proposedFix": "Add temporal.WithLogger() server option that accepts a log.Logger interface implementation, allowing custom logger instances and tag implementations to be supplied.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented WithLogger() server option accepting log.Logger interface with Tag support, enabling custom Zap logger instances and alternative logging implementations.",
      "related": [],
      "keyQuote": "It will be great if there is an option to customize Zaplogger through server options.",
      "number": 1329,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:13:12.948Z"
    },
    {
      "summary": "S3 archival integration fails with 'only operation = is support for CloseTime' error when querying visibility or history data through archival providers. The error occurs regardless of whether enableRead is enabled in the S3 configuration.",
      "category": "bug",
      "subcategory": "archival-s3",
      "apis": [],
      "components": [
        "archival",
        "s3store",
        "visibility",
        "history"
      ],
      "concepts": [
        "archival",
        "s3-integration",
        "query-operation",
        "visibility-archival",
        "history-archival"
      ],
      "severity": "high",
      "userImpact": "Users cannot use S3 archival for history and visibility storage, blocking archival functionality in production deployments.",
      "rootCause": "S3 archival provider does not support CloseTime filtering operations in query requests, restricting supported query operations.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "unknown",
      "resolutionDetails": null,
      "related": [
        3544,
        273
      ],
      "keyQuote": "only operation = is support for CloseTime",
      "number": 1314,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:56.516Z"
    },
    {
      "summary": "The workflow state (created, executing, completed, etc.) is not exposed via the Temporal API, making it impossible for the web UI or tctl to distinguish between workflows that are running versus those created but not yet started (e.g., cron workflows waiting for their scheduled time). This feature request asks to expose the internal state field through the APIs.",
      "category": "feature",
      "subcategory": "api-exposure",
      "apis": [],
      "components": [
        "web-ui",
        "tctl",
        "api-server",
        "persistence"
      ],
      "concepts": [
        "workflow-state",
        "visibility",
        "scheduling",
        "cron-workflows",
        "api-design",
        "developer-experience"
      ],
      "severity": "medium",
      "userImpact": "Developers cannot determine the true state of workflows without clicking through the UI, making it difficult to distinguish between running and scheduled (created but not yet due) workflows.",
      "rootCause": "The workflow state field exists in the persistence layer but is not exposed through the Temporal APIs (gRPC or HTTP).",
      "proposedFix": "Expose the workflow state field through the Temporal APIs so it is accessible to both web UI and tctl clients.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal knows that the workflow is created but not yet started, the persistence record has \"created\" as the state. The state field isn't currently exposed via the Temporal APIs at all.",
      "number": 1311,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:59.724Z"
    },
    {
      "summary": "Temporal incorrectly responds to queries for cron-scheduled workflows that have been created but not yet started, sending query requests to workers when the workflow hasn't begun execution. This causes issues in SDKs like Ruby that expect a workflow task in the history.",
      "category": "bug",
      "subcategory": "cron-scheduling",
      "apis": [],
      "components": [
        "query-handler",
        "cron-scheduler",
        "workflow-execution"
      ],
      "concepts": [
        "cron-scheduling",
        "query-dispatch",
        "workflow-lifecycle",
        "history",
        "timing"
      ],
      "severity": "high",
      "userImpact": "Developers using cron-scheduled workflows cannot query workflows before they start, and workers may crash when handling premature query requests.",
      "rootCause": "Temporal sends queries to workflows created by cron schedules even when the workflow hasn't reached its scheduled start time, resulting in incomplete history with only WorkflowExecutionStarted event.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #2826",
      "related": [
        2826
      ],
      "keyQuote": "Temporal responds to poll requests with the query set and a history with only one event, WorkflowExecutionStarted.",
      "number": 1310,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:55.401Z"
    },
    {
      "summary": "Temporal Server/Cassandra tools discover and connect to all Cassandra nodes returned by seed nodes, but some discovered nodes may be unreachable due to network policies in multi-datacenter setups. Users need the ability to restrict connections to only specified seed nodes or nodes in a specific datacenter to comply with regulatory requirements.",
      "category": "bug",
      "subcategory": "cassandra-configuration",
      "apis": [],
      "components": [
        "cassandra-client",
        "gocql-integration",
        "cqlclient"
      ],
      "concepts": [
        "multi-datacenter",
        "network-connectivity",
        "service-discovery",
        "configuration",
        "cassandra-seeds",
        "host-filtering"
      ],
      "severity": "high",
      "userImpact": "Users in regulated multi-datacenter environments cannot use Temporal with managed Cassandra services due to unwanted cross-datacenter connection attempts.",
      "rootCause": "The cqlclient does not expose the Datacenter filtering knob available in the cassandraCluster configuration, causing the gocql library to attempt connections to all discovered nodes regardless of datacenter restrictions.",
      "proposedFix": "Expose the Datacenter configuration parameter in cqlclient to allow filtering of Cassandra hosts by datacenter, combining both cassandra-side topology awareness and gocql-side host filtering.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "PR #1315 fixed the issue by exposing datacenter filtering configuration in cqlclient",
      "related": [
        1315,
        1278
      ],
      "keyQuote": "can we tell Temporal-server/Cassandra to only connect to the nodes specified in CASSANDRA_SEEDS as we can populate this variable with Cassandra nodes only in the current DC?",
      "number": 1309,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:42.992Z"
    },
    {
      "summary": "Build failure in temporal v1.5.1 due to internal package import restriction in kafka-client dependency. User attempted to build from source but encountered a Go compiler error related to vendor package access.",
      "category": "bug",
      "subcategory": "build",
      "apis": [],
      "components": [
        "build-system",
        "kafka-client",
        "vendor-dependencies"
      ],
      "concepts": [
        "build-failure",
        "dependency-management",
        "internal-packages",
        "version-compatibility"
      ],
      "severity": "low",
      "userImpact": "Users attempting to build older versions of Temporal from source may encounter build failures due to dependency issues.",
      "rootCause": "The kafka-client vendor package contains an internal package that cannot be imported directly by the Go compiler, blocking the build process.",
      "proposedFix": null,
      "workaround": "Upgrade to a newer version of Temporal (1.6.4 or 1.7.0 as suggested in comments).",
      "resolution": "wontfix",
      "resolutionDetails": "Issue was closed as wontfix because users are advised to use the latest version (1.7.0) instead of the outdated v1.5.1.",
      "related": [],
      "keyQuote": "the latest version is 1.7, if you are doing evaluation, try to use the latest version",
      "number": 1306,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:37.988Z"
    },
    {
      "summary": "Docker builds for Temporal Server run slowly on Apple Silicon hardware due to x86 emulation, causing QEMU crashes. The feature request asks for native ARM64 builds to be published to Docker Hub for better compatibility.",
      "category": "feature",
      "subcategory": "docker-builds",
      "apis": [],
      "components": [
        "docker-images",
        "base-builder",
        "server-image"
      ],
      "concepts": [
        "arm64",
        "apple-silicon",
        "docker-buildx",
        "multi-platform-builds",
        "qemu-emulation",
        "performance"
      ],
      "severity": "high",
      "userImpact": "Developers using Apple Silicon Macs cannot efficiently run Temporal Server in Docker, blocking local development.",
      "rootCause": "x86_64 Docker images require QEMU emulation on ARM64 hardware, which causes segmentation faults during database schema initialization.",
      "proposedFix": "Build and publish ARM64 native Docker images for Temporal Server, base-builder, admin-tools, and auto-setup using docker buildx with multi-platform support.",
      "workaround": "Delete and recreate postgres container to force fresh initialization instead of using existing state (works inconsistently on some versions).",
      "resolution": "fixed",
      "resolutionDetails": "New GitHub Actions pipelines were added to build and release both arm64 and amd64 images for server, admin-tools, auto-setup, and base images.",
      "related": [
        1575
      ],
      "keyQuote": "Running amd64 builds on Apple Silicon is very slow and will trigger a qemu crash on Docker for Mac on Apple Silicon hardware.",
      "number": 1305,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:41.441Z"
    },
    {
      "summary": "Request to implement backoff strategy for workflow tasks to improve resilience and operational efficiency when tasks fail or encounter transient issues.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "workflow-executor",
        "task-processor",
        "retry-handler"
      ],
      "concepts": [
        "backoff",
        "retry",
        "resilience",
        "task-execution",
        "exponential-backoff",
        "transient-failures"
      ],
      "severity": "medium",
      "userImpact": "Users need better control over how failed workflow tasks are retried, allowing for exponential backoff to reduce load during transient failures.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Issue was marked as duplicate, suggesting this functionality was addressed elsewhere or consolidated with another backoff implementation request.",
      "related": [],
      "keyQuote": null,
      "number": 1303,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:26.289Z"
    },
    {
      "summary": "The VerifyCompatibleVersion function uses type-switch anti-pattern to check SQL plugin versions, creating hard dependencies on mysql and postgresql plugins even when using custom SQL plugins, and panics on unknown plugin types.",
      "category": "bug",
      "subcategory": "sql-plugin-architecture",
      "apis": [],
      "components": [
        "sql-plugin",
        "persistence-layer",
        "schema-versioning",
        "handler"
      ],
      "concepts": [
        "plugin-architecture",
        "dependency-management",
        "version-checking",
        "database-abstraction",
        "type-safety"
      ],
      "severity": "high",
      "userImpact": "Users cannot build custom SQL plugins without including mysql and postgresql driver dependencies, and the server panics if an unknown SQL plugin type is used.",
      "rootCause": "Type-switch pattern in VerifyCompatibleVersion that hardcodes mysql and postgresql plugin types instead of using interface-based version checking abstraction.",
      "proposedFix": "Implement version checking via a plugin interface method that each SQL plugin implements, checking against the plugin's static version number rather than switching on concrete types.",
      "workaround": "Use the master branch version after commit c71d197c394aa440338ffd6f36f23a87ef9bad84 or go.temporal.io/server@v1.8.x where version checking is handled by the driver.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved in v1.8.x by moving version checking responsibility to the SQL driver implementation, eliminating the need for type-switches in the server code.",
      "related": [],
      "keyQuote": "Version checking is done via a type-switch anti-pattern... Moreover there is a panic if an unknown sql plugin type is recognized, effectively rendering the plug-in concept useless",
      "number": 1292,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:26.347Z"
    },
    {
      "summary": "New user asks whether Temporal workflows support storing and sharing data between activities that can be retrieved from outside the workflow using SDK methods.",
      "category": "question",
      "subcategory": "workflow-data-sharing",
      "apis": [],
      "components": [
        "workflow",
        "activity",
        "sdk"
      ],
      "concepts": [
        "data-persistence",
        "inter-activity-communication",
        "external-retrieval",
        "workflow-state",
        "shared-data"
      ],
      "severity": "low",
      "userImpact": "Users new to Temporal need guidance on whether and how to implement shared, retrievable data structures between activities in a workflow.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "is it possible to store some data structure in a running workflow, I want the data can be shared between activities and retrievable from outside",
      "number": 1290,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:25.016Z"
    },
    {
      "summary": "Signal received during workflow task execution that completes with continue-as-new causes task failure, forcing replay and potentially creating infinite loops. The request is to pass signals to the next workflow run instead, making this behavior configurable.",
      "category": "feature",
      "subcategory": "signal-handling",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "workflow-task-processor",
        "signal-handler",
        "workflow-state-machine",
        "replay-engine"
      ],
      "concepts": [
        "signal-handling",
        "continue-as-new",
        "workflow-replay",
        "state-management",
        "race-condition",
        "signal-transfer"
      ],
      "severity": "high",
      "userImpact": "Workflows receiving frequent signals cannot safely use continue-as-new without hitting history size limits or signal limits, forcing users to implement complex signal carryover logic.",
      "rootCause": "Task failure on signal receipt during continue-as-new command creation forces workflow replay, and repeated signals prevent completion of continue-as-new execution.",
      "proposedFix": "By default, pass received signals to the next workflow run instead of failing the task. Make this behavior configurable through options.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "By default pass signal to the next run of the workflow. This way the continue-as-new is never blocked and the signal is not lost.",
      "number": 1289,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:14.215Z"
    },
    {
      "summary": "Remove the dependency on database visibility configuration when Elasticsearch is being used. This is an enhancement to simplify configuration by eliminating an unnecessary DB visibility config requirement for users who have ES enabled.",
      "category": "feature",
      "subcategory": "configuration",
      "apis": [],
      "components": [
        "elasticsearch",
        "database",
        "visibility",
        "configuration"
      ],
      "concepts": [
        "dependency-management",
        "configuration-simplification",
        "elasticsearch",
        "visibility-config",
        "optional-dependency"
      ],
      "severity": "low",
      "userImpact": "Users with Elasticsearch enabled can simplify their configuration by removing unnecessary database visibility settings.",
      "rootCause": "DB visibility configuration is unnecessarily required even when Elasticsearch is used for visibility purposes.",
      "proposedFix": "Make DB visibility configuration optional when Elasticsearch is enabled.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The DB visibility configuration dependency was removed for systems using Elasticsearch.",
      "related": [],
      "keyQuote": null,
      "number": 1280,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:11.538Z"
    },
    {
      "summary": "The cassandra-tool fails to connect to a Cassandra cluster with a timeout error, even though cqlsh can connect successfully using the same credentials and host information.",
      "category": "bug",
      "subcategory": "cassandra-tool",
      "apis": [],
      "components": [
        "cassandra-tool",
        "gocql",
        "connection-management"
      ],
      "concepts": [
        "connection-timeout",
        "cassandra-client",
        "authentication",
        "network-connectivity"
      ],
      "severity": "medium",
      "userImpact": "Users are unable to use the temporal-cassandra-tool to interact with their Cassandra cluster, blocking database administration and verification tasks.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue could not be reproduced by maintainers on their local machines. Reporter indicated the problem may have been environment-specific (macOS vs Linux container) or related to their specific Cassandra setup.",
      "related": [],
      "keyQuote": "gocql: unable to dial control conn 127.0.0.1:9042: gocql: no response to connection startup within timeout",
      "number": 1279,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:12.805Z"
    },
    {
      "summary": "The CASSANDRA_DATACENTER environment variable is shadowed by a subcommand flag with the same name in temporal-cassandra-tool create, preventing its use with the autosetup feature.",
      "category": "bug",
      "subcategory": "cassandra-tool",
      "apis": [],
      "components": [
        "cassandra-tool",
        "docker",
        "setup"
      ],
      "concepts": [
        "environment-variables",
        "flag-shadowing",
        "cassandra-configuration",
        "datacenter",
        "autosetup"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use the CASSANDRA_DATACENTER environment variable with the temporal-cassandra-tool create command due to flag shadowing, limiting automation options.",
      "rootCause": "The create subcommand defines a flag with the same name as the global CASSANDRA_DATACENTER environment variable, which shadows the environment variable.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Closed with response that autosetup is not intended for production; users should manually set up Cassandra clusters for production use.",
      "related": [],
      "keyQuote": "An implication is that, no way can we use autosetup with that environment variable.",
      "number": 1278,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:00.394Z"
    },
    {
      "summary": "Request to provide build steps and pre-built binaries for temporal-sql-tool so users can build it without installing extra dependencies, potentially leveraging Docker for reproducible builds.",
      "category": "feature",
      "subcategory": "tooling-build",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "build-system",
        "docker"
      ],
      "concepts": [
        "binary-distribution",
        "build-automation",
        "docker-containerization",
        "developer-experience",
        "reproducible-builds"
      ],
      "severity": "low",
      "userImpact": "Users need clearer documentation and easier methods to build temporal-sql-tool binaries without installing Go or other dependencies.",
      "rootCause": null,
      "proposedFix": "Add build steps to documentation, provide pre-built binaries on GitHub/DockerHub, or create a Dockerfile to build the tool in a container.",
      "workaround": "Download source, use Docker with Go image to build using the Makefile target, then install the binary manually to /usr/local/bin/",
      "resolution": "fixed",
      "resolutionDetails": "Cassandra/SQL tool made available via Docker Hub at temporalio/admin-tools",
      "related": [],
      "keyQuote": "Cassandra /SQL tool is available here: https://hub.docker.com/r/temporalio/admin-tools",
      "number": 1271,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:11:59.415Z"
    },
    {
      "summary": "Temporal Cassandra Tools CLI is missing a ConnectTimeout configuration option. While a generic Timeout flag exists, it doesn't expose gocql's ConnectTimeout field, which is needed when latency between client and Cassandra seed is high.",
      "category": "bug",
      "subcategory": "cassandra-tools",
      "apis": [],
      "components": [
        "cassandra-tools",
        "cli",
        "gocql-integration"
      ],
      "concepts": [
        "timeout",
        "connection",
        "cassandra",
        "networking",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users with high-latency connections to Cassandra cannot configure ConnectTimeout, causing connection failures.",
      "rootCause": "The Cassandra Tools CLI exposes a generic Timeout flag but not the specific ConnectTimeout field from gocql.ClusterConfig.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #1277",
      "related": [
        1277
      ],
      "keyQuote": "Gocql will fail to connect if the latency between the client and the CASSANDRA_SEED is high.",
      "number": 1270,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:12:01.601Z"
    },
    {
      "summary": "Workflow execution with oversized arguments creates a corrupted history event with no events, causing subsequent workflow retrievals to fail with 'corrupted history event batch' errors and blocking other workflow executions in the namespace.",
      "category": "bug",
      "subcategory": "history-corruption",
      "apis": [
        "ExecuteWorkflow"
      ],
      "components": [
        "history-service",
        "workflow-execution",
        "event-storage",
        "batch-handling"
      ],
      "concepts": [
        "payload-size-limits",
        "history-corruption",
        "event-batching",
        "database-consistency",
        "error-handling"
      ],
      "severity": "critical",
      "userImpact": "Users cannot execute workflows with large arguments, and corrupted executions block all subsequent workflows in the namespace until manually cleared from the database.",
      "rootCause": "When workflow arguments exceed max history size, the server creates a workflow execution with no initial history events, leaving the event stream in an inconsistent state that database queries cannot read.",
      "proposedFix": null,
      "workaround": "Manually delete the corrupted execution from the database.",
      "resolution": "wontfix",
      "resolutionDetails": "Closed as not officially supported issue - Scylla database behavior differs from Cassandra regarding batch size thresholds, and Scylla is not officially supported (Cassandra 3.11, MySQL 5.7, PostgreSQL 9.6 only).",
      "related": [],
      "keyQuote": "A workflow execution is created with no history. Subsequent attempts to retrieve the workflow get 'corrupted history event batch, eventID is not continuous'",
      "number": 1267,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:11:46.754Z"
    },
    {
      "summary": "Temporal services fail to start when running as non-root user in Kubernetes due to permission errors when writing configuration files. The issue blocks enterprises with security policies requiring non-root containers.",
      "category": "bug",
      "subcategory": "kubernetes-deployment",
      "apis": [],
      "components": [
        "docker-config",
        "helm-charts",
        "file-permissions"
      ],
      "concepts": [
        "security",
        "kubernetes",
        "permissions",
        "non-root-user",
        "container-security",
        "psp"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy Temporal in Kubernetes environments with Pod Security Policies requiring non-root user execution.",
      "rootCause": "Configuration template processing by dockerize tool fails when attempting to write to /etc/temporal/config/ directory without sufficient permissions for non-root user.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved, likely through updates to helm charts and configuration file permissions handling to support non-root deployments.",
      "related": [],
      "keyQuote": "Run temporal cluster in kubernetes environment as non-root user. Some enterprises have restrictions in k8s environment to block running containers as root user.",
      "number": 1263,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:11:47.555Z"
    },
    {
      "summary": "Request for workflows to have the ability to reschedule or deschedule themselves dynamically. Currently, cron workflows that need to complete based on certain conditions require manual scheduling logic implementation.",
      "category": "feature",
      "subcategory": "workflow-scheduling",
      "apis": [],
      "components": [
        "workflow-scheduler",
        "cron-engine"
      ],
      "concepts": [
        "cron-workflows",
        "self-scheduling",
        "workflow-lifecycle",
        "dynamic-scheduling",
        "workflow-completion"
      ],
      "severity": "medium",
      "userImpact": "Users implementing periodic cron workflows cannot dynamically stop or reschedule execution without implementing custom scheduling logic.",
      "rootCause": null,
      "proposedFix": "Add a `Workflow.reschedule` or similar method to allow workflows to deschedule or reschedule themselves.",
      "workaround": "Implement scheduling logic manually or call an activity to cancel the workflow execution.",
      "resolution": "fixed",
      "resolutionDetails": "Schedule feature added to Temporal that supports pause/resume functionality, providing the requested capability.",
      "related": [],
      "keyQuote": "After a cron workflow has started, I would like to be able to deschedule (or reschedule) itself if it determines that it no longer needs to run.",
      "number": 1256,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:11:43.495Z"
    },
    {
      "summary": "Request to support timezone-aware cron schedules in Temporal workflows. Currently, cron schedules only work with UTC times, but users need to run workflows at specific local times (e.g., 9AM Zurich time) with proper DST handling. The underlying robfig/cron library already supports CRON_TZ parameter, but this feature is undocumented and untested in Temporal.",
      "category": "feature",
      "subcategory": "cron-scheduling",
      "apis": [],
      "components": [
        "cron-scheduler",
        "workflow-execution",
        "time-handling"
      ],
      "concepts": [
        "timezone",
        "DST",
        "cron-schedule",
        "UTC",
        "time-zone-aware",
        "scheduling",
        "workflow-timing"
      ],
      "severity": "medium",
      "userImpact": "Users working across timezones cannot reliably schedule workflows at specific local times without manual UTC conversion and DST management.",
      "rootCause": "Temporal uses robfig/cron which supports CRON_TZ parameter, but the version (v1.2.0) in use lacks this support; needs upgrade to v3.0.1+",
      "proposedFix": "Upgrade robfig/cron dependency to v3.0.1 or later and expose CRON_TZ parameter support in the cron schedule configuration with proper documentation",
      "workaround": "Manually convert local times to UTC and manage DST changes in application code",
      "resolution": "fixed",
      "resolutionDetails": "Resolved via PR #2215 which added timezone support to the cron scheduling implementation",
      "related": [
        2215
      ],
      "keyQuote": "It seems that temporal already could support timezones as the library used already provides timezone support... this should work out of the box already today",
      "number": 1255,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:08:36.133Z"
    },
    {
      "summary": "User encounters 'Unknown system variable transaction_isolation' error when running temporal-sql-tool on MySQL 5.7 (Aurora RDS). The issue is resolved by setting the SQL_CONNECT_ATTRIBUTES environment variable to specify the transaction isolation level.",
      "category": "bug",
      "subcategory": "database-setup",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "mysql-driver",
        "connection-configuration"
      ],
      "concepts": [
        "transaction-isolation",
        "mysql-compatibility",
        "database-connection",
        "environment-variables",
        "rds-mysql"
      ],
      "severity": "medium",
      "userImpact": "Users setting up Temporal with MySQL 5.7 on RDS encounter blocking errors when initializing the database schema tool.",
      "rootCause": "MySQL 5.7 uses 'tx_isolation' variable name instead of 'transaction_isolation', and the connection string must explicitly configure this for compatibility.",
      "proposedFix": "Set SQL_CONNECT_ATTRIBUTES environment variable with tx_isolation parameter when running temporal-sql-tool on MySQL 5.7.",
      "workaround": "Use 'SQL_CONNECT_ATTRIBUTES=tx_isolation=\"READ-COMMITTED\" ./temporal-sql-tool ...' to work around the issue on Aurora MySQL 5.7.",
      "resolution": "fixed",
      "resolutionDetails": "Workaround provided by maintainers specifying the correct environment variable and parameter name for MySQL 5.7 compatibility.",
      "related": [],
      "keyQuote": "Use env to pass in the necessary: SQL_CONNECT_ATTRIBUTES=tx_isolation=\"READ-COMMITTED\" ./temporal-sql-tool",
      "number": 1251,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:08:33.003Z"
    },
    {
      "summary": "Multiple Temporal clusters in the same Kubernetes cluster can experience service discovery cross-contamination when pod IP addresses are reused. A frontend service from cluster A can incorrectly discover and connect to matching services from cluster B, leading to cross-cluster communication.",
      "category": "bug",
      "subcategory": "service-discovery",
      "apis": [],
      "components": [
        "frontend",
        "matching-service",
        "service-resolver",
        "cluster-metadata"
      ],
      "concepts": [
        "service-discovery",
        "cluster-isolation",
        "ip-address-reuse",
        "kubernetes-deployments",
        "pod-membership",
        "network-segmentation"
      ],
      "severity": "high",
      "userImpact": "Users running multiple independent Temporal clusters in the same Kubernetes cluster risk cross-cluster communication and data consistency issues when IP addresses are reused.",
      "rootCause": "Service discovery reads stale IP addresses from cluster_metadata table and lacks validation that discovered nodes belong to the same cluster before adding them to the membership ring.",
      "proposedFix": "Add cluster validation checks to ensure discovered nodes are part of the correct cluster, or use separate membership ports per cluster to prevent discovery across cluster boundaries.",
      "workaround": "Use different membership ports for each Temporal cluster or implement Kubernetes NetworkPolicy to restrict inter-cluster communication.",
      "resolution": "fixed",
      "resolutionDetails": "Cluster ID was added to the membership app name in PR #3415 to prevent cross-cluster service discovery.",
      "related": [
        3415
      ],
      "keyQuote": "Temporal cluster A's frontend service is seeing 6 matching nodes, three from A and three from B through IP address reuse and stale cluster metadata.",
      "number": 1234,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:08:33.136Z"
    },
    {
      "summary": "The Slack community link in the README.md is broken and non-functional, preventing users from joining the Temporal Slack community.",
      "category": "docs",
      "subcategory": "documentation-links",
      "apis": [],
      "components": [
        "documentation",
        "readme"
      ],
      "concepts": [
        "community",
        "outreach",
        "broken-link",
        "user-onboarding"
      ],
      "severity": "low",
      "userImpact": "Users cannot access the Slack community through the provided link in the documentation.",
      "rootCause": "Slack invitation link is expired or invalid",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The Slack link in README was likely updated or replaced with a working invitation link",
      "related": [],
      "keyQuote": "I'd love to join Temporal's Slack community but it seems as though the link provided at the bottom of README.md is broken",
      "number": 1230,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:08:15.531Z"
    },
    {
      "summary": "Docker image temporalio/server:1.6.1 reports version 1.6.0 instead of 1.6.1 when running temporal-server -version command. The release did not include the necessary version number update in the binary.",
      "category": "bug",
      "subcategory": "version-reporting",
      "apis": [],
      "components": [
        "docker-image",
        "server-binary",
        "version-reporting"
      ],
      "concepts": [
        "versioning",
        "docker-packaging",
        "release-management",
        "binary-metadata"
      ],
      "severity": "low",
      "userImpact": "Users cannot verify the correct version of the Temporal server running in their Docker container, causing potential confusion about which version they have deployed.",
      "rootCause": "The 1.6.1 release did not include the necessary change to update the version number reported by the server binary.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in version 1.6.2 which included the necessary version number update.",
      "related": [],
      "keyQuote": "the release version 1.6.1 did not include the necessary change to version number reported by server",
      "number": 1226,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:08:18.558Z"
    },
    {
      "summary": "User asks how to orchestrate and coordinate multiple workflows running in different microservices in sequence (ACB). Clarification on workflow composition patterns across service boundaries.",
      "category": "question",
      "subcategory": "workflow-composition",
      "apis": [
        "StartChildWorkflow",
        "ChildWorkflowFuture"
      ],
      "components": [
        "workflow-orchestration",
        "child-workflows",
        "temporal-client"
      ],
      "concepts": [
        "workflow-coordination",
        "microservices",
        "asynchronous-execution",
        "parent-child-workflows",
        "cross-service-integration",
        "workflow-sequencing"
      ],
      "severity": "low",
      "userImpact": "Users need guidance on structuring workflows across multiple microservices to execute them in a specific sequence.",
      "rootCause": null,
      "proposedFix": "Use a parent workflow that orchestrates child workflows sequentially by awaiting each child workflow's completion before starting the next one.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was answered by suggesting parent workflow pattern with sequential child workflow execution using StartChildWorkflow and Future.Get() pattern.",
      "related": [],
      "keyQuote": "start child workflow A (workflow A) and get future A / future A.Get() / start child workflow B (workflow B) and get future B",
      "number": 1225,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:08:17.960Z"
    },
    {
      "summary": "Optimize performance by skipping visibility task generation when direct deletion of visibility records succeeds during workflow retention cleanup.",
      "category": "feature",
      "subcategory": "workflow-retention",
      "apis": [],
      "components": [
        "visibility",
        "task-generation",
        "retention-cleanup"
      ],
      "concepts": [
        "performance-optimization",
        "task-generation",
        "visibility-records",
        "workflow-lifecycle",
        "retention-policy"
      ],
      "severity": "medium",
      "userImpact": "Reducing unnecessary task generation improves performance and reduces overhead during workflow retention cleanup.",
      "rootCause": "Current implementation generates delete visibility tasks even when direct deletion succeeds, creating unnecessary work.",
      "proposedFix": "Bypass visibility task generation if the direct call to delete visibility record was successful.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Performance optimization implemented by skipping task generation on successful direct visibility deletion.",
      "related": [],
      "keyQuote": "The code can be optimized by bypassing visibility task generation if direct call to delete visibility record was succeeded.",
      "number": 1224,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:08:03.989Z"
    },
    {
      "summary": "CancelWorkflow API does not properly cancel activities in version 1.6.0. The workflow transitions to PENDING_ACTIVITY_STATE_CANCEL_REQUESTED but the worker continues running, indicating the cancellation signal is not being propagated to the activity execution.",
      "category": "bug",
      "subcategory": "activity-cancellation",
      "apis": [
        "CancelWorkflow"
      ],
      "components": [
        "worker",
        "activity-executor",
        "workflow-cancellation"
      ],
      "concepts": [
        "cancellation",
        "activity-lifecycle",
        "context-propagation",
        "signal-handling"
      ],
      "severity": "high",
      "userImpact": "Users cannot cancel long-running activities, forcing them to wait for timeouts or restart workers to stop workflows.",
      "rootCause": "The cancellation signal is not being propagated from the workflow engine to the activity executor in version 1.6.0.",
      "proposedFix": null,
      "workaround": "Upgrade to server version 1.6.1 or later and use the latest Go samples.",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved by upgrading to server version 1.6.1 or later, which included fixes for the heartbeat and cancellation propagation issues.",
      "related": [],
      "keyQuote": "The Workflow State change to PENDING_ACTIVITY_STATE_CANCEL_REQUESTED, but the worker/main.go is still running.",
      "number": 1218,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:08:03.291Z"
    },
    {
      "summary": "User was able to set up a new cluster with an incorrect database schema version without validation, resulting in a mismatch between the actual schema and the recorded version. The tool should prevent this configuration error.",
      "category": "bug",
      "subcategory": "schema-management",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "schema-versioning",
        "database-setup"
      ],
      "concepts": [
        "schema-validation",
        "version-mismatch",
        "database-initialization",
        "configuration-error",
        "data-integrity"
      ],
      "severity": "medium",
      "userImpact": "Users can accidentally set up a database with an incorrect schema version, leading to data integrity issues and potential runtime failures.",
      "rootCause": "The schema setup tool does not validate that the specified version matches the actual schema being applied, allowing mismatches between recorded and actual schema versions.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "The maintainers clarified that combining multiple versions of schema changes in one file is a supported feature, so the behavior is intentional rather than a bug.",
      "related": [],
      "keyQuote": "the command used above is actually one supported feature, i.e. combining multiple version worth of schema changes into one file",
      "number": 1215,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:08:04.170Z"
    },
    {
      "summary": "Add application_name to PostgreSQL connection string to enable database logging and metrics to identify the Temporal application. This provides better observability for database operations.",
      "category": "feature",
      "subcategory": "postgres-connection",
      "apis": [],
      "components": [
        "postgres-plugin",
        "sql-persistence",
        "connection-pool"
      ],
      "concepts": [
        "database-observability",
        "application-identification",
        "connection-string",
        "logging",
        "metrics"
      ],
      "severity": "low",
      "userImpact": "Users can better track and monitor Temporal application database activity through PostgreSQL logs and metrics.",
      "rootCause": null,
      "proposedFix": "Set the application_name parameter in the PostgreSQL connection string configuration in the postgresql plugin.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by pull request #1214",
      "related": [
        1214
      ],
      "keyQuote": "Setting application_name informs the database. The value may be used in logs and metrics, depending on the DB implementation.",
      "number": 1208,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:50.179Z"
    },
    {
      "summary": "Build fails for 32-bit Linux systems (GOARCH=386) due to untyped int64 constants exceeding 32-bit signed integer limits in the Elasticsearch visibility store.",
      "category": "bug",
      "subcategory": "build-system",
      "apis": [],
      "components": [
        "persistence",
        "elasticsearch",
        "visibility-store"
      ],
      "concepts": [
        "32-bit-architecture",
        "compilation",
        "constants",
        "overflow",
        "int64"
      ],
      "severity": "low",
      "userImpact": "Users cannot build Temporal server binaries for 32-bit Linux systems; support for 386 architecture is not viable.",
      "rootCause": "Untyped int64 constants (INT64_MIN/MAX) in esVisibilityStore.go overflow when compiled for 32-bit systems; Go treats untyped constants as errors on 32-bit platforms per Go spec.",
      "proposedFix": "Type constants explicitly as int64 (e.g., MAX_INT64 = int64(math.MaxInt64)) or use string conversion approaches, though 32-bit support may not be feasible due to service logic requiring int64.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Team determined 32-bit support is not a viable option since service logic fundamentally requires int64 types.",
      "related": [
        23086
      ],
      "keyQuote": "for now, i don't think support 32bit will be an option.",
      "number": 1207,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:51.823Z"
    },
    {
      "summary": "Request to add a SignalWithReset API that atomically resets a workflow to its last task and delivers a signal to it, enabling workflows to handle new signals and support complex reexecution patterns that are otherwise impossible with closed workflows.",
      "category": "feature",
      "subcategory": "workflow-signals",
      "apis": [
        "SignalWithReset"
      ],
      "components": [
        "workflow-engine",
        "signal-handler",
        "workflow-reset"
      ],
      "concepts": [
        "signal",
        "reset",
        "workflow-execution",
        "state-management",
        "atomic-operation",
        "reexecution"
      ],
      "severity": "medium",
      "userImpact": "Enables advanced DSL workflows to support reexecution of past tasks based on signals, which is currently impossible for closed workflows.",
      "rootCause": null,
      "proposedFix": "Implement SignalWithReset API that atomically resets the workflow to the last workflow task and delivers the signal to it.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When called the service is going to reset the workflow to the last workflow task and deliver the signal to it atomically.",
      "number": 1203,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:51.655Z"
    },
    {
      "summary": "The temporal-sql-tool's `-y` flag for dry-run is not working correctly - it still performs the schema migration instead of just simulating it.",
      "category": "bug",
      "subcategory": "sql-tool",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "schema-migration",
        "postgres-plugin"
      ],
      "concepts": [
        "dry-run",
        "schema-migration",
        "command-line-flags",
        "database-operations"
      ],
      "severity": "high",
      "userImpact": "Users cannot safely preview schema migrations before applying them, risking unintended database changes.",
      "rootCause": "The `-y` flag is not being interpreted as a dry-run flag; instead it appears to trigger the actual migration.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed as a bug fix in the sql-tool dry-run functionality.",
      "related": [],
      "keyQuote": "adding `-y` command should not run - rather perform dry-run.",
      "number": 1197,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:36.644Z"
    },
    {
      "summary": "TestActivityEnvironment lacks support for injecting ActivityCompletionClient, which is needed for testing activities that use asynchronous completion. Users currently cannot test isolated activities that require this client.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [
        "ActivityCompletionClient",
        "TestActivityEnvironment",
        "TestWorkflowEnvironment"
      ],
      "components": [
        "activity-executor",
        "test-framework",
        "activity-client"
      ],
      "concepts": [
        "activity-completion",
        "async-completion",
        "testing",
        "dependency-injection",
        "isolation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot effectively test activities that use asynchronous completion patterns with ActivityCompletionClient in isolation.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was recreated and addressed in the Go SDK with corresponding fixes applied across SDKs.",
      "related": [],
      "keyQuote": "We are using TestActivityEnvironment to isolate activity testing, but the challenge for us is that we need to inject an ActivityCompletionClient into the activity",
      "number": 1185,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:37.918Z"
    },
    {
      "summary": "Request to expose ActivityStarted, ActivityCompleted, and ActivityTimedOut events directly to running workflows so they can query activity state without iterating through the entire workflow history, which is slow and unscalable.",
      "category": "feature",
      "subcategory": "workflow-execution",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "workflow-execution",
        "activity-executor",
        "history",
        "query-engine"
      ],
      "concepts": [
        "activity-lifecycle",
        "event-exposure",
        "workflow-query",
        "history-iteration",
        "performance"
      ],
      "severity": "medium",
      "userImpact": "Users currently cannot efficiently query activity state during workflow execution and must resort to slow history iteration.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Workflow code can now access activity event information directly through the workflow context.",
      "related": [],
      "keyQuote": "If we want to store data about when the activity started, completed or timed out etc so that we can query those in Workflow. Currently - we have to iterate over workflow history which is slow and not scalable.",
      "number": 1175,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:39.868Z"
    },
    {
      "summary": "The temporal-cassandra-tool and auto-setup Docker image default to consistency level ALL, which is not supported by AWS Keyspaces. Users need CLI arguments and environment variables to configure alternative consistency levels for cloud-managed Cassandra services.",
      "category": "feature",
      "subcategory": "cassandra-configuration",
      "apis": [],
      "components": [
        "temporal-cassandra-tool",
        "auto-setup",
        "gocql-driver"
      ],
      "concepts": [
        "consistency-level",
        "cassandra-configuration",
        "aws-keyspaces",
        "cloud-compatibility",
        "database-connection"
      ],
      "severity": "high",
      "userImpact": "Users cannot use Temporal with AWS Keyspaces due to unsupported consistency level defaults, blocking cloud adoption.",
      "rootCause": "temporal-cassandra-tool and auto-setup Docker image hardcode consistency level ALL, which AWS Keyspaces does not support (only ONE, LOCAL_QUORUM, LOCAL_ONE supported).",
      "proposedFix": "Add CLI argument to temporal-cassandra-tool for consistency level selection and environment variable to auto-setup Docker image to control gocql driver consistency level.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was unblocked by PR #1189 starting from version 1.6.x which made consistency level configurable.",
      "related": [
        513,
        1189
      ],
      "keyQuote": "AWS Keyspaces does not support the consistency level that the CLI tool is defaulting to. A temporal-cassandra-tool CLI argument should be provided to select alternative consistency levels.",
      "number": 1168,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:23.613Z"
    },
    {
      "summary": "Temporal Server loads TLS certificates only at startup and never updates them, requiring server restarts to rotate certificates. The feature request asks for hot reload support, ideally with blue/green certificate pairs to enable rotation without restarts.",
      "category": "feature",
      "subcategory": "tls-certificates",
      "apis": [],
      "components": [
        "tls-configuration",
        "certificate-manager",
        "server-startup"
      ],
      "concepts": [
        "certificate-rotation",
        "tls-certificates",
        "hot-reload",
        "zero-downtime",
        "blue-green-deployment",
        "server-restart"
      ],
      "severity": "high",
      "userImpact": "Users must perform rolling restarts to rotate TLS certificates before expiry, causing shard reallocation and increased latency.",
      "rootCause": "Server loads certificates once at startup and never reloads them from disk or configuration.",
      "proposedFix": "Implement certificate rotation without restart, with optional blue/green pair support to rotate one certificate at a time.",
      "workaround": "Perform rolling restart of servers in the cluster to pick up updated certificates.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved via #1415",
      "related": [
        1415
      ],
      "keyQuote": "To rotate a certificate before it expires, one has to update the configuration or the certificate file and then restart the server.",
      "number": 1162,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:24.812Z"
    },
    {
      "summary": "User requests a feature to resume failed workflows at the point of the last failed activity, rather than only at workflow task boundaries. The Temporal team indicated this capability is not currently supported and has no immediate plans for implementation.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-execution",
        "reset-points",
        "activity-execution"
      ],
      "concepts": [
        "workflow-reset",
        "failure-recovery",
        "execution-resumption",
        "checkpoint",
        "activity-failure",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Users cannot resume workflows at granular failure points, limiting their ability to recover from activity failures without replaying earlier workflow steps.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Team confirmed that reset points are limited to WorkflowTask boundaries and indicated no immediate plans to support activity-level reset points.",
      "related": [],
      "keyQuote": "Reset is currently supported on WorkflowTask boundaries. Currently we don't have immediate plans to support other reset points.",
      "number": 1157,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:24.649Z"
    },
    {
      "summary": "Request for a custom Cassandra seed node provider hook to support dynamic discovery of seed node hostnames/IPs without requiring configuration updates when cluster membership changes.",
      "category": "feature",
      "subcategory": "cassandra-persistence",
      "apis": [],
      "components": [
        "cassandra",
        "persistence",
        "server-config"
      ],
      "concepts": [
        "dynamic-discovery",
        "seed-provider",
        "cluster-configuration",
        "service-discovery",
        "plugin-interface"
      ],
      "severity": "medium",
      "userImpact": "Organizations with dynamic Cassandra clusters can implement custom seed discovery logic without modifying Temporal configuration.",
      "rootCause": null,
      "proposedFix": "Add a new ServerOption (WithCassandraSeedProvider) that accepts a custom seed provider implementation via a well-defined interface.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented as a new ServerOption for custom Cassandra seed provider",
      "related": [],
      "keyQuote": "It should be possible to supply a custom seed provider via a well defined interface.",
      "number": 1156,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:10.458Z"
    },
    {
      "summary": "Refactor and unify error handling in the Cassandra persistence layer to improve code maintainability and consistency. The issue targets two specific sections of the cassandraPersistence.go file that have duplicated or inconsistent error handling patterns.",
      "category": "feature",
      "subcategory": "persistence-cassandra",
      "apis": [],
      "components": [
        "cassandra-persistence",
        "error-handling",
        "database-layer"
      ],
      "concepts": [
        "error-handling",
        "code-refactoring",
        "persistence-layer",
        "cassandra",
        "maintainability",
        "consistency"
      ],
      "severity": "medium",
      "userImpact": "Better error handling in the persistence layer improves system reliability and makes it easier for developers to maintain and extend the codebase.",
      "rootCause": null,
      "proposedFix": "Unify and refactor the error handling logic in the two referenced sections of cassandraPersistence.go (lines 1515-1598 and 973-1091).",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Unify the error handling in Cassandra layer",
      "number": 1154,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:11.513Z"
    },
    {
      "summary": "Add unit tests for NoSQL persistence layer queries covering all persistence abstractions (cluster metadata, history, matching, namespace, queue, and visibility). This is part of a broader NoSQL persistence refactoring effort to ensure comprehensive test coverage.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "persistence",
        "nosql-plugin",
        "unit-tests",
        "sql-abstraction"
      ],
      "concepts": [
        "testing",
        "persistence-layer",
        "query-abstraction",
        "coverage",
        "refactoring",
        "validation"
      ],
      "severity": "medium",
      "userImpact": "Ensures reliability and correctness of NoSQL persistence implementations across all query types used by Temporal.",
      "rootCause": null,
      "proposedFix": "Create unit tests following the existing SQL plugin test pattern for each of the 22 persistence query types listed.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Test suite was implemented as part of the NoSQL persistence refactoring effort.",
      "related": [],
      "keyQuote": "Unit test for each NoSQL persistence query",
      "number": 1153,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:10.644Z"
    },
    {
      "summary": "Refactor NoSQL persistence layer by creating side interfaces similar to the existing SQL plugin architecture. This involves defining standardized interfaces for 22 different persistence concerns including cluster metadata, history activities, workflow execution, and task queue management.",
      "category": "feature",
      "subcategory": "persistence-layer",
      "apis": [],
      "components": [
        "persistence",
        "nosql-plugin",
        "cluster-metadata",
        "history-store",
        "matching-service"
      ],
      "concepts": [
        "interface-standardization",
        "plugin-architecture",
        "persistence-abstraction",
        "refactoring",
        "data-storage",
        "task-management"
      ],
      "severity": "medium",
      "userImpact": "Enables organizations to implement custom NoSQL persistence backends by providing clear interface contracts for all persistence operations.",
      "rootCause": null,
      "proposedFix": "Create NoSQL side interface following the SQL plugin pattern with implementations for all 22 persistence concerns (cluster metadata, history activities, history nodes, matching tasks, etc.)",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "NoSQL persistence interfaces were implemented following the planned refactoring checklist.",
      "related": [],
      "keyQuote": "Come up with NoSQL side interface, similar to the SQL plugin architecture",
      "number": 1152,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:06:58.503Z"
    },
    {
      "summary": "Refactor NoSQL persistence layer to separate business logic from query interfaces, consolidating logic across cluster metadata, history events, namespace, queue, shard, matching tasks, visibility, execution, and related components into a single location.",
      "category": "feature",
      "subcategory": "persistence-refactoring",
      "apis": [],
      "components": [
        "persistence",
        "nosql",
        "cluster-metadata",
        "history",
        "namespace",
        "queue",
        "shard",
        "matching",
        "visibility",
        "execution"
      ],
      "concepts": [
        "business-logic-separation",
        "query-interface",
        "code-organization",
        "abstraction",
        "persistence-layer",
        "refactoring"
      ],
      "severity": "medium",
      "userImpact": "Better organized and maintainable persistence layer will improve code quality and make future enhancements easier for developers.",
      "rootCause": "NoSQL persistence business logic is scattered across multiple locations rather than centralized like the SQL persistence module.",
      "proposedFix": "Consolidate business logic into a single location similar to the existing SQL persistence structure at common/persistence/sql.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating refactoring work was completed or resolved.",
      "related": [],
      "keyQuote": "Separate NoSQL side business logic into one place, similar to common/persistence/sql",
      "number": 1151,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:06:55.909Z"
    },
    {
      "summary": "Activity context deadline is not updated for retry attempts. The deadline should reflect the retry start time + StartToCloseTimeout, but instead remains set to the original activity execution time, causing context deadline exceeded errors on retry.",
      "category": "bug",
      "subcategory": "activity-retry",
      "apis": [],
      "components": [
        "activity-executor",
        "context-deadline",
        "scheduler"
      ],
      "concepts": [
        "deadline",
        "retry",
        "timeout",
        "context",
        "scheduling",
        "activity-execution"
      ],
      "severity": "high",
      "userImpact": "Users experience immediate context deadline exceeded errors when activities are retried, making retried activity execution impossible without workarounds.",
      "rootCause": "The task.GetScheduledTime() value never changes on retry, so the deadline calculation uses the original scheduled time instead of the retry attempt's start time.",
      "proposedFix": "Update the deadline calculation in the activity execution path to use the current retry attempt's scheduled time rather than the original task scheduled time.",
      "workaround": "Use context.Background() instead of the provided activity context.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed in patch release v1.5.1 shortly after being reported.",
      "related": [
        1141
      ],
      "keyQuote": "When an activity is retried, the deadline value in the activity environment should reflect the start time of the retry attempt + StartToCloseTimeout.",
      "number": 1139,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:07:00.187Z"
    },
    {
      "summary": "Workflows and activities need access to validated JWT claims, permissions, and token fields from the Temporal authorizer and claim mappers. Currently, users must implement custom propagators and re-validate tokens, creating unnecessary complexity when calling other services or child workflows.",
      "category": "feature",
      "subcategory": "authorization-jwt",
      "apis": [
        "workflow.Context",
        "context.Context",
        "NewClient",
        "RegisterWorkflow",
        "RegisterActivity"
      ],
      "components": [
        "authorizer",
        "claim-mapper",
        "jwt-validation",
        "context-propagation"
      ],
      "concepts": [
        "jwt-claims",
        "authorization",
        "token-validation",
        "context-passing",
        "client-side-auth",
        "jwks",
        "authentication"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently access JWT claims in workflows and activities, forcing them to implement custom propagators and re-validate tokens when calling other services.",
      "rootCause": "The authorizer and claim mapper components only validate tokens for Temporal frontend calls but don't expose the parsed JWT and claims to workflow/activity code.",
      "proposedFix": "Pass validated JWT tokens, claims, and other fields through workflow.Context in workflows and context.Context in activities, allowing reuse for downstream service calls and child workflow execution.",
      "workaround": "Implement custom propagators to pass token fields from client to workflow to activity, though this requires re-validating tokens and is inefficient.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved via issue #1144, which passes the authorization header to workflows and activities for access to validated JWT claims and token fields.",
      "related": [
        1144
      ],
      "keyQuote": "workflows and activities should be able to reuse the provided token for calling other services, executing child workflows, and executing activities",
      "number": 1135,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:06:42.277Z"
    },
    {
      "summary": "Built-in authorizers and claimMappers cannot be configured via YAML config files; they are hardcoded to noop implementations. Users must build their own server from code or use server options to enable JWT-based authorization.",
      "category": "feature",
      "subcategory": "authorization-configuration",
      "apis": [],
      "components": [
        "server-config",
        "authorizer",
        "claim-mapper",
        "jwt-provider"
      ],
      "concepts": [
        "configuration-management",
        "authorization",
        "jwt",
        "claim-mapping",
        "security",
        "plugin-architecture"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use official Temporal Docker images with built-in JWT authorization without building their own server or using custom server options.",
      "rootCause": "Authorizers and claim mappers are hardcoded in cmd/server/main.go instead of being selectable through configuration files.",
      "proposedFix": "Extend the YAML configuration system to allow selection and configuration of built-in claim mappers and authorizers like NewDefaultJWTClaimMapper().",
      "workaround": "Build Temporal server from code using server options, or use the customization-samples/extensibility/authorizer example.",
      "resolution": "fixed",
      "resolutionDetails": "Addressed in issue #1291 and merged, enabling built-in plugins to be configurable through config for use with standard Docker images.",
      "related": [
        1291
      ],
      "keyQuote": "The provided claim mappers and authorizers (JWT) should be usable through config... rather than building their own upstream CI/CD pipeline.",
      "number": 1134,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:06:44.009Z"
    },
    {
      "summary": "Temporal server logs a TransactionRetryWithProtoRefreshError when processing RespondActivityTaskCompleted operations in history service. The error occurs during workflow execution updates and appears to be a CockroachDB transaction retry issue that was resolved in later versions.",
      "category": "bug",
      "subcategory": "database-transaction",
      "apis": [
        "RespondActivityTaskCompleted",
        "UpdateWorkflowExecution"
      ],
      "components": [
        "history-service",
        "persistence-layer",
        "transaction-management",
        "workflow-execution"
      ],
      "concepts": [
        "transaction-retry",
        "database-error",
        "workflow-update",
        "activity-completion",
        "serialization"
      ],
      "severity": "high",
      "userImpact": "Users experience workflow execution failures and server errors when activities complete, preventing normal workflow processing.",
      "rootCause": "CockroachDB transaction serialization error (TransactionRetryWithProtoRefreshError) occurring during concurrent workflow execution updates, likely due to lock contention or transaction conflict.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved in a later version (v1.6.3 or later). User confirmed the error no longer occurs with the latest release.",
      "related": [],
      "keyQuote": "TransactionRetryWithProtoRefreshError: TransactionRetryError: retry txn (RETRY_SERIALIZABLE)",
      "number": 1132,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:06:41.482Z"
    },
    {
      "summary": "The default value for FrontendMaxNamespaceRPSPerInstance is set to 1200 but may not be optimal. Proper load testing is needed to determine appropriate throttle parameters for frontend RPS limiting.",
      "category": "feature",
      "subcategory": "performance-tuning",
      "apis": [],
      "components": [
        "frontend",
        "dynamic-config",
        "rps-limiter"
      ],
      "concepts": [
        "throttling",
        "rate-limiting",
        "load-testing",
        "performance",
        "defaults",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Suboptimal default throttle parameters may cause either unnecessary request rejection or insufficient protection against overload.",
      "rootCause": null,
      "proposedFix": "Conduct proper load testing to determine optimal value for FrontendMaxNamespaceRPSPerInstance",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed through performance analysis and parameter adjustment",
      "related": [],
      "keyQuote": "we should do proper load testing for the rps",
      "number": 1131,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:02:05.840Z"
    },
    {
      "summary": "WorkflowExecutionInfo.ParentExecution is not populated for child workflows in List APIs, returning nil values. The field is only set in DescribeWorkflowExecution but not in ListOpenWorkflow/ListClosedWorkflow because parent execution info is not stored in the visibility database/Elasticsearch index.",
      "category": "bug",
      "subcategory": "workflow-visibility",
      "apis": [
        "ListOpenWorkflow",
        "ListClosedWorkflow",
        "DescribeWorkflowExecution",
        "WorkflowExecutionInfo"
      ],
      "components": [
        "visibility-store",
        "persistence-sql",
        "protobuf-migration",
        "elasticsearch-index"
      ],
      "concepts": [
        "parent-execution",
        "child-workflows",
        "visibility-database",
        "list-apis",
        "workflow-metadata",
        "data-consistency"
      ],
      "severity": "medium",
      "userImpact": "Users cannot retrieve parent execution information for child workflows through List APIs, limiting visibility into workflow relationships and hierarchy.",
      "rootCause": "ParentExecution, AutoResetPoints, and ParentNamespaceId fields are only populated in DescribeWorkflowExecution but not stored in the visibility database/Elasticsearch index used by List APIs.",
      "proposedFix": "Store missing fields (ParentExecution, AutoResetPoints, ParentNamespaceId) in visibility database/Elasticsearch index and properly populate them in List APIs.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Store this information in visibility database/Elasticsearch index and properly fill it in List* APIs.",
      "number": 1119,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:02:07.358Z"
    },
    {
      "summary": "Add history_size_bytes field to WorkflowExecutionInfo and PollWorkflowTaskQueueResponse to expose aggregated workflow history size. This allows users and workflows to monitor history size for performance and make decisions like triggering continue-as-new operations.",
      "category": "feature",
      "subcategory": "workflow-history",
      "apis": [
        "DescribeWorkflowExecution",
        "PollWorkflowTaskQueueResponse",
        "WorkflowExecutionInfo"
      ],
      "components": [
        "api",
        "history",
        "workflow-execution"
      ],
      "concepts": [
        "history-size",
        "performance",
        "workflow-monitoring",
        "continue-as-new",
        "memory-management",
        "api-exposure"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently monitor or act on workflow history size, limiting their ability to optimize performance and prevent history bloat.",
      "rootCause": null,
      "proposedFix": "Expose history_size_bytes as a new field in WorkflowExecutionInfo and PollWorkflowTaskQueueResponse, and also expose it to workflow task started event so workflow code can access it.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented and merged to add history_size_bytes field to the specified API structures.",
      "related": [],
      "keyQuote": "Controlling a workflow history size is important to keep the system performant. None of the existing APIs exposes the aggregated size of the history.",
      "number": 1114,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:02:05.626Z"
    },
    {
      "summary": "Workflow visibility queries on large MySQL databases are extremely slow, taking 20-30 seconds to retrieve workflow lists from a namespace with 10 million visibility records. The query planner chooses the PRIMARY index resulting in a near table scan instead of using more efficient indexes.",
      "category": "bug",
      "subcategory": "visibility-query-performance",
      "apis": [],
      "components": [
        "visibility-database",
        "mysql-store",
        "workflow-list-query"
      ],
      "concepts": [
        "query-optimization",
        "index-selection",
        "database-performance",
        "pagination",
        "large-dataset"
      ],
      "severity": "high",
      "userImpact": "Users experience severe performance degradation (20-30 seconds) when viewing workflow lists in namespaces with large visibility databases, making the UI unusable at scale.",
      "rootCause": "MySQL query planner selects the PRIMARY index (involving 7.7M row scan with 3.55% filter rate) instead of more selective indexes like by_status_by_close_time, causing excessive disk I/O and sorting overhead.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Currently it takes around 20 to 30 seconds to view the list of workflows in a namespace that has nearly 10 million records in the visibility database.",
      "number": 1106,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:01:52.989Z"
    },
    {
      "summary": "Add built-in TokenKeyProvider implementation that supports both EC and RSA public signing keys, and handles JSON key responses without the optional x5c attribute.",
      "category": "feature",
      "subcategory": "authentication",
      "apis": [
        "TokenKeyProvider"
      ],
      "components": [
        "TokenKeyProvider",
        "key-management",
        "authentication"
      ],
      "concepts": [
        "JWT",
        "public-key-infrastructure",
        "EC-keys",
        "RSA-keys",
        "certificate-chain",
        "key-provider"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use JWT providers that only supply EC keys or omit the x5c certificate attribute, limiting authentication options.",
      "rootCause": "Current rsaKeyProvider only supports RSA keys and requires the x5c attribute, which is optional in the JSON response spec.",
      "proposedFix": "Implement a new TokenKeyProvider that supports both EC and RSA keys and instantiates keys from non-optional fields only.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented enhanced TokenKeyProvider supporting EC and RSA keys without x5c dependency.",
      "related": [],
      "keyQuote": "The build in TokenKeyProvider should support instantiating keys from the non-optional fields of the json response.",
      "number": 1090,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:01:55.191Z"
    },
    {
      "summary": "PostgreSQL database schema initialization fails on fresh Temporal database with 'relation schema_version does not exist' error when using AUTO_SETUP=true. The issue occurs because the regular temporal:1.4.1 image does not properly initialize the PostgreSQL schema, requiring the auto-setup image instead.",
      "category": "bug",
      "subcategory": "database-schema-setup",
      "apis": [],
      "components": [
        "database-schema",
        "postgresql-initialization",
        "auto-setup"
      ],
      "concepts": [
        "schema-initialization",
        "database-migration",
        "configuration",
        "docker-setup",
        "postgresql"
      ],
      "severity": "high",
      "userImpact": "Users cannot start Temporal Server on a fresh PostgreSQL database with the standard server image and AUTO_SETUP=true, requiring them to use the auto-setup image instead.",
      "rootCause": "The regular temporal-server:1.4.1 Docker image does not properly initialize the PostgreSQL schema when AUTO_SETUP=true is set; the schema_version table is not created.",
      "proposedFix": "Use the temporalio/auto-setup:1.4.1 Docker image instead of temporalio/server:1.4.1 for automatic database schema setup.",
      "workaround": "Use the auto-setup image (temporalio/auto-setup:1.4.1) for fresh database initialization.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by using the correct Docker image (auto-setup) that properly initializes the PostgreSQL schema.",
      "related": [],
      "keyQuote": "should be image: temporalio/auto-setup:1.4.1",
      "number": 1088,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:01:51.928Z"
    },
    {
      "summary": "The temporal-sql-tool is not idempotent - if it fails halfway through execution, re-running the same command fails because some tables were already created. This affects schema migrations from v0 to v1.0 and subsequent version upgrades, forcing users to manually drop and recreate the schema.",
      "category": "bug",
      "subcategory": "database-schema-setup",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "auto-setup",
        "schema-migration"
      ],
      "concepts": [
        "idempotency",
        "database-initialization",
        "error-recovery",
        "version-upgrade",
        "schema-versioning",
        "deployment"
      ],
      "severity": "high",
      "userImpact": "Users who experience connection failures or interruptions during schema setup must manually intervene to drop and recreate the database schema rather than simply rerunning the tool.",
      "rootCause": "The tool attempts to create tables without checking if they already exist, and the auto-setup script exits on errors rather than continuing with schema updates.",
      "proposedFix": "Make the temporal-sql-tool idempotent by checking if tables/schema versions already exist before attempting creation, and allow schema migration to proceed even if some steps have already been applied.",
      "workaround": "Manually drop and recreate the schema before retrying the command.",
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [
        1370
      ],
      "keyQuote": "We should also make sure that v1.0 --> v1.1 upgrade and so on are also idempotent",
      "number": 1087,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:01:40.955Z"
    },
    {
      "summary": "The Cassandra data backend should use a lower consistency level (LOCAL_QUORUM or QUORUM) instead of ALL when performing schema version checks during server startup, to improve reliability and performance.",
      "category": "other",
      "subcategory": "cassandra-persistence",
      "apis": [],
      "components": [
        "cassandra-backend",
        "schema-version-check",
        "startup"
      ],
      "concepts": [
        "consistency-level",
        "data-persistence",
        "server-initialization",
        "cassandra",
        "reliability"
      ],
      "severity": "medium",
      "userImpact": "Using ALL consistency level for schema checks can cause startup failures or performance degradation when Cassandra nodes are unavailable.",
      "rootCause": "Schema version check during startup uses ALL consistency level, which is overly strict for this non-critical operation.",
      "proposedFix": "Change consistency level to LOCAL_QUORUM or QUORUM for schema version checks during startup.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Cassandra data backend should not use consistency level `ALL` when doing schema version check during start up",
      "number": 1085,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:01:38.657Z"
    },
    {
      "summary": "Eliminate unnecessary deserialization and serialization cycles of workflow history events between frontend and SDKs. Currently, events are deserialized, then serialized, then deserialized again when passed to SDKs, which is inefficient and redundant.",
      "category": "feature",
      "subcategory": "workflow-history",
      "apis": [],
      "components": [
        "frontend",
        "history-events",
        "sdk-interface",
        "serialization"
      ],
      "concepts": [
        "performance-optimization",
        "serialization",
        "deserialization",
        "workflow-history",
        "data-pipeline",
        "efficiency"
      ],
      "severity": "medium",
      "userImpact": "Users experience improved performance and reduced CPU usage from eliminating redundant serialization/deserialization cycles in workflow history processing.",
      "rootCause": "Current architecture unnecessarily cycles workflow history events through multiple serialization stages instead of passing raw history events directly.",
      "proposedFix": "Use raw history events API to pass history directly from frontend to SDKs without intermediate serialization steps.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We should remove the unnecessary deserialized then serialized logic within frontend",
      "number": 1080,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T01:01:40.080Z"
    },
    {
      "summary": "Allow direct task injection into the database without requiring mutable state updates. This capability is needed for workflow record deletion in Elasticsearch, enabling the DB layer to handle tasks independently of mutable state management.",
      "category": "feature",
      "subcategory": "persistence-layer",
      "apis": [],
      "components": [
        "persistence",
        "db-layer",
        "workflow-execution",
        "elasticsearch"
      ],
      "concepts": [
        "task-injection",
        "mutable-state",
        "workflow-deletion",
        "zombie-state",
        "database-operations"
      ],
      "severity": "medium",
      "userImpact": "Enables more efficient deletion of workflow records by decoupling task injection from mutable state requirements.",
      "rootCause": "Current implementation requires mutable state to be present when injecting tasks, limiting flexibility in cleanup operations.",
      "proposedFix": "Modify the DB layer to provide functionality for directly injecting tasks without requiring mutable state updates, and re-enable zombie state validation checks.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented to allow direct task injection without mutable state, with zombie state checks re-enabled and test case resurrected.",
      "related": [
        988
      ],
      "keyQuote": "DB layer should provide the functionality to directly inject tasks without mutable state",
      "number": 1079,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:56:59.543Z"
    },
    {
      "summary": "Request to include cassandra-schema-tool binary in release artifacts. User questions whether the tool needs to be rebuilt when updating versions or only when .proto files are modified.",
      "category": "feature",
      "subcategory": "packaging",
      "apis": [],
      "components": [
        "cassandra-schema-tool",
        "release-artifacts"
      ],
      "concepts": [
        "packaging",
        "tooling",
        "distribution",
        "cassandra",
        "schema-management"
      ],
      "severity": "low",
      "userImpact": "Users must manually build cassandra-schema-tool instead of getting pre-built binaries from releases, increasing setup complexity.",
      "rootCause": null,
      "proposedFix": "Include pre-built cassandra-schema-tool binaries in release artifacts",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I expect the cassandra-schema-tool to be available in the release artifacts.",
      "number": 1075,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:57:00.892Z"
    },
    {
      "summary": "User's long-running workflow is being terminated after one day because the workflow history size/count exceeds the server's limit. The issue discusses the limitations of running workflows for years with continuous event accumulation.",
      "category": "question",
      "subcategory": "workflow-history-limits",
      "apis": [
        "Workflow.await"
      ],
      "components": [
        "history-service",
        "workflow-task-processor",
        "event-store"
      ],
      "concepts": [
        "workflow-history",
        "history-limits",
        "long-running-workflows",
        "event-accumulation",
        "continue-as-new",
        "history-size"
      ],
      "severity": "high",
      "userImpact": "Users cannot run workflows continuously for extended periods without hitting history size limits, causing unexpected workflow termination.",
      "rootCause": "Workflows accumulate events in history without pruning, and the server enforces a hard limit on history size/count per workflow execution.",
      "proposedFix": "Use continue-as-new pattern to start a new workflow execution and clear history periodically.",
      "workaround": "Implement continue-as-new to reset workflow history and prevent exceeding limits on long-running processes.",
      "resolution": "wontfix",
      "resolutionDetails": "User accepted the suggested workaround (continue-as-new) as the solution for managing long-running workflows.",
      "related": [],
      "keyQuote": "Workflow history size / count exceeds limit. - User's workflow was terminated with this message after running for one day.",
      "number": 1069,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:57:04.015Z"
    },
    {
      "summary": "Tasks are being dropped when polling requests are cancelled under load, particularly when workers are restarted during high activity scheduling. This causes activities and workflows to timeout instead of being dispatched to an available poller.",
      "category": "bug",
      "subcategory": "polling-cancellation",
      "apis": [],
      "components": [
        "poller",
        "worker",
        "grpc-channel",
        "task-dispatch"
      ],
      "concepts": [
        "task-dispatch",
        "polling",
        "cancellation",
        "worker-restart",
        "timeout",
        "connection-management"
      ],
      "severity": "high",
      "userImpact": "Activities and workflows timeout and require retries when workers are restarted during high concurrent activity load, causing delays and potential data inconsistencies.",
      "rootCause": "Tasks are being dispatched over cancelled GRPC channels when polling requests are cancelled, rather than being re-dispatched to available pollers or queued.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue could not be reproduced by the team; marked as not reproducible.",
      "related": [],
      "keyQuote": "poll requests are getting canceled and no tasks are received when this happens...they are dispatched over a cancelled GRPC channel",
      "number": 1058,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:56:46.868Z"
    },
    {
      "summary": "Temporal server accepts client connections before the default namespace is created during initial startup, causing workers to fail with 'Namespace default does not exist' error. The server should not accept connections until it's in a fully operable state with the default namespace initialized.",
      "category": "bug",
      "subcategory": "startup-initialization",
      "apis": [
        "NewClient",
        "Run"
      ],
      "components": [
        "frontend",
        "namespace-management",
        "initialization",
        "server-startup"
      ],
      "concepts": [
        "namespace-readiness",
        "graceful-startup",
        "initialization-order",
        "connection-handling",
        "default-namespace",
        "bootstrap"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably start workers when launching Temporal server for the first time without implementing retry logic, blocking initial setup workflows.",
      "rootCause": "Server listens for gRPC connections before the default namespace setup process completes during initialization.",
      "proposedFix": "Defer accepting client connections until after the default namespace is created and registered.",
      "workaround": "Implement retry logic in client/worker startup code to handle namespace-not-found errors, or use auto-setup Docker image with waiting mechanisms.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        1941
      ],
      "keyQuote": "On initial startup, Temporal accepts connections from clients, but then fails when a worker attempts to Run() with Namespace default does not exist.",
      "number": 1057,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:56:38.014Z"
    },
    {
      "summary": "TLS support for PostgreSQL was not properly configured, using 'verify-ca' mode instead of 'require' mode when host verification is disabled. The fix adds the missing SSL mode constant and corrects the SSL parameter setting logic.",
      "category": "bug",
      "subcategory": "postgresql-tls",
      "apis": [],
      "components": [
        "persistence",
        "sql-plugin",
        "postgresql",
        "tls-configuration"
      ],
      "concepts": [
        "tls",
        "ssl",
        "postgresql",
        "connection-security",
        "host-verification",
        "database-configuration"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal with PostgreSQL were unable to properly enable TLS encryption for database connections due to incorrect SSL mode configuration.",
      "rootCause": "The dsnTSL function was setting the wrong PostgreSQL SSL mode (verify-ca) when EnableHostVerification was false, instead of using the more secure 'require' mode.",
      "proposedFix": "Add postgreSQLSSLModeRequire constant and change the SSL mode to 'require' instead of 'verify-ca' when host verification is disabled.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was fixed by implementing the proposed solution to use the correct SSL mode for PostgreSQL TLS connections.",
      "related": [],
      "keyQuote": "TLS for PostgreSQL is not properly supported... sslParams.Set(postgreSQLSSLMode, postgreSQLSSLModeRequire)",
      "number": 1047,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:56:47.560Z"
    },
    {
      "summary": "Convert NDC RPC stack from periodic polling to gRPC stream functionality to reduce complexity in handling replication task throttling, lag management, and backpressure control.",
      "category": "feature",
      "subcategory": "replication",
      "apis": [],
      "components": [
        "NDC",
        "RPC stack",
        "replication",
        "gRPC"
      ],
      "concepts": [
        "streaming",
        "replication tasks",
        "throttling",
        "backpressure",
        "lag management",
        "shard isolation"
      ],
      "severity": "medium",
      "userImpact": "Improves reliability and efficiency of cross-cluster replication by simplifying control flow and reducing replication lag.",
      "rootCause": "Periodic polling approach requires fine-grained manual control over wait times, throttling, and lag monitoring that could be simplified with stream-based approach.",
      "proposedFix": "Replace periodic query-based approach with gRPC stream functionality to handle replication task delivery with built-in backpressure.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented gRPC stream-based approach for NDC replication",
      "related": [],
      "keyQuote": "Using stream functionality can potentially reduce the above complexity",
      "number": 1046,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:51:58.009Z"
    },
    {
      "summary": "Unify the common.Daemon interface and abstract the repeated start/stop logic with atomic compare-and-swap pattern that is scattered throughout the server codebase into a reusable common library.",
      "category": "feature",
      "subcategory": "refactoring-daemon-pattern",
      "apis": [],
      "components": [
        "daemon",
        "common-library",
        "lifecycle-management"
      ],
      "concepts": [
        "atomic-operations",
        "thread-safety",
        "idempotence",
        "lifecycle-management",
        "code-reuse",
        "initialization-pattern"
      ],
      "severity": "low",
      "userImpact": "Reduces code duplication and improves maintainability for internal server components implementing background threads.",
      "rootCause": "Daemon start/stop logic with atomic compare-and-swap patterns is duplicated across multiple structs in the codebase.",
      "proposedFix": "Abstract the common daemon initialization and cleanup logic into a reusable common library module.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The daemon pattern logic was unified and abstracted into a common library.",
      "related": [],
      "keyQuote": "To guarantee the at most once execution, following logic is added... The above logic is scattered within the entire server repo, we should abstract a common lib and reuse",
      "number": 1043,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:51:59.155Z"
    },
    {
      "summary": "Temporal server should support upgrade paths from major.minor.patch to major.minor+1.patch versions. This is a feature request to ensure compatibility and provide a clear upgrade strategy for users moving between minor version releases.",
      "category": "feature",
      "subcategory": "versioning",
      "apis": [],
      "components": [
        "server",
        "upgrade-manager",
        "version-handler"
      ],
      "concepts": [
        "upgrade-path",
        "versioning",
        "compatibility",
        "minor-version",
        "major-version",
        "patch-version"
      ],
      "severity": "medium",
      "userImpact": "Users need a guaranteed upgrade path between minor version releases to safely deploy new versions of Temporal server.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal server should ensure major.minor.patch to major.minor+1.patch upgrade path",
      "number": 1041,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:52:00.805Z"
    },
    {
      "summary": "Request to add support for sending batch workflow signals instead of only one signal at a time. Currently, signal workflow and signal with start workflow APIs only accept a single signal, but users need the ability to send multiple signals together.",
      "category": "feature",
      "subcategory": "workflow-signals",
      "apis": [
        "SignalWorkflow",
        "SignalWithStartWorkflow"
      ],
      "components": [
        "workflow-engine",
        "signal-dispatcher",
        "client-api"
      ],
      "concepts": [
        "signaling",
        "batch-operations",
        "workflow-interaction",
        "bulk-requests",
        "asynchronous-communication"
      ],
      "severity": "low",
      "userImpact": "Users cannot efficiently send multiple signals to a workflow in a single operation, requiring multiple API calls instead.",
      "rootCause": null,
      "proposedFix": "Extend signal workflow and signal with start workflow APIs to accept batch/multiple signals",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Currently, the signal workflow / signal with start workflow only allow one and only one signal, we should consider allowing batch of signals",
      "number": 1036,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:51:46.407Z"
    },
    {
      "summary": "Request to emit workflow execution lock wait time as metrics. The enhancement was implemented by adding metric emissions in the history cache component.",
      "category": "feature",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "history-cache",
        "metrics"
      ],
      "concepts": [
        "workflow-execution",
        "lock-contention",
        "performance-metrics",
        "instrumentation"
      ],
      "severity": "low",
      "userImpact": "Users can now monitor workflow execution lock contention through metrics, improving visibility into system performance.",
      "rootCause": null,
      "proposedFix": "Emit workflow execution lock wait time metrics from the history cache component",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Metrics were implemented and added to historyCache.go",
      "related": [],
      "keyQuote": "metrics already emitted: https://github.com/temporalio/temporal/blob/master/service/history/historyCache.go#L169",
      "number": 1035,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:51:44.957Z"
    },
    {
      "summary": "Migrate AddWorkflowTask and AddActivityTask APIs to use task expiration time instead of TTL duration. This is a two-version migration where the first version sets both expiration time and TTL (matching service prefers expiration), and the second version deprecates TTL entirely.",
      "category": "feature",
      "subcategory": "task-expiration",
      "apis": [
        "AddWorkflowTask",
        "AddActivityTask"
      ],
      "components": [
        "history-service",
        "matching-service",
        "mutable-state"
      ],
      "concepts": [
        "task-expiration",
        "ttl-migration",
        "timeout",
        "backward-compatibility",
        "api-deprecation"
      ],
      "severity": "medium",
      "userImpact": "Improves task timeout handling by using absolute expiration times instead of relative durations, reducing timing-related bugs and improving API clarity.",
      "rootCause": "Current TTL duration approach is less precise than using absolute expiration times for task deadline management.",
      "proposedFix": "Two-phase migration: Phase 1 persists concrete activity timeout as expiration time, sets both expiration time and TTL (matching service prefers expiration); Phase 2 deprecates TTL duration proto definition and uses only expiration time.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Migration implemented across two versions to maintain backward compatibility during transition from TTL duration to expiration time.",
      "related": [],
      "keyQuote": "The migration should be done in 2 versions... matching service should use expiration time over TTL duration.",
      "number": 1027,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:51:44.817Z"
    },
    {
      "summary": "Request to add a CLI command and web dashboard tab to display all task queues and their associated pollers. Currently, pollers can only be viewed by navigating through a workflow detail page, making it difficult to debug dynamically registered task queues.",
      "category": "feature",
      "subcategory": "taskqueue-management",
      "apis": [],
      "components": [
        "web-dashboard",
        "cli-tctl",
        "task-queue-service"
      ],
      "concepts": [
        "debugging",
        "task-queues",
        "pollers",
        "visibility",
        "dynamic-registration"
      ],
      "severity": "medium",
      "userImpact": "Developers and operators cannot easily discover and monitor all task queues and their active pollers without drilling through workflow details.",
      "rootCause": null,
      "proposedFix": "Add a new 'taskqueues' tab in the web dashboard (before 'settings') and a corresponding TCTL CLI command to list all task queues with their pollers and metadata.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "taskQueues are dynamically registered in Temporal, it can be a great debugger option to check the actual taskQueues",
      "number": 1024,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:47:00.645Z"
    },
    {
      "summary": "Temporal worker needs a background maintenance workflow to clean up stale queue tasks (workflow and activity tasks) even in Cassandra, where tasks are considered stale if their workflow is finished and no poller is actively polling the task queue.",
      "category": "feature",
      "subcategory": "maintenance-workflow",
      "apis": [],
      "components": [
        "worker",
        "scanner",
        "task-queue",
        "cassandra"
      ],
      "concepts": [
        "stale-tasks",
        "cleanup",
        "maintenance",
        "queue-management",
        "background-workflow",
        "task-polling"
      ],
      "severity": "medium",
      "userImpact": "Without automatic cleanup, stale tasks accumulate in task queues, potentially degrading system performance and consuming storage resources.",
      "rootCause": null,
      "proposedFix": "Implement a maintenance workflow similar to the existing scanner service (service/worker/scanner) to identify and remove stale tasks from queues.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "A task is considered stale if corresponding workflow is finished and no poller is polling the corresponding task queue.",
      "number": 1021,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:46:56.825Z"
    },
    {
      "summary": "DescribeWorkflowExecution API result does not indicate when a workflow cancel has been requested. The feature request asks for adding a CANCEL_REQUESTED status or additional property to the DescribeWorkflowExecutionResult to distinguish between canceled and canceling states.",
      "category": "feature",
      "subcategory": "workflow-status",
      "apis": [
        "DescribeWorkflowExecution",
        "DescribeWorkflowExecutionResult"
      ],
      "components": [
        "workflow-execution",
        "api",
        "status-management"
      ],
      "concepts": [
        "workflow-status",
        "cancellation",
        "state-tracking",
        "execution-visibility",
        "workflow-lifecycle"
      ],
      "severity": "medium",
      "userImpact": "Users cannot determine through the API whether a workflow cancellation was requested, making it difficult to track workflow lifecycle state accurately.",
      "rootCause": null,
      "proposedFix": "Add CANCEL_REQUESTED status or additional property to the DescribeWorkflowExecutionResult.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "DescribeWorkflowExecution result doesn't indicate if workflow cancel was requested.",
      "number": 1018,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:46:58.743Z"
    },
    {
      "summary": "Request to support pausing and resuming Temporal services during blue/green deployments. Users need to put services into a disabled state during rollouts where infrastructure remains hot but doesn't process work, enabling faster rollback without full service restart.",
      "category": "feature",
      "subcategory": "service-lifecycle",
      "apis": [],
      "components": [
        "frontend",
        "worker-factory",
        "service-api"
      ],
      "concepts": [
        "deployment",
        "blue-green",
        "service-state",
        "traffic-control",
        "operational-agility",
        "rollback"
      ],
      "severity": "medium",
      "userImpact": "Users performing blue/green deployments of Temporal services need a way to pause work processing during rollouts to enable faster rollback without full infrastructure teardown.",
      "rootCause": null,
      "proposedFix": "An extension point similar to WorkerFactory's suspendPolling/enablePolling, or an API to send signals to the service process to enable/disable work processing.",
      "workaround": "Deploy a sidecar that subscribes to Discovery status change events and sends sigstop/sigcont signals, though this breaks down in containerized environments like Titus.",
      "resolution": "duplicate",
      "resolutionDetails": "Closed in favor of issue #3006 which provides a more comprehensive solution for pausing/resuming work.",
      "related": [
        3006
      ],
      "keyQuote": "I'd like an extension point that would allow us to write a listener into the services themselves to actuate an enable/disabled state for the process",
      "number": 1012,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:46:42.941Z"
    },
    {
      "summary": "Request to add an extensibility mechanism allowing users to pass in custom metrics reporters instead of being limited to built-in statsd, prometheus, and m3db backends. This would enable organizations to use proprietary or less common metrics backends without needing to contribute to the Temporal open source repository.",
      "category": "feature",
      "subcategory": "metrics-extensibility",
      "apis": [],
      "components": [
        "metrics-reporter",
        "configuration",
        "extension-mechanism"
      ],
      "concepts": [
        "extensibility",
        "plugin-system",
        "custom-backend",
        "metrics-integration",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users can now implement custom metrics reporters for unsupported backends without forking or contributing to the Temporal repository.",
      "rootCause": null,
      "proposedFix": "Add extension point in Temporal for users to pass in custom metric reporters, similar to the existing extension points for custom Authorizer implementations.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Extension mechanism was implemented allowing custom metrics reporters to be plugged in through configuration.",
      "related": [
        998
      ],
      "keyQuote": "Add extension point in Temporal for users to pass in custom metric reporters. We already have well defined extension points to plugin in custom Authorizer.",
      "number": 1008,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:46:43.241Z"
    },
    {
      "summary": "tctl admin database commands fail on Aurora MySQL due to an unknown 'transaction_isolation' system variable. The service config supports connection attributes to work around this, but tctl lacks a mechanism to pass these attributes.",
      "category": "bug",
      "subcategory": "cli-database",
      "apis": [],
      "components": [
        "tctl",
        "metadata-manager",
        "database-connector",
        "mysql-driver"
      ],
      "concepts": [
        "database-connection",
        "mysql-compatibility",
        "aurora-mysql",
        "transaction-isolation",
        "connection-attributes",
        "cli-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use tctl admin commands with Aurora MySQL databases without encountering connection errors.",
      "rootCause": "Aurora MySQL doesn't support the 'transaction_isolation' system variable that the standard MySQL driver attempts to set, and tctl provides no way to configure connection attributes like the service config does.",
      "proposedFix": "Add support for connection attributes in tctl (similar to --ca flag suggested in comments) or automatically handle Aurora MySQL compatibility.",
      "workaround": "Service config supports connectAttributes: {tx_isolation: \"READ-COMMITTED\"} but no equivalent exists for tctl.",
      "resolution": "invalid",
      "resolutionDetails": "Issue labeled with 'close-after-30-days' suggesting closure due to age/inactivity rather than resolution of the underlying problem.",
      "related": [],
      "keyQuote": "The `transaction_isolation` variable issue remains to be an issue with Aurora MySQL. We are able to overcome this in the regular service config by specifying connection attributes",
      "number": 1004,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:46:44.923Z"
    },
    {
      "summary": "Feature request to allow workflow retention to be specified in finer-grained units (hours) instead of just days. Discussion includes concerns about implications for global namespaces with replication.",
      "category": "feature",
      "subcategory": "workflow-retention",
      "apis": [],
      "components": [
        "retention-policy",
        "namespace-configuration",
        "replication"
      ],
      "concepts": [
        "retention",
        "time-granularity",
        "global-namespace",
        "replication",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users can now configure workflow retention with hourly precision instead of daily, with appropriate safeguards for replicated namespaces.",
      "rootCause": "Retention granularity is currently hardcoded to days, limiting flexibility for users with different retention needs.",
      "proposedFix": "Implement finer-grained retention units (hours) while restricting very small retention periods (less than 1 day) to non-global/non-replicated namespaces to avoid replication failures.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature implemented with safeguards to prevent replication issues in global namespaces with very small retention periods.",
      "related": [],
      "keyQuote": "maybe we should only allow non-global namespace (those does not enable replications) to have small retention (less than 1 day)?",
      "number": 1000,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:31:15.480Z"
    },
    {
      "summary": "Request for multiple API enhancements to support Temporal Web, including combined OPEN+CLOSED workflow queries, backwards pagination, server-side event filtering in History, and parent workflow to child workflow discovery.",
      "category": "feature",
      "subcategory": "web-api",
      "apis": [
        "History"
      ],
      "components": [
        "api",
        "workflow-query",
        "history-service",
        "mutable-state"
      ],
      "concepts": [
        "pagination",
        "filtering",
        "parent-child-workflows",
        "event-types",
        "workflow-visibility"
      ],
      "severity": "medium",
      "userImpact": "Temporal Web lacks APIs for efficient workflow discovery and filtering, requiring workarounds like client-side filtering and multiple API calls.",
      "rootCause": null,
      "proposedFix": "Implement server-side API endpoints for: (1) combined OPEN+CLOSED workflow queries, (2) backwards pagination, (3) event-type filtering in History, (4) parent-to-child workflow discovery via mutableState ID tracking.",
      "workaround": "Client-side filtering of event types; making multiple API calls for OPEN and CLOSED workflows separately.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Given a parent workflow, find all the child workflows - We want to make it easy to find the child workflows of a parent workflow in Temporal Web.",
      "number": 996,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:31:16.263Z"
    },
    {
      "summary": "GetLastCompletionResult returns null when a cron workflow times out or fails in the previous run, instead of flowing the result from the actual last completion. The completion result should be preserved across cron reruns regardless of timeout/failure status.",
      "category": "bug",
      "subcategory": "cron-workflow",
      "apis": [
        "GetLastCompletionResult"
      ],
      "components": [
        "cron-workflow",
        "workflow-execution",
        "state-management"
      ],
      "concepts": [
        "cron-job",
        "timeout",
        "failure-handling",
        "state-preservation",
        "workflow-retry",
        "completion-result"
      ],
      "severity": "medium",
      "userImpact": "Users relying on GetLastCompletionResult for cron workflows lose access to completion data when the previous run times out or fails, breaking business logic that depends on this state.",
      "rootCause": "GetLastCompletionResult does not properly flow results from the last actual completion when the immediately preceding run terminated due to timeout or failure.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved to properly flow LastCompletionResult from the most recent successful completion in cron workflows, even when intermediate runs timeout or fail.",
      "related": [],
      "keyQuote": "Flow LastCompletionResult from previous run if the cron workflow times out",
      "number": 992,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:31:17.976Z"
    },
    {
      "summary": "Request to allow force-completing activities that are retrying or backing off without restarting the workflow. The RespondActivityTaskCompletedById API needs to support retry attempts and allow completion when activities are in backoff state.",
      "category": "feature",
      "subcategory": "activity-retry",
      "apis": [
        "RespondActivityTaskCompleted",
        "RespondActivityTaskCompletedById"
      ],
      "components": [
        "activity-executor",
        "retry-handler",
        "server-api"
      ],
      "concepts": [
        "retry",
        "activity-completion",
        "backoff",
        "error-recovery",
        "cascading-failure"
      ],
      "severity": "high",
      "userImpact": "Users cannot recover from activity failures without restarting entire workflows when activities are retrying or backing off, causing cascading failures in dependent activities.",
      "rootCause": "RespondActivityTaskCompletedById API does not support completing activities during retry backoff state, and lacks retry attempt parameter.",
      "proposedFix": "Update RespondActivityTaskCompleted and RespondActivityTaskCompletedById to accept activity completions even when activity is backing off by default, and add retry attempt parameter support.",
      "workaround": "Reset workflow to just before the failed activity was scheduled, or restart workflow from beginning.",
      "resolution": "fixed",
      "resolutionDetails": "Server behavior updated to accept activity completions even if activity is currently backing off. Closed by PR #5724, though some edge cases noted in #5877.",
      "related": [
        5877
      ],
      "keyQuote": "Provide a mechanism to force complete an activity in retry without restarting the workflow.",
      "number": 987,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:31:04.927Z"
    },
    {
      "summary": "User reports significant performance degradation when increasing concurrent load in Temporal workflows, with scheduling time increasing from 1s (10 concurrent) to 10s (100 concurrent). Seeks configuration options to improve performance.",
      "category": "question",
      "subcategory": "performance-tuning",
      "apis": [],
      "components": [
        "scheduler",
        "worker",
        "local-activity"
      ],
      "concepts": [
        "performance",
        "scheduling-latency",
        "concurrency",
        "load-testing",
        "throughput"
      ],
      "severity": "medium",
      "userImpact": "Users unable to achieve acceptable performance under moderate to high concurrent load without configuration guidance.",
      "rootCause": "Likely bottleneck in Temporal server's scheduling mechanism under increased concurrency, not bandwidth/CPU/disk constraints.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed with recommendation to use Temporal's load testing tools (Maru) and community forums for performance optimization guidance rather than code-level configuration changes.",
      "related": [],
      "keyQuote": "The whole process is about 10s... It seems that the bottleneck is not in bandwidth, cpu, disk, disk io...",
      "number": 984,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:31:03.955Z"
    },
    {
      "summary": "Error messages from S3 archival failures are not properly propagated when creating namespaces, showing only generic \"Forbidden\" errors without indicating the underlying S3 access key misconfiguration.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [
        "RegisterNamespace"
      ],
      "components": [
        "frontend",
        "archival",
        "error-logging",
        "namespace-handler"
      ],
      "concepts": [
        "error-propagation",
        "stack-trace",
        "s3-integration",
        "error-context",
        "logging",
        "diagnostics"
      ],
      "severity": "medium",
      "userImpact": "Users cannot diagnose archival configuration issues because underlying S3 errors are masked by generic 403 responses.",
      "rootCause": "Error handling in RegisterNamespace does not chain or expose underlying archival/S3 errors, only returning the final HTTP status code.",
      "proposedFix": null,
      "workaround": "Set TEMPORAL_CLI_SHOW_STACKS=1 environment variable to see more detailed stack traces.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Error should have underlying cause... Archival related errors do not get shown up in create name space calls",
      "number": 983,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:31:03.569Z"
    },
    {
      "summary": "History service panics during startup with PostgreSQL/CRDB 20.1.x when creating workflow executions due to a reflection error in the sqlx library used for SQL binding. The issue was resolved by updating the sqlx dependency with a temporal workaround.",
      "category": "bug",
      "subcategory": "persistence-sql",
      "apis": [
        "StartWorkflowExecution",
        "CreateWorkflowExecution"
      ],
      "components": [
        "persistence-sql",
        "sqlplugin-postgresql",
        "history-service"
      ],
      "concepts": [
        "database-panic",
        "dependency-version",
        "sql-binding",
        "reflection-error",
        "startup-failure"
      ],
      "severity": "critical",
      "userImpact": "Services cannot start when using PostgreSQL/CRDB, blocking all workflow execution.",
      "rootCause": "sqlx library version v1.2.0 has a reflection bug that causes panic during struct field binding in database operations. Temporal had a workaround but it wasn't applied when used as a dependency.",
      "proposedFix": "Override the sqlx dependency with the workaround version in go.mod: replace github.com/jmoiron/sqlx v1.2.0 => github.com/longquanzheng/sqlx v0.0.0-20191125235044-053e6130695c",
      "workaround": "Add the replace directive to go.mod to use the patched sqlx version instead of the standard v1.2.0",
      "resolution": "fixed",
      "resolutionDetails": "User applied the sqlx dependency override workaround documented in Temporal's go.mod, which resolved the panic.",
      "related": [
        2863
      ],
      "keyQuote": "Adding that fixed things. Thanks!",
      "number": 978,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:30:51.651Z"
    },
    {
      "summary": "Remove unnecessary type conversions in the mutable state builder. The code at lines 3900-3913 in mutableStateBuilder.go contains conversions that serve no purpose and should be eliminated.",
      "category": "feature",
      "subcategory": "code-quality",
      "apis": [],
      "components": [
        "mutableStateBuilder",
        "history-service"
      ],
      "concepts": [
        "refactoring",
        "code-simplification",
        "type-conversion",
        "technical-debt"
      ],
      "severity": "low",
      "userImpact": "No direct user impact; this is an internal code cleanup that improves maintainability.",
      "rootCause": "Unnecessary type conversions exist in mutableStateBuilder.go that don't contribute to functionality.",
      "proposedFix": "Remove the conversion logic at lines 3900-3913 in mutableStateBuilder.go.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The unnecessary conversions were removed from the mutable state builder.",
      "related": [],
      "keyQuote": "Above conversions are unnecessary",
      "number": 977,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:30:52.019Z"
    },
    {
      "summary": "Feature request to enhance the convert rate limiter to allow burst capability, enabling temporary rate exceeding during conversion operations.",
      "category": "feature",
      "subcategory": "rate-limiting",
      "apis": [],
      "components": [
        "rate-limiter",
        "convert-service"
      ],
      "concepts": [
        "rate-limiting",
        "burst",
        "throttling",
        "conversion",
        "performance"
      ],
      "severity": "low",
      "userImpact": "Users cannot temporarily exceed rate limits during conversion operations, potentially slowing down bulk conversion tasks.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was already solved as indicated by the author's comment.",
      "related": [],
      "keyQuote": "already solved",
      "number": 976,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:30:50.554Z"
    },
    {
      "summary": "Request to add support for batch activity execution to reduce unnecessary command tasks and improve performance when executing multiple activities.",
      "category": "feature",
      "subcategory": "activity-execution",
      "apis": [],
      "components": [
        "activity-executor",
        "command-processor",
        "workflow-engine"
      ],
      "concepts": [
        "batch-processing",
        "performance-optimization",
        "activity-execution",
        "command-consolidation",
        "throughput"
      ],
      "severity": "medium",
      "userImpact": "Users executing multiple activities sequentially experience unnecessary command overhead that batching could reduce.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Add support of batch activity execution and batch wait to same unnecessary command tasks",
      "number": 973,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:30:38.182Z"
    },
    {
      "summary": "Design discussion about marking Cassandra records as deleted instead of actually deleting them to address potential tombstone performance issues in the mutable state machine.",
      "category": "feature",
      "subcategory": "persistence",
      "apis": [],
      "components": [
        "mutable-state",
        "cassandra",
        "state-machine"
      ],
      "concepts": [
        "deletion",
        "tombstone",
        "soft-delete",
        "data-persistence",
        "performance-optimization",
        "cassandra-optimization"
      ],
      "severity": "medium",
      "userImpact": "Addresses Cassandra tombstone issues that could impact performance when deleting entities in the state machine.",
      "rootCause": "Cassandra tombstone accumulation from hard deletes of records",
      "proposedFix": "Mark records as deleted instead of physically deleting them from Cassandra",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "mark the record as deleted, to deal with possible Cassandra tombstone issue",
      "number": 972,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:30:36.832Z"
    },
    {
      "summary": "Optimize SQL query supporting batch delete operations to improve performance and efficiency when deleting multiple records.",
      "category": "feature",
      "subcategory": "database-performance",
      "apis": [],
      "components": [
        "database",
        "sql-queries",
        "batch-operations"
      ],
      "concepts": [
        "performance",
        "optimization",
        "batch-delete",
        "sql-efficiency",
        "query-performance"
      ],
      "severity": "medium",
      "userImpact": "Users benefit from faster deletion operations when processing large batches of records, reducing server load and improving overall system responsiveness.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved through PR #982 which implemented the SQL query optimization for batch delete operations.",
      "related": [
        982
      ],
      "keyQuote": "solve by #982",
      "number": 971,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:30:40.329Z"
    },
    {
      "summary": "User encountered ResourceExhausted error when sending large payloads (~13MB) through Temporal, exceeding the default max message size of 4MB. Request for ability to configure max payload size limit.",
      "category": "feature",
      "subcategory": "payload-limits",
      "apis": [],
      "components": [
        "grpc",
        "message-handling",
        "payload-serialization"
      ],
      "concepts": [
        "message-size",
        "resource-limits",
        "payload-capacity",
        "configuration",
        "grpc-limits"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to send large payloads through Temporal workflows encounter ResourceExhausted errors and cannot process responses.",
      "rootCause": "Default gRPC message size limit (4MB) is exceeded by large payloads (13MB+), causing message reception to fail.",
      "proposedFix": null,
      "workaround": "Avoid sending large objects through Temporal (anti-pattern per maintainer comment).",
      "resolution": "fixed",
      "resolutionDetails": "Response payload message size limit was increased by maintainers to accommodate larger payloads.",
      "related": [],
      "keyQuote": "serviceerror.ResourceExhausted: grpc: received message larger than max (13335484 vs. 4194304)",
      "number": 961,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:30:25.630Z"
    },
    {
      "summary": "Request to support in-memory database option and optimize Temporal for edge/IoT deployments with limited resources (2 cores, 2GB RAM). Users need documentation and a simpler setup for running Temporal with good throughput in resource-constrained environments.",
      "category": "feature",
      "subcategory": "deployment-configuration",
      "apis": [],
      "components": [
        "database",
        "temporal-server",
        "packaging"
      ],
      "concepts": [
        "resource-constraints",
        "edge-computing",
        "in-memory-storage",
        "iot",
        "minimal-setup",
        "throughput"
      ],
      "severity": "medium",
      "userImpact": "Organizations running Temporal on edge/IoT devices with limited hardware resources cannot currently deploy Temporal efficiently, limiting adoption in resource-constrained environments.",
      "rootCause": null,
      "proposedFix": "Develop in-memory database option for Temporal and create simplified binary distribution optimized for limited resource scenarios with supporting documentation.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We consider that perhaps an in-memory DB along with simple binary (temporal) would be a better choice for executing workflows with good throughput.",
      "number": 960,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:30:26.901Z"
    },
    {
      "summary": "Remove unused WorkflowExecutionTerminatedEventAttributes and Cause field from WorkflowExecutionCancelRequestedEventAttributes to clean up the codebase.",
      "category": "other",
      "subcategory": "code-cleanup",
      "apis": [],
      "components": [
        "event-attributes",
        "workflow-execution"
      ],
      "concepts": [
        "unused-code",
        "refactoring",
        "cleanup",
        "code-quality",
        "deprecation"
      ],
      "severity": "low",
      "userImpact": "No direct user impact; this is internal code cleanup that improves codebase maintainability.",
      "rootCause": null,
      "proposedFix": "Remove unused WorkflowExecutionTerminatedEventAttributes and Cause field from WorkflowExecutionCancelRequestedEventAttributes",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "PR was submitted by vaibhavyadav-dev and approved by yycptt, ready to be landed after tests pass",
      "related": [],
      "keyQuote": "Yeah that sounds like a great first issue. Thanks @vaibhavyadav-dev!",
      "number": 958,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:30:25.361Z"
    },
    {
      "summary": "Request for documentation on multi-region deployment architecture for Temporal, including strategies for scaling globally with region-aware storage backends like CockroachDB while managing consistency trade-offs.",
      "category": "docs",
      "subcategory": "multi-region-deployment",
      "apis": [],
      "components": [
        "database",
        "storage-backend",
        "server-configuration"
      ],
      "concepts": [
        "multi-region",
        "geo-partitioning",
        "locality-awareness",
        "consistency",
        "latency",
        "high-availability",
        "global-scaling"
      ],
      "severity": "medium",
      "userImpact": "Organizations requiring multi-region deployments with low-latency local access cannot effectively implement Temporal without documented guidance on architecture and trade-offs.",
      "rootCause": null,
      "proposedFix": "Create documentation covering multi-region deployment patterns, region-aware database configuration (particularly CockroachDB geo-partitioning), consistency considerations, and locality awareness strategies.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I strongly consider using Temporal in my organization but one strong requirement is to support multiple regions (especially with CockroachDB as storage backend) to service local clients within low latency.",
      "number": 954,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:15:00.342Z"
    },
    {
      "summary": "The CLI 'namespace update' command incorrectly reports success when users specify a retention period in a non-day format (e.g., '0h10m0s'). The command fails to parse non-integer retention values but does not indicate the failure, leaving users unaware that the update did not occur.",
      "category": "bug",
      "subcategory": "cli-namespace-management",
      "apis": [],
      "components": [
        "cli",
        "namespace-update",
        "flag-parsing"
      ],
      "concepts": [
        "retention-period",
        "flag-validation",
        "error-handling",
        "user-feedback",
        "input-parsing"
      ],
      "severity": "medium",
      "userImpact": "Users receive misleading success messages when attempting to update namespace retention with non-day durations, leading to confusion and undetected configuration failures.",
      "rootCause": "The CLI flag parser only accepts integer values for the '--retention' flag and fails silently when given duration strings, without reporting an error to the user.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in version 1.16.1 where the namespace update command now properly handles duration formats like '0h10m0s' and reports the correct retention period.",
      "related": [],
      "keyQuote": "Notice that the CLI reported success when in fact the retention period had not been updated (presumably because it failed to parse the supplied retention value as an integer).",
      "number": 944,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:15:01.929Z"
    },
    {
      "summary": "CLI namespace register and update commands need to support sub-day retention period granularity (at least to the minute level) instead of only accepting day-level values.",
      "category": "feature",
      "subcategory": "cli-namespace-operations",
      "apis": [],
      "components": [
        "cli",
        "namespace-management",
        "retention-policy"
      ],
      "concepts": [
        "retention-period",
        "granularity",
        "namespace-configuration",
        "cli-flags",
        "time-units"
      ],
      "severity": "low",
      "userImpact": "Testing and operational scenarios require fine-grained retention period control which is currently unavailable through the CLI.",
      "rootCause": null,
      "proposedFix": "Support time duration formats like '30h' or other sub-day granular values in the --retention flag for namespace register and update commands.",
      "workaround": "Retention can be specified using time duration format (e.g., '--retention 30h') instead of just days.",
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented - the --retention flag supports duration formats allowing sub-day granularity.",
      "related": [],
      "keyQuote": "Retention could be specified as `--retention 30h` for example",
      "number": 943,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:15:03.952Z"
    },
    {
      "summary": "Unify TTL (Time-To-Live) handling logic across SQL and NoSQL persistence backends. Currently these implementations handle record TTL differently, and the goal is to standardize the approach so both follow the same pattern already established in the SQL worker setup.",
      "category": "feature",
      "subcategory": "persistence-ttl",
      "apis": [],
      "components": [
        "persistence-layer",
        "sql-backend",
        "nosql-backend",
        "worker"
      ],
      "concepts": [
        "ttl",
        "record-lifecycle",
        "database-cleanup",
        "persistence-abstraction",
        "configuration-consistency"
      ],
      "severity": "medium",
      "userImpact": "Inconsistent TTL handling between SQL and NoSQL backends creates confusion for users and increases maintenance burden across different database implementations.",
      "rootCause": "SQL and NoSQL persistence implementations evolved separately with different TTL management strategies.",
      "proposedFix": "NoSQL backend should adopt the same TTL handling pattern already implemented in SQL with corresponding worker setup.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Unify all persistence TTL handling logic since currently SQL / NoSQL record TTL are handled differently.",
      "number": 940,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:47.385Z"
    },
    {
      "summary": "Consolidate all persistence layer serialization and deserialization logic into a unified location instead of having it scattered across SQL and NoSQL implementations.",
      "category": "feature",
      "subcategory": "persistence-serialization",
      "apis": [],
      "components": [
        "persistence-layer",
        "sql-backend",
        "nosql-backend",
        "serialization"
      ],
      "concepts": [
        "serialization",
        "deserialization",
        "code-consolidation",
        "architectural-refactoring",
        "persistence",
        "data-consistency"
      ],
      "severity": "medium",
      "userImpact": "Consolidating serialization logic would reduce code duplication and maintenance burden for the persistence layer across different storage backends.",
      "rootCause": null,
      "proposedFix": "Unify all persistence serialization/deserialization logic into a single centralized location.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Unify all persistence serialization / deserialization login into one place, instead of letting SQL / NoSQL layer handle separately",
      "number": 939,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:45.923Z"
    },
    {
      "summary": "Workflows with cron or retry policies emit ContinueAsNew instead of workflow_failed metrics, causing confusing and missing failure reporting that doesn't accurately reflect actual workflow failures.",
      "category": "bug",
      "subcategory": "metrics",
      "apis": [],
      "components": [
        "metrics",
        "workflow-execution",
        "retry-policy",
        "cron-policy"
      ],
      "concepts": [
        "metrics",
        "workflow-completion",
        "retry",
        "cron",
        "failure-reporting",
        "observability"
      ],
      "severity": "medium",
      "userImpact": "Users cannot accurately monitor workflow failures when retry or cron policies are active, leading to incomplete observability and confusing metrics interpretation.",
      "rootCause": "Temporal emits ContinueAsNew event for workflow completions when retry or cron policy is attached, instead of emitting workflow_failed counter, misrepresenting the actual workflow state.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue references the code location in workflow/transaction_impl.go#L466 where the metric emission logic needs to be corrected to emit workflow completion metrics properly on ContinueAsNew.",
      "related": [],
      "keyQuote": "Temporal converts completion of workflow into ContinueAsNew event if there is a cron or retry policy attached to the execution. This is confusing as workflow failure is not reported as `workflow_failed` counter",
      "number": 937,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:48.671Z"
    },
    {
      "summary": "Request to provide prebuilt binary releases for Temporal server, admin tools, and web UI to simplify deployment and installation for users.",
      "category": "feature",
      "subcategory": "build-distribution",
      "apis": [],
      "components": [
        "temporal-server",
        "temporal-admin-tools",
        "temporal-web"
      ],
      "concepts": [
        "binary-distribution",
        "release-packaging",
        "deployment-simplification",
        "installation"
      ],
      "severity": "low",
      "userImpact": "Users currently must build Temporal components from source, which adds friction to getting started and deployment.",
      "rootCause": null,
      "proposedFix": "Provide prebuilt binary releases for temporal, temporal-admin-tools, and temporal-web with each release.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Prebuilt binaries are now provided as part of the standard release process.",
      "related": [],
      "keyQuote": "is it possible to add prebuild binaries of temporal, temporal-admin-tools, and temporal-web for release?",
      "number": 927,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:33.793Z"
    },
    {
      "summary": "Feature request to add configurable initialDelay for CRON job setup to control when the first run executes. Currently, the initial delay is automatically calculated and unpredictable; users need explicit configuration options.",
      "category": "feature",
      "subcategory": "cron-scheduling",
      "apis": [
        "setInitialDelay"
      ],
      "components": [
        "cron-scheduler",
        "workflow-options",
        "client-sdk"
      ],
      "concepts": [
        "cron-job",
        "scheduling",
        "initial-delay",
        "timing-control",
        "workflow-execution"
      ],
      "severity": "medium",
      "userImpact": "Users cannot predictably control when the first run of a CRON job executes, making it difficult to schedule jobs with specific timing requirements.",
      "rootCause": null,
      "proposedFix": "Add a configurable initialDelay option to workflow options (e.g., workflowOptions.setInitialDelay(Long seconds))",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Client sdk should have one option to configure initialDelay for cron schedule. Like this: workflowOptions.setInitialDelay(Long seconds)",
      "number": 926,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:34.947Z"
    },
    {
      "summary": "User asks for guidance on implementing human approval workflows in Temporal, specifically how to handle approval requests and conditional progression based on approval status. Community points to the expense example and discusses using signals instead of external pub/sub for handling human-driven workflows.",
      "category": "question",
      "subcategory": "human-approval-workflows",
      "apis": [
        "GetSignalChannel",
        "StartWorkflow"
      ],
      "components": [
        "workflow-execution",
        "signaling",
        "approval-handling"
      ],
      "concepts": [
        "human-approval",
        "workflow-signals",
        "conditional-progression",
        "event-driven-workflows",
        "approval-status"
      ],
      "severity": "low",
      "userImpact": "Developers seeking to implement human approval workflows get clear guidance on using signals instead of external systems like Redis.",
      "rootCause": null,
      "proposedFix": "Use Temporal's signal mechanism (GetSignalChannel) instead of external pub/sub systems for handling human approvals",
      "workaround": "External pub/sub systems like Redis can be used but are not recommended",
      "resolution": "invalid",
      "resolutionDetails": "Closed as discussion was resolved through community guidance and example references",
      "related": [],
      "keyQuote": "Ideally, you should not need Redis or any external pub sub to trigger the workflow.",
      "number": 906,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:33.789Z"
    },
    {
      "summary": "PostgreSQL database connections fail when the password contains special characters because the password isn't URL-encoded. The fix requires escaping the password using url.QueryEscape before constructing the connection URL.",
      "category": "bug",
      "subcategory": "database-connection",
      "apis": [],
      "components": [
        "persistence",
        "postgresql-plugin",
        "sql-driver",
        "url-parsing"
      ],
      "concepts": [
        "password-encoding",
        "special-characters",
        "url-escaping",
        "connection-string",
        "database-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users with PostgreSQL passwords containing special characters cannot establish database connections, preventing the system from starting.",
      "rootCause": "PostgreSQL connection URL password is not URL-encoded, causing invalid userinfo values when the password contains special characters like &, =, %, etc.",
      "proposedFix": "Use url.QueryEscape(cfg.Password) to escape the password before constructing the PostgreSQL connection URL in the plugin.go file.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Pull request #2014 implemented the fix by adding url.QueryEscape() to properly escape special characters in the PostgreSQL password.",
      "related": [
        2014
      ],
      "keyQuote": "Password should be escaped: url.QueryEscape(cfg.Password)",
      "number": 903,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:20.963Z"
    },
    {
      "summary": "The tctl admin hist desc command fails with 'Request is nil' error when attempting to describe history hosts. This prevents administrators from querying history host information via the CLI.",
      "category": "bug",
      "subcategory": "tctl-admin-commands",
      "apis": [],
      "components": [
        "tctl",
        "admin-commands",
        "history-host",
        "rpc"
      ],
      "concepts": [
        "CLI",
        "admin-interface",
        "request-validation",
        "nil-pointer",
        "history-service"
      ],
      "severity": "medium",
      "userImpact": "Administrators cannot use tctl to describe history hosts, blocking a critical administrative operation.",
      "rootCause": "The request object passed to the history host describe RPC call is nil, indicating a marshaling or construction error in the CLI argument handling.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by PR #1013 which fixed the request construction in the tctl admin hist describe command.",
      "related": [
        1013
      ],
      "keyQuote": "Describe history host failed - rpc error: code = InvalidArgument desc = Request is nil.",
      "number": 897,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:21.269Z"
    },
    {
      "summary": "Schema version compatibility check fails on startup when using PostgreSQL auto-setup due to mismatched schema directory structure and version mismatch between admin-tools and server versions.",
      "category": "bug",
      "subcategory": "schema-setup",
      "apis": [],
      "components": [
        "auto-setup",
        "schema-migration",
        "postgres-driver",
        "admin-tools"
      ],
      "concepts": [
        "schema-versioning",
        "database-initialization",
        "version-compatibility",
        "docker-setup",
        "configuration-mismatch"
      ],
      "severity": "high",
      "userImpact": "Users cannot start Temporal server with auto-setup on PostgreSQL due to schema version compatibility failures during initialization.",
      "rootCause": "Schema directory structure moved for better version control similar to MySQL, causing path mismatch. Additionally, version incompatibility between admin-tools and server images can cause the same error.",
      "proposedFix": "Updated in v1.2.1 release to handle new schema directory structure.",
      "workaround": "Ensure admin-tools and server versions match (e.g., both 1.25, not 1.24 and 1.25). Manually verify schema files exist in correct paths.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved in v1.2.1 release which addressed schema directory structure changes for PostgreSQL support.",
      "related": [
        862
      ],
      "keyQuote": "The issue you are seeing is due the the move of the schema dir for better version control / support, similar to MySQL.",
      "number": 896,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:23.072Z"
    },
    {
      "summary": "DynamicConfig TaskQueuePartitions has a TaskType constraint that compares an ENUM to a string/int, causing it to never match. The TaskType constraint should also be optional to allow applying the same partition count to both Activity and Workflow tasks.",
      "category": "bug",
      "subcategory": "dynamic-config",
      "apis": [],
      "components": [
        "dynamic-config",
        "task-queue",
        "partitioning"
      ],
      "concepts": [
        "task-type",
        "enum-comparison",
        "configuration-constraint",
        "task-queue-partitions"
      ],
      "severity": "medium",
      "userImpact": "Users cannot properly configure different partition counts for activity vs workflow tasks in DynamicConfig due to the TaskType constraint failing to match.",
      "rootCause": "TaskType constraint implementation compares ENUM values against string/int types, causing the comparison to always fail.",
      "proposedFix": "Fix the comparison logic to handle both ENUM and string/int values, and make the TaskType constraint optional.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "TaskType constraint fix was completed in v1.2 to accept both 'Activity'/'Workflow' string values and numeric representations. Making TaskType optional was landed to master branch.",
      "related": [],
      "keyQuote": "This has been fixed. The TaskType Constraint failure is in 1.2 (specify Activity or Workflow as TaskType) and making TaskType optional is landed to master and will be available on the next release.",
      "number": 894,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:05.833Z"
    },
    {
      "summary": "The history service should implement a pause mechanism to handle backpressure when worker processing cannot keep up with incoming signal events. This prevents worker overload by temporarily stopping signal acceptance.",
      "category": "feature",
      "subcategory": "signal-handling",
      "apis": [],
      "components": [
        "history-service",
        "worker",
        "signal-processor"
      ],
      "concepts": [
        "backpressure",
        "flow-control",
        "signal-handling",
        "worker-load",
        "queue-management",
        "event-rate-limiting"
      ],
      "severity": "medium",
      "userImpact": "Workers can become overwhelmed by incoming signals when processing cannot keep up, potentially causing failures or delays without a throttling mechanism.",
      "rootCause": null,
      "proposedFix": "Implement pause functionality in the history service to pause accepting new signals when the worker cannot keep up with incoming event rate.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Pause functionality, in case worker cannot keep up with the incoming events.",
      "number": 891,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:09.456Z"
    },
    {
      "summary": "Web UI displays completed workflows in the open workflows list instead of only showing running workflows. The root cause is that the visibility API provides eventually consistent semantics, so closed workflows may appear in open workflow responses when the visibility queue has a backlog.",
      "category": "bug",
      "subcategory": "web-ui-visibility",
      "apis": [
        "DescribeWorkflowExecution"
      ],
      "components": [
        "web-ui",
        "visibility-api",
        "workflow-list"
      ],
      "concepts": [
        "eventual-consistency",
        "visibility-queue",
        "workflow-state",
        "backlog",
        "elasticsearch"
      ],
      "severity": "medium",
      "userImpact": "Users see completed workflows listed as open in the Web UI, causing confusion about actual workflow status.",
      "rootCause": "Visibility API provides eventually consistent semantics rather than strong consistency. When the visibility queue has a backlog, closed workflows may not be immediately removed from the open workflows list.",
      "proposedFix": null,
      "workaround": "Use DescribeWorkflowExecution API directly, which is strongly consistent, for accurate workflow status.",
      "resolution": "wontfix",
      "resolutionDetails": "Marked as expected behavior. Visibility API is designed with eventual consistency semantics; the issue is by design and related to Kafka removal effort.",
      "related": [],
      "keyQuote": "Visibility API provides eventually consistent semantics, so in the case of certain failure conditions like backlog on visibility queue a closed workflow might appear in the list of open workflow executions",
      "number": 888,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:14:08.067Z"
    },
    {
      "summary": "Partitioned matching task queues may experience delays in task delivery if a matching service instance shuts down before workers poll. The system needs a centralized metadata manager to ensure all matching services initialize required task queue managers, and consider round-robin polling for better distribution.",
      "category": "feature",
      "subcategory": "matching-service",
      "apis": [],
      "components": [
        "matching-service",
        "task-queue-manager",
        "poller",
        "database"
      ],
      "concepts": [
        "partitioning",
        "task-distribution",
        "metadata-management",
        "service-initialization",
        "polling",
        "starvation"
      ],
      "severity": "medium",
      "userImpact": "Workers may experience significant delays in picking up tasks from partitioned task queues if matching service instances shut down before polls occur.",
      "rootCause": "Task queue managers are initialized only when directly polled, and if a partition's manager isn't initialized when its matching service shuts down, tasks remain in the database longer than necessary.",
      "proposedFix": "Implement a common metadata manager/service that notifies all matching services to initialize corresponding task queue managers, and consider implementing round-robin polling instead of randomization.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        459
      ],
      "keyQuote": "if the task queue (with partition) is not directly polled, and task queue manager not initialized, then the poller may take a long time picking up some tasks",
      "number": 881,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:53.583Z"
    },
    {
      "summary": "The Temporal Docker image's start.sh script hardcodes the database hostname to 'postgresql', preventing users from using custom database service names like 'temporal-db'. This causes initialization failures when the actual service name differs from the expected one.",
      "category": "bug",
      "subcategory": "docker-setup",
      "apis": [],
      "components": [
        "docker",
        "start.sh",
        "auto-setup"
      ],
      "concepts": [
        "configuration",
        "database-connection",
        "docker-compose",
        "service-discovery",
        "hardcoded-values"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use custom database service names in Docker Compose setups, forcing them to name their database service 'postgresql' or face connection failures during initialization.",
      "rootCause": "The start.sh script checks for hardcoded database hostname 'postgresql' rather than checking for environment variables like POSTGRES_SEEDS being non-empty or a DB_TYPE parameter.",
      "proposedFix": "Check for POSTGRES_SEEDS being non-empty or add an additional DB_TYPE argument (e.g., DB_TYPE=postgresql) to determine which seed scripts to use, rather than relying on the database service hostname.",
      "workaround": "Set POSTGRES_SEEDS environment variable and ensure database service is named 'postgresql' in Docker Compose, or build from master branch with the fix.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved in version 1.2.1 release - the startup script was updated to properly detect PostgreSQL configuration via environment variables rather than hardcoded hostname.",
      "related": [],
      "keyQuote": "temporal should not hardcode database hostnames since most people will want to dedicate a custom name to the database, such as `temporal-db`.",
      "number": 877,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:53.526Z"
    },
    {
      "summary": "The tctl admin db scan command fails with a segmentation violation (nil pointer dereference) when scanning Cassandra databases. The scan completes the first shard but crashes during execution listing due to an uninitialized pointer in the cassandraPersistence.ListConcreteExecutions method.",
      "category": "bug",
      "subcategory": "admin-tools",
      "apis": [],
      "components": [
        "cassandra-persistence",
        "admin-db-scan",
        "database-operations"
      ],
      "concepts": [
        "database-corruption",
        "data-integrity",
        "scan-validation",
        "memory-safety",
        "cassandra"
      ],
      "severity": "high",
      "userImpact": "Users cannot perform database scans on Cassandra databases with tctl, preventing corruption analysis and validation of production data.",
      "rootCause": "Invalid memory address or nil pointer dereference in cassandraPersistence.ListConcreteExecutions at offset 0x30, indicating an uninitialized or null pointer being dereferenced.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fix was merged and included in the next release after 1.0.0/0.29.0.",
      "related": [],
      "keyQuote": "panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x30",
      "number": 868,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:50.127Z"
    },
    {
      "summary": "Temporal server fails to start when ElasticSearch cluster is unavailable, even though ES is non-essential for data durability since writes are queued to Kafka. This blocks critical operations like node replacement during infrastructure outages.",
      "category": "feature",
      "subcategory": "elasticsearch-integration",
      "apis": [],
      "components": [
        "server-startup",
        "elasticsearch-client",
        "kafka-queue"
      ],
      "concepts": [
        "dependency-failure",
        "graceful-degradation",
        "service-availability",
        "fault-tolerance",
        "startup-resilience",
        "infrastructure-outage"
      ],
      "severity": "high",
      "userImpact": "Users cannot restart Temporal nodes when ElasticSearch is temporarily unavailable, blocking disaster recovery and maintenance operations.",
      "rootCause": "Temporal startup process treats ElasticSearch connectivity as a hard requirement rather than a soft dependency, failing the entire startup if ES is unreachable.",
      "proposedFix": "Allow Temporal to start successfully even when ES is unavailable, queuing writes to Kafka for later processing when ES comes back online.",
      "workaround": "Docker startup script variable can be configured to work around the blocking behavior (per linked PR).",
      "resolution": "fixed",
      "resolutionDetails": "PR was created to address the issue, allowing servers to start even when ES cluster is unavailable.",
      "related": [],
      "keyQuote": "since Temporal queues ES writes to Kafka, it doesn't strictly _need_ ES in order to avoid data loss",
      "number": 863,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:37.960Z"
    },
    {
      "summary": "Database migration from auto-setup:1.0.0 to 1.1.0 fails because the PostgreSQL visibility schema files were moved to a versioned subdirectory (schema/postgresql/v96), but the migration script still looks for them in the old location.",
      "category": "bug",
      "subcategory": "database-migration",
      "apis": [],
      "components": [
        "auto-setup",
        "schema-migration",
        "database-tools"
      ],
      "concepts": [
        "database-migration",
        "schema-versioning",
        "postgresql",
        "backward-compatibility",
        "upgrade-path"
      ],
      "severity": "high",
      "userImpact": "Users upgrading from 1.0.0 to 1.1.0 cannot start the Temporal server due to failed schema migration.",
      "rootCause": "Schema files were moved from schema/postgresql/ to schema/postgresql/v96/ between versions, but the migration tool still references the old path.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved in version 1.2.1 release",
      "related": [
        864
      ],
      "keyQuote": "the issue is probably due to the move of schema files from `schema/postgresql/` to `schema/postgresql/v96`",
      "number": 862,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:38.130Z"
    },
    {
      "summary": "Add cluster name configuration to Ringpop initialization to prevent nodes from different clusters from accidentally joining the same membership ring through invalid pings.",
      "category": "feature",
      "subcategory": "membership-ring",
      "apis": [],
      "components": [
        "ringpop",
        "membership",
        "cluster-management"
      ],
      "concepts": [
        "cluster-isolation",
        "membership-ring",
        "node-discovery",
        "cluster-validation",
        "network-partition"
      ],
      "severity": "medium",
      "userImpact": "Without cluster isolation, nodes from different Temporal clusters could incorrectly join the same membership ring, causing cluster corruption and node communication failures.",
      "rootCause": "Ringpop lacks cluster name validation in ping handling, allowing cross-cluster nodes to be accepted into the membership ring.",
      "proposedFix": "Add ability to pass cluster name as a configuration parameter used to initialize Ringpop AppName, with validation to reject pings from different clusters.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed in favor of issue #1234",
      "related": [
        1234
      ],
      "keyQuote": "Add ability to pass in cluster name which is used to initialize Ringpop AppName.",
      "number": 857,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:36.693Z"
    },
    {
      "summary": "Request to support in-memory TLS configuration (key, cert, CA) for PostgreSQL connections in Temporal Server, following similar functionality available in the pq PostgreSQL driver.",
      "category": "feature",
      "subcategory": "persistence-postgresql",
      "apis": [],
      "components": [
        "persistence",
        "postgresql",
        "tls-configuration"
      ],
      "concepts": [
        "tls",
        "certificate-management",
        "in-memory-storage",
        "security",
        "postgresql-driver"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently configure TLS certificates for PostgreSQL directly in memory, requiring file-based certificate management for secure database connections.",
      "rootCause": null,
      "proposedFix": "Add support for in-memory key, cert, and CA configuration for PostgreSQL TLS, similar to the functionality available in the lib/pq driver",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Support in mem key / cert / ca for PostgreSQL",
      "number": 852,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:25.953Z"
    },
    {
      "summary": "Remove unused replication_metadata and replication_metadata_encoding fields from the Cassandra schema to improve performance and reduce storage overhead.",
      "category": "feature",
      "subcategory": "cassandra-schema",
      "apis": [],
      "components": [
        "cassandra",
        "persistence",
        "schema"
      ],
      "concepts": [
        "schema-optimization",
        "storage-efficiency",
        "metadata",
        "replication",
        "performance"
      ],
      "severity": "medium",
      "userImpact": "Removing obsolete schema fields reduces Cassandra storage footprint and may improve query performance.",
      "rootCause": "Legacy replication_metadata fields are no longer needed in the schema design.",
      "proposedFix": "Remove replication_metadata and replication_metadata_encoding columns from Cassandra schema definition.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Schema fields were removed in a subsequent update.",
      "related": [],
      "keyQuote": "Remove replication_metadata and replication_metadata_encoding from Cassandra schema",
      "number": 851,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:23.801Z"
    },
    {
      "summary": "Support blue/green cluster deployment strategy that allows backward-incompatible cluster membership protocol changes without downtime. Currently, newly deployed instances cannot communicate with old versions, causing cluster partitioning and constant shard stealing. The request is for a mechanism to exclude newly added hosts from shard hosting during deployment, then switch to using only the new hosts after deployment completes.",
      "category": "feature",
      "subcategory": "cluster-deployment",
      "apis": [],
      "components": [
        "cluster-membership",
        "shard-distribution",
        "host-management"
      ],
      "concepts": [
        "blue-green-deployment",
        "backward-incompatibility",
        "cluster-partitioning",
        "shard-stealing",
        "zero-downtime-deployment",
        "version-migration"
      ],
      "severity": "high",
      "userImpact": "Users deploying backward-incompatible cluster membership changes currently experience downtime due to cluster partitioning; this feature would enable zero-downtime deployments.",
      "rootCause": "Cluster membership protocol changes are backward-incompatible, preventing new instances from reaching old ones, causing partitioning and continuous shard rebalancing.",
      "proposedFix": "Implement a blue/green deployment mechanism: prevent history and matching shard distribution to newly added hosts during deployment, then flip a config switch to use only new hosts after deployment completes.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Provide a mechanism to not distribute history and matching shards to the newly added hosts while they are deployed. After the deployment is complete flip a config switch to using only the newly added hosts for shard hosting.",
      "number": 848,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:25.775Z"
    },
    {
      "summary": "Request to generate signed tokens for workflow activities that certify worker identity and execution context (activity/workflow name, task list, etc.), enabling workers to authenticate with external systems using their temporal role.",
      "category": "feature",
      "subcategory": "authentication",
      "apis": [],
      "components": [
        "worker",
        "server",
        "task-list"
      ],
      "concepts": [
        "authentication",
        "identity",
        "token",
        "external-systems",
        "worker-identity",
        "authorization"
      ],
      "severity": "medium",
      "userImpact": "Workers cannot currently authenticate with external systems using Temporal identity, limiting integration with secure external resources.",
      "rootCause": null,
      "proposedFix": "Server generates signed tokens (or calls configurable plugin) when worker extracts work from task list, certifying activity/workflow information.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "let temporal workers access external systems using their own identity, that certifies what activity/workflow a worker is executing",
      "number": 845,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:13.217Z"
    },
    {
      "summary": "Request for pluggable authorization system that allows loading authorization implementations without recompiling Temporal Server. Users want to either load plugins via Go plugin interface or call external services like OPA (Open Policy Agent).",
      "category": "feature",
      "subcategory": "authorization-plugins",
      "apis": [],
      "components": [
        "authorization",
        "authorizer-interface",
        "config-management",
        "plugin-system"
      ],
      "concepts": [
        "pluggable-architecture",
        "authorization-policies",
        "external-services",
        "open-policy-agent",
        "runtime-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users must currently recompile Temporal Server to add custom authorization logic; external authorization would allow dynamic policy updates without recompilation.",
      "rootCause": "Authorization interface is tightly coupled to the server binary, requiring recompilation for custom implementations.",
      "proposedFix": "Support pluggable authorizers through Go plugin interface or OPA integration; add OPA authorizer implementation with configurable endpoint.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "OPA authorizer implementation was contributed and merged via PR #3702.",
      "related": [
        3702
      ],
      "keyQuote": "I would like to implement a generic authorizer that loads a plugin using the golang plugin interface. Or a generic authorizer that calls an external Open Policy Agent server.",
      "number": 844,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:11.112Z"
    },
    {
      "summary": "gRPC v1.33 was removed from the upstream repository, but both Temporal and its API module are still pinned to this deprecated version. This breaks `go mod vendor` operations for users trying to vendor dependencies.",
      "category": "bug",
      "subcategory": "dependency-management",
      "apis": [],
      "components": [
        "grpc-client",
        "module-dependencies",
        "vendor"
      ],
      "concepts": [
        "dependency-version",
        "breaking-change",
        "module-vendoring",
        "upstream-incompatibility"
      ],
      "severity": "high",
      "userImpact": "Users cannot successfully run `go mod vendor` due to the incompatible pinned gRPC version in both Temporal and API modules.",
      "rootCause": "Temporal and API modules depend on gRPC v1.33.0 which was removed from the upstream grpc-go repository.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The gRPC dependency was updated to a supported version.",
      "related": [
        3945
      ],
      "keyQuote": "go mod vendor will failed now, both temporal and api use grpc v1.33.0",
      "number": 842,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:13:11.615Z"
    },
    {
      "summary": "Workflow reset redundantly creates new history trees multiple times instead of once. The reset process should create a single new history tree for the workflow.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-reset",
        "history-tree",
        "state-management"
      ],
      "concepts": [
        "workflow-reset",
        "history-tree",
        "state-recreation",
        "performance",
        "idempotency"
      ],
      "severity": "medium",
      "userImpact": "Workflow resets incur unnecessary performance overhead by creating duplicate history tree structures.",
      "rootCause": "The workflow reset logic creates new history trees more than once during a single reset operation.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by fixing the workflow reset logic to create only one new history tree per reset operation.",
      "related": [],
      "keyQuote": "Workflow reset should only create a new history tree for new workflow once",
      "number": 818,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:59.265Z"
    },
    {
      "summary": "PostgreSQL persistence tests fail to properly drop test databases, resulting in 'database is being accessed by other users' errors. This prevents test cleanup and causes test suite failures.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "persistence",
        "postgresql",
        "test-database",
        "database-cleanup"
      ],
      "concepts": [
        "database-cleanup",
        "test-isolation",
        "connection-management",
        "database-drops",
        "test-concurrency",
        "resource-cleanup"
      ],
      "severity": "high",
      "userImpact": "Developers running PostgreSQL persistence tests experience test failures due to improper database cleanup, blocking test execution and development workflows.",
      "rootCause": "Test database connections are not being properly closed before attempting to drop the database, leaving active connections that prevent deletion.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented proper connection closure mechanism in PostgreSQL test cleanup to ensure all connections are terminated before database drop.",
      "related": [],
      "keyQuote": "test panicked: pq: database \"test_1008214627_jyv\" is being accessed by other users",
      "number": 817,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:57.103Z"
    },
    {
      "summary": "Feature request to add command-line option for disabling hostname and certificate verification in TLS/mTLS connections for dev/test/debug purposes. This allows testing TLS encryption independently from certificate validation.",
      "category": "feature",
      "subcategory": "tls-security",
      "apis": [],
      "components": [
        "tctl",
        "tls-client",
        "connection-handler"
      ],
      "concepts": [
        "tls",
        "mtls",
        "certificate-verification",
        "hostname-verification",
        "debugging",
        "development-testing",
        "insecure-skip-verify"
      ],
      "severity": "low",
      "userImpact": "Developers can more easily debug and test TLS/mTLS configurations by disabling certificate validation without needing to modify code or deploy test clusters.",
      "rootCause": null,
      "proposedFix": "Add a command-line option for tctl to enable InsecureSkipVerify for dev/test/debug purposes, or deploy a development cluster with relaxed TLS validation settings.",
      "workaround": "Deploy a development cluster where TLS clients use InsecureSkipVerify and TLS servers use RequireAnyClientCert for testing internode and frontend mTLS.",
      "resolution": "fixed",
      "resolutionDetails": "Fixed via PR #1478",
      "related": [
        812,
        1478
      ],
      "keyQuote": "Debugging TLS can be tough - it's nice to be able to test that TLS is working independently from making sure server name matches the cert",
      "number": 815,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:59.299Z"
    },
    {
      "summary": "Before modularizing and allowing public overrides of the persistence layer, the interfaces and naming conventions need to be revisited to ensure they are well-designed for this purpose.",
      "category": "feature",
      "subcategory": "persistence-layer",
      "apis": [],
      "components": [
        "persistence-layer",
        "sql-storage",
        "nosql-storage"
      ],
      "concepts": [
        "modularization",
        "interface-design",
        "naming-conventions",
        "persistence",
        "abstraction",
        "extensibility"
      ],
      "severity": "medium",
      "userImpact": "Users who want to implement custom persistence backends need well-designed interfaces and clear naming conventions to do so effectively.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Before making the persistence layer modular and ready for public override, revisit the interfaces and naming conventions.",
      "number": 810,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:43.629Z"
    },
    {
      "summary": "Feature request to enable input validation before workflow start. Currently impossible to reject StartWorkflow calls with invalid input data; proposal is to implement worker queries for validation before workflow execution begins.",
      "category": "feature",
      "subcategory": "workflow-validation",
      "apis": [
        "StartWorkflowExecution"
      ],
      "components": [
        "worker",
        "workflow-execution",
        "service-api"
      ],
      "concepts": [
        "input-validation",
        "error-handling",
        "synchronous-operations",
        "workflow-initialization",
        "worker-queries"
      ],
      "severity": "medium",
      "userImpact": "Users cannot validate workflow input before starting workflows, forcing them to accept invalid data or implement external validation mechanisms.",
      "rootCause": "Workflow execution is asynchronous, preventing synchronous validation before StartWorkflow returns.",
      "proposedFix": "Implement worker queries with a special built-in query for application-specific workflow input validation, allowing rejection of invalid start calls.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        390
      ],
      "keyQuote": "Ability to invoke user code that performs input argument validation before returning start call.",
      "number": 804,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:45.634Z"
    },
    {
      "summary": "WorkflowQueue needs to implement Serializable to enable event processing patterns that avoid long-running workflows. This is a Java SDK feature request to make WorkflowQueue serializable for ordered entity event processing.",
      "category": "feature",
      "subcategory": "workflow-queue",
      "apis": [
        "WorkflowQueue"
      ],
      "components": [
        "workflow-queue",
        "serialization",
        "java-sdk"
      ],
      "concepts": [
        "serialization",
        "event-ordering",
        "workflow-duration",
        "entity-processing",
        "queue-state"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use WorkflowQueue for ordered event processing patterns because it is not serializable, limiting their ability to avoid long-running workflows.",
      "rootCause": "WorkflowQueue does not implement the Serializable interface.",
      "proposedFix": "Enable Serialization on WorkflowQueue by implementing the Serializable interface.",
      "workaround": "Use Java Queue as an alternative.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was moved to Java SDK repository for implementation.",
      "related": [],
      "keyQuote": "To continually process events on a specific entity in the order we received the events, we can use WorkflowQueue... we need WorkflowQueue to be serializable.",
      "number": 792,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:43.188Z"
    },
    {
      "summary": "Code review needed for the ListTaskQueue functionality in the matching engine's SQL task manager implementation. The issue flags a specific code section that may need refactoring or optimization.",
      "category": "other",
      "subcategory": "matching-engine",
      "apis": [],
      "components": [
        "matching-engine",
        "sql-task-manager",
        "task-queue"
      ],
      "concepts": [
        "code-review",
        "task-queue-management",
        "matching-logic",
        "persistence",
        "sql-operations"
      ],
      "severity": "low",
      "userImpact": "Potential performance or correctness issues in task queue management that could affect workflow task routing and execution.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Revisit the code logic in matching engine",
      "number": 791,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:28.106Z"
    },
    {
      "summary": "S3 archival operations frequently timeout due to inconsistent context deadlines. While archiving to S3-compliant object stores, the timeout deadline varies from under 1 second to just under 60 seconds, causing many S3 calls to fail even with low-latency storage.",
      "category": "bug",
      "subcategory": "archival-timeout",
      "apis": [],
      "components": [
        "archiver",
        "s3store",
        "history-service",
        "context-management"
      ],
      "concepts": [
        "timeout",
        "context-deadline",
        "S3-archival",
        "background-worker",
        "inline-archival"
      ],
      "severity": "medium",
      "userImpact": "Users archiving to S3 experience frequent timeout errors in logs, creating noise even though background workers eventually complete the archival.",
      "rootCause": "The ensureContextTimeout function only enforces a 1-minute timeout if no deadline is set by the caller. Inline archival calls from timer and transfer task queue processors set very short timeouts (1 second and 200 milliseconds respectively), overriding the intended 60-second timeout.",
      "proposedFix": "Ensure consistent 1-minute deadline enforcement on all S3 API calls, possibly by reviewing how timeouts are set at call sites in the history service.",
      "workaround": "Increase timeout values using dynamic config settings; background worker will eventually archive workflows despite inline archival failures.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "ensureContextTimeout actually sets timeout to 1 minute only if it is not set at all. If it is set by caller then it stays as is.",
      "number": 787,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:32.960Z"
    },
    {
      "summary": "Request to enable direct invocation of activities outside of workflows for better interactive development experience, particularly for specialized hardware setups and dynamic testing against staging/production clusters.",
      "category": "feature",
      "subcategory": "activity-execution",
      "apis": [],
      "components": [
        "worker",
        "activity-executor",
        "runtime"
      ],
      "concepts": [
        "activity-invocation",
        "development-experience",
        "interactive-development",
        "specialized-hardware",
        "staging-deployment"
      ],
      "severity": "medium",
      "userImpact": "Developers using specialized hardware (GPU hosts) and working with Ruby/Python SDKs need direct activity invocation for improved interactive development workflows without workflow context.",
      "rootCause": null,
      "proposedFix": "Implement workflow-less activity execution capability that allows direct activity invocation against staging/production clusters.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Ability to directly invoke an activity out side of workflows",
      "number": 785,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:28.956Z"
    },
    {
      "summary": "Error handling in replication task fetcher logic is implicit and should be made explicit. The issue references #776 and requests that non-nil error cases be handled explicitly rather than implicitly.",
      "category": "bug",
      "subcategory": "replication",
      "apis": [],
      "components": [
        "replication",
        "task-fetcher"
      ],
      "concepts": [
        "error-handling",
        "code-clarity",
        "explicit-vs-implicit"
      ],
      "severity": "low",
      "userImpact": "Improper error handling in replication task fetching could lead to silent failures or unexpected behavior.",
      "rootCause": "Error return values in replication task fetcher are not explicitly checked and handled",
      "proposedFix": "Make error handling explicit by adding proper error checks instead of implicit handling",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [
        776
      ],
      "keyQuote": "Handle the err being non nil case explicitly, other than implicitly",
      "number": 784,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:15.880Z"
    },
    {
      "summary": "User asks how to install and build tctl (CLI tool) on Windows, as the documentation only mentions make command which is Unix-specific. Community provides go build workarounds, but long-term Windows binary distribution would be beneficial.",
      "category": "docs",
      "subcategory": "installation-windows",
      "apis": [],
      "components": [
        "tctl",
        "cli",
        "build-system"
      ],
      "concepts": [
        "installation",
        "windows-support",
        "binary-distribution",
        "documentation",
        "developer-experience"
      ],
      "severity": "low",
      "userImpact": "Windows developers cannot easily build tctl locally without workarounds, and lack of pre-built Windows binaries increases setup friction.",
      "rootCause": "Documentation assumes Unix/Linux build environment (make) without Windows-specific instructions or pre-built binaries.",
      "proposedFix": "Provide pre-built Windows binaries for tctl distribution, or update documentation with clear Windows build instructions.",
      "workaround": "Use 'go build -o tctl.exe <path>\\cmd\\tctl\\main.go' on Windows after installing Go.",
      "resolution": "fixed",
      "resolutionDetails": "Documentation was updated with Windows-specific build instructions.",
      "related": [],
      "keyQuote": "Long term, a Win dist for tctl would be really appreciated. Instructions for building it locally will inevitably become out of date.",
      "number": 781,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:16.410Z"
    },
    {
      "summary": "The WorkflowTaskScheduledEvent does not expose the ScheduleToStart timeout value, making it unclear to users why tasks timeout at 5 seconds for sticky workers despite the UI showing 10 seconds. Adding this field to the event would improve clarity.",
      "category": "feature",
      "subcategory": "workflow-task-event",
      "apis": [],
      "components": [
        "workflow-task",
        "event-api",
        "sticky-workers"
      ],
      "concepts": [
        "timeout",
        "schedule-to-start",
        "workflow-task",
        "event-visibility",
        "user-experience"
      ],
      "severity": "low",
      "userImpact": "Users are confused about why workflow tasks timeout at 5 seconds when the UI displays different timeout values.",
      "rootCause": "The ScheduleToStart timeout (5 seconds for sticky workers) is not exposed in the WorkflowTaskScheduledEvent, leaving users unable to see this important timeout constraint.",
      "proposedFix": "Add ScheduleToStart timeout field to the WorkflowTaskScheduledEvent to make the timeout value explicit and visible to users.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I think UI should show only what's present on the event. I don't see a problem in adding ScheduleToStart timeout to the WorkflowTaskScheduledEvent.",
      "number": 778,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:18.525Z"
    },
    {
      "summary": "User asks whether sessions apply to activities in child workflows and whether session data is preserved when passing context to child workflows. The issue questions if activities in child workflows execute on the same worker as the session.",
      "category": "question",
      "subcategory": "sessions-child-workflows",
      "apis": [
        "RecreateSession",
        "StartWorkflow"
      ],
      "components": [
        "session",
        "child-workflow",
        "activity-executor"
      ],
      "concepts": [
        "session-affinity",
        "worker-affinity",
        "workflow-context",
        "state-sharing",
        "task-queue"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently share sessions across multiple workflows, limiting session-based activity coordination across workflow boundaries.",
      "rootCause": "Sessions are designed for single workflow execution and cannot currently span multiple workflows due to architectural constraints.",
      "proposedFix": "A feature request to enable sessions to spawn multiple workflows, though this would require significant refactoring.",
      "workaround": "Use child workflows to keep whole workflow state in a single workflow execution, or implement a session keeper workflow that other workflows use to request activity executions through signals.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It would require pretty significant refactoring to accommodate your scenario of sharing a session across multiple workflows.",
      "number": 773,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:04.173Z"
    },
    {
      "summary": "When a workflow uses NewContinueAsNewError with a CronSchedule, the cron configuration is lost after the first continue-as-new, preventing future cron executions.",
      "category": "bug",
      "subcategory": "cron-scheduling",
      "apis": [
        "NewContinueAsNewError",
        "CronSchedule"
      ],
      "components": [
        "workflow-engine",
        "cron-scheduler",
        "continue-as-new"
      ],
      "concepts": [
        "cron-scheduling",
        "workflow-continuation",
        "state-persistence",
        "scheduling"
      ],
      "severity": "high",
      "userImpact": "Workflows using both cron scheduling and continue-as-new lose their cron schedule after the first continuation, breaking recurring workflow executions.",
      "rootCause": "The cron schedule configuration is not preserved when NewContinueAsNewError is processed, causing it to be erased from workflow settings.",
      "proposedFix": null,
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "After returning NewContinueAsNewError, the CronSchedule is erased from the workflow settings and thus the workflow will stop",
      "number": 746,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:04.933Z"
    },
    {
      "summary": "Integration tests for the GetWorkflowExecutionRawHistoryV2 API need to be created. Previous tests for this API were removed in PR #744 and require reimplementation.",
      "category": "other",
      "subcategory": "integration-tests",
      "apis": [
        "GetWorkflowExecutionRawHistoryV2"
      ],
      "components": [
        "admin-api",
        "history",
        "test-framework"
      ],
      "concepts": [
        "integration-testing",
        "workflow-execution",
        "raw-history",
        "api-testing"
      ],
      "severity": "medium",
      "userImpact": "Lack of integration tests for GetWorkflowExecutionRawHistoryV2 API reduces confidence in the stability and correctness of this API functionality.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Integration tests were created to cover the GetWorkflowExecutionRawHistoryV2 API functionality that was previously removed.",
      "related": [
        744
      ],
      "keyQuote": "Removed in this PR",
      "number": 745,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:12:01.537Z"
    },
    {
      "summary": "Feature request to support resetting a workflow from a specific activity type and status, enabling programmatic retrieval of event IDs for bulk workflow reruns when downstream services fail.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-reset",
        "activity-executor",
        "decision-task"
      ],
      "concepts": [
        "reset",
        "activity-type",
        "workflow-replay",
        "event-id",
        "decision-task-completion",
        "activity-status"
      ],
      "severity": "medium",
      "userImpact": "Enables users to programmatically reset workflows from a specific activity when handling failures in dependent services, improving operational efficiency for large-scale failure scenarios.",
      "rootCause": null,
      "proposedFix": "Implement reset functionality that accepts an activity type and status (OK, Failed, Any) to reset the workflow from that point, assuming the reset can recreate activity tasks for all activities completed after the reset point.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Support reset that accepts an activity type and status (OK, Failed, Any) and resets workflow from that point.",
      "number": 741,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:49.395Z"
    },
    {
      "summary": "Request to enable TLS configuration through Docker environment variables in the config template, allowing easier deployment in containerized environments like Kubernetes without requiring ConfigMap merges.",
      "category": "feature",
      "subcategory": "tls-configuration",
      "apis": [],
      "components": [
        "docker",
        "config-template",
        "tls"
      ],
      "concepts": [
        "tls",
        "docker",
        "environment-variables",
        "configuration",
        "kubernetes",
        "deployment"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal Server in Docker/Kubernetes environments cannot configure TLS through environment variables, requiring workarounds like ConfigMaps.",
      "rootCause": null,
      "proposedFix": "Add TLS-related keys to docker/config_template.yaml to support environment variable configuration",
      "workaround": "Use Kubernetes ConfigMap to override the entire config file, though this requires handling merges with auto-generated setup config",
      "resolution": "fixed",
      "resolutionDetails": "Fixed by PR #783",
      "related": [
        783
      ],
      "keyQuote": "Add the related keys to docker/config_template.yaml",
      "number": 736,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:50.806Z"
    },
    {
      "summary": "When a child workflow is reset while open, the parent workflow incorrectly receives a ChildWorkflowExecutionTerminated event instead of treating the reset as transparent. The parent should see the reset child as the same child with only a new completion event.",
      "category": "bug",
      "subcategory": "child-workflow-reset",
      "apis": [],
      "components": [
        "child-workflow-state-machine",
        "workflow-event-handling",
        "reset-logic"
      ],
      "concepts": [
        "workflow-reset",
        "child-workflow",
        "event-propagation",
        "state-management",
        "failure-handling"
      ],
      "severity": "high",
      "userImpact": "Parent workflows receive unexpected termination failures when their child workflows are reset, breaking the expected behavior and causing workflow failures.",
      "rootCause": "The reset operation sends a ChildWorkflowExecutionTerminated event to the parent instead of handling the reset transparently within the child's execution context.",
      "proposedFix": "Modify the reset logic to not propagate termination events to the parent when a child workflow is reset while open; the parent should only see the eventual completion result.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "PR #2913 addresses this issue by fixing the event handling for reset child workflows.",
      "related": [
        2913
      ],
      "keyQuote": "When a child is open resetting it should not be visible to its parent. So from the parent point of view, the reset child is still the same child and only its completion is reported.",
      "number": 725,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:49.837Z"
    },
    {
      "summary": "Child workflow namespace attribute is blank in history events when queried via GetWorkflowExecutionHistoryRequest, even though the namespace was specified in the initial query.",
      "category": "bug",
      "subcategory": "child-workflow",
      "apis": [
        "GetWorkflowExecutionHistoryRequest",
        "StartChildWorkflowExecutionInitiatedEventAttributes"
      ],
      "components": [
        "history-event",
        "child-workflow",
        "api-response"
      ],
      "concepts": [
        "namespace",
        "history-event",
        "child-workflow",
        "event-attributes",
        "data-population"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably identify the namespace of child workflows when processing history events, requiring them to track this information separately.",
      "rootCause": "History event attributes for child workflow initiation do not populate the namespace field from the query context.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved in v1.2.1 and could not be reproduced, indicating the bug was fixed in a later version.",
      "related": [],
      "keyQuote": "Issue cannot not be reproduced in v1.2.1",
      "number": 724,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:36.524Z"
    },
    {
      "summary": "User reports that automatic cluster failover is not working as expected in their NDC deployment. After killing the active cluster, no automatic switch to standby cluster occurred, and they request clarification on HA cluster switching behavior.",
      "category": "question",
      "subcategory": "cluster-failover",
      "apis": [],
      "components": [
        "cluster-manager",
        "namespace-replication",
        "multi-cluster-setup"
      ],
      "concepts": [
        "high-availability",
        "failover",
        "active-cluster",
        "cluster-switching",
        "disaster-recovery",
        "ndc"
      ],
      "severity": "medium",
      "userImpact": "Users deploying multi-cluster setups cannot rely on automatic failover to standby clusters, requiring manual intervention during outages.",
      "rootCause": "Automatic cluster switching for namespace failover is not implemented; cluster active/passive settings must be managed manually.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Active/passive cluster settings are managed manually by operators, automatic failover is not a supported feature.",
      "related": [],
      "keyQuote": "Namespace active / passive settings are currently managed by human, at least for now",
      "number": 716,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:34.970Z"
    },
    {
      "summary": "Local activity workflows experience history corruption during replay, resulting in corrupted workflow state. The issue affects the marker tracking for local activities, causing inconsistent state between cache and database mutable states.",
      "category": "bug",
      "subcategory": "local-activity-replay",
      "apis": [
        "ExecuteLocalActivity"
      ],
      "components": [
        "history-replay",
        "local-activity-executor",
        "mutable-state",
        "event-marker"
      ],
      "concepts": [
        "replay",
        "history-corruption",
        "local-activity",
        "state-consistency",
        "event-tracking",
        "marker-handling"
      ],
      "severity": "critical",
      "userImpact": "Workflows using local activities become corrupted and unable to process, blocking workflow execution and requiring manual intervention or reset.",
      "rootCause": "Local activity marker handling during history replay appears to introduce corruption in the workflow's mutable state, specifically affecting ActivityInfos tracking across replay cycles.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It looks like replay history for local activity workflows has some sort of bug. As a result, this Workflow is now corrupted.",
      "number": 711,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:37.893Z"
    },
    {
      "summary": "Unit tests for REPLICATION_TASK_TYPE_HISTORY_V2_TASK message decoding were removed during refactoring to remove 2dc-specific code paths. Need to add new unit tests to cover the history replication task decoding functionality.",
      "category": "other",
      "subcategory": "replication-task-processor",
      "apis": [],
      "components": [
        "replication-task-processor",
        "history-replication",
        "message-decoder"
      ],
      "concepts": [
        "unit-testing",
        "message-decoding",
        "replication-tasks",
        "refactoring",
        "test-coverage"
      ],
      "severity": "medium",
      "userImpact": "Reduced test coverage for critical replication functionality could lead to undetected bugs in the history replication task processing pipeline.",
      "rootCause": "Unit tests were deleted as part of PR #692 which removed 2dc-specific code paths, leaving the history replication task decoding untested.",
      "proposedFix": "Add new unit tests for REPLICATION_TASK_TYPE_HISTORY_V2_TASK message decoding that cover both success and failure scenarios.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Tests were added to cover the history v2 task replication decoding functionality after the 2dc code removal.",
      "related": [
        692
      ],
      "keyQuote": "I had to delete unit test `TestDecodeMsgAndSubmit_History_Success` as part of removal of 2dc specific code path",
      "number": 710,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:22.753Z"
    },
    {
      "summary": "After sequentially restarting Cassandra nodes with replication factor 1 while workflows are starting, corrupted history event batches occur with non-continuous event IDs, making workflows inaccessible.",
      "category": "bug",
      "subcategory": "history-persistence",
      "apis": [],
      "components": [
        "cassandra-persistence",
        "history-service",
        "event-batch"
      ],
      "concepts": [
        "data-corruption",
        "cassandra-replication",
        "history-events",
        "node-restart",
        "event-continuity"
      ],
      "severity": "high",
      "userImpact": "Workflows become inaccessible after Cassandra restarts due to corrupted history, requiring manual intervention to recover.",
      "rootCause": "When Cassandra nodes restart with replication factor 1 (no replicas), data loss occurs and hints on remaining coordinators are insufficient to maintain event continuity.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue marked as no longer relevant; appears to be a configuration/deployment issue with RF=1 rather than a product bug.",
      "related": [],
      "keyQuote": "Without any replicas (RF=1) and a node down you _will_ loose data. We capture hints on the remaining coordinators, but that's a best effort.",
      "number": 706,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:25.358Z"
    },
    {
      "summary": "Reset workflow execution is currently blocked if there are any pending child workflow executions. This feature request asks for support to allow reset operations even when child workflows are still running.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [
        "Reset"
      ],
      "components": [
        "workflow-execution",
        "child-workflows",
        "reset-handler"
      ],
      "concepts": [
        "reset",
        "workflow-state",
        "child-workflows",
        "execution-control",
        "pending-operations"
      ],
      "severity": "high",
      "userImpact": "Users cannot reset workflow executions if they have pending child workflows, limiting their ability to recover from certain failure scenarios.",
      "rootCause": null,
      "proposedFix": "Implement support for reset operations that can handle pending child workflow executions by properly managing their state during reset.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Support reset with pending children",
      "number": 705,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:23.098Z"
    },
    {
      "summary": "Local namespaces do not have VersionHistory set for workflow executions, preventing migration to global namespaces. This feature request asks for auto-populating VersionHistory for workflows created in local namespaces.",
      "category": "feature",
      "subcategory": "namespace-management",
      "apis": [],
      "components": [
        "namespace",
        "workflow-execution",
        "version-history"
      ],
      "concepts": [
        "namespace-migration",
        "versioning",
        "local-namespace",
        "global-namespace",
        "workflow-metadata"
      ],
      "severity": "medium",
      "userImpact": "Users cannot migrate workflows from local namespaces to global namespaces due to missing VersionHistory metadata.",
      "rootCause": "VersionHistory is not automatically backfilled when workflows are created in local namespaces.",
      "proposedFix": "Implement auto backfill of VersionHistory for workflow executions created in local namespaces.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "VersionHistory backfill was implemented for local namespace workflows.",
      "related": [],
      "keyQuote": "Local namespaces does not have VersionHistory set for workflow executions created using local namespaces.",
      "number": 703,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:06.717Z"
    },
    {
      "summary": "Remove unused fields (start version, last write version, current version) from MutableState record and related functions in persistence layer utilities.",
      "category": "other",
      "subcategory": "persistence-cleanup",
      "apis": [],
      "components": [
        "MutableState",
        "sqlExecutionManagerUtil",
        "cassandraPersistenceUtil"
      ],
      "concepts": [
        "code-cleanup",
        "technical-debt",
        "persistence",
        "unused-fields",
        "refactoring"
      ],
      "severity": "low",
      "userImpact": "No direct user impact; this is internal code cleanup to reduce maintenance burden.",
      "rootCause": "Unused fields in MutableState record and createOrUpdateCurrentExecution function that can be safely removed.",
      "proposedFix": "Remove start version, last write version, and current version fields from MutableState, and remove start version from createOrUpdateCurrentExecution function at specified lines.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Issue closed in favor of a broader end-to-end cleanup effort to be handled separately.",
      "related": [],
      "keyQuote": "there are other attr not really being used, close this specific issue, we will handle the e2e cleanup later",
      "number": 702,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:08.629Z"
    },
    {
      "summary": "Remove the admin kafka resend command which is no longer needed after the removal of 2dc in PR #692.",
      "category": "feature",
      "subcategory": "admin-commands",
      "apis": [],
      "components": [
        "admin",
        "kafka",
        "messaging"
      ],
      "concepts": [
        "cleanup",
        "command-removal",
        "legacy-code",
        "simplification"
      ],
      "severity": "low",
      "userImpact": "Removes an obsolete admin command that is no longer functional after other changes.",
      "rootCause": "The 2dc removal made this kafka resend command unnecessary.",
      "proposedFix": "Remove the admin kafka resend command entirely.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The command was already removed prior to this issue being created.",
      "related": [
        692
      ],
      "keyQuote": "After removal of 2dc in PR #692 this is no longer needed.",
      "number": 701,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:11:10.605Z"
    },
    {
      "summary": "HandleErr logic needs to be implemented for HistoryMetadataReplicationTask in the NDC (non-2dc) code path. This functionality was only available in the 2dc path before PR #692 removed all 2dc-specific logic.",
      "category": "feature",
      "subcategory": "history-replication",
      "apis": [],
      "components": [
        "history-replication-task",
        "ndc-replication",
        "error-handling"
      ],
      "concepts": [
        "replication",
        "error-handling",
        "history-metadata",
        "ndc-path",
        "task-processing"
      ],
      "severity": "medium",
      "userImpact": "Without this implementation, NDC replication workflows cannot properly handle errors in history metadata replication tasks.",
      "rootCause": "PR #692 removed 2dc-specific logic including HandleErr for HistoryMetadataReplicationTask, leaving no error handling implementation for the NDC code path.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed through implementation of HandleErr logic for the NDC code path, and later became obsolete once RPC replication stack was adopted.",
      "related": [
        692
      ],
      "keyQuote": "HandleErr for HistoryMetadataReplicationTask was only implemented with 2dc code path. We need to have ndc equivalent implemented",
      "number": 700,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:54.346Z"
    },
    {
      "summary": "History engine codebase needs refactoring into smaller packages to improve test organization and maintainability. Current test suites (TestEngine2Suite, TestEngine3Suite, TestEngineSuite) are messy, particularly after 2DC removal.",
      "category": "other",
      "subcategory": "history-engine-testing",
      "apis": [],
      "components": [
        "history-engine",
        "test-framework",
        "test-suites"
      ],
      "concepts": [
        "refactoring",
        "code-organization",
        "test-maintenance",
        "modularity",
        "package-structure",
        "test-cleanup"
      ],
      "severity": "medium",
      "userImpact": "Internal refactoring to improve code quality and test maintainability, enabling faster development and easier debugging for contributors.",
      "rootCause": "History engine tests became messy after 2DC removal, indicating the need for better code organization into smaller, focused packages.",
      "proposedFix": "Refactor history engine codebase into smaller packages and reorganize test suites for better maintainability.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We need to refactor history engine code base into smaller packages. Following test suites need lots of attention: TestEngine2Suite, TestEngine3Suite, TestEngineSuite",
      "number": 699,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:54.646Z"
    },
    {
      "summary": "Unit tests for activity replication were removed during 2DC deprecation in PR #692. Need to create equivalent NDC tests to restore coverage for activity sync scenarios including activity completion, running states, and version/attempt comparisons.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "activity-replicator",
        "test-suite",
        "ndc"
      ],
      "concepts": [
        "testing",
        "activity-sync",
        "replication",
        "version-management",
        "multi-region",
        "deprecation"
      ],
      "severity": "medium",
      "userImpact": "Lack of test coverage for activity replication in NDC mode reduces confidence in replication correctness and makes future changes riskier.",
      "rootCause": "2DC code path deprecation required removal of tests that exercised 2DC-specific replication behavior, leaving gaps in NDC test coverage.",
      "proposedFix": "Create equivalent NDC tests for the nine removed test cases covering activity completion, running states, version comparisons, and zombie workflow scenarios.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by PR #974 which implemented NDC equivalent tests for the removed 2DC test cases.",
      "related": [
        692,
        974
      ],
      "keyQuote": "We had to remove certain unit test in TestActivityReplicatorSuite as they were not exercising NDC code path. Need to create equivalent test for NDC.",
      "number": 698,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:53.683Z"
    },
    {
      "summary": "Custom tags cannot be added to existing metrics in the Java SDK. Users attempting to add custom tags to Temporal metrics are finding that the tags do not appear in the metric output.",
      "category": "bug",
      "subcategory": "metrics-custom-tags",
      "apis": [],
      "components": [
        "metrics",
        "java-sdk",
        "tag-management"
      ],
      "concepts": [
        "metrics",
        "custom-tags",
        "observability",
        "monitoring",
        "metric-labeling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot properly tag and label their metrics for better observability and monitoring purposes.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Closed as duplicate of sdk-java#200 which contains detailed correspondence about the issue",
      "related": [
        200
      ],
      "keyQuote": "We are trying to add the custom tags but its not getting appeared in the metrics",
      "number": 695,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:41.833Z"
    },
    {
      "summary": "Currently, Temporal doesn't validate or prevent scheduling workflow timers that would fire after the workflow execution timeout expires. The issue requests validation to either fail timer creation or auto-cap the timer duration to respect workflow execution limits.",
      "category": "bug",
      "subcategory": "workflow-timers",
      "apis": [
        "ScheduleWorkflowTimer"
      ],
      "components": [
        "workflow-engine",
        "timer-scheduler",
        "timeout-validation"
      ],
      "concepts": [
        "timeout",
        "timer",
        "workflow-execution",
        "validation",
        "scheduling",
        "business-logic"
      ],
      "severity": "medium",
      "userImpact": "Users can accidentally schedule timers that will never fire due to workflow timeout, leading to unexpected workflow behavior and potential logic errors.",
      "rootCause": "Missing validation logic to check if scheduled timer duration exceeds remaining workflow execution time.",
      "proposedFix": "Add validation to fail timer creation when duration exceeds remaining execution time, or provide a boolean option to enable auto-capping of timer duration to the minimum of remaining execution time vs. specified duration.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The current behavior is to not schedule the timer if it would fire after the workflow timeout, which prevents the invalid scenario.",
      "related": [],
      "keyQuote": "current behavior is NOT to schedule the timer is timer will fire after workflow timeout",
      "number": 690,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:42.928Z"
    },
    {
      "summary": "Request for a dynamic configuration option to enforce maximum duration limits on Workflow Timers. Administrators want the ability to prevent timers exceeding a specified duration from being scheduled, forcing users to adopt ContinueAsNew patterns and preventing workflows from becoming too old.",
      "category": "feature",
      "subcategory": "workflow-timers",
      "apis": [
        "TimerStarted"
      ],
      "components": [
        "workflow-engine",
        "timer-management",
        "configuration"
      ],
      "concepts": [
        "timer-duration",
        "workflow-age",
        "configuration-management",
        "enforcement",
        "ContinueAsNew",
        "scheduler-limits"
      ],
      "severity": "medium",
      "userImpact": "Administrators gain operational control to enforce organizational policies on workflow timer durations and prevent workflows from exceeding acceptable age limits.",
      "rootCause": null,
      "proposedFix": "Add a dynamic config knob to limit the maximum duration allowed for Workflow Timers",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Administrators may want to enforcer that Workflow Timers beyond a certain duration are not allowed to be scheduled",
      "number": 689,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:41.360Z"
    },
    {
      "summary": "Parent workflows currently exit immediately even if child workflows with abandon parent policy haven't started yet, preventing those children from ever starting. The proposal is to automatically delay workflow completion until abandoned children have reported successful startup.",
      "category": "feature",
      "subcategory": "child-workflow-management",
      "apis": [
        "ChildWorkflowStub.getExecution()"
      ],
      "components": [
        "workflow-task-handler",
        "child-workflow-logic",
        "decision-task",
        "workflow-completion"
      ],
      "concepts": [
        "child-workflow-start",
        "abandon-parent-policy",
        "workflow-lifecycle",
        "timing",
        "completion-delay",
        "parent-child-coordination"
      ],
      "severity": "medium",
      "userImpact": "Users must use obscure workarounds (waiting for Promise from getExecution()) to ensure child workflows start before parent completion, which is error-prone.",
      "rootCause": "Workflow completion happens before child workflow start requests are processed when parent exits early, combined with abandon parent policy semantics.",
      "proposedFix": "Automatically delay workflow completion by failing and rescheduling the decision task until child workflows with abandon parent policy have successfully started.",
      "workaround": "Wait for Promise returned from ChildWorkflowStub.getExecution() before allowing parent workflow to complete.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        74
      ],
      "keyQuote": "Currently, if a parent workflow exits before a child workflow has started the child is not going to start at all.",
      "number": 685,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:27.860Z"
    },
    {
      "summary": "Request to add DynamoDB as a persistence driver option for Temporal. The issue explores feasibility, noting DynamoDB's transaction size limitations and schema differences from SQL-based persistence drivers, with discussion about whether single-shard or alternative designs could work.",
      "category": "feature",
      "subcategory": "persistence-driver",
      "apis": [],
      "components": [
        "persistence-driver",
        "dynamodb-adapter",
        "transaction-handler",
        "schema-mapper"
      ],
      "concepts": [
        "dynamodb",
        "persistence",
        "transactions",
        "sharding",
        "aws-integration",
        "schema-design",
        "scalability"
      ],
      "severity": "medium",
      "userImpact": "AWS-focused enterprises cannot use Temporal at scale without PostgreSQL or Cassandra, limiting adoption for DynamoDB-native organizations.",
      "rootCause": "DynamoDB's transaction API has strict size limitations and lacks ACID semantics that Temporal's normalized SQL schema relies on, making direct persistence driver implementation infeasible.",
      "proposedFix": "Investigate single-table DynamoDB design patterns with denormalization to avoid transactions; create proof-of-concept leveraging DynamoDB's schemaless nature and access pattern-based design.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Closed due to DynamoDB transaction size limitations being incompatible with Temporal's persistence requirements. Initial assessment concluded it's not feasible as primary persistence store.",
      "related": [],
      "keyQuote": "DynamoDB transactions are limited just by size. Given this constraint, isn't it better to have just one shard?",
      "number": 684,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:29.770Z"
    },
    {
      "summary": "User inquires about backpressure support in Temporal to prevent overwhelming the database with excessive requests. The issue discusses existing service protection mechanisms and configuration options available for rate limiting.",
      "category": "question",
      "subcategory": "backpressure-rate-limiting",
      "apis": [],
      "components": [
        "frontend-service",
        "persistence-layer",
        "rate-limiter"
      ],
      "concepts": [
        "backpressure",
        "rate-limiting",
        "database-protection",
        "QPS",
        "service-protection",
        "load-handling"
      ],
      "severity": "low",
      "userImpact": "Users need guidance on how to configure backpressure mechanisms to prevent database overload from high-volume requests.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Configure service RPS limits and PersistenceMaxQPS dynamic config knobs to protect the database from being overwhelmed by the service.",
      "resolution": "stale",
      "resolutionDetails": "Closed due to inactivity. The issue was answered with existing configuration options (RPS and PersistenceMaxQPS) that already provide backpressure protection.",
      "related": [],
      "keyQuote": "Temporal already has lots of service protection baked in the system to protect DB from getting overloaded.",
      "number": 683,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:30.456Z"
    },
    {
      "summary": "Workflows cannot signal themselves, throwing UnknownExternalWorkflowExecution error. Users request this capability as an enhancement for metrics tracking and signal-driven architecture patterns, though the server currently prevents it due to implementation constraints (deadlock risk from locking).",
      "category": "feature",
      "subcategory": "signal-external-workflow",
      "apis": [
        "SignalExternalWorkflow",
        "GetInfo"
      ],
      "components": [
        "signal-processing",
        "workflow-execution",
        "external-workflow"
      ],
      "concepts": [
        "self-signaling",
        "signal-delivery",
        "deadlock-prevention",
        "workflow-communication",
        "metrics",
        "locking",
        "command-processing"
      ],
      "severity": "medium",
      "userImpact": "Users cannot use self-signaling for metrics tracking or architectural patterns requiring workflows to signal themselves, forcing workarounds like local channels.",
      "rootCause": "Server's signal command processing holds a lock on source execution while writing to target execution; same source/target causes deadlock, so server blocks self-signals as a protection mechanism.",
      "proposedFix": "Implement safe self-signaling support to allow workflows to signal themselves without causing deadlocks; suggested improvement: better error message (WorkflowCantSignalItself instead of UnknownExternalWorkflowExecution).",
      "workaround": "Use local workflow channels to pipe signals instead of SignalExternalWorkflow for self-signals.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal server does not have support for signal command which tries to send a signal to itself... If source and target are the same then it results in a deadlock",
      "number": 682,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:23:54.093Z"
    },
    {
      "summary": "Workflow code currently cannot wait for the completion of external workflows (workflows not started as child workflows). This feature would enable deciders to await completion of any workflow by ID, similar to how child workflows can be monitored.",
      "category": "feature",
      "subcategory": "workflow-coordination",
      "apis": [
        "StartWorkflow",
        "ExecuteChildWorkflow"
      ],
      "components": [
        "workflow-engine",
        "decision-handler",
        "event-system"
      ],
      "concepts": [
        "external-workflow",
        "workflow-completion",
        "workflow-parenting",
        "state-coordination",
        "async-coordination"
      ],
      "severity": "medium",
      "userImpact": "Users cannot elegantly coordinate with independently-started workflows from within a parent workflow, forcing workarounds like polling activities or using Nexus handlers.",
      "rootCause": "The Temporal workflow engine lacks native support for waiting on external workflow completion within workflow code; currently only child workflows (parent-started) and external client polling are supported.",
      "proposedFix": "Add WaitForExternalWorkflowCompletion decision, corresponding ExternalWorkflowCompleted/Failed/Cancelled/Terminated/TimedOut events, and WaitForExternalWorkflowCompletionFailed event.",
      "workaround": "Use an 'infinite activity' that executes the client-side workflow.execute() with USE_EXISTING conflict policy and heartbeats to track progress, or use Nexus WorkflowRunOperation for looser coupling.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        656
      ],
      "keyQuote": "Ability waiting for the completion of any workflow from a decider code is missing.",
      "number": 680,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:23:54.233Z"
    },
    {
      "summary": "TCTL commands truncate workflow activity input/output with ellipses, making it difficult to view complete data. Users need either truncation disabled or a flag to control the truncation behavior for full output display.",
      "category": "feature",
      "subcategory": "cli-output",
      "apis": [],
      "components": [
        "tctl",
        "cli",
        "workflow-display"
      ],
      "concepts": [
        "truncation",
        "output-formatting",
        "user-experience",
        "data-visibility",
        "pretty-print"
      ],
      "severity": "low",
      "userImpact": "Users cannot see complete workflow activity input/output in TCTL commands, forcing them to export to JSON files as a workaround.",
      "rootCause": null,
      "proposedFix": "Add a unified flag to control truncation limits for text displayed in Table/Card outputs, allowing users to see full content when needed.",
      "workaround": "Export output to a local JSON file to view complete data.",
      "resolution": "fixed",
      "resolutionDetails": "Addressed through CLI redesign work with a unified flag to control truncation limits for Table/Card output formatting.",
      "related": [],
      "keyQuote": "add a flag that lets us pretty print the full output",
      "number": 678,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:23:53.109Z"
    },
    {
      "summary": "History events are currently loaded by batch count, but when users write large payloads a single batch can be oversized. This causes unnecessary events to be loaded and discarded when the actual page size limit is reached. The feature request is to support loading events based on size in bytes rather than batch count.",
      "category": "feature",
      "subcategory": "history-pagination",
      "apis": [],
      "components": [
        "history-store",
        "event-pagination",
        "database-query"
      ],
      "concepts": [
        "batch-loading",
        "pagination",
        "payload-size",
        "memory-efficiency",
        "database-performance"
      ],
      "severity": "medium",
      "userImpact": "Users with large event payloads experience inefficient history loading and unnecessary memory consumption due to oversized batch reads.",
      "rootCause": "History store pagination uses batch count rather than cumulative byte size, causing misalignment between requested page size and actual data loaded.",
      "proposedFix": "Implement size-based history event loading that respects byte limits instead of batch count limits, with optional throttling based on payload sizes.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        675
      ],
      "keyQuote": "We may end of loading unnecessary events which we have to throw away when returning the events back to client based on the page size set by the caller.",
      "number": 677,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:23:14.708Z"
    },
    {
      "summary": "Current rate limiting is based on API call RPS only and doesn't account for large payload sizes, causing the database to load and ship huge amounts of data over the wire. The request is to implement payload-size-aware rate limiting that throttles namespaces exceeding configured quotas.",
      "category": "feature",
      "subcategory": "rate-limiting",
      "apis": [
        "GetHistory"
      ],
      "components": [
        "rate-limiter",
        "api-handler",
        "history-service",
        "namespace-throttling"
      ],
      "concepts": [
        "rate-limiting",
        "payload-size",
        "quota-management",
        "throttling",
        "database-performance",
        "network-efficiency"
      ],
      "severity": "medium",
      "userImpact": "Users with large history payloads experience poor performance and potential service degradation due to inadequate rate limiting that doesn't consider data volume.",
      "rootCause": "Rate limiting logic only considers API call count (RPS) without accounting for the volume of data being transferred, making it ineffective for operations like GetHistory that return large payloads.",
      "proposedFix": "Implement rate limiting that considers payload sizes and throttles namespaces that exceed configured quota thresholds.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Have rate limiting logic which also takes into account payload sizes and throttle namespaces which goes over configured quota.",
      "number": 675,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:23:14.709Z"
    },
    {
      "summary": "For workflows with long history, paginating through history events during replay can exceed the workflow task timeout, causing task retries and system thrashing. The request is to implement workflow task heartbeats during history pagination to keep the timeout renewed.",
      "category": "feature",
      "subcategory": "workflow-replay",
      "apis": [],
      "components": [
        "workflow-task-handler",
        "history-pagination",
        "replay-engine"
      ],
      "concepts": [
        "timeout",
        "heartbeat",
        "history-pagination",
        "workflow-task",
        "replay",
        "long-history"
      ],
      "severity": "high",
      "userImpact": "Workflows with long history fail with timeout errors and cause cascading retries that degrade system performance.",
      "rootCause": "History pagination during workflow replay can take longer than the workflow task timeout, causing the task to be retried on a different host, restarting the entire history replay.",
      "proposedFix": "Implement workflow task heartbeat mechanism during history pagination to extend the timeout while the client is reading paginated history, similar to the heartbeat mechanism used for local activities.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "For workflow executions with long history it could take a longer than workflow task timeout to paginate through history events during replay.",
      "number": 674,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:23:14.265Z"
    },
    {
      "summary": "Setting PROMETHEUS_ENDPOINT in docker-compose causes bind address conflicts and metric registration errors because multiple service roles (frontend, history, matching, worker) attempt to listen on the same port simultaneously.",
      "category": "bug",
      "subcategory": "metrics-prometheus",
      "apis": [],
      "components": [
        "prometheus-reporter",
        "metrics",
        "server-roles"
      ],
      "concepts": [
        "port-binding",
        "metrics-collection",
        "prometheus-integration",
        "docker-compose",
        "service-isolation",
        "metric-registration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot enable Prometheus metrics in docker-compose deployments due to port conflicts and incomplete metrics collection.",
      "rootCause": "All four service roles (frontend, history, matching, worker) are instantiated with the same listen address in docker-compose, causing TCP bind failures on the same port and metric descriptor conflicts.",
      "proposedFix": "Support multiple Prometheus ports for each service role, or run each role as a separate container/pod with its own port.",
      "workaround": "Run each service role as a separate pod in Kubernetes where each has its own process and port.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was fixed but the fix was initially merged only to miroswan/temporal fork before being merged to main temporalio/temporal repository.",
      "related": [],
      "keyQuote": "Since docker compose runs each service on the same host, we'll need four different prometheus ports for TCP binding.",
      "number": 673,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:22:36.246Z"
    },
    {
      "summary": "Temporal Server 0.28.0 fails to start with CockroachDB (PostgreSQL-compatible) due to a missing 'version_partition' column in the schema_version table. The schema migration tool reports the column does not exist when trying to read the current schema version.",
      "category": "bug",
      "subcategory": "database-schema",
      "apis": [],
      "components": [
        "schema-migration",
        "database-setup",
        "postgres-compatibility"
      ],
      "concepts": [
        "schema-versioning",
        "database-compatibility",
        "column-missing",
        "migration-failure",
        "initialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot start Temporal Server 0.28.0 with CockroachDB, blocking deployment and initialization in containerized environments.",
      "rootCause": "The schema_version table is missing the 'version_partition' column that the schema update migration tool expects when reading the current schema version.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved in a subsequent version after identifying the schema version table structure mismatch between CockroachDB and Temporal's expectations.",
      "related": [],
      "keyQuote": "error reading current schema version:pq: column \"version_partition\" does not exist",
      "number": 668,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:22:36.048Z"
    },
    {
      "summary": "Server logs are emitted in JSON format, which is suitable for production but makes local development and debugging painful because stack traces are encoded as single-line strings with escaped newlines and tabs. A pretty-printing or console log format mode is needed for improved readability during debugging.",
      "category": "feature",
      "subcategory": "logging",
      "apis": [],
      "components": [
        "logging",
        "server",
        "panic-handler"
      ],
      "concepts": [
        "readability",
        "debugging",
        "log-formatting",
        "stack-traces",
        "developer-experience",
        "local-development"
      ],
      "severity": "low",
      "userImpact": "Developers debugging local Temporal server instances have difficulty reading error messages and stack traces due to JSON encoding.",
      "rootCause": null,
      "proposedFix": "Add a mode for pretty logging that developers can use locally to have greater readability during debugging.",
      "workaround": "Set the log format from default JSON to console via configuration.",
      "resolution": "fixed",
      "resolutionDetails": "The solution was pointed out in the comments - the server already supports switching from JSON to console log format via configuration.",
      "related": [],
      "keyQuote": "We should add a mode for pretty logging, which developers can use locally when they want to have greater readability during debugging.",
      "number": 664,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:22:36.031Z"
    },
    {
      "summary": "CLI displays ambiguous 'context deadline exceeded' error during server failures instead of showing actual error details, and times out instead of returning immediately when errors occur.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "cli",
        "error-handler",
        "context-management"
      ],
      "concepts": [
        "error-handling",
        "timeout",
        "context-deadline",
        "user-feedback",
        "logging"
      ],
      "severity": "medium",
      "userImpact": "Users cannot diagnose CLI failures without manually checking server logs, and experience unnecessary delays waiting for context deadlines.",
      "rootCause": "CLI does not properly propagate server errors and instead waits for context deadline to be exceeded before returning, masking the actual error cause.",
      "proposedFix": null,
      "workaround": "Check server logs to find the actual error cause.",
      "resolution": "invalid",
      "resolutionDetails": "Issue deemed non-specific and potentially already fixed by error handling improvements. Requester asked to reactivate with specific details if still occurring.",
      "related": [],
      "keyQuote": "CLI times out and displays ambiguous error saying \"Error Details: context deadline exceeded\". Actual error cause can only be found in the server log.",
      "number": 663,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:21:57.501Z"
    },
    {
      "summary": "User requests addition of ConnectTimeout configuration option for Cassandra database connections in Temporal Server. This would allow operators to customize the connection timeout duration via YAML configuration.",
      "category": "feature",
      "subcategory": "cassandra-config",
      "apis": [],
      "components": [
        "cassandra-cluster",
        "config-parser",
        "persistence-layer"
      ],
      "concepts": [
        "timeout",
        "connection",
        "configuration",
        "cassandra",
        "networking"
      ],
      "severity": "medium",
      "userImpact": "Operators cannot customize Cassandra connection timeouts, limiting flexibility in network configurations with varying latency characteristics.",
      "rootCause": null,
      "proposedFix": "Add ConnectTimeout field to Cassandra config struct and parse it in cassandraCluster.NewCassandraCluster using YAML's built-in time.Duration support.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented in PR #679 with ConnectTimeout configuration option added server-side, leveraging YAML parser's native time.Duration support.",
      "related": [
        679
      ],
      "keyQuote": "I've added this config option on the server side in #679, luckily standard yaml parser already supports `time.Duration`",
      "number": 662,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:21:56.756Z"
    },
    {
      "summary": "respondWorkflowTaskCompleted returns OK instead of failing with INVALID_ARGUMENT when commands are malformed, such as RECORD_MARKER commands missing required attributes.",
      "category": "bug",
      "subcategory": "workflow-task-handling",
      "apis": [
        "respondWorkflowTaskCompleted",
        "pollWorkflowTaskQueue",
        "startWorkflowExecution"
      ],
      "components": [
        "history-engine",
        "workflow-task-handler",
        "command-validation"
      ],
      "concepts": [
        "validation",
        "error-handling",
        "command-processing",
        "gRPC-response",
        "malformed-input"
      ],
      "severity": "medium",
      "userImpact": "Users do not receive proper error feedback when submitting malformed workflow task commands, leading to silent failures and workflow execution issues.",
      "rootCause": "The respondWorkflowTaskCompleted endpoint does not validate command attributes before returning OK, allowing invalid commands to be processed without error.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The service was updated to properly validate command attributes in respondWorkflowTaskCompleted and return INVALID_ARGUMENT gRPC errors for malformed commands.",
      "related": [],
      "keyQuote": "respondWorkflowTaskCompleted call is expected to throw INVALID_ARGUMENT gRPC failure if commands are not formatted properly.",
      "number": 652,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:21:57.136Z"
    },
    {
      "summary": "Feature request to include CLI and server version information in generated JSON files such as history dumps for better traceability and debugging.",
      "category": "feature",
      "subcategory": "cli-output",
      "apis": [],
      "components": [
        "cli",
        "json-export",
        "history-dump"
      ],
      "concepts": [
        "version-tracking",
        "metadata",
        "traceability",
        "debugging",
        "generated-files"
      ],
      "severity": "low",
      "userImpact": "Users cannot easily determine which CLI/server version generated exported files, making it harder to troubleshoot version-specific issues.",
      "rootCause": null,
      "proposedFix": "Add CLI and server version fields to generated JSON output files.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Declined by maintainer (yiminc) as not a priority.",
      "related": [],
      "keyQuote": "Won't do.",
      "number": 651,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:21:16.871Z"
    },
    {
      "summary": "List workflow execution APIs require namespace and filter parameters that should have sensible defaults. The namespace should default to 'default' and time filters should not have confusing validation errors when only some fields are set.",
      "category": "bug",
      "subcategory": "list-workflows-api",
      "apis": [
        "ListOpenWorkflowExecutions",
        "ListClosedWorkflowExecutions"
      ],
      "components": [
        "list-workflows-api",
        "namespace-handling",
        "time-filter-validation"
      ],
      "concepts": [
        "api-defaults",
        "required-parameters",
        "namespace-management",
        "time-filtering",
        "validation-logic"
      ],
      "severity": "medium",
      "userImpact": "Users must explicitly provide namespace and carefully craft time filters when calling list workflow APIs, increasing API complexity and causing unexpected validation errors.",
      "rootCause": "Server design philosophy delegates default namespace handling to SDKs rather than providing server-side defaults. Time filter validator requires both earliest and latest time even when only one is logically necessary.",
      "proposedFix": "SDKs should follow GoSDK pattern of setting default namespace client-side. Fix time filter validation to allow earliest time without requiring latest time.",
      "workaround": "Explicitly set namespace to 'default' and provide both earliest and latest time values when calling list operations.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved by SDK implementation guidance (SDKs handle default namespace) and time filter validation fixes in related issues #641 and #1631.",
      "related": [
        641,
        1631
      ],
      "keyQuote": "Server doesn't have a concept of 'default namespace'. Clients are supposed to set default namespace if it is not set.",
      "number": 636,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:21:19.366Z"
    },
    {
      "summary": "Admin users cannot control the initial state of the archival feature through docker-compose parameters. The request is to add this configuration option with archival disabled by default.",
      "category": "feature",
      "subcategory": "docker-compose-configuration",
      "apis": [],
      "components": [
        "docker-compose",
        "archival-feature",
        "configuration"
      ],
      "concepts": [
        "feature-enablement",
        "initial-state",
        "docker-configuration",
        "admin-control",
        "default-behavior"
      ],
      "severity": "low",
      "userImpact": "Administrators cannot configure the archival feature state when deploying Temporal via docker-compose, limiting deployment flexibility.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Duplicate of issue #595",
      "related": [
        595
      ],
      "keyQuote": "As an admin i want to be able to control the initial state of archival feature through a docker-compose variable.",
      "number": 623,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:21:17.633Z"
    },
    {
      "summary": "AWS Elasticsearch instances on ports 80/443 fail to connect due to sniffing being enabled in the Elasticsearch client. The error 'context deadline exceeded' occurs because sniffing is not supported on HTTP-only ports. Disabling sniffing via a config flag would allow connections to AWS Elasticsearch instances.",
      "category": "bug",
      "subcategory": "elasticsearch-connectivity",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "visibility-service",
        "configuration"
      ],
      "concepts": [
        "elasticsearch",
        "sniffing",
        "aws",
        "connectivity",
        "http",
        "port",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal on AWS Elasticsearch cannot use advanced visibility features due to connection failures with HTTP-only endpoints.",
      "rootCause": "Elasticsearch client has sniffing enabled by default, which is incompatible with AWS Elasticsearch's HTTP-only access on ports 80/443. Sniffing requires TCP on port 9200.",
      "proposedFix": "Add a configuration flag to disable Elasticsearch sniffing for environments like AWS where only HTTP on ports 80/443 is supported.",
      "workaround": "Disable sniffing by adding `elastic.SetSniff(false)` when creating the Elasticsearch client.",
      "resolution": "wontfix",
      "resolutionDetails": "Maintainer indicated sniffing is already disabled by default in Temporal, suggesting the issue may have been resolved in a later version or the user's configuration was incorrect.",
      "related": [
        880
      ],
      "keyQuote": "AWS only supports HTTP for their Elasticsearch instances on port 80 or port 443. Elasticsearch doesn't support sniffing on those particular ports, you need to be using TCP on port 9200.",
      "number": 621,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:20:39.736Z"
    },
    {
      "summary": "TestActivityHeartbeatTimeouts test has a data race condition between goroutines reading and writing to the same memory location, causing the test to be flaky. The issue involves concurrent access to shared state in activity heartbeat timeout handling.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "activity-executor",
        "task-poller",
        "test-harness"
      ],
      "concepts": [
        "data-race",
        "concurrency",
        "heartbeat",
        "activity-retry",
        "goroutine-synchronization"
      ],
      "severity": "high",
      "userImpact": "Flaky tests reduce confidence in the codebase and complicate CI/CD pipelines, making it harder to distinguish real failures from race conditions.",
      "rootCause": "Unsynchronized concurrent access to memory location 0x00c0020682f8 between the main test goroutine and activity task polling goroutine in TestActivityHeartbeatTimeouts.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The comment indicates this data race detection is already enabled and running as part of the buildkite builds, suggesting the issue was resolved through the race detector integration.",
      "related": [],
      "keyQuote": "It is data race actually: Read at 0x00c0020682f8 by goroutine 2198... Previous write at 0x00c0020682f8 by goroutine 2439",
      "number": 620,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:20:39.593Z"
    },
    {
      "summary": "TestActivityHeartbeatTimeouts test is flaky, intermittently failing because activities are unexpectedly completing instead of timing out. Investigation revealed possible bug in timer task handling for heartbeat timeouts.",
      "category": "bug",
      "subcategory": "activity-heartbeat",
      "apis": [],
      "components": [
        "activity-executor",
        "timer-manager",
        "test-framework"
      ],
      "concepts": [
        "timeout",
        "heartbeat",
        "timer-task",
        "flaky-test",
        "race-condition"
      ],
      "severity": "medium",
      "userImpact": "Unreliable test flakiness masks potential production bugs with activity heartbeat timeout handling.",
      "rootCause": "Possible loss of timer tasks for heartbeat timeouts in the activity executor",
      "proposedFix": null,
      "workaround": null,
      "resolution": "stale",
      "resolutionDetails": "Closed as stale - no occurrence of this issue in buildkite logs over recent months",
      "related": [],
      "keyQuote": "This could be an actual bug where we are loosing timer tasks for heartbeat timeouts.",
      "number": 619,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:20:38.990Z"
    },
    {
      "summary": "CrossDC configuration in Temporal still uses the legacy `cadence-cluster-topics` key for Kafka topic mapping. This needs to be renamed to `temporal-cluster-topics` to align with the Temporal branding.",
      "category": "bug",
      "subcategory": "config",
      "apis": [],
      "components": [
        "cross-dc",
        "kafka",
        "cluster-config"
      ],
      "concepts": [
        "configuration",
        "naming",
        "kafka-integration",
        "cross-region",
        "cluster-mapping"
      ],
      "severity": "low",
      "userImpact": "Users operating Temporal in cross-datacenter mode may experience confusion due to legacy Cadence naming in configuration keys.",
      "rootCause": "Legacy naming convention from Cadence migration not fully updated in CrossDC configuration.",
      "proposedFix": "Rename the `cadence-cluster-topics` configuration key to `temporal-cluster-topics`.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Configuration key was renamed from cadence-cluster-topics to temporal-cluster-topics.",
      "related": [],
      "keyQuote": "CrossDC config in Temporal still uses the key `cadence-cluster-topics` for Temporal cluster to Kafka topic mapping. We need to rename this to `temporal-cluster-topics`",
      "number": 610,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:19:59.187Z"
    },
    {
      "summary": "Lack of unit/integration tests for parentclosepolicy workflow allowed a nil context bug to slip through to stress testing. The issue highlights missing test coverage that should have caught the basic error earlier in development.",
      "category": "bug",
      "subcategory": "test-coverage",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "workflow",
        "activity-executor",
        "test-framework"
      ],
      "concepts": [
        "testing",
        "context",
        "nil-reference",
        "workflow-execution",
        "bug-prevention"
      ],
      "severity": "medium",
      "userImpact": "Missing test coverage allows basic bugs like nil context errors to reach production instead of being caught during development.",
      "rootCause": "Activity was not passing in a context, causing workflow failures that were only caught by stress testing rather than unit/integration tests.",
      "proposedFix": "Add unit/integration tests for parentclosepolicy workflow and NilContextWorkflow to ensure proper context handling in activities.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2023
      ],
      "keyQuote": "Although we fixed the bug, there is a lack of unit/integration testing that should have protected us against this",
      "number": 607,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:19:59.520Z"
    },
    {
      "summary": "Parent workflow termination does not propagate to child workflows. When a parent workflow is terminated, child workflows with TERMINATE parent close policy remain in Running state instead of being terminated, and the parent close policy processor crashes with a nil pointer dereference when attempting to terminate children.",
      "category": "bug",
      "subcategory": "parent-close-policy",
      "apis": [
        "TerminateWorkflowExecution"
      ],
      "components": [
        "parent-close-policy",
        "history-client",
        "workflow-execution"
      ],
      "concepts": [
        "cascading-termination",
        "parent-child-workflows",
        "workflow-lifecycle",
        "context-handling",
        "nil-pointer"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably terminate hierarchical workflow structures; child workflows continue running even after parent termination, and the system processor crashes when attempting cleanup.",
      "rootCause": "The parent close policy processor passes nil context when calling TerminateWorkflowExecution, causing a nil pointer dereference in the IsValidContext validation function.",
      "proposedFix": "Either pass the actual context instead of nil when terminating child workflows, or modify context validation to handle nil context gracefully.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Context handling in parent close policy was corrected to pass valid context or handle nil context appropriately.",
      "related": [],
      "keyQuote": "Interestingly, we are passing a nil context when terminating from the system workflow for both terminate and cancel: Ultimately, that causes the nil reference exception",
      "number": 604,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:20:00.022Z"
    },
    {
      "summary": "Add support for MySQL database persistence in tctl admin commands that require direct database access. Currently, only Cassandra is supported for these operational commands.",
      "category": "feature",
      "subcategory": "database-persistence",
      "apis": [],
      "components": [
        "tctl",
        "persistenceUtil",
        "admin-commands"
      ],
      "concepts": [
        "database-persistence",
        "mysql-support",
        "direct-database-access",
        "operational-tooling",
        "admin-commands"
      ],
      "severity": "medium",
      "userImpact": "Users unable to run admin commands that require direct database access when using MySQL as their persistence backend.",
      "rootCause": "Implementation only supports Cassandra for direct database access in admin commands, MySQL support is missing.",
      "proposedFix": "Add MySQL database connection support similar to the existing Cassandra implementation in persistenceUtil.go.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "MySQL and PostgreSQL direct database access support was added via PR #1002.",
      "related": [
        1002
      ],
      "keyQuote": "tctl to support admin commands which directly connects to the database.",
      "number": 598,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:19:21.086Z"
    },
    {
      "summary": "Docker Compose setup enables archival by default, but for development purposes archival should be disabled. The issue requests moving archival configuration to a separate docker-compose-archival.yaml file.",
      "category": "feature",
      "subcategory": "docker-setup",
      "apis": [],
      "components": [
        "docker-compose",
        "archival",
        "file-storage"
      ],
      "concepts": [
        "development-environment",
        "archival-configuration",
        "deployment",
        "docker"
      ],
      "severity": "low",
      "userImpact": "Developers using Docker Compose for local development unnecessarily have archival enabled, which adds overhead and complexity for non-production use cases.",
      "rootCause": "Docker Compose configuration includes archival setup by default intended for production use",
      "proposedFix": "Disable archival in the default docker-compose.yaml and create a separate docker-compose-archival.yaml for users who need archival functionality",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Archival was disabled by default in the Docker Compose configuration",
      "related": [],
      "keyQuote": "Running temporal for development using `docker-compose up` should not have archival turned on",
      "number": 596,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:19:21.263Z"
    },
    {
      "summary": "Docker image lacks a mechanism to configure archival settings. Users need the ability to override archival configuration when deploying Temporal via Docker.",
      "category": "feature",
      "subcategory": "docker-configuration",
      "apis": [],
      "components": [
        "docker-image",
        "config-template",
        "server-configuration"
      ],
      "concepts": [
        "archival",
        "docker-deployment",
        "configuration-management",
        "environment-override",
        "containerization",
        "server-setup"
      ],
      "severity": "medium",
      "userImpact": "Users cannot configure archival settings when deploying Temporal via Docker, forcing them to use workarounds or manual configuration.",
      "rootCause": null,
      "proposedFix": "Update config_template.yaml to support overriding archival configuration through Docker environment variables or configuration files.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Temporal docker image does not provide a mechanism for users to pass in there own archival config for the server.",
      "number": 595,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:19:20.833Z"
    },
    {
      "summary": "TestActivityHeartbeatDetailsDuringRetry test was flaky but resolved itself through consistent passing over time.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-suite",
        "activity-heartbeat",
        "retry-logic"
      ],
      "concepts": [
        "flaky-test",
        "heartbeat",
        "retry",
        "test-stability",
        "activity"
      ],
      "severity": "low",
      "userImpact": "Test flakiness could mask legitimate issues or waste developer time with unreliable test results.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Test passed consistently over the last month, indicating the underlying flakiness was resolved.",
      "related": [],
      "keyQuote": "Resolving this issue as it is passing consistently during last month.",
      "number": 593,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:18:43.054Z"
    },
    {
      "summary": "TestCronChildWorkflowExecution test is flaky with nil pointer assertions. The error indicates a race condition or timing issue in the test suite where an expected value is unexpectedly nil.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "integration-tests",
        "cron-workflows",
        "child-workflows"
      ],
      "concepts": [
        "flaky-test",
        "race-condition",
        "timing",
        "test-reliability",
        "nil-pointer"
      ],
      "severity": "medium",
      "userImpact": "Flaky tests reduce developer productivity and reliability of the CI/CD pipeline, making it harder to identify real issues.",
      "rootCause": "Likely a timing or synchronization issue in the cron child workflow execution test where a value that should be set is null at assertion time.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Line number alignment was corrected in the test, resolving the flaky assertion.",
      "related": [],
      "keyQuote": "Expected value not to be nil.",
      "number": 592,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:18:43.700Z"
    },
    {
      "summary": "The TestCronWorkflow integration test is flaky, with a backoff calculation assertion failing intermittently. The test expects backoff 2-0 to be a multiplier of target backoff 3, but occasionally gets different values.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "integration-test",
        "cron-workflow",
        "backoff-calculator"
      ],
      "concepts": [
        "flaky-test",
        "backoff",
        "multiplier",
        "cron",
        "timing"
      ],
      "severity": "medium",
      "userImpact": "Flaky tests reduce confidence in the codebase and can block releases or CI pipelines.",
      "rootCause": "Timing or race condition in backoff calculation during cron workflow execution, causing intermittent assertion failures.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Test was fixed or underlying cron/backoff logic was corrected to eliminate the flakiness.",
      "related": [],
      "keyQuote": "exected backoff 2-0 should be multiplier of target backoff 3",
      "number": 591,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:18:43.718Z"
    },
    {
      "summary": "TestWorkflowTimeout test in the integration test suite is flaky, sometimes expecting 1 result but getting 2. The issue was reported as passing consistently after investigation.",
      "category": "bug",
      "subcategory": "test-flakiness",
      "apis": [],
      "components": [
        "integration-test",
        "workflow-timeout",
        "test-framework"
      ],
      "concepts": [
        "flaky-test",
        "timeout",
        "test-reliability",
        "race-condition",
        "test-execution"
      ],
      "severity": "medium",
      "userImpact": "Flaky tests reduce confidence in the test suite and make it harder to identify real issues versus test infrastructure problems.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Test was confirmed to be passing consistently after investigation, indicating the flakiness issue was resolved.",
      "related": [],
      "keyQuote": "This is passing consistently throughput last month.",
      "number": 590,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:18:05.801Z"
    },
    {
      "summary": "The listTaskQueuePartitions API in MatchingService has a bug that should have been caught by basic unit tests. A fix is ready but the codebase needs better test coverage for this functionality.",
      "category": "bug",
      "subcategory": "matching-service",
      "apis": [
        "listTaskQueuePartitions"
      ],
      "components": [
        "matching-service",
        "task-queue",
        "matching-engine"
      ],
      "concepts": [
        "unit-testing",
        "api-implementation",
        "bug-detection",
        "test-coverage",
        "code-quality"
      ],
      "severity": "medium",
      "userImpact": "Users may encounter unexpected behavior from the listTaskQueuePartitions API due to implementation bugs that lack test coverage.",
      "rootCause": "Implementation bug in listTaskQueuePartitions, potentially introduced during merge",
      "proposedFix": "Add comprehensive unit tests for listTaskQueuePartitions implementation",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "There was a fairly blatant bug in the implementation of the API... Even the most basic of unit tests would have caught this.",
      "number": 589,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:18:04.992Z"
    },
    {
      "summary": "SQL Visibility API paging logic had multiple bugs that were only discovered during manual testing of a separate issue. Test coverage for this critical component needs to be significantly improved to prevent future regressions.",
      "category": "bug",
      "subcategory": "visibility-paging",
      "apis": [],
      "components": [
        "visibility-api",
        "sql-paging",
        "persistence-layer"
      ],
      "concepts": [
        "paging",
        "sql",
        "visibility",
        "test-coverage",
        "regression-prevention",
        "pagination-logic"
      ],
      "severity": "high",
      "userImpact": "Bugs in SQL visibility paging can cause data retrieval failures or incorrect results in workflow history queries, affecting all users relying on visibility APIs.",
      "rootCause": "Insufficient unit test coverage for SQL paging logic allowed multiple bugs to go undetected until manual testing during a separate bug fix.",
      "proposedFix": "Expand unit test coverage for SQL visibility paging APIs to catch edge cases and prevent similar bugs in the future.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed through PRs #809 and #813 which addressed the paging logic issues and presumably added improved test coverage.",
      "related": [
        809,
        813
      ],
      "keyQuote": "Our SQL Paging logic for the Visibility API was broken in multiple ways and was only discovered manually when fixing a separate bug.",
      "number": 588,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:18:05.713Z"
    },
    {
      "summary": "A `tq lp --taskqueue foo` command causes the Temporal server to panic with a nil pointer dereference when marshaling TaskQueuePartitionMetadata. The panic occurs in the gRPC marshaling layer when the server attempts to serialize a nil pointer.",
      "category": "bug",
      "subcategory": "taskqueue-partition-metadata",
      "apis": [],
      "components": [
        "matching-service",
        "grpc-marshaling",
        "taskqueue-partition-metadata",
        "protobuf-serialization"
      ],
      "concepts": [
        "nil-pointer-dereference",
        "panic-recovery",
        "taskqueue-partitions",
        "grpc-serialization",
        "memory-safety"
      ],
      "severity": "critical",
      "userImpact": "Users cannot execute the `tq lp --taskqueue` command without causing the Temporal server process to crash with a segmentation fault.",
      "rootCause": "TaskQueuePartitionMetadata pointer is nil when MarshalToSizedBuffer is called, causing an invalid memory address access during gRPC response marshaling.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The nil pointer dereference in TaskQueuePartitionMetadata marshaling was fixed by ensuring proper validation before serialization.",
      "related": [],
      "keyQuote": "panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x18",
      "number": 586,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:17:27.579Z"
    },
    {
      "summary": "Workflow reset fails when history has gaps/holes in events, making the workflow unrecoverable even to points before the corruption. The reset command cannot proceed because it reads the entire history before attempting recovery.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-reset",
        "history-reader",
        "tctl"
      ],
      "concepts": [
        "workflow-reset",
        "history-corruption",
        "event-gaps",
        "recoverability",
        "data-integrity"
      ],
      "severity": "high",
      "userImpact": "Users cannot recover workflows when history events have gaps, even to restore them to a point before the corruption occurred.",
      "rootCause": "The reset logic reads the entire workflow history before attempting recovery, failing if any gaps exist rather than allowing reset to an uncorrupted point.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved in issue #661 with potential follow-up to add a flag for best-effort reset in case of corrupted history.",
      "related": [
        584,
        661
      ],
      "keyQuote": "This means the workflow is not recoverable, even to a point before the hole as it reads the whole history before trying to recover.",
      "number": 585,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:17:27.232Z"
    },
    {
      "summary": "A workflow became permanently stuck due to a corrupted history event batch with non-continuous event IDs (event 1851 missing between 1850 and 1852). The workflow repeatedly encounters an error and cannot progress.",
      "category": "bug",
      "subcategory": "history-event-batch",
      "apis": [],
      "components": [
        "history-store",
        "frontend",
        "event-batch-processing"
      ],
      "concepts": [
        "event-continuity",
        "history-corruption",
        "workflow-execution",
        "event-sequencing",
        "data-integrity"
      ],
      "severity": "critical",
      "userImpact": "Workflows can become permanently stuck and unusable when encountering corrupted event batches, with no recovery possible.",
      "rootCause": "Event ID 1851 was lost or not persisted correctly, creating a gap in the event sequence (1850 -> 1852), though the persistence layer (Cassandra) showed no errors.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "stale",
      "resolutionDetails": "Closed due to issue age and no reproduction in CICD pipeline.",
      "related": [],
      "keyQuote": "corrupted history event batch, eventID is not continouous",
      "number": 584,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:17:26.929Z"
    },
    {
      "summary": "Elasticsearch errors like 400 Bad Request are incorrectly wrapped as Internal service errors, triggering unwanted client retry logic. The solution requires replacing Internal with Unavailable error codes to differentiate retriable from non-retriable failures, which is a breaking change requiring SDK/server version coordination.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "error-handling",
        "service-errors"
      ],
      "concepts": [
        "error-codes",
        "retries",
        "elasticsearch-integration",
        "error-wrapping",
        "backwards-compatibility",
        "client-retry-logic"
      ],
      "severity": "high",
      "userImpact": "Users encounter unexpected retry behavior on non-retriable Elasticsearch errors, causing delays and incorrect error handling in their applications.",
      "rootCause": "All Elasticsearch errors are unconditionally wrapped as Internal service errors, which signals to clients that the error is retriable. HTTP 400 errors (bad query) should not be retried.",
      "proposedFix": "Replace Internal error code with Unavailable where appropriate to differentiate retriable failures. This requires updating error handling to use Unavailable instead of Internal for certain error codes.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Changed error communication strategy to use Unavailable instead of Internal for retriable errors, with breaking change requiring newer SDKs to be blocked on old servers.",
      "related": [],
      "keyQuote": "Internal needs to be replaced with Unavailable everywhere where it should be retried. Which is a breaking change and will require us to block newer SDKs on old servers.",
      "number": 582,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:16:49.010Z"
    },
    {
      "summary": "Elasticsearch visibility API calls lack timeout configuration, using context.Background() instead of a timeout context. This can cause requests to hang indefinitely if the Elasticsearch service becomes unresponsive.",
      "category": "bug",
      "subcategory": "visibility-elasticsearch",
      "apis": [],
      "components": [
        "visibility-store",
        "elasticsearch-client",
        "persistence"
      ],
      "concepts": [
        "timeout",
        "context",
        "elasticsearch",
        "visibility",
        "hanging-requests"
      ],
      "severity": "high",
      "userImpact": "Elasticsearch visibility queries can hang indefinitely without a timeout, potentially blocking workflow execution visibility operations.",
      "rootCause": "ESVisibilityStore.SearchWithDSL() calls use context.Background() which has no timeout, allowing requests to block indefinitely",
      "proposedFix": "Add timeout context to Elasticsearch API calls in visibility store operations",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Timeout context was added to Elasticsearch visibility API calls",
      "related": [],
      "keyQuote": "All visibility API calls to elastic search does not set any timeout",
      "number": 581,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:16:47.446Z"
    },
    {
      "summary": "Remove unused ports (7933, 7934, 7935, 7939) that are exposed in the Dockerfile and are leftover from Cadence's tchannel RPC stack, which has been completely removed from the codebase.",
      "category": "bug",
      "subcategory": "docker-configuration",
      "apis": [],
      "components": [
        "dockerfile",
        "docker-image"
      ],
      "concepts": [
        "port-exposure",
        "container-configuration",
        "legacy-code-cleanup",
        "tchannel",
        "cadence-migration"
      ],
      "severity": "low",
      "userImpact": "Reduces unnecessary port exposure in Docker images, improving security posture by removing unused network interfaces.",
      "rootCause": "Leftover ports from Cadence's tchannel RPC stack that was removed but the port exposures were not cleaned up.",
      "proposedFix": "Remove the four unused port declarations (7933, 7934, 7935, 7939) from the Dockerfile.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Closed by #650, which implemented the port removal from the Dockerfile.",
      "related": [
        650
      ],
      "keyQuote": "These seems to be leftover from Cadence as they were used by tchannel rpc stack which we have completely removed.",
      "number": 580,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:16:47.226Z"
    },
    {
      "summary": "The SQL visibility store orders closed workflow queries by start_time instead of close_time, which is confusing and prevents filtering workflows by close time. This needs to be changed to match expected behavior for closed workflows.",
      "category": "bug",
      "subcategory": "visibility-store",
      "apis": [
        "ListClosedWorkflowExecutions"
      ],
      "components": [
        "visibility-store",
        "sql-store",
        "workflow-queries"
      ],
      "concepts": [
        "ordering",
        "filtering",
        "closed-workflows",
        "query-results",
        "visibility",
        "search"
      ],
      "severity": "medium",
      "userImpact": "Users cannot intuitively search or filter closed workflows by close time and get confusing results when listing closed executions.",
      "rootCause": "SQL visibility store queries default to ordering by start_time for all workflow states instead of using close_time for closed workflows.",
      "proposedFix": "Change default ordering for closed workflow queries to use close_time instead of start_time.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implementation completed for SQL visibility stores to order closed workflows by close_time and support filtering by close_time.",
      "related": [
        312
      ],
      "keyQuote": "Currently all queries in non ES visibility are ordered by start_time which is very confusing for closed workflows where by close time ordering is expected.",
      "number": 576,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:16:09.888Z"
    },
    {
      "summary": "Proposal for a background job to scan timer queues and remove timers for completed workflows and tasks, reducing unnecessary timer overhead. Many timeout-protected tasks end up with redundant timers when they complete successfully.",
      "category": "feature",
      "subcategory": "timer-management",
      "apis": [],
      "components": [
        "timer-queue",
        "workflow-manager",
        "background-jobs"
      ],
      "concepts": [
        "timeout",
        "timer-cleanup",
        "resource-optimization",
        "queue-management",
        "workflow-completion"
      ],
      "severity": "medium",
      "userImpact": "Users can benefit from reduced resource overhead by automatically cleaning up unnecessary timers, improving system efficiency and reducing memory/storage usage.",
      "rootCause": null,
      "proposedFix": "Implement a background job that periodically scans all timer queues to identify and delete timer tasks for expired/completed workflows and tasks, with consideration for range delete operations.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "I'm proposing a background job which scans all timer queues and delete any tasks for expired workflows/tasks.",
      "number": 574,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:16:08.638Z"
    },
    {
      "summary": "Temporal currently creates a timeout task for every workflow execution with a server default of 10 years, which creates unnecessary timers in the database for short-lived workflows. The proposal is to introduce an infinite timeout concept and switch the server default to infinite, avoiding creation of timeout tasks when not needed.",
      "category": "feature",
      "subcategory": "workflow-timeout",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-execution",
        "timeout-management",
        "timer-service",
        "server-config"
      ],
      "concepts": [
        "timeout",
        "scalability",
        "resource-cleanup",
        "infinite-timeout",
        "database-accumulation",
        "server-defaults",
        "performance"
      ],
      "severity": "medium",
      "userImpact": "Users would benefit from reduced database overhead and improved scalability by eliminating unnecessary timeout timers for workflows that don't need them.",
      "rootCause": "Server default timeout of 10 years creates timeout tasks for all workflow executions, accumulating timers in the database even for short-lived workflows that don't require cleanup timeouts.",
      "proposedFix": "Introduce a concept of infinite timeout and change the server default for workflow execution/run timeout to infinite, preventing creation of workflow timeout tasks in these cases.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The infinite timeout concept was implemented and the server default was changed to infinite timeout.",
      "related": [],
      "keyQuote": "Introduce a concept of infinite timer and switch the server default for workflow execution/run timeout to be infinite.",
      "number": 573,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:16:10.066Z"
    },
    {
      "summary": "Request for a standardized abbreviation/suffix for 'Temporal' in code. The author seeks a 3-4 character abbreviation similar to how 'Cadence' was abbreviated as 'cdnc', with discussion leading to 'tmpl' as the proposed standard.",
      "category": "question",
      "subcategory": "naming-conventions",
      "apis": [],
      "components": [],
      "concepts": [
        "naming",
        "abbreviation",
        "code-conventions",
        "branding",
        "identifier-naming",
        "standards"
      ],
      "severity": "low",
      "userImpact": "Developers need guidance on standardized abbreviations for Temporal when used as variable names or identifiers in their code.",
      "rootCause": null,
      "proposedFix": "Use 'tmpl' as the standardized abbreviation for Temporal in code",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Community consensus settled on 'tmpl' as the appropriate 4-character abbreviation for Temporal",
      "related": [],
      "keyQuote": "I also really don't want to type of temporal everywhere, that is entirely too long of an identifier. Please specify a suffix that is appropriate",
      "number": 567,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:15:31.166Z"
    },
    {
      "summary": "ChildWorkflow inherits WorkflowRunTimeout from parent workflow execution instead of using the default or specified value. This causes child workflows to have unexpectedly short run timeouts when the parent has a shorter execution timeout.",
      "category": "bug",
      "subcategory": "child-workflow-timeout",
      "apis": [
        "StartWorkflow",
        "ChildWorkflowOptions"
      ],
      "components": [
        "workflow-execution",
        "child-workflow",
        "timeout-handling"
      ],
      "concepts": [
        "timeout",
        "inheritance",
        "workflow-execution-timeout",
        "workflow-run-timeout",
        "parent-child-relationship"
      ],
      "severity": "high",
      "userImpact": "Users experience unexpectedly short run timeouts on child workflows when parent workflows have shorter execution timeouts, potentially causing premature workflow termination.",
      "rootCause": "Child workflow timeout initialization incorrectly inherits WorkflowRunTimeout from parent execution instead of applying independent timeout defaults.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by correcting child workflow timeout inheritance behavior to not inherit WorkflowRunTimeout from parent.",
      "related": [],
      "keyQuote": "In the case of ChildWorkflow, looks like we inherit WorkflowRunTimeout from parent execution. This seems counter intuitive",
      "number": 565,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:15:31.308Z"
    },
    {
      "summary": "Elasticsearch HTTPS connections are not supported - the temporal Helm chart exposes an HTTPS scheme option, but the start script hardcodes HTTP, preventing secure connections to Elasticsearch.",
      "category": "bug",
      "subcategory": "elasticsearch-connectivity",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "helm-charts",
        "start-script"
      ],
      "concepts": [
        "https",
        "ssl",
        "elasticsearch",
        "connection",
        "security",
        "helm-configuration"
      ],
      "severity": "medium",
      "userImpact": "Users cannot securely connect Temporal to Elasticsearch over HTTPS despite the Helm chart suggesting support, causing startup failures.",
      "rootCause": "HTTP is hardcoded in the start script despite Helm chart exposing a scheme configuration option for HTTPS support.",
      "proposedFix": "Update the start script and Elasticsearch client initialization to respect the configured scheme parameter and support HTTPS connections.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved based on author's comment that the feature is now implemented.",
      "related": [],
      "keyQuote": "I'm not able to connect to elasticsearch over https. The temporal helm chart has a property for scheme which makes it seem like https would be supported",
      "number": 562,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:15:30.921Z"
    },
    {
      "summary": "Users request the ability to clear or unset search attributes in workflows, rather than only being able to set or update them. Currently, workarounds involve setting sentinel values or empty collections.",
      "category": "feature",
      "subcategory": "search-attributes",
      "apis": [
        "UpsertSearchAttributes"
      ],
      "components": [
        "search-attributes",
        "workflow-execution",
        "payload-metadata"
      ],
      "concepts": [
        "search-attributes",
        "clearing-values",
        "sentinel-values",
        "empty-collections",
        "workflow-state",
        "metadata-management"
      ],
      "severity": "medium",
      "userImpact": "Users must use workarounds like sentinel values or empty collections to simulate clearing search attributes, making documents less clean and queries more complex.",
      "rootCause": "Search attributes API only supports setting and updating values, not deletion or clearing.",
      "proposedFix": "Add flag to Payload metadata indicating deletion, or use nil/empty slice values as magic values to clear search attributes.",
      "workaround": "Set search attribute to sentinel value (e.g., -1 for integers) or send empty collection.",
      "resolution": "fixed",
      "resolutionDetails": "Implemented support for nil and empty slice values to clear search attributes during upsert operations.",
      "related": [],
      "keyQuote": "Both `nil` and empty slice values clears search attributes now.",
      "number": 561,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:14:53.377Z"
    },
    {
      "summary": "The tctl workflow start command fails with an 'Invalid Format' error for execution_timeout, which is no longer required by the server. The command should work without specifying this field.",
      "category": "bug",
      "subcategory": "cli-workflow-commands",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "tctl",
        "cli",
        "workflow-execution"
      ],
      "concepts": [
        "timeout",
        "command-parsing",
        "validation",
        "required-fields"
      ],
      "severity": "high",
      "userImpact": "Users cannot start workflows using tctl without specifying execution_timeout, blocking a common workflow operation.",
      "rootCause": "CLI validation requires execution_timeout parameter even though the server no longer requires it, causing a format validation error.",
      "proposedFix": "Make execution_timeout optional in the tctl workflow start command validation and update to support the new run_timeout parameter.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The cli validation was updated to not require execution_timeout, and run_timeout support was added.",
      "related": [],
      "keyQuote": "execution_timeout is not required by the server anymore",
      "number": 553,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:14:51.670Z"
    },
    {
      "summary": "AWS Elasticsearch connections fail due to default sniffing and health check features being incompatible. User requests configuration options to disable these features.",
      "category": "feature",
      "subcategory": "elasticsearch-config",
      "apis": [],
      "components": [
        "elasticsearch-client",
        "visibility-persistence"
      ],
      "concepts": [
        "connection",
        "configuration",
        "sniffing",
        "health-check",
        "aws-elasticsearch"
      ],
      "severity": "medium",
      "userImpact": "Users cannot connect Temporal workers to AWS Elasticsearch due to timeout errors from incompatible client settings.",
      "rootCause": "Elastic Go client has sniffing and health check enabled by default, which does not work with AWS Elasticsearch Service.",
      "proposedFix": "Add configuration options to enable/disable sniffing and health check features in the Elasticsearch client config.",
      "workaround": "Manually disable sniffing and health check in the Temporal elasticsearch client configuration.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by Temporal 1.6.3 which likely added the necessary Elasticsearch configuration options.",
      "related": [],
      "keyQuote": "Perhaps the ability to enable/disable these features could be added to the config?",
      "number": 551,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:14:51.095Z"
    },
    {
      "summary": "User requests native Podman support for development environment setup as an alternative to Docker Compose, particularly for systems that only allow user-space containers. They've created and shared a Kubernetes YAML file demonstrating how to run Temporal with Cassandra and Web UI using Podman.",
      "category": "feature",
      "subcategory": "dev-environment-setup",
      "apis": [],
      "components": [
        "docker",
        "podman",
        "dev-environment",
        "kubernetes-yaml"
      ],
      "concepts": [
        "containers",
        "user-space-execution",
        "development-setup",
        "pod-orchestration",
        "rootless-containers",
        "documentation"
      ],
      "severity": "low",
      "userImpact": "Enables developers in organizations that prohibit Docker to set up local Temporal development environments using Podman.",
      "rootCause": null,
      "proposedFix": "Include Podman-compatible Kubernetes YAML configuration in the codebase and documentation, potentially based on the example provided from the GitLab repository.",
      "workaround": "Users can manually create a Podman pod using the provided YAML file via `podman play kube temporal.yaml` and manage it with `podman pod start/stop/rm`.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "My corporation only allows containers running in user space, so docker is a no-go for me.",
      "number": 550,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:14:13.374Z"
    },
    {
      "summary": "tctl wf desc command shows that a workflow failed but doesn't display the failure reason, requiring users to run separate commands or use temporal-web to understand why the workflow failed.",
      "category": "feature",
      "subcategory": "cli-tooling",
      "apis": [],
      "components": [
        "tctl",
        "workflow-description",
        "cli"
      ],
      "concepts": [
        "error-reporting",
        "developer-experience",
        "workflow-diagnostics",
        "failure-context",
        "observability"
      ],
      "severity": "low",
      "userImpact": "Users cannot quickly diagnose workflow failures from tctl, forcing them to use alternative tools or commands for troubleshooting.",
      "rootCause": null,
      "proposedFix": "Enhance tctl wf desc to include failure reason details, potentially making additional server calls if needed.",
      "workaround": "Use wf show command or temporal-web to view failure details.",
      "resolution": "wontfix",
      "resolutionDetails": "tctl is deprecated, so the issue was closed without implementing the requested enhancement.",
      "related": [],
      "keyQuote": "tctl wf desc tells me that my workflow failed but gives me zero context as to why.",
      "number": 549,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:14:13.015Z"
    },
    {
      "summary": "TaskQueue Kind field is not being set when a WorkflowTask is scheduled, showing 'Unspecified' instead of the expected 'Normal' or 'Sticky' value. This affects visibility of task queue metadata in tctl output.",
      "category": "bug",
      "subcategory": "task-queue",
      "apis": [],
      "components": [
        "task-queue",
        "workflow-task",
        "tctl"
      ],
      "concepts": [
        "task-queue",
        "metadata",
        "workflow-execution",
        "scheduling",
        "visibility"
      ],
      "severity": "medium",
      "userImpact": "Users cannot see the correct Kind value for scheduled workflow tasks in tctl output, making it difficult to diagnose task queue routing issues.",
      "rootCause": "TaskQueue Kind field is not being populated during WorkflowTask scheduling",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "TaskQueue Kind field is now properly set to Normal or Sticky when WorkflowTask is scheduled",
      "related": [],
      "keyQuote": "Kind:Unspecified when it should be set to either Normal or Sticky",
      "number": 543,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:14:12.740Z"
    },
    {
      "summary": "Request to add SignalWithStart and UpdateWithStart commands that can be called from within workflow code, currently only available through external gRPC API. This would enable atomic signal-on-start operations directly from workflows instead of requiring workarounds through activities.",
      "category": "feature",
      "subcategory": "workflow-commands",
      "apis": [
        "SignalWithStart",
        "ExecuteChildWorkflow",
        "ExecuteWorkflow",
        "SignalWorkflow"
      ],
      "components": [
        "workflow-engine",
        "decision-commands",
        "grpc-api",
        "sdk-core"
      ],
      "concepts": [
        "child-workflows",
        "signaling",
        "atomic-operations",
        "workflow-execution",
        "external-api",
        "history-service"
      ],
      "severity": "medium",
      "userImpact": "Users must currently work around the lack of direct SignalWithStart support in workflows by calling it from activities, requiring extra complexity and losing atomicity.",
      "rootCause": "SignalWithStart is implemented at the gRPC/history service level but not exposed as a decision command in the workflow SDK.",
      "proposedFix": "Add SignalWithStartChildWorkflow decision command with an option to specify whether the started workflow becomes a child workflow.",
      "workaround": "Call SignalWithStart from an activity instead of directly from workflow code.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        3819
      ],
      "keyQuote": "Currently, SignalWithStart is available only through external gRPC API. There are multiple use cases when it is needed from a workflow.",
      "number": 537,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:13:34.829Z"
    },
    {
      "summary": "Task list partitioning configuration in v0.23.0 causes \"Error updating timer ack level for shard\" errors in the history service, along with persistence errors in the matching service when adding/updating task lists, though no canary workflow executions actually failed.",
      "category": "bug",
      "subcategory": "task-list-partitioning",
      "apis": [],
      "components": [
        "history-service",
        "matching-service",
        "timer-queue-processor",
        "shard-management"
      ],
      "concepts": [
        "task-list-partitioning",
        "shard-management",
        "timer-ack-level",
        "persistence",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Enabling task list partitioning triggers error logging and persistence failures in the history and matching services, potentially impacting reliability perception even if workflows continue to execute.",
      "rootCause": "Range ID conflict when updating shard state during task list partitioning, causing failed shard updates with \"Previous range ID: 46; new range ID: 47\" errors.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "stale",
      "resolutionDetails": "Closed as too old (reported in 2020) with no similar issues observed in load testing. Requested reopen only if issue reproduces in v1.7.x or later.",
      "related": [],
      "keyQuote": "Failed to update shard. Previous range ID: 46; new range ID: 47",
      "number": 530,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:13:34.651Z"
    },
    {
      "summary": "String search attributes are returned with quotation marks included, requiring manual trimming before decoding JSON or other encoded values.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [
        "GetSearchAttributes"
      ],
      "components": [
        "search-attributes",
        "indexing",
        "data-serialization"
      ],
      "concepts": [
        "string-encoding",
        "data-formatting",
        "search-queries",
        "attribute-retrieval"
      ],
      "severity": "medium",
      "userImpact": "Users must manually strip quotation marks from retrieved string search attributes before decoding, adding unnecessary complexity to search attribute handling.",
      "rootCause": "Search attributes encoding/retrieval includes quotation marks that should be stripped for string values before returning to caller.",
      "proposedFix": "Trim quotation marks from the first and last characters of string search attributes before returning them.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was post code complete and corresponding issues were created in SDK repositories.",
      "related": [],
      "keyQuote": "The string retrieved from Search attributes includes quotations marks. If json is stored, or any other string encoded value, the first and last characters must be trimmed before decoding.",
      "number": 524,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:13:33.515Z"
    },
    {
      "summary": "The Authorizer interface is too simplistic for multi-tenant cluster operators who need to enforce fine-grained authorization policies, including restricting namespace visibility and admin operations. Users want richer context to make authorization decisions beyond just API name and namespace name.",
      "category": "feature",
      "subcategory": "authorization",
      "apis": [
        "RegisterNamespace",
        "UpdateNamespace",
        "DeprecateNamespace",
        "ListNamespaces"
      ],
      "components": [
        "authorization",
        "authorizer",
        "access-control",
        "namespace-management"
      ],
      "concepts": [
        "multi-tenancy",
        "authorization",
        "access-control",
        "namespace-filtering",
        "admin-operations",
        "user-roles"
      ],
      "severity": "high",
      "userImpact": "Multi-tenant cluster operators cannot implement required authorization policies without brittle workarounds or maintaining forks of the upstream code.",
      "rootCause": "The Authorize interface only provides API name and target namespace as context for authorization decisions, lacking support for per-namespace user roles and result filtering.",
      "proposedFix": "Extend authorization.Result to include AuthorizedNamespaces field and support per-namespace user roles with ClaimMapper to filter results for APIs like ListNamespaces.",
      "workaround": "Implement custom AccessControlledWorkflowHandler or maintain a fork of upstream temporal code to perform post-filtering of results.",
      "resolution": "fixed",
      "resolutionDetails": "Refactored authorization mechanism with Authorizer and ClaimMapper now supports per-namespace user roles and system-level roles. ListNamespaces filtering remains as separate limitation.",
      "related": [],
      "keyQuote": "The only information we have to go on to make a Deny/Allow decision is whatever credentials we find in the context along with the name of the API call",
      "number": 522,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:12:53.316Z"
    },
    {
      "summary": "The Authorizer interface should be applied to AdminHandler APIs to allow custom authorization logic for multi-tenant clusters. Currently, authorization is only enforced for the workflow Handler, not admin commands, preventing operators from blocking unauthorized admin access.",
      "category": "feature",
      "subcategory": "authorization",
      "apis": [],
      "components": [
        "AdminHandler",
        "Authorizer",
        "AccessControlledWorkflowHandler",
        "gRPC interceptor"
      ],
      "concepts": [
        "authorization",
        "multi-tenant",
        "access control",
        "admin commands",
        "security",
        "authentication"
      ],
      "severity": "high",
      "userImpact": "Multi-tenant Temporal cluster operators cannot prevent normal users from accessing admin commands without authorizer support for AdminHandler.",
      "rootCause": "Authorizer interface is only wrapped around workflow Handler, not AdminHandler, in frontend/service.go.",
      "proposedFix": "Apply the same AccessControlledWorkflowHandler pattern to AdminHandler, wrapping it with authorization checks.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Authorization functionality was refactored so that Authorizer is invoked as an interceptor on all inbound gRPC calls for all handlers.",
      "related": [],
      "keyQuote": "By applying the Authorizer interface to the AdminHandler API's, we are able to first check that the caller has admin credentials before forwarding the API call.",
      "number": 521,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:12:54.552Z"
    },
    {
      "summary": "Request to add built-in support for passing large data payloads (30MB+) between activities and workflows. Currently users must implement their own workarounds like saving blobs to external storage and passing URLs instead.",
      "category": "feature",
      "subcategory": "activity-communication",
      "apis": [],
      "components": [
        "activity-executor",
        "workflow-execution",
        "data-serialization"
      ],
      "concepts": [
        "large-payloads",
        "blob-storage",
        "inter-activity-communication",
        "data-transfer",
        "scalability"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to pass large data between activities must implement custom blob storage solutions instead of using built-in Temporal mechanisms.",
      "rootCause": null,
      "proposedFix": "Add built-in blob storage (Cassandra/MySQL) that activities can call to save large data, returning IDs that workflows pass to subsequent activities.",
      "workaround": "Save large blobs to external storage and pass URLs between activities instead of raw data.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Let the activity call Temporal to save the blob in Cassandra/MySQL, returns the id of the blob to the workflow that can pass it on.",
      "number": 518,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:12:53.263Z"
    },
    {
      "summary": "CLI unable to display workflow details when heartbeat data contains non-string values (e.g., numbers). The CLI attempts to unmarshal JSON heartbeat details into a string type, causing deserialization failures.",
      "category": "bug",
      "subcategory": "activity-heartbeat",
      "apis": [
        "RecordHeartbeat"
      ],
      "components": [
        "cli",
        "heartbeat-handler",
        "payload-decoder"
      ],
      "concepts": [
        "serialization",
        "json-deserialization",
        "payload-encoding",
        "type-mismatch",
        "heartbeat-details"
      ],
      "severity": "high",
      "userImpact": "Users cannot inspect workflow details via CLI when activities record heartbeat data with non-string values, blocking operational visibility.",
      "rootCause": "CLI code attempts to unmarshal JSON heartbeat values directly into a string pointer without handling cases where the payload is a number or other non-string JSON type. The Payloads struct contains metadata about encoding but the CLI doesn't properly deserialize based on the actual payload type.",
      "proposedFix": "Return raw JSON byte array converted to string instead of unmarshalling to specific type, or make HeartbeatDetails an interface{} to handle arbitrary types, or provide fallback error message when decoding fails.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in PR #525 by properly handling heartbeat payload deserialization and supporting multiple heartbeat arguments.",
      "related": [
        525
      ],
      "keyQuote": "We are then unmarshalling the JSON number and then trying to assign it to a string pointer. You can't assign a number to an string, which is the error you are getting.",
      "number": 516,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:11:00.296Z"
    },
    {
      "summary": "Temporal server is hardcoded to output logs in JSON format, which is incompatible with infrastructure that requires specific constraints like low cardinality of field names. User requests the ability to use zap's console encoder for regular log line output.",
      "category": "feature",
      "subcategory": "logging-configuration",
      "apis": [],
      "components": [
        "logger",
        "config",
        "zap-encoder"
      ],
      "concepts": [
        "logging",
        "configuration",
        "encoder",
        "json-format",
        "infrastructure-compatibility"
      ],
      "severity": "low",
      "userImpact": "Users with specific infrastructure requirements for log formatting cannot use Temporal due to hardcoded JSON logging format.",
      "rootCause": null,
      "proposedFix": "Expose zap logger encoding configuration to allow use of alternative encoders like the console encoder instead of hardcoded JSON.",
      "workaround": "Attempt to process JSON-encoded logs as plain text, though this is not user-friendly.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Would like to be able to use the zap \"console\" encoder which I assume outputs regular log lines.",
      "number": 515,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:10:59.674Z"
    },
    {
      "summary": "Docker Compose configuration lacks support for Cassandra credentials and TLS options. Users can only specify Cassandra seeds and keyspaces, but cannot configure authentication parameters, SSL/TLS settings, or replication factors, unlike the MySQL configuration which supports environment variables for credentials.",
      "category": "feature",
      "subcategory": "docker-compose-configuration",
      "apis": [],
      "components": [
        "docker-compose",
        "config-template",
        "cassandra-client"
      ],
      "concepts": [
        "configuration",
        "credentials",
        "tls",
        "docker",
        "authentication",
        "environment-variables"
      ],
      "severity": "medium",
      "userImpact": "Users cannot deploy Temporal with Cassandra authentication enabled via Docker Compose, limiting deployment flexibility for secure environments.",
      "rootCause": "The config_template.yaml only expands CASSANDRA_SEEDS and NAMESPACE environment variables, ignoring other Cassandra configuration parameters like CASSANDRA_USER, CASSANDRA_PASSWORD, and TLS settings.",
      "proposedFix": "Extend config_template.yaml to support additional environment variables (CASSANDRA_USER, CASSANDRA_PASSWORD, CASSANDRA_TLS_ENABLED, SSL_VERSION, SSL_VALIDATE) similar to the MySQL docker-compose implementation.",
      "workaround": "Create a custom config_template.yaml and modify start.sh to manually pass credentials and TLS options, then override the template in the Dockerfile.",
      "resolution": "fixed",
      "resolutionDetails": "Support for Cassandra credentials and TLS configuration was added to the docker-compose setup, allowing users to specify authentication parameters via environment variables.",
      "related": [
        885
      ],
      "keyQuote": "It appears that our configuration template that we use when launching via docker-compose only expands CASSANDRA_SEEDS and *NAMESPACE values.",
      "number": 513,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:11:00.714Z"
    },
    {
      "summary": "Cassandra consistency levels (Consistency and SerialConsistency) are hardcoded throughout the codebase and should be made configurable via the Config.Cassandra struct and YAML configuration files.",
      "category": "feature",
      "subcategory": "persistence-configuration",
      "apis": [],
      "components": [
        "cassandra-persistence",
        "config-management",
        "cluster-config"
      ],
      "concepts": [
        "consistency",
        "configuration",
        "cassandra",
        "persistence",
        "yaml-config"
      ],
      "severity": "medium",
      "userImpact": "Users cannot customize Cassandra consistency levels and must rely on hardcoded defaults, limiting flexibility for different deployment requirements.",
      "rootCause": "Consistency and SerialConsistency levels are hardcoded in multiple locations when creating gocql.ClusterConfig structs instead of being part of the configurable Cassandra settings.",
      "proposedFix": "Add consistency and serialConsistency fields to the Config.Cassandra struct and expose them in YAML configuration under persistence.datastores[name].cassandra.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Change was merged to master in common/service/config/config.go and helm chart was updated to document how to specify these settings.",
      "related": [],
      "keyQuote": "These consistency levels should both be configurable. They should be made part of the Config.Cassandra struct which holds existing configurable Cassandra settings",
      "number": 512,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:03:46.017Z"
    },
    {
      "summary": "Workflow status displays as 'Unspecified' in the default web UI view instead of showing the actual workflow status. This is a display/UI bug affecting the Temporal Web dashboard.",
      "category": "bug",
      "subcategory": "web-ui",
      "apis": [],
      "components": [
        "web-ui",
        "workflow-status-display"
      ],
      "concepts": [
        "workflow-status",
        "ui-rendering",
        "web-dashboard",
        "state-display"
      ],
      "severity": "medium",
      "userImpact": "Users cannot see the correct workflow status in the web UI, making it difficult to monitor workflow execution state.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in PR #492",
      "related": [
        492
      ],
      "keyQuote": "The status for the running workflow is always shown as 'unspecified' on the default workflow view.",
      "number": 509,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:03:44.875Z"
    },
    {
      "summary": "DescribeWorkflowExecutionResponse lacks information about whether a workflow has received a cancellation request. Adding a CANCELLATION_REQUESTED status to WorkflowExecutionStatus enum or a new field would help with troubleshooting.",
      "category": "feature",
      "subcategory": "workflow-status",
      "apis": [
        "DescribeWorkflowExecutionResponse",
        "WorkflowExecutionStatus"
      ],
      "components": [
        "workflow-execution",
        "api-response",
        "status-tracking"
      ],
      "concepts": [
        "cancellation",
        "workflow-status",
        "observability",
        "troubleshooting",
        "execution-state"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily determine if a workflow has been requested to cancel, making troubleshooting and monitoring more difficult.",
      "rootCause": null,
      "proposedFix": "Add CANCELLATION_REQUESTED status to WorkflowExecutionStatus enum or add a dedicated field to DescribeWorkflowExecutionResponse",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #1018",
      "related": [
        1018
      ],
      "keyQuote": "DescribeWorkflowExecutionResponse doesn't contain information if a workflow has received a cancellation request.",
      "number": 508,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:03:45.322Z"
    },
    {
      "summary": "Adding a search attribute via tctl with type Keyword (type 1) fails with a dynamic config update error, but the attribute is still added with the wrong type (String instead of Keyword). The root cause is that the Elasticsearch index template must be manually updated in addition to the dynamic config file.",
      "category": "bug",
      "subcategory": "search-attributes",
      "apis": [],
      "components": [
        "tctl",
        "dynamic-config",
        "elasticsearch",
        "visibility"
      ],
      "concepts": [
        "search-attributes",
        "elasticsearch-schema",
        "dynamic-configuration",
        "type-mapping",
        "index-template"
      ],
      "severity": "medium",
      "userImpact": "Users cannot properly add custom search attributes of non-string types without manually rebuilding the server image to update the Elasticsearch index template.",
      "rootCause": "The tctl addSearchAttribute command only updates the dynamic config file but does not update the Elasticsearch index template schema, causing the attribute to be created with the default type (String) rather than the specified type.",
      "proposedFix": "Update documentation to clarify that both development_es.yaml and schema/elasticsearch/visibility/index_template.json must be modified. Later, implement a more seamless way to add search attributes without recompiling the image, such as through tctl commands or Kubernetes ConfigMaps.",
      "workaround": "Manually edit both config/dynamicconfig/development_es.yaml and schema/elasticsearch/visibility/index_template.json, rebuild the Docker image, and redeploy.",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved by clarifying the two-step process required to add search attributes. Later improvements made tctl the primary method for adding search attributes without requiring image rebuilds.",
      "related": [
        1442
      ],
      "keyQuote": "In addition to updating the development_es.yaml file, you also have to update the schema/elasticsearch/visibility/index_template.json file",
      "number": 505,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:03:07.694Z"
    },
    {
      "summary": "Activity start and failure events are not recorded in workflow history while activities are in retry loops, causing status loss if the workflow completes before the ScheduleToClose timeout is exhausted. The request is to record these events for activities in retry before workflow completion.",
      "category": "feature",
      "subcategory": "activity-history",
      "apis": [],
      "components": [
        "history",
        "activity-executor",
        "retry-logic",
        "workflow-completion"
      ],
      "concepts": [
        "retry",
        "history-events",
        "activity-status",
        "timeout",
        "workflow-completion",
        "event-recording"
      ],
      "severity": "medium",
      "userImpact": "Users lose visibility into activity execution status when workflows complete while activities are still in retry loops, making debugging difficult.",
      "rootCause": "Events are only recorded when ScheduleToClose timeout expires, not during intermediate failures in retry loops.",
      "proposedFix": "Record started and failed events for all activities in retry loops before completing the workflow.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Record started and failed events for all activities that are in a retry loop before completing the workflow.",
      "number": 503,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:03:05.227Z"
    },
    {
      "summary": "The CLI workflow start command fails to parse the --input parameter when it contains multiple arguments. The processJSONInput function cannot unmarshal the input because it expects valid JSON, but the CLI documentation suggests users should be able to pass multiple space-separated arguments.",
      "category": "bug",
      "subcategory": "cli-input-parsing",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "cli",
        "tctl",
        "input-parser"
      ],
      "concepts": [
        "json-parsing",
        "command-line-arguments",
        "input-marshaling",
        "parameter-validation"
      ],
      "severity": "medium",
      "userImpact": "Users cannot start workflows with multiple input parameters via the CLI, blocking a documented workflow operation.",
      "rootCause": "The processJSONInput function attempts to unmarshal space-separated arguments as a single JSON value, but JSON format requires proper structure and doesn't support bare values separated by spaces.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The issue was acknowledged by a maintainer as needing a proper fix after a quick fix was applied for single parameter support.",
      "related": [],
      "keyQuote": "CLI should be able to start a workflow with --input param containing multiple args",
      "number": 502,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:03:06.529Z"
    },
    {
      "summary": "Matching service unit tests are failing intermittently in both local development and CI environments. Once failures start occurring, they persist until the environment is reset.",
      "category": "bug",
      "subcategory": "test-flakiness",
      "apis": [],
      "components": [
        "matching-service",
        "test-framework",
        "unit-tests"
      ],
      "concepts": [
        "test-reliability",
        "race-condition",
        "environment-state",
        "flaky-tests"
      ],
      "severity": "medium",
      "userImpact": "Developers experience unreliable test runs that hinder development velocity and CI/CD pipeline reliability.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": "Reset the environment to clear the persistent failure state.",
      "resolution": "duplicate",
      "resolutionDetails": "Identified as a duplicate of issue #243.",
      "related": [
        243
      ],
      "keyQuote": "Sometimes tests randomly fail. I've seen this occur on local development environment and in Kite server.",
      "number": 499,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:02:27.836Z"
    },
    {
      "summary": "Integration test TestContinueAsNewWorkflow_Timeout exhibits flaky behavior, taking 5 minutes to pass when it should complete in 5 seconds, indicating timing or synchronization issues in the test or the timeout implementation.",
      "category": "bug",
      "subcategory": "test-flakiness",
      "apis": [],
      "components": [
        "test-framework",
        "continue-as-new",
        "timeout-handling"
      ],
      "concepts": [
        "test-flakiness",
        "timeout",
        "timing",
        "synchronization",
        "integration-test"
      ],
      "severity": "medium",
      "userImpact": "Flaky tests reduce confidence in the testing suite and make CI/CD pipelines unreliable.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, likely after test reliability improvements",
      "related": [],
      "keyQuote": "Test failed in 5 seconds. Test passes within 5 minutes.",
      "number": 497,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:02:26.848Z"
    },
    {
      "summary": "Integration test TestWorkflowTerminationSignalBeforeTransientDecisionStarted is flaky, timing out after a minute instead of completing reliably within seconds. The test appears to have race conditions in the workflow termination and signal handling logic.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-suite",
        "workflow-termination",
        "signal-handling"
      ],
      "concepts": [
        "flaky-test",
        "timeout",
        "race-condition",
        "workflow-lifecycle",
        "test-reliability"
      ],
      "severity": "medium",
      "userImpact": "Flaky integration tests reduce confidence in the codebase and slow down development workflows by causing sporadic test failures.",
      "rootCause": "Race condition in workflow termination signal handling during transient decision started state",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "TestWorkflowTerminationSignalBeforeTransientWorkflowTaskStarted is now passing consistently, indicating the underlying issue was resolved",
      "related": [],
      "keyQuote": "TestWorkflowTerminationSignalBeforeTransientWorkflowTaskStarted is passing consistently.",
      "number": 496,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:02:27.661Z"
    },
    {
      "summary": "The workflow reset API fails when attempting to reset workflows that have timed out without completing any workflow tasks. The reset should fall back to resetting to the start event when no completed workflow task exists.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [
        "Reset",
        "StartWorkflowExecution"
      ],
      "components": [
        "workflow-reset",
        "event-history",
        "decision-task"
      ],
      "concepts": [
        "reset",
        "timeout",
        "event-history",
        "workflow-task",
        "state-management"
      ],
      "severity": "high",
      "userImpact": "Users cannot reset workflows that timeout before completing any workflow tasks, blocking workflow recovery and replay operations.",
      "rootCause": "Reset logic assumes at least one WorkflowTaskCompleted event exists to determine reset point; it fails when event history only contains WorkflowExecutionStarted -> DecisionTaskScheduled -> WorkflowExecutionTimedOut.",
      "proposedFix": "Reset should fall back to resetting to the startWorkflowExecutionEvent when no completed workflow task event is found.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Reset now works for workflows without completed workflow tasks by falling back to resetting to the start event.",
      "related": [],
      "keyQuote": "If a workflow timed out without any workflow task started event, it should just reset to its startWorkflowExecutionEvent.",
      "number": 494,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:01:49.466Z"
    },
    {
      "summary": "Support for dynamic cluster IP addresses when all nodes fail simultaneously and restart with new IPs. Currently, nodes attempt to join previous cluster members instead of initializing a new cluster, causing cascading failures.",
      "category": "feature",
      "subcategory": "cluster-membership",
      "apis": [],
      "components": [
        "ringpop",
        "cluster-coordinator",
        "membership-database"
      ],
      "concepts": [
        "cluster-failure",
        "ip-address-change",
        "node-restart",
        "cluster-initialization",
        "container-deployment",
        "membership-coordination"
      ],
      "severity": "high",
      "userImpact": "Users running containerized Temporal clusters face cascading failures when all nodes fail and restart with new IP addresses.",
      "rootCause": "Membership coordination in the database cannot distinguish new replacement nodes from previously down nodes when IP addresses change, causing new nodes to attempt joining non-existent cluster members instead of forming a new cluster.",
      "proposedFix": "Retry initialization logic after the healthyHostLastHeartbeatCutoff timeout (20 seconds) to allow new nodes to detect stale cluster state and bootstrap independently.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating the feature was implemented or the underlying problem was resolved through cluster initialization logic improvements.",
      "related": [],
      "keyQuote": "there is no way to identify that these new nodes are replacement nodes for previously heartbeating members",
      "number": 493,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:01:48.606Z"
    },
    {
      "summary": "RequestCancelWorkflowExecutionRequest is missing reason and details fields that exist in TerminateWorkflowExecutionRequest, causing API inconsistency. These fields need to be added to the proto definition and supported in the CLI.",
      "category": "feature",
      "subcategory": "api-design",
      "apis": [
        "RequestCancelWorkflowExecutionRequest",
        "TerminateWorkflowExecutionRequest"
      ],
      "components": [
        "workflow-service",
        "api",
        "cli"
      ],
      "concepts": [
        "api-parity",
        "proto-schema",
        "workflow-cancellation",
        "termination",
        "request-details",
        "reason-field"
      ],
      "severity": "medium",
      "userImpact": "Users cannot provide reason and details when cancelling workflows, limiting visibility into cancellation context compared to termination operations.",
      "rootCause": "RequestCancelWorkflowExecutionRequest proto definition was not updated to match TerminateWorkflowExecutionRequest fields.",
      "proposedFix": "Add reason and details fields to RequestCancelWorkflowExecutionRequest proto and implement CLI support.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The reason field was already available in the temporal/api proto as noted in the comment, resolving the feature request.",
      "related": [],
      "keyQuote": "To keep parity these fields needs to be added and supported by CLI (at least `reason`).",
      "number": 491,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:01:48.626Z"
    },
    {
      "summary": "Users are forced to choose between database size and uniqueness guarantees because the workflow uniqueness period is directly tied to the retention period. The request is to allow these periods to be configured independently, keeping small uniqueness records after workflow data is deleted.",
      "category": "feature",
      "subcategory": "workflow-uniqueness",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-storage",
        "duplicate-detection",
        "retention-policy"
      ],
      "concepts": [
        "uniqueness-guarantee",
        "retention-period",
        "database-size",
        "idempotency",
        "deduplication"
      ],
      "severity": "medium",
      "userImpact": "Users must trade off between keeping workflows in the database longer (for uniqueness checks) and minimizing database storage costs.",
      "rootCause": "Uniqueness guarantees are implemented using the same retention mechanism that stores workflow data, preventing independent configuration of these concerns.",
      "proposedFix": "Decouple the uniqueness guarantee period from the data retention period by maintaining minimal uniqueness records (just enough to detect duplicates) separately from full workflow data.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Allow uniqueness guarantee to be larger than the retention period. It would mean removing all the workflow data from DB, but keeping a small record that would preclude duplicated workflow starts.",
      "number": 487,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:01:10.584Z"
    },
    {
      "summary": "Validation error messages lack context about which specific command caused the failure, making it difficult to troubleshoot issues when multiple similar commands are scheduled. Request to include eventId and other contextual information in validation messages.",
      "category": "feature",
      "subcategory": "error-messages",
      "apis": [],
      "components": [
        "decision-processor",
        "command-validator",
        "error-reporting"
      ],
      "concepts": [
        "error-context",
        "debugging",
        "command-validation",
        "event-tracking",
        "troubleshooting",
        "decision-identification"
      ],
      "severity": "medium",
      "userImpact": "Users cannot efficiently identify which of many scheduled activities failed due to missing validation error context.",
      "rootCause": null,
      "proposedFix": "Include eventId of the decision and full information about the scheduleActivityTask decision in validation error messages.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Include full information about the specific scheduleActivityTask decision that caused the problem. The minimal information included is eventId of the decision.",
      "number": 479,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:01:09.462Z"
    },
    {
      "summary": "MySQL database password is displayed in plain text in log files during Temporal cluster bootstrap, exposing sensitive credentials. The log statement should either obfuscate the password or avoid including it entirely.",
      "category": "bug",
      "subcategory": "security-logging",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "bootstrap",
        "logging"
      ],
      "concepts": [
        "password-exposure",
        "credential-security",
        "log-sanitization",
        "secret-management"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal clusters have their database passwords exposed in plain text logs, creating a significant security vulnerability.",
      "rootCause": "The bootstrap logging statement includes the plaintext password parameter when executing temporal-sql-tool commands.",
      "proposedFix": "Obfuscate the password in log output or remove it from the log statement entirely.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Password should be obfuscated rather than being displayed in plain text. Alternatively if log statement could be modified to avoid spilling out secret that would be great.",
      "number": 474,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:01:10.418Z"
    },
    {
      "summary": "Releases are not being automatically pushed to Docker Hub after GitHub releases are created. Currently this requires manual intervention, resulting in Docker Hub missing recent releases.",
      "category": "feature",
      "subcategory": "release-automation",
      "apis": [],
      "components": [
        "docker-hub",
        "release-pipeline",
        "github-actions"
      ],
      "concepts": [
        "automation",
        "release-management",
        "ci-cd",
        "docker-registry",
        "deployment"
      ],
      "severity": "medium",
      "userImpact": "Users may not have access to the latest Temporal Server versions on Docker Hub if releases are not manually published.",
      "rootCause": "Manual process for publishing releases to Docker Hub is prone to being missed or delayed.",
      "proposedFix": "Implement automated GitHub release to Docker Hub publishing pipeline.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "An internal pipeline was implemented to automatically carry out the publish of docker images.",
      "related": [],
      "keyQuote": "we have an internal pipeline which carries out the publish of docker",
      "number": 472,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:00:31.973Z"
    },
    {
      "summary": "Temporal Docker container fails to bind when connected to multiple Docker networks, as it receives multiple IP addresses as a single string which the IPv4 parser cannot handle. The container needs to bind to 0.0.0.0 instead of specific IPs in multi-network scenarios.",
      "category": "bug",
      "subcategory": "docker-networking",
      "apis": [],
      "components": [
        "rpc",
        "server",
        "docker-setup",
        "network-binding"
      ],
      "concepts": [
        "networking",
        "docker",
        "multi-interface",
        "ip-binding",
        "gRPC-listener",
        "container-deployment"
      ],
      "severity": "high",
      "userImpact": "Users cannot run Temporal in Docker containers connected to multiple networks, making it impossible to use common Docker Compose setups with multiple network backends.",
      "rootCause": "The getListenIP function in rpc.go:186 receives multiple space-separated IP addresses when a container is connected to multiple Docker networks, but the parser expects a single IPv4 address and fails to parse the string.",
      "proposedFix": "Bind to 0.0.0.0 in Docker scenarios to accept connections on all interfaces, or modify the IP parsing logic to handle multiple addresses and select one appropriately.",
      "workaround": "Set environment variables to explicitly bind to 0.0.0.0 and configure broadcastAddress for cluster communication, though documentation on how to do this via configuration is unclear.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        544
      ],
      "keyQuote": "Binding multiple networks to the temporal docker container results in: unable to parse bindOnIP value or it is not IPv4 address",
      "number": 471,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:00:32.169Z"
    },
    {
      "summary": "Child workflow retries were not working properly with MySQL persistence in Temporal server. The issue was reported as a potential port from Cadence but was later verified to not reproduce on the latest Temporal server bits.",
      "category": "bug",
      "subcategory": "child-workflows",
      "apis": [
        "StartChildWorkflowExecution"
      ],
      "components": [
        "child-workflow-executor",
        "persistence",
        "mysql"
      ],
      "concepts": [
        "retry",
        "persistence",
        "child-workflows",
        "workflow-execution",
        "state-management"
      ],
      "severity": "medium",
      "userImpact": "Users with MySQL persistence may experience child workflows not being retried as expected, leading to workflow failures.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was verified to not reproduce on the latest temporal server bits, suggesting it was fixed in a subsequent release.",
      "related": [
        3351
      ],
      "keyQuote": "This does not reproduce on the latest temporal server bits",
      "number": 470,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T17:00:31.177Z"
    },
    {
      "summary": "Request to add a configurable depth limit for child workflow creation to prevent infinite recursion due to programming mistakes. A commenter suggests also considering cycle detection or DAG validation.",
      "category": "feature",
      "subcategory": "child-workflows",
      "apis": [
        "StartWorkflow"
      ],
      "components": [
        "workflow-engine",
        "child-workflow-executor"
      ],
      "concepts": [
        "recursion",
        "depth-limit",
        "resource-protection",
        "error-prevention",
        "workflow-hierarchy"
      ],
      "severity": "medium",
      "userImpact": "Without depth limits, developers can accidentally create infinite recursive child workflows that consume resources and crash the system.",
      "rootCause": null,
      "proposedFix": "Allow configurable depth limit for child workflow creation",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Currently there is no ability to limit number of levels of child workflow creation. It can potentially lead to situations when due to programming mistake workflow keeps creating children recursively forever.",
      "number": 469,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:59:51.678Z"
    },
    {
      "summary": "Docker Compose README documentation claims graphite metrics dashboard is available at localhost:8080/dashboard, but the default docker-compose.yml (with Cassandra) does not actually run graphite, making the dashboard inaccessible.",
      "category": "docs",
      "subcategory": "docker-setup",
      "apis": [],
      "components": [
        "docker",
        "docker-compose",
        "graphite"
      ],
      "concepts": [
        "docker-setup",
        "metrics",
        "dashboard",
        "documentation-accuracy",
        "configuration"
      ],
      "severity": "low",
      "userImpact": "Users following the README documentation attempt to access a metrics dashboard that is not actually running in the default configuration, leading to confusion and failed browser connections.",
      "rootCause": "README.md documentation was not updated when Graphite was removed as a default component/dependency in the docker-compose configuration.",
      "proposedFix": "Update README.md to reflect that Graphite is no longer a default component, or add Graphite to the default docker-compose.yml.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "README documentation was updated in PR #548 to reflect that Graphite is no longer a default component/dependency.",
      "related": [
        548
      ],
      "keyQuote": "Graphite is no longer a default component/dependency. The temporal/docker/README file has been updated to reflect this",
      "number": 466,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:59:53.080Z"
    },
    {
      "summary": "Setting POSTGRES_PWD to empty in docker-compose-postgres.yml causes the POSTGRES_SEEDS environment variable to be ignored and defaults to 127.0.0.1, preventing users from specifying custom database seed hosts.",
      "category": "bug",
      "subcategory": "docker-configuration",
      "apis": [],
      "components": [
        "docker-compose",
        "postgres-driver",
        "configuration"
      ],
      "concepts": [
        "environment-variables",
        "database-seeds",
        "default-values",
        "docker-compose",
        "postgres-connection"
      ],
      "severity": "medium",
      "userImpact": "Users cannot configure custom PostgreSQL seed hosts when they set an empty password, forcing them to use localhost which may not be accessible in their environment.",
      "rootCause": "The docker-compose configuration likely has conditional logic that defaults POSTGRES_SEEDS to 127.0.0.1 when POSTGRES_PWD is empty or unset.",
      "proposedFix": null,
      "workaround": "Do not set POSTGRES_PWD to empty; use a placeholder password value instead.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by fixing the docker-compose configuration to properly honor POSTGRES_SEEDS regardless of POSTGRES_PWD value.",
      "related": [],
      "keyQuote": "Setting POSTGRES_PWD to nothing or '' causes POSTGRES_SEEDS value to be ignored and defaulted to 127.0.0.1",
      "number": 460,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:59:52.518Z"
    },
    {
      "summary": "Activity tasks are getting lost when matching roles restart, causing scheduled activities to timeout with SCHEDULE_TO_START errors instead of being dispatched to workers. This results in workflows getting stuck waiting for activity execution.",
      "category": "bug",
      "subcategory": "activity-dispatch",
      "apis": [],
      "components": [
        "matching-role",
        "activity-dispatcher",
        "task-routing"
      ],
      "concepts": [
        "activity-timeout",
        "task-dispatch",
        "role-restart",
        "task-loss",
        "schedule-to-start"
      ],
      "severity": "high",
      "userImpact": "Workflows hang indefinitely when activities fail to dispatch after matching role restarts, breaking application workflows that depend on activity execution.",
      "rootCause": "Activity tasks are not being properly redelivered to workers when matching roles restart, causing them to timeout rather than execute.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue closed due to inability to reproduce and lack of further information. No fix was implemented.",
      "related": [],
      "keyQuote": "Activity task times out due to ScheduleToStart when matching roles restart, causing workflow to get stuck",
      "number": 459,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:59:14.763Z"
    },
    {
      "summary": "When Temporal service startup fails due to missing configuration keys, the error log doesn't include the name of the missing key, making debugging difficult. Request to improve error messaging by logging the specific key name that caused the validation failure.",
      "category": "feature",
      "subcategory": "configuration-validation",
      "apis": [],
      "components": [
        "config-loader",
        "service-startup",
        "error-logging"
      ],
      "concepts": [
        "configuration-validation",
        "error-messaging",
        "debugging",
        "kubernetes-deployment",
        "yaml-parsing"
      ],
      "severity": "low",
      "userImpact": "Users deploying Temporal in Kubernetes face prolonged troubleshooting when config validation fails because the error message doesn't identify which configuration key is missing.",
      "rootCause": "Config validation error handling doesn't extract and log the specific missing key name from the YAML parsing error.",
      "proposedFix": "Include the name of the missing configuration key in the error log message when config validation fails.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "When validating configs and encountering a missing key, please include the name of the key in the log.",
      "number": 455,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:59:13.851Z"
    },
    {
      "summary": "Temporal Server fails to connect to CockroachDB on port 26257 with 'connection refused' error during Docker startup, even though the database schema setup succeeds and the port appears accessible via netcat.",
      "category": "bug",
      "subcategory": "database-connectivity",
      "apis": [],
      "components": [
        "server",
        "sql-adapter",
        "database-connection",
        "docker-setup"
      ],
      "concepts": [
        "connection-refused",
        "database-connectivity",
        "docker-networking",
        "port-binding",
        "postgresql-compatibility"
      ],
      "severity": "high",
      "userImpact": "Users cannot start Temporal Server with CockroachDB as the persistence backend in Docker environments due to connection failures.",
      "rootCause": "Server is attempting to connect to 127.0.0.1:26257 (localhost) instead of the Docker service hostname 'postgres', causing connection refused despite the database being reachable during schema setup.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by fixing the database connection configuration to use the correct hostname instead of localhost in Docker environments.",
      "related": [],
      "keyQuote": "sql schema version compatibility check failed: unable to create SQL connection: dial tcp 127.0.0.1:26257: connect: connection refused",
      "number": 454,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:59:14.941Z"
    },
    {
      "summary": "Docker container is considered ready by the orchestration system before Temporal services have finished warming up, causing connection failures. The request is to implement a HEALTHCHECK directive in the Dockerfile to signal readiness only after services are fully initialized.",
      "category": "feature",
      "subcategory": "docker-deployment",
      "apis": [],
      "components": [
        "docker",
        "container-healthcheck",
        "frontend-service"
      ],
      "concepts": [
        "container-readiness",
        "orchestration",
        "service-initialization",
        "startup-detection",
        "dependency-management",
        "docker-compose"
      ],
      "severity": "high",
      "userImpact": "Users deploying Temporal in Docker-based environments experience race conditions where clients connect before the server is ready, leading to cascading failures.",
      "rootCause": "Docker assumes container is ready when it starts running, but Temporal engine requires additional warmup time before accepting connections.",
      "proposedFix": "Implement the HEALTHCHECK directive in Dockerfile(s) to verify backend readiness before marking container as healthy.",
      "workaround": "Add HEALTHCHECK to docker-compose files using tctl commands (cluster h, workflow list) to verify service readiness, or use depends_on with service_healthy condition.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Currently docker will assume the container is ready once it is running, while the temporal engine is still warming up, leading to all sorts of problems.",
      "number": 453,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:58:34.775Z"
    },
    {
      "summary": "Add gRPC Reflection API support to enable tools like grpcurl and grpcui to explore Temporal Frontend APIs without requiring protobuf definitions. This feature was blocked by protobuf library constraints but became possible after migrating from gogo to google protobuf generation.",
      "category": "feature",
      "subcategory": "grpc-reflection",
      "apis": [],
      "components": [
        "frontend-api",
        "grpc-server",
        "protobuf"
      ],
      "concepts": [
        "grpc-reflection",
        "api-exploration",
        "protobuf-generation",
        "tooling",
        "developer-experience",
        "service-discovery"
      ],
      "severity": "low",
      "userImpact": "Users can now use standard gRPC tools to explore and test Temporal APIs without manual protobuf definition linking.",
      "rootCause": "gRPC reflection was incompatible with gogo protobuf implementation; required migration to google protobuf generation (issue #38 and #5032)",
      "proposedFix": "Support gRPC Reflection API for Frontend APIs; this was enabled through the protobuf library migration",
      "workaround": "Use alternative tools like evans for gRPC service exploration",
      "resolution": "fixed",
      "resolutionDetails": "Automatically enabled by issue #5032 (migration to google protobuf from gogo)",
      "related": [
        38,
        5032
      ],
      "keyQuote": "gRPC reflection will come automatically if we do #38. It doesn't work with gogo implementation.",
      "number": 448,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:58:33.421Z"
    },
    {
      "summary": "Temporal fails to start in Docker Swarm mode due to Ringpop membership issues. The service encounters broadcast address resolution errors on startup and connection timeout problems after restarts, with increasing cluster_membership table entries on each restart.",
      "category": "bug",
      "subcategory": "networking-docker-deployment",
      "apis": [],
      "components": [
        "ringpop",
        "membership",
        "history-service",
        "docker-networking"
      ],
      "concepts": [
        "broadcast-address",
        "cluster-discovery",
        "docker-swarm",
        "networking",
        "membership-discovery",
        "container-restart"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably run Temporal in Docker Swarm environments due to recurring startup failures and instability after container restarts.",
      "rootCause": "Ringpop fails to discover and join the cluster when containers receive dynamic IP addresses in Docker Swarm, and the bootstrap join timeout (30s) is exceeded. Stale cluster membership records accumulate on each restart.",
      "proposedFix": null,
      "workaround": "Manually set TEMPORAL_BROADCAST_ADDRESS to the container's IP and BIND_ON_IP to 0.0.0.0 via bash command wrapper; however, this only partially mitigates the issue.",
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "join duration of 49.070856456s exceeded max 30s",
      "number": 442,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:58:33.548Z"
    },
    {
      "summary": "Consistent query feature is disabled by default in clusters, but should be enabled by default or the toggle should be removed entirely to ensure consistent behavior.",
      "category": "feature",
      "subcategory": "query-consistency",
      "apis": [],
      "components": [
        "cluster-config",
        "query-engine"
      ],
      "concepts": [
        "consistency",
        "default-behavior",
        "query-configuration",
        "cluster-settings"
      ],
      "severity": "medium",
      "userImpact": "Users must manually enable consistent query, which may lead to unexpected inconsistent query results if not configured.",
      "rootCause": "Consistent query is disabled by default rather than being the default behavior.",
      "proposedFix": "Enable consistent query by default for all clusters or remove the ability to disable it.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Removed the toggle as noted in PR #468, making consistent query always enabled.",
      "related": [
        468
      ],
      "keyQuote": "Consistent query should be on by default for a cluster. Or may be completely remove the ability to turn it off.",
      "number": 439,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:57:53.830Z"
    },
    {
      "summary": "Child workflow fails to retry when retryPolicy is set but retryPolicy.maximumAttempts is 0 or not set. The server needs intelligent defaults for child workflow retry options instead of relying on explicit client-side settings.",
      "category": "bug",
      "subcategory": "child-workflow-retry",
      "apis": [
        "ChildWorkflowOptions"
      ],
      "components": [
        "child-workflow",
        "retry-policy",
        "server-defaults"
      ],
      "concepts": [
        "retry",
        "maximumAttempts",
        "backoff",
        "defaults",
        "child-workflow"
      ],
      "severity": "high",
      "userImpact": "Child workflows fail permanently instead of retrying when retry policy is configured but maximumAttempts is 0, requiring workarounds by setting explicit attempt counts.",
      "rootCause": "Server does not apply intelligent defaults for retry policy fields when maximumAttempts is 0 or unset; client SDK defaults like BackoffCoefficient are not consistently applied.",
      "proposedFix": "Implement intelligent defaults for ChildWorkflowOptions on the server side; remove client SDK defaults like BackoffCoefficient=2 and rely on server settings instead.",
      "workaround": "Explicitly set RetryPolicy.MaximumAttempts to a non-zero value (e.g., 5) to enable retries.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed by implementing server-side intelligent defaults for retry policies in child workflows.",
      "related": [],
      "keyQuote": "We need to basically set intelligent defaults for ChildWorkflowOptions on the server.",
      "number": 437,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:57:54.906Z"
    },
    {
      "summary": "Race conditions exist in mutable state updates because they rely only on lastEventId for conditional protection, while operations like retries and heartbeats don't increase history size, potentially causing concurrent update conflicts.",
      "category": "bug",
      "subcategory": "state-management",
      "apis": [],
      "components": [
        "mutable-state",
        "history",
        "retry-logic",
        "heartbeat-handler"
      ],
      "concepts": [
        "race-condition",
        "concurrency",
        "state-synchronization",
        "history-ordering",
        "event-id",
        "idempotency"
      ],
      "severity": "high",
      "userImpact": "Users may experience inconsistent or corrupted mutable state when retries and heartbeats occur concurrently.",
      "rootCause": "lastEventId alone is insufficient to guard against race conditions for operations that don't increment history size, such as retries and heartbeats.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "updates to the mutable state are conditional on lastEventId. But various operations like retries and heartbeats do not grow history size.",
      "number": 435,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:57:54.416Z"
    },
    {
      "summary": "Request to add broadcastPort configuration option alongside existing broadcastAddress to support dynamic port mapping scenarios in Temporal Server.",
      "category": "feature",
      "subcategory": "networking",
      "apis": [],
      "components": [
        "server-config",
        "networking"
      ],
      "concepts": [
        "broadcast-address",
        "port-mapping",
        "dynamic-configuration",
        "network-setup"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal Server in dynamic port mapping environments cannot configure broadcast ports separately from addresses.",
      "rootCause": null,
      "proposedFix": "Add broadcastPort configuration option to complement existing broadcastAddress option",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Feature was implemented and completed, though documentation may not fully reflect the capability",
      "related": [],
      "keyQuote": "In order to support dynamic port mapping scenarios we should add broadcastPort alongside the existing broadcastAddress option.",
      "number": 430,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:57:13.574Z"
    },
    {
      "summary": "After adding a second node to a single-node Temporal cluster, the original node repeatedly logs 'New timer generated is less than read level' warnings at ~1 second intervals. The issue appears to be related to shard timer management during cluster scaling.",
      "category": "bug",
      "subcategory": "cluster-membership",
      "apis": [],
      "components": [
        "history-service",
        "shard-context",
        "timer-management"
      ],
      "concepts": [
        "cluster-scaling",
        "node-addition",
        "shard-allocation",
        "log-spam",
        "timer-synchronization"
      ],
      "severity": "medium",
      "userImpact": "After scaling a single-node cluster to multi-node, users experience excessive warning logs that obscure other important information and suggest underlying cluster coordination issues.",
      "rootCause": "Likely related to how shard timers are managed when a new node joins the cluster; the new node may trigger timer reallocations that are out of sync with the original node's read level.",
      "proposedFix": "Ensure services are deployed as one service per host rather than all services in one container; issue may also be related to PR #915 regarding metrics client instantiation per deployment.",
      "workaround": "Deploy services to dedicated Docker containers or pods (one service per host) instead of running all services together.",
      "resolution": "fixed",
      "resolutionDetails": "Issue resolved when services were deployed to dedicated docker containers/pods rather than all services on the same host. Related to PR #915 which ensures one metrics client per deployment.",
      "related": [
        915
      ],
      "keyQuote": "Issue got resolved once services are deployed into dedicated docker / pods.",
      "number": 425,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:57:16.323Z"
    },
    {
      "summary": "CRON-scheduled workflows stop executing after a few runs in server version 0.23.1 due to incorrect timeout configuration. The issue occurs when using WorkflowExecutionTimeout instead of WorkflowRunTimeout, as the former caps the total duration for all periodic executions.",
      "category": "bug",
      "subcategory": "cron-scheduling",
      "apis": [
        "WorkflowOptions",
        "setWorkflowExecutionTimeout",
        "setWorkflowRunTimeout",
        "setWorkflowTaskTimeout"
      ],
      "components": [
        "cron-scheduler",
        "workflow-execution",
        "timeout-management"
      ],
      "concepts": [
        "cron-scheduling",
        "timeout-configuration",
        "workflow-lifecycle",
        "execution-timeout",
        "periodic-execution"
      ],
      "severity": "high",
      "userImpact": "CRON-scheduled workflows stop running after a few executions, preventing critical scheduled tasks from continuing unless the configuration is corrected.",
      "rootCause": "Confusion between timeout parameters in SDK 0.23.1: WorkflowExecutionTimeout applies to the entire execution lifetime and caps all periodic runs, while WorkflowRunTimeout applies to each individual run. Users migrating from 0.19.0 incorrectly used WorkflowExecutionTimeout instead of WorkflowRunTimeout.",
      "proposedFix": "Use WorkflowRunTimeout for CRON workflows instead of WorkflowExecutionTimeout. Do not set WorkflowExecutionTimeout for workflows that run indefinitely.",
      "workaround": "Replace WorkflowExecutionTimeout with WorkflowRunTimeout in workflow options configuration for CRON-scheduled workflows.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by identifying the correct timeout configuration. User determined that WorkflowRunTimeout should be used for CRON workflows, as WorkflowExecutionTimeout caps the entire execution lifetime.",
      "related": [],
      "keyQuote": "WorkflowRunTimeout is the timeout for each workflow run whereas WorkflowExecutionTimeout is the timeout for the overall execution and in the case of CRON workflows, since the same workflow ID is used for each periodic execution, this parameter caps the total duration till which the workflow schedule is valid.",
      "number": 424,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:57:15.553Z"
    },
    {
      "summary": "Add an admin command to dump dynamic configuration values for the Temporal cluster to help with troubleshooting and visibility into current configuration state.",
      "category": "feature",
      "subcategory": "admin-commands",
      "apis": [],
      "components": [
        "admin-cli",
        "dynamic-config",
        "cluster-management"
      ],
      "concepts": [
        "configuration",
        "troubleshooting",
        "observability",
        "cluster-state",
        "operations"
      ],
      "severity": "medium",
      "userImpact": "Operators and administrators cannot easily inspect current cluster dynamic configuration values, making troubleshooting and configuration verification difficult.",
      "rootCause": null,
      "proposedFix": "Add a new admin command that allows dumping either all dynamic config values or a specific config value",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Currently it is hard to know the current cluster configuration. Dumping all dynamic config values or at least a specific one would be very useful for troubleshooting.",
      "number": 421,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:56:34.497Z"
    },
    {
      "summary": "The reset canary fails with a panic when processing scheduled activities, logging 'lookup failed for scheduledID to activityID' error during workflow task processing in the Go SDK v0.22.8.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "decision-state-machine",
        "event-handler",
        "workflow-task-handler",
        "activity-scheduler"
      ],
      "concepts": [
        "workflow-reset",
        "activity-scheduling",
        "event-processing",
        "state-machine",
        "canary-testing",
        "panic-recovery"
      ],
      "severity": "high",
      "userImpact": "Workflow reset operations fail with unhandled panics, preventing users from resetting workflows in production.",
      "rootCause": "Mismatch in scheduled activity ID to activity ID lookup during workflow reset event processing in the decision state machine.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed on both canary and SDK sides according to contributor comment.",
      "related": [],
      "keyQuote": "lookup failed for scheduledID to activityID: scheduleID: 8, activity: 8",
      "number": 420,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:56:36.519Z"
    },
    {
      "summary": "The Go SDK does not emit `temporal_workflow_start` metrics for child workflows, creating an asymmetry where child workflow completions are tracked but their starts are not. The issue was resolved by migrating from `temporal_workflow_start` to granular `temporal_request` metrics tagged with API names.",
      "category": "bug",
      "subcategory": "metrics-observability",
      "apis": [
        "StartWorkflowExecution",
        "SignalWithStartWorkflowExecution"
      ],
      "components": [
        "metrics",
        "workflow-start",
        "observability"
      ],
      "concepts": [
        "child-workflows",
        "metrics-tracking",
        "observability",
        "monitoring",
        "canary"
      ],
      "severity": "medium",
      "userImpact": "Users cannot track child workflow starts in metrics, making it difficult to monitor and debug workflow execution patterns in production systems.",
      "rootCause": "The `temporal_workflow_start` metric was only emitted for non-child workflows, not for child workflows, creating inconsistent telemetry.",
      "proposedFix": "Emit `temporal_workflow_start` for all workflow starts including child workflows, or provide equivalent visibility through alternative metrics.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "The `temporal_workflow_start` metric was deprecated in favor of granular `temporal_request` metrics tagged with API names (e.g., StartWorkflowExecution, SignalWithStartWorkflowExecution).",
      "related": [],
      "keyQuote": "We don't emit `temporal_workflow_start` anymore. Every API request emits `temporal_request` metrics tagged with API name.",
      "number": 415,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:56:36.050Z"
    },
    {
      "summary": "Feature request to improve workflow query consistency by making strong consistency the default and updating documentation. The user encountered an error when attempting strongly consistent queries and had to manually enable a dynamic config setting.",
      "category": "docs",
      "subcategory": "workflow-query",
      "apis": [],
      "components": [
        "workflow-query",
        "dynamic-config",
        "consistency-level"
      ],
      "concepts": [
        "strong-consistency",
        "query-latency",
        "dynamic-configuration",
        "namespace-settings",
        "api-documentation"
      ],
      "severity": "low",
      "userImpact": "Users encounter errors when attempting strongly consistent workflow queries and must manually configure dynamic settings to enable this functionality.",
      "rootCause": "Dynamic config setting (history.EnableConsistentQueryByNamespace) needs to be enabled for strong consistency; documentation is outdated.",
      "proposedFix": "Make history.EnableConsistentQueryByNamespace default to true and update documentation to reflect current behavior.",
      "workaround": "Manually enable the dynamic config variable history.EnableConsistentQueryByNamespace: true",
      "resolution": "fixed",
      "resolutionDetails": "Query consistency level was removed with default changed to strong consistency; dynamic config setting eliminated. Documentation needs updating.",
      "related": [
        468
      ],
      "keyQuote": "Query consistency level was completely removed together with dynamic config settings... Default is `strong`. Docs need to be updated though.",
      "number": 414,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:55:57.982Z"
    },
    {
      "summary": "Temporal CLI workflow query command returns JSON unmarshal errors when querying with certain query types like __open_sessions or getStats, failing to properly decode response payloads.",
      "category": "bug",
      "subcategory": "cli-workflow-query",
      "apis": [],
      "components": [
        "cli",
        "workflow-query",
        "json-decoder"
      ],
      "concepts": [
        "query-result",
        "json-unmarshalling",
        "response-decoding",
        "workflow-state",
        "error-handling"
      ],
      "severity": "high",
      "userImpact": "Users cannot query workflow status using the CLI with certain query types due to JSON decoding failures.",
      "rootCause": "JSON unmarshal logic incorrectly expects string type but receives array or object in response payload.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in PR #542 according to comment from maintainer.",
      "related": [
        542
      ],
      "keyQuote": "unable to decode JSON: json: cannot unmarshal array into Go value of type string",
      "number": 413,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:55:57.869Z"
    },
    {
      "summary": "Memory usage in the canary process continuously increases over time, eventually exhausting 1GB of memory after about three days and causing an out-of-memory (OOM) kill in the Kubernetes cluster.",
      "category": "bug",
      "subcategory": "canary-memory-leak",
      "apis": [],
      "components": [
        "canary",
        "memory-management",
        "process-lifecycle"
      ],
      "concepts": [
        "memory-leak",
        "resource-exhaustion",
        "long-running-process",
        "container-orchestration",
        "oom-killer"
      ],
      "severity": "high",
      "userImpact": "Users running canary monitoring cannot maintain continuous operation as the process will be killed by OOM after several days of execution.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved, likely through memory leak identification and fix in the canary implementation.",
      "related": [],
      "keyQuote": "The memory usage of the canary process is always increasing, and after about three days it eventually used up 1GB of memory",
      "number": 411,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:55:58.185Z"
    },
    {
      "summary": "python-betterproto compatibility issues were identified with temporal-proto: recursive Failure object initialization causing stack overflow, and 'None' being a reserved Python keyword in QueryRejectCondition enum values. Author notes these are betterproto limitations, not proto issues, and plans to fork betterproto to accommodate.",
      "category": "other",
      "subcategory": "proto-compatibility",
      "apis": [],
      "components": [
        "proto-definitions",
        "failure-handling",
        "query-reject-condition"
      ],
      "concepts": [
        "protobuf",
        "python-betterproto",
        "enum-naming",
        "recursive-objects",
        "compatibility"
      ],
      "severity": "low",
      "userImpact": "Python SDK users considering python-betterproto for better gRPC experience encounter incompatibility issues that require workarounds.",
      "rootCause": "python-betterproto's default field initialization behavior doesn't support HasField/ClearField semantics needed for recursive Failure objects, and uses Python reserved keywords in enum value generation.",
      "proposedFix": "Temporal team could rename enum values with type prefixes per protobuf style guide to avoid Python keywords, though this would not fully solve the betterproto compatibility.",
      "workaround": "Fork python-betterproto and modify field initialization to not instantiate sub-message fields, and adjust enum naming casing.",
      "resolution": "wontfix",
      "resolutionDetails": "Temporal decided not to change proto semantics. Team noted they would add enum type prefixes anyway per style guide. Author's fork of python-betterproto is the recommended approach.",
      "related": [],
      "keyQuote": "I will need to fork python-betterproto whether or not any changes are made in temporal-proto for the two above issues",
      "number": 409,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:55:19.778Z"
    },
    {
      "summary": "StartChildWorkflowExecution failure enum includes UnknownExternalWorkflowExecution which is inappropriate for workflow creation. The enum should be separated into distinct types for starting workflows versus signaling/canceling external executions.",
      "category": "bug",
      "subcategory": "child-workflow",
      "apis": [
        "StartChildWorkflowExecution"
      ],
      "components": [
        "child-workflow-execution",
        "workflow-execution-failed-cause",
        "external-workflow-execution"
      ],
      "concepts": [
        "enum-design",
        "api-design",
        "error-handling",
        "external-workflow",
        "signal-cancel"
      ],
      "severity": "medium",
      "userImpact": "Users encounter semantically incorrect failure reasons when child workflow execution fails, causing confusion about actual failure causes.",
      "rootCause": "Single enum used for multiple different operations (start, signal, cancel) leads to invalid reason values for specific operations",
      "proposedFix": "Separate the enum into two: one for StartChildWorkflowExecution failures and another for signal/cancel external execution failures",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The same enum is used when signalling and cancelling external executions. The solution is to separate this enum in two.",
      "number": 407,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:55:18.404Z"
    },
    {
      "summary": "Feature request to add a flag allowing CRON workflows to execute their first run immediately instead of waiting for the scheduled time. Currently, the first execution always follows the cron schedule.",
      "category": "feature",
      "subcategory": "cron-scheduling",
      "apis": [],
      "components": [
        "cron-workflow",
        "workflow-scheduler"
      ],
      "concepts": [
        "cron-schedule",
        "immediate-execution",
        "first-run",
        "workflow-timing"
      ],
      "severity": "low",
      "userImpact": "Users cannot configure CRON workflows to start immediately on creation; they must wait for the next scheduled time.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of issue #404",
      "related": [
        404
      ],
      "keyQuote": "There are use cases when the first run needs to execute immediately",
      "number": 405,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:55:16.967Z"
    },
    {
      "summary": "Request to add the ability to force immediate execution of a waiting cron workflow without terminating it or creating a new workflow with a different ID. The feature has been implemented using TerminateIfRunning policy and SignalWithStartWorkflowExecution.",
      "category": "feature",
      "subcategory": "cron-workflows",
      "apis": [
        "SignalWithStartWorkflowExecution",
        "StartWorkflow"
      ],
      "components": [
        "workflow-execution",
        "cron-scheduler",
        "workflow-reuse-policy"
      ],
      "concepts": [
        "cron-execution",
        "workflow-restart",
        "immediate-execution",
        "terminate-if-running",
        "workflow-idempotency",
        "signal-with-start"
      ],
      "severity": "medium",
      "userImpact": "Users can now trigger immediate execution of waiting cron workflows without terminating or restarting with a different ID.",
      "rootCause": null,
      "proposedFix": "Use TerminateIfRunning id reuse policy with StartWorkflow or SignalWithStartWorkflowExecution to restart workflow atomically and force immediate cron execution.",
      "workaround": "SignalWithStartWorkflowExecution can be used to execute the waiting cron next run immediately.",
      "resolution": "fixed",
      "resolutionDetails": "TerminateIfRunning support was added to Temporal API and SignalWithStartWorkflowExecution can be used to execute waiting cron immediately.",
      "related": [],
      "keyQuote": "SignalWithStartWorkflowExecution can be used to execute waiting cron (next run) immediately.",
      "number": 404,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:54:38.988Z"
    },
    {
      "summary": "Canary process exits with error when the 'canary' namespace already exists, instead of silently ignoring the AlreadyExists error as in prior versions. This breaks canary startup on clusters with existing namespaces.",
      "category": "bug",
      "subcategory": "canary-startup",
      "apis": [
        "CreateNamespace"
      ],
      "components": [
        "canary",
        "namespace-management",
        "client"
      ],
      "concepts": [
        "namespace-creation",
        "error-handling",
        "startup-behavior",
        "idempotency"
      ],
      "severity": "high",
      "userImpact": "Users running canary on clusters with existing namespaces experience startup failures instead of graceful continuation.",
      "rootCause": "Changes in v0.23.0 removed the error suppression logic for AlreadyExists errors during namespace creation, causing the canary to exit instead of proceeding.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "self_resolved",
      "resolutionDetails": "Author identified the issue was caused by building canary with wrong version of proto submodule, not actual code behavior change.",
      "related": [],
      "keyQuote": "Prior to v0.23.0, the canary would start up and always attempt to create the `canary` namespace. If the namespace already existed, it would silently ignore the error and continue.",
      "number": 399,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:54:39.093Z"
    },
    {
      "summary": "Canary workflow stops executing after ~20 minutes on v0.23.0, with the cron workflow being invoked repeatedly but never spawning the sanity workflow. The timer_tasks table grows at >1 row/second, and the reset canary fails consistently.",
      "category": "bug",
      "subcategory": "canary-testing",
      "apis": [],
      "components": [
        "cron-workflow",
        "timer-tasks",
        "workflow-execution",
        "task-scheduler"
      ],
      "concepts": [
        "cron-schedule",
        "workflow-scheduling",
        "timer-accumulation",
        "database-growth",
        "canary-testing"
      ],
      "severity": "high",
      "userImpact": "Temporal cluster operators cannot reliably use the canary to verify cluster correctness, breaking their quality assurance workflow.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Once the cluster was upgraded, I started running the canary...it ran for almost 20 minutes then the cron workflow...stopped starting.",
      "number": 394,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:54:39.595Z"
    },
    {
      "summary": "The dryrun option in temporal-sql-tool misleads users by appearing to simulate schema changes without applying them, but actually executes the schema updates regardless. The feature was removed because it provided false confidence without meaningful protection.",
      "category": "bug",
      "subcategory": "schema-management",
      "apis": [],
      "components": [
        "temporal-sql-tool",
        "schema-setup",
        "database-migration"
      ],
      "concepts": [
        "dryrun",
        "schema-migration",
        "database-changes",
        "tool-behavior",
        "safety-testing"
      ],
      "severity": "medium",
      "userImpact": "Users attempting to safely preview schema changes with --dryrun are misled into thinking changes won't be applied, but the schema is actually modified, defeating the purpose of the dry run.",
      "rootCause": "The dryrun functionality creates a temporary database and executes schema updates on it, but still applies the actual schema changes, making the dryrun option meaningless and confusing.",
      "proposedFix": "Remove the dryrun functionality entirely since it does not provide the intended safety mechanism.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Dry run ability was removed entirely because the functionality did not provide meaningful protection and only allowed users to inadvertently cause problems.",
      "related": [
        1300
      ],
      "keyQuote": "dry run ability is removed, since dry run functionality does not really do anything other than letting user to shoot them in the foot.",
      "number": 392,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:54:00.719Z"
    },
    {
      "summary": "XDC (cross-datacenter) replication failing with two errors: ShardId not set on deserialization and connection timeouts to incorrect worker port (7233 instead of 7239). Workflows don't continue after namespace failover between clusters.",
      "category": "bug",
      "subcategory": "xdc-replication",
      "apis": [],
      "components": [
        "replicator",
        "worker",
        "replication-task-processor",
        "scanner"
      ],
      "concepts": [
        "xdc-replication",
        "failover",
        "namespace",
        "cluster",
        "deserialization",
        "port-configuration"
      ],
      "severity": "high",
      "userImpact": "Users cannot perform successful cross-datacenter replication and namespace failovers, preventing active-active cluster setups from functioning.",
      "rootCause": "ShardId not being set during replication task deserialization; worker attempting to connect to wrong port (7233) instead of configured port (7239)",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved (closed status indicates fix was implemented)",
      "related": [],
      "keyQuote": "ShardId not set on request + connection error attempting 172.17.0.10:7233 when worker listens on 7239",
      "number": 379,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:54:00.281Z"
    },
    {
      "summary": "ParentClosePolicy is currently always applied when a parent workflow calls continue-as-new, but in most cases it should only apply when the parent completes. The request is to restructure ParentClosePolicy as a struct with an applyOnParentContinueAsNew field to control this behavior.",
      "category": "feature",
      "subcategory": "workflow-parent-close-policy",
      "apis": [],
      "components": [
        "workflow-engine",
        "parent-child-workflow",
        "policy-management",
        "continue-as-new"
      ],
      "concepts": [
        "parent-close-policy",
        "continue-as-new",
        "workflow-completion",
        "policy-configuration",
        "child-workflow-lifecycle"
      ],
      "severity": "low",
      "userImpact": "Users cannot control whether ParentClosePolicy applies to parent continue-as-new operations, forcing them to accept undesired behavior in most cases.",
      "rootCause": "ParentClosePolicy is hardcoded to apply in all parent completion scenarios including continue-as-new, without configuration option.",
      "proposedFix": "Change ParentClosePolicy to a struct with two fields: the current enum and an applyOnParentContinueAsNew boolean flag to conditionally apply the policy.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Change ParentClosePolicy to a structure with two fields: The current ParentClosePolicy enum and applyOnParentContinueAsNew.",
      "number": 373,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:54:00.290Z"
    },
    {
      "summary": "Docker Compose with Elasticsearch fails on macOS Catalina due to insufficient memory allocation (2GB default). Elastic Search crashes with error 137, indicating out-of-memory issues when running temporal with Elasticsearch.",
      "category": "docs",
      "subcategory": "docker-setup",
      "apis": [],
      "components": [
        "docker-compose",
        "elasticsearch",
        "configuration"
      ],
      "concepts": [
        "memory-allocation",
        "docker-configuration",
        "docker-compose",
        "system-requirements",
        "resource-constraints"
      ],
      "severity": "medium",
      "userImpact": "Users on macOS cannot run docker-compose with Elasticsearch due to insufficient default memory allocation causing the container to crash.",
      "rootCause": "Default memory allocated to Docker is set to 2GB, which is insufficient for running Temporal with Elasticsearch together.",
      "proposedFix": "Documentation should be enhanced to mention that users need to increase the memory setting for Docker before running the temporal ES docker container.",
      "workaround": "Manually increase Docker memory allocation before running docker-compose.",
      "resolution": "fixed",
      "resolutionDetails": "Documentation was updated via PR #240 to address the memory requirement issue.",
      "related": [],
      "keyQuote": "Documentation should be enhanced to mention that users change the memory setting for docker before running temporal es docker container.",
      "number": 372,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:53:20.882Z"
    },
    {
      "summary": "User reports inability to reset a workflow multiple times - the first reset succeeds, but subsequent reset requests return the runID of the second execution instead of creating a new reset. The issue reporter questions whether this is expected behavior or a bug.",
      "category": "bug",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "workflow-reset",
        "tctl",
        "execution-management"
      ],
      "concepts": [
        "workflow-reset",
        "execution-state",
        "idempotency",
        "runID",
        "event-history"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably reset workflows multiple times, limiting the utility of the reset feature for testing and recovery scenarios.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Reproducer was unable to reproduce the issue. The maintainer (vitarb) successfully performed multiple resets with different runIDs being returned correctly, suggesting the issue may be user error or environment-specific.",
      "related": [],
      "keyQuote": "it seems to be able to do it once, but every subsequent request returns the runID of the 2nd execution and won't create another one",
      "number": 368,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:53:21.705Z"
    },
    {
      "summary": "ParentClosePolicy's terminate action fails to terminate child workflows that have called continue-as-new or are cron workflows, because it uses the wrong RunID.",
      "category": "bug",
      "subcategory": "workflow-execution",
      "apis": [],
      "components": [
        "parentclosepolicy",
        "worker",
        "workflow-termination"
      ],
      "concepts": [
        "parent-close-policy",
        "continue-as-new",
        "cron-workflow",
        "runid",
        "workflow-chain"
      ],
      "severity": "high",
      "userImpact": "Users cannot reliably terminate child workflows that use continue-as-new, breaking the parent-child workflow lifecycle management.",
      "rootCause": "The terminate request uses the RunID of the child workflow, which becomes invalid after continue-as-new creates a new execution. The correct RunID should be the first workflow in the continue-as-new chain.",
      "proposedFix": "Either change the semantic of RunID in the terminate request to reference the first workflow in the continue-as-new chain, or introduce a new parameter to handle this case.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by modifying the parentclosepolicy implementation to handle continue-as-new workflows correctly",
      "related": [],
      "keyQuote": "runID of a child is passed to a terminate request which doesn't work for children that called continue as new",
      "number": 364,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:53:20.970Z"
    },
    {
      "summary": "The tctl describe command fails when a workflow has pending activities with failures, unable to decode the failure details due to JSON unmarshaling into an incorrect Go type.",
      "category": "bug",
      "subcategory": "tctl-describe",
      "apis": [],
      "components": [
        "tctl",
        "failure-decoder",
        "workflow-describe"
      ],
      "concepts": [
        "failure-handling",
        "pending-activities",
        "json-deserialization",
        "type-mismatch"
      ],
      "severity": "medium",
      "userImpact": "Users cannot inspect workflows that have pending activities with failures, blocking troubleshooting and monitoring operations.",
      "rootCause": "JSON unmarshaling attempts to decode a failure details object into a string type instead of the correct object structure.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was likely fixed in subsequent releases after verification.",
      "related": [],
      "keyQuote": "Error: Unable to decode last failure details. Error Details: payload item 0: unable to decode JSON: json: cannot unmarshal object into Go value of type string",
      "number": 361,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:52:42.712Z"
    },
    {
      "summary": "User requests documentation comparing Temporal to its predecessor Cadence, noting major differences between the two projects.",
      "category": "docs",
      "subcategory": "documentation",
      "apis": [],
      "components": [],
      "concepts": [
        "migration",
        "comparison",
        "documentation"
      ],
      "severity": "low",
      "userImpact": "Users migrating from Cadence lack clear guidance on differences, potentially causing confusion during adoption.",
      "rootCause": null,
      "proposedFix": "Create documentation page describing major differences from Cadence",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Team committed to providing comparison documentation with initial reference to Stack Overflow discussion",
      "related": [],
      "keyQuote": "As this project originates from Cadence, is there a page that describes major differences from it?",
      "number": 359,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:52:41.930Z"
    },
    {
      "summary": "Workflows fail to start when TTL would exceed Cassandra's 2038-01-19 expiration limit. Request to support explicit infinite timeout to bypass this limitation when using Temporal with Cassandra.",
      "category": "feature",
      "subcategory": "workflow-timeout",
      "apis": [],
      "components": [
        "workflow-engine",
        "cassandra-persistence",
        "task-creation"
      ],
      "concepts": [
        "timeout",
        "TTL",
        "expiration",
        "workflow-lifecycle",
        "cassandra-limitation"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal on Cassandra cannot create long-running workflows that exceed 2038-01-19T03:14:06+00:00 without changing database configuration or accepting truncated TTLs.",
      "rootCause": "Cassandra has a built-in limitation where TTLs cannot extend past 2038-01-19T03:14:06+00:00 due to signed 32-bit integer overflow, and this limitation propagates to Temporal's task creation.",
      "proposedFix": "Add support for explicit infinite timeout option in workflow configuration to allow users to bypass TTL constraints when using Cassandra.",
      "workaround": "Adjust Cassandra overflow policy to truncate TTLs to the 2038 limit, though this may affect retention guarantees.",
      "resolution": "fixed",
      "resolutionDetails": "Resolved in v1.4.1 by supporting infinite timeouts for workflows.",
      "related": [],
      "keyQuote": "Temporal could solve this problem for end users by explicitly allowing an infinite timeout when running workflows.",
      "number": 358,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:52:43.081Z"
    },
    {
      "summary": "Cron workflows need the ability to stop scheduled executions without terminating currently running instances. This request supports use cases where users want to pause future executions or schedule workflows to run only once.",
      "category": "feature",
      "subcategory": "cron-scheduling",
      "apis": [],
      "components": [
        "cron-workflow",
        "workflow-scheduler",
        "execution-engine"
      ],
      "concepts": [
        "scheduling",
        "cron-jobs",
        "workflow-termination",
        "execution-control",
        "pause-resume"
      ],
      "severity": "medium",
      "userImpact": "Users cannot stop cron workflows from scheduling new executions without terminating currently running instances, limiting workflow lifecycle management.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed by introducing Schedule Workflow feature which supports stopping new runs without terminating current runs and single-run schedules.",
      "related": [],
      "keyQuote": "is there a way to terminate a cron scheduled workflow in a way that doesn't terminate it while it's running, but prevents the next execution?",
      "number": 356,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:52:03.508Z"
    },
    {
      "summary": "UI displays incorrect timestamps for workflows, always showing the most recent event timestamp instead of the correct timestamp for each individual workflow. This is a duplicate issue reported in the temporal-web repository.",
      "category": "bug",
      "subcategory": "ui-timestamp",
      "apis": [],
      "components": [
        "ui",
        "temporal-web",
        "workflow-display"
      ],
      "concepts": [
        "timestamp",
        "event-ordering",
        "ui-display",
        "data-consistency"
      ],
      "severity": "medium",
      "userImpact": "Users cannot accurately track when individual workflows were triggered because the UI displays the wrong timestamp for all workflows.",
      "rootCause": "temporal-web repository issue causing incorrect timestamp assignment to workflow events",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Marked as duplicate of temporalio/temporal-web#33 which contains the fix and more details about the issue",
      "related": [
        33
      ],
      "keyQuote": "Multiple workflows get triggered within different times of the day, but it looks like it just sets everything to the newest date.",
      "number": 353,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:52:03.242Z"
    },
    {
      "summary": "Users need to query the most recent workflow execution per workflow ID to identify which actors/workflows are currently in a failed state, filtering out those that have since recovered. Current list APIs return all executions, not the latest per workflow.",
      "category": "feature",
      "subcategory": "visibility-querying",
      "apis": [],
      "components": [
        "visibility",
        "list-apis",
        "elasticsearch-integration"
      ],
      "concepts": [
        "workflow-execution-state",
        "filtering",
        "query-aggregation",
        "actor-model",
        "latest-run"
      ],
      "severity": "high",
      "userImpact": "Users cannot efficiently query the state of millions of actors to identify which ones are currently failing, requiring workarounds or custom Elasticsearch queries outside the Temporal API.",
      "rootCause": "Temporal list APIs return all workflow executions chronologically, but there is no built-in way to get only the most recent execution per workflow ID for aggregated state queries.",
      "proposedFix": "Add support for querying the most recent execution per workflow through either enhanced tctl/SDK list APIs or official Elasticsearch aggregation support, potentially requiring additional indexing on current run state.",
      "workaround": "Issue direct Elasticsearch aggregation queries (not officially supported, requires depending on internal unstable interfaces).",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "what I really want is to list the set of most recent executions per workflow",
      "number": 351,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:52:03.937Z"
    },
    {
      "summary": "Request to make the runId parameter optional for workflow reset operations, as manually extracting and providing it is cumbersome and error-prone when users could rely on automatic extraction based on the workflow ID.",
      "category": "feature",
      "subcategory": "workflow-reset",
      "apis": [],
      "components": [
        "cli",
        "workflow-reset"
      ],
      "concepts": [
        "workflow-reset",
        "runId",
        "usability",
        "automation",
        "workflow-execution"
      ],
      "severity": "low",
      "userImpact": "Users must manually extract the runId for each workflow reset operation instead of having it automatically determined from the workflow ID.",
      "rootCause": null,
      "proposedFix": "Make runId optional in the reset command and automatically extract it from the workflow execution information when not provided.",
      "workaround": "Manually extract runId using workflow describe and pass it explicitly to the reset command.",
      "resolution": "wontfix",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Right now Im doing cadence workflow reset with manual extraction but Im not sure if that automatic extraction of runid wont cause some surprising behaviour",
      "number": 345,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:51:24.475Z"
    },
    {
      "summary": "The CLI README contains a broken link to outdated external documentation. The link should point to https://docs.temporal.io/docs/learn-cli instead of https://docs.temporal.io/docs/08_cli.",
      "category": "docs",
      "subcategory": "documentation-links",
      "apis": [],
      "components": [
        "cli",
        "documentation",
        "readme"
      ],
      "concepts": [
        "documentation",
        "broken-links",
        "external-references",
        "docs-maintenance"
      ],
      "severity": "low",
      "userImpact": "Users following the CLI documentation link encounter outdated or broken pages, disrupting their learning experience.",
      "rootCause": "Outdated hardcoded URL in CLI README pointing to a deprecated documentation page.",
      "proposedFix": "Update the link in tools/cli/README.md from https://docs.temporal.io/docs/08_cli to https://docs.temporal.io/docs/learn-cli",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Documentation link was updated to point to the correct URL.",
      "related": [],
      "keyQuote": "The external link is outdated and instead should be: https://docs.temporal.io/docs/learn-cli",
      "number": 340,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:51:24.743Z"
    },
    {
      "summary": "CLI arguments should use dashes instead of underscores (e.g., --execution-timeout instead of --execution_timeout) to match standard CLI conventions and improve usability.",
      "category": "feature",
      "subcategory": "cli-interface",
      "apis": [],
      "components": [
        "cli",
        "argument-parser"
      ],
      "concepts": [
        "usability",
        "cli-conventions",
        "argument-naming",
        "user-experience"
      ],
      "severity": "low",
      "userImpact": "Users find CLI arguments easier to type when using dashes instead of underscores, reducing cognitive load from switching between special characters.",
      "rootCause": null,
      "proposedFix": "Standardize all CLI arguments to use dashes instead of underscores throughout the codebase",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed by recent CLI work and approved/implemented via proposal PR #30",
      "related": [],
      "keyQuote": "Usually CLIs use dashes instead of _ . That way is easier to type the arguments as you only use one special character",
      "number": 338,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:51:23.765Z"
    },
    {
      "summary": "Cron workflows and workflows waiting to retry currently show the same status as other workflows until execution time. The request is to return a distinct status value from DescribeWorkflowExecution to indicate workflows in the 'waiting to execute' state.",
      "category": "feature",
      "subcategory": "workflow-status",
      "apis": [
        "DescribeWorkflowExecution"
      ],
      "components": [
        "workflow-state-machine",
        "cron-workflow",
        "retry-logic"
      ],
      "concepts": [
        "workflow-status",
        "cron",
        "retry",
        "execution-timing",
        "state-visibility"
      ],
      "severity": "low",
      "userImpact": "Users cannot easily determine whether a cron or retry workflow is waiting for its scheduled execution time versus being in other states.",
      "rootCause": null,
      "proposedFix": "Return a different status value from DescribeWorkflowExecution for workflows waiting to execute.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Status is 'created' for workflows not in 'running' state, providing the visibility requested.",
      "related": [],
      "keyQuote": "It would be nice to return different status value from DescribeWorkflowExecution for workflows in this state.",
      "number": 331,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:50:45.741Z"
    },
    {
      "summary": "The CLI 'workflow run' command times out after 2 minutes while polling for workflow completion events, even though the workflow continues executing. The gRPC connection used for history event streaming has a default 2-minute timeout that causes the client to stop polling.",
      "category": "bug",
      "subcategory": "cli-workflow-timeout",
      "apis": [],
      "components": [
        "cli",
        "workflow-run-command",
        "grpc-connection",
        "history-event-polling"
      ],
      "concepts": [
        "timeout",
        "context-deadline",
        "long-polling",
        "grpc-connection",
        "event-streaming",
        "workflow-completion"
      ],
      "severity": "medium",
      "userImpact": "Users cannot wait for workflow completion via CLI for long-running workflows without manually specifying a context timeout flag.",
      "rootCause": "The gRPC connection listening to history events has a default 2-minute timeout; when it expires, the client stops making long poll requests even though the workflow continues executing.",
      "proposedFix": "Either change the default context timeout behavior, improve the long polling mechanism to refresh before timeout, or provide better documentation/automatic handling of the --context_timeout flag.",
      "workaround": "Add the --context_timeout flag when running workflows expected to take longer than 2 minutes.",
      "resolution": "fixed",
      "resolutionDetails": "Issue addressed through improved timeout handling and context management in the CLI workflow run command.",
      "related": [],
      "keyQuote": "The workflow execution continues regardless of the error, what happens is that the gRPC connection that listens to history events is getting timed out. By default it is set to 2 minutes.",
      "number": 329,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:50:46.567Z"
    },
    {
      "summary": "Request to support Amazon Keyspaces as a persistence backend for Temporal. AWS Keyspaces claims to support LWT (Lightweight Transactions), which would enable Temporal to run on AWS's managed Cassandra-compatible database.",
      "category": "feature",
      "subcategory": "database-support",
      "apis": [],
      "components": [
        "persistence",
        "database-adapter",
        "cassandra-client"
      ],
      "concepts": [
        "database-backend",
        "cassandra-compatibility",
        "aws-integration",
        "lightweight-transactions",
        "managed-database"
      ],
      "severity": "medium",
      "userImpact": "Users running on AWS would be able to use Temporal with Amazon Keyspaces instead of needing to manage their own Cassandra or PostgreSQL database.",
      "rootCause": null,
      "proposedFix": "Implement support for Amazon Keyspaces as a persistence backend, leveraging its LWT support to meet Temporal's requirements.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This would be great for users running on AWS.",
      "number": 328,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:50:45.255Z"
    },
    {
      "summary": "Postgres SQL plugin lacks maxConns configuration support that MySQL plugin already provides, limiting connection pool management capabilities for Postgres users.",
      "category": "feature",
      "subcategory": "persistence-postgres",
      "apis": [],
      "components": [
        "sql-plugin",
        "postgres-plugin",
        "connection-pool"
      ],
      "concepts": [
        "database-configuration",
        "connection-management",
        "sql-persistence",
        "plugin-feature-parity"
      ],
      "severity": "medium",
      "userImpact": "Users deploying Temporal with Postgres cannot configure maximum connections, reducing control over database resource utilization compared to MySQL deployments.",
      "rootCause": "Postgres plugin implementation missing maxConns configuration parameter that exists in MySQL plugin.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Postgres plugin was updated to support maxConns configuration matching MySQL plugin functionality.",
      "related": [],
      "keyQuote": "Mysql supports it... Postgres does not",
      "number": 325,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:50:06.976Z"
    },
    {
      "summary": "Request to enable TLS support for database connections in temporal_sql_tool. The tool accepts TLS parameters but they are not being specified in the Docker startup script, leaving database connections unencrypted.",
      "category": "feature",
      "subcategory": "database-configuration",
      "apis": [],
      "components": [
        "docker",
        "temporal_sql_tool",
        "database-connection"
      ],
      "concepts": [
        "tls",
        "encryption",
        "database-security",
        "configuration",
        "mysql",
        "postgresql"
      ],
      "severity": "medium",
      "userImpact": "Users cannot secure their database connections with TLS, exposing database communication to potential interception.",
      "rootCause": null,
      "proposedFix": "Update docker/start.sh to pass TLS parameters to temporal_sql_tool invocations.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "MySQL and PostgreSQL TLS support has been implemented.",
      "related": [],
      "keyQuote": "MySQL & PostgreSQL all support TLS now",
      "number": 323,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:50:06.504Z"
    },
    {
      "summary": "Users encounter 'RESOURCE_EXHAUSTED: Persistence Max QPS Reached for List Operations' errors with the default frontend.visibilityListMaxQPS setting. The current default was calibrated for internal Uber use but is too restrictive for the open source version.",
      "category": "bug",
      "subcategory": "frontend-qps-limits",
      "apis": [],
      "components": [
        "frontend",
        "persistence",
        "visibility-service"
      ],
      "concepts": [
        "qps-limiting",
        "resource-exhaustion",
        "list-operations",
        "configuration-defaults"
      ],
      "severity": "high",
      "userImpact": "Users experience resource exhaustion errors when querying workflow lists, limiting the usability of the Temporal UI.",
      "rootCause": "The default frontend.visibilityListMaxQPS limit was designed for Uber's internal use patterns and is too low for typical open source deployments.",
      "proposedFix": "Increase the default value for frontend.visibilityListMaxQPS to accommodate open source use cases.",
      "workaround": "Users can manually adjust the frontend.visibilityListMaxQPS configuration setting.",
      "resolution": "invalid",
      "resolutionDetails": "Issue marked as no longer relevant by maintainer, suggesting it may have been addressed through configuration changes or default adjustments.",
      "related": [],
      "keyQuote": "A lot of users are running into 8 RESOURCE_EXHAUSTED: Persistence Max QPS Reached for List Operations.",
      "number": 322,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T16:50:07.757Z"
    },
    {
      "summary": "User requested the ability to inject dynamic config files into the Temporal Docker image, making server configuration more flexible for containerized deployments. The request was resolved by confirming that dynamic config file paths are already configurable via the DYNAMIC_CONFIG_FILE_PATH environment variable.",
      "category": "feature",
      "subcategory": "deployment-configuration",
      "apis": [],
      "components": [
        "docker-image",
        "dynamic-config",
        "server-configuration"
      ],
      "concepts": [
        "configuration-injection",
        "environment-variables",
        "docker-deployment",
        "volume-mounting",
        "kubernetes-deployment"
      ],
      "severity": "low",
      "userImpact": "Users can now configure dynamic config files for Temporal server deployments using environment variables and volume mounts.",
      "rootCause": null,
      "proposedFix": "Expose dynamic config file path as an environment variable (DYNAMIC_CONFIG_FILE_PATH) for the server Docker image.",
      "workaround": "Attach a volume to the Docker image and pass the config file path using the DYNAMIC_CONFIG_FILE_PATH environment variable.",
      "resolution": "fixed",
      "resolutionDetails": "Dynamic config file path is already configurable via DYNAMIC_CONFIG_FILE_PATH environment variable, implemented in commit 2a4449cc764dc4d24e226525e85ce0a05539dfab.",
      "related": [],
      "keyQuote": "you could volume mount a config file at runtime and set the `DYNAMIC_CONFIG_FILE_PATH` env var to tell the temporal server which path to read the config from",
      "number": 321,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:14.246Z"
    },
    {
      "summary": "Listing task list partitions crashes the server with a nil pointer dereference during protobuf marshaling. The crash occurs in TaskListPartitionMetadata.MarshalToSizedBuffer when a null partition metadata object is encountered.",
      "category": "bug",
      "subcategory": "tasklist-partition",
      "apis": [],
      "components": [
        "matching-service",
        "tasklist-manager",
        "protobuf-serialization"
      ],
      "concepts": [
        "nil-pointer",
        "marshaling",
        "memory-safety",
        "tasklist-partitions",
        "grpc-serialization"
      ],
      "severity": "high",
      "userImpact": "Users cannot list task list partitions using tctl without the server crashing.",
      "rootCause": "TaskListPartitionMetadata object is nil when being marshaled to protobuf in the ListTaskListPartitionsResponse.",
      "proposedFix": "Fix in PR #639 which handles nil partition metadata objects properly during serialization.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by PR #639 which adds proper nil checks before marshaling TaskListPartitionMetadata.",
      "related": [
        639
      ],
      "keyQuote": "panic: runtime error: invalid memory address or nil pointer dereference in TaskListPartitionMetadata.MarshalToSizedBuffer",
      "number": 320,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:12.720Z"
    },
    {
      "summary": "Statsd container in docker-compose grows to 7-8GB within days when the system is idle, causing excessive disk usage. The issue questions whether statsd should be included in the default development docker-compose setup.",
      "category": "bug",
      "subcategory": "docker-compose",
      "apis": [],
      "components": [
        "docker-compose",
        "statsd",
        "metrics"
      ],
      "concepts": [
        "disk-usage",
        "storage-growth",
        "development-setup",
        "monitoring",
        "metrics-collection",
        "container-configuration"
      ],
      "severity": "medium",
      "userImpact": "Developers using default docker-compose experience unexpected and rapid disk space consumption from statsd, impacting local development environments.",
      "rootCause": "Statsd container accumulates metrics indefinitely without proper cleanup or retention policies, causing unbounded volume growth.",
      "proposedFix": "Remove statsd from default development docker-compose configuration or implement proper metrics retention/cleanup policies.",
      "workaround": "Manually remove statsd from docker-compose files or configure prometheus as an alternative metrics backend.",
      "resolution": "fixed",
      "resolutionDetails": "Statsd was removed from docker-compose-es.yml and addressed in remaining docker-compose files. Users can alternatively configure prometheus endpoint via environment variables.",
      "related": [
        311
      ],
      "keyQuote": "After 4/5 days having left the system IDLE, without doing anything, the graphite/statsd container volume grew to 7/8GB size",
      "number": 319,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:17.027Z"
    },
    {
      "summary": "Gocql client resolves Cassandra hosts only once during session creation, causing cluster unavailability when all Cassandra hosts move simultaneously (e.g., in Kubernetes with persistent volumes). The server cannot reconnect even after hosts come back online.",
      "category": "bug",
      "subcategory": "cassandra-connectivity",
      "apis": [],
      "components": [
        "cassandra-client",
        "gocql",
        "persistence",
        "host-discovery"
      ],
      "concepts": [
        "host-resolution",
        "connection-pooling",
        "kubernetes",
        "failover",
        "reconnection",
        "dynamic-discovery"
      ],
      "severity": "critical",
      "userImpact": "Temporal cluster becomes unavailable when all Cassandra hosts move, requiring manual server restart to reconnect even after Cassandra recovers.",
      "rootCause": "Gocql resolves Cassandra hosts only at session creation time and does not attempt to rediscover hosts when all connections fail, leaving stale IP addresses in the connection pool.",
      "proposedFix": "Implement periodic host rediscovery or integrate gocql's dynamic host discovery mechanism to allow the server to connect back to Cassandra after all hosts move.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved through multiple related PRs (#1342, #1343, #1344, #1345) that implement host rediscovery mechanisms.",
      "related": [
        1342,
        1343,
        1344,
        1345
      ],
      "keyQuote": "Gocql client only resolve hosts once on session creation. This is a problem if you are running Cassandra on kubernetes using persisted volumes as all your Cassandra hosts might move at the same time.",
      "number": 317,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:28:50.926Z"
    },
    {
      "summary": "ElasticSearch version requirement (6.x) needs to be clearly documented. The issue also requests better error handling when this requirement is not met to improve user experience.",
      "category": "docs",
      "subcategory": "dependencies",
      "apis": [],
      "components": [
        "elasticsearch",
        "persistence"
      ],
      "concepts": [
        "version-compatibility",
        "dependencies",
        "error-handling",
        "documentation",
        "user-experience"
      ],
      "severity": "low",
      "userImpact": "Users may encounter confusing errors when using unsupported ElasticSearch versions without clear guidance on version requirements.",
      "rootCause": null,
      "proposedFix": "Document ElasticSearch 6.x requirement and implement clear error handling for version mismatches",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "All dependency versions are now documented",
      "related": [
        313
      ],
      "keyQuote": "The current codebase requires ElasticSearch 6 (we currently test with 6.8.8). Please document this requirement",
      "number": 314,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:28:48.561Z"
    },
    {
      "summary": "Add support for ElasticSearch 7 to replace the current ElasticSearch 6 dependency. Custom search attributes fail to work properly with ES7 because the mapping doesn't include them, preventing searches from functioning correctly.",
      "category": "feature",
      "subcategory": "elasticsearch-upgrade",
      "apis": [],
      "components": [
        "elasticsearch",
        "search-attributes",
        "visibility"
      ],
      "concepts": [
        "elasticsearch-version",
        "custom-search-attributes",
        "mapping",
        "backwards-compatibility",
        "search-functionality"
      ],
      "severity": "high",
      "userImpact": "Users cannot use custom search attributes with ElasticSearch 7 because mappings are not automatically updated, breaking search functionality.",
      "rootCause": "Codebase is hardcoded for ElasticSearch 6; mapping generation does not account for ElasticSearch 7's schema requirements for custom search attributes.",
      "proposedFix": "Update codebase to support ElasticSearch 7 and properly generate mappings for custom search attributes.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by PR #1164 which added ElasticSearch 7 support.",
      "related": [
        1164
      ],
      "keyQuote": "Please upgrade to 7 ASAP, because it causes problems with custom search attributes - they make it to ES, but because mapping does not include them, search does not work.",
      "number": 313,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:28:49.128Z"
    },
    {
      "summary": "The Cassandra Visibility Store orders closed workflow queries by start_time instead of close_time, which is confusing and prevents filtering workflows by close time. The default ordering should be changed to close_time for closed workflows.",
      "category": "bug",
      "subcategory": "visibility-store",
      "apis": [
        "ListClosedWorkflowExecutions"
      ],
      "components": [
        "visibility-store",
        "cassandra",
        "query-engine"
      ],
      "concepts": [
        "ordering",
        "filtering",
        "closed-workflows",
        "close-time",
        "visibility",
        "query-semantics"
      ],
      "severity": "medium",
      "userImpact": "Users cannot effectively query or filter closed workflows by close time, and results are ordered by start time which doesn't match user expectations.",
      "rootCause": "The visibility store implementation uses start_time as the default ordering for all queries, including those on closed workflows, rather than using close_time which is more semantically appropriate.",
      "proposedFix": "Change the default ordering for closed workflow queries to use close_time instead of start_time, and add close_time filtering capability.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The ordering logic was updated to use close_time for closed workflow queries in the Cassandra visibility store.",
      "related": [],
      "keyQuote": "Currently all queries in non ES visibility are ordered by start_time which is very confusing for closed workflows where by close time ordering is expected.",
      "number": 312,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:28:08.758Z"
    },
    {
      "summary": "Long poll intervals are currently hardcoded in the service configuration and cannot be modified, which prevents clients from adjusting these values without breaking due to deadline constraints.",
      "category": "feature",
      "subcategory": "long-polling-configuration",
      "apis": [],
      "components": [
        "service-config",
        "long-polling",
        "client-communication"
      ],
      "concepts": [
        "configuration",
        "polling-interval",
        "client-deadline",
        "service-config",
        "timeout"
      ],
      "severity": "medium",
      "userImpact": "Users cannot customize long poll intervals through service configuration, limiting their ability to optimize for different deadline requirements.",
      "rootCause": null,
      "proposedFix": "Make long poll intervals configurable through the service config to allow clients to adjust values according to their deadline constraints.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Currently long poll intervals are hardcoded and cannot be changed as it is going to break clients which have too short of a deadline.",
      "number": 309,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:28:07.332Z"
    },
    {
      "summary": "Reduce memory consumption of the docker-compose-es development environment, particularly for ElasticSearch and Cassandra containers, to make it feasible to run on developer machines with limited resources.",
      "category": "feature",
      "subcategory": "docker-compose",
      "apis": [],
      "components": [
        "docker-compose",
        "elasticsearch",
        "cassandra"
      ],
      "concepts": [
        "memory-optimization",
        "resource-constraints",
        "development-environment",
        "docker-configuration",
        "performance"
      ],
      "severity": "medium",
      "userImpact": "Developers can run the full Temporal development stack (Cadence+ES+Kibana) on machines with limited RAM without performance degradation.",
      "rootCause": "Default ElasticSearch and Cassandra configurations in docker-compose consume excessive memory for development purposes.",
      "proposedFix": "Tune ElasticSearch and Cassandra container configurations to reduce memory requirements by approximately 50%.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "ElasticSearch memory configuration was optimized in commit 729c8e458ac0fd3bf6d6f1ad509b2e20300e2bd9, with details documented in PR #311.",
      "related": [
        311
      ],
      "keyQuote": "tune the configs for those to get them to consume less RAM (hoping we might be able to cut their RAM utilization in half)",
      "number": 308,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:28:09.839Z"
    },
    {
      "summary": "Request to provide prebuilt binaries for the Temporal service and tctl command-line tool as release artifacts, as an alternative to Docker for users who don't use containerization.",
      "category": "feature",
      "subcategory": "release-distribution",
      "apis": [],
      "components": [
        "release-artifacts",
        "tctl",
        "service-binary"
      ],
      "concepts": [
        "deployment",
        "distribution",
        "binary-release",
        "docker-alternative",
        "installation",
        "accessibility"
      ],
      "severity": "medium",
      "userImpact": "Users who don't use Docker currently lack a convenient way to obtain and run Temporal server and tctl, limiting accessibility of the platform.",
      "rootCause": null,
      "proposedFix": "Include prebuilt binaries for the service and tctl in release artifacts.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Prebuilt binaries were added to release artifacts",
      "related": [],
      "keyQuote": "Not everyone uses docker so having prebuilt binaries for the service and tctl would help.",
      "number": 305,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:27:30.030Z"
    },
    {
      "summary": "Add support for Server TLS/SSL to secure gRPC communication in Temporal. Server TLS and mutual TLS authentication are now supported across all components except Temporal CLI and Temporal Web.",
      "category": "feature",
      "subcategory": "security-tls",
      "apis": [],
      "components": [
        "grpc",
        "server",
        "tls-configuration",
        "certificate-loading"
      ],
      "concepts": [
        "tls",
        "ssl",
        "encryption",
        "certificate",
        "mutual-tls",
        "secure-communication",
        "grpc-security"
      ],
      "severity": "high",
      "userImpact": "Users can now secure Temporal Server gRPC communication using TLS/SSL certificates for encrypted and authenticated connections.",
      "rootCause": null,
      "proposedFix": "Implement TLS certificate loading and configuration across Temporal Server components to enable secure gRPC communication.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Server TLS and Mutual TLS support was implemented across all components except Temporal CLI and Temporal Web, with configuration documented.",
      "related": [],
      "keyQuote": "Server TLS and Mutual TLS is now supported across all components except Temporal CLI and Temporal Web",
      "number": 304,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:27:30.315Z"
    },
    {
      "summary": "tctl's `wf listall --open` command fails to list open workflow executions, even though they are visible in Temporal Web. The issue occurs when querying a running workflow after starting it via the Go sample.",
      "category": "bug",
      "subcategory": "cli-workflow-listing",
      "apis": [],
      "components": [
        "tctl",
        "workflow-visibility",
        "list-open-workflows"
      ],
      "concepts": [
        "workflow-listing",
        "open-executions",
        "visibility",
        "query",
        "cli-tool"
      ],
      "severity": "high",
      "userImpact": "Users cannot view open workflow executions through the CLI tool, limiting observability and forcing reliance on web UI.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "expected: hello world execution is listed in the output actual: hello world is not listed",
      "number": 302,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:27:30.186Z"
    },
    {
      "summary": "The QueryWorkflowBeforeFirstDecision error is incorrectly classified as an InvalidArgument failure when it should be a QueryFailed error, since it's related to workflow state rather than the request itself.",
      "category": "bug",
      "subcategory": "query-workflow",
      "apis": [],
      "components": [
        "history-engine",
        "query-handler"
      ],
      "concepts": [
        "error-classification",
        "query-execution",
        "workflow-state",
        "error-handling"
      ],
      "severity": "medium",
      "userImpact": "Users receive misleading error codes when querying workflows before they've reached their first decision, making it harder to understand the actual problem.",
      "rootCause": "The QueryWorkflowBeforeFirstDecision failure is mapped to InvalidArgument error type, which is semantically incorrect since the error is caused by workflow state, not invalid request arguments.",
      "proposedFix": "Change the error type from InvalidArgument to QueryFailed, possibly with additional information in the QueryFailed structure.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Error classification was corrected to use QueryFailed instead of InvalidArgument for pre-decision queries.",
      "related": [],
      "keyQuote": "This failure is not related to the request itself. So it shouldn't be InvalidArgument.",
      "number": 300,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:26:52.236Z"
    },
    {
      "summary": "Evaluation of whether QueryConsistencyLevel.Eventual should exist as an option. The enum was ultimately removed with default behavior changed to Strong consistency.",
      "category": "other",
      "subcategory": "query-consistency",
      "apis": [
        "QueryConsistencyLevel"
      ],
      "components": [
        "query-engine",
        "consistency-control"
      ],
      "concepts": [
        "consistency-level",
        "query-semantics",
        "api-design"
      ],
      "severity": "low",
      "userImpact": "Users no longer have the option to query with eventual consistency; all queries default to strong consistency.",
      "rootCause": "Design decision to simplify the API by removing the eventual consistency option for queries.",
      "proposedFix": "Remove QueryConsistencyLevel enum and default to Strong consistency.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "QueryConsistencyLevel enum was removed with default changed to Strong consistency.",
      "related": [],
      "keyQuote": "QueryConsistencyLevel enum was removed with default to Strong.",
      "number": 299,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:26:52.724Z"
    },
    {
      "summary": "Request to support running Temporal as an embedded library with only a database dependency, similar to NATS's in-process server model, to enable small-scale on-premises deployments without requiring a separate server process.",
      "category": "feature",
      "subcategory": "server-embedding",
      "apis": [],
      "components": [
        "server",
        "database-backend",
        "connection-management"
      ],
      "concepts": [
        "embedded-deployment",
        "in-process-server",
        "library-mode",
        "on-premises",
        "single-dependency",
        "client-connection"
      ],
      "severity": "medium",
      "userImpact": "Users running small-scale on-premises deployments could run Temporal as an embedded library without needing a separate server process.",
      "rootCause": null,
      "proposedFix": "Implement an embedded server API similar to NATS that allows creating an in-process connection to the server without using loopback network calls, with database as the only external dependency.",
      "workaround": "SQL (MySQL/PostgreSQL/SQLite) can be used as both default and visibility store, though network-based connections are still required. Example embedded configuration exists at https://github.com/abtinf/temporal-a-day/blob/main/001-all-in-one-hello/main.go",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "For small scale on prem deployments having ability to run Temporal as a library which has only DB as a dependency would be really great.",
      "number": 298,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:26:52.723Z"
    },
    {
      "summary": "Community discussion about the future of Uber Cadence in light of the Temporal project's launch. Uber Cadence team provided a statement reaffirming their commitment to the Cadence project's continued development and collaboration with the Temporal open-source community.",
      "category": "other",
      "subcategory": "project-strategy",
      "apis": [],
      "components": [],
      "concepts": [
        "open-source",
        "project-governance",
        "community-collaboration",
        "backward-compatibility"
      ],
      "severity": "low",
      "userImpact": "Clarifies the roadmap and commitment of the Uber Cadence team regarding the future of their workflow orchestration platform.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "invalid",
      "resolutionDetails": "Issue was a discussion question rather than a bug or feature request. Answered by project maintainer with official statement from Uber Cadence team.",
      "related": [],
      "keyQuote": "We want to reinforce that Uber's Cadence team is committed to the growth and open source development of the Cadence project.",
      "number": 296,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:26:13.603Z"
    },
    {
      "summary": "Request to return LongPollExpirationInterval in poll call results so worker code can automatically adjust long poll intervals. Should be included in timeout errors returned by the frontend.",
      "category": "feature",
      "subcategory": "polling",
      "apis": [],
      "components": [
        "worker",
        "frontend",
        "polling"
      ],
      "concepts": [
        "long-poll",
        "timeout",
        "expiration",
        "interval",
        "configuration"
      ],
      "severity": "medium",
      "userImpact": "Workers cannot currently auto-adjust their long poll intervals without manual configuration, leading to potential inefficiencies.",
      "rootCause": null,
      "proposedFix": "Return LongPollExpirationInterval as part of poll call results and include it in timeout error responses from frontend",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This way worker code can adjust automatically for the correct long poll interval.",
      "number": 294,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:26:12.632Z"
    },
    {
      "summary": "Dynamic config property names are inconsistent across the codebasesome use uppercase, others lowercase; some use abbreviations like 'num' and 'max', others use full words. Component prefixes are also inconsistently applied, and alphabetic organization by component would improve maintainability.",
      "category": "other",
      "subcategory": "configuration-naming",
      "apis": [],
      "components": [
        "dynamicconfig",
        "configuration-properties"
      ],
      "concepts": [
        "naming-consistency",
        "code-organization",
        "configuration-management",
        "api-design"
      ],
      "severity": "low",
      "userImpact": "Inconsistent naming makes the configuration API harder to understand and use, requiring developers to remember which properties use abbreviations or uppercase conventions.",
      "rootCause": "No standardized naming convention or organization system established for dynamic config properties in constants.go",
      "proposedFix": "Establish consistent naming conventions (case style, abbreviation usage, component prefixes) and reorganize properties alphabetically by component",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Names of dynamic config properties are not consistent. Some of them start from capital letter, some of them from lower case.",
      "number": 293,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:26:13.932Z"
    },
    {
      "summary": "Activity timeout and retry policy design has fundamental issues: ScheduleToClose timeout conflicts with RetryPolicy.expiration, and activities default to no retries which is counterintuitive. Proposal to redesign timeouts as real intervals, merge retry policy into activity attributes, and make unlimited retries the default with configurable MaximumAttempts.",
      "category": "feature",
      "subcategory": "activity-timeout-retry",
      "apis": [
        "ScheduleActivityTaskDecisionAttributes",
        "RetryPolicy",
        "MaximumAttempts"
      ],
      "components": [
        "activity-executor",
        "retry-policy",
        "timeout-management",
        "decision-attributes"
      ],
      "concepts": [
        "timeout",
        "retry-policy",
        "activity-scheduling",
        "expiration",
        "backoff-strategy",
        "api-design"
      ],
      "severity": "high",
      "userImpact": "Users face confusing and unintuitive activity timeout/retry defaults that require workarounds for most real-world use cases.",
      "rootCause": "Current design conflates ScheduleToClose timeout with RetryPolicy.expiration, and defaults to no retries which contradicts typical user expectations.",
      "proposedFix": "Redesign timeouts as real intervals, merge retry policy into ScheduleActivityTaskDecisionAttributes, remove ExpirationIntervalInSeconds, default MaximumAttempts to 0 for unlimited retries up to ScheduleToClose, and set sensible defaults: InitialRetryInterval=1s, MaximumRetryInterval=90s, ExponentialCoefficient=2.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Design proposal accepted and implemented as part of activity timeout and retry policy redesign.",
      "related": [],
      "keyQuote": "By default activities are not retried which is not intuitive for the new users as well as not desired behaviour in 99.9% of use cases.",
      "number": 290,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:25:35.225Z"
    },
    {
      "summary": "Namespace lookup in the UI and API is case-sensitive, but namespace names are displayed in lowercase. Clicking on a namespace in the UI fails because it constructs URLs with the lowercased name instead of the original case-sensitive name that the service expects.",
      "category": "bug",
      "subcategory": "namespace-lookup",
      "apis": [
        "ListOpenWorkflowExecutions"
      ],
      "components": [
        "ui",
        "namespace-service",
        "api-gateway"
      ],
      "concepts": [
        "case-sensitivity",
        "namespace-resolution",
        "ui-routing",
        "api-consistency",
        "url-construction"
      ],
      "severity": "high",
      "userImpact": "Users cannot access namespaces through the UI if the namespace name contains uppercase letters, receiving internal errors instead.",
      "rootCause": "Namespace names are displayed in lowercase in the UI, but the service performs case-sensitive lookups. The UI constructs URLs using the lowercased display name instead of the original case-sensitive namespace identifier.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by fetching namespace names directly from the temporal service, ensuring case-sensitivity is handled correctly throughout the lookup chain.",
      "related": [],
      "keyQuote": "this is not an issue anymore since the namespace names are actually fetched from the temporal service",
      "number": 289,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:25:33.003Z"
    },
    {
      "summary": "Request to add a Docker Compose example using CockroachDB as the database for Temporal. User wanted to demonstrate running Temporal with CockroachDB, a PostgreSQL-compatible cloud-native database.",
      "category": "feature",
      "subcategory": "docker-compose-examples",
      "apis": [],
      "components": [
        "docker-compose",
        "database-configuration",
        "postgres-driver",
        "cockroachdb-integration"
      ],
      "concepts": [
        "database-compatibility",
        "docker-deployment",
        "high-availability",
        "postgres-compliance",
        "container-orchestration",
        "cloud-native"
      ],
      "severity": "low",
      "userImpact": "Users can now easily run Temporal with CockroachDB using a provided Docker Compose example for cloud-native deployments.",
      "rootCause": null,
      "proposedFix": "Provide a docker-compose-cockroach.yml example file showing how to configure Temporal with CockroachDB as the database backend.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Docker Compose CockroachDB example was created and moved to the docker-compose repository at docker-compose-cockroach.yml",
      "related": [],
      "keyQuote": "I would love to see a docker-compose.yml example using cockroach and/or postgres.",
      "number": 287,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:25:32.753Z"
    },
    {
      "summary": "Feature request to automatically create domains/namespaces if they don't exist when clients connect, reducing manual setup complexity in microservices deployments where re-deploys happen frequently.",
      "category": "feature",
      "subcategory": "namespace-management",
      "apis": [],
      "components": [
        "server",
        "client",
        "namespace-registry"
      ],
      "concepts": [
        "auto-provisioning",
        "namespace-creation",
        "configuration",
        "microservices",
        "deployment-automation"
      ],
      "severity": "low",
      "userImpact": "Users running microservices with frequent re-deploys currently need manual domain registration steps or complex init containers.",
      "rootCause": null,
      "proposedFix": "Add configuration flag to client to automatically create domains/namespaces on first connection if they don't exist.",
      "workaround": "Use docker init container (cadence-init) with domain register command, or manually create domains before deployment.",
      "resolution": "invalid",
      "resolutionDetails": "Commenter clarified that default namespace is already created automatically during service startup via docker/start.sh, addressing the core concern.",
      "related": [],
      "keyQuote": "Default namespace with name `default` is created automatically during service startup",
      "number": 286,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:24:53.715Z"
    },
    {
      "summary": "Make ScheduleToStart timeout optional for activities by defaulting it to the workflow execution timeout. Currently both ScheduleToStart and StartToClose are required when ScheduleToClose is not specified, but defaulting ScheduleToStart would simplify the common case.",
      "category": "feature",
      "subcategory": "activity-timeouts",
      "apis": [],
      "components": [
        "activity-execution",
        "timeout-management",
        "workflow-execution"
      ],
      "concepts": [
        "timeout",
        "scheduling",
        "optional-parameter",
        "workflow-execution",
        "activity-timeout",
        "default-value"
      ],
      "severity": "medium",
      "userImpact": "Users will have a simpler API requiring only one timeout specification instead of two in common use cases.",
      "rootCause": null,
      "proposedFix": "Default ScheduleToStart timeout to the workflow execution timeout when not explicitly specified, making it optional.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented via PR #292",
      "related": [
        292
      ],
      "keyQuote": "For the majority of use cases defaulting ScheduleToStart to the workflow execution timeout is the right thing to do.",
      "number": 280,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:24:53.270Z"
    },
    {
      "summary": "When a single decision batch contains both StartChildWorkflowExecution and RequestCancelExternalWorkflowExecution, the cancellation fails and the child workflow is started instead. The system should immediately cancel the child workflow by writing appropriate cancellation events to history.",
      "category": "bug",
      "subcategory": "child-workflow-cancellation",
      "apis": [
        "StartChildWorkflowExecution",
        "RequestCancelExternalWorkflowExecution"
      ],
      "components": [
        "decision-processor",
        "child-workflow-manager",
        "event-history"
      ],
      "concepts": [
        "workflow-cancellation",
        "decision-batch",
        "event-ordering",
        "immediate-cancellation",
        "external-workflow-request"
      ],
      "severity": "high",
      "userImpact": "Child workflows cannot be immediately cancelled when a parent requests cancellation in the same decision batch, resulting in unexpected workflow execution and broken cancellation semantics.",
      "rootCause": "The decision processor does not properly handle the case where StartChildWorkflowExecution and RequestCancelExternalWorkflowExecution decisions are in the same batch, causing the cancellation request to fail before the child workflow is created.",
      "proposedFix": "Reorder decision processing or add special handling to ensure RequestCancelExternalWorkflowExecution is processed after StartChildWorkflowExecution within the same batch, writing the appropriate cancellation events (RequestCancelExternalWorkflowExecutionInitiated, ExternalWorkflowExecutionCancelRequested, ChildWorkflowExecutionCanceled) to history.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by fixing the decision batch processing logic to properly handle immediate child workflow cancellation requests.",
      "related": [],
      "keyQuote": "Currently when a single decision batch contains StartChildWorkflowExecution and RequestCancelExternalWorkflowExecution the cancellation fails and the child is started.",
      "number": 276,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:24:54.661Z"
    },
    {
      "summary": "Matching service experiences significant performance degradation when exposed to a large number of task lists, with symptoms including memory pressure and database timer operations on UpdateTaskList calls. Performance recovers after process restart.",
      "category": "bug",
      "subcategory": "matching-service",
      "apis": [
        "UpdateTaskList"
      ],
      "components": [
        "matching-service",
        "task-list-manager",
        "database",
        "timer"
      ],
      "concepts": [
        "performance-degradation",
        "resource-leak",
        "scaling",
        "database-pressure",
        "task-list-proliferation"
      ],
      "severity": "high",
      "userImpact": "Users experience severe performance degradation in the matching service when creating many task lists, impacting workflow execution throughput.",
      "rootCause": "Likely resource leak or inefficient database operations related to UpdateTaskList when handling large numbers of task lists",
      "proposedFix": null,
      "workaround": "Process restart temporarily restores normal performance",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Seeing significant degradation in the matching service after being exposed to many task lists. Some timer is operating on the DB because of them.",
      "number": 266,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:24:13.859Z"
    },
    {
      "summary": "Worker fails to process decision tasks with \"requested workflow history does not exist\" error when running the branch sample. The issue occurs during workflow execution with multiple parallel activities, causing decision task completion failure.",
      "category": "bug",
      "subcategory": "workflow-history",
      "apis": [
        "ExecuteActivity"
      ],
      "components": [
        "workflowTaskPoller",
        "taskHandler",
        "historyCache"
      ],
      "concepts": [
        "workflow-history",
        "decision-task",
        "task-processing",
        "event-replay",
        "cache-invalidation",
        "concurrency"
      ],
      "severity": "high",
      "userImpact": "Users running workflows with parallel activities experience consistent decision task failures, preventing workflow execution.",
      "rootCause": "Cached workflow history state becomes stale when task has unexpected events, causing mismatch between cached previous started event ID and actual task state.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed through pull request #262",
      "related": [
        262
      ],
      "keyQuote": "Cached state staled, new task has unexpected events",
      "number": 256,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:24:14.738Z"
    },
    {
      "summary": "Customer requested ability to terminate or cancel a single run of a cron workflow without killing the entire cron job. Currently, terminating/canceling a cron workflow always stops all runs, not just the specified one.",
      "category": "feature",
      "subcategory": "cron-workflows",
      "apis": [
        "TerminateWorkflow",
        "CancelWorkflow"
      ],
      "components": [
        "cron-workflow",
        "workflow-termination",
        "workflow-cancellation"
      ],
      "concepts": [
        "cron",
        "workflow-lifecycle",
        "selective-termination",
        "run-isolation",
        "scheduler-independence"
      ],
      "severity": "medium",
      "userImpact": "Users cannot selectively terminate or cancel individual runs of cron workflows; they must choose between letting the run complete or killing the entire cron scheduler.",
      "rootCause": "Cron workflow termination logic does not differentiate between terminating a specific run versus the entire cron job when a runID is specified.",
      "proposedFix": "Modify termination/cancellation behavior to only target a specific cron run when runID is provided, leaving the cron scheduler intact.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Addressed by the new Schedule feature which separates the scheduler from individual workflow runs, allowing termination of individual runs without stopping the scheduler.",
      "related": [],
      "keyQuote": "This is handled by the new schedule feature. The target workflow is separate from the scheduler. Terminate an individual workflow run won't stop the scheduler.",
      "number": 246,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:24:14.975Z"
    },
    {
      "summary": "Cassandra timestamp type used in the schema can lose precision, potentially causing timers to fire earlier than expected due to rounding errors. The issue raises concerns about data integrity when the database modifies stored values without explicit consent.",
      "category": "bug",
      "subcategory": "persistence-cassandra",
      "apis": [],
      "components": [
        "schema",
        "cassandra",
        "timer",
        "persistence"
      ],
      "concepts": [
        "precision loss",
        "data integrity",
        "timestamp",
        "timer granularity",
        "rounding error",
        "database contract"
      ],
      "severity": "medium",
      "userImpact": "Timers may fire earlier than expected due to Cassandra timestamp precision loss, causing workflows to behave unpredictably.",
      "rootCause": "Cassandra timestamp type has insufficient precision and modifies stored values during persistence, violating the principle that stored data should match retrieved data.",
      "proposedFix": "Change the timestamp column in the execution table to bigint or another data type that preserves precision without loss.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        574
      ],
      "keyQuote": "I get the same data that I stored. the precision issue will open a can of worms",
      "number": 245,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:23:35.992Z"
    },
    {
      "summary": "A test in the matcher component (TestMustOfferRemoteMatch) is flaky, failing intermittently with nil pointer and assertion errors. The test expects specific values for task list names and partition keys but sometimes receives empty strings instead.",
      "category": "bug",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "matcher",
        "task-list",
        "test-suite"
      ],
      "concepts": [
        "flaky-test",
        "race-condition",
        "test-reliability",
        "remote-matching",
        "task-list-routing"
      ],
      "severity": "medium",
      "userImpact": "Flaky tests reduce developer productivity by causing intermittent CI failures and making it difficult to identify real regressions.",
      "rootCause": "Race condition or timing issue in the remote matcher logic causing task list names and partition keys to sometimes be empty instead of populated with expected values.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The flaky test was resolved, likely through fixing the underlying race condition in the matcher component or adding proper synchronization to the test.",
      "related": [],
      "keyQuote": "Expected value not to be nil. Should be true. Not equal: expected: \"/__temporal_sys/tl0/1\" actual: \"\"",
      "number": 243,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:23:34.226Z"
    },
    {
      "summary": "The attempt field in ActivityTaskStarted is confusing because it starts at 0 for the first execution. Should either start at 1 for consistency or be renamed to retryCount for clarity.",
      "category": "bug",
      "subcategory": "activity-execution",
      "apis": [
        "ActivityTaskStarted"
      ],
      "components": [
        "activity-executor",
        "event-processing"
      ],
      "concepts": [
        "attempt",
        "retry",
        "activity-execution",
        "naming-convention",
        "api-semantics"
      ],
      "severity": "low",
      "userImpact": "Developers find the 0-based attempt numbering unintuitive and confusing when implementing activity retry logic.",
      "rootCause": "API design choice to use 0-based indexing for attempt field inconsistent with typical retry counting conventions.",
      "proposedFix": "Either start attempt numbering from 1 instead of 0, or rename the field to retryCount.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It is usually set 0 on the first activity execution and then to 1 and more in case of retries. I find value of 0 for \"attempt\" very confusing.",
      "number": 232,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:23:34.227Z"
    },
    {
      "summary": "CLI command `tctl workflow list -op` crashes with SIGSEGV (nil pointer dereference) in the appendWorkflowExecutionsToTable function. The same command works without the -op option.",
      "category": "bug",
      "subcategory": "cli-workflow-list",
      "apis": [],
      "components": [
        "cli",
        "workflow-commands",
        "tctl"
      ],
      "concepts": [
        "nil-pointer",
        "segmentation-fault",
        "option-parsing",
        "panic"
      ],
      "severity": "high",
      "userImpact": "Users cannot use the `-op` flag with workflow list command without the CLI crashing.",
      "rootCause": "Nil pointer dereference in appendWorkflowExecutionsToTable when processing workflow execution data with the -op option enabled.",
      "proposedFix": null,
      "workaround": "Use `tctl workflow list` without the -op option to list workflows.",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by fixing the nil pointer dereference in the appendWorkflowExecutionsToTable function.",
      "related": [],
      "keyQuote": "panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x522dbf7]",
      "number": 228,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:22:56.049Z"
    },
    {
      "summary": "Dynamic configuration initialization is failing on local dev setups, spamming logs with errors about missing keys. The issue is that development.yaml path doesn't resolve correctly, causing repeated warnings for each ServiceRole.",
      "category": "bug",
      "subcategory": "dynamic-config",
      "apis": [],
      "components": [
        "dynamicconfig",
        "config",
        "service"
      ],
      "concepts": [
        "configuration",
        "initialization",
        "logging",
        "error-handling",
        "local-development"
      ],
      "severity": "medium",
      "userImpact": "Developers experience log spam and configuration failures when setting up local dev environments.",
      "rootCause": "The path referenced by development.yaml for dynamic config YAML does not resolve correctly, and errors are repeated per ServiceRole.",
      "proposedFix": "Update the path reference in development.yaml configuration as mentioned in #234.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Path resolution was updated in #234 to correctly reference the dynamic config YAML file.",
      "related": [
        234
      ],
      "keyQuote": "the path referenced by development.yaml for the dynamic config yaml, did not appear to resolve correctly, this was updated in #234",
      "number": 226,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:22:55.692Z"
    },
    {
      "summary": "The GetWorkflowExecutionHistory RPC should be split into two separate endpoints: one for normal polling and one for long-polling with WaitForNewEvent. This change would improve API clarity and allow clients to set appropriate timeouts for each use case.",
      "category": "feature",
      "subcategory": "history-api",
      "apis": [
        "GetWorkflowExecutionHistory",
        "QueryWorkflow"
      ],
      "components": [
        "workflow-handler",
        "rpc-api",
        "history-service"
      ],
      "concepts": [
        "long-polling",
        "timeout",
        "rpc-design",
        "api-split",
        "event-waiting"
      ],
      "severity": "low",
      "userImpact": "Users would benefit from clearer API semantics and better timeout control for different polling scenarios.",
      "rootCause": null,
      "proposedFix": "Split GetWorkflowExecutionHistory into two separate RPCs: one standard query and one long-poll variant. Set longer timeout for QueryWorkflow.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Deferred as not critical before code complete. No immediate need identified, to be revisited later if necessary.",
      "related": [],
      "keyQuote": "This RPC needs to be splitted into two (long poll and normal) RPC. Client should have appropriate timeouts set.",
      "number": 218,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:22:55.518Z"
    },
    {
      "summary": "Review and standardize JSON serialization across the codebase by replacing native json.Marshal/Unmarshal with proto serialization and JSONPBEncoder. Several components still need updates including archiver, elasticsearch, persistence, mysql, and history modules.",
      "category": "bug",
      "subcategory": "serialization",
      "apis": [],
      "components": [
        "archiver",
        "elasticsearch",
        "persistence",
        "mysql",
        "history"
      ],
      "concepts": [
        "json-serialization",
        "protobuf",
        "encoding",
        "deserialization",
        "marshaling"
      ],
      "severity": "medium",
      "userImpact": "Inconsistent JSON serialization patterns can lead to maintenance issues and potential compatibility problems across different SDK components.",
      "rootCause": "Native json.Marshal/Unmarshal used inconsistently throughout codebase instead of standardized proto serialization and JSONPBEncoder for proto objects.",
      "proposedFix": "Replace json.Marshal/Unmarshal with proto serialization or JSONPBEncoder depending on whether JSON output is required, with specific guidance for archiver, elasticsearch, persistence, mysql, and history modules.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was addressed through systematic replacement of native JSON serialization with proto-based serialization and JSONPBEncoder as recommended in the issue description.",
      "related": [
        198,
        193
      ],
      "keyQuote": "Native `json.Marshal/Unmarshal` should be used only for pure go objects and only when it is necessary.",
      "number": 216,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:22:15.815Z"
    },
    {
      "summary": "Feature request for rate limiting per dynamically created partition keys in task lists. Users need to limit concurrent activity execution based on custom keys (e.g., per customer) without managing client-side workers manually.",
      "category": "feature",
      "subcategory": "task-list-throttling",
      "apis": [],
      "components": [
        "task-list",
        "worker",
        "activity-dispatcher"
      ],
      "concepts": [
        "rate-limiting",
        "partitioning",
        "concurrency-control",
        "throttling",
        "multi-tenancy",
        "dynamic-keys"
      ],
      "severity": "medium",
      "userImpact": "Users can implement rate limiting per customer or other dynamic attributes without complex client-side worker management.",
      "rootCause": null,
      "proposedFix": "Provide a special task list type that supports rate limiting per partition key, with the key supplied during activity scheduling and worker configuration on the server side.",
      "workaround": "Possible to achieve using task list throttling, but requires managing client-side workers dynamically.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "The proposed feature is to provide a special task list type that would support rate limiting per some \"partition key\".",
      "number": 196,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:22:16.062Z"
    },
    {
      "summary": "Request to publish an official Helm Chart for Temporal to enable easy installation, upgrading, and rolling back of Temporal instances. The chart has been created and published, though upgrade and rollback functionality are tracked separately.",
      "category": "feature",
      "subcategory": "deployment-helm",
      "apis": [],
      "components": [
        "helm-chart",
        "deployment",
        "kubernetes"
      ],
      "concepts": [
        "installation",
        "upgrade",
        "rollback",
        "kubernetes",
        "deployment-automation",
        "release-management",
        "infrastructure"
      ],
      "severity": "medium",
      "userImpact": "Users can now install and manage Temporal instances on Kubernetes using standard Helm workflows, reducing deployment complexity.",
      "rootCause": null,
      "proposedFix": "Publish Helm Chart to official repository",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Helm chart published at https://github.com/temporalio/helm-charts; upgrade and rollback scenarios tracked separately",
      "related": [],
      "keyQuote": "We now have a helm chart for installing temporal: https://github.com/temporalio/helm-charts",
      "number": 190,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:22:14.922Z"
    },
    {
      "summary": "Activity retry policy loses important error details when the last retry attempt times out before the expiration deadline. Users need access to the last application error instead of just the timeout error to make informed workflow decisions.",
      "category": "bug",
      "subcategory": "activity-retry",
      "apis": [],
      "components": [
        "activity-executor",
        "retry-policy",
        "timeout-handler"
      ],
      "concepts": [
        "retry",
        "timeout",
        "error-handling",
        "expiration",
        "activity-lifecycle"
      ],
      "severity": "high",
      "userImpact": "Workflows lose critical error context when activities exhaust retries near the expiration deadline, making debugging and error handling difficult.",
      "rootCause": "When scheduling the last retry attempt close to the expiration deadline, the activity times out with a StartToClose timeout error, masking the original application error that could provide useful context.",
      "proposedFix": "Either prevent scheduling retry attempts that would exceed the expiration deadline (if next_retry_time + StartToClose > expiration_time, skip retry), or store and return the last application error instead of the timeout error when retries expire.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [
        2627
      ],
      "keyQuote": "Store last application error and uses the application error when timeout error occur in last attempt.",
      "number": 185,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:21:34.972Z"
    },
    {
      "summary": "Matching service recomputes task expiry times using relative timeouts, causing incorrect metrics in forwarding scenarios. Both task expiry and creation time should use absolute timestamps provided by callers rather than computed locally.",
      "category": "bug",
      "subcategory": "task-expiry-timing",
      "apis": [
        "AddDecisionTask",
        "AddActivityTask"
      ],
      "components": [
        "matching-service",
        "history-service",
        "task-management"
      ],
      "concepts": [
        "timeout",
        "expiry",
        "creation-time",
        "forwarding",
        "metrics",
        "timestamp",
        "latency"
      ],
      "severity": "high",
      "userImpact": "AsyncMatchLatency metrics are incorrect, and task expiry times are recomputed multiple times in forwarding scenarios, leading to inconsistent behavior.",
      "rootCause": "Matching service computes absolute expiry time from relative scheduledToStartTimeout on each invocation, and stamps CreationTime locally instead of receiving it from history.",
      "proposedFix": "Provide absolute timestamps for both task expiry and creation time in AddDecisionTask/AddActivityTask calls from history instead of relative times.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue closed; timestamps changed from relative to absolute in task management calls.",
      "related": [],
      "keyQuote": "Long term, absolute timestamps for both Expiry and Creation should be provided by callers rather than relative times.",
      "number": 181,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:21:34.303Z"
    },
    {
      "summary": "The CLI hangs when attempting to list workflows in a domain that doesn't exist. The command `./cadence -do sample workflow list` blocks indefinitely instead of returning an error when the specified domain is not found.",
      "category": "bug",
      "subcategory": "cli-domain-handling",
      "apis": [],
      "components": [
        "cli",
        "domain-client",
        "workflow-list"
      ],
      "concepts": [
        "error-handling",
        "timeout",
        "domain-validation",
        "blocking-behavior",
        "user-feedback"
      ],
      "severity": "high",
      "userImpact": "Users are unable to work with the CLI when using non-existent domains, as the application hangs without providing feedback.",
      "rootCause": "The CLI does not validate domain existence before attempting to list workflows, and lacks a timeout mechanism to prevent indefinite blocking.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This hangs if domain sample doesn't exist: `./cadence -do sample workflow list`",
      "number": 179,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:21:33.364Z"
    },
    {
      "summary": "Convert all mocks in the repository to be generated on-the-fly using mockgen, similar to the gRPC mock generation process. This involves removing unused mocks and organizing generated mocks in a separate package structure.",
      "category": "other",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "mocks",
        "test-utilities",
        "build-system"
      ],
      "concepts": [
        "mock-generation",
        "code-generation",
        "testing",
        "build-process",
        "package-organization"
      ],
      "severity": "medium",
      "userImpact": "Reduces maintenance burden and build complexity by automating mock generation instead of manually maintaining mocks in the repository.",
      "rootCause": "Manual maintenance of mocks creates build failures when incompatible mocks are left in the codebase, and mockgen cannot regenerate them properly.",
      "proposedFix": "Migrate all mocks to be generated on-the-fly using mockgen in source mode, place generated mocks in a separate package/sub-directory.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "All mocks were converted to mockgen and checked into the repo. Source mode of mockgen only parses single files, so separate package organization was not needed.",
      "related": [],
      "keyQuote": "All mocks are converted to `mockgen` and checked into the repo.",
      "number": 172,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:20:55.441Z"
    },
    {
      "summary": "Decision needed on the default value for EmitMetric attribute in Domain Config. Currently defaults to false, requiring users to explicitly enable metrics for each domain. The issue proposes three options: default to true (not recommended for protobuf), keep false and require explicit opt-in, or rename to DisableMetric to enable by default.",
      "category": "feature",
      "subcategory": "domain-configuration",
      "apis": [],
      "components": [
        "domain-config",
        "metrics",
        "proto-definitions"
      ],
      "concepts": [
        "metrics",
        "domain-registration",
        "default-behavior",
        "configuration",
        "proto-defaults"
      ],
      "severity": "low",
      "userImpact": "Users must explicitly enable metrics for each domain during registration, which could be inconvenient if metrics are desired for most domains by default.",
      "rootCause": null,
      "proposedFix": "Rename EmitMetric attribute to DisableMetric so that metrics are enabled by default with a default value of false, eliminating the need for explicit opt-in.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by implementing option 3 - renaming the attribute to DisableMetric to enable metrics by default.",
      "related": [],
      "keyQuote": "Rename attribute to \"DisableMetric\": Default value of false will cause it to enable metric by default for domains.",
      "number": 162,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:20:55.945Z"
    },
    {
      "summary": "Add validation to prohibit cross-domain calls for global domains, which are currently broken due to domains being active in different data centers. This temporary validation will prevent failures until full cross-domain support for global domains is implemented.",
      "category": "feature",
      "subcategory": "domain-validation",
      "apis": [],
      "components": [
        "history-service",
        "command-checker",
        "domain-validation"
      ],
      "concepts": [
        "cross-domain-calls",
        "global-domains",
        "data-center",
        "validation",
        "domain-isolation"
      ],
      "severity": "high",
      "userImpact": "Users attempting cross-domain calls with global domains will receive validation errors instead of silent failures or broken behavior.",
      "rootCause": "Global domains can be active in different data centers, making cross-domain calls incompatible until full support is implemented.",
      "proposedFix": "Add validation in the command checker to fail any cross-domain call attempts for global domains.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Validation check was implemented in service/history/commandChecker.go at line 641.",
      "related": [],
      "keyQuote": "Cross domain calls for global domains are currently broken as domains can be active in different DCs.",
      "number": 154,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:20:55.655Z"
    },
    {
      "summary": "Cron workflows internally used continue as new to schedule the next run, which conflicted with user workflows that called continue as new explicitly. The proposal is to allow explicit continue as new calls while automatically scheduling the next run only when the workflow completes.",
      "category": "feature",
      "subcategory": "cron-workflows",
      "apis": [
        "ContinueAsNew"
      ],
      "components": [
        "cron-engine",
        "workflow-scheduler",
        "continue-as-new-handler"
      ],
      "concepts": [
        "cron-scheduling",
        "workflow-continuation",
        "automatic-retry",
        "workflow-lifecycle",
        "temporal-scheduling"
      ],
      "severity": "medium",
      "userImpact": "Users can now explicitly call continue as new in cron workflows without conflicts, enabling more flexible scheduling patterns.",
      "rootCause": "Cron workflows were internally using continue as new for scheduling, which prevented users from using continue as new explicitly in their workflow logic.",
      "proposedFix": "Allow explicit continue as new calls and defer next run scheduling until workflow completion instead of using continue as new internally.",
      "workaround": "Trigger an activity at the end of each workflow to create the next cron workflow and kill the previous one at the start of the following workflow.",
      "resolution": "fixed",
      "resolutionDetails": "The cron implementation was changed to not use continueAsNew internally, allowing user workflows to call continue as new explicitly. Next runs are scheduled automatically when the user workflow completes.",
      "related": [],
      "keyQuote": "User workflow could do continue as new and next run is only scheduled when user workflow completed.",
      "number": 148,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:20:17.589Z"
    },
    {
      "summary": "Request to add an Admin CLI command to inspect the DB cluster_membership table, allowing comparison with Ringpop membership information for cluster state inspection.",
      "category": "feature",
      "subcategory": "admin-cli",
      "apis": [],
      "components": [
        "admin-cli",
        "cluster-membership",
        "database"
      ],
      "concepts": [
        "cluster-management",
        "membership",
        "inspection",
        "database",
        "ringpop"
      ],
      "severity": "low",
      "userImpact": "Users can now inspect and compare DB cluster membership state through the admin CLI for better cluster diagnostics.",
      "rootCause": null,
      "proposedFix": "Add `tctl admin membership list_db` command to inspect cluster_membership table",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The tctl admin membership list_db command was implemented to inspect the DB cluster_membership table",
      "related": [
        83
      ],
      "keyQuote": "Admin CLI lets us inspect Ringpop membership, lets also have a way to inspect the DB cluster_membership table and be able to compare.",
      "number": 146,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:20:17.060Z"
    },
    {
      "summary": "An upgrade to gocql library breaks paging behavior in Cassandra queries. The issue was temporarily fixed by reverting to an older version, but a proper investigation is needed to understand the behavioral differences and find a way to use a newer gocql version.",
      "category": "bug",
      "subcategory": "cassandra-paging",
      "apis": [],
      "components": [
        "cassandra",
        "gocql",
        "paging"
      ],
      "concepts": [
        "pagination",
        "database-driver",
        "version-upgrade",
        "backwards-compatibility",
        "cassandra-queries"
      ],
      "severity": "high",
      "userImpact": "Paging queries fail or behave incorrectly when using newer versions of gocql, requiring workarounds and limiting the server's ability to upgrade dependencies.",
      "rootCause": "Behavioral change in gocql v0.0.0-20200203083758-81b8263d9fe5 compared to v0.0.0-20171220143535-56a164ee9f31 affecting paging implementation, details unknown pending investigation.",
      "proposedFix": null,
      "workaround": "Revert gocql to an older version (v0.0.0-20171220143535-56a164ee9f31).",
      "resolution": "fixed",
      "resolutionDetails": "Issue was resolved by reverting gocql to a working version; however, the underlying root cause in the newer gocql version remains unresolved.",
      "related": [
        136
      ],
      "keyQuote": "We're unfortunately using a quite old version of gocql, so I need to investigate further on what the behavioral difference is",
      "number": 140,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:20:17.419Z"
    },
    {
      "summary": "Remove the leftover OneboxService interface used only in onebox workers and replace it with the standard Resource interface used throughout the codebase.",
      "category": "feature",
      "subcategory": "onebox-service",
      "apis": [],
      "components": [
        "onebox",
        "worker",
        "service-interface"
      ],
      "concepts": [
        "interface-standardization",
        "code-cleanup",
        "resource-pattern",
        "service-abstraction"
      ],
      "severity": "low",
      "userImpact": "Improves internal code consistency and maintainability by removing legacy interface patterns.",
      "rootCause": "OneboxService interface exists as legacy code not aligned with standard Resource interface pattern used elsewhere.",
      "proposedFix": "Replace OneboxService interface with Resource interface and remove OneboxService completely.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "OneboxService was replaced with Resource interface and removed from codebase.",
      "related": [],
      "keyQuote": "There is leftover `OneboxService` interface, used by workers in onebox only. It needs to be replaced with `Resource` interface",
      "number": 135,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:19:37.167Z"
    },
    {
      "summary": "Request to support different retry policies based on failure type, allowing intermittent errors to retry immediately while non-transient errors retry with longer intervals or not at all.",
      "category": "feature",
      "subcategory": "retry-policy",
      "apis": [],
      "components": [
        "retry-policy",
        "workflow-retry",
        "activity-retry",
        "error-handling"
      ],
      "concepts": [
        "retry",
        "failure-type",
        "error-classification",
        "backoff",
        "transient-errors",
        "non-retryable-failures"
      ],
      "severity": "medium",
      "userImpact": "Users cannot currently configure different retry behaviors for different failure types, limiting their ability to handle transient vs non-transient errors appropriately.",
      "rootCause": null,
      "proposedFix": "Allow specifying different retry options for different failure types, enabling users to map error types to appropriate retry policies.",
      "workaround": "Users can throw non-retryable application failures or mark failure type strings as not retryable to work around this limitation.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Allow specifying different retry options for different failure types. For example intermittent errors can be retried immediately, but some errors that are not intermittent (like NPE) and require human intervention can be retried with much higher intervals.",
      "number": 131,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:19:37.988Z"
    },
    {
      "summary": "Request to add cron scheduling support for activities, similar to existing workflow cron functionality. Users want the ability to automatically re-execute activities on a schedule or manually schedule the next invocation from within the activity.",
      "category": "feature",
      "subcategory": "cron-scheduling",
      "apis": [
        "StartWorkflow",
        "SignalWorkflow",
        "SignalWithStartWorkflow"
      ],
      "components": [
        "activity-executor",
        "workflow-scheduler",
        "cron-engine"
      ],
      "concepts": [
        "cron-scheduling",
        "periodic-jobs",
        "activity-retry",
        "workflow-continuation",
        "polling"
      ],
      "severity": "medium",
      "userImpact": "Users cannot easily implement periodic activity execution patterns, requiring workarounds for common use cases like polling or scheduled re-execution.",
      "rootCause": null,
      "proposedFix": "Extend cron scheduling to activities, either by: 1) Supporting cron option on activities to re-execute on completion, or 2) Adding a method like Activity.executeAgainIn(Duration) to allow activities to schedule their own re-execution.",
      "workaround": "Use cron workflows to wrap periodic activity needs, though this adds complexity for activity-specific scheduling.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "This would re execute the activity upon its completion. Another option is to support an ability to schedule the next invocation from the activity itself.",
      "number": 130,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:19:39.135Z"
    },
    {
      "summary": "MySQL schema for clusterMembership table has implicit timestamp default behavior that differs between MySQL 5.7 and 8.0, causing deployment failures. The issue requires explicit DEFAULT values for all TIMESTAMP columns to ensure consistent cross-version compatibility.",
      "category": "bug",
      "subcategory": "database-schema",
      "apis": [],
      "components": [
        "database-schema",
        "mysql-persistence",
        "cluster-membership"
      ],
      "concepts": [
        "timestamp",
        "default-values",
        "database-compatibility",
        "schema-migration",
        "mysql-versions"
      ],
      "severity": "high",
      "userImpact": "Users cannot deploy Temporal on MySQL 5.7 or certain MariaDB versions due to schema compatibility issues with implicit timestamp defaults.",
      "rootCause": "MySQL's implicit timestamp default behavior differs between versions - 5.7 has different defaults than 8.0, and some timestamp formats with timezone offsets only work in MySQL 8.0.19+.",
      "proposedFix": "Set explicit DEFAULT '1970-01-02 00:00:01' for all TIMESTAMP columns to avoid timezone adjustment issues and ensure compatibility across MySQL versions.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by setting explicit DEFAULT '1970-01-02 00:00:01' for TIMESTAMP columns to ensure compatibility across MySQL 5.7, 8.0, and MariaDB versions.",
      "related": [
        2210
      ],
      "keyQuote": "Set it to '1970-01-02 00:00:01' so that it's safe against timezone adjustment but doesn't require any particular MySQL release.",
      "number": 119,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:18:59.025Z"
    },
    {
      "summary": "Initial docker-compose startup is slow due to schema creation. Request to ship Docker container with pre-populated schema to speed up developer setup, or explore other optimization approaches.",
      "category": "feature",
      "subcategory": "docker-development-experience",
      "apis": [],
      "components": [
        "docker",
        "database-schema",
        "docker-compose"
      ],
      "concepts": [
        "startup-performance",
        "developer-experience",
        "schema-initialization",
        "containerization",
        "setup-time"
      ],
      "severity": "low",
      "userImpact": "Developers experience slow local development environment setup due to lengthy initial docker-compose initialization.",
      "rootCause": "Database schema must be created during initial docker-compose execution, which is time-consuming.",
      "proposedFix": "Ship Docker container with schema already populated; consider using scratch image with confd for configuration rendering.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "Consider shipping a docker container with a schema already populated to speed up the startup.",
      "number": 109,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:18:58.798Z"
    },
    {
      "summary": "Temporal Docker container emits excessive noisy logs (99.9% useless) without clear service readiness indication. Proposal is to emit startup/ready messages and only fatal errors by default, with optional verbose logging for troubleshooting.",
      "category": "feature",
      "subcategory": "logging-configuration",
      "apis": [],
      "components": [
        "docker",
        "logging",
        "server-startup"
      ],
      "concepts": [
        "log-verbosity",
        "service-readiness",
        "debugging",
        "observability",
        "user-experience"
      ],
      "severity": "medium",
      "userImpact": "Users running Temporal via Docker or temporalite struggle to identify meaningful information amid excessive logs, making it difficult to determine when the service is ready.",
      "rootCause": "Default logging level is set too verbose; JSON debug format by default makes logs hard to parse; no clear service readiness signal.",
      "proposedFix": "Emit 'Temporal Service Starting...' and 'Temporal Service Started' messages, suppress non-fatal logs by default, provide option to enable verbose logging for troubleshooting.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "It's hard on the eyes to parse screen filling JSON messages in real time...",
      "number": 108,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:18:59.261Z"
    },
    {
      "summary": "Add metadata headers to all blob payload fields to support encoding schemes, encryption, compression, and encoding information for interoperability across services and migration scenarios.",
      "category": "feature",
      "subcategory": "payload-encoding",
      "apis": [],
      "components": [
        "payload-codec",
        "data-converter",
        "proto-marshaling",
        "event-attributes"
      ],
      "concepts": [
        "metadata",
        "encoding",
        "interoperability",
        "serialization",
        "encryption",
        "schema-evolution",
        "data-format"
      ],
      "severity": "high",
      "userImpact": "Enables services using Temporal to exchange payloads with different encodings and allows applications to migrate encoding schemes without disrupting running workflows.",
      "rootCause": "Binary payload fields lack metadata about their encoding, compression, and encryption, causing compatibility issues in service mesh deployments and preventing encoding migration.",
      "proposedFix": "Wrap all binary blob fields in a Payload message containing a Header with metadata map and data bytes. Support encoding types (json, raw, proto) with metadata for name, encoding, and other attributes.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented Payload wrapper with PayloadItem structure supporting metadata for encoding, names, and proto serialization across all blob fields and event attributes.",
      "related": [],
      "keyQuote": "This would allow to specify something like: this is an encrypted field with ## version of certificate, gziped, JSON encoded.",
      "number": 107,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:18:20.072Z"
    },
    {
      "summary": "Remove the 30-day maximum retention limit that was inherited from Cadence. This limit is problematic for external users with low event rates who don't use archival, as it forces them to configure retention within an arbitrary constraint.",
      "category": "feature",
      "subcategory": "retention-policy",
      "apis": [],
      "components": [
        "retention",
        "archival",
        "workflow-execution"
      ],
      "concepts": [
        "retention-limit",
        "data-archival",
        "workflow-lifetime",
        "event-rate",
        "storage-policy"
      ],
      "severity": "medium",
      "userImpact": "Users with low event rates and no archival setup are constrained by an arbitrary 30-day retention maximum, limiting their ability to keep workflows active longer.",
      "rootCause": "The 30-day limit was carried over from Cadence and doesn't align with external user needs for workflows with infrequent events.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "The limit was removed via PR #3148",
      "related": [],
      "keyQuote": "Obviously this is not what most external users want if they have low rate and no archival.",
      "number": 103,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:18:19.238Z"
    },
    {
      "summary": "Remove InterruptedException from WorkflowQueue since workflow code never throws it, as CancellationScope should be used for cancellation instead.",
      "category": "feature",
      "subcategory": "workflow-cancellation",
      "apis": [
        "CancellationScope"
      ],
      "components": [
        "workflow-queue",
        "cancellation",
        "exception-handling"
      ],
      "concepts": [
        "cancellation",
        "interruption",
        "scope",
        "exception",
        "workflow-execution"
      ],
      "severity": "low",
      "userImpact": "Removes unnecessary exception type from WorkflowQueue API, simplifying the cancellation model for workflow developers.",
      "rootCause": "InterruptedException is redundant since CancellationScope is the proper mechanism for handling workflow cancellation.",
      "proposedFix": "Remove InterruptedException from WorkflowQueue API.",
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Issue was moved to the Java SDK repository (temporal-java-client#12) for implementation.",
      "related": [
        12
      ],
      "keyQuote": "Workflow code never throws InterruptedException as CancellationScope should be used for cancellation.",
      "number": 92,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:18:18.424Z"
    },
    {
      "summary": "Workflow cannot signal external workflows without providing input, despite this being allowed when signalling from outside. This inconsistency causes BadRequestError when attempting to signal without input.",
      "category": "bug",
      "subcategory": "workflow-signalling",
      "apis": [
        "SignalExternalWorkflow"
      ],
      "components": [
        "workflow-engine",
        "signal-handler",
        "decision-processor"
      ],
      "concepts": [
        "workflow-signal",
        "external-workflow",
        "input-validation",
        "api-consistency"
      ],
      "severity": "medium",
      "userImpact": "Users cannot signal external workflows without input, limiting workflow communication patterns and creating inconsistent behavior between internal and external signalling.",
      "rootCause": "Input validation is incorrectly enforced when signalling external workflows from within a workflow, while the same operation is allowed from external clients.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed by commit 84c85cd00e08d0084b2f18ee632d2ddee7ad66a6 which removed the input requirement for external workflow signals.",
      "related": [],
      "keyQuote": "There is no reason to enforce input when workflow signals an external workflow as it is already allowed when signalling from outside",
      "number": 91,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:17:40.017Z"
    },
    {
      "summary": "Default values for EventId and Version fields were changed to EmptyEventID and EmptyVersion constants, but this is error-prone since new objects require explicit setting. Proposal to change constant values to 0, which would serve as a proper 'unset' indicator for EventIds (which start at 1), pending verification that Versions also start at 1.",
      "category": "bug",
      "subcategory": "proto-schema",
      "apis": [],
      "components": [
        "event-handling",
        "proto-definitions",
        "versioning"
      ],
      "concepts": [
        "default-values",
        "proto-structs",
        "unset-state",
        "error-prone-patterns",
        "schema-design"
      ],
      "severity": "medium",
      "userImpact": "Developers must explicitly set EventId and Version to custom constants on new objects, making code error-prone and inconsistent if forgotten.",
      "rootCause": "Using non-zero constants for 'unset' values in proto structs, rather than leveraging 0 as a natural 'unset' indicator.",
      "proposedFix": "Change EventId and Version constant values to 0, since EventIds start at 1. Verify that Versions also start at 1 throughout the codebase.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Closed by PRs #634 and #642 which implemented the proposed fix.",
      "related": [
        86,
        634,
        642
      ],
      "keyQuote": "Everytime new object with these values is created, they need to be explicitly set to those default value. Otherwise they will be 0 which will be treated as regular value.",
      "number": 90,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:17:39.747Z"
    },
    {
      "summary": "Refactor the Failure representation in APIs to use a generic protobuf structure for error chains instead of language-specific serialization formats. This enables proper error chaining across SDKs implemented in different languages.",
      "category": "feature",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "failure-representation",
        "proto-structures",
        "error-chain",
        "sdk-interoperability"
      ],
      "concepts": [
        "error-serialization",
        "exception-chaining",
        "language-interoperability",
        "stack-trace",
        "protobuf",
        "api-design",
        "cross-language"
      ],
      "severity": "high",
      "userImpact": "Users working with multi-language Temporal workflows can now properly chain and handle errors across SDKs without language-specific serialization.",
      "rootCause": "Current `reason` and `details` fields use language-specific serialization formats (Java, Python, Go) making cross-language error handling impossible.",
      "proposedFix": "Introduce generic protobuf structures: ApplicationFailureInfo with language identifier, Failure with failureInfo oneof, and FailureChain to represent error chains with stacktraces.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Proto changes merged via temporal-proto PR #43, Go SDK implementation in temporal-go-sdk PR #138.",
      "related": [],
      "keyQuote": "errors coming from different components can be chained even if they are from different languages",
      "number": 89,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:17:39.603Z"
    },
    {
      "summary": "Proposal to rename Temporal repositories for clarity: temporal  temporal-server, temporal-go-client  temporal-go-sdk, and temporal-java-client  temporal-java-sdk. Also includes entity renames as referenced in design documentation.",
      "category": "feature",
      "subcategory": "project-organization",
      "apis": [],
      "components": [
        "repository-structure",
        "naming-conventions",
        "sdk-packages"
      ],
      "concepts": [
        "branding",
        "repository-naming",
        "sdk-consolidation",
        "project-structure",
        "developer-experience"
      ],
      "severity": "low",
      "userImpact": "Developers experience improved clarity in repository naming that better reflects the purpose of each repository.",
      "rootCause": null,
      "proposedFix": "Rename repositories to use consistent naming: server repository and SDK repositories explicitly named as such",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Repositories were renamed according to the proposal",
      "related": [],
      "keyQuote": "temporal -> temporal-server, temporal-go-client -> temporal-go-sdk, temporal-java-client -> tempoal-java-sdk",
      "number": 87,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:16:58.814Z"
    },
    {
      "summary": "Reorganize tests into clear categories (pure unit, integration with database/mocks, integration without mocks using onebox, and end-to-end) with dedicated Makefile targets and local run instructions.",
      "category": "feature",
      "subcategory": "test-framework",
      "apis": [],
      "components": [
        "test-suite",
        "makefile",
        "testing-infrastructure"
      ],
      "concepts": [
        "test-categorization",
        "unit-testing",
        "integration-testing",
        "end-to-end-testing",
        "test-isolation",
        "dependency-management",
        "test-execution"
      ],
      "severity": "medium",
      "userImpact": "Enables developers to run targeted test suites with clear understanding of what each category tests and what dependencies are required.",
      "rootCause": null,
      "proposedFix": "Create four test categories with separate Makefile targets: pure unit tests, integration suite 0 (with database/mocks), integration suite 1 (with onebox server), and end-to-end tests (with real temporal-server).",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "We need to categorize all tests: Pure unit tests, Integration tests Suite 0/1, End to end tests",
      "number": 84,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:16:58.636Z"
    },
    {
      "summary": "Redesign cluster membership discovery and host identity management to use the persistence layer instead of Ringpop seed configuration, supporting custom host identities and separate broadcast addresses for NAT'd networking setups.",
      "category": "feature",
      "subcategory": "cluster-membership",
      "apis": [],
      "components": [
        "ringpop",
        "persistence-layer",
        "cluster-metadata",
        "configuration"
      ],
      "concepts": [
        "cluster-discovery",
        "host-identity",
        "network-topology",
        "membership-persistence",
        "heartbeat",
        "nat-networking",
        "bootstrap"
      ],
      "severity": "high",
      "userImpact": "Users running Temporal in NAT'd environments or with custom network topologies can now properly configure host identity and separate broadcast addresses, while cluster membership becomes more reliable through persistence-based discovery.",
      "rootCause": null,
      "proposedFix": "Implement persistence-based cluster membership with cluster_membership table, custom host identity support, separate broadcast address configuration, and heartbeat-based membership tracking with TTL cleanup.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed after implementation of persistence-based cluster membership discovery system with support for custom host identities and separate broadcast addresses.",
      "related": [
        2942
      ],
      "keyQuote": "Support custom host identity in order to support bind on 0.0.0.0 and separate broadcast address vs bind address for nat'd networking setups.",
      "number": 83,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T13:17:00.440Z"
    },
    {
      "summary": "Proposal to use code generators to eliminate boilerplate and hand-maintained code in persistence clients and resource implementations. This would improve development speed and safety by centralizing changes.",
      "category": "feature",
      "subcategory": "code-generation",
      "apis": [],
      "components": [
        "persistence-clients",
        "metrics-client",
        "resource-impl",
        "code-generation"
      ],
      "concepts": [
        "boilerplate",
        "code-generation",
        "DRY-principle",
        "maintainability",
        "duplication",
        "refactoring",
        "code-quality"
      ],
      "severity": "medium",
      "userImpact": "Reduces maintenance burden and potential for inconsistencies in internal codebase infrastructure.",
      "rootCause": "Hand-maintained boilerplate code in multiple locations (persistenceRateLimitedClients, persistedMetricsClient, resourceImpl) creates maintenance overhead and increases risk of divergence.",
      "proposedFix": "Implement code generation for boilerplate code patterns, referencing tools like dupl for identifying candidates.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Decided that refactoring would be cleaner than code generation, as noted by reviewer who questioned whether code gen was the right architectural choice.",
      "related": [],
      "keyQuote": "Using a code generator in some of these places would be very beneficial to future development speed and safety by keeping changes in one place.",
      "number": 76,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:10:00.482Z"
    },
    {
      "summary": "Remove deprecated V1 API definitions from proto files where V2 versions exist. This involves deleting the V1 message/RPC definitions and removing the V2 suffix from the V2 versions to make them the new standard.",
      "category": "feature",
      "subcategory": "api-cleanup",
      "apis": [],
      "components": [
        "proto-files",
        "api-definitions"
      ],
      "concepts": [
        "api-versioning",
        "deprecation",
        "cleanup",
        "backward-compatibility",
        "proto-definitions"
      ],
      "severity": "medium",
      "userImpact": "Simplifies the API surface by removing deprecated V1 definitions, making the SDK cleaner and easier to maintain for users.",
      "rootCause": null,
      "proposedFix": "Go through all proto files, remove V1 message/RPC definitions where V2 exists, then rename V2 definitions by removing the V2 suffix.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "V1 APIs were removed and V2 definitions were renamed to become the standard API definitions.",
      "related": [],
      "keyQuote": "Go through all proto files and for everything that has V2 in name, remove corresponding message/rpc w/o V2 and then remove V2 from name.",
      "number": 75,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:09:59.563Z"
    },
    {
      "summary": "Refactor the Makefile to remove Thrift-related targets after gRPC migration and consolidate remaining targets. Update golint installation to prevent command not found errors during builds.",
      "category": "other",
      "subcategory": "build-system",
      "apis": [],
      "components": [
        "makefile",
        "build-tooling",
        "development-environment"
      ],
      "concepts": [
        "build-configuration",
        "dependency-management",
        "development-workflow",
        "migration",
        "tooling",
        "build-automation"
      ],
      "severity": "low",
      "userImpact": "Developers will have cleaner, more maintainable build configuration and improved build experience without spurious command not found errors.",
      "rootCause": "Post-gRPC migration, Thrift targets are obsolete and should be removed; golint installation is missing causing build spam.",
      "proposedFix": "Remove Thrift targets from Makefile, add golint installation via 'go get -u golang.org/x/lint/golint', remove tools.go file.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, indicating refactoring was completed and Thrift targets were removed.",
      "related": [],
      "keyQuote": "After gRPC migration is done, remove all Thrift targets and arrange all existing ones.",
      "number": 73,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:09:56.553Z"
    },
    {
      "summary": "Reduce persistence layer boilerplate by implementing a SQL query builder for SQL-based databases (MySQL, PostgreSQL) instead of maintaining separate implementations for each database type. This would enable faster feature development and reduce database-specific bugs while maintaining support for Cassandra.",
      "category": "feature",
      "subcategory": "persistence-layer",
      "apis": [],
      "components": [
        "persistence-layer",
        "sql-databases",
        "mysql",
        "postgresql"
      ],
      "concepts": [
        "database-abstraction",
        "code-generation",
        "sql-builder",
        "boilerplate-reduction",
        "feature-velocity"
      ],
      "severity": "high",
      "userImpact": "Users benefit from faster feature development and reduced risk of database-specific bugs as the codebase scales to support more databases.",
      "rootCause": "Multiple custom SQL implementations required for each database type (Cassandra, PostgreSQL, MySQL) create maintenance burden and slow down feature development.",
      "proposedFix": "Adopt a SQL query builder library (such as Masterminds/squirrel, elgris/sqrl, or go-jet/jet) to provide object interfaces similar to C# LINQ/Java streams for SQL operations, potentially with compile-time type safety.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Issue was closed, likely indicating the SQL query builder approach was adopted or decision was made regarding persistence layer architecture.",
      "related": [],
      "keyQuote": "These would give us object interfaces similar to C# linq/Java streams for SQL operations, with some of the libraries providing full compile-time type safety.",
      "number": 72,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T12:20:52.342Z"
    },
    {
      "summary": "Move the canary testing system out of the main server repository to a separate dedicated repository. Currently it resides in the server repo due to legacy reasons and needs to be decoupled.",
      "category": "feature",
      "subcategory": "test-infrastructure",
      "apis": [],
      "components": [
        "canary",
        "test-framework",
        "repository-structure"
      ],
      "concepts": [
        "refactoring",
        "repository-separation",
        "build-organization",
        "maintenance"
      ],
      "severity": "low",
      "userImpact": "Separating the canary into its own repository will improve maintainability and reduce complexity of the main server repository.",
      "rootCause": "Legacy architectural decision to co-locate canary testing with server code",
      "proposedFix": "Create a separate repository for canary and migrate all canary-related code",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Canary was moved to a separate repository as indicated by the closed status and refactoring label",
      "related": [],
      "keyQuote": "Currently it lives in the same server repo due to legacy reason. Needs to be moved out.",
      "number": 69,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T12:20:51.778Z"
    },
    {
      "summary": "Decompose the workflow service gRPC API by extracting replication and metadata operations into separate services (ReplicationService and MetadataService) to improve service organization and reduce coupling.",
      "category": "feature",
      "subcategory": "api-design",
      "apis": [],
      "components": [
        "workflow-service",
        "replication-service",
        "metadata-service",
        "proto-definitions"
      ],
      "concepts": [
        "service-decomposition",
        "api-separation",
        "grpc-architecture",
        "backward-compatibility",
        "domain-management",
        "replication"
      ],
      "severity": "medium",
      "userImpact": "Improves API organization and clarity for server implementers by grouping related operations into cohesive services.",
      "rootCause": "Workflow service contains unrelated RPC functions for replication and metadata that should be in separate services.",
      "proposedFix": "Extract ReplicationService with GetReplicationMessages, GetDomainReplicationMessages, ReapplyEvents; extract MetadataService with domain and search attribute operations.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Completed as a backward-compatible decomposition after Code Complete.",
      "related": [
        1
      ],
      "keyQuote": "Currently workflow service contains RPC funcs which are not related to workflow. My suggestion is to extract replicationservice and metadataservice.",
      "number": 68,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T12:20:51.531Z"
    },
    {
      "summary": "Replace the service locator pattern (Resource and RPCFactory) with proper dependency injection using a compile-time dependency container like Wire or Uber FX to improve testability and maintainability.",
      "category": "feature",
      "subcategory": "architecture",
      "apis": [],
      "components": [
        "resource",
        "rpc-factory",
        "service-locator"
      ],
      "concepts": [
        "dependency-injection",
        "architecture-pattern",
        "testability",
        "maintainability",
        "service-locator",
        "inversion-of-control"
      ],
      "severity": "medium",
      "userImpact": "Better architecture reduces technical debt and improves code quality, making the codebase easier to maintain and test.",
      "rootCause": "Use of service locator pattern (anti-pattern) instead of proper dependency injection throughout the codebase.",
      "proposedFix": "Implement dependency injection using a compile-time dependency container such as Wire (from Google) or Uber FX.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Resolved by implementing dependency injection with Uber FX as noted in final comment.",
      "related": [],
      "keyQuote": "The good alternative is to use dependency injection. It requires dependency container. Wire is a compile time dependency container from Google.",
      "number": 67,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:09:45.030Z"
    },
    {
      "summary": "Temporal needs a flexible, pluggable load balancing mechanism for gRPC frontend connections to replace the previous YARPC dispatcher pattern, supporting round-robin and custom load balancers.",
      "category": "feature",
      "subcategory": "frontend-loadbalancing",
      "apis": [],
      "components": [
        "frontend",
        "grpc-client",
        "connection-pool"
      ],
      "concepts": [
        "load-balancing",
        "grpc",
        "round-robin",
        "service-discovery",
        "dispatcher",
        "pluggable-architecture"
      ],
      "severity": "high",
      "userImpact": "Users need built-in support for different load balancing strategies when connecting to Temporal frontend servers.",
      "rootCause": "Migration from YARPC to gRPC removed the flexible DispatcherProvider mechanism, replacing it with simple host:port connections without load balancing support.",
      "proposedFix": "Implement gRPC load balancer support using service configs with round-robin configuration and dns:/// prefix for addresses, with pluggable architecture for custom load balancers.",
      "workaround": "Use grpc.Dial with WithDefaultServiceConfig and dns:/// prefix: `grpc.Dial(address, grpc.WithInsecure(), grpc.WithDisableServiceConfig(), grpc.WithDefaultServiceConfig(`{\"loadBalancingConfig\": [{\"round_robin\":{}}]}`))",
      "resolution": "fixed",
      "resolutionDetails": "Load balancing support was implemented for gRPC frontend connections with proper service config and round-robin support.",
      "related": [
        60
      ],
      "keyQuote": "gRPC has support for round robin and custom load balancers as well as service configs. These approaches need to be investigated and Temporal should provide flexible plugable mechanism.",
      "number": 62,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:09:45.431Z"
    },
    {
      "summary": "NumHistoryShards configuration value can currently be changed after initial cluster setup, which should not be allowed. The issue proposes persisting this value in a new CLUSTER_METADATA table and enforcing consistency between config and persisted values.",
      "category": "feature",
      "subcategory": "cluster-configuration",
      "apis": [],
      "components": [
        "cluster-metadata",
        "configuration",
        "persistence"
      ],
      "concepts": [
        "immutability",
        "cluster-initialization",
        "configuration-validation",
        "schema-design",
        "database-persistence"
      ],
      "severity": "high",
      "userImpact": "Users could inadvertently misconfigure their cluster by changing NumHistoryShards after initialization, potentially causing system inconsistencies and corruption.",
      "rootCause": "NumHistoryShards lacks persistence mechanism and validation to prevent changes after initial configuration",
      "proposedFix": "Create CLUSTER_METADATA table to persist configuration values; implement validation logic to compare config with persisted values on startup",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Implemented CLUSTER_METADATA table with serialized protobuf storage, added ClusterName configuration validation, mismatch behavior logs error and continues with persisted value, and created Manager/Store interface for cluster metadata management",
      "related": [],
      "keyQuote": "Creating a new table 'cluster_metadata' with data stored as serialized protobuf. No versioning, only one row should exist.",
      "number": 61,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-13T00:09:45.684Z"
    },
    {
      "summary": "Failure reasons are hardcoded as strings with proto enum values, creating a fragile contract between server and client. The string format (e.g., 'cadenceInternal:Timeout TimeoutTypeStartToClose') should be replaced with a proper proto/thrift enum field.",
      "category": "bug",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "failure-reason",
        "error-handling",
        "proto-enum",
        "client-server-contract"
      ],
      "concepts": [
        "timeout",
        "error-parsing",
        "string-serialization",
        "enum-representation",
        "protocol-contract"
      ],
      "severity": "high",
      "userImpact": "Client retry logic depends on fragile string parsing of hardcoded error reasons, making the system brittle when error codes or formats change.",
      "rootCause": "Failure reasons are encoded as strings with hardcoded prefixes and enum values (e.g., 'cadenceInternal:Timeout TimeoutTypeStartToClose') rather than using proper proto/thrift enum fields in the contract.",
      "proposedFix": "Replace hardcoded string failure reasons with proper proto/thrift enum fields, using timeSequence.go as a reference implementation.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Fixed in issue #397 and temporal-go-sdk PR #138 by replacing string-based error reason parsing with structured enum-based approach.",
      "related": [
        397
      ],
      "keyQuote": "This string is a part of contract and it relies on proto/thrift enum representation in string format. There should be a better approach!",
      "number": 51,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T11:30:00.683Z"
    },
    {
      "summary": "Request to add TLS support for MySQL database connections. This enhancement was implemented to support secure database communication for both MySQL and PostgreSQL.",
      "category": "feature",
      "subcategory": "database-security",
      "apis": [],
      "components": [
        "persistence",
        "mysql-driver",
        "database-connection"
      ],
      "concepts": [
        "tls",
        "encryption",
        "database-security",
        "mysql",
        "postgresql",
        "secure-connection"
      ],
      "severity": "medium",
      "userImpact": "Users can now establish encrypted TLS connections to MySQL databases, improving security for Temporal Server deployments.",
      "rootCause": null,
      "proposedFix": null,
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "TLS support was implemented for both MySQL and PostgreSQL as confirmed in the resolution comment.",
      "related": [],
      "keyQuote": "TLS for both mysql & postgresql are supported",
      "number": 49,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T11:29:59.042Z"
    },
    {
      "summary": "Migration from gogo/protobuf to a modern, actively maintained Protocol Buffers library. The team evaluated multiple options including protobuf-go, vtprotobuf, and csproto before ultimately deciding to migrate to the standard Google library.",
      "category": "feature",
      "subcategory": "protobuf-migration",
      "apis": [],
      "components": [
        "protobuf-compiler",
        "serialization",
        "grpc-transport",
        "code-generation"
      ],
      "concepts": [
        "protobuf-runtime",
        "performance-optimization",
        "dependency-management",
        "code-generation",
        "stdlib-compatibility",
        "maintenance-sustainability"
      ],
      "severity": "medium",
      "userImpact": "Users benefit from moving to a well-maintained standard library that ensures long-term stability and performance improvements in Temporal's Protocol Buffer handling.",
      "rootCause": "gogo/protobuf is no longer actively maintained, requiring migration to a supported alternative",
      "proposedFix": "Migrate from gogo/protobuf to google.golang.org/protobuf standard library, with optional optimization using vtprotobuf code generator",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "Migration completed in PR #5032 and followed-up PRs to the standard Google Protocol Buffers library",
      "related": [
        5032
      ],
      "keyQuote": "The best alternative available today for performance-conscious Go programmers is vtprotobuf...It's pretty easy to integrate with grpc-go.",
      "number": 38,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T11:30:00.211Z"
    },
    {
      "summary": "Request to add workers autoscaling capability through KEDA (Kubernetes Event Driven Autoscaling). This involves integrating KEDA as a product integration to enable automatic scaling of Temporal workers based on event-driven metrics.",
      "category": "feature",
      "subcategory": "worker-autoscaling",
      "apis": [],
      "components": [
        "worker",
        "autoscaling",
        "keda-integration"
      ],
      "concepts": [
        "autoscaling",
        "kubernetes",
        "event-driven",
        "resource-management",
        "worker-pool",
        "load-balancing"
      ],
      "severity": "medium",
      "userImpact": "Enables users running Temporal on Kubernetes to automatically scale worker instances based on workload demand, reducing operational overhead and improving cost efficiency.",
      "rootCause": null,
      "proposedFix": "Implement KEDA scaler integration for Temporal workers as a product integration/packaging feature.",
      "workaround": null,
      "resolution": "fixed",
      "resolutionDetails": "KEDA support for Temporal workers was implemented and released in KEDA v2.17.0. A community sample application (temporal-scaling-demo) demonstrates the integration.",
      "related": [
        6191
      ],
      "keyQuote": "OMG it got **released**! https://github.com/kedacore/keda/releases/tag/v2.17.0",
      "number": 33,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T10:40:02.586Z"
    },
    {
      "summary": "Update Temporal Server's error handling to use Go's new error checking methods (errors.Is and errors.As) instead of type assertions and comparisons, following Go's error handling best practices.",
      "category": "other",
      "subcategory": "error-handling",
      "apis": [],
      "components": [
        "error-handling",
        "service",
        "client"
      ],
      "concepts": [
        "error-checking",
        "type-assertions",
        "error-wrapping",
        "go-best-practices"
      ],
      "severity": "low",
      "userImpact": "No direct user impact; improves code maintainability and aligns with Go best practices for error handling.",
      "rootCause": "Server uses type assertions for error checking instead of the newer, more idiomatic Go error handling methods introduced in Go 1.13.",
      "proposedFix": "Replace error type comparisons and assertions with errors.Is and errors.As methods throughout the Server codebase.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "Issue was active for too long without consensus on priority. Decision made to close without implementing the refactoring.",
      "related": [],
      "keyQuote": "this issue is active for too long. We should make a call and decide if anything is needed here. Otherwise let's close the issue.",
      "number": 30,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T10:40:02.414Z"
    },
    {
      "summary": "Payloads need associated metadata to support different serialization formats, compression, and encryption algorithms. Currently, there's no standardized way to specify these properties across different payload types.",
      "category": "feature",
      "subcategory": "payload-encoding",
      "apis": [],
      "components": [
        "payload-serialization",
        "metadata-handling",
        "encoding-framework"
      ],
      "concepts": [
        "metadata",
        "serialization",
        "compression",
        "encryption",
        "encoding",
        "payload-format"
      ],
      "severity": "medium",
      "userImpact": "Users cannot reliably handle payloads with different compression and encryption algorithms without additional metadata context.",
      "rootCause": "Payloads lack a standardized header mechanism to carry encoding and compression metadata that tools need to properly deserialize them.",
      "proposedFix": null,
      "workaround": null,
      "resolution": "duplicate",
      "resolutionDetails": "Issue marked as duplicate, suggesting this functionality was addressed or consolidated with another issue.",
      "related": [],
      "keyQuote": "payloads can be serialized, zipped and encrypted protobuf. So metadata would need to specify compression and encryption algorithm",
      "number": 25,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T10:56:32.817Z"
    },
    {
      "summary": "Investigation into a better approach for handling Docker builds with private repositories. The Dockerfile was updated to support the private temporal repo, but needs a more sustainable solution (multiple Dockerfile versions or Makefile docker target) for when the repo becomes public.",
      "category": "other",
      "subcategory": "docker-build",
      "apis": [],
      "components": [
        "dockerfile",
        "build-system"
      ],
      "concepts": [
        "private-repository",
        "docker-build",
        "build-configuration",
        "deployment"
      ],
      "severity": "low",
      "userImpact": "Developers need a flexible way to build Docker images regardless of repository privacy status without manual Dockerfile modifications.",
      "rootCause": "Dockerfile hardcoded to work with private repo, lacking support for public repo access patterns.",
      "proposedFix": "Consider multiple Dockerfile versions or adding a docker target in Makefile to handle SSH key specification when needed.",
      "workaround": null,
      "resolution": "wontfix",
      "resolutionDetails": "The Dockerfile has been refactored since this issue was opened and the approach is no longer relevant. The repository is now public and the original concern is moot.",
      "related": [
        14
      ],
      "keyQuote": "From what I can tell the Dockerfile has since been refactored and it seems like this might not be relevant anymore.",
      "number": 20,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T10:22:54.199Z"
    },
    {
      "summary": "Add a clear startup message to the Temporal service logs indicating when the service is up and running. Currently, users starting the service via docker-compose lack visual confirmation that initialization is complete.",
      "category": "feature",
      "subcategory": "startup-messaging",
      "apis": [],
      "components": [
        "docker",
        "startup",
        "logging"
      ],
      "concepts": [
        "service-readiness",
        "startup-confirmation",
        "user-experience",
        "docker-compose",
        "log-output"
      ],
      "severity": "low",
      "userImpact": "Users running docker-compose or other startup methods cannot easily determine when the Temporal service has finished initializing and is ready to accept connections.",
      "rootCause": null,
      "proposedFix": "Display a prominent ASCII art banner or clear message in logs on successful service startup, similar to the example provided in the issue.",
      "workaround": null,
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "docker-compose up and other ways to start a service should produce a clear message into the log that indicates that service is up and running",
      "number": 19,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T10:22:53.952Z"
    },
    {
      "summary": "Activities with multiple phases need different timeouts for each phase. Currently, users work around this by creating separate activities and using workflow signals, but this complicates code. The proposal is to model activity execution as a list of phases, each with its own timeout and retry policy.",
      "category": "feature",
      "subcategory": "activity-phases",
      "apis": [
        "CompleteActivityTask",
        "heartbeat"
      ],
      "components": [
        "activity-executor",
        "activity-scheduler",
        "retry-engine",
        "timeout-handler"
      ],
      "concepts": [
        "activity-phases",
        "timeout",
        "retry-policy",
        "heartbeat",
        "activity-completion",
        "workflow-state"
      ],
      "severity": "medium",
      "userImpact": "Users with multi-phase activities must create complex workarounds using multiple activities and signals, significantly complicating workflow code.",
      "rootCause": null,
      "proposedFix": "Two options proposed: (1) Allow specifying phase/timeout/retryPolicy triples when scheduling activities and add phase completion API, or (2) Allow activities to override heartbeat timeout during execution to handle different phase requirements.",
      "workaround": "Create separate activities for each phase and use workflow signals to coordinate state changes between phases.",
      "resolution": null,
      "resolutionDetails": null,
      "related": [],
      "keyQuote": "An activity execution as a list of phases with each phase having its own timeout.",
      "number": 13,
      "repo": "temporalio-temporal",
      "generatedAt": "2026-01-12T10:22:53.915Z"
    }
  ]
}