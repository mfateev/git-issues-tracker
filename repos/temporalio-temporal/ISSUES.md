# temporalio/temporal - Complete Issue Dump

**Generated:** 2025-12-31
**Total Issues:** 530
**Total Upvotes:** 899
**Total Comments:** 865

## Table of Contents

- [Summary](#summary)
- [Top Labels](#top-labels)
- [All Issues](#all-issues)

## Summary

| Metric | Value |
|--------|-------|
| Open Issues | 530 |
| Issues with Upvotes | 143 (27%) |
| Total Upvotes | 899 |
| Total Comments | 865 |

## Top Labels

| Label | Count |
|-------|-------|
| enhancement | 349 |
| potential-bug | 126 |
| up-for-grabs | 30 |
| API | 25 |
| difficulty: easy | 20 |
| operations | 20 |
| devexp | 14 |
| difficulty: medium | 11 |
| refactoring | 11 |
| bug | 11 |
| schedules | 9 |
| config | 7 |
| planning | 7 |
| good first issue | 7 |
| packaging | 6 |

---

## All Issues

Issues are sorted by priority score (upvotes √ó 2 + comments).

---

### #680: Add ability for workflow to wait for completion of an external workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/680 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-08-18 03:48:13.000 UTC (5y 4m ago) |
| **Updated** | 2025-09-02 16:14:48.000 UTC |
| **Upvotes** | 70 |
| **Comments** | 14 |
| **Priority Score** | 154 |
| **Labels** | enhancement, API, devexp, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 70 |

#### Description

Currently, the workflow code can wait for the completion of its child workflow.
It is also possible to wait for the completion of any workflow given its id from an external client.
Ability waiting for the completion of any workflow from a decider code is missing.

Strawman proposal:

Add a WaitForExternalWorkflowCompletion decision and the corresponding event.
Add a WaitForExternalWorkflowCompletionFailed event
Add ExternalWorkflowCompleted/Failed/Cancelled/Terminated/TimedOut events. Another option is to have an
ExternalWorkflowCompleted event that contains the termination event of the external workflow as a field.

Copied from https://github.com/uber/cadence/issues/656

#### Comments (14)

<details>
<summary><strong>thewmo</strong> commented on 2022-02-03 18:06:07.000 UTC</summary>

+1 for this change - it would make it possible to "re-parent" child workflows across continued-as-new parents, which is very useful in batch-like workflows where children take varying amount of time to execute and you don't want to have to wait for all children to complete before moving on to the next chunk in the batch.

</details>

<details>
<summary><strong>shaunco</strong> commented on 2022-07-26 09:40:52.000 UTC</summary>

+1

This would be extremely helpful as the current mechanisms for a workflow to wait for the completion of an external workflow are all quite messy. I really just want my workflow to go to "sleep" until that external workflow completes.

Reactions: üëç 12

</details>

<details>
<summary><strong>akhil-ggv</strong> commented on 2023-08-14 11:29:39.000 UTC</summary>

Is this feature added?

</details>

<details>
<summary><strong>paulnpdev</strong> commented on 2024-01-28 23:52:38.000 UTC</summary>

discussed in-depth internally

</details>

<details>
<summary><strong>dylanturn</strong> commented on 2024-01-30 23:50:44.000 UTC</summary>

> discussed in-depth internally

We wait with bated breath...

Reactions: üòÑ 1

</details>

<details>
<summary><strong>moredure</strong> commented on 2024-03-28 13:14:10.000 UTC</summary>

+1

</details>

<details>
<summary><strong>AdilsonTorres</strong> commented on 2024-04-29 19:27:18.000 UTC</summary>

+1

</details>

<details>
<summary><strong>Sammo98</strong> commented on 2024-05-16 18:09:06.000 UTC</summary>

+1
`It is also possible to wait for the completion of any workflow given its id from an external client.`

Is this actually possible in temporal currently? Or this is something copied from cadence issue?

Reactions: üëç 2 üëÄ 2

</details>

<details>
<summary><strong>binjamil</strong> commented on 2025-01-22 17:48:52.000 UTC</summary>

@mfateev any expected timeline for this feature request? More than 4 years old now for this simple yet very useful utility 

</details>

<details>
<summary><strong>sowmyaseshathri</strong> commented on 2025-03-11 15:33:33.000 UTC</summary>

@mfateev  Any updates on this? This will be good to have 

</details>

<details>
<summary><strong>lukeramsden</strong> commented on 2025-05-26 16:32:44.000 UTC</summary>

There's quite of a lot of advanced state co-ordination in our architecture and just come across this limitation - would help our use case a lot to be able to do this

</details>

<details>
<summary><strong>earodriguez</strong> commented on 2025-05-30 19:51:24.000 UTC</summary>

+1 to being able to "re-parent" a workflow that's executed on its own externally but can also function as a child workflow. In my org we've run into an issue where we have long running workflows that are often called on their own, but those same exact workflows can be "redundantly" called as child workflows as part of a bigger set of activities. 

For example, bigParent can execute child workflows childA, childB, and childC. childA can often be executed by something else before bigParent is called. So if we:

1. execute childA
2. execute bigParent
3. bigParent tries to execute child workflows childA (already running) + childB + childC

We hit an issue where we either have to throw an error or erroneously continue executing bigParent because childA was already running. bigParent is relying on some data created from childA which causes further issues.

The solution we're going to try to run with for now is, in the parent workflow, check for prior executed workflows like childA before executing children and poll their status at a regular interval until its finished as a workaround, but official support for this would be awesome. It would make workflow history clearer + make this more ergonomic.

</details>

<details>
<summary><strong>lukeramsden</strong> commented on 2025-05-31 14:32:23.000 UTC</summary>

The workaround to this is an "infinite activity" that uses the client at the service layer to do this rather than doing it inside the workflow, which means you can use USE_EXISTING

```typescript
result = await proxyActivities<ActivitiesType>({
  // wait forever on this activity
  retry: {
    initialInterval: '100ms',
    maximumInterval: '1s',
    maximumAttempts: Infinity,
  },
  // it heartbeats so we know its alive
  heartbeatTimeout: '5s',
  startToCloseTimeout: '5m',
}).executeChildA(args);
```

```typescript
const activityContext = Context.current();
const heartbeatInterval = setInterval(() => { activityContext.heartbeat({ now: clock.now().toISOString() }); }, 1000);
try {
  return temporalClient.workflow.execute(childA, {
    workflowId: `workflow-id-child-a`,
    args: [args],
    taskQueue: "task queue",
    workflowIdConflictPolicy: 'USE_EXISTING',
    workflowIdReusePolicy: 'ALLOW_DUPLICATE',
  });
} finally {
  clearInterval(heartbeatInterval);
}
```

Reactions: üëç 1

</details>

<details>
<summary><strong>bergundy</strong> commented on 2025-09-02 16:14:48.000 UTC</summary>

The preferred option today is to use a WorkflowRunOperation via Nexus, it will create a workflow that is not parented to the caller workflow but instead more loosely linked. When using `WorkflowIdConflictPolicy` `USE_EXISTING`, if a workflow is already running for the provided ID, a callback will be attached to it and invoked when the workflow finally reaches a terminal state. This includes following the entire run chain across retries and continue-as-new.

There is a limitation of the number of allowed callbacks to be attached to a single workflow, currently 32, configurable via the `system.maxCallbacksPerWorkflow` dynamic config. This limitation will likely be increased in the near future.

We should also allow similar behavior without going through a Nexus handler though.

</details>


---

### #1507: Provide priority task queues

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1507 |
| **State** | OPEN |
| **Author** | oskwazir (Omer Wazir) |
| **Created** | 2021-04-29 18:58:01.000 UTC (4y 8m ago) |
| **Updated** | 2025-12-01 22:31:09.000 UTC |
| **Upvotes** | 50 |
| **Comments** | 27 |
| **Priority Score** | 127 |
| **Labels** | enhancement, API, architecture |
| **Assignees** | stephanos, dnr, bchav, bechols |
| **Milestone** | None |
| **Reactions** | üëç 50 üëÄ 3 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently there is no way to assign priority to a task and also ensure fairness in workers executing tasks. 

Quote from [sjansen ](https://community.temporal.io/t/rate-limiting-based-on-metadata/385/4) on community.temporal.io
>Imagine, for example, a parent workflow that starts an expensive child workflow for each user in the account. If multiple accounts start the parent workflow near the same time, an account with a much larger number of users could monopolize all available workers by simply being first to queue up a large number of activities associated with each user. When there‚Äôs no contention, it‚Äôs desirable for an account to be able to use 100% of available workers but as soon as there‚Äôs contention between accounts it‚Äôs desirable to attempt fair scheduling in order to keep latency proportional to account or workflow size.

> Obviously it would be possible to partition capacity by creating separate queues for each account, but the result is either potentially significant idle capacity or latency waiting for workers to scale out.

**Describe the solution you'd like**
Be able to assign priority to a task, which ensures that the task queue is always ordered based on the highest priority. Fair scheduling of tasks is important too, so that high priority work doesn't consume all workers leaving lower priority work queued. 

**Describe alternatives you've considered**
 Multiple task queues which don't necessarily solve this problem because it allows for idle workers or latency in scaling up workers.

**Additional context**
This feature request stems from a question & discussion on the community website: 
https://community.temporal.io/t/rate-limiting-based-on-metadata/385/


#### Comments (27)

<details>
<summary><strong>wxing1292</strong> commented on 2021-05-13 19:22:37.000 UTC</summary>

BTW, first the there must be consensus on the design 

the feature request will require the following:

SDK: allow workflow logic providing priority of workflow / activity

Server:
* propagate the above priority within history service logic
* pass the above priority to matching service for dispatch
* new priority queue impl
* refactoring matching service for the new priority queue impl
* refactoring matching service's DB schema & impl adding support of priority
* forward / backward compatibility

</details>

<details>
<summary><strong>rainfd</strong> commented on 2022-02-08 08:01:16.000 UTC</summary>

Are there any plans to support priority taskqueue or workflow?

Reactions: üëç 6

</details>

<details>
<summary><strong>PenguinToast</strong> commented on 2022-11-09 21:55:55.000 UTC</summary>

Roughly how much work would this be to implement? We're pretty interested in having this sort of functionality and would be happy to help out with implementation here.

Reactions: üëÄ 1

</details>

<details>
<summary><strong>medihack</strong> commented on 2022-11-10 16:30:02.000 UTC</summary>

Also super interested. We would like to replace a Celery setup (esprecially [Canvas workflows](https://docs.celeryq.dev/en/stable/userguide/canvas.html#canvas-designing-work-flows)) with Temporal, but we make heavy use of [RabbitMQ Message Priorities](https://docs.celeryq.dev/en/stable/userguide/routing.html#rabbitmq-message-priorities) which is not available in Temporal.

Reactions: üëç 5

</details>

<details>
<summary><strong>amajedi-a</strong> commented on 2022-11-29 22:01:02.000 UTC</summary>

I'm exploring a similar concept, something I'm curious about is how being able to assign task priority would unlock fairness in execution?

</details>

<details>
<summary><strong>dnr</strong> commented on 2022-12-01 11:03:32.000 UTC</summary>

Priorities and fairness are definitely features we're looking at as we evolve the matching portion of Temporal. I think it's fair to say that it will be a lot of work to develop and productionize, and it's going to involve the internals in the matching service, so it might be difficult for outside developers to work on. At this point, the most helpful thing might be to describe your requirements in some more detail, so we can keep them in mind as we do the design (since "priority" and "fairness" sometimes mean slightly different things to different people.) E.g. how do you want to specify priorities, with what granularity, do you need reserved capacity for different priorities?

A few more notes:
- From my point of view, priority doesn't "unlock" fairness, but they are two features that affect the order that tasks are dispatched in, and they are often desired to be used together. So it makes some sense to work on both at once, or at least design them together.
- One thing that we're unlikely to ever offer is _guaranteed_ priorities or ordering, because of task queue partitions that need to work independently for scalability.

Reactions: üëÄ 2

</details>

<details>
<summary><strong>medihack</strong> commented on 2022-12-01 12:59:42.000 UTC</summary>

In [our case](https://github.com/radexperts/adit), we are looking for a replacement for [Celery](https://docs.celeryq.dev/en/stable/userguide/routing.html#rabbitmq-message-priorities) which itself uses [RabbitMQ queue priorities](https://www.rabbitmq.com/priority.html). If there is for example only one worker it will always take the task with the highest priority (even if that task is later in the queue than others with lower priority).


</details>

<details>
<summary><strong>zedongh</strong> commented on 2023-01-19 07:12:54.000 UTC</summary>

Is there any work in progress?

</details>

<details>
<summary><strong>medihack</strong> commented on 2023-03-04 00:25:15.000 UTC</summary>

Is it completed? Where can I find more information?

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-03-06 21:26:18.000 UTC</summary>

Sorry, the close/open was a github issues mishap. This is still planned but no ETA yet.

Reactions: üòï 1

</details>

<details>
<summary><strong>tarampampam</strong> commented on 2023-09-25 15:12:35.000 UTC</summary>

Any update on this? Lack of prioritization is not infrequently a reason for abandoning Temporal in my practice...

Reactions: üëç 2

</details>

<details>
<summary><strong>charlesmelby</strong> commented on 2023-10-05 00:34:40.000 UTC</summary>

Dealing with large work spikes from individual clients is common in our systems.
This feature would greatly simplify management and help make a stronger case for adopting Temporal.

Reactions: üëç 3

</details>

<details>
<summary><strong>Multiply</strong> commented on 2023-10-05 06:36:55.000 UTC</summary>

> Dealing with large work spikes from individual clients is common in our systems. This feature would greatly simplify management and help make a stronger case for adopting Temporal.

Can't you run multiple task queues, and manage it by swapping which ones you want to handle?

</details>

<details>
<summary><strong>gmintoco</strong> commented on 2024-06-12 15:03:01.000 UTC</summary>

this would be very useful for us too :) primarily the priority queue idea

</details>

<details>
<summary><strong>darkms</strong> commented on 2024-10-31 09:49:36.000 UTC</summary>

Having priorities and fairness would definitely help to increase Temporal adoption in our company.

Our use case for priority is that we have the same operations used for real-time use cases, such as when user click a button in UI and expect immediate answer, and reused for asynchronous batch processing, but with much larger scale (can make all workers fully utilized for minutes-hours per each batch).
Ideally we'd want to reuse all the same infrastructure, avoid over-provisioning & idle resources, and just have Temporal workers execute items with higher priority before going to lower priority ones (ie have worker activity queue be sorted by priority).
Currently we work-around this by slightly over-provisioning our infra and reserving some capacity for real-time use cases while limiting the activity tasks concurrency for workers. Sadly it is not always enough to process sudden influx of real-time requests, but also when there aren't any real-time requests that leaves the reserved capacity idle, leading to inefficient utilization.

Our use case for fairness is to allocate available service capacity equally for each group (user/tenant/other grouping dimension), but at the same time also we are OK with allowing even a single group to consume all of available capacity if nobody else is using the service at the moment.
So far we haven't found a way to achieve it with Temporal, we're still using our bespoke external scheduler component with centralized database that schedules items 1by1 up to pre-configured amount of concurrently running jobs, I don't really like it much because it's so hard to reuse across services.
Would absolutely love if Temporal offered a similar feature out of the box, maybe it could be in form of job scheduler configuration where we could define dynamic priority formula with ability to lookup # of workflow/activities with same workflow/activity attributes already being executed (at the time of scheduling).

Reactions: üëç 12

</details>

<details>
<summary><strong>atihkin</strong> commented on 2024-11-07 18:19:50.000 UTC</summary>

Thanks @darkms that's really good input. 

The team is actively looking into this feature so we welcome feedback on use cases. 

Reactions: üëç 3 ‚ù§Ô∏è 4

</details>

<details>
<summary><strong>Hubro</strong> commented on 2025-03-03 11:27:26.000 UTC</summary>

I'm using Temporal at my job to process large batches of data that can take multiple days to complete. This creates hundreds or thousands of workflows in Temporal that all "start" at the same time. This works fine, but one big issue is that it can take hours before the first workflow finishes, even though a single workflow only takes a couple of minutes to finish. This is because Temporal spreads the limited workers around to progress on all workflows concurrently. I want Temporal to prioritize *finishing* workflows higher than progressing other workflows.

In my case, there is almost no value in having 1000 workflows with the first 2 activities completed, it would be far more valuable to have 200 workflows completed and 800 unstarted. This could be achieved by prioritizing later activities in a workflow higher than earlier activities, which is why I'm very interested in this feature.

Reactions: üëç 11

</details>

<details>
<summary><strong>millerick</strong> commented on 2025-03-06 02:23:25.000 UTC</summary>

> The team is actively looking into this feature so we welcome feedback on use cases.

Is there any guidance on how far in the future priority queues are?  Asking to know how much time/effort/energy we should invest in crafting our own solution that is robust vs. waiting for one to be provided.

</details>

<details>
<summary><strong>atihkin</strong> commented on 2025-03-06 13:18:12.000 UTC</summary>

Thank you everyone for your patience! Yes, we (@dnr, @stephanos, myself, several others) are actively working on features for task queue level priority and fairness. At a high-level, here's where we're heading directionally. 

We plan to ship features in the order described here starting from "least complex" to the "most complex". Timing-wise, we will start to roll this out mid-year 2025 and incrementally add more features as we go. 

### Key features

**Simple priority**: This is an integer in the range [1, 5], with default 3. Lower numbers are a higher priority. Tasks are dispatched strictly in order i.e. all ‚Äú1‚Äù tasks come before all ‚Äú4‚Äù tasks in a matching backlog.

The intended semantics of priority are to differentiate e.g. ‚Äúinteractive‚Äù/‚Äùreal-time‚Äù vs ‚Äúbatch‚Äù/‚Äùidle‚Äù work. Also, it can be used as an ‚Äúemergency priority‚Äù mechanism for a user to say ‚Äúrun this right now, ignoring everything else‚Äù.

These fixed priorities are mostly exact (exact within one task queue partition). You will also be able to define rate limits per priority-level. 

**Fairness by key and weight**: Fairness is controlled by two values: a short string key (up to 64 bytes) and a positive float weight in [1e-6, 1e6]. The default is an empty string for the key and 1.0 for the weight. 

Fairness is intended for a few different purposes (note that this is a non-exhaustive list and can be used in conjunction):
- In multi-tenant situations, giving each tenant an approximately equal (or weighted unequal) share of resources.
- Defining priority tiers with weighted capacity, e.g. ‚Äúhigh‚Äù/‚Äùmedium‚Äù/‚Äùlow‚Äù with weights 70/20/10, to allow spending more resources on more important work without starving less important. If one class is not present, the other classes can expand to use all capacity.
- Activity type could be used as a fairness key to avoid starving specific activities on a shared task queue.

You will be able to define rate limits per fairness-level. 

Tasks are dispatched such that the ratio of dispatch rates between two keys is approximately equal to the ratio of their weights. i.e. for two keys with the same weight, their tasks should get dispatched at approximately the same rate, no matter how many tasks each of them has in the backlog. This prevents ‚Äúheavy‚Äù keys from disrupting ‚Äúlight‚Äù keys.

There is no limit on cardinality of fairness keys, but if more than a certain number are used, the accuracy may degrade.

Fairness is approximate across both time and space: a) it may take some time for the fairness mechanism to react to a change in distribution of fairness keys, and b) not all decisions would be made the same as if we could use infinite resources for tracking past and future tasks. 

**Ordering i.e. arbitrary priorities**: The ordering key is given by an arbitrary positive integer (64-bit) with no limit on cardinality with default of 1. We attempt to dispatch tasks in order of ordering key (small to large), after taking fairness into account.

Ordering is intended for situations like:

-  Using a continuous value like workflow start time, to order activities of older workflows before activities of newer workflows. This reduces the negative effects of starting many workflows that compete for resources, which could otherwise starve the older ones.
- Similarly, using original activity start time to prioritize activity retries over new activities, or deadline to act as a deadline scheduler.
- Users who want precise control over activity execution order and want to supply an arbitrary scheduling ordering.

_Levels apply in order from priority to fairness to ordering: Priority first, then fairness applies within a priority level, then ordering applies within a fairness key. Priority and ordering are exact (at least within partitions), while fairness may be approximate._

**Please note that these features are in active development, so everything described here is subject to change as we build and iterate. To stay in touch with the latest developments or to provide feedback / ask questions, please join the channel #priority-fairness in Temporal's community [Slack](https://join.slack.com/share/enQtODU0NTI4OTE4NDU2Ny1hZDEyMjBhZTk5MWNkNzFmNTYwNDI1ZGRjNGRhYWZmYWViMWU1NmNlOGUzYTc1MWQ1ZjBjMzIzZjk1OGU0YTZj).**

Reactions: üëç 2 üéâ 32 ‚ù§Ô∏è 35 üöÄ 21 üëÄ 2

</details>

<details>
<summary><strong>JobaDiniz</strong> commented on 2025-03-19 17:21:46.000 UTC</summary>

> I want Temporal to prioritize _finishing_ workflows higher than progressing other workflows.

That's **exactly** our use case!

We don't even need to "start" all workflow **at the same time**. By starting batches of workflows within a timeframe, we want the first ones to finish earlier than the last ones without much contention. Imagine the batches keep coming, eventually we want the later batches to not even start (not the workflow, not the activities) until earlier batches completely finish.

Reactions: üëç 7

</details>

<details>
<summary><strong>kshitij-g</strong> commented on 2025-04-03 20:15:29.000 UTC</summary>

Eagerly waiting for this feature. We have Celery and RabbitMQ implementation which we want to migrate to temporal. Priorities are a must for us otherwise some workflows will never complete. E.g. if activity A within a workflow has returned, then next activity B needs to be finished within 5 mins, otherwise the results of activity A are invalid and activity A itself needs to be retried.

</details>

<details>
<summary><strong>NikhilVerma</strong> commented on 2025-08-20 06:28:34.000 UTC</summary>

We have a heap based slot supplier, i am considering to try and tweak it to adjust slots based on priority.

```ts
import v8 from "node:v8";
import { logger } from "@nonfx/logger";
import {
	type CustomSlotSupplier,
	type SlotInfo,
	type SlotReserveContext,
	type SlotMarkUsedContext,
	type SlotReleaseContext,
	type SlotPermit
} from "@temporalio/worker";
import { uuid4 } from "@temporalio/workflow";

// Enable detailed logging for debugging
const DETAILED_LOGGING = false;

/**
 * A simple heap-based slot supplier that uses a threshold approach.
 * When heap usage exceeds the threshold, it triggers garbage collection
 * and blocks new slot reservations until memory usage drops.
 */
export class HeapBasedSlotSupplier<SI extends SlotInfo> implements CustomSlotSupplier<SI> {
	public readonly type = "custom" as const;

	private readonly options: HeapBasedSlotSupplierOptions;
	private readonly activeSlots = new Set<SlotPermit>();
	private lastHeapCheck = 0;
	private isOverThreshold = false;

	constructor(options: Partial<HeapBasedSlotSupplierOptions> = {}) {
		this.options = {
			heapThreshold: 80,
			checkInterval: 1000,
			enableGC: true,
			minWaitBeforeSlotReservation: 50,
			...options
		};

		logger.info("HeapBasedSlotSupplier initialized", {
			heapThreshold: this.options.heapThreshold,
			checkInterval: this.options.checkInterval,
			enableGC: this.options.enableGC,
			minWaitBeforeSlotReservation: this.options.minWaitBeforeSlotReservation
		});
	}

	/**
	 * Get current heap usage percentage
	 */
	private getHeapUsagePercent(): number {
		const heapStats = v8.getHeapStatistics();
		return (heapStats.used_heap_size / heapStats.heap_size_limit) * 100;
	}

	/**
	 * Check if heap usage is above threshold and trigger GC if needed
	 * @param force - Force check regardless of checkInterval
	 */
	private checkHeapUsage(force = false): void {
		const now = Date.now();

		// Only check periodically to avoid excessive overhead, unless forced
		if (!force && now - this.lastHeapCheck < this.options.checkInterval) {
			return;
		}

		this.lastHeapCheck = now;
		const heapUsagePercent = this.getHeapUsagePercent();

		if (heapUsagePercent > this.options.heapThreshold) {
			if (!this.isOverThreshold) {
				this.isOverThreshold = true;
				logger.warn("Heap usage exceeded threshold", {
					heapUsagePercent: heapUsagePercent.toFixed(2),
					threshold: this.options.heapThreshold,
					activeSlots: this.activeSlots.size
				});
			}

			// Trigger garbage collection if enabled and available
			if (this.options.enableGC && global.gc) {
				try {
					global.gc({
						execution: "sync"
					});
					logger.debug("Garbage collection triggered");
				} catch (error) {
					logger.error("Failed to trigger garbage collection", { error });
				}
			}
		} else {
			if (this.isOverThreshold) {
				this.isOverThreshold = false;
				logger.info("Heap usage back below threshold", {
					heapUsagePercent: heapUsagePercent.toFixed(2),
					threshold: this.options.heapThreshold
				});
			}
		}
	}

	/**
	 * Wait for heap usage to drop below threshold
	 */
	private waitForHeapRecovery(abortSignal: AbortSignal): Promise<void> {
		return new Promise((resolve, reject) => {
			const checkInterval = setInterval(() => {
				if (abortSignal.aborted) {
					clearInterval(checkInterval);
					reject(abortSignal.reason ?? new Error("Slot reservation aborted"));
					return;
				}

				this.checkHeapUsage();

				if (!this.isOverThreshold) {
					clearInterval(checkInterval);
					resolve();
				}
			}, 100); // Check every 100ms when waiting
		});
	}

	/**
	 * Wait a minimum time before slot reservation to allow existing slots to consume memory
	 */
	private waitBeforeSlotReservation(abortSignal: AbortSignal): Promise<void> {
		if (this.options.minWaitBeforeSlotReservation <= 0) {
			return Promise.resolve();
		}

		return new Promise((resolve, reject) => {
			const timeout = setTimeout(() => {
				resolve();
			}, this.options.minWaitBeforeSlotReservation);

			// Handle abort signal
			const handleAbort = () => {
				clearTimeout(timeout);
				reject(abortSignal.reason ?? new Error("Slot reservation aborted"));
			};

			if (abortSignal.aborted) {
				clearTimeout(timeout);
				reject(abortSignal.reason ?? new Error("Slot reservation aborted"));
				return;
			}

			abortSignal.addEventListener("abort", handleAbort, { once: true });
		});
	}

	async reserveSlot(ctx: SlotReserveContext, abortSignal: AbortSignal): Promise<SlotPermit> {
		// Check if already aborted
		abortSignal.throwIfAborted();

		// Wait minimum time before attempting reservation to let existing slots consume memory
		await this.waitBeforeSlotReservation(abortSignal);

		// Block until we have capacity and heap is below threshold

		while (true) {
			abortSignal.throwIfAborted();

			// Always check current heap usage before proceeding (force check)
			this.checkHeapUsage(true);

			// Wait for heap recovery if needed
			if (this.isOverThreshold) {
				await this.waitForHeapRecovery(abortSignal);
				continue; // Re-check everything after heap recovery
			}

			break;
		}

		// Final abort check before creating permit
		abortSignal.throwIfAborted();

		// Create and track the permit
		const permit: SlotPermit = uuid4();

		this.activeSlots.add(permit);

		if (DETAILED_LOGGING) {
			logger.debug("Slot reserved", {
				slotType: ctx.slotType,
				taskQueue: ctx.taskQueue,
				activeSlots: this.activeSlots.size,
				heapUsagePercent: this.getHeapUsagePercent().toFixed(2)
			});
		}

		return permit;
	}

	tryReserveSlot(ctx: SlotReserveContext): SlotPermit | null {
		// For tryReserveSlot, we can't wait async, but we can check if we should be more conservative
		// by considering how recently we handed out slots
		const timeSinceLastCheck = Date.now() - this.lastHeapCheck;
		if (timeSinceLastCheck < this.options.minWaitBeforeSlotReservation) {
			// Be more conservative and reject if we haven't waited long enough
			return null;
		}

		// Always check current heap usage before proceeding (force check)
		this.checkHeapUsage(true);

		// If heap usage is above threshold, reject reservation
		if (this.isOverThreshold) {
			return null;
		}

		// Create and track the permit
		const permit: SlotPermit = uuid4();

		this.activeSlots.add(permit);

		if (DETAILED_LOGGING) {
			logger.debug("Slot eagerly reserved", {
				slotType: ctx.slotType,
				taskQueue: ctx.taskQueue,
				activeSlots: this.activeSlots.size,
				heapUsagePercent: this.getHeapUsagePercent().toFixed(2)
			});
		}

		return permit;
	}

	markSlotUsed(ctx: SlotMarkUsedContext<SI>): void {
		if (DETAILED_LOGGING) {
			logger.debug("Slot marked as used", {
				slotInfo: ctx.slotInfo,
				activeSlots: this.activeSlots.size
			});
		}
	}

	releaseSlot(ctx: SlotReleaseContext<SI>): void {
		if (this.activeSlots.has(ctx.permit)) {
			this.activeSlots.delete(ctx.permit);

			if (DETAILED_LOGGING) {
				logger.debug("Slot released", {
					slotInfo: ctx.slotInfo,
					activeSlots: this.activeSlots.size,
					heapUsagePercent: this.getHeapUsagePercent().toFixed(2)
				});
			}
		}
	}

	/**
	 * Get current statistics
	 */
	getStats() {
		return {
			activeSlots: this.activeSlots.size,
			heapUsagePercent: this.getHeapUsagePercent(),
			isOverThreshold: this.isOverThreshold,
			options: this.options
		};
	}
}

export type HeapBasedSlotSupplierOptions = {
	/**
	 * Heap usage threshold as a percentage (0-100).
	 * When heap usage exceeds this threshold, garbage collection will be triggered.
	 * Default: 80
	 */
	heapThreshold: number;

	/**
	 * How often to check heap usage (in milliseconds).
	 * Default: 1000
	 */
	checkInterval: number;

	/**
	 * Whether to enable garbage collection when threshold is exceeded.
	 * Default: true
	 */
	enableGC: boolean;

	/**
	 * Minimum wait time before slot reservation (in milliseconds).
	 * This allows existing slots to consume memory before handing out more slots.
	 * Set to 0 to disable. Default: 50
	 */
	minWaitBeforeSlotReservation: number;
};
```

</details>

<details>
<summary><strong>liran</strong> commented on 2025-08-25 18:49:00.000 UTC</summary>

Simple priority for task queues - pre-release https://github.com/temporalio/temporal/releases/tag/v1.28.0

Reactions: üöÄ 3

</details>

<details>
<summary><strong>JobaDiniz</strong> commented on 2025-08-26 15:03:29.000 UTC</summary>

> Simple priority for task queues - pre-release https://github.com/temporalio/temporal/releases/tag/v1.28.0

### Use case clarification: prioritizing active workflows over starting new ones

We run a backlog-driven scenario with a limited pool of workers (and limited slots) processing a large number of workflows. All workflows are of the same type, and they all tend to ‚Äústart‚Äù around the same time, quickly filling every worker.

**The result**: workflows that began early stagnate because their subsequent tasks or activities are queued behind many others, preventing completion. For example, Workflow A may already be partway done, but once workers are saturated, its remaining tasks get stuck in the same backlog as entirely new workflows. Temporal continues pulling in new workflows, leaving us with hundreds or thousands that are all partially completed rather than a smaller set that are fully done.

**Desired behavior**: Once a workflow has started (executed some activities), it should be prioritized to finish before beginning new workflows from the backlog. In practice, it‚Äôs far better to end up with 200 completed workflows and 800 untouched than 1000 half-finished.

### Does v1.28.0 address this?
Unfortunately, no. The new **Task Queue Priority** mechanism allows assigning priority values within a single queue for activities, but since all our workflows are of the same type, there‚Äôs no meaningful way to differentiate them by priority. The mechanism doesn‚Äôt ensure that in-progress workflows get worker time over brand-new ones.

### Roadmap
Is there anything planned to support prioritizing _already-started_ workflows over new ones when worker capacity is limited?

Reactions: üëç 3

</details>

<details>
<summary><strong>fabianoengler</strong> commented on 2025-09-04 22:02:47.000 UTC</summary>

> **The result**: workflows that began early stagnate because their subsequent tasks or activities are queued behind many others, preventing completion. (...)
> 
> **Desired behavior**: Once a workflow has started (executed some activities), it should be prioritized to finish before beginning new workflows from the backlog. In practice, it‚Äôs far better to end up with 200 completed workflows and 800 untouched than 1000 half-finished.
> ### Does v1.28.0 address this?
> 
> Unfortunately, no. 

We have a similar use case. For now we are testing a concept of throttling the start of new workflows by having our own queue and only starting workflows on temporal if there is capacity, if there is not we keep waiting for workflows to finish, some sort of sliding window, but of course this a somewhat poor workaround, it's hard to measure capacity correctly because even thought the workflows and activities are of the same type, the data they process varies a lot in size and on the load the impose.

</details>

<details>
<summary><strong>hckhanh</strong> commented on 2025-09-09 01:43:42.000 UTC</summary>

FYI: https://youtu.be/Cf6_PBoyxbk?si=H43wXaZUd1qX6_lt

Reactions: üëç 2 ‚ù§Ô∏è 3

</details>

<details>
<summary><strong>testingmodels43-beep</strong> commented on 2025-09-29 16:11:45.000 UTC</summary>

is fairness supported in OSS version 1.28.1? 
I am using the official compose setup for temporal and even with the dynamic config etc enabled its not working like the demo

Reactions: üëç 1

</details>


---

### #2668: Document POSTGRES_SEEDS (or better yet, use a descriptive var name)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2668 |
| **State** | OPEN |
| **Author** | colelawrence (Cole Lawrence) |
| **Created** | 2022-03-29 13:20:48.000 UTC (3y 9m ago) |
| **Updated** | 2025-04-29 13:26:38.000 UTC |
| **Upvotes** | 46 |
| **Comments** | 11 |
| **Priority Score** | 103 |
| **Labels** | enhancement, config |
| **Assignees** | jbreiding |
| **Milestone** | None |
| **Reactions** | üëç 46 üòï 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
I have been very frustrated over the last two hours, because I haven't seen any documentation for how to actually customize the connection for my postgresql database.

I have finally realized after searching this codebase for `POSTGRES_SEEDS` that it is actually being used as "postgres connection host".

**Describe the solution you'd like**

I would strongly recommend rethinking the consistency of these environment variables, as it has lead me and to a great deal of confusion, or documenting all the docker-compose files with descriptions.

#### Comments (11)

<details>
<summary><strong>jbreiding</strong> commented on 2022-04-11 15:40:27.000 UTC</summary>

Thanks, we are going to track this under a larger project to reconcile all the different ways temporal can be configured to a simple, and documented path.

</details>

<details>
<summary><strong>gajus</strong> commented on 2023-01-19 19:49:42.000 UTC</summary>

Unbelievable. @colelawrence This prob saved me hours of search ü§¶ 

Reactions: üëç 10

</details>

<details>
<summary><strong>fowlmouth</strong> commented on 2023-08-08 15:53:08.000 UTC</summary>

In 2023 the best source for info about POSTGRES_SEEDS is still searching through the issues in github

Reactions: üéâ 2 üòï 7

</details>

<details>
<summary><strong>happyxhw</strong> commented on 2023-08-22 08:15:54.000 UTC</summary>

In 2023 the best source for info about POSTGRES_SEEDS is still searching through the issues in github

Reactions: üòÑ 1

</details>

<details>
<summary><strong>MysticalMount</strong> commented on 2024-01-07 13:51:09.000 UTC</summary>

Ditto this, just realised myself but its not obvious at all

</details>

<details>
<summary><strong>jxc876</strong> commented on 2024-02-07 17:43:20.000 UTC</summary>

I ran into the same thing in 2024,

I was super confused by `POSTGRES_SEEDS` I assumed it was some type of Seed for randomness.

I had to look through source code of several projects to find this:

```yaml
        default:
            sql:
                pluginName: "{{ $db }}"
                databaseName: "{{ default .Env.DBNAME "temporal" }}"
                connectAddr: "{{ default .Env.POSTGRES_SEEDS "" }}:{{ default .Env.DB_PORT "5432" }}"
                
        visibility:
            sql:
                {{ $visibility_seeds_default := default .Env.POSTGRES_SEEDS "" }}
                {{ $visibility_seeds := default .Env.VISIBILITY_POSTGRES_SEEDS $visibility_seeds_default }}

                databaseName: "{{ default .Env.VISIBILITY_DBNAME "temporal_visibility" }}"
                connectAddr: "{{ $visibility_seeds }}:{{ $visibility_port }}"
```

* https://github.com/temporalio/temporal/blob/main/docker/config_template.yaml#L102

</details>

<details>
<summary><strong>MysticalMount</strong> commented on 2024-02-07 17:49:39.000 UTC</summary>

The Helm chart does reveal it but you have to dig into the templates for the configmap

</details>

<details>
<summary><strong>KhanMechAI</strong> commented on 2025-01-06 03:07:07.000 UTC</summary>

2025 and we're still looking here

Reactions: üöÄ 7

</details>

<details>
<summary><strong>bilal-fazlani</strong> commented on 2025-04-01 10:10:16.000 UTC</summary>

April 2025 :(

</details>

<details>
<summary><strong>IzioDev</strong> commented on 2025-04-29 04:46:39.000 UTC</summary>

I was here in 2025, and i felt desperate.

</details>

<details>
<summary><strong>gajus</strong> commented on 2025-04-29 13:26:36.000 UTC</summary>

If anything, this thread is very motivating.

https://temporal.io/blog/temporal-series-c-announcement

If they can do it, so can you. üí™ 

Reactions: üòÑ 2

</details>


---

### #2318: MongoDB as Persistent

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2318 |
| **State** | OPEN |
| **Author** | milanof-huma (Milano Fili) |
| **Created** | 2021-12-21 13:47:23.000 UTC (4 years ago) |
| **Updated** | 2025-12-28 12:21:05.000 UTC |
| **Upvotes** | 46 |
| **Comments** | 7 |
| **Priority Score** | 99 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 46 ‚ù§Ô∏è 5 üöÄ 5 |

#### Description

**Is your feature request related to a problem? Please describe.**
I'm working in a company that heavily uses MongoDB and MongoDB Atlas. So it makes sense for us to run everything through MongoDB as our primary data store specially when it comes to backup / restore, you don't want to have multiple databases.

**Describe the solution you'd like**
I would say, there is support for casandra and sql databases, it shows there should be some way ( maybe easy way ) to use Atlas as persistent layer.

**Describe alternatives you've considered**
We are already using python celery in other part of our business services, but we are looking for alternatives and this can help us to move forward with evaluations.

**Additional context**
Also, we are happy to invest into the project by providing development capacity for this task. Let me know if that could be an option to consider.


#### Comments (7)

<details>
<summary><strong>StanislavPrusac</strong> commented on 2023-09-26 16:01:39.000 UTC</summary>

+1 for MongoDB as Persistent! <3 

Reactions: üëç 4

</details>

<details>
<summary><strong>ajzawawi</strong> commented on 2024-03-27 02:17:06.000 UTC</summary>

+1 for mongodb. Is this open for contribution? 

Reactions: üëç 4

</details>

<details>
<summary><strong>AbhijithHarness</strong> commented on 2024-04-23 19:02:35.000 UTC</summary>

+1 for mongoDB.

Reactions: üëç 4

</details>

<details>
<summary><strong>algobot76</strong> commented on 2024-07-19 08:28:51.000 UTC</summary>

any updates?

Reactions: üëç 4

</details>

<details>
<summary><strong>leov2000</strong> commented on 2025-02-19 02:51:49.000 UTC</summary>

Any updates on this? 

</details>

<details>
<summary><strong>Dhruv-Acharya</strong> commented on 2025-03-02 03:41:04.000 UTC</summary>

Can contribute as well. üëçüèº 

</details>

<details>
<summary><strong>sagdelen</strong> commented on 2025-12-28 12:21:05.000 UTC</summary>

Hi there, I have completed the mongodb persistence implementation. You can check the implementation details from #8908 

</details>


---

### #5680: Official support for Opensearch

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5680 |
| **State** | OPEN |
| **Author** | jsecchiero (Jacopo Secchiero) |
| **Created** | 2024-04-08 12:11:43.000 UTC (1y 8m ago) |
| **Updated** | 2025-10-07 12:15:29.000 UTC |
| **Upvotes** | 30 |
| **Comments** | 14 |
| **Priority Score** | 74 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 30 |

#### Description

As new versions are released, the features of OpenSearch and Elasticsearch will continue to diverge ([ref1](https://opensearch.org/faq/#q1.14) [ref2](https://nextbrick.com/opensearch-vs-elasticsearch-check-everything-before-you-choose/)). However, i'm wondering if there will be an official statement in the documentation about Temporal's support for OpenSearch in the future. 
Currently, only recent versions [Elasticsearch are supported](https://docs.temporal.io/self-hosted-guide/visibility#elasticsearch). This information will be helpful when deciding on the datastore to use with Temporal

#### Comments (14)

<details>
<summary><strong>yiminc</strong> commented on 2024-05-16 04:43:49.000 UTC</summary>

It is not officially claimed as supported, but there is no issue using OpenSearch as visibility store. 

</details>

<details>
<summary><strong>mike-wilson-wellhive</strong> commented on 2024-05-30 14:49:44.000 UTC</summary>

There may be no issue using OpenSearch as the visibility store _today_, but some Temporal consumers need at least some assurance that the integration will continue working in the future, before they decide to start using it.

I would feel comfortable using OpenSearch as my visibility store for production workloads if the Temporal team formally supported it by doing things like:
- Document OpenSearch-specific guidance and compatible versions of OpenSearch and Temporal. A section similar to this one would be great: https://docs.temporal.io/self-hosted-guide/visibility#elasticsearch.
- Regular testing of the integration with the documented compatible version pairs. (Whatever the team does to ensure postrgres, mysql, and elasticseach integrations continue to work)

Reactions: üëç 7

</details>

<details>
<summary><strong>ninjaMikeG</strong> commented on 2024-10-14 14:54:03.000 UTC</summary>

Latest versions of opensearch and ES have diverged - the response type header has changed so ES client libraries are looking for the new content-type and not recognizing Opensearch responses. With 1.25.1, I am not able to connect to my Opensearch instance after an upgrade from 1.22 (going through intermediate releases)

`Content-Type header [application/vnd.elasticsearch+json; compatible-with=8]`

Related issue where versions noted: https://github.com/elastic/elasticsearch-py/issues/1933 


Reactions: üëç 4

</details>

<details>
<summary><strong>Hronom</strong> commented on 2024-12-02 02:00:33.000 UTC</summary>

When it will be supported? For example in AWS you can't deploy Elasticsearch anymore, only OpenSearch. So you in scope of self-hosted you need to bother with Elastic Cloud.

</details>

<details>
<summary><strong>jc2707</strong> commented on 2025-04-09 23:29:45.000 UTC</summary>

Can someone from AWS speak to this? Are you working with Temporal folks for support for latest OS versions? Or if temporal has it roadmapped already.... 

</details>

<details>
<summary><strong>VLZZZ</strong> commented on 2025-05-22 11:48:47.000 UTC</summary>

We're using Temporal with AWS Opensearch for a while already. I understand it's not supported officially, but I believe the only thing we need to change to support the Opensearch is to enable backward compatibility

```
PUT _cluster/settings
{
    "persistent" : {
        "compatibility.override_main_response_version" : true
    }
}
```

But we're still looking forward to dropping this workaround as soon as it will be supported

Reactions: üëç 3

</details>

<details>
<summary><strong>samof76</strong> commented on 2025-06-10 16:14:51.000 UTC</summary>

@VLZZZ which version of Opensearch are you using?

</details>

<details>
<summary><strong>matlegit</strong> commented on 2025-06-16 18:31:03.000 UTC</summary>

> We're using Temporal with AWS Opensearch for a while already. I understand it's not supported officially, but I believe the only thing we need to change to support the Opensearch is to enable backward compatibility
> 
> ```
> PUT _cluster/settings
> {
>     "persistent" : {
>         "compatibility.override_main_response_version" : true
>     }
> }
> ```
> 
> But we're still looking forward to dropping this workaround as soon as it will be supported

@VLZZZ Thank you for sharing your workaround. What version of Temporal and OpenSearch are you using? Any issues with OpenSearch 2.19? Thanks!

</details>

<details>
<summary><strong>damar-block</strong> commented on 2025-07-29 18:33:10.000 UTC</summary>

Anyone using AWS managed OpenSearch? It seems not support the `PUT _cluster/settings` setting mentioned above?

</details>

<details>
<summary><strong>jc2707</strong> commented on 2025-07-29 20:18:37.000 UTC</summary>

https://docs.opensearch.org/latest/api-reference/cluster-api/cluster-settings/ 
Very much supported.

Reactions: üëç 1

</details>

<details>
<summary><strong>VLZZZ</strong> commented on 2025-07-30 11:24:11.000 UTC</summary>

@samof76 @matlegit 
I can confirm no issues with Temporal `1.25.2` and AWS managed OpenSearch `2.13` historically.
Not sure about `2.19` so far, but I see no blockers so far

</details>

<details>
<summary><strong>alicancakil</strong> commented on 2025-09-09 21:57:46.000 UTC</summary>

I'd love to see official support for opensearch as well.

I am currently on Opensearch `2.13` with temporal `v1.27.2` and things are running perfectly fine.

@VLZZZ have you tested opensearch `2.19` by any chance?

Reactions: üëé 1

</details>

<details>
<summary><strong>yashdjoshi</strong> commented on 2025-09-29 03:26:54.000 UTC</summary>

I tried it for the 2.19 but Temporal's Elasticsearch client couldn't connect to AWS OpenSearch 2.19, getting stuck on: "Waiting for Elasticsearch to start up...". 

I did have this setting set as well:
"compatibility.override_main_response_version" : true

I think the 2.19 apis might have diverged more.

</details>

<details>
<summary><strong>karbovsky</strong> commented on 2025-10-07 12:15:29.000 UTC</summary>

When should we expect Opensearch support to be implemented?

</details>


---

### #537: Add SignalWithStart\UpdateWithStart command

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/537 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-07-13 18:32:33.000 UTC (5y 5m ago) |
| **Updated** | 2025-09-22 22:44:44.000 UTC |
| **Upvotes** | 30 |
| **Comments** | 2 |
| **Priority Score** | 62 |
| **Labels** | enhancement, API, difficulty: easy, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 30 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, SignalWithStart is available only through external gRPC API. There are multiple use cases when it is needed from a workflow.

**Describe the solution you'd like**
Add SignalWithStartChildWorkflow decision. This would allow calling SignalWithStart from the workflow code.
Add an option to specify if the started workflow becomes a child workflow.

**Describe alternatives you've considered**
Do not implement and keep using the current workaround of calling it from an activity.



#### Comments (2)

<details>
<summary><strong>wxing1292</strong> commented on 2020-12-15 02:20:46.000 UTC</summary>

ref: https://github.com/uber/cadence/pull/3819

</details>

<details>
<summary><strong>edmondop</strong> commented on 2023-07-01 18:47:56.000 UTC</summary>

I started looking into this and I approached both from the server side and the sdk-go side, but I realized I need a little hint because I don't fully understand the architecture.

It seems that the history service already has such an API, so I am not fully sure why this couldn't be an sdk-go issue. With this (probably) wrong assumption I started to implement new commands and events, but this might not be the right approach. Also, I am a little confused because `workflow.ExecuteChildWorkflow` exists, but this issue seems to suggest we want to have an atomic way to execute from workflows

```
workflow.ExecuteWorkflow()...
workflow.SignalWorkflow()
```
but the first operation is not, as far as I know, exposed via the sdk (only ExecuteChildWorkflow is available)




</details>


---

### #131: Different retry options based on failure type

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/131 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-02-09 19:38:55.000 UTC (5y 10m ago) |
| **Updated** | 2024-07-30 07:25:29.000 UTC |
| **Upvotes** | 28 |
| **Comments** | 4 |
| **Priority Score** | 60 |
| **Labels** | enhancement, API |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 28 |

#### Description

Currently both workflow and activity retry options do not differentiate between failure types besides allowing to not retry configured list of failures.

The proposal is to allow specifying different retry options for different failure types. For example intermittent errors can be retried immediately, but some errors that are not intermittent (like NPE) and require human intervention can be retried with much higher intervals.

#### Comments (4)

<details>
<summary><strong>ronnie660</strong> commented on 2024-01-11 21:32:23.000 UTC</summary>

upvoting this ticket! This feature will be very useful for activities that are hitting external APIs and needs to backoff for some period of time on specific error type.

Reactions: üëç 4

</details>

<details>
<summary><strong>utkarshpaliwal9</strong> commented on 2024-01-17 08:06:15.000 UTC</summary>

Just came here to create a similar issue.
Also, it should be possible for the workflow to not even retry in the cases of certain types of failure (which are not transient). This error mapping with retry policy can be provided by temporal or be taken as input from the user.

</details>

<details>
<summary><strong>dhoepelman</strong> commented on 2024-05-30 07:43:18.000 UTC</summary>

> Also, it should be possible for the workflow to not even retry in the cases of certain types of failure (which are not transient).

@utkarshpaliwal9 
This is possible, you can mark a failure type string as not retryable or throw a non retryable application failure.

</details>

<details>
<summary><strong>MatanYadaev</strong> commented on 2024-07-30 07:25:28.000 UTC</summary>

Is there any workaround?

</details>


---

### #328: Try to run temporal on top of  Amazon Keyspaces

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/328 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-04-25 19:27:14.000 UTC (5y 8m ago) |
| **Updated** | 2025-08-14 12:57:36.000 UTC |
| **Upvotes** | 19 |
| **Comments** | 16 |
| **Priority Score** | 54 |
| **Labels** | enhancement, product-integration, database, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 19 |

#### Description

They claim supporting LWT: https://docs.aws.amazon.com/keyspaces/latest/devguide/functional-differences.html

This would be great for users running on AWS.

#### Comments (16)

<details>
<summary><strong>rfwagner</strong> commented on 2021-07-14 20:19:42.000 UTC</summary>

They don't support LOGGED BATCH. Isn't that a showstopper at this point?

</details>

<details>
<summary><strong>samarabbas</strong> commented on 2021-07-14 20:29:28.000 UTC</summary>

Yes we need ability to update multiple rows in single batch protected by condition.  So this would be a blocker.

</details>

<details>
<summary><strong>joebowbeer</strong> commented on 2021-10-18 22:54:14.000 UTC</summary>

Some discussion here as well:

https://community.temporal.io/t/running-on-amazon-keyspaces/159/7

</details>

<details>
<summary><strong>joebowbeer</strong> commented on 2021-10-18 23:02:25.000 UTC</summary>

AWS Keyspaces differences:
- https://docs.aws.amazon.com/keyspaces/latest/devguide/keyspaces-vs-cassandra.html
- https://docs.aws.amazon.com/keyspaces/latest/devguide/functional-differences.html
- https://docs.aws.amazon.com/keyspaces/latest/devguide/cassandra-apis.html
- https://docs.aws.amazon.com/keyspaces/latest/devguide/consistency.html#UnsupportedConsistency

Is `LOGGED BATCH` support the only thing that is missing?

In community forum someone stated that `LOCAL_QUORUM` consistency level may be sufficient. Can someone confirm?

Does anyone know how to track Keyspaces progress other than via AWS support? I can't find a public roadmap or issue list.

</details>

<details>
<summary><strong>robzienert</strong> commented on 2021-10-18 23:27:59.000 UTC</summary>

Conditional delete is another feature missing.

Reactions: üëÄ 1

</details>

<details>
<summary><strong>nwheeler81</strong> commented on 2022-02-15 22:58:33.000 UTC</summary>

> Conditional delete is another feature missing.

I think, Amazon Keyspaces supports condition delete.
delete from table where pk=? and cl=? IF col>9;

@ Row 1
-----------+-------
 [applied] | False
 col           | 9 

</details>

<details>
<summary><strong>freeqaz</strong> commented on 2022-03-25 22:54:20.000 UTC</summary>

Any update on this, out of curiosity? I'm likely not the only person looking to answer this question. We're looking to use Temporal, but we're not sure if we should go with Cassandra or SQL.

Managing a Cassandra cluster ourselves sounds pretty bad, and I would prefer to use Keyspaces over that. If anybody is more skilled at testing out a deployment of Temporal on Keyspaces, that would be super helpful. Thanks!

Reactions: üëç 3

</details>

<details>
<summary><strong>freeqaz</strong> commented on 2022-03-25 23:08:32.000 UTC</summary>

Just to add some more context from poking around on existing threads:

## LWT support (added)

@mfateev said [here](https://community.temporal.io/t/aws-keyspace-as-managed-cassandra-for-temporal/1762) that they need AWS Keyspaces to add LWT support. From the time this was posted, AWS must have added this because [this](https://docs.aws.amazon.com/keyspaces/latest/devguide/functional-differences.html#functional-differences.light-transactions) page says the following:

> "Amazon Keyspaces (for Apache Cassandra) fully supports compare and set functionality on INSERT, UPDATE, and DELETE commands, which are known as lightweight transactions (LWTs) in Apache Cassandra. As a serverless offering, Amazon Keyspaces (for Apache Cassandra) provides consistent performance at any scale, including for lightweight transactions. With Amazon Keyspaces, there is no performance penalty for using lightweight transactions."

## Conditional delete support (probably added, but I'm a noob)

The comment above from @nwheeler81 shows conditional deletes working, and the Keyspaces docs say the [following](https://docs.aws.amazon.com/keyspaces/latest/devguide/functional-differences.html#functional-differences.batch):

> "Amazon Keyspaces supports unlogged batch commands with up to 30 commands in the batch. Only unconditional INSERT, UPDATE, or DELETE commands are permitted in a batch. Logged batches are not supported."

This is for batches, and nothing else is mentioned anywhere on the website. Maybe with more digging I can sort this out, but @nwheeler81's comment likely trumps the docs.

## Logged Batch support (not supported still)

This page on [supported Cassandra features](https://docs.aws.amazon.com/keyspaces/latest/devguide/cassandra-apis.html#cassandra-api-support) is probably the best place to "glance" for info.

They say that Logged Batch is unsupported, but unlogged batches are supported. I don't know enough about Temporal or Cassandra to understand what this means.

-------

That's my summary of the state of this so far. If there is anything else missing that Temporal uses, I'm happy to dig in and verify. Otherwise, it seems like the only thing missing is "Logged Batch" support.

Is there a workaround possible for that?

</details>

<details>
<summary><strong>ouanixi</strong> commented on 2023-01-26 09:27:42.000 UTC</summary>

I think there's also a consistency level that is not matched. 
A few comments in the code state that "quorum" level consistency is required. [Here's one.](https://github.com/temporalio/temporal/blob/ad8342bd5b95f5a3c243f7d0f12ae561a91633b6/common/persistence/cassandra/cassandraPersistence.go#L2551)
Which is [not supported](https://docs.aws.amazon.com/keyspaces/latest/devguide/consistency.html) by keyspaces

</details>

<details>
<summary><strong>Zonalds</strong> commented on 2023-06-01 18:02:14.000 UTC</summary>

hi,

Has anything changed? 

Reactions: üëç 6

</details>

<details>
<summary><strong>karunagoyal</strong> commented on 2024-01-17 14:17:10.000 UTC</summary>

Hi all,
Can anyone clarify if it is supported?

</details>

<details>
<summary><strong>meet-bhagdev</strong> commented on 2024-03-12 18:53:27.000 UTC</summary>

Temporal team - i am a product manager at AWS working on Amazon Keyspaces. Would love to re-engage here to understand what we can do to enable [Temporal.io](http://temporal.io/) for Amazon Keyspaces

Reactions: üëç 23 üëÄ 1

</details>

<details>
<summary><strong>wojo</strong> commented on 2024-05-08 00:45:08.000 UTC</summary>

@meet-bhagdev we are evaluating Temporal and would much prefer to use Keyspaces, I think the gaps are above so we'll be watching this closely.

Reactions: üëç 2

</details>

<details>
<summary><strong>jamesbuddrige</strong> commented on 2024-05-13 18:03:19.000 UTC</summary>

Also keen to see this! We're currently using Postgres on RDS, but would love to see keyspaces support.

</details>

<details>
<summary><strong>maxime-gaudron</strong> commented on 2024-07-02 13:56:53.000 UTC</summary>

@meet-bhagdev any news? :) 

</details>

<details>
<summary><strong>matanbaruch</strong> commented on 2025-08-14 12:57:36.000 UTC</summary>

+1

Reactions: üëç 1

</details>


---

### #2609: Add ability to transfer child workflows to new parent run when parent calls continue as new

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2609 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2022-03-14 18:51:43.000 UTC (3y 9m ago) |
| **Updated** | 2024-10-12 07:26:17.000 UTC |
| **Upvotes** | 23 |
| **Comments** | 7 |
| **Priority Score** | 53 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 23 |

#### Description

Request: Add ability to transfer child workflows to new parent run when parent calls continue as new. 

Currently the child workflow is not transferred and follows the parentClosePolicy. 

#### Comments (7)

<details>
<summary><strong>albertyfwu</strong> commented on 2022-06-22 16:37:25.000 UTC</summary>

@tsurdilo Can you clarify what "transfer child workflows" would mean in practice?

Today, upon continue as new, do child workflows continue to follow the parentClosePolicy or are they effectively abandoned?

</details>

<details>
<summary><strong>bkniffler</strong> commented on 2022-08-17 08:20:08.000 UTC</summary>

Today, it seems that with the default `PARENT_CLOSE_POLICY_TERMINATE`, children are terminated if the parent workflow continues as new. Until there is a built-in way to handle transfer of children to the next WF run, I'm doing the following:
- Save an array/map `pendingChildren` of all running children in the parent worklfow 
- Pass workflowIds to the next run, as an argument of the continueAsNew fn
- Call an activity for each workflow that waits for their result and clears items from `pendingChildren` on error or success

Unfortunately, workflows (at least in TS SDK) don't allow to wait for workflows that they didn't directly spawn. We can use getExternalWorkflowHandle, but it doesn't really expose much info/functionalyity (like wait for result or the status). Thus, we need to use activities.

</details>

<details>
<summary><strong>bkniffler</strong> commented on 2022-08-17 08:21:02.000 UTC</summary>

(if there is better strategies @tsurdilo, would love some feedback)

</details>

<details>
<summary><strong>picpromusic</strong> commented on 2024-01-22 07:15:30.000 UTC</summary>

Is #373 related(a valid preparation) to this issue?

</details>

<details>
<summary><strong>JakeCooper</strong> commented on 2024-07-03 03:11:54.000 UTC</summary>

Just a heads up this would be super useful

We've got an example usecase where we use Temporal as a mutex on a buildQueue. We're essentially pushing child workflow executions (builds) into the parent (buildQueue) workflow

This works great, until we have to continue as new. 

Since we can't move the handle over (aka just Promise.any await on the number of builds), the top level buildQueue just kinda has to poll the child workflows on an interval

</details>

<details>
<summary><strong>JakeCooper</strong> commented on 2024-07-03 06:26:54.000 UTC</summary>

We've also attempted the bidirection signals (As suggested by Maxim and Roey on [Slack](https://temporalio.slack.com/archives/CTRCR8RBP/p1719985220287989?thread_ts=1719890618.119439&cid=CTRCR8RBP)). However, this has resulted in a couple times accidentally recursively signaling between both workflows

Would love to be able to just grab something which we can dynamically await on in the workflow itself. Makes this pattern a lot easier and less error prone

</details>

<details>
<summary><strong>pilsy</strong> commented on 2024-10-12 07:26:16.000 UTC</summary>

Take a look at https://www.npmjs.com/package/chrono-forge?activeTab=readme#handling-circular-workflow-relationships @JakeCooper 

</details>


---

### #2582: Add health check handler for worker service

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2582 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2022-03-07 19:04:05.000 UTC (3y 10m ago) |
| **Updated** | 2024-06-20 13:45:28.000 UTC |
| **Upvotes** | 18 |
| **Comments** | 13 |
| **Priority Score** | 49 |
| **Labels** | enhancement, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 18 |

#### Description

Looks as currently worker service https://github.com/temporalio/temporal/tree/master/service/worker
does not expose a gRPC handler and the health check method like matching and history services do. 

Please add this if possible.

#### Comments (13)

<details>
<summary><strong>yiminc</strong> commented on 2022-03-11 22:48:19.000 UTC</summary>

Worker does not have a gRPC service endpoint. It would be good to have sdk to implement the health check so whoever running a worker would have it.

</details>

<details>
<summary><strong>paymog</strong> commented on 2023-08-01 08:49:34.000 UTC</summary>

While we wait for this to get implemented, does anyone have a workaround for those of us running workers in kubernetes? We recently deployed a broken docker image for our worker and didn't realize for a few days.

Reactions: üëç 4

</details>

<details>
<summary><strong>tredmon</strong> commented on 2024-04-01 23:12:39.000 UTC</summary>

> While we wait for this to get implemented, does anyone have a workaround for those of us running workers in kubernetes? 

I've just started playing with temporal in kubernetes, but a simple liveness probe could look like (assuming your image has `sh` and `pidof` installed and that your worker has a binary which is unique on your pod runtime):
```yaml
livenessProbe:
  exec:
    command:
      - /bin/sh
      - -c
      - pidof your-worker-binary >/dev/null
```

For the readiness probe I'm considering creating a file in an `emptyDir` volume when the worker is ready, and creating another file when the worker receives a termination signal. Then the readiness probe could look for the existence of said files.

</details>

<details>
<summary><strong>robholland</strong> commented on 2024-06-20 13:05:49.000 UTC</summary>

If the worker process died, the container would restart, you'd see crash backoff loop. A `pidof` health check is never going to report unhealthy.

</details>

<details>
<summary><strong>robholland</strong> commented on 2024-06-20 13:06:40.000 UTC</summary>

@paymog do you mean your application worker, or Temporal's (internal) worker, which is what this issue relates to? In what way was your worker failing that wasn't resulting in the container exiting?

</details>

<details>
<summary><strong>paymog</strong> commented on 2024-06-20 13:18:49.000 UTC</summary>

I'm referencing the application worker here and specifically interested in having a health check to make staged rollouts easier in kubernetes.

</details>

<details>
<summary><strong>robholland</strong> commented on 2024-06-20 13:22:47.000 UTC</summary>

This issue was referring to the internal system worker. We can't provide a health check for your application workers.

</details>

<details>
<summary><strong>paymog</strong> commented on 2024-06-20 13:24:16.000 UTC</summary>

ah, my bad then! I think other folks which üëç [this comment](https://github.com/temporalio/temporal/issues/2582#issuecomment-1659862445) are similarly confused. Why can't you provide a health check? There's a default way to expose prometheus metrics on a provided port, why can't the SDK spin up an http server that responds with a 200 once the worker is ready to process workflows/activities?

</details>

<details>
<summary><strong>robholland</strong> commented on 2024-06-20 13:29:14.000 UTC</summary>

Healthy is context specific. Your application worker may be running but failing to process workflow tasks because it lacks some resource that we can't know about. The worker being able to respond 200 ok to an incoming request doesn't mean it was successfully able to connect to Temporal for all the Workers within the process (there can be many Temporal SDK Worker per process). There is just no clear meaning to what healthy means.

</details>

<details>
<summary><strong>paymog</strong> commented on 2024-06-20 13:38:17.000 UTC</summary>

I see. Could temporal instead provide a sample doc of how to spin up a health check server that each user could implement and customize?

</details>

<details>
<summary><strong>robholland</strong> commented on 2024-06-20 13:40:06.000 UTC</summary>

Maybe, but it's "just" spinning up a HTTP listener with one endpoint, which in most languages is very easy. The rest would all be custom, so I'm not sure how much value there is there. There would be nothing Temporal specific about it.

</details>

<details>
<summary><strong>robholland</strong> commented on 2024-06-20 13:40:44.000 UTC</summary>

Put another way, it would be no different to how you'd add healthcheck endpoints to any other part of your application.

</details>

<details>
<summary><strong>paymog</strong> commented on 2024-06-20 13:44:13.000 UTC</summary>

Is there a canonical way to hook into the worker sdk to only spin up this server after the worker has connected to temporal?

Ninja edit: well this is embarrassing, turns out I already implemented this in our worker by copy-pasting https://github.com/temporalio/sdk-typescript/blob/c1e7fffcd807493298d66d0063c4618eaf851206/packages/test/src/load/worker.ts#L191

EDIT: I suspect lots of users would be happy if there was a flag we could set during worker initiation that would handle this for us. Agreed that healthy is context specific, something like this will likely satisfy the vast majority of users though.

</details>


---

### #5302: YDB Temporal support.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5302 |
| **State** | OPEN |
| **Author** | avran02 |
| **Created** | 2024-01-16 13:56:04.000 UTC (1y 11m ago) |
| **Updated** | 2024-01-24 08:34:56.000 UTC |
| **Upvotes** | 23 |
| **Comments** | 2 |
| **Priority Score** | 48 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 23 üéâ 4 ‚ù§Ô∏è 22 üöÄ 4 |

#### Description

We would like to thank the Temporal developers very much for their hard work in creating and maintaining this amazing framework. The contribution of the Temporal team to the development of tools for the development of long-lived applications is incredibly important for many developers and represents a truly valuable resource in modern software development.

Due to the growing demand for the use of Yandex Database (YDB) in modern applications with high requirements for scalability and fault tolerance, we consider it important to add YDB support to the Temporary framework. Temporary is a powerful tool for developing long-lived applications, but currently the lack of native integration with YDB limits the capabilities of developers and reduces the competitiveness of the framework.

Functionality Details:
1. Support for Yandex Database as a repository for Temporary, which will allow developers to take full advantage of the features provided by YDB, such as geographic data replication, automatic scaling and high fault tolerance.
2. Integration of YDB functionality with the Temporary SDK to ensure ease of use and compatibility with existing applications.

Advantages:
1. Expanding the potential audience of Temporary by YDB users interested in developing long-lived applications.
2. Improved performance and scalability of applications using Temporary in combination with YDB.
3. Increasing the competitiveness of Temporal in the market of long-lived application development tools.

This extension of Temporal functionality will enable developers to take full advantage of the benefits provided by YDB in their long-lived applications, which will make Temporal an even more attractive development tool.

#### Comments (2)

<details>
<summary><strong>rodrigozhou</strong> commented on 2024-01-23 17:21:43.000 UTC</summary>

@avran02 Yandex has fork of Temporal using YDB: https://github.com/yandex/temporal-over-ydb
cc: @aromanovich 

</details>

<details>
<summary><strong>avran02</strong> commented on 2024-01-24 08:34:55.000 UTC</summary>

> @avran02 Yandex has fork of Temporal using YDB: https://github.com/yandex/temporal-over-ydb
> cc: @aromanovich 

This fork is not a 100% functional solution, it would be great to see YDB support in Temporal out of the box. With the option to select DB as storage.

Reactions: üëç 10

</details>


---

### #1797: Ability to get all task queues per namespace

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1797 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2021-08-04 01:00:48.000 UTC (4y 5m ago) |
| **Updated** | 2025-09-22 11:46:18.000 UTC |
| **Upvotes** | 22 |
| **Comments** | 4 |
| **Priority Score** | 48 |
| **Labels** | enhancement, API |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 22 üëÄ 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently public API only allow query a specific task queue. Temporal should consider adding functionality to list all task queue per namespace for maintenance & debugging purpose.

**Describe the solution you'd like**
See above

**Describe alternatives you've considered**
N/A

**Additional context**
there exists one persistence layer [API](https://github.com/temporalio/temporal/blob/a4ce3a1/common/persistence/dataInterfaces.go#L1330) which allow pagination of ALL task queues, but this API impl is not efficient & some cases does not allow pagination according to namespace:
* Cassandra tasks table schema use [`PRIMARY KEY ((namespace_id, task_queue_name, task_queue_type), type, task_id)`](https://github.com/temporalio/temporal/blob/a4ce3a1/schema/cassandra/temporal/schema.cql#L90)
* MySQL task queue schema use [`PRIMARY KEY (range_hash, task_queue_id)`](https://github.com/temporalio/temporal/blob/a4ce3a1/schema/mysql/v57/temporal/schema.sql#L89)
* PostgreSQL task queue schema use [`PRIMARY KEY (range_hash, task_queue_id)`](https://github.com/temporalio/temporal/blob/a4ce3a1/schema/postgresql/v96/temporal/schema.sql#L89)




#### Comments (4)

<details>
<summary><strong>amalkh5</strong> commented on 2023-10-11 08:25:27.000 UTC</summary>

@wxing1292 is there any update or workaround to get this info?

</details>

<details>
<summary><strong>marchmallow</strong> commented on 2024-06-17 08:32:55.000 UTC</summary>

Any updates on plans for this? Thanks!!

</details>

<details>
<summary><strong>AdilsonTorres</strong> commented on 2025-03-10 20:13:44.000 UTC</summary>

While searching, I found this issue. It seems there hasn‚Äôt been any update on this feature yet. I would also like to have this feature.

It could be based on the history of already executed workflows and/or on active workers and their queues (preferred).

Thanks.

</details>

<details>
<summary><strong>Srirammkm</strong> commented on 2025-09-22 11:46:18.000 UTC</summary>

Any update on this? This is much needed feature for building client side tools and wrappers. Thank you


Reactions: üëç 1

</details>


---

### #1460: Add task queue query

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1460 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2021-04-14 17:42:05.000 UTC (4y 8m ago) |
| **Updated** | 2025-09-05 00:33:32.000 UTC |
| **Upvotes** | 16 |
| **Comments** | 11 |
| **Priority Score** | 43 |
| **Labels** | enhancement, devexp, difficulty: medium, planning, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 16 üëÄ 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, there is no way to query a specific worker for information as queries are always related to workflow execution. For example it is not possible to query a worker for activity types it supports.

**Describe the solution you'd like**
Add `task queue` query concept. Such a query specifies a task queue name to send the query to. Then a worker picks it up and replies using a new API like `RespondTaskQueueQueryCompleted`.

**Additional context**
This would support the ability to query for registered activity and workflow types, worker configuration options as well as execute activities synchronously. 

https://community.temporal.io/t/list-registered-activities/1927


#### Comments (11)

<details>
<summary><strong>ilmn-aeolus</strong> commented on 2021-04-29 17:14:20.000 UTC</summary>

We are exploring a design where third parties would provide activity workers by running our activity daemon that communicates to a pre-configured, party-specific namespace. This feature would allow our central service to monitor whether third-parties have active workers. 

</details>

<details>
<summary><strong>ilmn-aeolus</strong> commented on 2021-04-29 17:15:42.000 UTC</summary>

Also, does this ticket service https://community.temporal.io/t/ability-to-inspect-task-queues/1765 ?

</details>

<details>
<summary><strong>swyxio</strong> commented on 2021-09-16 23:45:48.000 UTC</summary>

discussing with Samar an alternative name for this is "worker queries" 

- "interceptor for starting a workflow" for input validation before starting a workflow 
- we think we can implement worker queries first, THEN synchronous start https://github.com/temporalio/temporal/issues/804

</details>

<details>
<summary><strong>jlegrone</strong> commented on 2021-12-18 00:24:17.000 UTC</summary>

> This would support the ability to query for registered activity and workflow types

We do something like this internally, inspired by the [GRPC server reflection protocol](https://github.com/grpc/grpc/blob/master/doc/server-reflection.md). Our "reflection" service is just a Temporal workflow type that each of our workers exposes. The reflection workflow is automatically registered and implemented via our worker package, but if this were moved upstream each language SDK could probably do the same.

Internally we also use protobuf to define workflow/activity/signal/query interfaces, so we expose the message descriptors for those payloads as well. This is the full reflection interface:

```proto
message MessageDescriptor {
    // Serialized message descriptor. We avoid taking a dependency on
    // descriptor.proto, which uses proto2 only features, by making this opaque
    // bytes instead.
    //
    // Source: https://github.com/protocolbuffers/protobuf/blob/c57ee04cf32274681f45a0c0acb35e8605fff9e7/src/google/protobuf/descriptor.proto#L93-L126
    bytes message_descriptor_proto = 1;
}

message SignalType {
    string name = 1;
    // A description of the signal.
    string doc = 2;
    MessageDescriptor payload_type = 3;
}

message QueryType {
    string name = 1;
    // A description of the query.
    string doc = 2;
    MessageDescriptor request_type = 3;
    MessageDescriptor response_type = 4;
}

message WorkflowType {
    string name = 1;
    // A description of the workflow.
    string doc = 2;
    temporal.StartWorkflowOptions default_options = 3;
    MessageDescriptor request_type = 4;
    MessageDescriptor response_type = 5;
    repeated SignalType signals = 6;
    repeated QueryType queries = 7;
}

message ActivityType {
    string name = 1;
    // A description of the activity.
    string doc = 2;
    temporal.ActivityOptions default_options = 3;
    MessageDescriptor request_type = 4;
    MessageDescriptor response_type = 5;
}

message ReflectResponse {
    repeated WorkflowType workflows = 1;
    repeated ActivityType activities = 2;
}

// The `Reflection` interface is exposed by every worker on its own task queue.
// This allows external systems to query the list of workflows, activities,
// signals, and queries supported on any task queue.
//
// To use the Reflection service, the client should send a request to execute
// the Reflect workflow on the target task queue. If the worker listening on
// the task queue has enabled Reflection, it will respond with a list of
// available workflows and activities.
service Reflection {
    // Reflect returns a list of workflows and activities registered on the current task queue.
    rpc Reflect(google.protobuf.Empty) returns (ReflectResponse) {
        option (temporal.workflow).config = {};
    };
}
```



</details>

<details>
<summary><strong>jlegrone</strong> commented on 2021-12-18 00:33:42.000 UTC</summary>

> Add `task queue` query concept. Such a query specifies a task queue name to send the query to. Then a worker picks it up and replies using a new API like `RespondTaskQueueQueryCompleted`.

What does a new API provide vs. a pre-defined workflow type?

</details>

<details>
<summary><strong>bergundy</strong> commented on 2022-01-12 02:05:09.000 UTC</summary>

We should consider supporting querying each worker separately so we can compare that they're all the same version and contain the same set of workflows and activities.

</details>

<details>
<summary><strong>mdross95</strong> commented on 2023-07-27 20:14:27.000 UTC</summary>

Any update with this?

</details>

<details>
<summary><strong>JacobJae</strong> commented on 2024-05-16 18:02:31.000 UTC</summary>

Any update on this?

</details>

<details>
<summary><strong>skyf0cker</strong> commented on 2024-06-21 03:24:58.000 UTC</summary>

Any update on this? 

</details>

<details>
<summary><strong>alexseedkou</strong> commented on 2024-09-26 03:23:42.000 UTC</summary>

I was wondering if anyone has taken this. If not, I‚Äôm happy to work on it.

Reactions: üëç 5

</details>

<details>
<summary><strong>mfateev</strong> commented on 2025-09-05 00:33:31.000 UTC</summary>

This is being solved differently by running a per-process Nexus service endpoint. 

Reactions: üëç 2

</details>


---

### #3228: Metadata for Timers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3228 |
| **State** | OPEN |
| **Author** | Mattchewone (Matt Chaffe) |
| **Created** | 2022-08-15 13:29:50.000 UTC (3y 4m ago) |
| **Updated** | 2025-02-21 07:21:27.000 UTC |
| **Upvotes** | 16 |
| **Comments** | 6 |
| **Priority Score** | 38 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 16 |

#### Description

**Is your feature request related to a problem? Please describe.**
I'm always frustrated when we have multiple timers being created, and it's unclear in the UI which is related to what part of the flow. We have a resettable timer and an overall timer, and when several timers get created, it can be tricky to see whether the right one was created or it was part of the loop and the first timer just re-created

**Describe the solution you'd like**
A way to add metadata to the Timer. Say adding a name or something that would help readability in the UI.

Current
```
Timer 37()
...
Timer 41()
```

Suggested
```
Timer 37() SubscriptionTimer
...
Timer 41() UserCancelTimer
```

#### Comments (6)

<details>
<summary><strong>yiminc</strong> commented on 2022-08-19 19:02:00.000 UTC</summary>

Would it work for you if the UI shows the duration of the timer? 

</details>

<details>
<summary><strong>yeshwanthPentakota</strong> commented on 2022-12-06 03:13:56.000 UTC</summary>

The duration of the timer helps us for sure.  But there could be a number of timers in a workflow and having a human-readable text next to it might be more useful. 

</details>

<details>
<summary><strong>calum-stripe</strong> commented on 2023-03-30 18:27:26.000 UTC</summary>

just wanted to +1 this feature request. We have lots of teams using Temporal and we are constantly asked about what timers do what and which ones are specific ones. 

</details>

<details>
<summary><strong>alrz</strong> commented on 2024-01-20 14:43:34.000 UTC</summary>

Can we do the same for child workflows? I'd love to tag workflows instead of creating one per each for a cleaner timeline view in UI.

Reactions: üëç 1

</details>

<details>
<summary><strong>ManeeshaSenarath13</strong> commented on 2024-07-15 13:00:56.000 UTC</summary>

+1 on this

</details>

<details>
<summary><strong>chaowss</strong> commented on 2025-02-21 07:21:26.000 UTC</summary>

+1! Would be really nice to be able to name timers for QOL

</details>


---

### #3366: Support sqlite in production.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3366 |
| **State** | OPEN |
| **Author** | shanna (Shane Hanna) |
| **Created** | 2022-09-12 09:28:36.000 UTC (3y 3m ago) |
| **Updated** | 2025-08-30 19:02:30.000 UTC |
| **Upvotes** | 16 |
| **Comments** | 4 |
| **Priority Score** | 36 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 16 |

#### Description

**Is your feature request related to a problem? Please describe.**
I'm a fan of self hosting early stage and startup projects on micro budgets (think 5USD to 100USD a month at most). I've been using Temporal on work projects but more recently I've been playing with Temporalite using sqlite replicated with https://litestream.io to https://min.io. It has worked out great in development but I'd like to take Temporal (or Temporalite) to production without clusting, postgres, elasticsearch etc which all blow out small budgets in a heartbeat.

**Describe the solution you'd like**
Officially support sqlite in production.

I think this would involve (at least):
* Add sqlite as an option to the sql tool.
* Add sqlite support to the auto-setup shell scripts.
* Add sqlite visibility support? (perhaps requiring the fulltext search extension?).

**Describe alternatives you've considered**
I have been tempted to use temporalite in production despite all the risks and downsides.

**Additional context**
I don't fully utilise Temporal. Generally my workflows are for service to service coordination and short lived so I'm OK with a single nodes and short stretches of downtime which I understand probably isn't typical for Temporal users.

I asked about future plans for sqlite support here https://community.temporal.io/t/future-sqlite3-support/5915?u=shane


#### Comments (4)

<details>
<summary><strong>jlegrone</strong> commented on 2022-10-11 14:07:09.000 UTC</summary>

FYI, the vision is for Temporalite to be used in production contexts. I might make a milestone to make tracking easier, but this depends on https://github.com/temporalio/temporalite/issues/23, https://github.com/temporalio/temporalite/issues/110, and signoff from more Temporal server maintainers. I'd love to get published benchmarks, which are in the works.

Of course you are free to use the regular Temporal server binary with the sqlite driver as well! Any improvements to performance and stability will also be available in Temporalite.

-----

Also, üëç for litestream. I haven't gotten around to playing with it yet, but glad to know it's working for you with Temporalite!

</details>

<details>
<summary><strong>evgenii-moriakhin</strong> commented on 2024-03-08 20:26:09.000 UTC</summary>

Hello

There is no progress or discussion on this issue? I see the problem has been reopened

I'm interested because I too want to use temporal with sqlite on a virtualization product where thousands of hosts may have agents that don't have broad access to the enterprise network, but I'd like to be able to execute temporal workflows on them without having to deploy a "serious" DBMS like PostgreSQL on each host, but take advantage of temporal. 

I read this topic https://community.temporal.io/t/future-sqlite3-support/5915/6?u=shane
and find the use cases completely similar to mine

and I had the question after reading this topic, if these points

> One reason is it starts all server roles in the same container and in a single process (similar to auto-setup) which makes it hard to scale and tune among other things

 are not critical for me, can I use temporal with sqlite in production?

@shanna , did you finally get it to work?



Reactions: üëç 5

</details>

<details>
<summary><strong>robholland</strong> commented on 2025-07-03 16:47:03.000 UTC</summary>

The Temporal CLI dev-server (https://docs.temporal.io/cli#start-dev-server) supports having the SQLite database on-disk. Is that sufficient for your use case? 

</details>

<details>
<summary><strong>raymondsze</strong> commented on 2025-08-30 19:01:46.000 UTC</summary>

@robholland
Is it possible to support something like rqlite? Some usecase for me is we may not able to offer a database like postgres or mysql, but we still want to have some kind of durability.

I can configure a litestream to periodically backup the data to s3 compatible storage. But its is a sidecar container, not guarantee 100% durability.

</details>


---

### #4795: Schedules - add option for execution workflowid to be "as-is" (not unique per run)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4795 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2023-08-23 20:54:30.000 UTC (2y 4m ago) |
| **Updated** | 2025-10-29 01:24:44.000 UTC |
| **Upvotes** | 14 |
| **Comments** | 6 |
| **Priority Score** | 34 |
| **Labels** | enhancement, schedules |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 14 üòï 1 üëÄ 9 |

#### Description

Schedules currently add timestamp of when the execution is started as part of workflowid.
This request is to add option for workflow id used to be as user defines in workflow options for each execution schedule creates.



#### Comments (6)

<details>
<summary><strong>zifanwwang</strong> commented on 2024-10-22 20:49:55.000 UTC</summary>

Hi! Our team persist workflow ids in DB, and these are currently defaulted to "schedule id + timestamp". We do appreciate it if we can have custom format of workflowId that created by schedules. Is there any update on this open issue? 

Reactions: üòï 1

</details>

<details>
<summary><strong>xhocquet</strong> commented on 2025-02-21 18:41:54.000 UTC</summary>

Hello, this also interests me. In my team's case, we have a daily schedule and I would like to use a simpler date format. It would require being able to pass a proc to the id parameter, or a new option to control the formatting of the timestamp that Temporal appends to the workflow ID.

</details>

<details>
<summary><strong>Jank1310</strong> commented on 2025-04-10 05:29:51.000 UTC</summary>

We want to run a schedule every 5 minutes or trigger the workflow in response to some specific event. However, the added timestamp prevents us from using any method to start a unique workflow other than schedule.trigger()

</details>

<details>
<summary><strong>iceman91176</strong> commented on 2025-05-16 17:27:49.000 UTC</summary>

Hi there - having an option to format the suffix would be great.  There should be various options, e.g.

* adding a hardcoded-suffix to every created workflow-instance (even ommiting one, which can be a valid use-case)
* format the appendes timestamp (e.g, using an Instant, instead of the human-readable Datetime)
* configure a custom formatter by using a callback

How could we get this going ?


Reactions: üëç 1 üöÄ 1

</details>

<details>
<summary><strong>matthewteeter</strong> commented on 2025-06-06 22:31:12.000 UTC</summary>

Agree it would be useful to be able to control the format of the suffix. For example, if a Workflow runs each day, it would be nice to have a predictable suffix of dd-MM-yyyy for example so that other Workflows dependent on it (possibly from another namespace) can construct the WorkflowId to inspect independently.

</details>

<details>
<summary><strong>ammar-deriv</strong> commented on 2025-10-29 01:24:44.000 UTC</summary>

Hello. Any progress on this optional suffix scheduled workflow id?

</details>


---

### #1988: Implement dynamic task queue routing

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1988 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2021-09-28 20:29:00.000 UTC (4y 3m ago) |
| **Updated** | 2024-10-03 20:39:31.000 UTC |
| **Upvotes** | 15 |
| **Comments** | 3 |
| **Priority Score** | 33 |
| **Labels** | enhancement |
| **Assignees** | samarabbas |
| **Milestone** | None |
| **Reactions** | üëç 15 üòÑ 1 üéâ 1 ‚ù§Ô∏è 2 üöÄ 1 üëÄ 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
Once an activity or workflow task is scheduled its task queue name is fixed. In many cases for development/troubleshooting and other scenarios ability to route all or subset of tasks to a different task queue can be very useful. See this @robzienert [forum post](https://community.temporal.io/t/using-dynamic-task-queues-for-traffic-routing/3045) for some usage examples.

**Describe the solution you'd like**
Add API to manage routing rules that forward tasks to other queues.

**Describe alternatives you've considered**
Use SDK library as described by @robzienert  in his post.



#### Comments (3)

<details>
<summary><strong>Multiply</strong> commented on 2021-09-28 20:40:17.000 UTC</summary>

We'd be very interested in this, assuming it hits the go-sdk.

Our primary headache is running all microservices locally, and we've been trying various solutions to intercept traffic and route it to local environments.
This feature could solve that to some degree, at least for the things running inside Temporal.

</details>

<details>
<summary><strong>jlegrone</strong> commented on 2021-10-11 23:25:20.000 UTC</summary>

We've written some tooling to propagate request-scoped routing rules that allow targeting new task queues for child workflows or activities, including when these are deeply nested in the call stack. This approach is heavily inspired by Envoy's [http routing rules](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/http/http_routing).

Right now we use this technique primarily to support local dev in a shared Temporal cluster. Developers start a version of the worker they're making changes to locally, register this worker on a unique task queue, then make requests with routing rules on the context which target their new worker version.

I think this approach is complementary to the one described by Rob, as we also have use cases that require writing code to map task queues to worker topology based on the contents of the request or the client's environment. These rules are universally enforced and encoded in the worker/client binaries however, while request scoped routing rules allow targeted, last mile customization and support debugging without needing to deploy new worker versions.

This is something we've been considering open sourcing. Happy to go into more detail, but for now here's the API we're using to define routing rules. You'll see lots of reserved fields; we've considered use cases beyond modifying the task queue, including routing requests to a new workflow or activity type.

```proto
syntax = "proto3";

package temporal.router.v1;

message RoutingRule {
    message StringMatchRule {
        oneof match {
            // Match full text of string
            string exact = 1;
            // Match if string has prefix
            string prefix = 2;
        }
    }
    message MatchCondition {
        // Temporal task queue. This is required.
        StringMatchRule task_queue = 1;
        // Temporal namespace. If unset, all namespaces will match.
        reserved "namespace"; // not available for non-worker clients
        // Other fields that apply to both workflows and activities
        reserved 2 to 50;
        // The workflow name (also referred to as workflow type)
        reserved "workflow_name";
        // Other fields that apply only to workflows
        reserved 51 to 100;
        // The activity name (also referred to as activity type)
        reserved "activity_name";
        // Other fields that apply only to activities
        reserved 101 to 150;
    }
    message Destination {
        // New task queue
        string task_queue = 1;
        reserved 2 to 50;
        // New workflow name
        reserved "workflow_name";
        reserved 51 to 100;
        // New activity name
        reserved "activity_name";
        reserved 101 to 150;
    }
    // A human-readable name for the routing rule. Useful for debugging.
    string name = 1;
    // A set of conditions that determine whether the rule should be applied. When multiple
    // conditions are supplied, only one must match in order for the rule to apply.
    repeated MatchCondition match = 2;
    // A destination that determines how the request should be modified.
    Destination destination = 3;
    // A weight from 1-100 that represents the percentage of traffic to which this rule
    // should be applied.
    reserved "runtime_weight";
}

message RoutingRules {
    // A list of rules that will be matched, in order, for outgoing requests.
    // The first rule in the list which matches the request will be used.
    repeated RoutingRule rules = 1;
}
```

Reactions: üëç 3 üëÄ 2

</details>

<details>
<summary><strong>ikonst</strong> commented on 2024-10-03 20:39:17.000 UTC</summary>

> Once an activity or workflow task is scheduled its task queue name is fixed.

@robzienert's post was about routing when scheduling, not after scheduling (i.e, "moving" queues). Did you also mean routing when scheduling, or is this issue about something else?

</details>


---

### #298: Support embedded version of the service

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/298 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-04-12 20:31:53.000 UTC (5y 8m ago) |
| **Updated** | 2024-11-25 04:20:52.000 UTC |
| **Upvotes** | 14 |
| **Comments** | 4 |
| **Priority Score** | 32 |
| **Labels** | enhancement, packaging |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 14 ‚ù§Ô∏è 1 |

#### Description

For small scale on prem deployments having ability to run Temporal as a library which has only DB as a dependency would be really great.

#### Comments (4)

<details>
<summary><strong>yiminc</strong> commented on 2022-10-29 18:03:44.000 UTC</summary>

This is currently possible to do using SQL (MySQL/PostgresSQL/SQLite) as both default and visibility store. Is that what is requested here?

</details>

<details>
<summary><strong>brettinternet</strong> commented on 2024-07-24 22:24:02.000 UTC</summary>

@yiminc Is this possible? Is it documented anywhere or is the only method to do something like running the server on a port like Temporal CLI's implementation?

I would love to see something similar to the way NATS does this by allowing an in-process connection to the server instead of via the loopback:

```
server, err := server.NewServer(opts)
conn, err := nats.Connect(server.ClientURL(), nats.InProcessServer(server))
```

Here's a related forum topic: https://community.temporal.io/t/embed-the-temporal-server-in-a-go-application/12914/3

Reactions: üëç 1

</details>

<details>
<summary><strong>abtinf</strong> commented on 2024-10-03 23:11:37.000 UTC</summary>

@brettinternet I went through all the dev server code and figured out the basic config needed to get embedded working. See https://github.com/abtinf/temporal-a-day/blob/main/001-all-in-one-hello/main.go

Reactions: üëç 6 ‚ù§Ô∏è 6

</details>

<details>
<summary><strong>yg7001</strong> commented on 2024-11-25 04:20:51.000 UTC</summary>

@abtinf great job!

</details>


---

### #4180: Official CockroachDB (CRDB) support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4180 |
| **State** | OPEN |
| **Author** | Hades32 (Martin Rauscher) |
| **Created** | 2023-04-18 08:35:23.000 UTC (2y 8m ago) |
| **Updated** | 2023-06-06 05:57:33.000 UTC |
| **Upvotes** | 13 |
| **Comments** | 1 |
| **Priority Score** | 27 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 13 ‚ù§Ô∏è 5 üëÄ 4 |

#### Description

Currently CRDB is working well with Temporal and the PostgreSQL v9.6 DB backend. But as e.g. https://github.com/temporalio/temporal/issues/4136 has shown, this was more accidental, although Temporal even provides a Docker Compose template that uses it.

We'd like to see CRDB becoming officially supported. The least I'd consider required for this is
* SQL scripts either work for CRDB too or alternative scripts are provided
* at least a subset of tests are run against CRDB in CI

#### Comments (1)

<details>
<summary><strong>navnit</strong> commented on 2023-06-06 05:57:33.000 UTC</summary>

It would be great if we could get the official cockroachdb support. Please consider.

</details>


---

### #1492: Optimize large payload fan-out to activities and child workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1492 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2021-04-25 18:24:29.000 UTC (4y 8m ago) |
| **Updated** | 2023-03-03 20:22:03.000 UTC |
| **Upvotes** | 12 |
| **Comments** | 1 |
| **Priority Score** | 25 |
| **Labels** | enhancement, devexp, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 12 |

#### Description

**Is your feature request related to a problem? Please describe.**
Some use cases need to pass the same payload to a large number of activities or child worklfows. In the current model, the payload will be repeated in the history as many times as the number of invocations. Recording the same payload many times is clearly nonoptimal.

**Describe the solution you'd like**
I don't have a specific API/feature proposal to resolve this issue. But I believe some solution that saves such a payload only once should be possible. 

One simplistic strawman is to have a special command to save a payload and then allow referencing it in other places of a workflow. 

Another idea is to perform payload deduplication by storing large payload hashes in the mutable state.

**Additional context**
A [forum post ](https://community.temporal.io/t/workflow-history-size-count-exceeds-limit/1999) that describes a customer having issues with a large payload passed to many child workflows.


#### Comments (1)

<details>
<summary><strong>mycroftcanner</strong> commented on 2023-01-15 00:50:13.000 UTC</summary>

I wanted to add that the idea of "payload deduplication by storing large payload hashes in the mutable state" mentioned in the GitHub issue reminds me of [Plan 9's Venti](http://doc.cat-v.org/plan_9/4th_edition/papers/venti/). This approach could also potentially enforce a write-once policy, preventing accidental or malicious destruction of data.


</details>


---

### #5811: Support last day of month as a recurring schedule option

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5811 |
| **State** | OPEN |
| **Author** | nnoto (Nicholas Noto) |
| **Created** | 2024-04-29 22:13:58.000 UTC (1y 8m ago) |
| **Updated** | 2025-09-16 17:27:51.000 UTC |
| **Upvotes** | 12 |
| **Comments** | 0 |
| **Priority Score** | 24 |
| **Labels** | enhancement, schedules |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 12 |

#### Description

**Is your feature request related to a problem? Please describe.**
Use cases may require that a schedule runs on the last day of the month every month. Because the last day of the month changes month to month, there isn't currently a temporal schedule configuration which supports this. 

**Describe the solution you'd like**
This requested support most likely pertains to calendar specs. In the context of the Temporal Python SDK ScheduleCalendarSpec, a day_of_month from 1-31 may be given. A value of 31 could match the last day of the month regardless of which month the schedule runs on, or some alternative value such as -1.

**Describe alternatives you've considered**
Assembling a list of calendar specs, one per month, where the last day of each month is explicitly provided. This does not work for a schedule that needs to run indefinitely.

**Additional context**
https://temporalio.slack.com/?redir=%2Farchives%2FCTT84KXK9%2Fp1710480826848909%3Fname%3DCTT84KXK9%26perma%3D1710480826848909


---

### #3056: Support wildcard search for workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3056 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2022-07-05 17:19:08.000 UTC (3y 6m ago) |
| **Updated** | 2023-03-03 20:19:37.000 UTC |
| **Upvotes** | 11 |
| **Comments** | 2 |
| **Priority Score** | 24 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 11 |

#### Description

To mirror issue in Cadence: https://github.com/uber/cadence/issues/3962 

#### Comments (2)

<details>
<summary><strong>lorensr</strong> commented on 2022-07-20 18:14:32.000 UTC</summary>

### I propose:

- Keep built-in Keyword search attributes as Keywords
- Allow wildcards and/or regexes in List Filters

### Background info

Summarizing comments from internal Slack:

- Wildcard search for built-in Keyword search attributes is a recurring request/need (like with a `WorkflowId` that is `<productId>-<uuid>`, looking it up by `productId`).
- Text type is more work for ES than Keyword ‚ÄîLiang
- Keyword supports these term-level queries: regexp and wildcard:
  - https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-wildcard-query.html
  - https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-regexp-query.html

> Use a keyword family field type if:
> - The content is machine-generated, such as a log message or HTTP request information.
> - You plan to search the field for exact full values, such as org.foo.bar, or partial character sequences, such as org.foo.*, using [term-level queries](https://www.elastic.co/guide/en/elasticsearch/reference/current/term-level-queries.html).

Note that the regexp and wildcard require `allow_expensive_queries: true`:

![image](https://user-images.githubusercontent.com/251288/180052465-d394664b-0bd7-4ea6-886a-26548c94b3fc.png)

### Alternative

Use custom Search Attributes. (This works fine, but it would be nicer from DX perspective if users didn't have to do this.)

Reactions: üëç 6

</details>

<details>
<summary><strong>lorensr</strong> commented on 2022-08-04 19:45:23.000 UTC</summary>

Another alternative:

I don't know how performant this is, but @mjameswh shared that BETWEEN works on Keywords, so for example:

`WorkflowId BETWEEN "myprojid1-" AND "myprojid1-zzzzzzzzzzz"`

would match `'myprojid1-randomalpha'`

Reactions: üëç 3 ‚ù§Ô∏è 1

</details>


---

### #2617: Allow retention period to be set per workflow completion type

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2617 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2022-03-17 23:46:13.000 UTC (3y 9m ago) |
| **Updated** | 2025-05-06 06:08:36.000 UTC |
| **Upvotes** | 9 |
| **Comments** | 6 |
| **Priority Score** | 24 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 9 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently setting retention period does not take in account workflow completion type. For example users would like to
set a different retention period for "Completed" vs "Failed" workflows.

**Describe the solution you'd like**
Allow to set workflow completion type when updating retention for a namespace in 
tctl namespace update --retention X



#### Comments (6)

<details>
<summary><strong>Ryan-Thomas</strong> commented on 2022-03-23 16:53:45.000 UTC</summary>

Main reason we want this is that we set a retention period of 7 days, but it would be very helpful to have a longer retention to debug failed workflows without needing to store all the successful ones for longer.

It was also suggested that you make it possible to set retention period as part of WorkflowOptions on each workflow. We run Temporal as a multitenant service and we wouldn't want a team hogging the service's resources by setting some extraordinarily long retention for their own workflows. If you decide to allow setting retention on WorkflowOptions, it would be great if there is some server level config option that lets us enforce that WorkflowOptions retention doesn't exceed the namespace level default retention.

We don't really need to be able to set retention period in WorkflowOptions, we are mainly just interested in setting retention by completion type.

</details>

<details>
<summary><strong>mxvsh</strong> commented on 2023-11-04 06:08:50.000 UTC</summary>

any update?

</details>

<details>
<summary><strong>mittalsuraj18</strong> commented on 2024-02-06 09:56:29.000 UTC</summary>

Hi, we too are interested in knowing if this can be done. 

</details>

<details>
<summary><strong>msainty</strong> commented on 2024-09-30 16:02:01.000 UTC</summary>

This would be interesting in regards to data governance e.g. some data you can keep for longer should it be related to something that is purchased.  However you may trigger a workflow and only want to retain this data for a few days as something was not purchased.  This flexibility would be very useful rather than having a set retention period for everything.

</details>

<details>
<summary><strong>yosefrev</strong> commented on 2024-10-17 21:51:25.000 UTC</summary>

Hi, this feature would be really helpful for us as well. Thanks!

</details>

<details>
<summary><strong>petrkoutnycz</strong> commented on 2025-05-06 06:07:21.000 UTC</summary>

I'm up for it as well, would be nice :-) the reasoning is very similar to sampling in tracing - it is usually desired to keep failed traces but keep only a sample of successful ones.

</details>


---

### #1428: Aim for error free rolling bounces and upgrades of Temporal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1428 |
| **State** | OPEN |
| **Author** | rfwagner (Richard Wagner) |
| **Created** | 2021-04-02 16:55:05.000 UTC (4y 9m ago) |
| **Updated** | 2024-10-10 15:02:55.000 UTC |
| **Upvotes** | 11 |
| **Comments** | 2 |
| **Priority Score** | 24 |
| **Labels** | enhancement, difficulty: hard, planning, operations |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 11 |

#### Description

Currently a rolling bounce or rolling upgrade of Temporal results in lots of error messages related to shard stealing. Error messages include:

```
Failed to update shard
Error updating ack level for shard
Error updating timer ack level for shard
```

These errors make it harder to identify real issues during a deployment and lead operators to tend to ignore these potentially important error messages.

Ideally, we should be able to do a rolling bounce or rolling upgrade of Temporal without encountering these errors on a routine basis.

One thing to keep in mind here is that Temporal is very often deployed using Kubernetes. Any solution should be compatible with the default Kubernetes Deployment RollingUpdate strategy.

#### Comments (2)

<details>
<summary><strong>tristan-instacart</strong> commented on 2021-06-23 17:52:43.000 UTC</summary>

I wonder if there is a way to throttle these error messages into warnings with a counter and then issue an error if the counter is exceeded? 
In practice that is what I have done with my monitoring systems for temporal, essentially ignoring a certain level of these errors during deploys established by looking at historical deploys. 

Another alternative might be to have some kind of knowledge that a shard rebalance or ringchange event is in progress and for a duration after that occurs drop these messages to a warning level. 

</details>

<details>
<summary><strong>bbemis017</strong> commented on 2024-10-10 14:54:40.000 UTC</summary>

We are seeing this issue a lot with hosting temporal in ECS. Typically on every deployment we see ~1,100 errors and 3 different error messages make up about 80% of those:

- Error updating queue state
- service failures
- failed reaching server: Frontend is not healthy yet

These errors tend to subside within 10 minutes of the deployment so I believe they are related to this rolling bounce issue with temporal.

We ultimately want to make sure that we have good alerting that can accurately alert us when there is an issue but currently the error logs are very noisy. Since this issue has been open for a few years I'm curious if there are any updates on it or if folks have found any workarounds for this?

Reactions: üëç 1

</details>


---

### #4386: Add new Workflow Id Reuse Policy: Allow Duplicate with Queueing

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4386 |
| **State** | OPEN |
| **Author** | jsjeannotte |
| **Created** | 2023-05-23 22:30:39.000 UTC (2y 7m ago) |
| **Updated** | 2025-11-06 20:05:38.000 UTC |
| **Upvotes** | 6 |
| **Comments** | 11 |
| **Priority Score** | 23 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 6 |

#### Description

**Is your feature request related to a problem? Please describe.**
N/A

**Describe the solution you'd like**
Currently, when a duplicate Workflow ID is submitted and the "Reject Duplicate" policy is used (default), the SDK raises an exception, which is the desired behavior. It would be useful to add a new reuse policy like "Queue Duplicate" where the workflow would be queued (with a new state like "Queued"). 

**Describe alternatives you've considered**
The alternative is to use the "master-workflow + signal + child-workflow" recipe, which reduces the observability (i.e. the UI can't be used to see what workflows are queued) and increases the design complexity for simple use-cases.

**Additional context**
N/A


#### Comments (11)

<details>
<summary><strong>samarabbas</strong> commented on 2023-10-12 05:21:27.000 UTC</summary>

@jsjeannotte can you provide more details about the use case?  You referenced `Reject Duplicate` which guarantees a single execution for that WorkflowID irrespective of the Close Status.  Why can't you use `Allow Duplicate` which would allow a new execution regardless of how the previous execution closed.  Or the scenario you have in mind is to allow queuing up multiple executions simultaneously and then process then sequentially one after another.  
Can you provide more details about the source where the StartWorkflowExecution requests are coming from?

</details>

<details>
<summary><strong>jsjeannotte</strong> commented on 2023-10-12 20:17:01.000 UTC</summary>

I can't use `Allow Duplicate` because I don't want to allow duplicates, but instead, want to queue them. We're trying to replicate a feature we leverage on Jenkins and our Spinnaker Pipelines where we can configure our executions to "Queue concurrent requests". Without that feature in Temporal, we'll have to build a queueing system ourselves and keep trying to submit new executions until Temporal completes the pending one. 

So yes, the scenario is exactly that: Allow queuing up multiple executions simultaneously and then process them sequentially one after another.

> Can you provide more details about the source where the StartWorkflowExecution requests are coming from?
Mostly from a user requesting a Workflow execution to be queued. 

Simplified fictional example: Let‚Äôs say we have a Temporal Workflow to perform an Offline maintenance on a Database node. Assume we can only have one Database node at a time. Assume the Workflow ID is "offline-replace-node-database-<name_of_database_here>". The user makes a request to trigger the Offline maintenance on Database A for Node X. All good. Another user makes a request to trigger an offline maintenance of Node Y on the same Database A. Since we use `Reject Duplicate`, the user gets a warning and the only options are to try again later.

Options for us are:
1) Build a queuing system so the second user receives an "All good, your request was queued" message instead of a `429` ...
2) Temporal supports `Queue Duplicate` so the second user receives the same "All good, your request was queued" response, but we didn't have to build a queuing system :)



</details>

<details>
<summary><strong>samarabbas</strong> commented on 2023-10-12 23:19:19.000 UTC</summary>

Have you considered modeling this using SignalWithStart?  Basically the idea would be any operation on a Node will be communicated to a workflow through a signal.  SignalWithStart will allow a workflow execution to be created if none exists.  If a workflow execution already exists and there is an operation in flight you just queue up new operation within the workflow itself.  I might actual try and build a sample which showcases this approach.  Basically you are building a serialization mechanism for a resource, and doing it within a single execution is much simpler than spreading it across multiple executions.

I also want to clarify some confusions around `Allow Duplicate`. 

- If no policy is specified then the default should be `Allow Duplicate` instead of `Reject Duplicate`.  Here are the [docs](https://docs.temporal.io/workflows#workflow-id-reuse-policy).
- Can you clarify what you meant by this comment `because I don't want to allow duplicates, but instead, want to queue them`?  From my understanding of the ticket, you want even a more loose version of `Allow Duplicate` where we allow another execution (queue'ed up) when there is already one in flight.  Where today `Allow Duplicate` only allows the execution when the current one finishes.





</details>

<details>
<summary><strong>jsjeannotte</strong> commented on 2023-10-13 20:12:13.000 UTC</summary>

Yes, I often get confused about `Allow Duplicate` vs `Reject Duplicate` and the fact that `Allow Duplicate` actually means: `Allow Duplicate but fail if that workflow ID is still running`. So yes, it would be more like `Allow Duplicate with Queueing` (loose version of `Allow Duplicate`).

> Have you considered modeling this using SignalWithStart?
I haven't. Having an example might help me wrap my head around what you are suggesting indeed. 

For example, you mention that "you just queue up a new operation ..." which if I understand correctly means that I still need to build and maintain queues right? 

I also failed to mention that I provide a platform that includes Temporal as a way for our users to write their Ops automation (and more) and so having the simplest interface possible helps them onboard to Temporal (for example, not having to understand Signals or Long Running Workflow with ContinueAsNew for doing very basic things). A lot of their use-cases would be solved by a single activity wrapped in a workflow (since these use-cases are each using a single Jenkins job running a single Python script). 

We've even build an abstraction that allows them to wrap a single Python function into a Schedule + Workflow + Activity so that for extremely simple use-cases, our users don't even have to understand how Temporal works:

The user only writes this:
```
register_periodic_worker(
        PeriodicWorker(
            name="demo_test_hourly_with_arg",
            interval=timedelta(hours=1),
            start_to_close_timeout=timedelta(minutes=5),
            task=partial(test_callable_with_arg, "param1"),
            maximum_attempts=3,
        )
 ```
 
 So having the ability for them to do something like:
 ```
 register_queued_worker(
        QueuedWorker(
            name="demo_test_hourly_with_arg",
            start_to_close_timeout=timedelta(minutes=5),
            task=partial(test_callable_with_arg, "param1"),
            maximum_attempts=3,
        )
```
... which would configure the Workflow Id Reuse Policy to `Allow Duplicate with Queuing`, would be extremely useful.

</details>

<details>
<summary><strong>mjameswh</strong> commented on 2023-10-16 18:40:18.000 UTC</summary>

First thing first: you mention that these Workflows only get started from Schedules. If that's correct, then the easiest way to serialize Workflow execution so that no more than one execution is running at any time would be to simply set that schedule's `policy.overlap` option to `ScheduleOverlapPolicy.BUFFER_ALL` ([see docs](https://python.temporal.io/temporalio.client.ScheduleOverlapPolicy.html#BUFFER_ALL)).

Now, I understand this might not be sufficient for your needs, as the Schedules API currently provides no way to inspect buffered executions. I opened a feature request for this [here](https://github.com/temporalio/temporal/issues/4984).

Regarding implementing execution queuing by yourself, I would generally recommend the approach mentioned by Samar (that is a single workflow on which you do `signalWithStart`, and everything happens in that unique workflow, or in child workflows started by that single workflow), but I again understand that this doesn't cover some of your needs.

Instead, it may make sense to reverse this pattern: start a different workflow execution for each task, then have each workflow do `signalWithStart` on a controller Workflow, and wait for the controller to signal back when its ok to proceed. That obviously means the task workflow need to signal the controller again when it completes. That approach adds some of overhead compared to the single workflow pattern described previously, but that overhead pays off in improved visibility, as queued tasks are now visible in workflow listing.

For example, you could have something like this:

```
Workflow Id                            Workflow Type                Status
-------------------------------------  ---------------------------  ---------
replace-database-mydb-20231012-045623  ReplaceNodeDatabaseWorkflow  Completed
replace-database-mydb-20231012-051276  ReplaceNodeDatabaseWorkflow  Running...
replace-database-mydb-20231012-051276  ReplaceNodeDatabaseWorkflow  Running...
replace-database-mydb-20231012-064712  ReplaceNodeDatabaseWorkflow  Running...
replace-database-mydb                  OneAtATimeCoordinator        Running...
```

This also makes it possible for users to interact directly with task Workflows, so they can for example cancel a queued execution, or inspect result/history of a specific completed task. This pattern also works better with Schedules than the previous suggestion.

To avoid making it harder for your users to write their own workflows, you may easily extract that coordination work (ie. `signalWithStart` the coordination workflow, wait for a signal from it, and sent it back an unlock signal once the task workflow completes), for example by moving this to a Workflow interceptor, having them wrapping their own workflow code into some wrapper function, or using the dynamic Workflow feature.

Does that make sense to you?

</details>

<details>
<summary><strong>jsjeannotte</strong> commented on 2023-10-26 20:34:50.000 UTC</summary>

> First thing first: you mention that these Workflows only get started from Schedules

No. Could be on a Schedule, or could be on-demand.

> Instead, it may make sense to reverse this pattern: start a different workflow execution for each task, then have each workflow do signalWithStart on a controller Workflow, and wait for the controller to signal back when its ok to proceed. That obviously means the task workflow need to signal the controller again when it completes. That approach adds some of overhead compared to the single workflow pattern described previously, but that overhead pays off in improved visibility, as queued tasks are now visible in workflow listing.

That's something I was thinking about this week :) This would indeed help with visibility. And it might be easier to extract as a building block. I'll play with this a bit.

Thanks all! But again, would still appreciate if `Allow Duplicate with Queueing` was supported ;) 


</details>

<details>
<summary><strong>jsjeannotte</strong> commented on 2023-10-26 20:36:51.000 UTC</summary>

@mjameswh By the way, I'm also from Montreal :)

Reactions: üéâ 1

</details>

<details>
<summary><strong>MrTravisB</strong> commented on 2025-03-07 19:19:30.000 UTC</summary>

Bump this.

Our use case for this is we are processing messages from users. Like conversations. But processing one messages requires context from all previous messages. So we need each user (or thread) to be processed serially. Would be nice if we could just use the user/thread id as the workflow id and queue to that.

This needs to work for millions of users (and growing).

</details>

<details>
<summary><strong>paul-paliychuk</strong> commented on 2025-03-10 16:43:10.000 UTC</summary>

+1

</details>

<details>
<summary><strong>hippasus</strong> commented on 2025-07-17 22:18:07.000 UTC</summary>

+1

</details>

<details>
<summary><strong>savic1231</strong> commented on 2025-11-06 20:05:38.000 UTC</summary>

+1

</details>


---

### #130: Add cron activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/130 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-02-09 19:35:47.000 UTC (5y 10m ago) |
| **Updated** | 2023-03-03 20:23:27.000 UTC |
| **Upvotes** | 6 |
| **Comments** | 11 |
| **Priority Score** | 23 |
| **Labels** | enhancement, CRON |
| **Assignees** | dnr |
| **Milestone** | None |
| **Reactions** | üëç 6 |

#### Description

Currently a workflow can be started with cron option. When specified the workflow on completion is not going to close, but schedule a next run according to schedule. This is very useful for modeling periodic jobs.

For variety of use cases (like polling for a result) it would be nice to specify cron on an activity. This would re execute the activity upon its completion. Another option is to support an ability to schedule the next invocation from the activity itself. Something like:
`
Activity.executeAgaintIn(Duration.ofMinutes(10));
`


#### Comments (11)

<details>
<summary><strong>hazcod</strong> commented on 2020-04-09 05:58:35.000 UTC</summary>

Especially useful if we could trigger the workflow cron  immediately, regardless of cron schedule.

</details>

<details>
<summary><strong>mfateev</strong> commented on 2020-04-09 19:05:32.000 UTC</summary>

@hazcod You can currently do it by sending a signal to a cron workflow which is waiting for its time to run.

</details>

<details>
<summary><strong>hazcod</strong> commented on 2020-04-09 19:39:58.000 UTC</summary>

@mfateev Hi maxim, do you mean via `SignalWithStart`? Or a `workflow.NewSelector(ctx).AddReceive` at the start of my workflow?

</details>

<details>
<summary><strong>mfateev</strong> commented on 2020-04-10 00:54:11.000 UTC</summary>

It looks like I don't understand your question. I thought you asked how to force immediate execution of a cron workflow which is blocked waiting for its time to run. For such workflow sending normal signal to it from outside will force an immediate execution.

If you are asking about how to start cron workflow which is not running in non cron mode, then just don't pass cron parameter to the start method.


</details>

<details>
<summary><strong>hazcod</strong> commented on 2020-04-10 06:50:06.000 UTC</summary>

@mfateev I have a workflow that I schedule via CronSchedule (later) indeed.
But how would a signal work here? Can you give an example?
It's unclear to me since the workflow wouldn't be running, right?

</details>

<details>
<summary><strong>mfateev</strong> commented on 2020-04-10 17:27:10.000 UTC</summary>

You don't need to handle signal inside the workflow code. It would just force the server to start executing workflow code. 
The tricky part is that the cron workflow instance exists even when it is waiting to execute in the future. So you can signal it using normal signalling techniques.

</details>

<details>
<summary><strong>hazcod</strong> commented on 2020-04-11 13:13:46.000 UTC</summary>

Could you give an example on how to signal a cron workflow to immediately start executing? I don‚Äôt quite see how. I‚Äôm looking at client.SignalWithStartWorkflow , would I just send a placeholder signal to just start it? (edited) 

</details>

<details>
<summary><strong>mfateev</strong> commented on 2020-04-11 21:47:29.000 UTC</summary>

Use client.SignalWorkflow to wake up already exising cron workflow which is waiting for its time to run.

</details>

<details>
<summary><strong>hazcod</strong> commented on 2020-04-12 10:09:02.000 UTC</summary>

@mfateev For the `WorkflowOptions` argument in `SignalWithStartWorkflow`, can that be the same as the cron workflow, thus containing the CronSchedule?
Or should it not contain ChronSchedule to execute immediately?

</details>

<details>
<summary><strong>mfateev</strong> commented on 2020-04-12 14:49:36.000 UTC</summary>

If you just want to execute it immediately one time call StartWorkflow without cron schedule.

</details>

<details>
<summary><strong>simararneja</strong> commented on 2020-10-20 10:39:25.000 UTC</summary>

@mfateev SignalWorkflow only seems to accept typed workflow stub, is there a  way to start the untyped ones ?

</details>


---

### #4383: Support log-less graceful shutdown without "Error looking up host for shardID" errors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4383 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-05-22 21:10:40.000 UTC (2y 7m ago) |
| **Updated** | 2024-01-05 17:31:15.000 UTC |
| **Upvotes** | 10 |
| **Comments** | 2 |
| **Priority Score** | 22 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 10 |

#### Description

**Is your feature request related to a problem? Please describe.**

Today shutting down a single-binary server gives something like:

```
2023-05-11T14:45:49.683-0700	ERROR	Error looking up host for shardID	{"component": "shard-controller", "address": "127.0.0.1:33401", "error": "Not enough hosts to serve the request", "operation-result": "OperationFailed", "shard-id": 1, "logging-call-at": "controller_impl.go:387"}
go.temporal.io/server/common/log.(*zapLogger).Error
	/home/runner/go/pkg/mod/go.temporal.io/server@v1.20.1/common/log/zap_logger.go:150
go.temporal.io/server/service/history/shard.(*ControllerImpl).acquireShards.func2
	/home/runner/go/pkg/mod/go.temporal.io/server@v1.20.1/service/history/shard/controller_impl.go:387
go.temporal.io/server/service/history/shard.(*ControllerImpl).acquireShards.func3
	/home/runner/go/pkg/mod/go.temporal.io/server@v1.20.1/service/history/shard/controller_impl.go:427
2023-05-11T14:45:50.685-0700	WARN	Failed to poll for task.	{"service": "worker", "Namespace": "temporal-system", "TaskQueue": "temporal-sys-tq-scanner-taskqueue-0", "WorkerID": "431854@monolith@", "WorkerType": "WorkflowWorker", "Error": "error reading from server: EOF", "logging-call-at": "internal_worker_base.go:308"}
2023-05-11T14:45:50.685-0700	WARN	Failed to poll for task.	{"service": "worker", "Namespace": "temporal-system", "TaskQueue": "temporal-sys-processor-parent-close-policy", "WorkerID": "431854@monolith@", "WorkerType": "WorkflowWorker", "Error": "error reading from server: EOF", "logging-call-at": "internal_worker_base.go:308"}
2023-05-11T14:45:50.685-0700	WARN	Failed to poll for task.	{"service": "worker", "Namespace": "temporal-system", "TaskQueue": "temporal-sys-history-scanner-taskqueue-0", "WorkerID": "431854@monolith@", "WorkerType": "ActivityWorker", "Error": "error reading from server: EOF", "logging-call-at": "internal_worker_base.go:308"}
2023-05-11T14:45:50.685-0700	WARN	Failed to poll for task.	{"service": "worker", "Namespace": "temporal-system", "TaskQueue": "temporal-sys-tq-scanner-taskqueue-0", "WorkerID": "431854@monolith@", "WorkerType": "ActivityWorker", "Error": "error reading from server: EOF", "logging-call-at": "internal_worker_base.go:308"}
2023-05-11T14:45:50.685-0700	WARN	Failed to poll for task.	{"service": "worker", "Namespace": "temporal-system", "TaskQueue": "temporal-sys-batcher-taskqueue", "WorkerID": "431854@monolith@", "WorkerType": "ActivityWorker", "Error": "error reading from server: EOF", "logging-call-at": "internal_worker_base.go:308"}
2023-05-11T14:45:50.685-0700	WARN	Failed to poll for task.	{"service": "worker", "Namespace": "temporal-system", "TaskQueue": "temporal-sys-history-scanner-taskqueue-0", "WorkerID": "431854@monolith@", "WorkerType": "WorkflowWorker", "Error": "error reading from server: EOF", "logging-call-at": "internal_worker_base.go:308"}
```

**Describe the solution you'd like**

Any solution that does not make a user think there is an error when they see logs. If this needs to be only half-done here and then something done at https://github.com/temporalio/cli to properly shutdown or swallow or something, no prob.

#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2023-10-05 12:21:23.000 UTC</summary>

Also, sometimes you get:

```
2023-10-05T11:58:24.496+0300	ERROR	Unable to call matching.PollActivityTaskQueue.	{"service": "frontend", "wf-task-queue-name": "/_sys/temporal-sys-history-scanner-taskqueue-0/2", "timeout": "1m9.999694s", "error": "error reading from server: EOF", "logging-call-at": "workflow_handler.go:1133"}
```

With a full trace.

</details>

<details>
<summary><strong>garrettks</strong> commented on 2024-01-05 17:31:14.000 UTC</summary>

And by "full trace", @cretz means a V E R Y full trace üòÖ

</details>


---

### #685: Delay workflow completion until abandoned children start 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/685 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-08-24 16:04:45.000 UTC (5y 4m ago) |
| **Updated** | 2025-08-07 14:25:45.000 UTC |
| **Upvotes** | 9 |
| **Comments** | 4 |
| **Priority Score** | 22 |
| **Labels** | enhancement |
| **Assignees** | yycptt |
| **Milestone** | None |
| **Reactions** | üëç 9 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, if a parent workflow exits before a child workflow has started the child is not going to start at all. It is possible to resolve by waiting for a Promise returned from ChildWorkflowStub.getExecution(). But this is a pretty obscure and error-prone feature.

**Describe the solution you'd like**
The proposal is to automatically delay workflow completion if there is an outstanding start child request that has abandon parent policy until the child reported that it started successfully. It can be done by failing the decision task and scheduling it after children have started. 

**Describe alternatives you've considered**
I tried implementing it at the SDK level. The problem was dealing with new events (like signals) received after the workflow code has completed. 

**Additional context**
The [original SDK issue](https://github.com/temporalio/java-sdk/issues/74).

#### Comments (4)

<details>
<summary><strong>shaharco</strong> commented on 2022-12-15 11:14:45.000 UTC</summary>

Hi, any plans for solving this soon?

</details>

<details>
<summary><strong>nitesh237</strong> commented on 2023-05-17 11:47:47.000 UTC</summary>

Hi @yycptt @yiminc 

I was looking for an opportunity to contribute, this seems like a good starting point (judging by the labels). Let me know if this is yet to be picked. I would love to work on this issue. 

</details>

<details>
<summary><strong>yycptt</strong> commented on 2023-05-17 19:29:31.000 UTC</summary>

@yiminc Do we want to implement what's proposed in the issue as a quick fix (for 1.21)? I am a little hesitant because this involves in workflow task fail & retry, which means extended workflow start to close timeout (and all related issues for example query and buffered events) & sdk needs to do workflow history replay from very beginning.

Eventually (like in 1-2 month, and for 1.22 release I think) we need to change existing start child workflow logic to allow child workflow to start when parent is closed (which is a much more complex change) 

</details>

<details>
<summary><strong>wdsouza-mn</strong> commented on 2025-08-07 14:25:45.000 UTC</summary>

Any updates on fixing this issue? 



Reactions: üëç 1

</details>


---

### #3709: Workflows/activities to post event notifications that could be listened to

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3709 |
| **State** | OPEN |
| **Author** | sstro |
| **Created** | 2022-12-14 08:28:36.000 UTC (3 years ago) |
| **Updated** | 2023-12-20 18:36:46.000 UTC |
| **Upvotes** | 8 |
| **Comments** | 5 |
| **Priority Score** | 21 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 8 |

#### Description

We would like to listen to Temporal workflow/activity lifecycle events (e.g. _WorkflowExecutionStarted_, _ActivityTaskStarted_, _ActivityTaskCompleted_), apply some filtering and transformation and then persist the events to our own database. Business users could query this database directly instead of querying Temporal and transforming/aggregating information at the runtime.

An additional requirement would be ability to distinguish workflow / activity replay / retry events from "first execution" events.

It would be nice if it were possible to attach some kind of listener to a workflow / an activity at the worker level.

We've considered workflow/activity inbound/outbound call interceptors, however, they only seem to fire on a subset of events (start/finish). Additionally, workflow interceptors do not distinguish first start and replay events.

We would be interested in having the functionality available in Java SDK.

Related discussions in the support forum: 
- https://community.temporal.io/t/how-to-watch-all-workflows-events-inside-a-namespace/4658
- https://community.temporal.io/t/way-to-listen-on-workflow-lifecycle-events/5887
- https://community.temporal.io/t/consume-temporal-io-events/1630
- https://community.temporal.io/t/can-i-attach-a-listener-to-workflow-execution/4275


#### Comments (5)

<details>
<summary><strong>mindaugasrukas</strong> commented on 2022-12-14 17:06:44.000 UTC</summary>

Is this a use case for having the ability to create a custom UI (user interface)?

</details>

<details>
<summary><strong>sstro</strong> commented on 2022-12-15 08:57:03.000 UTC</summary>

Not exactly, it's for integration with our own process information system.

</details>

<details>
<summary><strong>yiminc</strong> commented on 2022-12-16 22:51:28.000 UTC</summary>

We have 2 options currently available:
1) use interceptor to intercept workflow/activity, example  https://github.com/temporalio/samples-go/tree/main/interceptor
2) intercept calls using a proxy, example: https://github.com/temporalio/samples-go/tree/main/grpc-proxy


</details>

<details>
<summary><strong>sstro</strong> commented on 2022-12-19 09:02:28.000 UTC</summary>

We've considered workflow/activity inbound/outbound call interceptors, however, they only seem to fire on a subset of events (start/finish). Additionally, workflow interceptors do not distinguish first start and replay events, which is important for our use case.

Is the proxy supported in Java SDK ?

</details>

<details>
<summary><strong>Quinn-With-Two-Ns</strong> commented on 2023-12-20 18:36:45.000 UTC</summary>

Proxy and interceptors are not a complete solution for this issue since they are not invoked on server side workflow completions like timeout or termination.

</details>


---

### #3383: Schema name selection for postgres

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3383 |
| **State** | OPEN |
| **Author** | vinodnaidu02 |
| **Created** | 2022-09-14 20:15:35.000 UTC (3y 3m ago) |
| **Updated** | 2025-03-21 14:52:59.000 UTC |
| **Upvotes** | 4 |
| **Comments** | 13 |
| **Priority Score** | 21 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 4 |

#### Description

**Is your feature request related to a problem? Please describe.**
Trying to set schema namespace which temporal has to use but not able to set using sql-tool.

**Describe the solution you'd like**
based on the search path selection, temporal has to use that schema.


#### Comments (13)

<details>
<summary><strong>alexshtin</strong> commented on 2022-09-17 00:51:22.000 UTC</summary>

Sounds like a good idea. Are you willing to contribute? 
Schema file is here: https://github.com/temporalio/temporal/blob/master/schema/postgresql/v96/temporal/schema.sql
Queries are here:
https://github.com/temporalio/temporal/tree/master/common/persistence/sql/sqlplugin/postgresql (almost in every file).

Visibility schema/queries are also needs to be updated.

</details>

<details>
<summary><strong>vinodnaidu02</strong> commented on 2022-09-17 04:49:00.000 UTC</summary>

Hi Alex

I am able to set schema while using sql-tool. I used the below command while installing. The only problem is there is no documentation that explains how to use search_path,

```
 SQL_DATABASE=temporal ./temporal-sql-tool --ca search_path=temporal setup-schema -v 0.0
```
 
The main issue I faced was configuring the temporal server to use this schema, I also found a vague solution to fix this. still I am not very sure if this is proper,

```
connectAttributes: 
                    search_path:  "{{ default .Env.SCHEMA_NAME "temporal" }}"
```

The above config value is part of Mysql config_template, however, for Postgres, there is no option. By adding this line to both temporal and visibility databases in the Postgres part, the temporal server is connecting to the respective schema.


Please let me know if you want I can do these changes(Hopefully my approach is correct).
                 


</details>

<details>
<summary><strong>anjulaendowus</strong> commented on 2023-02-09 08:00:53.000 UTC</summary>

@vinodnaidu02 Anything changed with this approach? Im facing this same problem.

</details>

<details>
<summary><strong>vinodnaidu02</strong> commented on 2023-02-09 09:29:37.000 UTC</summary>

@anjulaendowus  I am still using the above approach I mentioned. I created a docker image based on the above config which accepts schema ENV and uses that while connecting. 

</details>

<details>
<summary><strong>anjulaendowus</strong> commented on 2023-02-09 09:38:41.000 UTC</summary>

@vinodnaidu02 DId u use this command? 
`SQL_DATABASE=temporal_visibility ./temporal-sql-tool update -schema-dir schema/postgresql/v96/visibility/versioned`
did you pass the --ca flag as well for this? Does this work with postgres? Thanks for the reply

</details>

<details>
<summary><strong>vinodnaidu02</strong> commented on 2023-02-09 09:45:23.000 UTC</summary>

Yes I did use the command and pass the --ca flag and it is working with the Postgres. Not sure if something has changed recently.

</details>

<details>
<summary><strong>anjulaendowus</strong> commented on 2023-02-09 10:08:55.000 UTC</summary>

Did u manually create the schema @vinodnaidu02 ?


</details>

<details>
<summary><strong>vinodnaidu02</strong> commented on 2023-02-09 10:24:58.000 UTC</summary>

Yes, I manually created the schema inside DB before temporal starts.

Reactions: üëç 1

</details>

<details>
<summary><strong>chlunde</strong> commented on 2024-03-04 10:27:33.000 UTC</summary>

https://www.percona.com/blog/public-schema-security-upgrade-in-postgresql-15/

So I think we need a  `CREATE SCHEMA IF NOT EXISTS schema_name` (which is manual today), in addition to the `search_path` (which is possible to set, but only as an attribute which means that temporal doesn't really know what's going on).

@alexshtin do you have any input on how to implement this? I'm not familiar with this code base, so I wonder what's the natural place to:
1. Define the schema name as a property (for templating the above query below and possibly setting search_path automatically)
2. Put the CREATE statement. Maybe after `CREATE DATABASE` (but I would like an `IF NOT EXISTS` on that too, because the database provider might have created the db but not the schema, or neither db and schema).


</details>

<details>
<summary><strong>bkcsfi</strong> commented on 2024-03-06 03:42:53.000 UTC</summary>

I encountered a problem today with temporal-sql-tool not working with postgres when the database already exists and the db user is NOT a superuser.

I am using temporal-operator in conjunction with cloudnative-pg. The later handles database creation, replication and lifecycle operations. Therefore the database already exists by the time the `temporal-sql-tool create` operation is executed by the temporal-operator.

In addition, cloudnative-pg no longer creates users with superuser access. 

The temporal-sql-tool postgres module checks for "duplicate database" error AFTER attempting to create the database. However since the user does not  have superuser access, temporal-sql-tool create fails with `pq: permission denied to create database`

Is this a reasonable issue on which to graft "check for existing database before attempting to create"?

If that change were implemented, its possible that a non-superuser database account would work properly with temporal-sql-tool.   (though I don't know what roles the schema upgrade steps require).





</details>

<details>
<summary><strong>hansliu</strong> commented on 2024-06-17 15:39:28.000 UTC</summary>

Could we add the **connectAttributes: search_path** to the default [config_template.yaml](https://github.com/temporalio/temporal/blob/main/docker/config_template.yaml)?

Or how to override the config_template.yaml from docker hub temporalio/server image?

</details>

<details>
<summary><strong>enricojonas</strong> commented on 2024-11-29 15:28:26.000 UTC</summary>

We would also like a custom schema name, either automatically created or created beforehand.

</details>

<details>
<summary><strong>justinrixx</strong> commented on 2025-03-21 14:50:07.000 UTC</summary>

i'd like this ability as well.

one workaround that seems to work in an internal test is to create a new schema and set the search path to the new schema:

```
CREATE SCHEMA <schema_name>;
ALTER ROLE <admin_user> SET search_path TO <schema_name>, '$user', public;
```

this works as long as temporal doesn't reference the `public` schema directly, but rather continues to use the default search path. like i said, this appears to work in an internal test, but i'm hesitant to rely on it without confirmation from the project that this contract won't be broken later.

edit: fwiw i'm using the helm chart

</details>


---

### #2941: Host level cache for history

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2941 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2022-06-02 23:38:11.000 UTC (3y 7m ago) |
| **Updated** | 2024-09-20 16:50:08.000 UTC |
| **Upvotes** | 9 |
| **Comments** | 3 |
| **Priority Score** | 21 |
| **Labels** | potential-bug |
| **Assignees** | prathyushpv |
| **Milestone** | None |
| **Reactions** | üëç 9 |

#### Description

History uses mutable state cache and history event cache, and the caches are per shard, this is not efficient and hard to config.

We should make cache host level that is shared among all shards hosted on that host.

The dynamic config for cache size is entry count, but it is not easy to estimate the average entry size and make it harder to config. We should change the config to be byte size so cache would take up to the configured memory byte size. 
Here are the current dynamic configs for cache size:
`history.cacheInitialSize` default 128
`history.cacheMaxSize` default 512
`history.eventsCacheInitialSize` default 128
`history.eventsCacheMaxSize` default 512

We should also emit a gauge metric that measures the cache size in use. We already have the cache_hit and cache_miss, which we should keep after upgrade to host level.



#### Comments (3)

<details>
<summary><strong>danthegoodman1</strong> commented on 2023-08-22 14:33:30.000 UTC</summary>

yes please!

</details>

<details>
<summary><strong>emmercm</strong> commented on 2024-09-20 15:49:52.000 UTC</summary>

I believe this was made the default in v1.25.0: https://github.com/temporalio/temporal/pull/5894

What is the earliest version that I can enable `history.enableHostHistoryCache = true` with? I couldn't find it in the release notes. This commit leads me to believe v1.23.0: https://github.com/temporalio/temporal/commit/7b146c22af13ef6101db6252733ab3ed10eb881d

</details>

<details>
<summary><strong>yycptt</strong> commented on 2024-09-20 16:49:55.000 UTC</summary>

cc @prathyushpv 

</details>


---

### #4233: Too high memory usage on history service 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4233 |
| **State** | OPEN |
| **Author** | koolay (koolay) |
| **Created** | 2023-04-26 10:25:46.000 UTC (2y 8m ago) |
| **Updated** | 2025-10-22 11:07:10.000 UTC |
| **Upvotes** | 6 |
| **Comments** | 7 |
| **Priority Score** | 19 |
| **Labels** | potential-bug |
| **Assignees** | prathyushpv |
| **Milestone** | None |
| **Reactions** | üëç 6 |

#### Description

## Expected Behavior
Require less memory usage.

## Actual Behavior

container resources:

![image](https://user-images.githubusercontent.com/4273490/234535219-0a0f3858-bb33-4202-8726-46fa96059027.png)

server metrics:

![image](https://user-images.githubusercontent.com/4273490/234550779-036d1636-bfd4-4308-b29d-98000fccfefa.png)


go tool pprof "http://localhost:7936/debug/pprof/heap"

![image](https://user-images.githubusercontent.com/4273490/234534796-fa97c39c-64ed-49d8-94bd-f1150712fa30.png)


## Steps to Reproduce the Problem

  1.  create one thousand workflows

  `temporal-dynamic-config`

 ```yaml
apiVersion: v1
data:
  dynamic_config.yaml: |-
    history.cacheInitialSize:
    - value: "10"
    history.cacheMaxSize:
    - value: "50"
    history.cacheTTL:
    - value: "0.5"
    history.eventsCacheInitialSize:
    - value: "10"
    history.eventsCacheMaxSize:
    - value: "50"
    matching.useOldRouting:
    - value: "false"
kind: ConfigMap
metadata:
  annotations:
    meta.helm.sh/release-name: temporal
    meta.helm.sh/release-namespace: temporal
  creationTimestamp: "2022-11-09T04:02:55Z"
  labels:
    app.kubernetes.io/instance: temporal
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: temporal
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/version: 1.20.2
    helm.sh/chart: temporal-0.20.2
  name: temporal-dynamic-config
  namespace: temporal
  resourceVersion: "64727569"
  uid: 01925a28-4772-42e9-891e-bdb467bf496f
```

## Specifications

  - Version: temporalio/server:1.20.2
  - Platform: k8s


#### Comments (7)

<details>
<summary><strong>leididi</strong> commented on 2023-08-10 02:19:53.000 UTC</summary>

encountered the same problemÔºõ

```
Name:         temporal-dynamic-config
Namespace:    temporal
Labels:       app.kubernetes.io/instance=temporal
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/name=temporal
              app.kubernetes.io/part-of=temporal
              app.kubernetes.io/version=1.20.2
              helm.sh/chart=temporal-0.20.2
Annotations:  meta.helm.sh/release-name: temporal
              meta.helm.sh/release-namespace: temporal

Data
====
dynamic_config.yaml:
----
history.cacheInitialSize:
- value: "10"
history.cacheMaxSize:
- value: "50"
history.cacheTTL:
- value: "0.1"
history.eventsCacheInitialSize:
- value: "10"
history.eventsCacheMaxSize:
- value: "50"
matching.useOldRouting:
- value: "false"
```


![image](https://github.com/temporalio/temporal/assets/34613810/52cf6d6b-8810-44cf-b892-549ab9719bd4)


</details>

<details>
<summary><strong>trangtran-ct</strong> commented on 2023-12-24 04:01:22.000 UTC</summary>

Hi, we experienced the same problem. Any update on this? Thanks a lot.

</details>

<details>
<summary><strong>DhirajBhakta</strong> commented on 2024-05-27 01:30:27.000 UTC</summary>

Noticing the same issue. Could you help us understand if the history service detects available memory(available to the pod) and automatically set max cache size to available value?

</details>

<details>
<summary><strong>vikrant-shetty-s1</strong> commented on 2024-12-20 09:13:50.000 UTC</summary>

Is there any progress on this issue?

</details>

<details>
<summary><strong>danielserrao</strong> commented on 2025-01-07 13:25:28.000 UTC</summary>

I'm experiencing the same issue.

</details>

<details>
<summary><strong>szbauer</strong> commented on 2025-10-07 19:53:35.000 UTC</summary>

I see there was an MR for this, but now it's been marked as stale, is there any intention of getting this resolved? 

</details>

<details>
<summary><strong>unleftie</strong> commented on 2025-10-22 11:07:10.000 UTC</summary>

@prathyushpv hi, could you please take a look at the mentioned PR? I've noticed a similar behavior with high RAM usage

</details>


---

### #3006: Workflow Pause / Unpause

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3006 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2022-06-17 19:29:21.000 UTC (3y 6m ago) |
| **Updated** | 2025-10-21 06:32:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 18 |
| **Priority Score** | 18 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ‚ù§Ô∏è 61 |

#### Description

There are many uses cases where the ability to pause a workflow would be very much appreciated. 

Pause a workflow would mean no more workflow task would be scheduled for that workflow. The unpause would mean create new workflow task if one is needed but not present because it was paused. The tricky part is to replicate paused state to passive side.

We can have a default pause policy that apply to workflows that experience continued workflow task failure, for example due to bug in workflow code.

We could also enable pausing on activity failure as part of retry policy. 

Paused workflow should be visible through visibility API, and some kind of batch operation to support unpause them on demand.


#### Comments (18)

<details>
<summary><strong>Zonalds</strong> commented on 2022-11-25 09:40:26.000 UTC</summary>

+1 

</details>

<details>
<summary><strong>peiminmin</strong> commented on 2022-12-19 09:32:01.000 UTC</summary>

+1

</details>

<details>
<summary><strong>Zonalds</strong> commented on 2023-03-04 06:48:07.000 UTC</summary>

Any changes to this? 

</details>

<details>
<summary><strong>NarmathaBala</strong> commented on 2023-05-05 19:12:17.000 UTC</summary>

@yiminc We have a couple of engineers on our team looking to contribute to this feature and would like to understand if this is  work-in-progress. We would love see how we can collaborate with you on this feature

@mfateev 

Reactions: üëç 2

</details>

<details>
<summary><strong>gmintoco</strong> commented on 2023-06-20 13:09:15.000 UTC</summary>

This would be a really nice feature for us as well. In my mind "pause" would suspend all timeouts and prevent any progression through the workflow code (preventing retries of any current activities as well).

This is especially useful in scenarios where a workflow encounters errors and we would like to push a fix to the worker code without worrying about timeouts on the workflow (basically we can prevent the workflow timing out and cleaning up after itself, usually this can save use recomputing anything etc.).

Perhaps the issues with this could be mitigated by us adopting a different architecture. But currently our workflow provision infrastructure for themselves to run computationally heavy work on (in particular with GPUs). Ideally we don't want to waste this work by having our workflow timeout. There are ways for us to work around this of course (saving work to S3, recovery workflows and methods to resume etc.) but in general it would be much cleaner if we would simply pause a workflow that is encountering errors, scale down the compute (IE. scale the ECS service to 0), debug and fix the problem, push the change and update the service and scale back up again.


</details>

<details>
<summary><strong>Gabrn</strong> commented on 2023-12-24 08:59:54.000 UTC</summary>

Does a simpler pause mechanism exist?
I don't mind if timeouts are not paused as well, e.g if there is an Activity that is supposed to run in 5 minutes and the workflow is paused for 5 minutes I don't mind if the activity runs as soon as the workflow is unpaused. Is this possible with the current framework?

</details>

<details>
<summary><strong>prithage</strong> commented on 2024-04-04 08:53:35.000 UTC</summary>

Hi @yiminc ,

Do you have any update or is there a development timeline decided on this?

We could really use this feature and any update on the timeline or a tentative release date helps a lot.

Reactions: üëç 3

</details>

<details>
<summary><strong>dimankus</strong> commented on 2024-05-23 16:59:57.000 UTC</summary>

+1
not sure if the op covers this
but looking at a use-case to pause a workflow to stop activities from retrying (for example if there is a known service interruption)

</details>

<details>
<summary><strong>pilsy</strong> commented on 2024-07-31 09:15:39.000 UTC</summary>

+1 for this

</details>

<details>
<summary><strong>tom-g-dane</strong> commented on 2025-02-07 12:24:20.000 UTC</summary>

Hi @yiminc is there any update on timeline for this feature? We're currently discussing a range of painful/complicated alternatives to allow us to pause specific workflows/queues. Thanks!

Reactions: üëç 4

</details>

<details>
<summary><strong>Zonalds</strong> commented on 2025-02-20 17:32:44.000 UTC</summary>

Is there any update yet? 

</details>

<details>
<summary><strong>drewhoskins-temporal</strong> commented on 2025-02-20 22:55:39.000 UTC</summary>

Hey folks.  
* Soon, we are shipping Activity pause, which should help some, though not all, of you.  I'll post here when it's ready.
* We are working on designing Workflow pause, though I don't have a timeline to announce yet.  

Please ping me in the Temporal community Slack if you're interested in sharing your use case, looking at a design, or being an early adopter.

Reactions: üëç 2

</details>

<details>
<summary><strong>owardlaw</strong> commented on 2025-03-10 18:26:44.000 UTC</summary>

+1

</details>

<details>
<summary><strong>sajid-moinuddin</strong> commented on 2025-05-03 20:50:55.000 UTC</summary>

+1

</details>

<details>
<summary><strong>nebenwelt</strong> commented on 2025-07-23 21:24:46.000 UTC</summary>

+1

</details>

<details>
<summary><strong>markz25</strong> commented on 2025-08-12 12:58:26.000 UTC</summary>

https://github.com/mfateev/temporal-java-samples/blob/retryonsignalintercpetorwithcleanup/core/src/main/java/io/temporal/samples/retryonsignalinterceptor/

</details>

<details>
<summary><strong>40lsgy1</strong> commented on 2025-09-09 02:28:25.000 UTC</summary>

@drewhoskins-temporal 
‚ÄåRegarding the current progress on "Activity pause"ÔºåIn my scenario, if an Activity is determined to be ‚Äåbound to fail‚Äå (for instance, due to a system exception), then retrying would be meaningless. In such cases, I can ‚Äåproactively pause‚Äå the Activity, wait for the system exception to be resolved, and then ‚Äåresume‚Äå the Activity.

Reactions: üëç 2

</details>

<details>
<summary><strong>paulps</strong> commented on 2025-10-21 06:32:09.000 UTC</summary>

+1

</details>


---

### #487: Consider separating retention period from uniqueness guarantee period

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/487 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-06-30 02:56:17.000 UTC (5y 6m ago) |
| **Updated** | 2023-06-22 07:50:51.000 UTC |
| **Upvotes** | 8 |
| **Comments** | 2 |
| **Priority Score** | 18 |
| **Labels** | enhancement, API, difficulty: medium |
| **Assignees** | paulnpdev |
| **Milestone** | None |
| **Reactions** | üëç 8 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, the uniqueness of a workflow is directly related to the retention period after a workflow is closed.
This forces users to make uncomfortable choices between the size of the DB and the uniqueness guarantee.

**Describe the solution you'd like**
Allow uniqueness guarantee to be larger than the retention period. It would mean removing all the workflow data from DB, but keeping a small record that would preclude duplicated workflow starts.

**Describe alternatives you've considered**
Keep the system as it is.



#### Comments (2)

<details>
<summary><strong>paulnpdev</strong> commented on 2023-01-05 17:11:30.000 UTC</summary>

fwiw, the complexity of this is higher than it appears

</details>

<details>
<summary><strong>aksdb</strong> commented on 2023-06-22 07:50:32.000 UTC</summary>

Would it be feasible to add a timestamp to each workflow that records the last time it was in conflict with another workflow, and using that timestamp as part of the retention rules? So basically: everytime the workflow is involved in a duplicate check, its "lifetime" gets prolonged.

</details>


---

### #7037: [bug] Viewing schedule throws a 504 "context deadline exceeded"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7037 |
| **State** | OPEN |
| **Author** | thomas-maurice (Thomas Maurice) |
| **Created** | 2024-12-27 17:40:29.000 UTC (1 years ago) |
| **Updated** | 2025-12-18 13:23:29.000 UTC |
| **Upvotes** | 5 |
| **Comments** | 7 |
| **Priority Score** | 17 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 5 |

#### Description

## Expected Behavior

Expected behaviour is that I would be able to see the schedule

## Actual Behavior
Out of the blue the schedules seem to be inaccessible, by which I mean the schedule page (for a single schedule, not the schedule list) throws a 504. Similarly I am unable to `describe` the schedule with the CLI:
```
$ temporal --env dev  schedule describe -smy-schedule 
time=2024-12-27T18:32:25.632 level=ERROR msg="context deadline exceeded"
```

I do not see anything in the error logs of the temporal server, I am using the `auto-setup` docker image. I have also noticed that this does not affect newly created schedules.

The schedules do not run anymore either.

Apart from that, the rest of temporal seems to be working properly

## Steps to Reproduce the Problem

Not sure of the reproductability, the problem started on `1.25.1` seemingly randomly, upgrading it to `1.25.2` did not seem to change anything

## Specifications

  - Version: 1.25.1 & 1.25.2 with PostgreSQL 14
  - Platform: linux amd64


#### Comments (7)

<details>
<summary><strong>michalkurzeja</strong> commented on 2025-06-23 12:52:44.000 UTC</summary>

I have the same issue. It started after upgrading my cluster from 1.24.1 (I did a step-by-step upgrade, first to 1.25.2, then 1.26.2 and then finally 1.27.2). Not all schedules are affected (3/6) - some load instantly, some return 504 with "context deadline exceeded". I have no idea what makes them different.

There is nothing in the logs of temporal services, only temporal-ui logs that a 504 has been returned, but no further details.

Here's an example request made by the UI:
![Image](https://github.com/user-attachments/assets/e67c84c5-2c0b-4edd-a453-ad1a42ec67d1)

And the response after 30s:
```json
{
  "code": 4,
  "message": "context deadline exceeded"
}
```

Requests from CLI also result in deadline exceeded, e.g.:
```
$ temporal  --address=... --tls=true schedule describe -s "Profile export: v2-data-changed-daily"
```

What's interesting, running `temporal schedule trigger` or `temporal schedule toggle` doesn't return any errors, but also does nothing - the schedule remains unpaused (verified via UI and `temporal schedule list`) and trigger doesn't run any workflows.

In the end, I was able to delete the faulty schedules with `temporal schedule delete` and re-create them. It would be good to know what could've caused this behaviour, though.

# Specifications
- Version:
-- Server: 1.27.2
-- PostgreSQL 17.4 (schema 1.16 + visibility 1.9)
-- UI: 2.37.2
- Platform: linux arm64

Reactions: üëç 5

</details>

<details>
<summary><strong>dimaulupov</strong> commented on 2025-08-20 08:35:15.000 UTC</summary>

Same problem with fresh, empty 1.28.1 with postgresql 17.5.
Can create and delete the schedule, but it never runs and can't be described with "context deadline exceeded" error.

</details>

<details>
<summary><strong>tback</strong> commented on 2025-12-15 09:40:15.000 UTC</summary>

Same thing is happening here.
Temporal 1.29.1
Postgres: AlloyDB, Postgres16 compatible

</details>

<details>
<summary><strong>emalihin</strong> commented on 2025-12-15 13:29:27.000 UTC</summary>

Same here, can list all schedules, but cannot open/pause/unpause/trigger ~0.5% of all schedules in the namespace (2056 schedules currently). Receiving `context deadline exceeded` from UI/CLI, and `slow gRPC call` from `frontend` deployment.

Temporal 1.28.0
Postgres: Aurora 15.12

</details>

<details>
<summary><strong>tsurdilo</strong> commented on 2025-12-15 15:16:18.000 UTC</summary>

Would probably go through info mentioned in community Slack threads
https://temporalio.slack.com/archives/CTRCR8RBP/p1765657904864699?thread_ts=1765530797.822339&cid=CTRCR8RBP
https://temporalio.slack.com/archives/CTTJCPZQE/p1764606712273519
https://temporalio.slack.com/archives/CTRCR8RBP/p1763737324199379

and see if helps. Please report. Thanks.

Reactions: üëç 1

</details>

<details>
<summary><strong>tback</strong> commented on 2025-12-15 17:05:24.000 UTC</summary>

Thank you so much. In my case I set `worker.enabled: false` in the helm chart. Schedules load now that it's fixed.






Reactions: üëç 1 üéâ 1

</details>

<details>
<summary><strong>emalihin</strong> commented on 2025-12-18 13:23:29.000 UTC</summary>

In my case where I could not Describe only some of the schedules in a namespace, I found that resetting the underlying workflow has helped bring those schedules back.

</details>


---

### #4105: Full support for scylladb as persistence layer

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4105 |
| **State** | OPEN |
| **Author** | IOR88 (Igor Miazek) |
| **Created** | 2023-03-26 11:35:44.000 UTC (2y 9m ago) |
| **Updated** | 2025-10-21 08:01:41.000 UTC |
| **Upvotes** | 8 |
| **Comments** | 1 |
| **Priority Score** | 17 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 8 üëÄ 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
Temporal community is time to time struggling when using scylladb as persistence layer, scylladb is not officially integrated
as one of available persistence layers.

**Describe the solution you'd like**
I would like to provide support for temporal community and help to accept scylladb as persistence layer, I would like to ask
for guidance how to approach this integration, is there is a list of features that persistence layer must support ? How I make sure
that scylladb integration meet all of the requirements ?

**Describe alternatives you've considered**
I know that some community members already use scylladb, I could define the scope of work and ask if any of the community members would like to help on making scylladb officially supported. 

**Additional context**
Nothing to add for now, I general I need guidance and advise how to approach the integration, how to test so it is accepted by temporal.


#### Comments (1)

<details>
<summary><strong>einali</strong> commented on 2025-10-21 08:01:41.000 UTC</summary>

Hi everybody
is there any update for enhancement?

</details>


---

### #4829: Allow to specify task queue when resetting an execution

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4829 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2023-09-01 18:39:05.000 UTC (2y 4m ago) |
| **Updated** | 2023-09-01 18:39:05.000 UTC |
| **Upvotes** | 8 |
| **Comments** | 0 |
| **Priority Score** | 16 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 8 |

#### Description

Request is for adding ability to specify task queue for reset api


---

### #6995: Error during VisibilityDeleteExecution

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6995 |
| **State** | OPEN |
| **Author** | steveetm |
| **Created** | 2024-12-15 16:28:22.000 UTC (1 years ago) |
| **Updated** | 2025-05-12 16:55:12.000 UTC |
| **Upvotes** | 6 |
| **Comments** | 3 |
| **Priority Score** | 15 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 6 |

#### Description

We getting extreme amount of logs from temporal server:
```
{"level":"error","ts":"2024-12-15T16:21:36.296Z","msg":"Operation failed with an error.","error":"context deadline exceeded","logging-call-at":"visiblity_manager_metrics.go:264","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/persistence/visibility.(*visibilityManagerMetrics).updateErrorMetric\n\t/home/builder/temporal/common/persistence/visibility/visiblity_manager_metrics.go:264\ngo.temporal.io/server/common/persistence/visibility.(*visibilityManagerMetrics).DeleteWorkflowExecution\n\t/home/builder/temporal/common/persistence/visibility/visiblity_manager_metrics.go:128\ngo.temporal.io/server/service/history.(*visibilityQueueTaskExecutor).processDeleteExecution\n\t/home/builder/temporal/service/history/visibility_queue_task_executor.go:494\ngo.temporal.io/server/service/history.(*visibilityQueueTaskExecutor).Execute\n\t/home/builder/temporal/service/history/visibility_queue_task_executor.go:122\ngo.temporal.io/server/service/history/queues.(*executableImpl).Execute\n\t/home/builder/temporal/service/history/queues/executable.go:236\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).executeTask.func1\n\t/home/builder/temporal/common/tasks/fifo_scheduler.go:223\ngo.temporal.io/server/common/backoff.ThrottleRetry.func1\n\t/home/builder/temporal/common/backoff/retry.go:119\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\n\t/home/builder/temporal/common/backoff/retry.go:145\ngo.temporal.io/server/common/backoff.ThrottleRetry\n\t/home/builder/temporal/common/backoff/retry.go:120\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).executeTask\n\t/home/builder/temporal/common/tasks/fifo_scheduler.go:233\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).processTask\n\t/home/builder/temporal/common/tasks/fifo_scheduler.go:211"}
{"level":"error","ts":"2024-12-15T16:21:36.304Z","msg":"Fail to process task","shard-id":1,"address":"127.0.0.1:7234","component":"visibility-queue-processor","wf-namespace-id":"064f58ee-d88c-4c7c-8b81-77b93c315829","wf-id":"*","wf-run-id":"f4dd4001-fdbd-44d7-aaf1-9c401226e546","queue-task-id":23085605,"queue-task-visibility-timestamp":"2024-12-14T13:07:44.404Z","queue-task-type":"VisibilityDeleteExecution","queue-task":{"NamespaceID":"064f58ee-d88c-4c7c-8b81-77b93c315829","WorkflowID":"*","RunID":"f4dd4001-fdbd-44d7-aaf1-9c401226e546","VisibilityTimestamp":"2024-12-14T13:07:44.404345212Z","TaskID":23085605,"Version":0,"CloseExecutionVisibilityTaskID":9663191,"StartTime":null,"CloseTime":null},"wf-history-event-id":0,"error":"context deadline exceeded","lifecycle":"ProcessingFailed","logging-call-at":"lazy_logger.go:68","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/log.(*lazyLogger).Error\n\t/home/builder/temporal/common/log/lazy_logger.go:68\ngo.temporal.io/server/service/history/queues.(*executableImpl).HandleErr\n\t/home/builder/temporal/service/history/queues/executable.go:347\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).executeTask.func1\n\t/home/builder/temporal/common/tasks/fifo_scheduler.go:224\ngo.temporal.io/server/common/backoff.ThrottleRetry.func1\n\t/home/builder/temporal/common/backoff/retry.go:119\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\n\t/home/builder/temporal/common/backoff/retry.go:145\ngo.temporal.io/server/common/backoff.ThrottleRetry\n\t/home/builder/temporal/common/backoff/retry.go:120\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).executeTask\n\t/home/builder/temporal/common/tasks/fifo_scheduler.go:233\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).processTask\n\t/home/builder/temporal/common/tasks/fifo_scheduler.go:211"}

```

Any idea how to investigate and/or recover from this?

## Expected Behavior
Not getting errors, visibility correctly updated

## Actual Behavior

Getting an extrem amount of errors, we can see past events listed in temporal-ui, way after retention period.
Workflows seems to be running, finishing, we can see them in temporal-ui.

## Steps to Reproduce the Problem

Not sure. We did nothing special, it was working fine. We changed mysql password, temporal-service run into some access denied error, service restarted and these logs flooding since then.

## Specifications

  - Version: 1.22.4


#### Comments (3)

<details>
<summary><strong>steveetm</strong> commented on 2024-12-16 21:17:19.000 UTC</summary>

After upgrading to the latest version the issue is not fixed, but got a new error:
```
{"level":"error","ts":"2024-12-16T20:48:10.526Z","msg":"Operation failed with an error.","error":"unable to delete custom search attributes: context deadline exceeded","logging-call-at":"/home/runner/work/docker-builds/docker-builds/temporal/common/persistence/visibility/visiblity_manager_metrics.go:195","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/log/zap_logger.go:155\ngo.temporal.io/server/common/persistence/visibility.(*visibilityManagerMetrics).updateErrorMetric\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/persistence/visibility/visiblity_manager_metrics.go:195\ngo.temporal.io/server/common/persistence/visibility.(*visibilityManagerMetrics).DeleteWorkflowExecution\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/persistence/visibility/visiblity_manager_metrics.go:129\ngo.temporal.io/server/service/history.(*visibilityQueueTaskExecutor).processDeleteExecution\n\t/home/runner/work/docke^Coff/retry.go:64\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).executeTask\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/tasks/fifo_scheduler.go:233\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).processTask\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/tasks/fifo_scheduler.go:211"}
```

The number of logs emitted is considerably lower, but there are 170k rows in visibility tasks and 64k in executions_visibility (the retention period is one day, this is way more than we should have)

</details>

<details>
<summary><strong>gkarthiks</strong> commented on 2024-12-20 21:41:28.000 UTC</summary>

I have the same issue as well. 

 ServerVersion :  1.25.1

</details>

<details>
<summary><strong>pranchals</strong> commented on 2025-05-12 16:54:35.000 UTC</summary>

I am also facing similar issue.

Workflow records are maintained in executions_visibility table even after the retention period set on the database, this is leading to degraded db performance is impacting all functionalities in temporal.

Is this any way to cleanly delete the records in visibility store that have passed the retention period ?

Temporal Server Version: 1.26.2

</details>


---

### #6323: Frontend Service - goroutine (CPU & Memory) Leak

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6323 |
| **State** | OPEN |
| **Author** | uritig (guriti) |
| **Created** | 2024-07-22 23:49:06.000 UTC (1y 5m ago) |
| **Updated** | 2025-12-12 07:01:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 15 |
| **Priority Score** | 15 |
| **Labels** | potential-bug |
| **Assignees** | gow |
| **Milestone** | None |

#### Description

## Expected Behavior
There should be no memory leak resulting from objects not being properly garbage collected.

## Actual Behavior
Number of objects on Heap keeps growing. This seems to be result in slow increase of cpu & memory usage eventually resulting in outage.

## Steps to Reproduce the Problem

- Start server

` temporal server start-dev --port 7233 --ui-port 8233 --metrics-port 9233`

- Do not start workflows or make any grpc calls, use the sdks or the web ui. aka keep the usage to a minimum.

- Periodically check pprof & metrics. This shows that the goroutinue counts, objects on heap (aka memory allocations for objects) & memory allocations keeps growing perpetually.

![image](https://github.com/user-attachments/assets/5eb28209-2bf2-411c-939c-fa45bb0d52e2)

- runtime.MemStats from from http://localhost:<pprof-port>/debug/pprof/heap?debug=1
```
# runtime.MemStats
# Alloc = 65091800
# TotalAlloc = 318383929504
# Sys = 119521560
# Lookups = 0
# Mallocs = 1641446565
# Frees = 1641167975
# HeapAlloc = 65091800
# HeapSys = 82378752
# HeapIdle = 11141120
# HeapInuse = 71237632
# HeapReleased = 4464640
# HeapObjects = 278590
# Stack = 18284544 / 18284544
# MSpan = 825408 / 960048
# MCache = 19200 / 31200
# BuckHashSys = 2638467
# GCSys = 12561120
# OtherSys = 2667429
# NextGC = 69589984
# LastGC = 1721688648441657922
...
# NumGC = 10489
# NumForcedGC = 0
# GCCPUFraction = 4.835076125703521e-05
# DebugGC = false
# MaxRSS = 178909184

```

- Flame graph

![flame_graph](https://github.com/user-attachments/assets/44044c3b-2fae-4160-8b87-28f2e6b80407)

- This was observed across local & shared & production environments. Please see the prometheus chart in a production environment where the num_goroutine count kept increasing until a restart. Notice that the "leap" appears to be isolated to the frontend service. The rest seem fine. The CPU usage & memory usage charts followed the same pattern.
 
![image](https://github.com/user-attachments/assets/034b8b5e-96b2-4879-85ac-18def393131e)


## Specifications
This was observed across multiple versions
  - Server Version: 1.22.5, 1.22.7, 1.23.1, etc.
  - Platform: Linux
  - MTLS enabled
  - Auth disabled

## Links
This issue is potentially related to https://community.temporal.io/t/high-cpu-usage-memory-leakage-on-frontend-service/4246/1 


#### Comments (15)

<details>
<summary><strong>clayzermk1</strong> commented on 2024-07-23 19:08:49.000 UTC</summary>

I am also experiencing this issue.

</details>

<details>
<summary><strong>TheHiddenLayer</strong> commented on 2024-07-26 08:21:30.000 UTC</summary>

ditto

</details>

<details>
<summary><strong>gow</strong> commented on 2024-08-05 22:27:54.000 UTC</summary>

Hello, I took a brief look into this and was not able to reproduce it locally. I left the server running for over 2 days and didn't observe go-routines growing. Plus in the screenshot you provided (from http://localhost/debug/pprof/heap?debug=1) I don't find anything abnormal. Even an idle server has periodic internal activity and it uses about 2800 goroutines. The `heap` and `allocs` count you see are total allocations (count and size) that may have been freed as well. It doesn't represent currently "in-use" allocated objects and their size.

However the last graph you linked showing `num_goroutines` from a production environment is a bit concerning. Since `num_goroutines` are reported [as a guage](https://prometheus.io/docs/tutorials/understanding_metric_types/#gauge), I initially thought applying sum operation on it will result in such graphs. But seeing this happen only in frontend and not in other services is a bit confusing. So we need to dig a little deeper. As a first step, could you please apply the `avg` or `avg_over_time` function on `num_goroutines` and plot the same graph? If you still see the same behavior, then we need to connect to one of the server and obtain the go-routine profile to see what is holding up all those goroutines. You can do this by port-forwarding to one of those frontend hosts and visiting the http://localhost:your_port/debug/pprof. Then click on "goroutines" link. Please let us know what you find out.

</details>

<details>
<summary><strong>uritig</strong> commented on 2024-08-08 23:02:30.000 UTC</summary>

@gow thank you for the response.
Please note that this issue still persists in production & (fortunately) in our sandbox environment as well.
I'm about to enable pprof & leave it running for a couple of weeks. But first, captured:
![image](https://github.com/user-attachments/assets/f2ce41b5-5963-45ba-9f79-d7f760ae89be)
![image](https://github.com/user-attachments/assets/36618caa-6721-4a0c-8b1c-0c713a811275)

Will post back with the same 2 charts and results of the "goroutines" link in a couple of weeks.

</details>

<details>
<summary><strong>gow</strong> commented on 2024-08-08 23:32:10.000 UTC</summary>

Thanks for posting the avg(num_goroutine) graphs @guritinvda 
To enable pprof, do you have to restart the service? If not, could you please connect one of the frontend hosts in your sandbox environment and visit `/debug/pprof` and get the goroutine dump in its current state?

</details>

<details>
<summary><strong>uritig</strong> commented on 2024-08-08 23:46:37.000 UTC</summary>

>To enable pprof, do you have to restart the service?

Unfortunately, yes.

</details>

<details>
<summary><strong>gow</strong> commented on 2024-08-16 19:31:05.000 UTC</summary>

@guritinvda any new findings?

</details>

<details>
<summary><strong>uritig</strong> commented on 2024-08-19 17:02:17.000 UTC</summary>

I was able to repro the issue in our sandbox environment. Blue arrow in the screenshot is when I enabled pprof and "restarted" the frontend service.
![image](https://github.com/user-attachments/assets/c3c2e077-b0c2-4b55-b14c-2771e435772e)

![image](https://github.com/user-attachments/assets/76755eef-4147-4cde-ad99-e536cb69a8ce)

Here's the goroutine dump (2 debug levels):
[goroutine-1.txt](https://github.com/user-attachments/files/16663260/goroutine-1.txt)
[goroutine-2.txt](https://github.com/user-attachments/files/16663261/goroutine-2.txt)



</details>

<details>
<summary><strong>abhishekhugetech</strong> commented on 2024-08-20 18:05:22.000 UTC</summary>

Hey which version of temporal server or temporal SDK version are you using?

</details>

<details>
<summary><strong>uritig</strong> commented on 2024-08-28 20:29:11.000 UTC</summary>

We observed this issue across multiple temporal server versions - 1.22.5, 1.22.7, 1.23.1, etc.
go-SDK version - v1.24.0 (majority of the workers and clients are here).
python-SDK version - 1.5.1

</details>

<details>
<summary><strong>gow</strong> commented on 2024-08-29 19:08:19.000 UTC</summary>

@uritig thanks for posting the data. Looking at the hung Goroutines, it seems to be coming from improper `grpc.ClientConn` cleanup.
```
goroutine profile: total 4204
3444 @ 0x43e5ce 0x44ea05 0x971c75 0x471661
#	0x971c74	google.golang.org/grpc/internal/grpcsync.(*CallbackSerializer).run+0x114
/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/internal/grpcsync/callback_serializer.go:76
```
I found a similar issue posted here on `grpc-go` repo that explains not closing `grpc.ClientConn` as the cause - https://github.com/grpc/grpc-go/issues/6413#issuecomment-1610047629

There is a PR (https://github.com/temporalio/temporal/pull/6441) in temporal to cache these connections instead of creating them frequently on demand. This issue might be fixed with that change.

</details>

<details>
<summary><strong>gow</strong> commented on 2024-08-29 19:08:42.000 UTC</summary>

I'll followup once that PR lands.

Reactions: üëç 1

</details>

<details>
<summary><strong>dnr</strong> commented on 2024-08-29 21:27:28.000 UTC</summary>

I've seen this before on the worker service. I'm not sure why we would recreate grpc.ClientConns since we have a stub-level cache that should avoid creating new ones. The new cache should only make a difference if we connect to multiple services on the same endpoint, which we generally don't do. So there might be an issue with the stub cache also.

</details>

<details>
<summary><strong>gtejasvi</strong> commented on 2025-03-25 16:32:10.000 UTC</summary>

seeing a similar behaviour with the history service where we are seeing that after a day of contineous usage the history service uses 100% of cpu available and comes back to normal for 2-3 hours after a pod restart. The pod continues to use 100% cpu till it is restarted even if the no of active workflows are less than 100.

</details>

<details>
<summary><strong>vikas-rampp</strong> commented on 2025-12-12 07:01:24.000 UTC</summary>

Hi Team, any update on the fix for this as we are also observing continuously increasing memory consumption by history service. Is there any configuration we need to pass to control this as reading this conversation does not landed me any specific solution.

</details>


---

### #5516: Support for Azure Blob Storage for Archival

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5516 |
| **State** | OPEN |
| **Author** | omm-mishra |
| **Created** | 2024-03-12 10:18:37.000 UTC (1y 9m ago) |
| **Updated** | 2025-10-10 14:59:23.000 UTC |
| **Upvotes** | 7 |
| **Comments** | 1 |
| **Priority Score** | 15 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 7 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently Archival is only supported for Gcloud and AWS S3, our org uses Azure as its cloud provider, hence we would like to use Azure Storage for Archival.

**Describe the solution you'd like**
We would like a plugin based solution where we can go ahead and implement our own code and pass them to something like the Server Options or any native support from Temporal side is also welcome.

**Describe alternatives you've considered**
We did try to go ahead and implement the Azure dependencies on our own but since it requires changes in the original source code, it would add a significant overhead on our team to maintain the codebase and keep it updated with newly released versions, we could not proceed with it.

#### Comments (1)

<details>
<summary><strong>raygervais</strong> commented on 2025-10-10 14:59:23.000 UTC</summary>

üëã! I just wanted to follow up and ask if there was any interest in this still? I'd love to try to contribute the implementation if there was interest from the community and team

Reactions: üëç 3

</details>


---

### #4902: Custom authentication between frontend and other services

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4902 |
| **State** | OPEN |
| **Author** | ndtretyak (Nikolay Tretyak) |
| **Created** | 2023-09-26 14:30:14.000 UTC (2y 3m ago) |
| **Updated** | 2023-10-14 10:37:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 15 |
| **Priority Score** | 15 |
| **Labels** | enhancement |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
There is a `HeadersProvider` interface in the go-sdk which I use together with custom `Authorizer` to implement custom authentication mechanism for workers. But  I couldn't find anything similar to `HeadersProvider` in the server side code. So, I think there is no way to replace client certificates with other authentication method for the requests between Temporal's microservices.

**Describe the solution you'd like**
I'd like to define some interceptor for all requests between Temporal's microservices, so that these requests contain additional information for custom authentication.

If you are okay with this idea, I'd be happy to work on it.



#### Comments (15)

<details>
<summary><strong>dnr</strong> commented on 2023-09-27 15:50:31.000 UTC</summary>

This isn't a priority for the team so it's unlikely to happen soon. We do recommend mTLS for securing the connections between services. Depending on the complexity, I think it's possible we could accept something like HeadersProvider as a contribution, but no guarantees. Before spending too much time on it, maybe a proposal with a little more detail (i.e. interfaces) would be helpful.

</details>

<details>
<summary><strong>ndtretyak</strong> commented on 2023-10-03 21:18:26.000 UTC</summary>

I've made a draft PR https://github.com/temporalio/temporal/pull/4928 adding `WithClientHeadersProvider` option that accepts `HeadersProvider` interface from the go-sdk.


Reactions: üëç 3

</details>

<details>
<summary><strong>amitbet</strong> commented on 2023-10-09 09:49:54.000 UTC</summary>

How is it not a priority, if this means that you have to accept **nil** in the Authorizer in order to allow for inter-process communication? does this not negate the whole concept of the custom auth, and allow anyone without a token to pass?

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-10-09 16:56:28.000 UTC</summary>

You can use mTLS to secure inter-node communication within a temporal cluster. The Authorizer (custom or otherwise) is not used for inter-node communication, except for one case (server worker‚Üífrontend), and you can use internal-frontend to eliminate that case.

Supporting token-based authentication for worker‚Üífrontend is desirable, but the lack of it does not negate the existing security features for external client‚Üífrontend traffic.

</details>

<details>
<summary><strong>amitbet</strong> commented on 2023-10-10 08:26:43.000 UTC</summary>

But even when implementing mTLS for worker->frontend and tokens for external, 
in order to accept the worker->frontend traffic without a token the Authorizer must allow nil token headers (it can't discern between external and internal traffic) - this means that malicious traffic coming from the outside can create a message with a nil header, and it will go right through auth... 
**this makes the whole token based auth mechanism completely useless**
It might also mean that the Temporal cloud has a huge security vulnerability

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-10-10 16:20:46.000 UTC</summary>

1. The Authorizer is not given TLS connection metadata or tokens, it's given claims from the ClaimMapper. The ClaimMapper is given both TLS metadata and authorization tokens, and can construct claims based on one or the other or both. E.g. it can require either mTLS or a valid token, but not neither; if there's no token, it can require mTLS, if there's no mTLS, it can require a token. (Yes, you can pass anything from ClaimMapper to Authorizer in the Extensions field but that doesn't change the point.)
2. Using internal-frontend gives an easy way to differentiate internal and external traffic without writing a custom ClaimMapper. Internal traffic bypasses the ClaimMapper (i.e. it always gets full admin claims), external traffic does not. It's not the only way to do it, you can also use more complicated TLS configs, but internal-frontend is probably easier.

</details>

<details>
<summary><strong>pilotofbalance</strong> commented on 2023-10-11 13:25:20.000 UTC</summary>

@dnr in case of implementing custom ClaimMapper, it is not given...not just an authorization.AuthInfo is empty, but **GetClaims** method is not triggered at all, so how token or mTLS could be checked for temporal services or web ui?

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-10-12 04:38:35.000 UTC</summary>

I'm not sure I understand. You can see the logic here: https://github.com/temporalio/temporal/blob/main/common/authorization/interceptor.go#L102

If a claim mapper and authorizer are set, and then if either 1. the connection has a client certificate, 2. the connection has an `authorization` header, or 3. the claim mapper returns false for `AuthInfoRequired`, then the claim mapper is called with whatever `AuthInfo` it can put together from those.

(Note that the change to allow customizing the name of the `authorization` header isn't released yet.)

I don't know that much about the web ui, but my understanding is that it will forward an `authorization` header with its requests.

</details>

<details>
<summary><strong>pilotofbalance</strong> commented on 2023-10-12 07:19:47.000 UTC</summary>

@dnr I'm talking about this case: https://github.com/temporalio/samples-server/blob/main/extensibility/authorizer/myClaimMapper.go. Now if you are running only temporal default services + web ui, without any workers, so this method: `func (c myClaimMapper) GetClaims(authInfo *authorization.AuthInfo) (*authorization.Claims, error) {` is not even triggered, if it's not triggered, you can't really check client cert or auth header and return claims with some flag. Although this method is always triggered `
func (a *myAuthorizer) Authorize(_ context.Context, claims *authorization.Claims,
	target *authorization.CallTarget) (authorization.Result, error) {
` ,but with no claims (nil)

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-10-12 15:53:00.000 UTC</summary>

Can you confirm that the web ui sending a grpc header named `authorization`? If it is, and the claim mapper is still not getting called, then that's a bug and we should probably split it to another issue. My guess is that the web ui is not sending that header for some reason.

</details>

<details>
<summary><strong>pilotofbalance</strong> commented on 2023-10-13 14:26:56.000 UTC</summary>

@dnr let's say I'm a hacker and I'm running my own web ui, I decided not to send `authorization` token to the temporal cluster? Temporal cluster should validate token or certificate in any case and not cos web ui or other internal temporal services have it...pls, try to run this code https://github.com/temporalio/samples-server/tree/main/extensibility/authorizer and debug ClaimMapper method you will see that it's never called.

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-10-13 14:42:56.000 UTC</summary>

If you don't send an authorization token and don't authenticate with a client cert, then as you said, the claim mapper is not called, and `claims` will be `nil` when passed to the Authorizer. The authorizer (either default or custom) should reject any request with nil claims, except for health checks. I still don't see any problem here.

</details>

<details>
<summary><strong>pilotofbalance</strong> commented on 2023-10-13 16:04:51.000 UTC</summary>

@dnr so we are returning to the original issue, there is no option to send authorization token from "matching", "history" and "worker" services to the "frontend" and this is what this PR is about, as you said the only option is to configure mTLS, but then there is no point for custom authorizer and claimMapper implementation, they just useless..(btw you just mentioned health checks, how I can recognize health check in case of **nil** claims?)

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-10-13 17:02:35.000 UTC</summary>

Yes, we should fix the original issue. No, the lack of ability to send tokens for internode communication does not mean that custom authorizer+claimmappers are useless: it's perfectly reasonable to use mTLS to authenticate internode communication and tokens to authenticate external clients, and you can do that with the current state of the code.

For health checks, see https://github.com/temporalio/temporal/blob/main/common/authorization/frontend_api.go#L46. The default authorizer allows calls to those two methods even with no claims, and you probably want to have your custom authorizer do the same. This allows kubernetes or similar systems to do readiness probes without needing to use mTLS there.

</details>

<details>
<summary><strong>pilotofbalance</strong> commented on 2023-10-14 10:37:51.000 UTC</summary>

@dnr I didn't find it in temporal documentation...you said that for custom Authorizer and ClaimMapper there have to be `external frontend` and for temporal services `internal frontend` with mTLS...may be it worth it to provide kinda example or at least HL documentation..
Default authorizer require "sub" claim which should be optional and not mandatory + in my case only I can't change token structure and include "permissions"

</details>


---

### #3309: Notify all workers on a task queue

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3309 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2022-09-06 16:16:45.000 UTC (3y 3m ago) |
| **Updated** | 2023-12-08 16:57:04.000 UTC |
| **Upvotes** | 7 |
| **Comments** | 1 |
| **Priority Score** | 15 |
| **Labels** | enhancement |
| **Assignees** | dnr |
| **Milestone** | None |
| **Reactions** | üëç 7 |

#### Description

**Is your feature request related to a problem? Please describe.**

***User scenario:***

**My scenario:**
I have multiple workers listening on the same task queue for high throughput.
All the workers need to do some setup and cleanup work from time to time.
**My requirement:**
the business client can send a workflow or a command, which can be picked up and executed by all workers, instead of only one of them.
Is there any solution?

**Describe the solution you'd like**
My strawman proposal is to have a metadata map that is returned as part of every workflow or activity task. An admin API should support updating this map. The SDKs should watch for map changes and execute appropriate logic to notify workers about the changes.



#### Comments (1)

<details>
<summary><strong>skandragon</strong> commented on 2023-12-08 16:57:03.000 UTC</summary>

I have a similar use case for this feature.

I am running a workflow that requires a large amount of data to be available and updated for workflows to run, so downloading this every run is undesirable. I could have each workflow check the centralized source of this, but that is also an API call per workflow, and most of the time there's no update.

Reactions: üëç 3

</details>


---

### #1412: Add soft workflow timeout

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1412 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2021-03-26 23:14:54.000 UTC (4y 9m ago) |
| **Updated** | 2023-03-03 20:22:40.000 UTC |
| **Upvotes** | 7 |
| **Comments** | 1 |
| **Priority Score** | 15 |
| **Labels** | enhancement, API, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 7 |

#### Description

**Is your feature request related to a problem? Please describe.**
RunTimeout terminates workflow without giving it any chance to perform cleanup. So to deal with business logic timeouts timers have to be used which adds complexity to the user code.

**Describe the solution you'd like**
Add `WorkflowRunCancellationTimeout` (name is strawman) wich sends a cancellation request to the workflow when exceeded. This way a workflow can perform all necessary cleanup.



#### Comments (1)

<details>
<summary><strong>ccl0326</strong> commented on 2022-07-21 09:14:58.000 UTC</summary>

any progress? 

Reactions: üëç 1

</details>


---

### #804: Synchronous Start

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/804 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-10-06 16:58:58.000 UTC (5y 2m ago) |
| **Updated** | 2021-07-04 07:04:35.000 UTC |
| **Upvotes** | 5 |
| **Comments** | 5 |
| **Priority Score** | 15 |
| **Labels** | enhancement, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 5 |

#### Description

**Is your feature request related to a problem? Please describe.**
It is not possible to reject the start workflow call if it provides invalid input data.

**Describe the solution you'd like**
Ability to invoke user code that performs input argument validation before returning start call.

**Additional context**
https://community.temporal.io/t/managing-workflows-code-repositories/467/5


#### Comments (5)

<details>
<summary><strong>saurabnigam</strong> commented on 2020-12-24 10:19:21.000 UTC</summary>

Hi @mfateev,
I would like to do contribute to this issue but would need help getting started. Any guidance is appreciated!

</details>

<details>
<summary><strong>mfateev</strong> commented on 2020-12-29 18:55:44.000 UTC</summary>

@saurabnigam this sounds exciting!

This feature is not designed yet. So I cannot really give direct guidance. Let us have a design discussion internally first and I'll come back with a proposal.

Reactions: üëç 2

</details>

<details>
<summary><strong>saurabnigam</strong> commented on 2020-12-29 19:01:56.000 UTC</summary>

If it's possible I'd love to take part in design discussions as well. Also, I would appreciate some materials to learn more about the internals of temporal. I have already cloned the repo locally and started reading the code, but there are just too many unknowns and I am having trouble figuring things out.

</details>

<details>
<summary><strong>mfateev</strong> commented on 2020-12-29 19:27:13.000 UTC</summary>

The requirements question. There are two ways to think about this feature. 

1. Call user code to validate workflow input and if the validation logic returns an error do not start the workflow. The drawback of this approach that if the workflow worker is down then workflows cannot be even started.
2. Always start a workflow, but provide a way to not return the start call immediately waiting for the workflow code to explicitly indicate that it can be unblocked. It also could return some information to the start call from the workflow. This can be thought of as a specific application of the synchronous workflow update feature. The synchronous update is a very frequently requested feature that opens up a lot of cool use cases. But it is pretty complex to implement. Our estimate is at least a couple of months of full time developer work. 

</details>

<details>
<summary><strong>mfateev</strong> commented on 2020-12-30 00:14:06.000 UTC</summary>

I believe that we have a short term solution.

The proposal is to implement [worker query ](https://github.com/uber/cadence/issues/390) first.

Then create a special built-in worker query that would invoke application-specific workflow input validation code and call StartWorkflowExecution through the service API.

DM me to schedule a zoom meeting to go over the design in more detail. 

Reactions: üëç 1

</details>


---

### #7930: Replace "github.com/olivere/elastic/v7" with the official client "github.com/elastic/go-elasticsearch"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7930 |
| **State** | OPEN |
| **Author** | jmbarzee (Jacob Barzee) |
| **Created** | 2025-06-18 00:55:01.000 UTC (6 months ago) |
| **Updated** | 2025-12-22 09:38:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 14 |
| **Priority Score** | 14 |
| **Labels** | enhancement |
| **Assignees** | rodrigozhou |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
github.com/olivere/elastic is deprecated.

**Describe the solution you'd like**
Replace with official version github.com/elastic/go-elasticsearch

**Describe alternatives you've considered**
There aren't other alternatives. Most community projects have been deprecated in favor of the official client.

**Additional context**
Nope.


#### Comments (14)

<details>
<summary><strong>jmbarzee</strong> commented on 2025-06-18 01:04:11.000 UTC</summary>

This is quite a large change but, its not overly complicated, Its mostly just a client swap. I

We should probably leave the bulk client/processor till later in the work. It is a bit more complex and will require more verification. We can test a branch locally even if we are still using both clients. We should consider starting with a single method in common/persistence/visibility/store/elasticsearch/client/client.go Its implementation is in common/persistence/visibility/store/elasticsearch/client/client_v7.go

Reactions: üöÄ 1

</details>

<details>
<summary><strong>lakshya8066</strong> commented on 2025-06-20 06:50:28.000 UTC</summary>

Hi @jmbarzee I'd like to work on this if it is open. Can you assign it to me?

The interface common/persistence/visibility/store/elasticsearch/client/client.go uses github.com/olivere/elastic as the return instance, so I think a new client interface is also needed and then we can implement it's methods in common/persistence/visibility/store/elasticsearch/client/client_v8.go

</details>

<details>
<summary><strong>vaibhavyadav-dev</strong> commented on 2025-06-20 09:11:13.000 UTC</summary>

Okay @jmbarzee , this looks great, I can take this one.

</details>

<details>
<summary><strong>lakshya8066</strong> commented on 2025-06-29 16:57:55.000 UTC</summary>

Hi @jmbarzee I have raised a PR for the migration. There are multiple functions to be migrated, I have started with a few and we can track the others in the PR description. 

@vaibhavyadav-dev - I see that you are working on it as well.üòÖ Do you want to collaborate on the PR? We can each pick functions and implement them, it will make the migration faster. 

I guess the first starting point of discussion is should we go with go-elasticsearch v8 or v9. 
My thought process with going with v8 was:
1. v8 is more mature compared to v9 
2. It has lower migration risk since migrating directly to v9 from v7 can have breaking changes. 
3. With v8 it is easier to maintain backward compatibility.
4. v9.0.0 was released a few months ago, maybe we should wait for it to get more stable?

Jacob - what do you think? 

</details>

<details>
<summary><strong>vaibhavyadav-dev</strong> commented on 2025-06-29 18:32:45.000 UTC</summary>

Hey @lakshya8066 , this one is currently assigned specifically to me, but I also appreciate your effort on it. 
Since there‚Äôs been parallel work, I‚Äôll leave it to @jmbarzee  and @bergundy to decide which one to looks best.
Well, It look like v8 would be much better than v9 as of now.

Also, it‚Äôd be great to comment, before starting work on an issue, just to avoid any overlap. 
Thanks!

</details>

<details>
<summary><strong>jmbarzee</strong> commented on 2025-07-08 21:21:10.000 UTC</summary>

Wow, Amazing assertiveness from the both of you.

@vaibhavyadav-dev, I see in the community slack that Roey found different work for you. I'll support you in that direction, though the changes will probably be reviewed by someone with more experience in that section.

@lakshya8066 Great questions, I'm sorry I didn't get to them before you got to implementation.

@Apurer I saw you submitted a very similar PR. Lets coordinate here.

Reactions: üëç 1

</details>

<details>
<summary><strong>jmbarzee</strong> commented on 2025-07-08 21:29:06.000 UTC</summary>

First: lets coordinate to reduce duplicated work.

Second: Ideally, most code looks the same and the import paths just change. I've done some quick prototyping of this and haven't found that its quite this perfect, but in most cases it was at least close.

**Now, details:**

- We need to stay on ES v7. We need to decouple the migration to the official library from the version migration.
- The official client has the bulk of the implementation we need. Reimplementing clients that are already implemented there would defeat the purpose of depending on the official package. 
- In cases where we accept/return structs that are defined in olivere, we should find the corresponding structs in the official client and aim for the most minimal change possible. 

Happy to answer further questions or address anything I've missed


Reactions: üöÄ 1

</details>

<details>
<summary><strong>lakshya8066</strong> commented on 2025-07-18 18:16:23.000 UTC</summary>

@jmbarzee thanks for the clarifications! I agree it is probably better to move to v7 first. Although migrating to v8 whenever we decide to would be exciting because of the [typedapis](https://github.com/elastic/go-elasticsearch/tree/v8.18.1/typedapi) üôÇ 

What do you think should be our merging strategy? Do we want to merge everything except the bulk processor in one go or we can keep implementing functions and keep merging?

</details>

<details>
<summary><strong>lakshya8066</strong> commented on 2025-07-21 03:34:45.000 UTC</summary>

If we want to completely remove dependency from Oliver Elasticsearch and move to official v7 client, it would require implementing all the custom query builder that Oliver has. I was also thinking since we will lose all the types available in Oliver, there has to be an extensive JSON marshaling and unmarsheling logic written. We would also lose compile-time type safety and would need extensive runtime JSON validation. The official client mostly return raw json, PIT operations would also be affected. Just thinking out loud, do you think this migration would call for a reimplementation of the entire client code?

</details>

<details>
<summary><strong>hferentschik</strong> commented on 2025-07-24 15:55:19.000 UTC</summary>

üëã hi, we are also interested in this migration. Not so much because the driver is deprecated, but also because it depends on the AWS SDK v1 which has reached EOL. This relates to https://github.com/temporalio/temporal/pull/4499 which addresses the second functional area (archival) where Temporal has a dependency to the AWS SDK v1. 

Maybe anyone from Temporal can provide an update on when they expect this elastic search driver migration to land and in extension when we can expect a Temporal server free of a dependency to AWS SDK v1?

</details>

<details>
<summary><strong>hferentschik</strong> commented on 2025-07-24 16:01:02.000 UTC</summary>

On a different note, is elastic/go-elasticsearch working with OpenSearch? I believe v7 does, but not sure about later versions and whether there would be potential problems. See also https://github.com/temporalio/temporal/issues/5680.

I know that there is a OpenSearch specific go driver (https://github.com/opensearch-project/opensearch-go), but I am not sure what the compatibility guarantees there are between OpenSearch and ElasticSearch and the various drivers. Does the driver in the new solution need to be pluggable in some form?

Reactions: üëç 1

</details>

<details>
<summary><strong>damar-block</strong> commented on 2025-07-29 18:20:59.000 UTC</summary>

Hey, will that change break compatibility with OpenSearch? 

</details>

<details>
<summary><strong>hferentschik</strong> commented on 2025-08-22 12:56:09.000 UTC</summary>

üëã any update on this issue? 

I already asked about making the driver pluggable related to OpenSearch, but there is another use case. We have a custom driver wrapping the go-elastic driver. It implements amongst other things other forms of authentication. It would be awesome to be able to have a way to configure your driver. At the very least provide a custom transport. Wdyt?

</details>

<details>
<summary><strong>omerlic</strong> commented on 2025-12-22 09:38:02.000 UTC</summary>

üëã any update on this issue?

</details>


---

### #3468: Add ability to list task queues for a given namespace

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3468 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2022-10-10 23:16:47.000 UTC (3y 2m ago) |
| **Updated** | 2024-06-21 21:16:50.000 UTC |
| **Upvotes** | 6 |
| **Comments** | 2 |
| **Priority Score** | 14 |
| **Labels** | enhancement |
| **Assignees** | MichaelSnowden |
| **Milestone** | None |
| **Reactions** | üëç 6 |

#### Description

Task queue in temporal is dynamic, they are created on demand. There is no pre-registration step needed. This is great because it is very easy to use and you can create dynamic task queues at runtime. 
There are cases where task queue is mis configured, or the generated task queue is incorrect. This could result in workflow been send to task queues that have no workers listening on. 

The ability to list all active task queues could be very useful to help diagnose these problems. Active task queues are the task queues that are loaded into matching service. Any task queues that have either new tasks added to or have poller waiting on them in last 5m is considered as active task queue.

This is not to be confused with all task queues that ever exists and been created in persistence. We don't want to go to persistence to load up all existing task queues. (Maybe we will in the future, but not for now.)



#### Comments (2)

<details>
<summary><strong>jeacott1</strong> commented on 2023-11-08 06:00:28.000 UTC</summary>

the ability to query for: task queue + registration(s) (with a count per type/impl of the existing state) would be great.
I'd like to be able to list all active workers too, and then show, node + process + worker + task queue + registration(s). 
In combination with a list any unmatched workflows/activities (that don't match any workers - is this possible already?), I think these could help in fault finding and tuning environments.


Reactions: üëç 1

</details>

<details>
<summary><strong>jlegrone</strong> commented on 2024-06-21 21:13:24.000 UTC</summary>

This seems like it may duplicate https://github.com/temporalio/temporal/issues/1797

</details>


---

### #351: List Workflow Executions ignoring previous runs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/351 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-05-04 18:51:45.000 UTC (5y 8m ago) |
| **Updated** | 2023-03-03 20:23:34.000 UTC |
| **Upvotes** | 5 |
| **Comments** | 4 |
| **Priority Score** | 14 |
| **Labels** | CRON, visibility |
| **Assignees** | dnr, alexshtin |
| **Milestone** | None |
| **Reactions** | üëç 5 |

#### Description

From Slack:

> We‚Äôre considering using Cadence to run something akin to the actor model.
> We will have an actor for every ‚Äúuser‚Äù, of which there are tens of millions, and each actor will be in charge of processing messages for its corresponding user.
> We would love to be able to query cadence for all actors that are currently in a failed state. This is a payments related use case, and its important for us to know every actor that is currently stuck.
> With that, we want to filter out all actors that are no longer in a failed state. That is, if there was an actor that had a workflow execution fail, but has since had a workflow execution succeed, we consider that actor fixed, and we want it filtered out of the query.
> Is there any way to do this?
> If I understand correctly, a lot of the listing features that Cadence provides list workflow executions, but what I really want is to list the set of most recent executions per workflow.

#### Comments (4)

<details>
<summary><strong>barakalon</strong> commented on 2020-06-15 21:58:50.000 UTC</summary>

@mfateev any idea of the difficulty/priority of this feature?

Also, is this possible to achieve today with elasticsearch/search attributes enabled? Looking [at docs](https://docs.temporal.io/docs/learn-workflow-filtering#query-capabilities), I don't see that it is.

</details>

<details>
<summary><strong>alexshtin</strong> commented on 2021-07-03 05:44:23.000 UTC</summary>

How do you want to get this data/run this query? Is issuing a direct query to Elasticsearch an option? It definitely not supported with Temporal list APIs (both `tctl` and SDKs) but most likely supported with Elasticsearch [aggregations](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html).

</details>

<details>
<summary><strong>barakalon</strong> commented on 2021-07-06 15:25:49.000 UTC</summary>

_Ideally_, this query can be run directly from the UI/tctl. 

As for issuing queries directly against Elasticsearch- that would be a big compromise for us. We want to use this query for manual operations, so its important to have some kind of user interface here. We'd like to avoid building some new CLI for this purpose or having to spin up something like Kibana. Especially since Elasticsearch - IIUC - is an internal component of the Temporal service, and it might require us depending on undocumented, unstable interfaces.

</details>

<details>
<summary><strong>yiminc</strong> commented on 2022-10-29 18:17:00.000 UTC</summary>

Looks like we need additional index on current run. 

</details>


---

### #8724: Support Azure Entra ID Authentication for PostgreSQL Backend

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8724 |
| **State** | OPEN |
| **Author** | GabrielAlacchi (Gabriel Alacchi) |
| **Created** | 2025-12-01 19:30:35.000 UTC (1 months ago) |
| **Updated** | 2025-12-03 18:18:39.000 UTC |
| **Upvotes** | 6 |
| **Comments** | 1 |
| **Priority Score** | 13 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 6 |

#### Description

**Is your feature request related to a problem? Please describe.**
When self-hosting temporal on Azure, it is generally preferred to use managed identity or service principal authentication based on Entra ID to authenticate with PGSQL to reduce the need for managing secrets.
https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/security-entra-concepts

The mechanism in question simply substitutes the password in the PGSQL connection with a JWT.

A team in my organization is currently maintaining a set of patches that adds this support to temporal-server and temporal-sql-tool binaries. I'm wondering if there's any approach, we can take to upstream this into Temporal open-source release.

The internal patches we maintain do the following:
1. Add azure-identity SDK and add a config flag to temporal to enable its usage to fetch a token in the Postgresql temporal plugin code.
2. Add some error handling logic to force a re-connection whenever a query fails due to token expiry. This feels hacky to me, because temporal already seems to have a max-connection lifetime capability which can be tuned dynamically to the expiry of the token.

**Describe the solution you'd like**
Bringing in our internal patches is a non-starter since it involves bringing in [azidentity](https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/azidentity) SDK components into temporal's build which will not be used outside of our niche use-case.

Other cloud providers also have similar token based auth schemes for PGSQL that involve managing the lifecycle of a time-bound token.
- https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html
- https://docs.cloud.google.com/sql/docs/postgres/iam-authentication#auto-vs-manual

I would propose a more generic plugin-based approach can be taken here to make it possible to use any cloud provider that follows a similar JWT auth pattern. I'm open to suggestions on the specific details on how that would work. Two ideas I have:

1. Use some plugin binary that can be invoked (either CLI or GO plugin https://pkg.go.dev/plugin) to get a token + expiry when creating a new connection. Cloud-specific connector plugins can likely be built separately from some separate contrib folder / separate repositories and brought in optionally by consumers in the container build.
2. Temporal can be configured to grab token + expiry from a file; can support for this be added to dynamic config?

**Describe alternatives you've considered**
- Password auth: Not ideal for our organization's security & compliance concerns.
- Custom fork: We are basically doing this now and can continue if strictly necessary. However, since other big 3 cloud providers support similar token-based auth schemes for PostgreSQL, we may widen the audience that is interested in this feature enough to build a mainline solution.

**Additional context**
Happy to bring some cycles to contribute PRs to make this happen if we can align on a suitable approach. Our interest in contributing is limited to the Azure use-case, but we can lay the foundation for supporting other clouds in the process.

#### Comments (1)

<details>
<summary><strong>GabrielAlacchi</strong> commented on 2025-12-01 19:34:25.000 UTC</summary>

This might apply to MySQL as well on the big 3 clouds / other clouds.

</details>


---

### #8356: Resource-Aware Worker Concurrency

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8356 |
| **State** | OPEN |
| **Author** | abdel-0 |
| **Created** | 2025-09-20 12:41:45.000 UTC (3 months ago) |
| **Updated** | 2025-12-03 14:15:05.000 UTC |
| **Upvotes** | 6 |
| **Comments** | 1 |
| **Priority Score** | 13 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 6 |

#### Description


### Problem

Today, Temporal workers enforce concurrency with static limits (`max_concurrent_activity_task_executions`, `max_concurrent_workflow_task_executions`, etc.). These limits assume that workloads are uniform, but in practice workflows and activities have very different profiles (short IO-bound tasks vs. long CPU-bound tasks vs. memory-heavy ones).

This creates a false trade-off:

* **Set limits too low ‚Üí** workers are underutilized, leaving CPU/RAM idle and wasting money.
* **Set limits too high ‚Üí** workers oversubscribe, pods risk OOM/CPU thrash, and robustness is undermined.

Kubernetes autoscaling works on CPU/memory thresholds, but Temporal workers don‚Äôt adapt to that. The two systems don‚Äôt ‚Äúspeak the same language,‚Äù leading to either inefficiency or instability.

---

### Proposed Direction

Introduce **resource-aware concurrency controls** in workers. For example:

* Workers should stop polling when pod CPU/memory reaches configurable thresholds (e.g. 70‚Äì80%).
* Concurrency could scale dynamically based on resource availability, rather than fixed magic numbers.
* Optionally, expose hooks so operators can plug in their own resource metrics or scaling policies.

This would allow Temporal‚Äôs built-in backpressure to align with Kubernetes autoscaling, ensuring:

* Pods never overload themselves.
* Idle capacity is minimized.
* Scaling is smooth and cost-efficient.

---

### Why This Matters

Temporal‚Äôs core value proposition is **reliability at scale**. Without resource-aware workers, teams are forced to choose between:

* **Wasted resources** (cost inefficiency).
* **Risky oversubscription** (reliability gaps).

A resource-adaptive model would make Temporal more robust, cloud-native, and cost-efficient out of the box.



#### Comments (1)

<details>
<summary><strong>baptistejouin</strong> commented on 2025-12-03 14:15:05.000 UTC</summary>

Hey, I'm also searching for this type of feature, and I've just found this page on the Temporal documentation:

> Temporal offers three types of slot suppliers: fixed assignment, resource-based, and custom. Here‚Äôs how to choose the best approach based on your system requirements and workload characteristics.

> The following use cases are particularly well suited to resource-based auto-tuning slot suppliers:
Fluctuating workloads with low per-Task consumption [...]
Protection from out-of-memory & over-subscription in the face of unpredictable per-task consumption [...]


https://docs.temporal.io/develop/worker-performance#worker-performance-tuning

I haven't tested it yet, but it seems promising.


Reactions: üëç 1

</details>


---

### #6799: Support `WorkflowIdConflictPolicy` for child workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6799 |
| **State** | OPEN |
| **Author** | stephanos (Stephan Behnke) |
| **Created** | 2024-11-12 16:48:52.000 UTC (1y 1m ago) |
| **Updated** | 2025-12-04 20:09:08.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 7 |
| **Priority Score** | 13 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 3 |

#### Description

**Is your feature request related to a problem? Please describe.**

`WorkflowIdReusePolicy`'s Terminate-if-Running is being deprecated and replaced with `WorkflowIdConflictPolicy`'s Terminate-Existing.

To complete this effort, support for child workflows is needed.

**Describe the solution you'd like**
TBD 

**Describe alternatives you've considered**
TBD

**Additional context**

This is blocking https://github.com/temporalio/features/issues/558

#### Comments (7)

<details>
<summary><strong>john-behm-bertelsmann</strong> commented on 2025-04-30 09:49:18.000 UTC</summary>

bump

</details>

<details>
<summary><strong>lukeramsden</strong> commented on 2025-05-26 17:07:32.000 UTC</summary>

Just ran in to this limitation - not great. This, combined with the fact that "external workflow handles" don't allow you to wait on the result of another workflow, is causing quite a friction for my usecase right now. Going to have to re-think my workflow design because of this now.

</details>

<details>
<summary><strong>mpontus</strong> commented on 2025-05-27 18:50:44.000 UTC</summary>

@lukeramsden You can wait for external workflow completion using an activity, that's what we did. You can rely on retries to keep activity running indefinitely or implement more complex logic involving heartbeats.


```ts
import { cancelled } from '@temporalio/activity';

async function waitForExternalWorkflowCompletion(workflowId: string): Promise<void> {
	await Promise.race([
		workflowClient.getHandle(workflowId).result(),
		cancelled(),
	]);
}
```

If you implement long-polling based on retry behavior, just make sure the promise completes when the activity is cancelled. Otherwise, the activity slots won't be freed.



Reactions: üëç 2

</details>

<details>
<summary><strong>prakharb10</strong> commented on 2025-06-10 21:02:50.000 UTC</summary>

Ran into a use case today

</details>

<details>
<summary><strong>madhav2302</strong> commented on 2025-08-11 16:46:45.000 UTC</summary>

We also have a use-case for this, and getting it through local activity and signal is ugly. 

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-08-11 17:59:22.000 UTC</summary>

See comment at https://github.com/temporalio/features/issues/558#issuecomment-3176176866 on why use-existing may not make sense from a technical perspective.

</details>

<details>
<summary><strong>sovanny</strong> commented on 2025-12-04 20:09:08.000 UTC</summary>

How we can achieve "terminate existing" without this? I want to achieve the following: When a child workflow with the same ID as an existing/running child workflow (of _any_ parent), terminate that.

</details>


---

### #8110: v1.28.0 vulnerabilities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8110 |
| **State** | OPEN |
| **Author** | daravindth |
| **Created** | 2025-07-29 13:47:53.000 UTC (5 months ago) |
| **Updated** | 2025-10-06 08:58:35.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 10 |
| **Priority Score** | 12 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

We see the below components of the server:v1.28.0 image have some vulnerabilities. 

alpine://3.22:libcrypto3:3.5.0-r0 - **CVE-2025-4575**
go://go.temporal.io/server:1.18.1-0.20230217005328-b313b7f58641 - **CVE-2023-3485**
go://go.temporal.io/api:1.18.1 - **CVE-2025-1243**
go://golang.org/x/net:0.34.0 - **CVE-2025-22870**
go://github.com/golang/go:1.24.1 - **CVE-2025-4673**
go://google.golang.org/grpc:1.56.3 - **CVE-2024-7246**
go://go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc:0.36.4 - **CVE-2023-47108**
go://golang.org/x/crypto:0.32.0 - **CVE-2025-22869**
go://golang.org/x/oauth2:0.7.0 - **CVE-2025-22868**
go://github.com/golang-jwt/jwt/v4:4.5.1 - **CVE-2025-30204**

Please help us with a docker image without these vulnerabilities.

#### Comments (10)

<details>
<summary><strong>matt-whitaker</strong> commented on 2025-07-29 22:22:31.000 UTC</summary>

+1, bump for visibility

</details>

<details>
<summary><strong>joe-kramer</strong> commented on 2025-07-29 22:22:39.000 UTC</summary>

+1

</details>

<details>
<summary><strong>bergundy</strong> commented on 2025-07-31 17:48:43.000 UTC</summary>

We will look into this, thanks for reporting.

</details>

<details>
<summary><strong>daravindth</strong> commented on 2025-08-12 05:45:49.000 UTC</summary>

Any updates on this?

</details>

<details>
<summary><strong>korncola</strong> commented on 2025-08-20 08:34:03.000 UTC</summary>

+1

</details>

<details>
<summary><strong>daravindth</strong> commented on 2025-08-28 03:35:12.000 UTC</summary>

Can you please expedite on this? So, that we can plan accordingly on the remediation. 

</details>

<details>
<summary><strong>daravindth</strong> commented on 2025-09-18 12:49:07.000 UTC</summary>

Please let us know if there are any progress on these on this vulnerability fix. Can we get a timeline on when we can have an image? 

</details>

<details>
<summary><strong>daravindth</strong> commented on 2025-09-26 09:54:26.000 UTC</summary>

Can we get a timeline on when we can have an image?

</details>

<details>
<summary><strong>daravindth</strong> commented on 2025-10-03 11:55:24.000 UTC</summary>

Can we get a timeline on when we can have an image?

</details>

<details>
<summary><strong>daravindth</strong> commented on 2025-10-06 08:58:35.000 UTC</summary>

I can see in the 1.29.0's release page that components will be removed in the 1.30.0.
Can we have an timeline of that?

</details>


---

### #5156: Allow configuration of expected audience value for Temporal authorization

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5156 |
| **State** | OPEN |
| **Author** | dzmitry-panamarenka-absa |
| **Created** | 2023-11-20 18:21:35.000 UTC (2y 1m ago) |
| **Updated** | 2025-07-16 10:33:31.000 UTC |
| **Upvotes** | 4 |
| **Comments** | 4 |
| **Priority Score** | 12 |
| **Labels** | None |
| **Assignees** | dnr |
| **Milestone** | None |
| **Reactions** | üëç 4 |

#### Description

**Author**: Dima Ponomarenko

### Summary of the feature being proposed
Extend `global:authorization` section of docker config to allow configuring of expected audience value.
Currently, it's possible to configure JWTClaimMapper via config file. If `defaultJWTClaimMapper` is used, one of the existing checks there is to validate `aud` claim against the expected values received from `audienceGetter.Audience`. But the `audienceGetter` is not defined by default and the only way to set it is to use custom temporal build. If `audienceGetter` is not set, 'aud' is not validated at all. There is no way to configure it using docker config file unlike claimMapper and authorizer. Ideally, we should allow specifying the expected audience value in the docker config itself, but I'd like to hear if you can suggest a better way.

### What value does this feature bring to Temporal?
This feature will simplify Temporal setup when JWT-based authentication is used.
As an example, Azure Active Directory oAuth2. In's commonly used in enterprise environment, where you must check the `aud` because application within your organisation share the same JWT signing key. As a result, another application from your org can successfully authenticate and mimic your JWT claims to get access into your system, unless you check `aud`. https://learn.microsoft.com/en-us/entra/identity-platform/id-token-claims-reference#payload-claims

It's possible to build custom Temporal with your own `audienceGetter` backed in. But it requires more knowledge (including golang) and make is harder to maintain. As well as keeping your custom build up to date.
But I believe the following security feature should be achievable using configuration file and without custom Temporal builds.
### Are you willing to implement this feature yourself?
Yes

#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2023-11-27 16:28:29.000 UTC</summary>

I am going to forward this to the Temporal server repo...

</details>

<details>
<summary><strong>dzmitry-panamarenka-absa</strong> commented on 2024-02-13 09:50:05.000 UTC</summary>

Are there any plans to implement this change?
Perhaps you would be willing to accept a contribution to this issue, if we can agree on the design first.

</details>

<details>
<summary><strong>dzmitry-panamarenka-absa</strong> commented on 2024-02-22 11:00:16.000 UTC</summary>

I'd to highlight importance of `aud` check by putting this link to "JSON Web Token Best Current Practices" standard [RFC 8725](https://datatracker.ietf.org/doc/html/rfc8725#name-use-and-validate-audience)

</details>

<details>
<summary><strong>robholland</strong> commented on 2025-07-16 10:33:31.000 UTC</summary>

We would definitely consider a config interface to set this.

</details>


---

### #4044: Postgres schema support partition table feature

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4044 |
| **State** | OPEN |
| **Author** | zedongh (zedongh) |
| **Created** | 2023-03-11 05:58:33.000 UTC (2y 9m ago) |
| **Updated** | 2025-04-07 20:09:47.000 UTC |
| **Upvotes** | 5 |
| **Comments** | 2 |
| **Priority Score** | 12 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 5 |

#### Description

**Is your feature request related to a problem? Please describe.**
No.

**Describe the solution you'd like**
Most temporal sql query use shard_id as condition, for postgres database partition by hash is nice for large table.
Maybe some table partition number could be number of shards.  

**Describe alternatives you've considered**
No.

**Additional context**
https://www.postgresql.org/docs/current/ddl-partitioning.html


#### Comments (2)

<details>
<summary><strong>jeacott1</strong> commented on 2023-11-09 07:11:30.000 UTC</summary>

I'd prefer to see multi-db/schema sharding support instead of in-db table sharding.
Cadence appears to be capable of this, why not temporal?

</details>

<details>
<summary><strong>ericsun2</strong> commented on 2025-04-07 20:08:40.000 UTC</summary>

https://github.com/ericsun2/sandbox/tree/master/database/temporal

Instead of switching to Cassandra, both Postgres and MySql can benefit from the Hash Partition for in-database scaling.

* Postgres: `PARTITION BY HASH (shard_id, run_id)`
* MySql: `PARTITION BY KEY(shard_id, tree_id)`

</details>


---

### #2426: [Feature Request] Add reason field to RequestCancelWorkflowExecutionRequest

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2426 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2022-01-27 15:01:26.000 UTC (3y 11m ago) |
| **Updated** | 2025-08-20 13:45:08.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 10 |
| **Priority Score** | 12 |
| **Labels** | None |
| **Assignees** | yycptt, yiminc |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

Currently both TerminateWorkflowExecutionRequest and ResetWorkflowExecutionRequest have a "reason" field. 
RequestCancelWorkflowExecutionRequest does not.


We should add "reason" to RequestCancelWorkflowExecutionRequest as well please. 

#### Comments (10)

<details>
<summary><strong>bergundy</strong> commented on 2022-01-27 17:00:24.000 UTC</summary>

`reason` should be added to [`RequestCancelExternalWorkflowExecutionCommandAttributes`](https://github.com/temporalio/api/blob/81c06e37652d5d8e3cd0a5a98290c3aa19fdfb8b/temporal/api/command/v1/message.proto#L113) too.

</details>

<details>
<summary><strong>darewreck54</strong> commented on 2022-01-28 01:30:19.000 UTC</summary>

What would be nice if there could be a unique identifier which can be used to reference and correlate the scope cancelation to the cancelfailure.  Even if this is fixed you are relying on a string comparison.

</details>

<details>
<summary><strong>yiminc</strong> commented on 2022-01-28 18:08:19.000 UTC</summary>

@darewreck54 could you explain a little bit on the use case of this scope cancelation and the need of unique identifier.

</details>

<details>
<summary><strong>darewreck54</strong> commented on 2022-03-22 00:11:02.000 UTC</summary>

How would you distinguish a cancelFailure that is triggered from a cancelation of a workflow vs. a detach cancelation that you triggered yourself.  I tried to pass a reason into the cancelation message and use that to tell the difference, but that doesn‚Äôt propagate to the CancelFailure at least from what I can tell

for example, trigger this‚Ä¶
```
getCancellationScope().cancel("reason");
````
Then when I catch the CancelFailure, I would expect it to be see it somewhere but the ‚Äúreason‚Äù message isn‚Äôt anywhere
<img width="1279" alt="image" src="https://user-images.githubusercontent.com/16171726/159382841-e51d8374-98d0-41cb-b3ad-67a909bf5593.png">


</details>

<details>
<summary><strong>feedmeapples</strong> commented on 2022-04-19 01:48:08.000 UTC</summary>

dependent on go SDK support issue https://github.com/temporalio/tctl/issues/144#issuecomment-1101912150
cc @bergundy 

</details>

<details>
<summary><strong>darewreck54</strong> commented on 2023-04-13 12:09:13.000 UTC</summary>

@feedmeapples & @yiminc  Is there any update on this being addressed or eta?

</details>

<details>
<summary><strong>feedmeapples</strong> commented on 2023-05-11 05:15:53.000 UTC</summary>

@darewreck54 yes we are considering adding this. I'll let you know asap about the ETA

</details>

<details>
<summary><strong>dhang-sc</strong> commented on 2024-08-21 06:32:08.000 UTC</summary>

@feedmeapples is there any update on this?

Reactions: üëç 1

</details>

<details>
<summary><strong>nargiz</strong> commented on 2025-04-10 13:16:09.000 UTC</summary>

this will be very useful for us, too. Any updates?

Reactions: üëç 1

</details>

<details>
<summary><strong>tshtark</strong> commented on 2025-08-20 13:45:07.000 UTC</summary>

we would love to have this too

</details>


---

### #1289: Do not fail continue as new on a new signal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1289 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2021-02-14 02:04:40.000 UTC (4y 10m ago) |
| **Updated** | 2023-04-03 21:49:23.000 UTC |
| **Upvotes** | 5 |
| **Comments** | 2 |
| **Priority Score** | 12 |
| **Labels** | enhancement, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 5 üëÄ 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
If a signal is received during a workflow task processing that created continue-as-new command the task is failed. This is to allow the workflow to process the newly received signal.  The workflow handles such a signal by rolling back its state through replay from the beginning. This creates a problem if signals are constantly sent to the workflow. It keeps replaying without ever finishing. In reality, it reaches the hard limit of signals per workflow and then continue-as-new is executed.

**Describe the solution you'd like**
By default pass signal to the next run of the workflow. This way the continue-as-new is never blocked and the signal is not lost.
This behavior should be configurable through options as in some use cases signal shouldn't be transferred. 

The diagram that illustrates the current handling of the signal received during a workflow task execution that completes a workflow run:
![CompleteSignalRaceCondition](https://user-images.githubusercontent.com/1463622/107866440-cf8e8000-6e25-11eb-8298-c3ceb422d27f.png)


#### Comments (2)

<details>
<summary><strong>XingLiu0923</strong> commented on 2022-12-10 23:43:51.000 UTC</summary>

I found this issue from `samples-java`. It is a interesting issue, making me realize that signal will also affect the replay effect. I'm curious whether this issue has a solution now?

</details>

<details>
<summary><strong>longquanzheng</strong> commented on 2023-04-03 21:48:53.000 UTC</summary>

> In reality, it reaches the hard limit of signals per workflow and then continue-as-new is executed

Just a feedback on this. In reality, it could reach the workflow history size/length first and get hard terminated by server. 
Or because of too many signals received, continueAsNew is too challenging(near impossible to most users) to implement correctly -- they need to carry over hundreds of signals to next run (unless next run use a query handler to fetch the signals from previous run using pagination )

I want to call it out how bad that would be so that you may prioritize this üòÇ 

Also, I just realize that Temporal doesn't have a default signalsPerExecution limit -- the default is 0, should we change that? 

https://github.com/temporalio/temporal/blob/6f1f02280f72bc92212c98a87c8a21122724490b/service/history/configs/config.go#L417

(In Cadence it's now 10000 )

Reactions: üëç 1 üëÄ 1

</details>


---

### #983: loggging/exception stack trace not capturing underlying errors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/983 |
| **State** | OPEN |
| **Author** | kkcmadhu |
| **Created** | 2020-11-12 08:17:34.000 UTC (5y 1m ago) |
| **Updated** | 2021-07-04 20:05:35.000 UTC |
| **Upvotes** | 5 |
| **Comments** | 1 |
| **Priority Score** | 11 |
| **Labels** | good first issue, refactoring, potential-bug, planning, operations, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 5 |

#### Description

I configured temporal 1.21 using k8s and s3 archival on EKS. apparently my s3 access key was in correct. when i started temporal it started fine, but when i 
## Expected Behavior
Error should have underlying cause

## Actual Behavior
Archival related errors do not get shown up in create name space calls

## Steps to Reproduce the Problem

  1. configure temporal on eks cluster with s3 archival
  2. do not configure s3 access key/or invalid key
  3. start temporal, it gets started successfully
  4.  create a namespace from tctl or client library and you will get 403 without any hint that it origniated from archival/s3.



## Specifications

  - Version: 1.2.1
  - Platform: k8s/eks


here is the stack trace :

bash-5.0# tctl --namespace samples-namespace namespace register
Error: Register namespace operation failed.
Error Details: rpc error: code = Unknown desc = Forbidden: Forbidden
status code: 403, request id: 250DCE81B9026FA0, host id: bTkqpS/CChOZrwZw14CL1p8mOfKlnEMU66Pm25Mx33DxGk6a+KD6xLvdso3cX7xhjAopvQ/dUGA=
(‚Äòexport TEMPORAL_CLI_SHOW_STACKS=1‚Äô to see stack traces)

in the front end i see the following in logs

{‚Äúlevel‚Äù:‚Äúerror‚Äù,‚Äúts‚Äù:‚Äú2020-11-11T11:22:30.917Z‚Äù,‚Äúmsg‚Äù:‚ÄúUnknown error‚Äù,‚Äúservice‚Äù:‚Äúfrontend‚Äù,‚Äúerror‚Äù:‚ÄúForbidden: Forbidden\n\tstatus code: 403, request id: C486AFCD8246BCA6, host id: p3YC1NdyUDJbVfGd6OrQ8IYuLcOmqSYpQaauroLUyQUoF7qkekVFxR+mQrX5bgQC3DVf2/L7BWI=‚Äù,‚Äúlogging-call-at‚Äù:‚ÄúworkflowHandler.go:3399‚Äù,‚Äústacktrace‚Äù:‚Äúgo.temporal.io/server/common/log/loggerimpl.(*loggerImpl).Error\n\t/temporal/common/log/loggerimpl/logger.go:138\ngo.temporal.io/server/service/frontend.(*WorkflowHandler).error\n\t/temporal/service/frontend/workflowHandler.go:3399\ngo.temporal.io/server/service/frontend.(*WorkflowHandler).RegisterNamespace\n\t/temporal/service/frontend/workflowHandler.go:252\ngo.temporal.io/server/service/frontend.(*DCRedirectionHandlerImpl).RegisterNamespace\n\t/temporal/service/frontend/dcRedirectionHandler.go:177\ngo.temporal.io/server/service/frontend.(*AccessControlledWorkflowHandler).RegisterNamespace\n\t/temporal/service/frontend/accessControlledHandler.go:469\ngo.temporal.io/server/service/frontend.(*WorkflowNilCheckHandler).RegisterNamespace\n\t/temporal/service/frontend/workflowNilCheckHandler.go:62\ngo.temporal.io/api/workflowservice/v1._WorkflowService_RegisterNamespace_Handler.func1\n\t/go/pkg/mod/go.temporal.io/api@v1.1.0/workflowservice/v1/service.pb.go:957\ngo.temporal.io/server/common/rpc.Interceptor\n\t/temporal/common/rpc/interceptor.go:35\ngo.temporal.io/api/workflowservice/v1._WorkflowService_RegisterNamespace_Handler\n\t/go/pkg/mod/go.temporal.io/api@v1.1.0/workflowservice/v1/service.pb.go:959\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.32.0/server.go:1194\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.32.0/server.go:1517\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.2\n\t/go/pkg/mod/google.golang.org/grpc@v1.32.0/server.go:859‚Äù}


#### Comments (1)

<details>
<summary><strong>slayerjain</strong> commented on 2021-02-18 10:07:38.000 UTC</summary>

Just happened to me as well. Thank god for this issue, I was puzzled about why I wasn't able to create a domain. Forgot about the archival feature. 

</details>


---

### #5455: docker-compose multirole sometimes stuck on boot

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5455 |
| **State** | OPEN |
| **Author** | kbumsik (Bumsik Kim) |
| **Created** | 2024-02-27 13:45:09.000 UTC (1y 10m ago) |
| **Updated** | 2024-02-27 13:49:57.000 UTC |
| **Upvotes** | 5 |
| **Comments** | 0 |
| **Priority Score** | 10 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 5 |

#### Description

The issue is about [docker-compose-multirole.yaml](https://github.com/temporalio/docker-compose/blob/main/docker-compose-multirole.yaml) example in [temporalio/docker-compose](https://github.com/temporalio/docker-compose) repo. I am posting this here because temporalio/docker-compose does not have an issue page.

## Expected Behavior

When I run `docker compose -f docker-compose-multirole.yaml up` the whole multirole cluster is up and running normally.

## Actual Behavior

When I run `docker compose -f docker-compose-multirole.yaml up`, temporal-history is sometimes stuck at "Waiting for Temporal server to start...", unable to reach the frontend service via nginx. The whole cluster does not seem to be in sync because I cannot connect to the UI service as well.

This does not always happen. So you need to try this multiple times. To me it feels like the chance is about 20%, especially when you stop docker-compose after running the service for a long time. 

Restarting temporal-frontend or temporal-frontend2 sometimes make the cluster to the normal state, but not always.

## Steps to Reproduce the Problem

  1. `git clone https://github.com/temporalio/docker-compose`
  1.  `cd docker-compose`
  1.  `docker plugin install grafana/loki-docker-driver:latest --alias loki --grant-all-permissions`
  2. `docker compose -f docker-compose-multirole.yaml up`
  3. If it works well, press control+C to stop the docker-compose project and wait for a moment. Then run the previous command again until you come across the problem.

## Specifications

  - Version: 1.22.4 (it happens in the previous versions too)
  - Platform: macOS Sonoma 14.3.1 (newest version)
  - Docker Desktop: 4.27.2 (newest version) 


Here is the following log:

```
temporal-nginx          | 192.168.16.6 - - [27/Feb/2024:13:03:16 +0000] "POST /grpc.health.v1.Health/Check HTTP/2.0" 204 0 "-" "grpc-go/1.59.0"
temporal-nginx          | 2024/02/27 13:03:16 [error] 29#29: *73 connect() failed (111: Connection refused) while connecting to upstream, client: 192.168.16.6, server: , request: "POST /grpc.health.v1.Health/Check HTTP/2.0", upstream: "grpc://192.168.16.8:7237", host: "temporal-nginx:7233"
temporal-nginx          | 2024/02/27 13:03:16 [error] 29#29: *73 connect() failed (111: Connection refused) while connecting to upstream, client: 192.168.16.6, server: , request: "POST /grpc.health.v1.Health/Check HTTP/2.0", upstream: "grpc://192.168.16.9:7236", host: "temporal-nginx:7233"
temporal-history        | Error: unable to health check "temporal.api.workflowservice.v1.WorkflowService" service: unexpected HTTP status code received from server: 204 (No Content); malformed header: missing HTTP content-type
temporal-history        | ('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
temporal-history        | Waiting for Temporal server to start...
temporal-nginx          | 2024/02/27 13:03:18 [error] 29#29: *76 connect() failed (111: Connection refused) while connecting to upstream, client: 192.168.16.6, server: , request: "POST /grpc.health.v1.Health/Check HTTP/2.0", upstream: "grpc://192.168.16.9:7236", host: "temporal-nginx:7233"
temporal-nginx          | 2024/02/27 13:03:18 [error] 29#29: *76 connect() failed (111: Connection refused) while connecting to upstream, client: 192.168.16.6, server: , request: "POST /grpc.health.v1.Health/Check HTTP/2.0", upstream: "grpc://192.168.16.8:7237", host: "temporal-nginx:7233"
temporal-nginx          | 192.168.16.6 - - [27/Feb/2024:13:03:18 +0000] "POST /grpc.health.v1.Health/Check HTTP/2.0" 204 0 "-" "grpc-go/1.59.0"
temporal-history        | Error: unable to health check "temporal.api.workflowservice.v1.WorkflowService" service: unexpected HTTP status code received from server: 204 (No Content); malformed header: missing HTTP content-type
temporal-history        | ('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
temporal-history        | Waiting for Temporal server to start...
temporal-history        | Error: unable to health check "temporal.api.workflowservice.v1.WorkflowService" service: unexpected HTTP status code received from server: 204 (No Content); malformed header: missing HTTP content-type
temporal-history        | ('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
temporal-nginx          | 2024/02/27 13:03:19 [error] 29#29: *79 connect() failed (111: Connection refused) while connecting to upstream, client: 192.168.16.6, server: , request: "POST /grpc.health.v1.Health/Check HTTP/2.0", upstream: "grpc://192.168.16.8:7237", host: "temporal-nginx:7233"
temporal-nginx          | 2024/02/27 13:03:19 [error] 29#29: *79 connect() failed (111: Connection refused) while connecting to upstream, client: 192.168.16.6, server: , request: "POST /grpc.health.v1.Health/Check HTTP/2.0", upstream: "grpc://192.168.16.9:7236", host: "temporal-nginx:7233"
temporal-nginx          | 192.168.16.6 - - [27/Feb/2024:13:03:19 +0000] "POST /grpc.health.v1.Health/Check HTTP/2.0" 204 0 "-" "grpc-go/1.59.0"
temporal-history        | Waiting for Temporal server to start...
temporal-history        | Error: unable to health check "temporal.api.workflowservice.v1.WorkflowService" service: unexpected HTTP status code received from server: 204 (No Content); malformed header: missing HTTP content-type
temporal-history        | ('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
temporal-nginx          | 2024/02/27 13:03:20 [error] 29#29: *82 connect() failed (111: Connection refused) while connecting to upstream, client: 192.168.16.6, server: , request: "POST /grpc.health.v1.Health/Check HTTP/2.0", upstream: "grpc://192.168.16.9:7236", host: "temporal-nginx:7233"
temporal-nginx          | 2024/02/27 13:03:20 [error] 29#29: *82 connect() failed (111: Connection refused) while connecting to upstream, client: 192.168.16.6, server: , request: "POST /grpc.health.v1.Health/Check HTTP/2.0", upstream: "grpc://192.168.16.8:7237", host: "temporal-nginx:7233"
temporal-nginx          | 192.168.16.6 - - [27/Feb/2024:13:03:20 +0000] "POST /grpc.health.v1.Health/Check HTTP/2.0" 204 0 "-" "grpc-go/1.59.0"
temporal-history        | Waiting for Temporal server to start...
```


Setting `TEMPORAL_CLI_SHOW_STACKS=true` on temporal-history does not help much:

```
temporal-history        | Error: unable to health check "temporal.api.workflowservice.v1.WorkflowService" service: unexpected HTTP status code received from server: 204 (No Content); malformed header: missing HTTP content-type
temporal-history        | Stack trace:
temporal-history        | goroutine 1 [running]:
temporal-history        | runtime/debug.Stack()
temporal-history        |       /opt/hostedtoolcache/go/1.20.11/x64/src/runtime/debug/stack.go:24 +0x64
temporal-history        | runtime/debug.PrintStack()
temporal-history        |       /opt/hostedtoolcache/go/1.20.11/x64/src/runtime/debug/stack.go:16 +0x1c
temporal-history        | github.com/temporalio/cli/app.HandleError(0x40003df438?, {0x2b492a0, 0x40007481e0})
temporal-history        |       /home/runner/work/cli/cli/app/app.go:73 +0x134
temporal-history        | github.com/urfave/cli/v2.(*App).handleExitCoder(0x40009d91e0?, 0x400014de00?, {0x2b492a0?, 0x40007481e0?})
temporal-history        |       /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/app.go:452 +0x3c
temporal-history        | github.com/urfave/cli/v2.(*Command).Run(0x40009d91e0, 0x40003eb480, {0x4000b10200, 0x1, 0x1})
temporal-history        |       /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/command.go:276 +0x768
temporal-history        | github.com/urfave/cli/v2.(*Command).Run(0x406ef60, 0x40003eb340, {0x4000b01be0, 0x2, 0x2})
temporal-history        |       /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/command.go:267 +0x948
temporal-history        | github.com/urfave/cli/v2.(*Command).Run(0x406f900, 0x40003eb280, {0x40009c92f0, 0x3, 0x3})
temporal-history        |       /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/command.go:267 +0x948
temporal-history        | github.com/urfave/cli/v2.(*Command).Run(0x40009dab00, 0x40003eb140, {0x400004c0c0, 0x4, 0x4})
temporal-history        |       /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/command.go:267 +0x948
temporal-history        | github.com/urfave/cli/v2.(*App).RunContext(0x40008da780, {0x2b6a678?, 0x4000058068}, {0x400004c0c0, 0x4, 0x4})
temporal-history        |       /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/app.go:332 +0x568
temporal-history        | github.com/urfave/cli/v2.(*App).Run(0x60?, {0x400004c0c0?, 0x400007c768?, 0x49b54?})
temporal-history        |       /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/app.go:309 +0x40
temporal-history        | main.main()
temporal-history        |       /home/runner/work/cli/cli/cmd/temporal/main.go:14 +0x38
temporal-history        | Waiting for Temporal server to start...
```


---

### #4000: bug: concurrent map read and map write

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4000 |
| **State** | OPEN |
| **Author** | vikstrous2 (Viktor Stanchev) |
| **Created** | 2023-03-02 01:25:43.000 UTC (2y 10m ago) |
| **Updated** | 2023-12-14 13:07:34.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 6 |
| **Priority Score** | 10 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

## Expected Behavior
I ran into a weird error in a test run, just using the temporal auto-setup docker image version 1.19.1. I don't think I'm doing anything special. Just starting some basic containers.

## Actual Behavior
The following logs show how temporal failed to start
https://gist.github.com/vikstrous2/7d016b5562903b723d93b6a403589620

## Steps to Reproduce the Problem
Start temporal from this docker-compose file over and over again until this error triggers:


```
version: '3.4'
services:
  temporal-db:
    image: postgres:9.6.24-alpine@sha256:8342bcb43446694428ec6594e72e4299692854f0fc3aca090b0ab46f4c7f32a1
    restart: unless-stopped
    environment:
      POSTGRES_PASSWORD: temporal
      POSTGRES_USER: temporal
    ports:
      - 5434:5432
    healthcheck:
      interval: 1000h
      test: 'true'
  temporal:
    image: temporalio/auto-setup:1.19.1@sha256:3b582c47c354e7f9958c098f168ceb514766ab93526e9be1d772179663710d0f
    restart: unless-stopped
    depends_on:
      - temporal-db
    environment:
      - DB=postgresql
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=temporal
      - POSTGRES_SEEDS=temporal-db
    ports:
      - 7233:7233
    healthcheck:
      interval: 1000h
      test: 'true'
```

## Specifications
* Version: 1.19.1
* Platform: docker



#### Comments (6)

<details>
<summary><strong>yiminc</strong> commented on 2023-03-03 22:41:31.000 UTC</summary>

It seems it is coming from the ringpop (which uses tchannel). We do plan to deprecate ringpop and replace with other better maintained membership library.

Reactions: üëç 2

</details>

<details>
<summary><strong>waseemshahwan</strong> commented on 2023-04-03 08:42:42.000 UTC</summary>

Regularly crashing in our CI environments. Every ~1/20 pipeline executions.

</details>

<details>
<summary><strong>MikePresman</strong> commented on 2023-04-24 22:24:35.000 UTC</summary>

Crashing our CI environments as well. More detail here -> https://github.com/temporalio/cli/issues/212

</details>

<details>
<summary><strong>shojaeix</strong> commented on 2023-07-31 11:46:36.000 UTC</summary>

It also crashed our CI a few times per week. I wish there was a way to control/catch the error before it happens

</details>

<details>
<summary><strong>MikePresman</strong> commented on 2023-10-07 19:04:00.000 UTC</summary>

What's the progress on this?

</details>

<details>
<summary><strong>nocive</strong> commented on 2023-12-14 13:07:33.000 UTC</summary>

bump

Reactions: üëç 3

</details>


---

### #2621: Align TLS enable env var names between temporal-sql-tool and auto-setup 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2621 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2022-03-18 18:11:46.000 UTC (3y 9m ago) |
| **Updated** | 2023-03-03 20:20:46.000 UTC |
| **Upvotes** | 5 |
| **Comments** | 0 |
| **Priority Score** | 10 |
| **Labels** | enhancement, config |
| **Assignees** | jbreiding |
| **Milestone** | None |
| **Reactions** | üëç 5 |

#### Description

Config template uses SQL_TLS_ENABLED: https://github.com/temporalio/temporal/blob/master/docker/config_template.yaml#L77
temporal-sql-tool uses SQL_TLS: https://github.com/temporalio/temporal/blob/master/tools/sql/main.go#L113

This causes some confusion for our users. Would be nice to align these two.
Related issue: https://github.com/temporalio/temporal/issues/2293#


---

### #1499: Ability to update workflow config, including cron schedule, retry policy, etc

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1499 |
| **State** | OPEN |
| **Author** | danielwang5 (Daniel Wang) |
| **Created** | 2021-04-27 21:48:28.000 UTC (4y 8m ago) |
| **Updated** | 2023-03-03 20:22:04.000 UTC |
| **Upvotes** | 4 |
| **Comments** | 2 |
| **Priority Score** | 10 |
| **Labels** | enhancement, CRON, difficulty: medium, planning |
| **Assignees** | dnr, yiminc |
| **Milestone** | None |
| **Reactions** | üëç 4 |

#### Description

Has this been added for temporal? https://github.com/uber/cadence/issues/2651

The ability to update a workflow config, including but not limited to CronSchedule,SearchAttributes, etc. without having to cancel and restart the workflow would be nice.

#### Comments (2)

<details>
<summary><strong>KamalAman</strong> commented on 2021-06-26 16:56:07.000 UTC</summary>

Is there any sort of API design docs available to see how this might look like? It would be nice to see what this might look like.

</details>

<details>
<summary><strong>bergundy</strong> commented on 2021-11-23 01:47:40.000 UTC</summary>

@yiminc mentioning you here so you prioritize this

</details>


---

### #1203: Add SignalWithReset

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1203 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2021-01-20 02:25:45.000 UTC (4y 11m ago) |
| **Updated** | 2023-03-03 20:22:35.000 UTC |
| **Upvotes** | 5 |
| **Comments** | 0 |
| **Priority Score** | 10 |
| **Labels** | enhancement, API, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 5 |

#### Description

**Is your feature request related to a problem? Please describe.**
[This post](https://community.temporal.io/t/use-case-async-input-collector/1332) in the forum describes a DSL workflow that supports reexecution of the past tasks based on a signal. Such reexecution is trivial to implement for an open workflow. The problem is that it is not possible to signal a closed workflow and resetting a workflow is not enough to change its behavior.

**Describe the solution you'd like**
I propose adding `SignalWithReset` API. When called the service is going to reset the workflow to the last workflow task and deliver the signal to it atomically. This would allow the workflow to handle the new signal according to its business logic.




---

### #6806: WorkflowTaskTimedOut if submiting a large number of activiteis within one workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6806 |
| **State** | OPEN |
| **Author** | mikellxy |
| **Created** | 2024-11-13 11:59:23.000 UTC (1y 1m ago) |
| **Updated** | 2025-10-07 19:54:47.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 3 |
| **Priority Score** | 9 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 3 |

#### Description

hi, I apologize for using an issue to seek help.

## Expected Behavior
In my workflow worker, firstly I start a timer to wait for the biz data to be ready. Then 1,000 activies are submitted, i expected that all these activities can be scheduled and run asynchronously after the submission. Then I wait for the results of the 1,000 Future object like a wait group model. 

This is a simplified version of my code:
```go
func (w *Worker) Start(wfCtx workflow.Context, req *Req) (result string, err error) {
	if err := workflow.Sleep(wfCtx, 30*time.Second); err != nil {
		return err.Error(), err
	}
        // submit 1000 activities
        var futures []workflow.Future
	for i:=0;i<1000;i++ {
                 f := workflow.ExecuteActivity(wfCtx, w.Act, req, i)
	         futures = append(futures, f)
	}
        //  Wait for all tasks to run successfully
	for _, f := range futures {
		res := new(Result)
		err := f.Get(wfCtx, res)
		if err != nil {
			return err.Error(), err
		}
		if res.KnownErr != "" {
			logger.Info("known error: %v, workflow can return directly")
			return res.KnownErr, nil
		}
	}
}
```
the activity options as follow:  
```go
func (w *Worker) GetActivityContext(wfCtx workflow.Context) workflow.Context {
	return workflow.WithActivityOptions(wfCtx, workflow.ActivityOptions{
		ScheduleToCloseTimeout: time.Hour,
		StartToCloseTimeout:    time.Hour,
		WaitForCancellation:    false,
		RetryPolicy: &temporal.RetryPolicy{
			MaximumAttempts:        20,         
			NonRetryableErrorTypes: []string{}, 
		},
	})
}
```
the workflow options:  
```json
"start_workflow_options": {
            "workflow_run_timeout": "1d",
            "workflow_task_timeout": "90m",
            "retry_policy": {
              "initial_interval": "1s",
              "backoff_coefficient": 2.0,
              "maximum_interval": "1h",
              "maximum_attempts": 3
            }
          }
```
the worker options:  
```json
"options": {
            "max_concurrent_activity_execution_size": 10
          }
```

## Actual Behavior
It reports WorkflowTaskTimeOut, and none of the activities was scheduled. 
The timeline is as follow:
1. WorkflowExecutionStarted
2. WorkflowTaskScheduled
3. WorkflowTaskStarted
4. WorkflowTaskCompleted
5. TimerStarted
6. TimerFired
7. WorkflowTaskScheduled  (happend at time T)
8. WorkflowTaskStarted
9. WorkflowTaskTimedOut (happend at time T+2 minutes, timeout Type was StartToClose. but my activities' StartToCloseTimeout option is one hour)

p.s. if i reduce the number of activities to 100, all the activities run successfully.


## Steps to Reproduce the Problem

  1.
  1.
  1.

## Specifications

  - Version:
  - Platform:


#### Comments (3)

<details>
<summary><strong>bergundy</strong> commented on 2024-11-14 23:30:45.000 UTC</summary>

There's a hard coded 4 MB limit for a gRPC response. If the accumulated size of the schedule activity commands goes above that limit the RespondWorkflowTaskCompleted request will be rejected and it will manifest as a timeout.

This is a known issue without a good mitigation today.
Please confirm this theory, there may be some logs emitted by the Go SDK.
As a workaround, you can add a sleep after every 100 activities scheduled.

</details>

<details>
<summary><strong>drewhoskins-temporal</strong> commented on 2024-11-14 23:55:44.000 UTC</summary>

There's a feature request open for a possible fix to this issue: https://github.com/temporalio/features/issues/363

</details>

<details>
<summary><strong>garfieldnate</strong> commented on 2025-10-07 19:54:47.000 UTC</summary>

>As a workaround, you can add a sleep after every 100 activities scheduled.

@bergundy How long of a sleep is required? Or, just a sleep(0) to allow a message-sending task to take precedent?

</details>


---

### #5741: Addressing a lot of security vulnerabilities in the latest Temporal admin-tools release 1.23.0

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5741 |
| **State** | OPEN |
| **Author** | sonpham96 |
| **Created** | 2024-04-17 09:47:18.000 UTC (1y 8m ago) |
| **Updated** | 2024-10-21 07:48:56.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 9 |
| **Priority Score** | 9 |
| **Labels** | potential-bug |
| **Assignees** | alexshtin |
| **Milestone** | None |
| **Reactions** | ‚ù§Ô∏è 1 |

#### Description

## Expected Behavior
There is no CVE found in the `temporalio/admin-tools` image.

## Actual Behavior
There are **30** vulnerabilities found for image temporalio/admin-tools:1.23.0, including 7 high, 20 medium and 3 low CVEs.

Scan results:
```sh
Scan results for: image temporalio/admin-tools:1.23.0 sha256:eea33c3a95cb7a67f4b10020f04f5fbd9ef4ead7e02c0945ba3e39b5cac30dfd
Vulnerabilities
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
|       CVE        | SEVERITY | CVSS |                                   PACKAGE                                   |                VERSION                |             STATUS              | PUBLISHED  | DISCOVERED |                    DESCRIPTION                     |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| PRISMA-2022-0168 | high     | 7.80 | pip                                                                         | 24.0                                  | open                            | > 1 years  | < 1 hour   | An issue was discovered in pip (all versions)      |
|                  |          |      |                                                                             |                                       |                                 |            |            | because it installs the version with the highest   |
|                  |          |      |                                                                             |                                       |                                 |            |            | version number, even if the user had intended to   |
|                  |          |      |                                                                             |                                       |                                 |            |            | obtain...                                          |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-47108   | high     | 7.50 | go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc | v0.42.0                               | fixed in 0.46.0                 | > 5 months | < 1 hour   | OpenTelemetry-Go Contrib is a collection of        |
|                  |          |      |                                                                             |                                       | > 5 months ago                  |            |            | third-party packages for OpenTelemetry-Go.         |
|                  |          |      |                                                                             |                                       |                                 |            |            | Prior to version 0.46.0, the grpc Unary Server     |
|                  |          |      |                                                                             |                                       |                                 |            |            | Interceptor out ...                                |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-47108   | high     | 7.50 | go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc | v0.36.4                               | fixed in 0.46.0                 | > 5 months | < 1 hour   | OpenTelemetry-Go Contrib is a collection of        |
|                  |          |      |                                                                             |                                       | > 5 months ago                  |            |            | third-party packages for OpenTelemetry-Go.         |
|                  |          |      |                                                                             |                                       |                                 |            |            | Prior to version 0.46.0, the grpc Unary Server     |
|                  |          |      |                                                                             |                                       |                                 |            |            | Interceptor out ...                                |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-39325   | high     | 7.50 | golang.org/x/net/http2                                                      | v0.7.0                                | fixed in 0.17.0                 | > 6 months | < 1 hour   | A malicious HTTP/2 client which rapidly creates    |
|                  |          |      |                                                                             |                                       | 52 days ago                     |            |            | requests and immediately resets them can cause     |
|                  |          |      |                                                                             |                                       |                                 |            |            | excessive server resource consumption. While the   |
|                  |          |      |                                                                             |                                       |                                 |            |            | total ...                                          |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-44487   | high     | 5.30 | golang.org/x/net                                                            | v0.7.0                                | fixed in 0.17.0                 | > 6 months | < 1 hour   | The HTTP/2 protocol allows a denial of service     |
|                  |          |      |                                                                             |                                       | > 6 months ago                  |            |            | (server resource consumption) because request      |
|                  |          |      |                                                                             |                                       |                                 |            |            | cancellation can reset many streams quickly, as    |
|                  |          |      |                                                                             |                                       |                                 |            |            | exploited...                                       |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-44487   | high     | 5.30 | golang.org/x/net                                                            | v0.15.0                               | fixed in 0.17.0                 | > 6 months | < 1 hour   | The HTTP/2 protocol allows a denial of service     |
|                  |          |      |                                                                             |                                       | > 6 months ago                  |            |            | (server resource consumption) because request      |
|                  |          |      |                                                                             |                                       |                                 |            |            | cancellation can reset many streams quickly, as    |
|                  |          |      |                                                                             |                                       |                                 |            |            | exploited...                                       |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-44487   | high     | 5.30 | google.golang.org/grpc                                                      | v1.53.0                               | fixed in 1.58.3, 1.57.1, 1.56.3 | > 6 months | < 1 hour   | The HTTP/2 protocol allows a denial of service     |
|                  |          |      |                                                                             |                                       | > 5 months ago                  |            |            | (server resource consumption) because request      |
|                  |          |      |                                                                             |                                       |                                 |            |            | cancellation can reset many streams quickly, as    |
|                  |          |      |                                                                             |                                       |                                 |            |            | exploited...                                       |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| PRISMA-2023-0056 | medium   | 6.20 | github.com/sirupsen/logrus                                                  | v1.9.0                                | fixed in v1.9.3                 | > 1 years  | < 1 hour   | The github.com/sirupsen/logrus module of all       |
|                  |          |      |                                                                             |                                       | > 1 years ago                   |            |            | versions is vulnerable to denial of service.       |
|                  |          |      |                                                                             |                                       |                                 |            |            | Logging more than 64kb of data in a single entry   |
|                  |          |      |                                                                             |                                       |                                 |            |            | without new...                                     |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2022-40897   | medium   | 5.90 | setuptools                                                                  | 65.5.0                                | fixed in 65.5.1                 | > 1 years  | < 1 hour   | Python Packaging Authority (PyPA) setuptools       |
|                  |          |      |                                                                             |                                       | > 1 years ago                   |            |            | before 65.5.1 allows remote attackers to cause a   |
|                  |          |      |                                                                             |                                       |                                 |            |            | denial of service via HTML in a crafted package or |
|                  |          |      |                                                                             |                                       |                                 |            |            | custo...                                           |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-6992    | medium   | 5.50 | zlib                                                                        | 1.3.1-r0                              |                                 | > 3 months | < 1 hour   | Cloudflare version of zlib library was found       |
|                  |          |      |                                                                             |                                       |                                 |            |            | to be vulnerable to memory corruption issues       |
|                  |          |      |                                                                             |                                       |                                 |            |            | affecting the deflation algorithm implementation   |
|                  |          |      |                                                                             |                                       |                                 |            |            | (deflate.c)...                                     |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-42366   | medium   | 5.50 | busybox                                                                     | 1.36.1                                |                                 | > 4 months | < 1 hour   | A heap-buffer-overflow was discovered in BusyBox   |
|                  |          |      |                                                                             |                                       |                                 |            |            | v.1.36.1 in the next_token function at awk.c:1159. |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-42365   | medium   | 5.50 | busybox                                                                     | 1.36.1                                |                                 | > 4 months | < 1 hour   | A use-after-free vulnerability was discovered in   |
|                  |          |      |                                                                             |                                       |                                 |            |            | BusyBox v.1.36.1 via a crafted awk pattern in the  |
|                  |          |      |                                                                             |                                       |                                 |            |            | awk.c copyvar function.                            |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-42364   | medium   | 5.50 | busybox                                                                     | 1.36.1                                |                                 | > 4 months | < 1 hour   | A use-after-free vulnerability in BusyBox v.1.36.1 |
|                  |          |      |                                                                             |                                       |                                 |            |            | allows attackers to cause a denial of service      |
|                  |          |      |                                                                             |                                       |                                 |            |            | via a crafted awk pattern in the awk.c evaluate    |
|                  |          |      |                                                                             |                                       |                                 |            |            | funct...                                           |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-42363   | medium   | 5.50 | busybox                                                                     | 1.36.1                                |                                 | > 4 months | < 1 hour   | A use-after-free vulnerability was discovered      |
|                  |          |      |                                                                             |                                       |                                 |            |            | in xasprintf function in xfuncs_printf.c:344 in    |
|                  |          |      |                                                                             |                                       |                                 |            |            | BusyBox v.1.36.1.                                  |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-2435    | moderate | 4.30 | github.com/temporalio/ui-server/v2                                          | v2.21.3                               | fixed in 2.25.0                 | 14 days    | < 1 hour   | For an attacker with pre-existing access to send   |
|                  |          |      |                                                                             |                                       | 14 days ago                     |            |            | a signal to a workflow, the attacker can make the  |
|                  |          |      |                                                                             |                                       |                                 |            |            | signal name a script that executes when a victim   |
|                  |          |      |                                                                             |                                       |                                 |            |            | vi...                                              |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-28180   | moderate | 0.00 | gopkg.in/square/go-jose.v2                                                  | v2.6.0                                | fixed in                        | 39 days    | < 1 hour   | Package jose aims to provide an implementation     |
|                  |          |      |                                                                             |                                       | 32 days ago                     |            |            | of the Javascript Object Signing and Encryption    |
|                  |          |      |                                                                             |                                       |                                 |            |            | set of standards. An attacker could send a JWE     |
|                  |          |      |                                                                             |                                       |                                 |            |            | containi...                                        |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-27304   | moderate | 0.00 | github.com/jackc/pgx/v5/internal/sanitize                                   | v5.4.3                                | fixed in 5.5.4                  | 42 days    | < 1 hour   | pgx: SQL Injection via Protocol Message Size       |
|                  |          |      |                                                                             |                                       | 33 days ago                     |            |            | Overflow                                           |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-27304   | moderate | 0.00 | github.com/jackc/pgx/v5/pgconn                                              | v5.4.3                                | fixed in 5.5.4                  | 42 days    | < 1 hour   | pgx: SQL Injection via Protocol Message Size       |
|                  |          |      |                                                                             |                                       | 33 days ago                     |            |            | Overflow                                           |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-27304   | moderate | 0.00 | github.com/jackc/pgx/v5/pgproto3                                            | v5.4.3                                | fixed in 5.5.4                  | 42 days    | < 1 hour   | pgx: SQL Injection via Protocol Message Size       |
|                  |          |      |                                                                             |                                       | 33 days ago                     |            |            | Overflow                                           |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-24786   | moderate | 0.00 | google.golang.org/protobuf/encoding/protojson                               | v1.28.1                               | fixed in 1.33.0                 | 42 days    | < 1 hour   | The protojson.Unmarshal function can enter an      |
|                  |          |      |                                                                             |                                       | 42 days ago                     |            |            | infinite loop when unmarshaling certain forms      |
|                  |          |      |                                                                             |                                       |                                 |            |            | of invalid JSON. This condition can occur when     |
|                  |          |      |                                                                             |                                       |                                 |            |            | unmarshalin...                                     |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-24786   | moderate | 0.00 | google.golang.org/protobuf/internal/encoding/json                           | v1.31.0                               | fixed in 1.33.0                 | 42 days    | < 1 hour   | The protojson.Unmarshal function can enter an      |
|                  |          |      |                                                                             |                                       | 42 days ago                     |            |            | infinite loop when unmarshaling certain forms      |
|                  |          |      |                                                                             |                                       |                                 |            |            | of invalid JSON. This condition can occur when     |
|                  |          |      |                                                                             |                                       |                                 |            |            | unmarshalin...                                     |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-24786   | moderate | 0.00 | google.golang.org/protobuf/encoding/protojson                               | v1.31.0                               | fixed in 1.33.0                 | 42 days    | < 1 hour   | The protojson.Unmarshal function can enter an      |
|                  |          |      |                                                                             |                                       | 42 days ago                     |            |            | infinite loop when unmarshaling certain forms      |
|                  |          |      |                                                                             |                                       |                                 |            |            | of invalid JSON. This condition can occur when     |
|                  |          |      |                                                                             |                                       |                                 |            |            | unmarshalin...                                     |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-24786   | moderate | 0.00 | google.golang.org/protobuf/internal/encoding/json                           | v1.28.1                               | fixed in 1.33.0                 | 42 days    | < 1 hour   | The protojson.Unmarshal function can enter an      |
|                  |          |      |                                                                             |                                       | 42 days ago                     |            |            | infinite loop when unmarshaling certain forms      |
|                  |          |      |                                                                             |                                       |                                 |            |            | of invalid JSON. This condition can occur when     |
|                  |          |      |                                                                             |                                       |                                 |            |            | unmarshalin...                                     |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-45288   | moderate | 0.00 | golang.org/x/net/http2                                                      | v0.22.0                               | fixed in 0.23.0                 | 12 days    | < 1 hour   | An attacker may cause an HTTP/2 endpoint to        |
|                  |          |      |                                                                             |                                       | 12 days ago                     |            |            | read arbitrary amounts of header data by sending   |
|                  |          |      |                                                                             |                                       |                                 |            |            | an excessive number of CONTINUATION frames.        |
|                  |          |      |                                                                             |                                       |                                 |            |            | Maintaining H...                                   |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-45288   | moderate | 0.00 | golang.org/x/net/http2                                                      | v0.7.0                                | fixed in 0.23.0                 | 12 days    | < 1 hour   | An attacker may cause an HTTP/2 endpoint to        |
|                  |          |      |                                                                             |                                       | 12 days ago                     |            |            | read arbitrary amounts of header data by sending   |
|                  |          |      |                                                                             |                                       |                                 |            |            | an excessive number of CONTINUATION frames.        |
|                  |          |      |                                                                             |                                       |                                 |            |            | Maintaining H...                                   |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-45288   | moderate | 0.00 | golang.org/x/net/http2                                                      | v0.18.0                               | fixed in 0.23.0                 | 12 days    | < 1 hour   | An attacker may cause an HTTP/2 endpoint to        |
|                  |          |      |                                                                             |                                       | 12 days ago                     |            |            | read arbitrary amounts of header data by sending   |
|                  |          |      |                                                                             |                                       |                                 |            |            | an excessive number of CONTINUATION frames.        |
|                  |          |      |                                                                             |                                       |                                 |            |            | Maintaining H...                                   |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-45288   | moderate | 0.00 | net/http                                                                    | 1.22.1                                | fixed in 1.21.9, 1.22.2         | 12 days    | < 1 hour   | An attacker may cause an HTTP/2 endpoint to        |
|                  |          |      |                                                                             |                                       | 12 days ago                     |            |            | read arbitrary amounts of header data by sending   |
|                  |          |      |                                                                             |                                       |                                 |            |            | an excessive number of CONTINUATION frames.        |
|                  |          |      |                                                                             |                                       |                                 |            |            | Maintaining H...                                   |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-3485    | low      | 3.00 | go.temporal.io/server                                                       | v1.18.1-0.20230217005328-b313b7f58641 | fixed in 1.20.0                 | > 9 months | < 1 hour   | Insecure defaults in open-source Temporal Server   |
|                  |          |      |                                                                             |                                       | > 9 months ago                  |            |            | before version 1.20 on all platforms allows an     |
|                  |          |      |                                                                             |                                       |                                 |            |            | attacker to craft a task token with access to a    |
|                  |          |      |                                                                             |                                       |                                 |            |            | namesp...                                          |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-25629   | low      | 0.00 | c-ares                                                                      | 1.24.0-r1                             | fixed in 1.27.0-r0              | 53 days    | < 1 hour   | c-ares is a C library for asynchronous DNS         |
|                  |          |      |                                                                             |                                       | 22 days ago                     |            |            | requests. `ares__read_line()` is used to           |
|                  |          |      |                                                                             |                                       |                                 |            |            | parse local configuration files such as            |
|                  |          |      |                                                                             |                                       |                                 |            |            | `/etc/resolv.conf`, `/etc/...                      |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-2511    | low      | 0.00 | openssl                                                                     | 3.1.4-r5                              | fixed in 3.1.4-r6               | n/a        | < 1 hour   | Issue summary: Some non-default TLS server         |
|                  |          |      |                                                                             |                                       | 7 days ago                      |            |            | configurations can cause unbounded memory growth   |
|                  |          |      |                                                                             |                                       |                                 |            |            | when processing TLSv1.3 sessions  Impact summary:  |
|                  |          |      |                                                                             |                                       |                                 |            |            | An attac...                                        |
+------------------+----------+------+-----------------------------------------------------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+

Vulnerabilities found for image temporalio/admin-tools:1.23.0: total - 30, critical - 0, high - 7, medium - 20, low - 3
Vulnerability threshold check results: PASS

Compliance found for image temporalio/admin-tools:1.23.0: total - 0, critical - 0, high - 0, medium - 0, low - 0
Compliance threshold check results: PASS
```

## Steps to Reproduce the Problem

  1. Pull the latest image `temporalio/admin-tools:1.23.0` from Dockerhub
  2. Scan the image with any vulnerability scanner

## Specifications

  - Version: `1.23.0`
  - Platform: N/A


#### Comments (9)

<details>
<summary><strong>sonpham96</strong> commented on 2024-05-02 08:51:27.000 UTC</summary>

@yycptt @yiminc, from what I found the CVEs originate from the outdated dependencies in `tctl` which is included in the temporal server image ([config](https://github.com/temporalio/docker-builds/blob/bd92d8f0a61be6ef750df9435690bd16298f92ed/server.Dockerfile#L26)) image and admin-tools image ([config](https://github.com/temporalio/docker-builds/blob/bd92d8f0a61be6ef750df9435690bd16298f92ed/admin-tools.Dockerfile#L11)). I believe upgrading the versions in tctl's [go.mod](https://github.com/temporalio/tctl/blob/main/go.mod) would resolve this issue and #5740.

</details>

<details>
<summary><strong>yycptt</strong> commented on 2024-05-02 18:26:52.000 UTC</summary>

`tctl` has already been deprecated and no longer being maintained. 
The binary is included in both server and admin-tools images but are not actually being used in any way. 

~~In the next release (1.24.0) which is coming soon, tctl will be removed from both images.~~

</details>

<details>
<summary><strong>yycptt</strong> commented on 2024-05-02 18:45:36.000 UTC</summary>

Update: Team is still discussing if `tctl` should be removed from the next release. If not, we will update the dependencies in it to address security vulnerabilities.

</details>

<details>
<summary><strong>hansliu</strong> commented on 2024-05-15 08:08:58.000 UTC</summary>

We are also seeing these security vulnerabilities after deploying 1.23.0 via DockerHub, could I know any updates on this?

</details>

<details>
<summary><strong>josh-berry</strong> commented on 2024-05-17 16:29:34.000 UTC</summary>

So, many of these (`pip`, `busybox`, the various Postgres modules) have nothing to do with tctl, and tctl does not depend on them directly or indirectly. @yiminc For those I would still ask the server team to take a look.

For the remaining tctl issues, I've merged a fix which addresses all the _relevant_ vulnerabilities‚Äînote that security scanning tools may still find vulnerabilities in code that happens to be linked in but is not used. (For example, I see that it's complaining about the HTTP library, because there's a server-side issue‚Äîbut tctl does not contain an HTTP server.)

Will try to get a release out in a bit for server folks to pick up.

</details>

<details>
<summary><strong>josh-berry</strong> commented on 2024-05-17 18:11:02.000 UTC</summary>

tctl 1.18.1 will be available shortly; passing back to @alexshtin for the server side things.

</details>

<details>
<summary><strong>alexshtin</strong> commented on 2024-05-22 23:54:44.000 UTC</summary>

I updated base images to the latest alpine 3.19. These base images will be used for future releases. But unfortunately many packages we install in admin-tools are behind latest versions in `apk` and they still have some of these CVEs. 

</details>

<details>
<summary><strong>vithubati</strong> commented on 2024-08-31 11:29:47.000 UTC</summary>

Hi when will you release the new image with the updated dependencies? 

</details>

<details>
<summary><strong>volver-13</strong> commented on 2024-10-21 07:48:54.000 UTC</summary>

Hi Everyone,

I just scanned the latest admin-tools image and has lots of high and critical vulnerabilities. Is there a plan to address them anytime soon?

```
temporalio/admin-tools:1.25.1-tctl-1.18.1-cli-1.1.0 (alpine 3.19.1)

Total: 76 (UNKNOWN: 4, LOW: 8, MEDIUM: 42, HIGH: 18, CRITICAL: 4)
```

```
temporalio/admin-tools:1.25 (alpine 3.19.1)

Total: 76 (UNKNOWN: 4, LOW: 8, MEDIUM: 42, HIGH: 18, CRITICAL: 4)
```

</details>


---

### #5286: Slack Notification Integration (or other messaging platform)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5286 |
| **State** | OPEN |
| **Author** | kevink1103 (Kevin (bum)) |
| **Created** | 2024-01-12 02:21:17.000 UTC (1y 11m ago) |
| **Updated** | 2025-04-14 11:38:17.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 3 |
| **Priority Score** | 9 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 3 üöÄ 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
As of now, you would have to create custom SDK metrics when you want to create some specific notifications.

**Describe the solution you'd like**
It would be great if there is a Slack menu in temporal web ui, allowing us to integrate with Slack and make custom messages (without having to implement on our own with SDK).
ex) `Workflow "HelloWorldWorkflow" just failed with an error: connection time out`

**Describe alternatives you've considered**
Or at least export flexible prometheus metrics with WorkflowID, WorkflowType...

**Additional context**


#### Comments (3)

<details>
<summary><strong>muratmirgun</strong> commented on 2024-01-13 14:53:04.000 UTC</summary>

Hey guys, I can take this issue üöÄ 

</details>

<details>
<summary><strong>kevink1103</strong> commented on 2024-03-12 06:42:16.000 UTC</summary>

@muratmirgun Thanks! are you currently working on this?

</details>

<details>
<summary><strong>georgeberar</strong> commented on 2025-04-14 11:38:16.000 UTC</summary>

Any updates on this? Would love to have Slack integration out of the box in Temporal directly without the need of extra development work on our side.

</details>


---

### #5022: Can't display archived workflows which have past the retention

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5022 |
| **State** | OPEN |
| **Author** | Aoao54 (Obu Yao) |
| **Created** | 2023-10-23 06:04:17.000 UTC (2y 2m ago) |
| **Updated** | 2025-09-15 07:32:35.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 5 |
| **Priority Score** | 9 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

I put this also on Slack : https://temporalio.slack.com/archives/CTRCR8RBP/p1697683052222109
## Expected Behavior
Display archived workflows which have past the retention from archived file store S3.

## Actual Behavior
Can't display it both in web-ui or tctl 

![image](https://github.com/temporalio/temporal/assets/84021683/bc01d620-1964-493a-8b96-6ed13e9c70b1)

`‚ûú  ~ tctl wf desc -w    hello_world_workflowID -r d07eaf4b-83d4-4bf6-b387-a3ae1caf584c                                    
Error: Describe workflow execution failed
Error Details: rpc error: code = NotFound desc = Workflow executionsRow not found.  WorkflowId: hello_world_workflowID, RunId: d07eaf4b-83d4-4bf6-b387-a3ae1caf584c
Stack trace:
goroutine 1 [running]:
runtime/debug.Stack()
	runtime/debug/stack.go:24 +0x64
runtime/debug.PrintStack()
	runtime/debug/stack.go:16 +0x1c
github.com/temporalio/tctl/cli_curr.printError({0x101f76e2c, 0x22}, {0x1026cd0e0, 0x140001262c0})
	github.com/temporalio/tctl/cli_curr/util.go:393 +0x1c0
github.com/temporalio/tctl/cli_curr.ErrorAndExit({0x101f76e2c?, 0x1026dd3a8?}, {0x1026cd0e0?, 0x140001262c0?})
	github.com/temporalio/tctl/cli_curr/util.go:404 +0x28
github.com/temporalio/tctl/cli_curr.describeWorkflowHelper(0x140000c0c60, {0x16efa77e8, 0x16}, {0x16efa7802, 0x24})
	github.com/temporalio/tctl/cli_curr/workflowCommands.go:905 +0x1f4
github.com/temporalio/tctl/cli_curr.DescribeWorkflow(0x140000c0c60)
	github.com/temporalio/tctl/cli_curr/workflowCommands.go:870 +0x68
github.com/temporalio/tctl/cli_curr.newWorkflowCommands.func15(0x140000a6640?)
	github.com/temporalio/tctl/cli_curr/workflow.go:237 +0x1c
github.com/urfave/cli.HandleAction({0x10241d620?, 0x1026c2060?}, 0x8?)
	github.com/urfave/cli@v1.22.10/app.go:526 +0x94
github.com/urfave/cli.Command.Run({{0x101f38abc, 0x8}, {0x0, 0x0}, {0x1400018b150, 0x1, 0x1}, {0x101f8357d, 0x26}, {0x0, ...}, ...}, ...)
	github.com/urfave/cli@v1.22.10/command.go:173 +0x504
github.com/urfave/cli.(*App).RunAsSubcommand(0x1400055d340, 0x140000c09a0)
	github.com/urfave/cli@v1.22.10/app.go:405 +0xa68
github.com/urfave/cli.Command.startApp({{0x101f395cc, 0x8}, {0x0, 0x0}, {0x1400018b720, 0x1, 0x1}, {0x101f5f7be, 0x19}, {0x0, ...}, ...}, ...)
	github.com/urfave/cli@v1.22.10/command.go:378 +0x9c4
github.com/urfave/cli.Command.Run({{0x101f395cc, 0x8}, {0x0, 0x0}, {0x1400018b720, 0x1, 0x1}, {0x101f5f7be, 0x19}, {0x0, ...}, ...}, ...)
	github.com/urfave/cli@v1.22.10/command.go:102 +0x650
github.com/urfave/cli.(*App).Run(0x1400055cfc0, {0x140001a6000, 0x7, 0x7})
	github.com/urfave/cli@v1.22.10/app.go:277 +0x7e4
main.main()
	./main.go:47 +0xc0`
## Steps to Reproduce the Problem

  1. Set up a local S3-server and local temporal-server with this archival config 
 

> archival:
>   history:
>     state: "enabled"
>     enableRead: true
>     provider:
>       s3store:
>         region: "us-east-1"
>         endpoint: "http://127.0.0.1:4566/"
>         s3ForcePathStyle: true
>   visibility:
>     state: "enabled"
>     enableRead: true
>     provider:
>       s3store:
>         region: "us-east-1"
>         endpoint: "http://127.0.0.1:4566/"
>         s3ForcePathStyle: true
> 
> namespaceDefaults:
>   archival:
>     history:
>       state: "enabled"
>       URI: "s3://temporal-development"
>     visibility:
>       state: "enabled"
>       URI: "s3://temporal-development"

  2. set a default namespace which the retention-period is 1h
> temporal operator namespace create --retention=1h default
  3. run some easy sample-go workflows. When the archived workflows past the retention, you can't open it from web-ui or describe it use tclt cmd.
> tctl wf desc -w    hello_world_workflowID -r d07eaf4b-83d4-4bf6-b387-a3ae1caf584c       

## Specifications

  - Version: 
    temporal-server: 1.22
    ui-server:  2.19.0
  - Platform: S3, MAC


#### Comments (5)

<details>
<summary><strong>yycptt</strong> commented on 2023-10-23 06:58:19.000 UTC</summary>

Describe workflow execution does not support archived workflows.

History for archived workflows can be retrieved from archival storage and returned via the GetWorkflowExecutionHistory API or `tctl wf show` when runID is specified.

</details>

<details>
<summary><strong>Aoao54</strong> commented on 2023-10-23 07:11:02.000 UTC</summary>

Thanks! Succeeded in getting history of archived workflow with **tctl wf show** . But still can't open it from web-ui. So this is supposed to be a UI bug. 
<img width="1528" alt="image" src="https://github.com/temporalio/temporal/assets/84021683/85488a43-5f32-4aa9-b562-67a643793a19">


</details>

<details>
<summary><strong>Marwan-Dalaty</strong> commented on 2024-05-27 08:08:01.000 UTC</summary>

HI I have similar issue to the above , from the UI it keeps loading and then give me no workflows found while using tctl I can find the archived workflows . It was working before normally

![Screenshot from 2024-05-27 12-07-27](https://github.com/temporalio/temporal/assets/109135194/e5d114b3-e720-4ae6-bacf-abe4edbb30d0)
`{"level":"error","ts":"2024-05-27T07:40:44.567Z","msg":"service failures","operation":"ListArchivedWorkflowExecutions","wf-namespace":"default","error":"RequestCanceled: request context canceled\ncaused by: context canceled","logging-call-at":"telemetry.go:341","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:341\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:174\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation.func2\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:238\ngo.temporal.io/server/service/frontend.(*NoopRedirectionPolicy).WithNamespaceRedirect\n\t/home/builder/temporal/service/frontend/dc_redirection_policy.go:125\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:235\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:195\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/authorization.(*interceptor).Interceptor\n\t/home/builder/temporal/common/authorization/interceptor.go:158\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/metrics.NewServerMetricsContextInjectorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.42.0/interceptor.go:344\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_logger.go:84\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_validator.go:111\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc.ServiceErrorInterceptor\n\t/home/builder/temporal/common/rpc/grpc.go:145\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1186\ngo.temporal.io/api/workflowservice/v1._WorkflowService_ListArchivedWorkflowExecutions_Handler\n\t/go/pkg/mod/go.temporal.io/api@v1.24.1-0.20231003165936-bb03061759c8/workflowservice/v1/service.pb.go:2059\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1376\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1753\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:998"}`



</details>

<details>
<summary><strong>kkcmadhu-IBM</strong> commented on 2025-05-28 14:33:00.000 UTC</summary>

i face the same issue.

</details>

<details>
<summary><strong>fabioyy</strong> commented on 2025-09-15 07:32:35.000 UTC</summary>

same here. I can get the workflow by using cli, but cannot get in the web ( passing workflow and run_id ) on the url.
strange that it was ok in the past. I redeployed my k8 cluster and reinstall the helm. ( a little older version ).

</details>


---

### #4387: Add blacklist/whitelist of signal and synchronous update types

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4387 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2023-05-23 22:49:22.000 UTC (2y 7m ago) |
| **Updated** | 2023-11-24 16:09:57.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 5 |
| **Priority Score** | 9 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 üéâ 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
Signals, even ones that workflow doesn't care about, still consume workflow history. In some use cases, signals are expected only in certain workflow states, and signals received in other states should be rejected.

**Describe the solution you'd like**
Workflow should support blacklisting and whitelisting signal and update types it cares about. Blacklisted signals and updates are rejected immediately without updating the mutable state and the history.



#### Comments (5)

<details>
<summary><strong>lorensr</strong> commented on 2023-05-27 06:31:27.000 UTC</summary>

Implementation: At Workflow Execution start time, you can specify an allow/denylist. List is mutable by command from Workflow.

In future, if we have a way for Workers to tell Server per-WorkflowType configuration, a default allow/denylist could be colocated with the Workflow definition, which in many cases would be better DX IMO.

</details>

<details>
<summary><strong>danthegoodman1</strong> commented on 2023-11-22 19:21:39.000 UTC</summary>

And update on this? We've had to move a lot _off_ temporal because of this

</details>

<details>
<summary><strong>danthegoodman1</strong> commented on 2023-11-22 19:22:26.000 UTC</summary>

Ideally we'd want to just be able to dynamically change an allow-list

</details>

<details>
<summary><strong>lorensr</strong> commented on 2023-11-24 06:19:43.000 UTC</summary>

@danthegoodman1 You can currently implement an Update validation handler (in all SDKs but PHP and TS, which is [coming soon](https://github.com/temporalio/sdk-typescript/pull/1277)) that rejects any Updates that aren't in your (dynamic) allowlist so they won't be included in History. See Validation phase: https://docs.temporal.io/workflows#update

</details>

<details>
<summary><strong>danthegoodman1</strong> commented on 2023-11-24 16:09:56.000 UTC</summary>

Yeah problem with that is mixing timeouts and such becomes hacky, but it‚Äôs a workaround 

</details>


---

### #3624: Reset should be allowed with pending childWF in current run, if current run is already closed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3624 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2022-11-19 00:13:02.000 UTC (3y 1m ago) |
| **Updated** | 2024-12-19 12:22:57.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 5 |
| **Priority Score** | 9 |
| **Labels** | None |
| **Assignees** | yiminc |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

https://github.com/temporalio/temporal/blob/0c735e4969e95ff8c113fc73ed15fca1bc2999c2/service/history/ndc/workflow_resetter.go#L290

Mirror issue https://github.com/uber/cadence/issues/5030

This is because child WF could get get closed after parent closed, and parent will not receive the children close signals. 

So that after parent is closed for any reasons(terminate/timeout/fail/etc), this parent is never allowed to reset to anywhere because it has a pending childWFs in its mutableState forever as "deadlock condition"

#### Comments (5)

<details>
<summary><strong>taonic</strong> commented on 2023-06-06 05:20:31.000 UTC</summary>

Relates to https://github.com/temporalio/temporal/issues/705

</details>

<details>
<summary><strong>MalteHB</strong> commented on 2024-01-05 09:30:34.000 UTC</summary>

Any updates on this @yiminc?

I am also encountering that a workflow, without any pending children, cannot be reset to anything else but the first `WorkflowTaskCompleted` due to the `WorkflowResetter encountered pending child workflows.` error. 

Reactions: üëç 1

</details>

<details>
<summary><strong>yujiachen-y</strong> commented on 2024-07-18 07:10:24.000 UTC</summary>

Hi @yiminc , do we have any plans to address this issue? I'm also stuck on this problem.

</details>

<details>
<summary><strong>snackattas</strong> commented on 2024-09-09 16:33:26.000 UTC</summary>

+1

</details>

<details>
<summary><strong>Vladyslav-Obochuk</strong> commented on 2024-12-19 12:22:56.000 UTC</summary>

+1

</details>


---

### #2691: Support for "workflow chain IDs" in all APIs that are able to address the "latest" run

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2691 |
| **State** | OPEN |
| **Author** | macrogreg (macrogreg) |
| **Created** | 2022-03-31 17:21:47.000 UTC (3y 9m ago) |
| **Updated** | 2023-03-03 20:20:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 9 |
| **Priority Score** | 9 |
| **Labels** | enhancement, API |
| **Assignees** | None |
| **Milestone** | None |

#### Description

We discussed this over slack. Adding an Issue here so that we can keep track. While this is not (yet) time-critical, I would like to design the [.NET SDK](https://github.com/temporalio/sdk-dotnet) under the assumption that this gets eventually implemented before we release production-ready versions of the SDK.

Below, I copy a slightly edited version of the slack conversation for context and records.

   **x  x  x  x  x  x  x  x  x  x  x  x**

Hey folks, I am trying to understand the feasibility of the following:

Consider the public server APIs that operate on a particular Workflow Run. E.g. `QueryWorkflow`, `TerminateWorkflowExecution`, and many others. These APIs tend to in-take `WorkflowExecution`, which is a tuple of (`workflow_id`, `run_id`). Also, for most (all?) such APIs the `run_id` may be omitted. In such cases the invocation will apply to the most recent run that carries the specified `workflow_id`.
This is the situation today. Please correct me if I am wrong. üòÉ 

Now two questions (first one may have been asked before).

**(1)**
Could those APIs be extended such that instead of specifying the `run_id`, the user could specify the `chain_start_run_id` (meaning `run_id` of the first (i.e. the oldest) run in the execution chain)? Then the API would apply to the most recent run (i.e. the newest) in the chain specified by the `chain_start_run_id`. If the chain finishes at some point and a new chain with the same workflow id is started, then invocations where (`run_id`, `chain_start_run_id`) is specified would not "flow" . They would continue to refer to the finished chain.

The purpose of this is hopefully clear: A chain represents a workflow with one or more runs (caused by retries, continue-as-new continuations, ...). Once such a chain finishes, the workflow logically concludes. A new chain is a completely new workflow (with the same workflow id). Typically, a user who interacts with a specific workflow does not want to switch to interacting with a new workflow without noticing.

I assume that the answer to this part of the question is Yes.
There is even a corresponding [PR for the API](https://github.com/temporalio/api/pull/151).

( There, `chain_start_run_id` is called `first_execution_run_id`, but the name does not matter at this stage. For the current discussion just the concept, not the term is critical, so I'll temporarily stick to `chain_start_run_id` for brevity/clarity. In fact, I would love to coin the term "_workflow chain id_", as it is such an important concept that it deserves its own name. But, again, this terminology is not in scope here. üòÉ ).

Either way, that PR does not really solve the issue completely. The SDKs need to not only be able to supply the `chain_start_run_id`, but also they need to know it. Thus:

**(2)**
Now the second (related) question: Can all those APIs be extended in a way so that their _return_ payload includes the `chain_start_run_id` of the chain that contains the run to which the call in fact applied?

For example:

A user calls `SignalWorkflowExecution(workflowId="W1", runId=null, ...)`.

This means "send a signal to the latest (=most recent) run with the workflow id `"W1"`".
Now, the server will determine the latest run with that workflow id and deliver the signal to that particular run. (Lets assume that the `run_id` of that run was `"R42"`.)

After that, the user probably wants to continue interacting with "the workflow" that was affected by that signal. On a technical level "the workflow" is a chain-of-workflow-runs. They likely want to continue interacting with the "latest" run _only_ as long as the "latest" run is still a part of the _same_ chain as the run `"R42"` was. If the chain finishes and a new run is started with the same workflow id, that run is no longer part of the same logical workflow. Then the user likely does _not_ want to interact with that run in the same session.

How can we enable the user to avoid unwillingly "overflowing" beyond the end of the workflow chain?

We ensure that `SignalWorkflowExecution(..)` includes the `chain_start_run_id` into its return payload. Then, after staring the set of interactions as described above, the client knows the `chain_start_run_id` of chain that contained the run with `run_id="R42"`. (Let's assume that `chain_start_run_id` was `"R18"`). So, all subsequent invocations would include that information.
E.g., to send another signal, the user invokes
`SignalWorkflowExecution(workflowId="W1", chain_start_run_id="R18", runId=null, ...)`

which means "send signal to the latest run in the chain with `chain_start_run_id="R18"`".

If the user wanted to address the actually latest run, without restricting the call to the same workflow chain as they interacted previously, they would simply no longer include the `chain_start_run_id`.

Problem solved. :)

This may sound a little complicated, but I believe once you think it though, it appears quite straight forward. And, of course, we do not actually expect users to deal with the complexity. Language-SDKs will store the `chain_start_run_id` into whatever object they use to refer to a workflow and to invoke APIs on it (e.g. to send a signal to a workflow).

So: my question to the server team is: how hard / feasible is it to extent the APIs in the manner described? It is something we can reasonably tackle?

Thank you!

   **x  x  x  x  x  x  x  x  x  x  x  x**

**Below is a minimally edited record of the Slack conversation about this topic between a few people.**



#### Comments (9)

<details>
<summary><strong>macrogreg</strong> commented on 2022-03-31 17:22:23.000 UTC</summary>

**Response from Slack**:

This should be pretty straight forward to do. Server already keep track of the `chain_start_run_id` as `FirstExecutionRunId` in mutable state.

</details>

<details>
<summary><strong>macrogreg</strong> commented on 2022-03-31 17:23:59.000 UTC</summary>

**Response from Slack**:

I agree that this might not be hard to do but I don‚Äôt think it‚Äôs the semantic we want to promulgate.  I realize that it‚Äôs possible to use the existing APIs in this fashion, but (with the acknowledgement that I‚Äôm not the most experienced user from the client side), I think this pattern promotes complexity instead of discouraging it.  Basically, I think that the correct model is for the user to **change** the workflow ID at the end of a chain.  So each Workflow is exactly one chain (in the parlance of macrogreg‚Äôs question).

Broadly speaking I think that we would be better served by directing our users to compose simpler semantic constructs to achieve complexity, and only adding new semantics like this ‚Äúchain_start_run_id‚Äù when the win is huge.  If the user follows the pattern that I‚Äôm proposing here, then he gets exactly the semantic macrogreg asked for using our existing API.

Please tell me if I am missing something.

</details>

<details>
<summary><strong>macrogreg</strong> commented on 2022-03-31 17:27:25.000 UTC</summary>

**Response from Slack**:

I completely agree that it would be conceptually much simpler if we simply disallowed to have multiple workflow chains with the same workflow id.

The problem discussed in this thread would not exist.

Moreover, we would not need to explain the confusing part about the workflow-id uniquely describing a _running_ chain, but not really uniquely describing a chain in general, because there may be multiple chains with the same workflow id as long as only one of them is running.

But, at some point, for some reason (probably a good one, but either way, it's a done deal now) we decided that we wanted to support a workflow-id-reuse-policy that allowed creating new workflow chains with an id that was used previously.

One of our central promises is that Temporal-base software is easy-to-use and robust. I am not sure how frequently the workflow-id-reuse-policy is used in practice. But in combination with a very powerful feature of being able to interact with a workflow that finished in the past, (query, result, ...), the workflow-id-reuse-policy does break the clean and simple "workflow id can be used to address a workflow" assumption.

The desired outcome of the strategy described in this thread is to solve a problem that, when it occurs, is hard to understand and diagnose for our users. Something that we strive to avoid for our users.

And, I think, the proposed solution is conceptually clean and simple with the help of an SDK that supports it. But perhaps there are other approaches to achieve this outcome that can work as well?

So, overall, I agree. It is easier and cleaner to discourage reusing workflow-ids. But if we believe that that feature has value and is not a historical mistake that we wish we did not make, then we need to have some way of protecting people from the caveat of the "chain overflow" described in this thread. üòÉ 

</details>

<details>
<summary><strong>macrogreg</strong> commented on 2022-03-31 17:28:14.000 UTC</summary>

**Response from Slack**:

the configured default already disallows workflow ID reuse.  Personally, I think it would be sufficient to provide a warning about this issue on the page where we tell users how to change the default.  There‚Äôs only so much you can go to prevent someone from shooting themselves in the foot.

</details>

<details>
<summary><strong>macrogreg</strong> commented on 2022-03-31 17:29:16.000 UTC</summary>

**Response from Slack**:

> the configured default already disallows workflow ID reuse

this is incorrect. The default ID reuse policy is "allow duplicate"

</details>

<details>
<summary><strong>macrogreg</strong> commented on 2022-03-31 17:30:40.000 UTC</summary>

**Response from Slack**:

> There‚Äôs only so much you can go to prevent someone from shooting themselves in the foot.

The "safety" proposed is, essentially, transparent / for free to the SDK users. What would be the drawback of having it?

</details>

<details>
<summary><strong>macrogreg</strong> commented on 2022-03-31 17:31:14.000 UTC</summary>

**Response from slack**:

I agree that if we disallow duplicate workflows with same ID, that would be much simpler. But reality is we allow it by the ID reuse policy. And there are many use cases that needed that feature. We cannot take it away. With that, I think the proposed solution is reasonable.

</details>

<details>
<summary><strong>macrogreg</strong> commented on 2022-03-31 17:31:50.000 UTC</summary>

**Response from Slack**:

Fun fact, we are not taking away ID reuse policy, but we are adding more option to it: https://github.com/temporalio/temporal/pull/2608 üòÑ 

</details>

<details>
<summary><strong>macrogreg</strong> commented on 2022-03-31 17:32:18.000 UTC</summary>

**Response from Slack**:

Ability to reuse ID is very important for many user facing scenarios.

</details>


---

### #471: temporal Docker fails to bind on multiple interfaces

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/471 |
| **State** | OPEN |
| **Author** | hazcod (Niels Hofmans) |
| **Created** | 2020-06-19 08:28:09.000 UTC (5y 6m ago) |
| **Updated** | 2025-10-03 08:59:32.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 7 |
| **Priority Score** | 9 |
| **Labels** | potential-bug, operations, needs-investigation |
| **Assignees** | wxing1292 |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

## Expected Behavior
`temporal/auto-setup:latest` should bind on `0.0.0.0` in docker scenarios, instead of binding to specific IPs.

## Actual Behavior
Binding multiple networks to the temporal docker container results in:
```
 {"level":"fatal","ts":"2020-06-19T08:24:43.081Z","msg":"ListenIP failed, unable to parse bindOnIP value %q or it is not IPv4 address","address":"172.23.0.4 172.19.0.3","logging-call-at":"rpc.go:186","stacktrace":"github.com/temporalio/temporal/common/log/loggerimpl.(*loggerImpl).Fatal\n\t/temporal/common/log/loggerimpl/logger.go:144\ngithub.com/temporalio/temporal/common/rpc.getListenIP\n\t/temporal/common/rpc/rpc.go:186\ngithub.com/temporalio/temporal/common/rpc.(*RPCFactory).GetGRPCListener\n\t/temporal/common/rpc/rpc.go:126\ngithub.com/temporalio/temporal/common/resource.New\n\t/temporal/common/resource/resourceImpl.go:154\ngithub.com/temporalio/temporal/service/history.NewService\n\t/temporal/service/history/service.go:471\ngithub.com/temporalio/temporal/cmd/server/temporal.(*server).startService\n\t/temporal/cmd/server/temporal/server.go:262\ngithub.com/temporalio/temporal/cmd/server/temporal.(*server).Start\n\t/temporal/cmd/server/temporal/server.go:85\ngithub.com/temporalio/temporal/cmd/server/temporal.startHandler\n\t/temporal/cmd/server/temporal/temporal.go:91\ngithub.com/temporalio/temporal/cmd/server/temporal.BuildCLI.func1\n\t/temporal/cmd/server/temporal/temporal.go:211\ngithub.com/urfave/cli.HandleAction\n\t/go/pkg/mod/github.com/urfave/cli@v1.22.4/app.go:528\ngithub.com/urfave/cli.Command.Run\n\t/go/pkg/mod/github.com/urfave/cli@v1.22.4/command.go:174\ngithub.com/urfave/cli.(*App).Run\n\t/go/pkg/mod/github.com/urfave/cli@v1.22.4/app.go:279\nmain.main\n\t/temporal/cmd/server/main.go:38\nruntime.main\n\t/usr/local/go/src/runtime/proc.go:203"}
```

## Steps to Reproduce the Problem
```yaml
  temporal:
    image: temporalio/auto-setup:latest
    restart: "on-failure:5"
    networks:
    - backend
    - backend2
    environment:
      - "DB=postgres"
      - "DB_PORT=26257"
      - "POSTGRES_USER=root"
      - "POSTGRES_PWD=postgres"
      - "POSTGRES_SEEDS=postgres"
    ports:
    - 7233
```



#### Comments (7)

<details>
<summary><strong>mikhailshilkov</strong> commented on 2020-09-07 21:10:37.000 UTC</summary>

I'm hitting the same error on Azure App Service. My `docker-compose.yml` is very similar to the default one, just without a cassandra container. No explicit `networks` configuration.

I get

```
{"level":"fatal","ts":"2020-09-07T21:00:28.845Z","msg":"ListenIP failed, unable to parse bindOnIP value or it 
is not IPv4 address","address":"172.16.3.2 172.16.0.3","logging-call-at":"rpc.go:186","stacktrace":
"go.temporal.io/server/common/log/loggerimpl.(*loggerImpl).Fatal\n\t/temporal/common/log/loggerimpl/logger.go:144
\ngo.temporal.io/server/common/rpc.getListenIP\n\t/temporal/common/rpc/rpc.go:186
\ngo.temporal.io/server/common/rpc.(*RPCFactory).GetGRPCListener\n\t/temporal/common/rpc/rpc.go:126
\ngo.temporal.io/server/common/resource.New\n\t/temporal/common/resource/resourceImpl.go:154
\ngo.temporal.io/server/service/history.NewService\n\t/temporal/service/history/service.go:479
\ngo.temporal.io/server/cmd/server/temporal.(*server).startService\n\t/temporal/cmd/server/temporal/server.go:265
\ngo.temporal.io/server/cmd/server/temporal.(*server).Start\n\t/temporal/cmd/server/temporal/server.go:85
\ngo.temporal.io/server/cmd/server/temporal.startHandler\n\t/temporal/cmd/server/temporal/temporal.go:91
\ngo.temporal.io/server/cmd/server/temporal.BuildCLI.func1\n\t/temporal/cmd/server/temporal/temporal.go:211
\ngithub.com/urfave/cli.HandleAction\n\t/go/pkg/mod/github.com/urfave/cli@v1.22.4/app.go:528
\ngithub.com/urfave/cli.Command.Run\n\t/go/pkg/mod/github.com/urfave/cli@v1.22.4/command.go:174
\ngithub.com/urfave/cli.(*App).Run\n\t/go/pkg/mod/github.com/urfave/cli@v1.22.4/app.go:279
\nmain.main\n\t/temporal/cmd/server/main.go:38\nruntime.main\n\t/usr/local/go/src/runtime/proc.go:203"}
```

Any workarounds?

</details>

<details>
<summary><strong>samarabbas</strong> commented on 2021-07-03 23:04:42.000 UTC</summary>

@wxing1292 can you confirm if this is still a problem?

</details>

<details>
<summary><strong>Miha-ha</strong> commented on 2021-07-28 14:34:24.000 UTC</summary>

I got the same error as soon as I added the second network:    
```
networks:
    - backend
    - backend2
```

</details>

<details>
<summary><strong>Miha-ha</strong> commented on 2021-08-09 13:37:18.000 UTC</summary>

Please clarify if a solution to this problem is planned?

</details>

<details>
<summary><strong>programmador</strong> commented on 2021-12-21 18:46:29.000 UTC</summary>

Crazyness, I've broken my brain solving it especially when there is actually no documentation for:
* this config: https://github.com/temporalio/temporal/blob/master/docker/config_template.yaml
* this config: https://github.com/temporalio/temporal/blob/master/config/dynamicconfig/development_es.yaml
* explanation how exactly the second of abovementioned configs should be used to overwrite the first one (They have different structure! Actually I cannot see what's common between them at all - they do not seem to share same keys, at least in abovementioned examples provided. For instance You won't find a word "system" in file which the first link is pointing to). The only documentation is "you can" [here](https://github.com/temporalio/temporal/blob/master/config/dynamicconfig/README.md). Wow... Thanks... It helped (sarcasm).
* explanation of environment variables used by temporal server. The only explanation on temporal.io website is about temporal-web, not about temporal server itself.

So after an evening of investigeting I've come across these issues:
* https://community.temporal.io/t/temporalio-temporal-server-overwrite-the-127-0-0-1-7233-ip-address-to-something-else/544
* https://github.com/temporalio/temporal/blob/master/docker/config_template.yaml#L214
* https://github.com/temporalio/temporal/blob/master/config/dynamicconfig/README.md
From which I've indirectly understood that could bind 0.0.0.0. Of course I could - this is first what I did but immediately encountered another error: I should propagate broadcastAddress. I had no Idea how (see my mention about docs above). And I still have no idea how to make it via config. I've also understood that broadcastAddress is used only for cluster intercommunication. Also I've seen mentions about few interesting env vars in those issues.

So this seems to work:
```
    temporal:
        depends_on:
            - mysql
            - elasticsearch
        environment:
            DBNAME: temporal
            VISIBILITY_DBNAME: temporal_visibility
            DB: mysql
            MYSQL_USER: temporal
            MYSQL_PWD: <passwd_here>
            MYSQL_SEEDS: mysql
            DYNAMIC_CONFIG_FILE_PATH: config/dynamicconfig/development_es.yaml
            ENABLE_ES: true
            ES_SEEDS: elasticsearch
            ES_VERSION: v7
            BIND_ON_IP: 0.0.0.0
            TEMPORAL_BROADCAST_ADDRESS: 127.0.0.1
        image: temporalio/auto-setup:1.14.0
        volumes:
            - ./dynamicconfig:/etc/temporal/config/dynamicconfig
        networks:
            - default # perform DB queries
            - traefik # receive requests from a load balancer
        labels:
            traefik.enable: 'true'
            traefik.frontend.rule: 'Host: temporal.local'
            traefik.port: '7233'
            traefik.protocol: 'h2c'

```

**The solution is the latter two env vars**. You could bind 0.0.0.0 though using 127.0.0.1 for cluster intercom:
```
            BIND_ON_IP: 0.0.0.0
            TEMPORAL_BROADCAST_ADDRESS: 127.0.0.1
```

Seems like not many people are encountering this problem if it wasn't answered 1.5 years.
Does everyone just expose a new port for every single stuff instead of using custom CA or LetsEncrypt and a load balancer? Really? Guuuuyz! How do you remember all those port numbers at all?

And one more thing. **This is not a bug**. Because:
* You **are able** to bind 0.0.0.0
* You **have to** know your node IP adress to set up production cluster environment (when 127.0.0.1 is not an option)

As for me the issue could be resolved as the above solution seems to work. Though would be nice to improve the doc. Yes I know I'm free to contribute instead of complaining :-)

BTW other solution would a way to disable cluster intercom at all if it is optional. I beleive it's optional if 127.0.0.1 is okay.

Reactions: üëç 29 üëÄ 1

</details>

<details>
<summary><strong>aleclerc-cio</strong> commented on 2024-07-31 18:39:32.000 UTC</summary>

Trying to add a healthcheck to temporal is tough since I can't curl since we're not bound to 127.0.0.1.

```
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:7233/health || exit 1"]
```


</details>

<details>
<summary><strong>emalihin</strong> commented on 2025-10-03 08:59:32.000 UTC</summary>

`command: ["CMD-SHELL", 'temporal operator cluster health --address $(hostname -i):7233']` if temporal is bound to an interface

Reactions: üëç 1

</details>


---

### #453: Feature: implement docker healthcheck

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/453 |
| **State** | OPEN |
| **Author** | hazcod (Niels Hofmans) |
| **Created** | 2020-06-14 12:41:44.000 UTC (5y 6m ago) |
| **Updated** | 2025-07-09 09:34:43.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 5 |
| **Priority Score** | 9 |
| **Labels** | enhancement, operations, up-for-grabs, P1 |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently docker will assume the container is ready once it is running, while the temporal engine is still warming up, leading to all sorts of problems.

**Describe the solution you'd like**
Implement the HEALTHCHECK directive in your Dockerfile(s).

**Describe alternatives you've considered**
Restarting clients.

**Additional context**


#### Comments (5)

<details>
<summary><strong>berardino</strong> commented on 2022-04-08 19:50:53.000 UTC</summary>

As a work around, until the HEALTHCHECK is added to the docker image,  I was able to implement a healthcheck in the docker-compose file as follow:

```yaml
healthcheck:
      test: [ "CMD", "tctl","cluster","h" ]
      interval: 10s
      timeout: 1m
      retries: 10
```

</details>

<details>
<summary><strong>igrayson</strong> commented on 2022-06-08 17:39:34.000 UTC</summary>

This is the docker-compose health-check that worked for me:
```yaml
    healthcheck:
      test:
        [
          "CMD",
          "tctl",
          "--address",
          "temporal:7233",
          "workflow",
          "list"
        ]
      interval: 1s
      timeout: 5s
      retries: 30
```

My temporal service didn't listen on `127.0.0.1`, so I added an `--address`. I used `workflow list` instead of `cluster h` as the latter doesn't seem to wait for the default namespace to be created causing a race condition for the error "namespace:default not found".

Reactions: üëç 13

</details>

<details>
<summary><strong>tarampampam</strong> commented on 2022-12-16 08:49:05.000 UTC</summary>

In addition, if you didn't want to set the container name, you can force temporal listening on any interface, eg:

```yaml
services:
  temporal:
    image: temporalio/auto-setup:1.19
    ports: ['7233:7233/tcp']
    environment: # https://github.com/temporalio/docker-builds/blob/main/docker/auto-setup.sh
      # ...
      BIND_ON_IP: 0.0.0.0
      TEMPORAL_BROADCAST_ADDRESS: 127.0.0.1
    healthcheck:
      test: ['CMD-SHELL', 'tctl --address 127.0.0.1:7233 workflow list']
      interval: 1s
      timeout: 5s
      start_period: 2s
```

Reactions: üëç 3 ‚ù§Ô∏è 3

</details>

<details>
<summary><strong>Mahmoud-Elbahnasawy</strong> commented on 2024-09-10 19:26:49.000 UTC</summary>

This is the health check that worker for me 
```
      test:
        ["CMD","tctl", "--address", "temporal:7233", "workflow", "list", "||", "exit 1"]
      interval: 1s
      timeout: 5s
      retries: 30
```
      and i added a condition on the container that needs to the wait the for the health check command to be succeeded this condition 

```
    depends_on:
      temporal:
        condition: service_healthy
```


Reactions: üëç 1 üéâ 2 ‚ù§Ô∏è 1

</details>

<details>
<summary><strong>adriantaut</strong> commented on 2025-07-09 09:34:43.000 UTC</summary>

this would be very helpful, especially for running ECS Task healthchecks inside the container...unfortunately the `tctl` only performs healthchecks for the `frontend` service, while there is no easy way to do healthchecks on `matching/history` services

</details>


---

### #421: Add admin command to dump dynamic config value[s]

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/421 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-05-29 17:02:45.000 UTC (5y 7m ago) |
| **Updated** | 2025-08-22 10:26:48.000 UTC |
| **Upvotes** | 4 |
| **Comments** | 1 |
| **Priority Score** | 9 |
| **Labels** | enhancement, difficulty: easy, operations, P1 |
| **Assignees** | feedmeapples |
| **Milestone** | None |
| **Reactions** | üëç 4 |

#### Description

Currently it is hard to know the current cluster configuration. Dumping all dynamic config values or at least a specific one would be very useful for troubleshooting.



#### Comments (1)

<details>
<summary><strong>vaibhavyadav-dev</strong> commented on 2025-08-19 20:27:06.000 UTC</summary>

@samarabbas @yiminc @mfateev @bergundy I've made PR, tell me if any changes required.
Thanks

</details>


---

### #196: Task list with rate limiting per key

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/196 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-03-09 17:03:34.000 UTC (5y 9m ago) |
| **Updated** | 2023-10-20 22:17:58.000 UTC |
| **Upvotes** | 4 |
| **Comments** | 1 |
| **Priority Score** | 9 |
| **Labels** | enhancement, difficulty: hard |
| **Assignees** | dnr |
| **Milestone** | None |
| **Reactions** | üëç 4 |

#### Description

There are multiple use cases that require rate limiting per dynamically created list of keys. For example some downstream API has rate limit per calling customer. This is possible to achieve using task list throttling, but as the set of keys is usually dynamic managing client side workers is a non trivial task.

The proposed feature is to provide a special task list type that would support rate limiting per some "partition key". So when an activity is scheduled the key should be provided when invoking it. And on the worker side no changes are needed (besides the mechanism to configure the task list itself).

#### Comments (1)

<details>
<summary><strong>lorensr</strong> commented on 2023-10-20 22:17:58.000 UTC</summary>

User has a feature request for:

> In this Queue, you can only process 1 activity at the time based on X field (per company, per whatever filtering)

I think the X field could be this key. And then if there were a concurrent-execution rate limit (versus a task per second rate limit) that could be set to 1, this would work for their case.

</details>


---

### #8205: Improve DST handling in schedules

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8205 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2025-08-18 19:18:52.000 UTC (4 months ago) |
| **Updated** | 2025-12-09 10:53:23.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 4 |
| **Priority Score** | 8 |
| **Labels** | enhancement, schedules |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

From slack:

According to the docs, when a cron is configured with local timestamp, we have the following behavior:
Beware Daylight Saving Time: If a Temporal Cron Job is scheduled around the time when daylight saving time (DST) begins or ends (for example, `30 2 * * *`), it might run zero, one, or two times in a day! The Cron library that we use does not do any special handling of DST transitions. Avoid schedules that include times that fall within DST transition periods.
Is there any plan to implement something like DST transition policies, along the line of:
* duplicate_dst: RUN_FIRST, RUN_LAST, RUN_NONE
* missing_dst: SHIFT, SKIP
or something like this that would control the behavior of Temporal Schedules in the case of DST transitions?

If not configurable, maybe make the default the most intuitive one, i.e. de-duplicate and shift missing run.
It's the default behavior of Airflow.

#### Comments (4)

<details>
<summary><strong>bergundy</strong> commented on 2025-08-18 19:19:08.000 UTC</summary>

https://temporalio.slack.com/archives/CTRCR8RBP/p1755181450558259

</details>

<details>
<summary><strong>korncola</strong> commented on 2025-08-20 08:36:24.000 UTC</summary>

old sysadmin rule in Europe: never schedule a cron EVER between including 2:00 - 3:00 üëç 
Also never use 23:59 or 0:00 cause leap second.

</details>

<details>
<summary><strong>kritika2</strong> commented on 2025-09-30 19:09:07.000 UTC</summary>

Is this issue available to be picked up?

</details>

<details>
<summary><strong>maximethebault</strong> commented on 2025-12-09 10:53:23.000 UTC</summary>

> old sysadmin rule in Europe: never schedule a cron EVER between including 2:00 - 3:00 üëç Also never use 23:59 or 0:00 cause leap second.

Agreed, always schedule in UTC when you can (and especially for sys-admin based tasks). However, there are some cases where you unfortunately need the schedule in local timezones. A simple example : triggering an e-mailing campaign at a local schedule (because 8AM vs 9AM makes a difference).

</details>


---

### #7666: –°oncurrency limit (built-in semaphore) per Workflow type

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7666 |
| **State** | OPEN |
| **Author** | isaryy |
| **Created** | 2025-04-27 23:37:33.000 UTC (8 months ago) |
| **Updated** | 2025-06-21 15:42:56.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 2 |
| **Priority Score** | 8 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 3 |

#### Description

**Is your feature request related to a problem? Please describe.**

It is a recurring orchestration need to limit the number of concurrently running Workflows of a specific type (e.g., never run more than N "MyWorkflow" simultaneously in a namespace or cluster). There is currently no built-in way to set a global concurrency limit by Workflow type in Temporal. Workarounds require custom dispatcher workflows or external distributed semaphores.

**Describe the solution you'd like**

A built-in semaphore/concurrency limit per Workflow type, configurable via API or workflow start options, so that Temporal itself restricts the number of simultaneously running workflows of a given type.

**Describe alternatives you've considered**

Building custom dispatcher workflows or using external distributed locks‚Äîbut these solutions are complex, fragile, and operationally heavy.

**Additional context**
A built-in semaphore would be a general and powerful solution for throttling, batching, and resource protection for a wide range of use cases.


#### Comments (2)

<details>
<summary><strong>bergundy</strong> commented on 2025-05-01 23:11:34.000 UTC</summary>

This is something that we are discussing internally but don't have clear timelines for.

</details>

<details>
<summary><strong>erdaltoprak</strong> commented on 2025-06-21 15:42:56.000 UTC</summary>

Hello @bergundy, are there any updates on concurrency limits for a given worker ? 

Reactions: üëç 6

</details>


---

### #3517: Provide a way to retrieve the list of failures for one activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3517 |
| **State** | OPEN |
| **Author** | antmendoza (Antonio Mendoza P√©rez) |
| **Created** | 2022-10-20 12:14:26.000 UTC (3y 2m ago) |
| **Updated** | 2025-05-06 11:07:59.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 2 |
| **Priority Score** | 8 |
| **Labels** | enhancement |
| **Assignees** | alexshtin |
| **Milestone** | None |
| **Reactions** | üëç 3 |

#### Description

**Is your feature request related to a problem? Please describe.**
When an activity retries more than one time, there is no way right now to access previous failures. Right now from both, the SDK and the UI, it is only possible to see the last failure.  

**Describe the solution you'd like**
It would be nice to have access to the activity failure history. 

**Describe alternatives you've considered**

**Additional context**


#### Comments (2)

<details>
<summary><strong>lorensr</strong> commented on 2022-12-20 23:58:24.000 UTC</summary>

Customers have asked how to know what the last failure was inside the activity code (either a server-side TimeoutFailure, in which case it would be helpful to know which timeout, or the activity throwing a retryable ApplicationFailure). I think this would be a useful feature.

> Right now from both, the SDK and the UI, it is only possible to see the last failure.

It's not possible to see from inside an activity the failure of the last activity try (unless the activity uses a Client to describe the workflow and look at `pendingActivities.lastFailure`). To get the last failure in the Activity Context ([like this](https://github.com/temporalio/sdk-features/issues/92)), the Worker would need a Failure returned in [PollActivityTaskQueueResponse](https://github.com/temporalio/api/blob/9645a99d585b426ebb7555b8f61f5b1ed1aa42af/temporal/api/workflowservice/v1/request_response.proto#L343) (or a list of all previous Failures).

cc @mjameswh 

</details>

<details>
<summary><strong>shamrin</strong> commented on 2025-05-06 11:07:58.000 UTC</summary>

I stumbled upon this problem recently. I had an activity failure due to a bug in my code. Temporal - as expected - was retrying the activity. However, only the first failure had a clue in about the bug. The rest of retried attempts failed due to db constraint violation, because the first failure left the system in semi-consistent state (unique ID was taken during the first attempt). It made the problem harder to debug, because I had to dig through worker logs to see the problem.

I think it would have helped if Temporal kept the details for the first failure too.

</details>


---

### #3141: Support reset to any point with pending ChildWorkflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3141 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2022-07-25 19:01:17.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:19:43.000 UTC |
| **Upvotes** | 4 |
| **Comments** | 0 |
| **Priority Score** | 8 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 4 |

#### Description

Mirror Cadence feature request(with proposal):  https://github.com/uber/cadence/issues/3914 

cc @yux0 @yycptt @meiliang86 LMK if you have any plan to implement this feature :D 
This becomes problematic because people want to use ChildWorkflow naturally. today, a workflow with childWorkflow can be not resettable which will be hard to recover from outage. 


---

### #954: docs: Add multi-region deployment documentation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/954 |
| **State** | OPEN |
| **Author** | robinbraemer (Robin Br√§mer) |
| **Created** | 2020-11-04 13:42:08.000 UTC (5y 1m ago) |
| **Updated** | 2022-11-22 19:06:35.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 2 |
| **Priority Score** | 8 |
| **Labels** | enhancement, database |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 3 üëÄ 1 |

#### Description

I would like to see in the documentation how Temporal could efficiently scale to consistently offer it's service to clients in multiple regions and how such setup would look like.
_(If it's even possible yet?)_

Beside others multiple decisions for Temporal have to be made for such a deployment.
- multi region storage backend deployment and locality awareness
  - additions to db tables in CockroachDB's case
  - e.g. [CockroachDB Geo-partitioning & Follower Reads](https://www.cockroachlabs.com/docs/stable/enterprise-licensing.html),  in which case we would need to add `PARTITION`s to CRDB tables
  - consistency trade-offs (e.g. when doing [Follower Reads](https://www.cockroachlabs.com/docs/v20.1/follower-reads))

**Additional context**
I strongly consider using Temporal in my organization but one strong requirement is to support multiple regions (especially with CockroachDB as storage backend) to service local clients within low latency.


#### Comments (2)

<details>
<summary><strong>morigs</strong> commented on 2022-11-19 11:15:21.000 UTC</summary>

As I understand from [this answer](https://community.temporal.io/t/temporal-performance-and-high-availablity/697) this is not supported.
It would be awesome if Temporal could work on top of cockroach/yugabyte/spanner/cosmos and be region aware

</details>

<details>
<summary><strong>morigs</strong> commented on 2022-11-19 11:21:56.000 UTC</summary>

Personally, I have a use case where it's required to execute a global workflow. Some tasks must be executed in specific locations only (also, actual implementation of the tasks will differ in different locations)

</details>


---

### #8719: Stale internode gRPC connections after pod termination when using Kubernetes Deployments

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8719 |
| **State** | OPEN |
| **Author** | Jrmy2402 (J√©r√©my S) |
| **Created** | 2025-11-28 15:31:57.000 UTC (1 months ago) |
| **Updated** | 2025-12-08 19:31:12.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 1 |
| **Priority Score** | 7 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 3 |

#### Description

## Expected Behavior
Temporal should drop internode gRPC connections to pods that no longer exist after they are removed from the membership ring, avoiding repeated dial attempts to stale pod IPs.

## Actual Behavior
We are experiencing the same issue described in PR [https://github.com/temporalio/temporal/pull/8586](https://github.com/temporalio/temporal/pull/8586?utm_source=chatgpt.com)
Temporal servers keep stale internode gRPC connections after Kubernetes pods are terminated (during scale-down, rolling updates, or node drains). This results in continuous dial tcp ‚Ä¶ i/o timeout errors targeting IPs of pods that no longer exist.

## Steps to Reproduce the Problem

1. Deploy Temporal using the official Helm chart (services running as Deployments).
2. Trigger pod churn (e.g., rolling update, scaling down, or node drain).
3. Observe internode gRPC logs showing repeated dial attempts to the IPs of pods that have already been terminated.

## Specifications

  - Version: 1.25.2
  - Platform: Kubernetes, using the official Temporal Helm chart, Temporal services deployed as Deployments

## Additional Note / Question

Before a fix becomes available, is there any known workaround?

We are considering switching from Deployment to StatefulSet to have more stable pod identities, but this would require diverging from the official Helm chart, something we would prefer to avoid.

Any guidance would be greatly appreciated!


#### Comments (1)

<details>
<summary><strong>jmooo</strong> commented on 2025-12-08 19:31:12.000 UTC</summary>

Is there a workaround people are currently using to handle this issue?

</details>


---

### #8461: Adding documentation for self-hosting temporal on cloud environments.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8461 |
| **State** | OPEN |
| **Author** | Pramodh-G (Pramodh Gopalan V) |
| **Created** | 2025-10-09 18:18:31.000 UTC (2 months ago) |
| **Updated** | 2025-11-19 06:57:52.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 3 |
| **Priority Score** | 7 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

Hi Temporal team!

In my day to day job, we recently tried deploying self-hosted temporal in both AWS EKS and Azure AKS. I found that there's a bunch of gotchas while deploying them. Some of them include:

- Lack of archival support for Azure with storage containers.
- AKS + Azure flexible postgres server fails with lots of tiny errors:
   - Azure forces you to have TLS by default, so you have to disable TLS when you deploy it.
   - To use visibility store as postgres in Azure, you need to enable BTREE_GIN extension which doesn't come by default.

I thought these might be helpful to add in documentation. Let me know if you think this is worthwhile and please feel free to leave comments.

#### Comments (3)

<details>
<summary><strong>wadhah101</strong> commented on 2025-10-15 12:01:54.000 UTC</summary>

> Azure forces you to have TLS by default, so you have to disable TLS when you deploy it.

How did you get over this ? 

</details>

<details>
<summary><strong>Pramodh-G</strong> commented on 2025-10-17 21:03:29.000 UTC</summary>

Two options here:

1. You can enable TLS, azure already loads up the certificates in the root correctly.
2. Disable TLS on azure infra side: This can be done with:
     - Navigating to your Postgres flexible server instance
     - Choose Server PArameters on the sidebar.
     - Search for `require_secure_transport` -> Set value from on to off.
     - save and redeploy.


In AWS - TLS is not strictly necessary but azure enforces it. We ended up with going to 2, as we deploy temporal in AWS without TLs and wanted to standardize.

</details>

<details>
<summary><strong>AlonGluz</strong> commented on 2025-11-19 06:57:52.000 UTC</summary>

Can I take this?, I just deployed Self hoted Temporal on EKS 

</details>


---

### #7777: Support workflow task completions larger than gRPC max message size

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7777 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-05-15 12:56:56.000 UTC (7 months ago) |
| **Updated** | 2025-05-15 23:31:12.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 1 |
| **Priority Score** | 7 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 3 |

#### Description

**Describe the solution you'd like**

Today when a > 4MB gRPC request is sent, server fails. SDK side is making the failures non-retryable in https://github.com/temporalio/features/issues/624, but a solution is needed to actually make large tasks work. It is to be determined whether this is splitting a single task response into multiple calls or multiple tasks or some other approach.

#### Comments (1)

<details>
<summary><strong>alexshtin</strong> commented on 2025-05-15 23:31:11.000 UTC</summary>

ü™ìü™ìü™ìü™ì

</details>


---

### #7421: Migration to AWS SDK for Go v2 needed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7421 |
| **State** | OPEN |
| **Author** | cheinema (Christian Heinemann) |
| **Created** | 2025-03-05 07:42:56.000 UTC (10 months ago) |
| **Updated** | 2025-12-07 13:06:12.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 3 |
| **Priority Score** | 7 |
| **Labels** | archival, feature-request |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

# Background
Temporal is currently using [AWS SDK for Go](https://github.com/aws/aws-sdk-go) (v1). As [announced](https://aws.amazon.com/blogs/developer/announcing-end-of-support-for-aws-sdk-for-go-v1-on-july-31-2025/) in AWS Developer Tools Blog, this SDK entered maintenance mode on July 31, 2024 and will reach end-of-support on July 31, 2025.

The migration path is to switch to [AWS SDK for Go v2](https://github.com/aws/aws-sdk-go-v2). [Migrate to the AWS SDK for Go v2](https://docs.aws.amazon.com/sdk-for-go/v2/developer-guide/migrate-gosdk.html) includes some general guidance.

# Affected packages
* [/common/archiver/s3store](https://github.com/temporalio/temporal/tree/main/common/archiver/s3store)
* [/common/persistence/visibility/store/elasticsearch/client](https://github.com/temporalio/temporal/tree/main/common/persistence/visibility/store/elasticsearch/client)

# Previous work
* #4499 by @MichaelSnowden

#### Comments (3)

<details>
<summary><strong>bergundy</strong> commented on 2025-05-01 22:24:17.000 UTC</summary>

Thanks for bringing it up to our attention. This work is blocked on us upgrading the deprecated ES client. We are discussing prioritization internally.

</details>

<details>
<summary><strong>JacobJae</strong> commented on 2025-09-24 18:17:47.000 UTC</summary>

Hi team,
I was wondering if there‚Äôs any timeline or ETA for when this feature might become available.

</details>

<details>
<summary><strong>cheinema</strong> commented on 2025-12-07 13:06:12.000 UTC</summary>

Related activities:
- #7930 
- #7985
- #8173 

</details>


---

### #6193: Cannot load archived (s3 provider) workflow history on UI

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6193 |
| **State** | OPEN |
| **Author** | gootik (Sasan Hezarkhani) |
| **Created** | 2024-06-25 23:26:18.000 UTC (1y 6m ago) |
| **Updated** | 2025-05-28 14:30:31.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 3 |
| **Priority Score** | 7 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

From my basic research (with no real context into the code) it seems like `DescribeWorkflowExecution` in frontend workflow_handler does not take into account that a workflow can be archived. In which case it needs to do special logic. 

`GetWorkflowExecutionHistory` on the other hand, handles archived workflows perfectly.

This results in the UI not being able to show workflow run page because `describe` request fails with 404.

## Expected Behavior
Be able to open a workflow run from Archived page and get the detailed history for history archival.

## Actual Behavior
404 on openning the page.

## Steps to Reproduce the Problem

  1. Create a namespace with short retention and archival s3 enabled
  1. run a workflow and let archival happen 
  1. let  retention time  pass
  2. try to open workflow from Archived UI page

## Specifications
  - Version: 
      - Server: 1.23.1
      - UI: 2.27.0
      - Postgres: 14
      - Archival: s3
  - Platform:


#### Comments (3)

<details>
<summary><strong>gootik</strong> commented on 2024-06-28 19:40:13.000 UTC</summary>

Looks like this is a duplicate of the following:
https://github.com/temporalio/ui/issues/1174
https://github.com/temporalio/ui/issues/1702

and an attempt to fix 
https://github.com/temporalio/temporal/pull/5117

</details>

<details>
<summary><strong>jontro</strong> commented on 2024-07-22 21:32:38.000 UTC</summary>

I'll just add that I think this issue is related to the web-ui. This used to work in web ui v1. It also works from the cli

</details>

<details>
<summary><strong>kkcmadhu-IBM</strong> commented on 2025-05-28 14:30:30.000 UTC</summary>

ya. its broken in web-ui, used to work with old ui.
@mfateev  can some one prioritize this, this is long pending issue.

</details>


---

### #1119: ParentExecution not set in WorkflowExecutionInfo

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1119 |
| **State** | OPEN |
| **Author** | ryanhall07 (Ryan Hall) |
| **Created** | 2020-12-21 03:21:30.000 UTC (5 years ago) |
| **Updated** | 2023-03-03 20:22:31.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 5 |
| **Priority Score** | 7 |
| **Labels** | bug, difficulty: easy, visibility |
| **Assignees** | alexshtin |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

## Expected Behavior
WorkflowExecutionInfo.ParentExecution should be set for child workflows. 

## Actual Behavior
At least ListOpenWorkflow returns a nil value for child workflows. 

## Steps to Reproduce the Problem

  1. Execute a child workflow
  1. Query ListOpenWorkflow

## Specifications

  - Version:1.4.1 w/ mysql persistence
  - Platform: docker


#### Comments (5)

<details>
<summary><strong>ryanhall07</strong> commented on 2020-12-21 03:25:58.000 UTC</summary>

looks like it's missing when fetching from the db. not sure if it's supposed to be decorated later.

https://github.com/temporalio/temporal/blob/a48274e22dc015d07575ddc7ddd6078ffaa218ad/common/persistence/sql/sqlVisibilityStore.go#L366

</details>

<details>
<summary><strong>wxing1292</strong> commented on 2020-12-21 22:25:38.000 UTC</summary>

@ryanhall07 

Thanks for reporting this.

The parent execution info was incorrectly introduced when migrating from thrift to protobuf.
Please ignore that field for all list open / close workflow APIs.

Keep this issue open in case we decide to introduce parent execution info to all visibility APIs in the future

</details>

<details>
<summary><strong>alexshtin</strong> commented on 2021-07-08 19:10:19.000 UTC</summary>

`WorkflowExecutionInfo` proto struct is used in both `List*` APIs and `DescribeWorkflowExecution` API. There are few fields: `ParentExecution`, `AutoResetPoints`, and `ParentNamespaceId` which are assigned only for `DescribeWorkflowExecution` but not for `List*` because this fields are not stored in visibility database/Elasticsearch index. There are 3 approaches to fix this:
1. Properly document existing behaviour with zero changes on a code side.
2. Introduce another structure `WorkflowExecutionInfo` (i.e. `ListWorkflowExecutionInfo`) w/o these fields and use it for `List*` APIs. This can be done in a backward compatible way on the wire level but not on a code level (this struct is exposed via SDK).
3. Store this information in visibility database/Elasticsearch index and properly fill it in `List*` APIs.

</details>

<details>
<summary><strong>alexshtin</strong> commented on 2021-07-10 03:14:24.000 UTC</summary>

After offline discussion we decided to proceed with (3). I will add missing fields to visibility database/Elasticsearch index and fill up missing fields.

</details>

<details>
<summary><strong>tsurdilo</strong> commented on 2022-12-05 20:03:21.000 UTC</summary>

@alexshtin hey just wanted to ask if there is any updates on this. we have users that are asking for this info

</details>


---

### #785: Workflow-less activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/785 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-10-01 23:30:57.000 UTC (5y 3m ago) |
| **Updated** | 2021-07-04 07:14:12.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 1 |
| **Priority Score** | 7 |
| **Labels** | enhancement, architecture, devexp, difficulty: medium |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 3 |

#### Description

**Is your feature request related to a problem? Please describe.**

N/A

**Describe the solution you'd like**

Ability to directly invoke an activity out side of workflows

**Describe alternatives you've considered**

N/A

**Additional context**

Consider worker setup on top of specialized hardware, e.g. GPU optimized hosts.

Customer / coder may need to directly invoke the activity against a staging / prod cluster for development.
The workflow-less activity functionality will provide better interactive development experience, especially for ruby / python.

#### Comments (1)

<details>
<summary><strong>hmoreno-ec</strong> commented on 2021-05-20 00:40:13.000 UTC</summary>

Any timeline?

</details>


---

### #682: Signal to self error in Workflow code

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/682 |
| **State** | OPEN |
| **Author** | pauldemarco (Paul DeMarco) |
| **Created** | 2020-08-19 01:35:16.000 UTC (5y 4m ago) |
| **Updated** | 2024-01-17 15:36:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 7 |
| **Priority Score** | 7 |
| **Labels** | enhancement, difficulty: easy, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
A workflow should be able to signal any workflow using a workflow ID, including itself.

## Actual Behavior
Throws a UnknownExternalWorkflowExecution when attempting to signal itself.

## Steps to Reproduce the Problem

  1. Try the following workflow code:
```
func TestWorkflow(ctx workflow.Context) error {
	we := workflow.GetInfo(ctx).WorkflowExecution
	err := workflow.SignalExternalWorkflow(ctx, we.ID, we.RunID, "messages", "hello").Get(ctx, nil)
	if err != nil {
		panic(fmt.Errorf("could not signal external workflow: %v", err))
	}
	return nil
}
```
  2. Error shows in temporal web history:
![image](https://user-images.githubusercontent.com/16725750/90581990-a359b580-e19a-11ea-862a-7e73994c8a82.png)

  3. The worker logs the error:
```
2020/08/18 21:29:57 ERROR Workflow panic. Namespace default TaskQueue test WorkerID 465773@beast-ubuntu@ WorkflowID TestWorkflow RunID 254db1e2-ebcc-489a-9835-1f5e2e3fb8f8 PanicError could not signal external workflow: UnknownExternalWorkflowExecution PanicStack coroutine root [panic]:
myproject.com/domain/root.TestWorkflow(0x1003500, 0xc000498a00, 0x0, 0x0)
        /home/paul/myproject.com/domain/root/testworkflow.go:41 +0x17a
reflect.Value.call(0xd69c40, 0xefbd48, 0x13, 0xeaf2fa, 0x4, 0xc000486de0, 0x1, 0x1, 0xfd2270, 0xe8e420, ...)
        /usr/local/go/src/reflect/value.go:460 +0x8ab
reflect.Value.Call(0xd69c40, 0xefbd48, 0x13, 0xc000486de0, 0x1, 0x1, 0xfd2270, 0xe8e420, 0xc0004b4640)
        /usr/local/go/src/reflect/value.go:321 +0xb4
go.temporal.io/sdk/internal.(*workflowEnvironmentInterceptor).ExecuteWorkflow(0xc0004b4640, 0x1003500, 0xc000498a00, 0xc0000362c0, 0xc, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
        /home/paul/go-sdk/internal/workflow.go:370 +0x2b2
go.temporal.io/sdk/internal.(*workflowExecutor).Execute(0xc000498940, 0x1003500, 0xc000498a00, 0x0, 0xc0004f3738, 0xc68596, 0x0)
        /home/paul/go-sdk/internal/internal_worker.go:759 +0x334
go.temporal.io/sdk/internal.(*syncWorkflowDefinition).Execute.func1(0x10036c0, 0xc00009b0e0)
        /home/paul/go-sdk/internal/internal_workflow.go:491 +0xf3
^C2020/08/18 21:30:00 INFO  Stopped Worker Namespace default TaskQueue test WorkerID 465773@beast-ubuntu@
```

## Specifications

  - Version: 0.28.0
  - Platform: Helm-chart


#### Comments (7)

<details>
<summary><strong>alexshtin</strong> commented on 2020-08-19 17:41:07.000 UTC</summary>

This is by design. Server has special code to handle this case: https://github.com/temporalio/temporal/blob/0ae59f62c4d0282aa5b7821706540a7f049473a4/service/history/transferQueueActiveTaskExecutor.go#L456. You may also check: https://github.com/uber/cadence/pull/539 and https://github.com/uber/cadence/issues/531.

</details>

<details>
<summary><strong>pauldemarco</strong> commented on 2020-08-19 18:49:00.000 UTC</summary>

You'll have to forgive me as I'm not very familiar with how signals are handled under the hood.
IMHO, from an end-user perspective, it would be nice if we could SignalWorkflow + SignalWorkflowWithStart from within workflow code and not have to worry about making sure the target workflow is external.
Is there a reason we have a special SignalExternalWorkflow instead of just SignalWorkflow(withStart bool) for workflow context?

**Note:** I'm aware of the performance implications when sending a signal to self rather than just directly placing in some internal queue. That's a decision I'd like to make for myself as an end-user. If I'm enforcing a signal-driven architecture a top temporal, than I'd like all tasks to be driven by actual Signals that are forced through temporal. Circumventing this feels like a hidden side effect and will not show in monitoring / middleware that I might utilize in the future.

</details>

<details>
<summary><strong>samarabbas</strong> commented on 2020-08-20 06:46:25.000 UTC</summary>

Temporal server does not have support for signal command which tries to send a signal to itself.  Implementation of signal command processing is written in a way where it holds a lock on the source execution while it is writing a signal to target execution.  If source and target are the same then it results in a deadlock and transfer task processing for that signal command will never succeed.  This is the reason server has a protection in place to not allow sending signal to itself.
I agree that it is a side effect of particular implementation choice made for processing of signal command.  But I don't think this is a bug.  I will convert this into a feature request which we could potentially consider to support in the future.

I still think that a workflow sending a signal to itself does not make much sense as it could simplify send a message through a channel.  I don't see why it requires a full round trip back from server just to deliver a message on signal channel. 

</details>

<details>
<summary><strong>alexshtin</strong> commented on 2020-08-20 17:53:30.000 UTC</summary>

Probably we should improve error message there though. Report something like `WorkflowCantSignalItself` instead of `UnknownExternalWorkflowExecution` we currently have.

</details>

<details>
<summary><strong>pauldemarco</strong> commented on 2020-08-20 18:42:02.000 UTC</summary>

@samarabbas that makes sense. I think the only reason would be to have clear metrics. For instance, Signals per minute in Grafana would not be a true representation of a signal driven architecture if a significant amount of self signals are not being sensed.

I have things working for now using a local workflow channel, and piping into that from Signal channel and local self signals.

I would appreciate having this as an enhancement, but if it‚Äôs a serious anti pattern to the way things are working under the hood I definitely would not make sacrifices for it.

Thanks.


</details>

<details>
<summary><strong>samarabbas</strong> commented on 2020-08-20 22:24:14.000 UTC</summary>

Understood.  I have converted this to a future enhancement which we can re-evaluate later.  For now looks like you have a reasonable workaround which is working.

</details>

<details>
<summary><strong>mattpercy-anz</strong> commented on 2022-12-19 10:36:13.000 UTC</summary>

Hi @samarabbas, any movement on this? I've just encountered the same problem, and would love to help work towards a solution here

</details>


---

### #8490: Scheduled Actions doesn't clear ContinuedFailure on null success payloads

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8490 |
| **State** | OPEN |
| **Author** | lina-temporal (Lina Jodoin) |
| **Created** | 2025-10-15 21:10:47.000 UTC (2 months ago) |
| **Updated** | 2025-12-11 22:14:50.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 2 |
| **Priority Score** | 6 |
| **Labels** | bug, schedules |
| **Assignees** | lina-temporal |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

## Expected Behavior
- When a scheduled action succeeds, subsequent scheduled actions should not receive a `ContinuedFailure` failure payload.

## Actual Behavior
- If a scheduled action fails, setting `ContinuedFailure`'s payload, and then a subsequent scheduled action succeeds without returning a payload (a `null` payload), `ContinuedFailure` will continue to propagate to scheduled actions.




#### Comments (2)

<details>
<summary><strong>lina-temporal</strong> commented on 2025-10-15 21:12:22.000 UTC</summary>

See: 
- https://github.com/temporalio/temporal/blob/main/service/worker/scheduler/workflow.go#L858C1-L858C29
- https://github.com/temporalio/temporal/blob/main/service/worker/scheduler/activities.go#L369

Instead, check the completed workflow's execution status to determine which fields to propagate.

</details>

<details>
<summary><strong>gow</strong> commented on 2025-12-11 22:14:50.000 UTC</summary>

This was closed by mistake (due to a faulty integration). Reopening it.

</details>


---

### #7894: Cannot load Archived Workflows on UI even archived are present in S3

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7894 |
| **State** | OPEN |
| **Author** | Mahee777 |
| **Created** | 2025-06-10 12:16:22.000 UTC (6 months ago) |
| **Updated** | 2025-07-29 08:06:03.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 2 |
| **Priority Score** | 6 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

## Problem Statement
We have archived 10K plus workflows then not able see archived worflows

## Expected Behavior
When we check archived workflows on UI, it should show all workflows from S3

## Actual Behavior
No workflows are retrieved from S3 and it says "No Workflows running in this Namespace". When I inspect the call, observed below API throwing 504.

https://workflow-temporal-ui-stratus.api.itg-thor-ue1.hpip-internal.com/api/v1/namespaces/<namespace>/archived-workflows?query=

## Steps to Reproduce the Problem

  1. archived 10K plus workflows
  2. check archived workflows from UI
 

## Specifications

Temporal Server Version | 1.27.2
Temporal UI Version | 2.37.1



#### Comments (2)

<details>
<summary><strong>bergundy</strong> commented on 2025-06-26 23:03:42.000 UTC</summary>

Can you try to list workflows with temporal CLI and see if you encounter a timeout still?

```
temporal workflow list --archived
```


</details>

<details>
<summary><strong>cterence</strong> commented on 2025-07-29 08:05:00.000 UTC</summary>

Experiencing the same behavior using GCS, getting timeouts with the UI and the CLI.
The UI displays around 56k completed workflows.
The bucket has around 2.2GB of objects.

A command output:

```bash
temporal-admintools-764cc85fdc-88x9n:/etc/temporal$ temporal workflow list --archived
time=2025-07-29T08:03:27.339 level=ERROR msg="failed listing workflows: Get \"https://storage.googleapis.com/REDACTED_BUCKET_NAME/visibility%2F3b2a72fd-94b8-4649-85d2-d6d611608509%2FcloseTimeout_2025-07-16T15:50:02Z_17702649171681512189_13679678564717940994_6285912084630613167.visibility\": context deadline exceeded"

```

Temporal Server Version | 1.28.0
Temporal UI Version | 2.38.3

</details>


---

### #7600: start_workflow_execution fails with duplicate key error despite TERMINATE_EXISTING policy (possible DB inconsistency)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7600 |
| **State** | OPEN |
| **Author** | ozeranskii (Sergei Ozeranskii) |
| **Created** | 2025-04-11 18:51:11.000 UTC (8 months ago) |
| **Updated** | 2025-04-22 06:37:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

Starting a workflow with the following parameters should either succeed or terminate the existing one as specified by the conflict policy:
- id_reuse_policy: ALLOW_DUPLICATE
- execution_timeout: 12h
- id_conflict_policy: TERMINATE_EXISTING

## Actual Behavior
The gRPC call start_workflow_execution fails after multiple retries with the following error:
```
WARN temporal_client::retry: gRPC call start_workflow_execution retried 7 times error=Status { code: Unavailable, message: "createOrUpdateCurrentExecution failed. Failed to insert into current_executions table. Error: pq: duplicate key value violates unique constraint \"current_executions_pkey\"", metadata: MetadataMap { headers: {"content-type": "application/grpc"} }, source: None }
```

This suggests that a previous workflow with the same ID was not properly cleaned up, possibly due to a failed rollback transaction in Temporal, resulting in data inconsistency in the current_executions table.

Manual deletion of the row in the database resolves the issue:
```sql
DELETE FROM current_executions WHERE workflow_id = 'problem-id';
```

## Steps to Reproduce the Problem
I haven't found a way to reproduce the problem, but the process looks like this:
1. Attempt to start a workflow with the given parameters (ALLOW_DUPLICATE, TERMINATE_EXISTING, long timeout).
2. Observe retries and eventual failure with the duplicate key value violates unique constraint error.
3. Verify that the conflicting workflow ID still exists in current_executions table.
4. Manually delete the row to allow the new workflow to start successfully.

## Specifications

  - Version: temporalio (python sdk) 1.10.0
  - Platform: Temporal Version 1.27.1 (PostgreSQL).


#### Comments (6)

<details>
<summary><strong>wrbbz</strong> commented on 2025-04-11 20:27:32.000 UTC</summary>

Some additional info:
It is impossible to obtain such workflow from UI or CLI
The command like:
```
$ temporal workflow describe --workflow-id ${workflow_id}
```
results with:
```
failed describing workflow: workflow execution not found for workflow ID "${workflow_id}
```

In UI there is no result of filtering by workflow id

Reactions: üëç 1

</details>

<details>
<summary><strong>dandavison</strong> commented on 2025-04-15 11:23:55.000 UTC</summary>

Thanks very much for this write-up @ozeranskii and @wrbbz. We're looking into it.

</details>

<details>
<summary><strong>ozeranskii</strong> commented on 2025-04-15 12:28:49.000 UTC</summary>

Hi! Thanks for starting to look into it. By the way, this is not an isolated incident and it happens occasionally. So the problem is more than relevant for us. 

</details>

<details>
<summary><strong>yycptt</strong> commented on 2025-04-21 19:00:06.000 UTC</summary>

Hi @ozeranskii!

Can you help confirm if you only start to see the issue recently with v1.27.x temporal server or it happens with older versions as well?

Also want to confirm if you have any replication set up for postgresql and if there's any DB failover?

Re. the issue itself, I feel it's either due to 
- An issue in the retention timer logic and it somehow leaves the record in the `current_executions` table undeleted. This is unlikely though, because our logic always deletes from `current_executions` [first](https://github.com/temporalio/temporal/blob/main/service/history/shard/context_impl.go#L1080), before deleting the actual data from `executions`. And the error basically suggests there's no matching record in `executions` table for the runID in `current_executions`.
- A previous start workflow request failed and also failed to rollback the inserted current record.

Do you see any other failures for the workflowID you are trying to start before the `duplicate key` error? 
Is the run_id in the row you manually deleted from current_execution table a runID you recognize and does it match with a previous completed run? 

</details>

<details>
<summary><strong>ozeranskii</strong> commented on 2025-04-21 19:04:49.000 UTC</summary>

@yycptt Hi! Previously, before v1.27.x, we did not notice this issue. But our load profile has changed lately and it may occur when using WF quite intensively.
No, we don't have a replication DB.
We do not notice any other glitches when starting WF.
It seems that runID is created uniquely each time.

</details>

<details>
<summary><strong>wrbbz</strong> commented on 2025-04-22 06:37:08.000 UTC</summary>

We have a replication. We are using [Patroni](https://github.com/patroni/patroni) for Postgres replication. However, we do not relay on its failovering and connect our Temporal instance with master directly
We tried to use pgBouncer, but there were issues with transaction mode and we switched back to direct connection

</details>


---

### #6664: Request unauthorized for SQL search-attributes (claims are not passed)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6664 |
| **State** | OPEN |
| **Author** | TheGeniesis |
| **Created** | 2024-10-16 10:06:18.000 UTC (1y 2m ago) |
| **Updated** | 2025-11-16 22:30:39.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 4 |
| **Priority Score** | 6 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

## Expected Behavior

I can execute CRUD commands with admin and non admin x-search-attributes without errors.

## Actual Behavior

I have `devl:admin` and `temporal-system:admin` roles assigned. I use only sql, postgres12 driver for "default" and "visibility" dbs.

When I execute one of x-search-attributes command inside admintools pod it fails with error:
> I use `devl` namespace as an example of not working scenario

```
tctl --namespace=devl admin cluster get-search-attributes
Error: Unable to get search attributes.
Error Details: rpc error: code = PermissionDenied desc = Request unauthorized.
```

When I run this command with not existing namespace I get different error: 
```
Error Details: rpc error: code = NotFound desc = Namespace searchtest2 is not found.
```

Cluster is working:
```
tctl cluster health
temporal.api.workflowservice.v1.WorkflowService: SERVING
```

I tested other admin commands (provided auth token as an env variable) related to cluster info (`tctl --namespace=devl admin cluster describe`, `tctl --namespace=devl admin cluster list` ), namespaces (`tctl --namespace=devl namespace list`, `tctl --namespace=devl namespace describe`), etc. and they are working. Only search-attributes has this problem. Non-admin command (`tctl --namespace=devl cluster get-search-attributes`) works as expected. ~Unfortunately only admin command allows to add attributes.~ 
Update: The newest Temporal version allows to add new search attributes, admin commands are marked as deprecated.

Log from frontend pod:
```
 {"level":"error","ts":"2024-10-15T13:59:29.365Z","msg":"service failures","operation":"AdminGetSearchAttributes","wf-namespace":"devl","error":"Unable to get namespace devl info with error: Request unauthorized.","logging-call-at":"telemetry.go:411","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/telemetry.go:411\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/telemetry.go:202\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/service/frontend/redirection_interceptor.go:187\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/common/authorization.(*Interceptor).Intercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/authorization/interceptor.go:181\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/service/frontend.GrpcServerOptionsProvider.NewServerMetricsContextInjectorInterceptor.func2\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/home/runner/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.51.0/interceptor.go:315\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/namespace_logger.go:85\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/namespace_validator.go:135\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/common/utf8validator.(*Validator).Intercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/utf8validator/validate.go:182\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/service/frontend.GrpcServerOptionsProvider.NewServiceErrorInterceptor.func1\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/grpc.go:178\ngoogle.golang.org/grpc.NewServer.chainUnaryServerInterceptors.chainUnaryInterceptors.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1177\ngo.temporal.io/server/api/adminservice/v1._AdminService_GetSearchAttributes_Handler\n\t/home/runner/work/docker-builds/docker-builds/temporal/api/adminservice/v1/service_grpc.pb.go:1062\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1369\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1780\ngoogle.golang.org/grpc.(*Server).serveStreams.func2.1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1019"}
 ```

Stacktrace from command:
```
Error: Unable to get search attributes.
Error Details: rpc error: code = Unavailable desc = Unable to get namespace devl info with error: Request unauthorized.
Stack trace:
goroutine 1 [running]:
runtime/debug.Stack()
	/opt/hostedtoolcache/go/1.21.11/x64/src/runtime/debug/stack.go:24 +0x5e
runtime/debug.PrintStack()
	/opt/hostedtoolcache/go/1.21.11/x64/src/runtime/debug/stack.go:16 +0x13
github.com/temporalio/tctl/cli_curr.printError({0x1990e48, 0x20}, {0x1dd3940, 0xc00051c000})
	/home/runner/work/docker-builds/docker-builds/tctl/cli_curr/util.go:393 +0x218
github.com/temporalio/tctl/cli_curr.ErrorAndExit({0x1990e48?, 0x1dfaf50?}, {0x1dd3940?, 0xc00051c000?})
	/home/runner/work/docker-builds/docker-builds/tctl/cli_curr/util.go:404 +0x25
github.com/temporalio/tctl/cli_curr.AdminGetSearchAttributes(0xc0005371e0)
	/home/runner/work/docker-builds/docker-builds/tctl/cli_curr/admin_cluster_search_attributes_commands.go:157 +0x89
github.com/temporalio/tctl/cli_curr.newAdminClusterCommands.func3(0xc0005371e0?)
	/home/runner/work/docker-builds/docker-builds/tctl/cli_curr/admin.go:496 +0x13
github.com/urfave/cli.HandleAction({0x16a4dc0?, 0x1a3db98?}, 0x15?)
	/home/runner/go/pkg/mod/github.com/urfave/cli@v1.22.10/app.go:526 +0x75
github.com/urfave/cli.Command.Run({{0x1978c96, 0x15}, {0x0, 0x0}, {0xc0006cb080, 0x1, 0x1}, {0x198e717, 0x1f}, {0x0, ...}, ...}, ...)
	/home/runner/go/pkg/mod/github.com/urfave/cli@v1.22.10/command.go:173 +0x63e
github.com/urfave/cli.(*App).RunAsSubcommand(0xc000239880, 0xc000536f20)
	/home/runner/go/pkg/mod/github.com/urfave/cli@v1.22.10/app.go:405 +0xe07
github.com/urfave/cli.Command.startApp({{0x195d3e0, 0x7}, {0x0, 0x0}, {0xc0006cb220, 0x1, 0x1}, {0x198c21d, 0x1e}, {0x0, ...}, ...}, ...)
	/home/runner/go/pkg/mod/github.com/urfave/cli@v1.22.10/command.go:378 +0xb58
github.com/urfave/cli.Command.Run({{0x195d3e0, 0x7}, {0x0, 0x0}, {0xc0006cb220, 0x1, 0x1}, {0x198c21d, 0x1e}, {0x0, ...}, ...}, ...)
	/home/runner/go/pkg/mod/github.com/urfave/cli@v1.22.10/command.go:102 +0x7e5
github.com/urfave/cli.(*App).RunAsSubcommand(0xc0002396c0, 0xc000536dc0)
	/home/runner/go/pkg/mod/github.com/urfave/cli@v1.22.10/app.go:405 +0xe07
github.com/urfave/cli.Command.startApp({{0x19598b4, 0x5}, {0x0, 0x0}, {0xc0006cb1d0, 0x1, 0x1}, {0x1974c0a, 0x13}, {0x0, ...}, ...}, ...)
	/home/runner/go/pkg/mod/github.com/urfave/cli@v1.22.10/command.go:378 +0xb58
github.com/urfave/cli.Command.Run({{0x19598b4, 0x5}, {0x0, 0x0}, {0xc0006cb1d0, 0x1, 0x1}, {0x1974c0a, 0x13}, {0x0, ...}, ...}, ...)
	/home/runner/go/pkg/mod/github.com/urfave/cli@v1.22.10/command.go:102 +0x7e5
github.com/urfave/cli.(*App).Run(0xc000239340, {0xc00003e0a0, 0x5, 0x5})
	/home/runner/go/pkg/mod/github.com/urfave/cli@v1.22.10/app.go:277 +0xb27
main.main()
	/home/runner/work/docker-builds/docker-builds/tctl/cmd/tctl/main.go:47 +0xa5
```

## Steps to Reproduce the Problem

  1. Deploy temporal on AKS cluster using modified chart
  1. Create proper groups, users and service principals (I followed [medium article](https://phongthaicao.medium.com/temporal-authentication-and-authorization-using-azure-ad-f940646b61e0) with small tweaks, since it wasn't 100% up-to-date)
  1. Login into admintools pod
  1. Create a new namespace using UI session `temporal operator namespace create --namespace <namespace> --grpc-meta=Authorization='Bearer <token_from_ui>'` (works)
  1. Run any of admin x-search-attributes command  `tctl --namespace=devl cluster get-search-attributes --auth='Bearer <token_from_ui>'` or `temporal operator search-attribute create --name email --type Keyword --grpc-meta=Authorization='Bearer <token_from_ui>'`

## Specifications

  - Version: 1.24.2 - after upgrade to 1.25.1 problem still exists
  - Chart version: 0.44.0 ([modified](https://github.com/temporalio/helm-charts/pull/571#issuecomment-2415945554) to enable internal frontend and oAuth) - after upgrade to 0.50.0 problem still exists

## Question

~Do I miss something related to role assignment? I searched frontend, admintools and worker logs, but I couldn't find anything which might help me to debug this problem.~

Update: See my comments below. Looks like a bug. Claims are not passed for DescribeNamespace and UpdateNamespace endpoints which are executed by *SearchAttributesSQL functions. This is the source of the error.



#### Comments (4)

<details>
<summary><strong>TheGeniesis</strong> commented on 2024-10-17 11:19:19.000 UTC</summary>

I did one more test - I enabled ES (changed flag `elasticsearch.enabled: true` in a chart and the feature works.

I checked [Temporal documentation](https://docs.temporal.io/visibility#custom-search-attributes) and I shouldn't have any problems with using search attributes without ES.

> If you use Elasticsearch as your Visibility store, your custom Search Attributes apply globally and can be used across Namespaces. However, if using any of the [supported SQL databases](https://docs.temporal.io/self-hosted-guide/visibility) with Temporal Server v1.20 and later, your custom Search Attributes are associated with a specific Namespace and can be used for Workflow Executions in that Namespace.

I just wonder if this is the reason of the problem. For PSQL it should be namespace specific, but command was created for admin.

I also checked UI and `https://<host>/api/v1/namespaces/devl/search-attributes?` returns 503 in the UI when PSQL is enabled (no problem with ES).

</details>

<details>
<summary><strong>TheGeniesis</strong> commented on 2024-10-21 15:20:30.000 UTC</summary>

For 503 error I added additional logs into the code and found that claims are not send for DescribeNamespace:

Changes in default_authorizer.go
```go
func (a *defaultAuthorizer) Authorize(_ context.Context, claims *Claims, target *CallTarget) (Result, error) {
	// APIs that are essentially read-only health checks with no sensitive information are
	// always allowed
	if IsHealthCheckAPI(target.APIName) {
		return resultAllow, nil
	}
	fmt.Println("Check claims for ", target.APIName)
	dbg2(claims)
	if claims == nil {
		return resultDeny, nil
	}
	metadata := api.GetMethodMetadata(target.APIName)
	fmt.Println("API nAME", target.APIName)
```

Logs from frontend service:
```
Check claims for  /temporal.api.operatorservice.v1.OperatorService/ListSearchAttributes
{"Subject":"DmXTqIs843B3i6f-AnAcr2cakEpztzAmMrCuG-d8DXM","System":9,"Namespaces":{"devl":9,"intg":9,"uacc":9},"Extensions":null}
API nAME /temporal.api.operatorservice.v1.OperatorService/ListSearchAttributes
hasRole 9

Check claims for  /temporal.api.workflowservice.v1.WorkflowService/DescribeNamespace
null
```

In docker-builds/temporal/service/frontend/admin_handler.go I see function `getSearchAttributesSQL` which executes the `DescribeNamespace` function.

Is there any easy way to add claims to the function execution?

Edit:

To confirm I added a condition to bypass claims check for `DescribeNamespace` endpoint and UI started working.
```go
if (strings.Contains(target.APIName, "DescribeNamespace")) {
  return resultAllow, nil
}
```

Edit 2:
After upgrade to the newest version I see commands to add search-attributes on operator level. I added the same bypass for `UpdateNamespace` and executed command - it works.

</details>

<details>
<summary><strong>bergundy</strong> commented on 2024-10-31 22:31:04.000 UTC</summary>

Thanks for the report. While these admin APIs are deprecated and so is `tctl` there may still be an actual issue here and we'll look into it.

</details>

<details>
<summary><strong>underrun</strong> commented on 2025-11-16 22:30:39.000 UTC</summary>

This is still an issue - also it's got nothing to do with deprecated admin apis or tctl.

The place i noticed this is in the web ui when i hit a namespace endpoint at a path like `/namespaces/hello-world`

The behavior is that the top half of the page renders but i get nothing under search attributes and a little 503 toast pops up at the bottom - which happens because ListSearchAttributesSQL performs an additional client call to frontend itself to DescribeNamespace but this client call can only use the level of auth allowed by the frontend's own claims.

This can for sure be worked around by allowing whatever auth's frontend to frontend to be allowed to describe a namespace even if that identity hasn't been granted specific access to that namespace, but this shouldn't really be necessary.

there's an obvious amount of access that the server has to have to do stuff to whatever is running in the server, but it would still be nice to be able to only allow those actions that are necessary.

the workaround i've taken is to make namespaced api calls for identities whose claims don't have that namespace as part of the claim only be authorized if their system roles include both Admin and whatever the other role that's required (for describe it'd be Read).

but the issue with that is that it also means any users with system role of admin can do more than what I would really like for them to do. i could maybe special case things that are identified as temporal server specifically but i'd rather keep my auth based on roles rather than specific identities for reasons...

so this isn't a "big deal" but it make my authorizer logic more complex than it needs to be and makes it a little difficult to separate users who are admins and temporal server service workloads from each other.

</details>


---

### #5874: Provide extra info of the received signal for re-apply from reset, and others

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5874 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2023-06-16 17:51:27.000 UTC (2y 6m ago) |
| **Updated** | 2024-05-07 20:14:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
After reset, the signal reapply is great but sometimes I need to partially reapply to the new run. For example, some signals are from human that are important to keep. Some are not from human and it‚Äôs intended to drop so that workflow can behave differently.
This is impossible to do today because there is no way to know if the signal is before or after reset point for reapply.

Additionally, it would be nice to provide info like the time of the signal. This is recorded in history event but not exposed in SDK.

**Describe the solution you'd like**
It is possible to get this info from history‚Äî we have a reset event and then comes with a batch of history to reapply in that batch. So it would be nice if SDK can provide a flag to tell whether or not the signal is from reapply.

The new api may be something like :
```golang
type SignalInfo struct{
    FromReapply bool
    SendingTime time.Time
}

info, ok := channel.ReceiveAsyncWithInfo(&val)
```
**Describe alternatives you've considered**
Another idea is to let server provide a feature to filter some signals when reapply ‚Äî maybe based on signal names. But I feel like an SDK feature is better ‚Äî more flexible and easier to implement

**Additional context**
Slack thread with Temporal team(Maxim) https://temporalio.slack.com/archives/CTDTU3J4T/p1686762087116689?thread_ts=1686722741.716739&cid=CTDTU3J4T 


#### Comments (6)

<details>
<summary><strong>cretz</strong> commented on 2023-06-16 19:38:36.000 UTC</summary>

I understand this generically, but if you have different signals that need to behave differently (e.g. some from human and some not), might it be best to provide that info in the signal argument?

I am a bit scared of encouraging people to write the workflow to do different things based on whether it was reset. Having said that, we have the unsafe/discouraged-for-use `workflow.IsReplaying(ctx) bool`. So maybe we can have a similar `workflow.IsReplayingBeforeResetPoint(ctx) bool`.

</details>

<details>
<summary><strong>longquanzheng</strong> commented on 2023-06-16 19:49:44.000 UTC</summary>

It‚Äôs actually for the signals after the reset point which are considered as ‚Äúreapply‚Äù. The one before reset point shouldn‚Äôt be treated differently in workflow code. 

There is no way for workflow to know by argument. 

</details>

<details>
<summary><strong>cretz</strong> commented on 2023-06-16 21:57:42.000 UTC</summary>

So is `!workflow.IsReplayingBeforeResetPoint(ctx)` enough to know that?

</details>

<details>
<summary><strong>longquanzheng</strong> commented on 2023-06-16 22:03:30.000 UTC</summary>

> So is `!workflow.IsReplayingBeforeResetPoint(ctx)` enough to know that?

No , it‚Äôs different than that‚Ä¶maybe I‚Äôll make up a history as an example later. 

The signals that I need to process differently are going to be re-processed again after reset, without any replay at all. 

</details>

<details>
<summary><strong>mfateev</strong> commented on 2024-05-07 18:16:17.000 UTC</summary>

This requires a server-side change. The service must mark reapplied signals to distinguish them from the newly received signals.

</details>

<details>
<summary><strong>cretz</strong> commented on 2024-05-07 20:14:07.000 UTC</summary>

Transferring to the server repo...

</details>


---

### #5725: Add PARENT_CLOSE_POLICY_DISCONNECTED

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5725 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2024-04-14 17:38:59.000 UTC (1y 8m ago) |
| **Updated** | 2024-04-14 17:38:59.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 0 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 3 |

#### Description

**Is your feature request related to a problem? Please describe.**

It is not possible to start child workflows in a fully disconnected mode which will not report completion/failure to the parent. It causes additional performance overhead as well as UnhandledCommand without any benefit:

![Screenshot 2024-04-14 at 10 35 31‚ÄØAM](https://github.com/temporalio/temporal/assets/1463622/4d0fc0b9-1d2f-4080-abd4-981fbbb582af)

**Describe the solution you'd like**

Add DISCONNECTED to the ParentClosePolicy. When a child is created with this policy, only its start (through ChildWorkflowExecutionStartedEvent) is reported to the parent. The child's completion and failure are not reported. The parent's completion doesn't affect the child as well.



---

### #4919: Allow Internal-Frontend to Register/Update Namespace (and other methods)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4919 |
| **State** | OPEN |
| **Author** | andreclaro (Andr√©) |
| **Created** | 2023-10-02 09:48:09.000 UTC (2y 3m ago) |
| **Updated** | 2023-10-02 23:42:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
Register/Update Namespace and other methods should be allow to be performed by connecting to the internal-frontend.

## Actual Behavior
Currently several actions can only be done by the Frontend service. It we try to connect to the internal-frontend and perform those actions, such as, Register/Update Namespace, we will get the following error: `unable to find bootstrap container for the given service name`

As @MichaelSnowden mentioned: 

```
Hi, Andr√©. That‚Äôs an interesting issue. To me, it looks like this is caused by the fact that the RegisterNamespace workflow service API‚Äôs implementation is using a hard-coded service name of primitives.FrontendService instead of fetching the current service name (which may be primitives.InternalFrontendService) [here](https://github.com/temporalio/temporal/blob/f1ef4db6783f11fa29c8f6e11f307959b3b23079/service/frontend/namespace_handler.go#L852) . We register the bootstrap container dynamically based on the service name, but it looks like, when reading it, we always assume that the caller is the frontend service. As you can imagine, there‚Äôs a lot of code shared between the two services, but there are some small differences, so it‚Äôs easy for bugs like this to crop up. This is all just my deduction from reading the code, but I‚Äôm not too familiar with it, since I didn‚Äôt write it. In the meantime, might I ask why you want to use the internal frontend service?

Btw, running all services together locally, I do not see this issue.
That is likely because the frontend service will write the bootstrap container to the shared registry that the internal frontend service will erroneously look into with the frontend service‚Äôs key when you run them together. If you just run the internal frontend service on its own, the bootstrap container won‚Äôt be there because the regular frontend service didn‚Äôt write it
```


Full error logs:
```
{
    "level": "error",
    "ts": "2023-09-28T15:25:12.080Z",
    "msg": "service failures",
    "operation": "RegisterNamespace",
    "wf-namespace": "test",
    "error": "unable to find bootstrap container for the given service name",
    "logging-call-at": "telemetry.go:328",
    "stacktrace": "go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:328\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:169\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1179\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleLocalAPIInvocation\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:214\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:188\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1179\ngo.temporal.io/server/common/metrics.NewServerMetricsContextInjectorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1179\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.42.0/interceptor.go:344\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1179\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_logger.go:84\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1179\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_validator.go:111\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1179\ngo.temporal.io/server/common/rpc.ServiceErrorInterceptor\n\t/home/builder/temporal/common/rpc/grpc.go:145\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1170\ngo.temporal.io/api/workflowservice/v1._WorkflowService_RegisterNamespace_Handler\n\t/go/pkg/mod/go.temporal.io/api@v1.24.0/workflowservice/v1/service.pb.go:1537\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1360\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1737\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:982"
}
```

## Steps to Reproduce the Problem

  1. Perform RegisterNamespace or UpdateNamespace to the internal-frontend
 
## Specifications

  - Version: 1.22
  - Platform: Temporal-server 


#### Comments (6)

<details>
<summary><strong>dnr</strong> commented on 2023-10-02 14:41:24.000 UTC</summary>

1. Internal-frontend is for use by the server worker role and history/matching roles only. All calls from clients should go through the normal frontend for proper authorization. In particular, there are no internal calls to RegisterNamespace so that just should't be happening.<br><br>I see the migration workflow (running on server worker) uses UpdateNamespace, which would trigger this. Are you using the migration workflow?

2. It looks this is only a problem if archival is enabled on the namespace.



</details>

<details>
<summary><strong>MichaelSnowden</strong> commented on 2023-10-02 16:20:19.000 UTC</summary>

> All calls from clients should go through the normal frontend for proper authorization.

In this case, I think Andr√© read the patch notes [here](https://github.com/temporalio/temporal/releases/tag/v1.20.0) about the internal frontend role and thought that it meant you could use it to bypass auth for admin requests as well. However, it seems like this is something we didn't intend to support because this was originally just for simplifying intern-node auth.

I think I see two ways we can go about this:

1. We can support this use case (not sure if we want to, though, as it seems insecure). This would mean making some code changes [here](https://github.com/temporalio/temporal/blob/f1ef4db6783f11fa29c8f6e11f307959b3b23079/service/frontend/namespace_handler.go#L852).
2. We can say that we don't support this use case, and we can help you find another solution, Andr√©. 

To me, this originally looked like a bug because we were propagating the "wrong" service name around, but if that's because we have, as a policy, that no one should be calling these APIs on the internal frontend, then it's not a bug. I think we should have a clearer error message, though. Maybe we can do something like check the request metadata to see if calls to the internal-frontend are from another service, and, if not, return an error message? Not as a method of protection, because it could obviously be spoofed, but as a way to detect misuse and inform the user.

In addition, it seems like no other nodes are calling RegisterNamespace on the internal-frontend (which makes sense because this is not something I see us automating). However, if we do in the future, it might make sense to fix this service name issue now (separately from the internal-frontend misuse error message change).

So, @dnr , I think we should do this:
- Not support this use case (because it's insecure)
- Return a descriptive error message if it looks like someone is calling the internal frontend from a non-internal service (because it might clear up misconceptions earlier)
- Dynamically get the service name instead of using the hard-coded one [here](https://github.com/temporalio/temporal/blob/f1ef4db6783f11fa29c8f6e11f307959b3b23079/service/frontend/namespace_handler.go#L852) (because it would make it possible for this API to be called by other services later, if we decide to do so). Or, add it to a list of APIs which the internal-frontend does not support.

Also, @andreclaro , would you mind elaborating on your use case? Why don't you want to use the frontend service here?

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-10-02 16:45:09.000 UTC</summary>

The point about the migration workflow using UpdateNamespace means we do need to fix this.

But to be clear, external clients must not be making calls to internal-frontend, that must be prevented (with network restrictions or mTLS or both), or else there's no point to it.

</details>

<details>
<summary><strong>andreclaro</strong> commented on 2023-10-02 22:26:17.000 UTC</summary>

First of all, thank you for the quick response!

We are using TLS (mTLS to be enabled later), network policies and we are now enabling authorization. We also have archival enabled for both history and visibility.

I totally understand that we shouldn't allow external service to access the internal-frontend.

My initial idea was to use the internal-frontend for administration tasks whenever required (only accessible by administrators with access to exec to the internal-frontend service), however we are planning to build a service to perform that by getting a JTW token from our authorization service. 

The other new use case is to use the [temporal-operator](https://github.com/alexandrevilain/temporal-operator) to manage namespaces but it seems that currently this operator does not support JTW token. 

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-10-02 22:51:22.000 UTC</summary>

I see. I don't think we'd go out of our way to break calling internal-frontend with tctl/temporal cli, but it's not what it's intended for, in the same way that you generally wouldn't do RPC calls directly to history or matching services. I'd recommend setting up proper authorization and doing administrative tasks through the regular frontend.

</details>

<details>
<summary><strong>andreclaro</strong> commented on 2023-10-02 23:42:45.000 UTC</summary>

Yes, that makes sense. What about point 2 (`It looks this is only a problem if archival is enabled on the namespace.`)? Are you going to fix this? thanks!

</details>


---

### #3503: Dynamic Task Queue rate limiting

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3503 |
| **State** | OPEN |
| **Author** | jdupl123 |
| **Created** | 2022-10-18 06:30:52.000 UTC (3y 2m ago) |
| **Updated** | 2024-09-26 23:35:38.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 2 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | dnr |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

We would like to dynamically rate limit a task queue in order to act as a circuit breaker. The task queue would be scaled back when a downstream service is strugling.

They aim would be Ability to be able to change the rate limit with changing having to restart the worker similar to https://github.com/temporalio/temporal/issues/3288

Further discussion at https://temporalio.slack.com/archives/CTRCR8RBP/p1665723548780049?thread_ts=1665722496.266549&cid=CTRCR8RBP


#### Comments (2)

<details>
<summary><strong>jlegrone</strong> commented on 2022-10-20 04:46:55.000 UTC</summary>

Not sure if there is already an issue for this, but an alternative/adjacent idea would be to support dynamically adjusting the backoff for individual activity retries via custom error metadata -- eg. in response to the [HTTP 429](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429) status code or grpc's [RESOURCE_EXHAUSTED](https://grpc.github.io/grpc/core/md_doc_statuscodes.html).

</details>

<details>
<summary><strong>jdupl123</strong> commented on 2024-09-26 23:35:38.000 UTC</summary>

@jlegrone were dynamic back off ever implemented?

</details>


---

### #2729: Temporal Is not able to connect to Cassandra even when one node is down in a cluster

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2729 |
| **State** | OPEN |
| **Author** | raghudeshu |
| **Created** | 2022-04-15 14:02:48.000 UTC (3y 8m ago) |
| **Updated** | 2023-11-14 02:12:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

We have a 3 node cluster Even when one node is down we are expecting the temporal to work by connecting to other two nodes.

## Actual Behavior:
We are doing the load test and we observed Temporal is not able to connect to Cassandra even when ONE node is down


## Steps to Reproduce the Problem

  1. In a cluster make sure one node in Cassandra is down. The temporal pods are not able to connect to remaining other two nodes.

Error:
2022/04/14 23:41:44 error: failed to connect to XX.XXX.XX.7:9042 due to error: write tcp XX.XXX.XX.79:44342->XX.XXX.XX.7:9042: write: connection reset by peer
unable to dial control conn XX.XXX.XX.7:9042 gocql: no response received from cassandra within timeout period

Below is my Configuration:

cassandra:
hosts: [‚ÄúXX.XXX.XX.7,XX.XXX.XX.9,XX.XXX.XX.10‚Äù]
port: 9042
keyspace: temporal
user: ‚Äútemporal‚Äù
password: ‚ÄúXXXXXXX‚Äù
existingSecret: ‚Äú‚Äù
replicationFactor: 3 (Tried both 1 and 3)
consistency:
default:
consistency: ‚Äúlocal_quorum‚Äù
serialConsistency: ‚Äúlocal_serial‚Äù
tls:
enabled: true
enableHostVerification: false

Note: We are mentioning the cluster info with comma separated ip‚Äôs. We did updated Replication factor with 1 ,3 both did not worked.

## Specifications

  - Version: 1.13
  - Platform: Temporal is deployed in Azure . Cassandra is managed by [azure]

Cassandra Configuration:


![MicrosoftTeams-image (1)](https://user-images.githubusercontent.com/35110571/163580027-8f383bbf-456b-41f1-8874-6739091d4afe.png)



#### Comments (6)

<details>
<summary><strong>yiminc</strong> commented on 2022-04-16 00:19:34.000 UTC</summary>

@raghudeshu , when Cassandra is overloaded, sometime you might encounter that problem. If you lower your load, temporal server should be able to function even if Cassandra lost one node.
Also, please upgrade to latest version for your load test, there have been many improvement/bugfixes since 1.13.


</details>

<details>
<summary><strong>johanforssell</strong> commented on 2023-05-02 10:35:46.000 UTC</summary>

I've had the exact same problem. Temporal **1.18.5**.

Also, when all three nodes came up again, we have the following errors in the **Frontend** servers. We've had these errors for days.
```
2023/05/02 10:29:51 gocql: unable to dial control conn x.x.x.133:9042: dial tcp x.x.x.165:9042: i/o timeout
2023/05/02 10:29:51 gocql: unable to dial control conn x.x.x.133:9042: dial tcp x.x.x.133:9042: i/o timeout
2023/05/02 10:29:51 gocql: unable to dial control conn x.x.x.133:9042: dial tcp x.x.x.197:9042: i/o timeout
```

More specifically:

`gocql: unable to create session: unable to connect to initial hosts: dial tcp x.x.x.165:9042: i/o timeout`
```
{"level":"error","ts":"2023-05-02T10:32:17.328Z","msg":"gocql wrapper: unable to refresh gocql session","error":"gocql: unable to create session: unable to connect to initial hosts: dial tcp x.x.x.165:9042: i/o timeout","logging-call-at":"session.go:99","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/persistence/nosql/nosqlplugin/cassandra/gocql.(*session).refresh\n\t/home/builder/temporal/common/persistence/nosql/nosqlplugin/cassandra/gocql/session.go:99\ngo.temporal.io/server/common/persistence/nosql/nosqlplugin/cassandra/gocql.(*session).handleError\n\t/home/builder/temporal/common/persistence/nosql/nosqlplugin/cassandra/gocql/session.go:191\ngo.temporal.io/server/common/persistence/nosql/nosqlplugin/cassandra/gocql.(*iter).Close.func1\n\t/home/builder/temporal/common/persistence/nosql/nosqlplugin/cassandra/gocql/iter.go:56\ngo.temporal.io/server/common/persistence/nosql/nosqlplugin/cassandra/gocql.(*iter).Close\n\t/home/builder/temporal/common/persistence/nosql/nosqlplugin/cassandra/gocql/iter.go:58\ngo.temporal.io/server/common/persistence/cassandra.(*ClusterMetadataStore).ListClusterMetadata\n\t/home/builder/temporal/common/persistence/cassandra/cluster_metadata_store.go:120\ngo.temporal.io/server/common/persistence.(*clusterMetadataManagerImpl).ListClusterMetadata\n\t/home/builder/temporal/common/persistence/clusterMetadataStore.go:127\ngo.temporal.io/server/common/persistence.(*clusterMetadataRateLimitedPersistenceClient).ListClusterMetadata\n\t/home/builder/temporal/common/persistence/persistenceRateLimitedClients.go:993\ngo.temporal.io/server/common/persistence.(*clusterMetadataPersistenceClient).ListClusterMetadata\n\t/home/builder/temporal/common/persistence/persistenceMetricClients.go:1393\ngo.temporal.io/server/common/persistence.(*clusterMetadataRetryablePersistenceClient).ListClusterMetadata.func1\n\t/home/builder/temporal/common/persistence/persistenceRetryableClients.go:972\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\n\t/home/builder/temporal/common/backoff/retry.go:194\ngo.temporal.io/server/common/persistence.(*clusterMetadataRetryablePersistenceClient).ListClusterMetadata\n\t/home/builder/temporal/common/persistence/persistenceRetryableClients.go:976\ngo.temporal.io/server/common/cluster.(*metadataImpl).listAllClusterMetadataFromDB.func1\n\t/home/builder/temporal/common/cluster/metadata.go:517\ngo.temporal.io/server/common/collection.(*PagingIteratorImpl[...]).getNextPage\n\t/home/builder/temporal/common/collection/pagingIterator.go:116\ngo.temporal.io/server/common/collection.NewPagingIterator[...]\n\t/home/builder/temporal/common/collection/pagingIterator.go:52\ngo.temporal.io/server/common/cluster.(*metadataImpl).listAllClusterMetadataFromDB\n\t/home/builder/temporal/common/cluster/metadata.go:534\ngo.temporal.io/server/common/cluster.(*metadataImpl).refreshClusterMetadata\n\t/home/builder/temporal/common/cluster/metadata.go:404\ngo.temporal.io/server/common/cluster.(*metadataImpl).refreshLoop\n\t/home/builder/temporal/common/cluster/metadata.go:391\ngo.temporal.io/server/internal/goro.(*Handle).Go.func1\n\t/home/builder/temporal/internal/goro/goro.go:64"}
```

And `operation ListClusterMetadata encountered gocql: no hosts available in the pool`
```
{"level":"error","ts":"2023-05-02T10:32:17.329Z","msg":"Operation failed with internal error.","error":"operation ListClusterMetadata encountered gocql: no hosts available in the pool","metric-scope":81,"logging-call-at":"persistenceMetricClients.go:1579","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/persistence.(*metricEmitter).updateErrorMetric\n\t/home/builder/temporal/common/persistence/persistenceMetricClients.go:1579\ngo.temporal.io/server/common/persistence.(*clusterMetadataPersistenceClient).ListClusterMetadata\n\t/home/builder/temporal/common/persistence/persistenceMetricClients.go:1397\ngo.temporal.io/server/common/persistence.(*clusterMetadataRetryablePersistenceClient).ListClusterMetadata.func1\n\t/home/builder/temporal/common/persistence/persistenceRetryableClients.go:972\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\n\t/home/builder/temporal/common/backoff/retry.go:194\ngo.temporal.io/server/common/persistence.(*clusterMetadataRetryablePersistenceClient).ListClusterMetadata\n\t/home/builder/temporal/common/persistence/persistenceRetryableClients.go:976\ngo.temporal.io/server/common/cluster.(*metadataImpl).listAllClusterMetadataFromDB.func1\n\t/home/builder/temporal/common/cluster/metadata.go:517\ngo.temporal.io/server/common/collection.(*PagingIteratorImpl[...]).getNextPage\n\t/home/builder/temporal/common/collection/pagingIterator.go:116\ngo.temporal.io/server/common/collection.NewPagingIterator[...]\n\t/home/builder/temporal/common/collection/pagingIterator.go:52\ngo.temporal.io/server/common/cluster.(*metadataImpl).listAllClusterMetadataFromDB\n\t/home/builder/temporal/common/cluster/metadata.go:534\ngo.temporal.io/server/common/cluster.(*metadataImpl).refreshClusterMetadata\n\t/home/builder/temporal/common/cluster/metadata.go:404\ngo.temporal.io/server/common/cluster.(*metadataImpl).refreshLoop\n\t/home/builder/temporal/common/cluster/metadata.go:391\ngo.temporal.io/server/internal/goro.(*Handle).Go.func1\n\t/home/builder/temporal/internal/goro/goro.go:64"}
```

</details>

<details>
<summary><strong>hema-kishore-gunda</strong> commented on 2023-05-04 22:18:10.000 UTC</summary>

We have hit the exact same problem. We have cassandra 3 node with pod disruption allowed as 1. We had a cluster upgrade where in we had 2 out of 3 cassandra nodes were available and the temporal server pods were restarted as part of upgrade. Old pods have been terminated and the new pods were failing to load with out any error message on the logs. Server pods have started working once all the cassandra nodes have been completely restarted. 

This issue exists on the latest version(1.20.2) of the temporal server as well. 

<img width="1009" alt="image" src="https://user-images.githubusercontent.com/70211877/236341249-b32e1599-0df9-449c-8723-6fd7a95a7fbe.png">


</details>

<details>
<summary><strong>yiminc</strong> commented on 2023-08-27 20:08:03.000 UTC</summary>

It maybe fixed by gocql 1.4.0 https://github.com/gocql/gocql/releases/tag/v1.4.0 which is included in temporal server v1.21.5

</details>

<details>
<summary><strong>mustaFAB53</strong> commented on 2023-10-10 10:35:35.000 UTC</summary>

We are connecting to an existing cassandra setup and seeing this issue even for latest temporal image with tag v1.21.5

</details>

<details>
<summary><strong>yiminc</strong> commented on 2023-11-14 02:12:59.000 UTC</summary>

From the error message, it seems even if the config provided multiple IP addresses as hosts, the gocql still fail to connect even if a single host was down. We log the error when try to [reconnect](https://github.com/temporalio/temporal/blob/1f497134f668aecad766a263895bd1f3f4d40bbc/common/persistence/nosql/nosqlplugin/cassandra/gocql/session.go#L107). From the above message, `"gocql wrapper: unable to refresh gocql session","error":"gocql: unable to create session: unable to connect to initial hosts: dial tcp x.x.x.165:9042: i/o timeout"`, it seems either there is only [one host configured](https://github.com/temporalio/temporal/blob/1f497134f668aecad766a263895bd1f3f4d40bbc/common/persistence/nosql/nosqlplugin/cassandra/gocql/client.go#L51-L53) or the gocql driver is not working as expected. 



</details>


---

### #2341: config: strict mode for configuration parsing

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2341 |
| **State** | OPEN |
| **Author** | danielhochman (Daniel Hochman) |
| **Created** | 2022-01-04 20:12:59.000 UTC (3y 12m ago) |
| **Updated** | 2023-03-03 20:21:13.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 4 |
| **Priority Score** | 6 |
| **Labels** | enhancement, up-for-grabs |
| **Assignees** | yiminc |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
My team is working on a new deployment of Temporal and we recently spent an entire day debugging an issue related to a missing key in configuration due to lack of error messaging and validation by the bootstrap code.

Example:
```
metrics:
  hostPort: "127.0.0.1:8125"
  prefix: "temporal"
```
needed to be 
```
metrics:
  statsd:
    hostPort: "127.0.0.1:8125"
    prefix: "temporal"
```

The server started up fine, the logic to configure the [bootstrap code looked for any of the custom configured reporters](https://github.com/temporalio/temporal/blob/e153684480968b9b271574b589abaedd5525a255/common/metrics/config.go#L260-L271), didn't find any, and everything booted as normal, only we were not getting any stats.

**Describe the solution you'd like**
I would like the option for strict config validation. If a key is unrecognized, the server should fail to start so it can be corrected.

I'm happy to put together a PR for option 1 below if someone can give their thoughts and approval on the approach.

**Describe alternatives you've considered**

- **Option 1**
Change YAML unmarshaling to reject unknown fields (see [yaml.Decoder.KnownFields](https://pkg.go.dev/gopkg.in/yaml.v3#Decoder.KnownFields)), and ideally expand the use of the `gopkg.in/validator.v2` throughout the config structs.
Could we move to this as a default? If not add a flag which the server supports to unmarshal the config in strict mode. 

- **Option 2**
Move to protobuf for configuration specifications. This would have the benefit of centralizing config specifications in a single place and definition language rather than scattering structs throughout the code base.
I previously made this suggestion in a [`#development Slack thread](https://temporalio.slack.com/archives/CTQU95E84/p1639591413096200):
> Has the team considered moving to protobuf for the config schema? discovery and validation are subpar with how it's all done with structs currently. Example from our project [lyft/clutch](github.com/lyft/clutch) which uses proto for defining config and uses [envoyproxy/protoc-gen-validate](https://github.com/envoyproxy/protoc-gen-validate) for validation:
> - Config definition: [api/config/gateway/v1/gateway.proto](https://github.com/lyft/clutch/blob/43aa67e73d776f3d232ea5d1188f01e194c50b63/api/config/gateway/v1/gateway.proto).
> - Example YAML conforming to definition: [clutch-config.yaml](https://github.com/lyft/clutch/blob/43aa67e73d776f3d232ea5d1188f01e194c50b63/backend/clutch-config.yaml)
> - Example output from a bad config: `{"level":"fatal","ts":1641326218.3299987,"caller":"gateway/config.go:71","msg":"configuration proto validation failed","file":"clutch-config.yaml","error":"invalid Config.Gateway: embedded message failed validation | caused by: invalid GatewayOptions.Listener: value is required"}`

#### Comments (4)

<details>
<summary><strong>yiminc</strong> commented on 2022-01-07 07:02:20.000 UTC</summary>

@danielhochman , option 1 looks good to me. I would like the behavior to enforce strict mode only for known configs. So, below example will fail the validation because hostPort and prefix is unknown field to metrics:
```
metrics:
  hostPort: "127.0.0.1:8125"
  prefix: "temporal"
```
And ideally, this should be enforced when opt-in so we don't break any production cluster when user upgrade to newer version. 

~~However, I don't want this restriction to apply to other top level config that is unknown to temporal. For example, there might be a zookeeper config in the file as top level config that temporal does not know about but is used by some other part of the process. So below config should not fail validation:~~
```
metrics:
  statsd:
    hostPort: "127.0.0.1:8125"
    prefix: "temporal"
myLeaderElection:
  someConfigKey: someValue
```
~~The `myLeaderElection` is unknown, but we should ignore it and any field underneath it.~~

~~Does that make sense?~~


</details>

<details>
<summary><strong>dnr</strong> commented on 2022-01-10 17:38:03.000 UTC</summary>

I don't really like the idea of ignoring unknown top-level fields. If it's going to be strict, it should all be strict.

Mixing config for multiple applications in the same file seems a little weird to me. What if the apps both specify some of the same top-level keys? If you want to do it, how about this instead: There can be an optional flag that takes a key name. If specified, use the value of that key in the yaml file as the temporal config, instead of the top level. That is, if you want to mix, it could look like:

```
temporal:
  metrics:
    statsd:
      ...
otherApp:
  myLeaderElection:
    ...
```

And then run `temporal-server --env=myenv --configsubkey=temporal`. Any other apps would also need to accept a similar flag.

</details>

<details>
<summary><strong>yiminc</strong> commented on 2022-01-10 23:46:55.000 UTC</summary>

Another approach in handling the config for other parts of the app is to require them to be organized in different config files. I think that might be cleaner. So I agree with @dnr to either enable or not enable for strict mode for the whole config. @danielhochman I would be welcome your proposed option 1 solution, it would be great if you could contribute to this work. :) 

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-01-11 16:02:37.000 UTC</summary>

I agree with @dnr - I don't like ignoring unknown top-level fields. If you want to reuse the config struct yourself and add other top-level fields, you can embed it and add the `inline` field tag or something.

</details>


---

### #2230: Add a feature flag to enable the self-signed certificates for the Elasticsearch transport

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2230 |
| **State** | OPEN |
| **Author** | andrey-dubnik (cloudwiz) |
| **Created** | 2021-11-26 09:01:10.000 UTC (4y 1m ago) |
| **Updated** | 2023-03-03 20:21:09.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 0 |
| **Priority Score** | 6 |
| **Labels** | enhancement |
| **Assignees** | meiliang86 |
| **Milestone** | None |
| **Reactions** | üëç 3 |

#### Description

**Is your feature request related to a problem? Please describe.**

At the moment Temporal server connection with Elasticsearch does not work if Elastic uses self-signed certificate for the TLS

**Describe the solution you'd like**

Enhance config options with a flag which enables self-signed certificates, e.g. 
https://github.com/olivere/elastic/issues/636#issuecomment-344516043

**Describe alternatives you've considered**

Bundling the CA cert with the image
Updating the Temporal startup script to import CAs from the mounted folder automatically





---

### #1994: Cron tailgate-like workflow triggering (start a workflow in some time after it completes and not according to cron schedule) [Java]

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1994 |
| **State** | OPEN |
| **Author** | pvolchak (pvolchakpax8) |
| **Created** | 2021-09-29 21:46:42.000 UTC (4y 3m ago) |
| **Updated** | 2023-03-03 20:21:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | enhancement, CRON |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently there is no way to trigger a workflow in some time. There is support for cron workflows but if the workflow is taking up the whole time bracket then the workflow runs might be immediate.

**Describe the solution you'd like**
Let's consider a scenario of polling a lot of data from multiple data sources. I start a cron workflow that is supposed to run with a 10 minute interval. Let's say the workflow started it's execution. It completes pulling all the data in 9 minutes. Then cron triggers it again in 1 minute instead of in 10 minutes, which is correct, but not a desired behavior.

**Describe alternatives you've considered**
Using some other cron or job mechanisms to identify when to start a non cron workflow


#### Comments (6)

<details>
<summary><strong>dnr</strong> commented on 2021-09-29 22:43:42.000 UTC</summary>

You're right that the behavior you want isn't possible with the current cron feature. We're working on redesigning that feature and will definitely take this use case into account.

In the meantime, there are a few ways to achieve that functionality with the current features:

You can use a simple loop: wrap the repeated logic followed by a sleep in a for loop that runs 100 times. After 100 or 1000 times, do continue-as-new to restart the workflow. The repeated logic could be included in that workflow itself, or as a child workflow that the parent starts and waits on.

Alternatively, put a sleep at the beginning of the workflow and use continue-as-new each time.

Make sure the workflow has an infinite execution and run timeout (the default). You could also add a workflow retry policy to ensure that it keeps running if the workflow somehow fails.

</details>

<details>
<summary><strong>pvolchak</strong> commented on 2021-09-30 15:13:02.000 UTC</summary>

the problem that i am facing with continue-as-new is its status. maybe its already in the working, but any time i see the status continue-as-new i have no idea if it gets to completed/error state, at least in the cron flow of things. Do you know of a way to fix/correct/modify that?

</details>

<details>
<summary><strong>dnr</strong> commented on 2021-09-30 15:32:23.000 UTC</summary>

If you define an outer workflow that just does a loop around a child workflow plus sleep, and put your logic in the child workflow, you'll be able to see the status of each run. (The outer workflow will always be "running" or "continued as new".) You could even adjust the sleep time based on the result of the child if you want.

</details>

<details>
<summary><strong>pvolchak</strong> commented on 2021-09-30 16:46:49.000 UTC</summary>

that is fair. i will play with that. thanks a lot.

</details>

<details>
<summary><strong>yiminc</strong> commented on 2022-02-12 05:09:25.000 UTC</summary>

Cron has been changed since 1.14 and the workflow is now complete with it actual state and not as continue-as-new.

</details>

<details>
<summary><strong>dnr</strong> commented on 2022-02-14 19:56:01.000 UTC</summary>

The original request is a valid use case that isn't addressed by the current cron feature: run a workflow repeatedly, sleeping for a fixed period in between runs.

I think we can address this in the new experience, so I'll keep it open until then (or we decide that a loop + child workflow is the best solution).

</details>


---

### #1485: Provide default mapping for authorizer roles to API names

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1485 |
| **State** | OPEN |
| **Author** | jmcnevin (Jeremy McNevin) |
| **Created** | 2021-04-23 00:34:40.000 UTC (4y 8m ago) |
| **Updated** | 2023-03-03 20:22:03.000 UTC |
| **Upvotes** | 3 |
| **Comments** | 0 |
| **Priority Score** | 6 |
| **Labels** | enhancement, API |
| **Assignees** | sergeybykov |
| **Milestone** | None |
| **Reactions** | üëç 3 |

#### Description

In defining a custom authorizer implementation, it would be nice if there was a default mapping available of the roles found in the authorization package to API names that may be passed down through CallTarget, while allowing users to remap these roles if they wish.


---

### #245: Cassandra timestamp type

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/245 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-03-23 07:35:49.000 UTC (5y 9m ago) |
| **Updated** | 2023-03-03 20:23:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 6 |
| **Priority Score** | 6 |
| **Labels** | potential-bug, needs-investigation |
| **Assignees** | wxing1292 |
| **Milestone** | Initial Temporal Release |

#### Description

Try not to use cassandra timestamp, due to loss of precision.

FYI:
https://github.com/temporalio/temporal/blob/master/schema/cassandra/temporal/schema.cql#L71

#### Comments (6)

<details>
<summary><strong>alexshtin</strong> commented on 2020-07-14 18:24:37.000 UTC</summary>

Most of them are gone besides these 3:
```
CREATE TABLE executions (
  visibility_ts                  timestamp, -- unique identifier for timer tasks for an execution

CREATE TABLE cluster_membership
(
    session_start        timestamp,
    last_heartbeat       timestamp,


</details>

<details>
<summary><strong>wxing1292</strong> commented on 2020-07-14 19:03:31.000 UTC</summary>

not sure about the cluster membership, maybe ok?

for execution table, plz change the `timestamp` to bigint or any data type which will not lose precision.


</details>

<details>
<summary><strong>samarabbas</strong> commented on 2020-07-31 18:18:42.000 UTC</summary>

We may not do this for Temporal.  Millisecond precision should be ok.

</details>

<details>
<summary><strong>wxing1292</strong> commented on 2020-08-06 07:30:39.000 UTC</summary>

this is not about ok or not ok
this is about persist a record into db, and db modifies it without consent.

consider in the future, using mutable state to calculate the timer to be deleted, say once a workflow is finished, delete all user timer in the future which have not fired yet.

</details>

<details>
<summary><strong>samarabbas</strong> commented on 2020-08-14 06:14:20.000 UTC</summary>

I'm not sure I fully understand the comment.  Originally the issue was filed to not use Cassandra timestamp due to precision issue.  We decided to not do this for Temporal as we feel timer granularity beyond millisecond is probably not needed at the moment.  I think we have a rounding issue where a timer could be fired earlier than expected due to rounding error.
As for your other comment we have filed an issue to delete unnecessary timers through a DB scan.  #574  

</details>

<details>
<summary><strong>wxing1292</strong> commented on 2020-08-14 16:14:28.000 UTC</summary>

> I'm not sure I fully understand the comment. Originally the issue was filed to not use Cassandra timestamp due to precision issue. We decided to not do this for Temporal as we feel timer granularity beyond millisecond is probably not needed at the moment. I think we have a rounding issue where a timer could be fired earlier than expected due to rounding error.
> As for your other comment we have filed an issue to delete unnecessary timers through a DB scan. #574

short answer: the precision issue will open a can of worms
the main idea here is to have a contract with DB layer: I get the same data that I stored.

If this issue is not addressed, later (now?) the business logic will do all kinds of hacks trying to compensate the missing precision.

</details>


---

### #8636: Auth plugin design

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8636 |
| **State** | OPEN |
| **Author** | cwjenkins (Colton Jenkins) |
| **Created** | 2025-11-13 22:39:08.000 UTC (1 months ago) |
| **Updated** | 2025-12-11 22:30:02.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 3 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
Compiling external services, temporal server, for custom authentication is inconvenient.

**Describe the solution you'd like**
Authentication provides a plugin architecture where one doesn't need to compile temporal-server.

**Describe alternatives you've considered**
1. Golang's plugin package - https://github.com/cwjenkins/temporal/pull/1/files (quick draft that works)
2. Hashicorp plugin package - https://github.com/hashicorp/go-plugin
3. wazero/wasm as a plugin - https://github.com/knqyf263/go-plugin (this is a wrapper for reference)

**Additional context**
Each has their drawbacks...
1. Go's plugin package isn't very friendly which they are very open about on the packages page
2. hashi's is over RPC so its a bit of a performance hit
3. wasm is more work than desirable to get extensions going.

Curious to converse given we like temporal thus far and we are going the compile w/ custom auth route, but looking to discuss what other options may be avail.


#### Comments (3)

<details>
<summary><strong>gow</strong> commented on 2025-11-14 02:41:24.000 UTC</summary>

Hello @cwjenkins, I'm assuming you have explored the [auth plugin interface](https://docs.temporal.io/temporal-service/configuration#authentication-and-authorization) and referring to that when you say "going the compile w/ custom auth route". We strongly encourage using this method to customize authentication and authorization in your temporal server. Some of the benefits of this method are:
1. Compile time checks (minimizes runtime errors)
2. Efficient (ex: no rpc calls)
3. Simplified deployment (since everything is in the same process)

The drawback is that it needs recompilation as you noted.

Regarding your request about plugin architecture without recompilation - If implemented, it would be in one of the forms you listed in the alternatives. And as you noted either they increase complexity or introduce performance hits. We have mainly stayed away from them for the same reasons. The server code is already quite complicated and we are careful about introducing additional complexity in form of new frameworks.

Doing a quick search of past issues, I didn't find any public conversations on this topic. Curious to hear what the community thinks.

</details>

<details>
<summary><strong>gow</strong> commented on 2025-12-11 22:17:05.000 UTC</summary>

This was closed by mistake (due to a faulty integration). Reopening it.

</details>

<details>
<summary><strong>cwjenkins</strong> commented on 2025-12-11 22:30:02.000 UTC</summary>

Thanks gow.
If there is any interest in the third option I'd be happy to assist.
Although I did just read today there are substantial performance improvements in CGO in the upcoming 1.26 release. 
Either way, would like to get away from compiling temporal and simply pull an image from the registry.

</details>


---

### #8284: Carry over signal to retry execution if original was created via SignalWithStart

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8284 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2025-09-04 18:25:12.000 UTC (3 months ago) |
| **Updated** | 2025-11-07 18:44:11.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 1 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

If client creates execution using SignalWithStarts and defines workflow retry policy.
If original execution created fails, the retry does not include the signal thats also sent via SignalWithStart exec creation. It currently only includes the workflow inputs of the initially created exec. 

#### Comments (1)

<details>
<summary><strong>mfateev</strong> commented on 2025-11-07 18:44:11.000 UTC</summary>

It also doesn't include any other signals received during previous attempt of a workflow. I think workflows that need to preserve signals received during their execution shouldn't use retries.

We could add an option to RetryOptions of a workflow to copy all signals from the previous attempts. But I'm not convinced that this is a good pattern.

</details>


---

### #6805: Scaling Temporal Workers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6805 |
| **State** | OPEN |
| **Author** | jahnavisana2812 (Jahnavi Sana) |
| **Created** | 2024-11-13 11:32:38.000 UTC (1y 1m ago) |
| **Updated** | 2025-01-18 14:32:35.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 1 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

I‚Äôd like to monitor Temporal metrics and automatically scale my Temporal workers based on the number of workflows in the task queue. My goal is to increase the number of workers if the queue has a high number of workflows waiting to be processed and to scale down if there are fewer workflows in the queue. However, I haven‚Äôt found a direct metric that specifically tracks the number of pending tasks in the queue for workers to pick up. Given this, what alternative methods are suggested to scale workers up and down based on task queue load?


#### Comments (1)

<details>
<summary><strong>seandavi</strong> commented on 2025-01-18 14:32:34.000 UTC</summary>

I believe the idea is to monitor the schedule-to-start latency. See, for example: https://dev.to/temporalio/scaling-temporal-the-basics-31l5

I'm not associated with temporal, just a user....

</details>


---

### #6307: Panic When Upgrading From 1.20.4 To 1.21.6

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6307 |
| **State** | OPEN |
| **Author** | ytaben (Yaroslav Taben) |
| **Created** | 2024-07-18 15:20:26.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-26 16:33:13.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 3 |
| **Priority Score** | 5 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

## Expected Behavior
History pods operate normally when redeployed from 1.20.4 to 1.21.6

## Actual Behavior
History pods panic with the following trace:
```
main.PanicErrorWrapper
panic: unable to find queue category by queye category ID: 5

goroutine 3960 [running]:
go.temporal.io/server/service/history/shard.loadShardInfoCompatibilityCheckWithoutReplication(0xc000f40500)
	/go/pkg/mod/go.temporal.io/server@v1.21.6/service/history/shard/compatibility.go:65 +0x334
go.temporal.io/server/service/history/shard.loadShardInfoCompatibilityCheck({0x304b310, 0xc00098a210}, 0xc001f6a810?)
	/go/pkg/mod/go.temporal.io/server@v1.21.6/service/history/shard/compatibility.go:47 +0x3f
go.temporal.io/server/service/history/shard.(*ContextImpl).loadShardMetadata(0xc000423680, 0xc001f7be7f)
	/go/pkg/mod/go.temporal.io/server@v1.21.6/service/history/shard/context_impl.go:1670 +0x1e9
go.temporal.io/server/service/history/shard.(*ContextImpl).acquireShard.func1()
	/go/pkg/mod/go.temporal.io/server@v1.21.6/service/history/shard/context_impl.go:1821 +0x46
go.temporal.io/server/common/backoff.ThrottleRetry.func1({0xc001c9bcf8, 0x40594a})
	/go/pkg/mod/go.temporal.io/server@v1.21.6/common/backoff/retry.go:119 +0x1b
go.temporal.io/server/common/backoff.ThrottleRetryContext({0x3035c20, 0xc00013c000}, 0xc001f7be28, {0x3015760, 0xc00052e630}, 0x28?)
	/go/pkg/mod/go.temporal.io/server@v1.21.6/common/backoff/retry.go:145 +0x20a
go.temporal.io/server/common/backoff.ThrottleRetry(0x0?, {0x3015760?, 0xc00052e630?}, 0x3?)
	/go/pkg/mod/go.temporal.io/server@v1.21.6/common/backoff/retry.go:120 +0x59
go.temporal.io/server/service/history/shard.(*ContextImpl).acquireShard(0xc000423680)
	/go/pkg/mod/go.temporal.io/server@v1.21.6/service/history/shard/context_impl.go:1889 +0x106
created by go.temporal.io/server/service/history/shard.(*ContextImpl).transition.func1
	/go/pkg/mod/go.temporal.io/server@v1.21.6/service/history/shard/context_impl.go:1483 +0x11a
panic: unable to find queue category by queye category ID: 5
```

## Steps to Reproduce the Problem

  1. Run Temporal cluster version 1.20.4 with archival enabled
  1. Disable archival globally in the static config
  1. Attempt to upgrade to 1.21.6

## Specifications

  - Version: 1.20.4\1.21.6
  - Platform: Linux amd64

## Additional Info:
The code that panics was introduced in [this PR](https://github.com/temporalio/temporal/pull/4190) (CC @wxing1292 )

We are not entirely sure how Temporal stores stuff in the persistence layer (a lot of it is blobs) but there appear to be shards with  archival queues while archival is actually disabled.

Given this server version is old and the code that bugs out appears to be migrationary in nature - we are ok with a workaround just for the upgrade but we wanted to make sure that whatever we do is safe.

Specifically - if we were to enable archival globally (but not for any namespace in particular), perform the upgrade and then again disable archival again - would we face any future issues?
Alternatively - should we hack the code to load the archival category for the compatibility code to work? (it appears to be loaded conditionally on archival being enabled cluster wide)

We were looking for a way to do the upgrade without archival since we disabled it because it has previously caused a different panic (patched in a newer version of Temporal we're not on yet)

Happy to provide additional info or help with the investigation in any way.
Thank you!

#### Comments (3)

<details>
<summary><strong>wxing1292</strong> commented on 2024-07-18 19:29:45.000 UTC</summary>

i believe the 1.21.x version does not take workflow archival into considersation and caused this issue. 
the bug should be fixed by 1.22.x? cc @yycptt
i will let oss server team to take over this conversation 

[1.21.x logic](https://github.com/temporalio/temporal/blame/v1.21.6/service/history/shard/context_impl.go#L1670)
[1.22.x logic](https://github.com/temporalio/temporal/blame/v1.22.7/service/history/shard/context_impl.go#L1670)

</details>

<details>
<summary><strong>ytaben</strong> commented on 2024-07-26 15:47:07.000 UTC</summary>

Hey @wxing1292 @yycptt,

Any chance you were able to take a look?
You mentioned that the bug is allegedly fixed in 1.22.x but unless it's safe to jump to 1.22.x (which I assume it isn't) - we need a safe way to upgrade to 1.21.x

We would be happy to help with the investigation and\or make a fix contribution, we just aren't familiar enough with the codebase and shard structure to be certain our fix works and we don't introduce another regression


</details>

<details>
<summary><strong>yycptt</strong> commented on 2024-07-26 16:33:12.000 UTC</summary>

One way I can think of is 
- Set `history.archivalProcessorSchedulerWorkerCount` dynamic config to 0 (this will prevent any archival task from being executed). You can also set `history.queuePendingTasksMaxCount` to a lower value like 1000 to prevent loading too many tasks into memory as there's no worker to process them.
- Enable archival in the static config (this will register the queue category so shard context logic won't panic). 
- Deploy 1.21, and then 1.22. 
- Revert both config changes and restart the service

If you don't care about archival task is executed or not, I guess just enabled the static config for archival, do the upgrade for 1.21 and 1.22 and then disable. 

</details>


---

### #6146: temporal on arm64 Linux is not working correctly ...amd64 binaries?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6146 |
| **State** | OPEN |
| **Author** | bmacauley (Brian Macauley) |
| **Created** | 2024-06-15 09:09:15.000 UTC (1y 6m ago) |
| **Updated** | 2025-12-31 17:06:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | potential-bug |
| **Assignees** | stephanos |
| **Milestone** | None |

#### Description

## Expected Behavior
temporal on arm64 Linux is working correctly

## Actual Behavior
temporal on arm64 Linux is not working correctly
I believe the binaries are amd64 binaries

## Steps to Reproduce the Problem
  1. I am setting up temporal on k3s on ubuntu 2204 in Multipass on a Mac M1. 
  architecture
  `ubuntu@temporal:~$ uname -m
aarch64`

  2.After running helm to deploy the temporal on k3s, I get crashloopbackoff errors on some of the containers
  
  running correctly...
  temporal-admintools
  temporal-web
  
  crashloopbackoff...
  temporal-frontend
  temporal-history
  temporal-matching
  temporal-worker
  
  error message....
  `/etc/temporal/start-temporal.sh: line 16: /usr/local/bin/temporal-server: cannot execute binary file: Exec format error`
  
  The correct arm64 containers are being pulled

3. Looked inside the temporal-admintools container

working....
tctl
temporal

not working...
tdbg
temporal-cassandra-toll
temporal-sql-tool

`
temporal-sql-tool
bash: /usr/local/bin/temporal-sql-tool: cannot execute binary file: Exec format error
`

i downloaded temporal-sql-tool to check it

`ubuntu@temporal:~$ file temporal-sql-tool
temporal-sql-tool: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=YKktCnQevbFAZNSNLwHy/GM78rayp1t0RHnJ99Q3e/vpPHjRcqExXPiFanYzQs/XafXwTQ1y_JNzqzuhTvq, with debug_info, not stripped`

It is an amd64 binary?

## Specifications

  - Version:  1.24.1
  - Platform: linux/arm64


#### Comments (5)

<details>
<summary><strong>stephanos</strong> commented on 2024-06-21 20:16:51.000 UTC</summary>

Thank you for your report, @bmacauley! We had the same issue reported earlier (see https://github.com/temporalio/docker-builds/issues/213) and pushed new images for the affected tags. Could you verify? (you might have to run `docker rmi` to clear the image from your cache)

</details>

<details>
<summary><strong>cemelo</strong> commented on 2024-08-18 12:21:44.000 UTC</summary>

This is still happening with `temporalio/admin-tools:1.24.2-tctl-1.18.1-cli-0.13.0`, which is the default version used by the helm chart.

</details>

<details>
<summary><strong>roytev</strong> commented on 2024-08-27 13:11:32.000 UTC</summary>

^ the admin-tools not working on arm64


</details>

<details>
<summary><strong>MrSaints</strong> commented on 2024-09-26 20:57:54.000 UTC</summary>

Seems like a few image tags are still affected including 1.22.7.1. 

</details>

<details>
<summary><strong>vimscientist69</strong> commented on 2025-12-31 17:06:33.000 UTC</summary>

I have a different issue, specifically unsymbolicated issues with temporal swift sdk on x86_64 when building for release builds using static linking. When not using temporal-swift-sdk, or when doing a debug build without static linking, there is no issue. This happens in a docker container, for more information and MRE repository, check out this issue. Been stuck on this for 3 weeks, I guess it is just a skill issue -> https://github.com/apple/swift-temporal-sdk/issues/55

</details>


---

### #5693: Multiple Hosts in connection string are not handled properly (POSTGRES_SEEDS accepts only single host?)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5693 |
| **State** | OPEN |
| **Author** | zoulja |
| **Created** | 2024-04-10 12:57:09.000 UTC (1y 8m ago) |
| **Updated** | 2024-07-17 16:43:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
According to https://github.com/jackc/pgx/pull/545 pgx can accept multiple Postgres hosts in the format like 2.2.2.2:1,127.0.0.1,4.2.4.2
I assume that temporal passes this to a driver via POSTGRES_SEEDS env variable
## Actual Behavior
From the logs I see that temporal sees string like host1,host2 as a single PG host "host1,host2" so fails to connect
```
temporal              | 2024-04-10T12:13:31.205Z	ERROR	Unable to connect to SQL database.	{"error": "dial tcp: lookup temporal-pg-host1,temporal-pg-host2: no such host", "logging-call-at": "handler.go:52"}
```
## Steps to Reproduce the Problem

  1. Get some Postgres cluster in master/replica mode consists of temporal-pg-host1 and temporal-pg-host2 nodes
  2. Modify [docker-compose-postgres.yml](https://github.com/temporalio/docker-compose/blob/main/docker-compose-postgres.yml), comment out postgres container section and set `POSTGRES_SEEDS=temporal-pg-host1,temporal-pg-host2`
  3. Run `docker compose -f docker-compose-postgres.yml up`

## Specifications

  - Version: Actual docker composer manifest from the official Github repo
  - Platform: MacOS, Docker Compose version v2.26.1-desktop.1


#### Comments (5)

<details>
<summary><strong>robholland</strong> commented on 2024-05-15 16:12:51.000 UTC</summary>

We don't use pgx driver by default. Did you try with `DB=postgres12_pgx`?

</details>

<details>
<summary><strong>zoulja</strong> commented on 2024-07-11 09:46:28.000 UTC</summary>

Hello @robholland 
Sorry for the late response and thanks a lot for this hint - with postgres12_pgx I really can use 2 hosts, like
`POSTGRES_SEEDS: dbhost1,dbhost2`
The issue is that in this setup app can't detect which one is master and which one is replica, so tries to connect to the first host, and in case of switchover it can be a replica host.
So I get the following result:
`2024-07-10T20:48:11.640Z    ERROR    Unable to setup SQL schema.    {"error": "ERROR: cannot execute CREATE TABLE in a read-only transaction  (SQLSTATE 25006)", "logging-call-at": "handler.go:57"}`

Do you have any idea how to get around it? 
From PGX documentation seems it can be (at least partially) achieved via connection string, like:
postgres://username:password@host1:5432,host2:5432,host3:5432/dbname?sslmode=disable&target_session_attrs=read-write&connect_timeout=10

Is there some way we can pass connection string instead of list of hosts?

</details>

<details>
<summary><strong>robholland</strong> commented on 2024-07-15 16:01:21.000 UTC</summary>

We don't currently allow you to set specific connect attributes via env variables. I'm going to be working on the config template soon, I will keep this in mind. You could work around this by overriding the config template that is used. You can see an example of how the connect attributes look in the MySQL transaction isolation support: https://github.com/temporalio/temporal/blob/main/docker/config_template.yaml#L88

If you duplicated that for the target_session_attrs setting (assuming that's the one that made the difference), then that will sent to the driver, as it does when you use a URL.

</details>

<details>
<summary><strong>robholland</strong> commented on 2024-07-15 16:03:45.000 UTC</summary>

You would also need to adjust the template to cope with multiple hosts in the seeds variable. Despite the name, we unfortunately expect only one hostname in that variable, so we append ":$DB_PORT" to it, which won't work with multiple host names. Again, given pgx supports multiple hosts, I'll give some thought to how this might look if supported by the template. Please feel free to suggest anything you come up with which feels good for you.

</details>

<details>
<summary><strong>zoulja</strong> commented on 2024-07-17 16:43:26.000 UTC</summary>

Hello @robholland, thank you very much for looking into this issue!
I'm really not sure what would be the best architectural/design solution in this case, the most simplest way to me would be if POSTGRES_SEEDS could accept the same formats as PGX can (if PGX is in use)   

</details>


---

### #5364: Inconsistent tctl workflow list behaviour

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5364 |
| **State** | OPEN |
| **Author** | cda0011 (Conor Allen) |
| **Created** | 2024-01-29 15:32:32.000 UTC (1y 11m ago) |
| **Updated** | 2025-10-31 14:40:16.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 1 |
| **Priority Score** | 5 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

## Expected Behavior
When repeatedly running tctl -n prd workflow list --pdt --query="ExecutionStatus=1"

I would expect that it would return the same empty list 
```
temporal-admintools-9bccdbb6f-wt4jj:/etc/temporal$ tctl --ns prd workflow list --pdt --query="ExecutionStatus=1"
  WORKFLOW TYPE | WORKFLOW ID | RUN ID | TASK QUEUE | START TIME | EXECUTION TIME | END TIME
temporal-admintools-9bccdbb6f-wt4jj:/etc/temporal$ tctl --ns prd workflow list --pdt --query="ExecutionStatus=1"
     WORKFLOW TYPE    |                                 WORKFLOW ID                                 |                RUN ID                |         TASK QUEUE          |      START TIME      |    EXECUTION TIME    |       END TIME
  create-dce-key      | fb8c5155-25a6-44a0-a9d4-cfb3c177961b                                        | b0a74e13-3274-4f5b-9e85-af70fec2a718 | prd.keycustodian.command | 2024-01-29T11:03:14Z | 2024-01-29T11:03:14Z | 0001-01-01T00:00:00Z
  mutex-workflow      | mutex-id-piedpiper::6c67b887-b967-3e01-adab-19f07885d6ed                    | 6cd24d0b-5fa1-4420-9f18-9127b590c7ab | prd.keycustodian.command | 2024-01-29T11:03:13Z | 2024-01-29T11:03:13Z | 0001-01-01T00:00:00Z
  vault-commands      | prd.attribute.pv-xQ/wsEgMSyL4pTEumHs7nTpNrHI8Rd1LdpfqyK8DynQ=            | eb31859e-2ac8-47ae-8f14-0718d4957d5c | prd.attribute.commands   | 2024-01-29T11:03:13Z | 2024-01-29T11:03:13Z | 0001-01-01T00:00:00Z
  mutex-workflow      | mutex-id-piedpiper::ca1f5c5b-2907-3b75-b08d-1b4cdcdbcdd8                    | a1c72520-ad82-4c1e-b706-986f981af3a9 | prd.keycustodian.command | 2024-01-29T11:03:13Z | 2024-01-29T11:03:13Z | 0001-01-01T00:00:00Z
  create-dce-key      | c7cab2ef-1fd9-4e6a-9b9f-3b778d588be6                                        | a8d26afa-f2a4-48cc-ab70-59d13b4edc2d | prd.keycustodian.command | 2024-01-29T11:03:12Z | 2024-01-29T11:03:12Z | 0001-01-01T00:00:00Z
  create-dce-key      | 9e726b71-b8a0-41df-aabc-72fe767c6539                                        | cdd407bc-3115-479e-b74f-b36b9d5425f9 | prd.keycustodian.command | 2024-01-29T11:03:12Z | 2024-01-29T11:03:12Z | 0001-01-01T00:00:00Z
  create-dce-key      | 0c1e5cf6-45ae-4e4f-8aa5-70ac2be9e6d4                                        | 20340756-1870-4624-a052-8bf124ceaeba | prd.keycustodian.command | 2024-01-29T11:03:12Z | 2024-01-29T11:03:12Z | 0001-01-01T00:00:00Z
  mutex-workflow      | mutex-id-piedpiper::31f62401-6c0d-31f0-a6bb-d690ae1321a7                    | 45f0729b-231b-428d-8d81-f1bc8f0c2d3c | prd.keycustodian.command | 2024-01-29T11:03:12Z | 2024-01-29T11:03:12Z | 0001-01-01T00:00:00Z
  mutex-workflow      | mutex-id-piedpiper::79e1a9ba-1f9e-32ea-93ae-0d9a02ede62c                    | 3d5409da-2911-490d-bcf7-92ea9796d896 | prd.keycustodian.command | 2024-01-29T11:03:12Z | 2024-01-29T11:03:12Z | 0001-01-01T00:00:00Z
  update-search-index | vault-service:search-indexer:piedpiper:8e98a171-a0c6-338c-b311-a74c1e57378e | 79b5acd1-c6ac-4df6-b366-592e12d902d0 | prd.vault.commands       | 2024-01-29T11:02:37Z | 2024-01-29T11:02:37Z | 0001-01-01T00:00:00Z

```

## Actual Behavior
When running 
```
tctl -n prd workflow list --pdt --query="ExecutionStatus=1"
```
multiple times, occasionally a set of completed WFs are returned, as shown above
Examining one of those WFs shows
```
temporal-admintools-9bccdbb6f-wt4jj:/etc/temporal$ tctl --ns prd workflow desc -w c7cab2ef-1fd9-4e6a-9b9f-3b778d588be6
{
  "executionConfig": {
    "taskQueue": {
      "name": "prd.keycustodian.command",
      "kind": "Normal"
    },
    "workflowExecutionTimeout": "0s",
    "workflowRunTimeout": "0s",
    "defaultWorkflowTaskTimeout": "10s"
  },
  "workflowExecutionInfo": {
    "execution": {
      "workflowId": "c7cab2ef-1fd9-4e6a-9b9f-3b778d588be6",
      "runId": "a8d26afa-f2a4-48cc-ab70-59d13b4edc2d"
    },
    "type": {
      "name": "create-dce-key"
    },
    "startTime": "2024-01-29T11:03:12.943155827Z",
    "closeTime": "2024-01-29T11:03:17.358251094Z",
    "status": "Completed",
    "historyLength": "23",
    "memo": {

    },
    "searchAttributes": {
      "indexedFields": {
        "BuildIds": "[\"unversioned\"]"
      }
    },
    "autoResetPoints": {

    },
    "stateTransitionCount": "14"
  }
}
temporal-admintools-9bccdbb6f-wt4jj:/etc/temporal$ tctl --ns prd workflow show -w c7cab2ef-1fd9-4e6a-9b9f-3b778d588be6
   1  WorkflowExecutionStarted                  {WorkflowType:{Name:create-dce-key}, ParentInitiatedEventId:0, TaskQueue:{Name:prd.keycustodian.command, Kind:Normal},
                                                Input:[TlBZAGOCagZoZWFkZXIwJG1hbmV0dS5wcm90b2J1Zi5SZXF1ZXN0SGVhZGVyLXJlY29yZGOEagVtYWdpYyoArM7eagd2ZXJzaW9uAGoEdHhpZGCkMWQ1MTE
                                                zYmUtNGU1ZC00YjQ1LTkwMmYtNTlkMGM3NzhkMWVkaglwcmV2LXR4aWQiagN2aWQwHm1hbmV0dS5wcm90b2J1Zi5WYXVsdElkLXJlY29yZGODagxwcm92aWR
                                                lci14aWRgiXBpZWRwaXBlcmoLdmF1bHQtbGFiZWxgpGNhMWY1YzViLTI5MDctM2I3NS1iMDhkLTFiNGNkY2RiY2RkOGoIa2V5c3BhY2Ui],
                                                WorkflowExecutionTimeout:0s, WorkflowRunTimeout:0s, WorkflowTaskTimeout:10s, Initiator:Unspecified,
                                                OriginalExecutionRunId:a8d26afa-f2a4-48cc-ab70-59d13b4edc2d, Identity:1@mcp-keycustodian-service-gw-6566847c95-dz625,
                                                FirstExecutionRunId:a8d26afa-f2a4-48cc-ab70-59d13b4edc2d, Attempt:1, FirstWorkflowTaskBackoff:0s, ParentInitiatedEventVersion:0}
   2  WorkflowTaskScheduled                     {TaskQueue:{Name:prd.keycustodian.command,
                                                Kind:Normal}, StartToCloseTimeout:10s,
                                                Attempt:1}
   3  WorkflowTaskStarted                       {ScheduledEventId:2,
                                                Identity:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h,
                                                RequestId:f5823e38-1d67-4522-b70e-3de33bd7c6e5,
                                                SuggestContinueAsNew:false, HistorySizeBytes:620}
   4  WorkflowTaskCompleted                     {ScheduledEventId:2, StartedEventId:3,
                                                Identity:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h,
                                                SdkMetadata:{CoreUsedFlags:[], LangUsedFlags:[1]},
                                                MeteringMetadata:{NonfirstLocalActivityExecutionAttempts:0}}
   5  MarkerRecorded                            {MarkerName:LocalActivity,
                                                Details:map{activityId:["3c7ac2f4-98ef-3d90-b7b2-4aa7ce073b8e"],
                                                meta:[{"firstSkd":1706526193081,"atpt":1,"backoff":null}],
                                                result:[TlBZAGoCb2s], time:[1706526193317],
                                                type:["start-mutex-activity"]}, WorkflowTaskCompletedEventId:4}
   6  WorkflowExecutionSignaled                 {SignalName:prd.temporal.mutex/acquired,
                                                Input:[TlBZAGOBag5hY3F1aXNpdGlvbi1pZGCkZjY2OGRiNzQtMDNkZS0zOTUyLTg0OTYtMDg4Yzg1NGNhNmUw],
                                                Identity:history-service}
   7  WorkflowTaskScheduled                     {TaskQueue:{Name:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h:502dcdd9-3d81-4828-ad63-8e99601c01dd,
                                                Kind:Sticky}, StartToCloseTimeout:10s, Attempt:1}
   8  WorkflowTaskStarted                       {ScheduledEventId:7,
                                                Identity:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h,
                                                RequestId:95864549-cdb7-4e72-ba25-f7649073b975,
                                                SuggestContinueAsNew:false, HistorySizeBytes:1526}
   9  WorkflowTaskCompleted                     {ScheduledEventId:7, StartedEventId:8,
                                                Identity:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h,
                                                MeteringMetadata:{NonfirstLocalActivityExecutionAttempts:0}}
  10  MarkerRecorded                            {MarkerName:LocalActivity,
                                                Details:map{activityId:["ce4d139a-1f8a-30b6-98ea-5e6f56d71ec3"],
                                                meta:[{"firstSkd":1706526195131,"atpt":1,"backoff":null}],
                                                result:[TlBZAAk], time:[1706526194555], type:["dce-key-exists?"]},
                                                WorkflowTaskCompletedEventId:9}
  11  MarkerRecorded                            {MarkerName:SideEffect,
                                                Details:map{data:[TlBZAA8AlQTMpL6T6/fJmNVZ/BnxixMoTzdmpUQ9aimrnq1gRdj+9+JFHNpysRgln4Ck4fGANOZdbq8pB6AeqNguk6AIVm2IedpFUSt9+YHubC
                                                iCF5FZDBvkuUNdtUojVz11KMXF8Ij286Yehysd9l9kj1y4lzGXydWrFxqiACIdIGb0uAKznkE92XBFUZp/aIVAhPOlL428ZSje]}, WorkflowTaskCompletedEventId:9}
  12  ActivityTaskScheduled                     {ActivityId:76b7d770-0ed8-3fa7-8fc7-2623d6ffe472, ActivityType:{Name:create-dce-record!},
                                                TaskQueue:{Name:prd.keycustodian.command, Kind:Normal},
                                                Input:[TlBZAGOIagR0eGlkYKQxZDUxMTNiZS00ZTVkLTRiNDUtOTAyZi01OWQwYzc3OGQxZWRqA3VpZGCvcGllZHBpcGVyOjpjYTFmNWM1Yi0yOTA3LTNiNzUtYjA
                                                4ZC0xYjRjZGNkYmNkZDhqBXJlYWxtYIlwaWVkcGlwZXJqBmtzcGFjZSJqBWxhYmVsYKRjYTFmNWM1Yi0yOTA3LTNiNzUtYjA4ZC0xYjRjZGNkYmNkZDhqA2t
                                                leQ8AlQTMpL6T6/fJmNVZ/BnxixMoTzdmpUQ9aimrnq1gRdj+9+JFHNpysRgln4Ck4fGANOZdbq8pB6AeqNguk6AIVm2IedpFUSt9+YHubCiCF5FZDBvkuUN
                                                dtUojVz11KMXF8Ij286Yehysd9l9kj1y4lzGXydWrFxqiACIdIGb0uAKznkE92XBFUZp/aIVAhPOlL428ZSjeagVzaGFyZCoAAAARag1sYXN0X3JvdGF0aW9
                                                uWgAAAAAAAAAA], ScheduleToCloseTimeout:0s, ScheduleToStartTimeout:30s, StartToCloseTimeout:10s, HeartbeatTimeout:0s,
                                                WorkflowTaskCompletedEventId:9, RetryPolicy:{InitialInterval:1s, BackoffCoefficient:2, MaximumInterval:1m40s, MaximumAttempts:3,
                                                NonRetryableErrorTypes:[]}}
  13  ActivityTaskStarted                       {ScheduledEventId:12,
                                                Identity:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h,
                                                RequestId:a4546f5a-1502-401c-9953-0877cb466349,
                                                Attempt:1}
  14  ActivityTaskCompleted                     {Result:[TlBZAGoHY3JlYXRlZA],
                                                ScheduledEventId:12, StartedEventId:13,
                                                Identity:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h}
  15  WorkflowTaskScheduled                     {TaskQueue:{Name:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h:502dcdd9-3d81-4828-ad63-8e99601c01dd,
                                                Kind:Sticky}, StartToCloseTimeout:10s, Attempt:1}
  16  WorkflowTaskStarted                       {ScheduledEventId:15,
                                                Identity:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h,
                                                RequestId:244027ee-1e7a-46e0-80e7-532beb216d1c,
                                                SuggestContinueAsNew:false, HistorySizeBytes:3293}
  17  WorkflowTaskCompleted                     {ScheduledEventId:15, StartedEventId:16,
                                                Identity:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h,
                                                MeteringMetadata:{NonfirstLocalActivityExecutionAttempts:0}}
  18  SignalExternalWorkflowExecutionInitiated  {WorkflowTaskCompletedEventId:17, NamespaceId:556fb5b4-1af3-442d-ad66-66617cca5380,
                                                WorkflowExecution:{WorkflowId:mutex-id-piedpiper::ca1f5c5b-2907-3b75-b08d-1b4cdcdbcdd8},
                                                SignalName:mutex-release-c7cab2ef-1fd9-4e6a-9b9f-3b778d588be6,
                                                Input:[TlBZAGOCaghpbnN0YW5jZWOBag5hY3F1aXNpdGlvbi1pZGCkZjY2OGRiNzQtMDNkZS0zOTUyLTg0OTYtMDg4Yzg1NGNhNmUwagR0eXBlagdyZWxlYXNl],
                                                ChildWorkflowOnly:false}
  19  ExternalWorkflowExecutionSignaled         {InitiatedEventId:18, Namespace:prd,
                                                NamespaceId:556fb5b4-1af3-442d-ad66-66617cca5380,
                                                WorkflowExecution:{WorkflowId:mutex-id-piedpiper::ca1f5c5b-2907-3b75-b08d-1b4cdcdbcdd8}}
  20  WorkflowTaskScheduled                     {TaskQueue:{Name:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h:502dcdd9-3d81-4828-ad63-8e99601c01dd,
                                                Kind:Sticky}, StartToCloseTimeout:10s, Attempt:1}
  21  WorkflowTaskStarted                       {ScheduledEventId:20,
                                                Identity:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h,
                                                RequestId:cc8b085e-7c6f-462b-9984-cfa6bb811ad8,
                                                SuggestContinueAsNew:false, HistorySizeBytes:4086}
  22  WorkflowTaskCompleted                     {ScheduledEventId:20, StartedEventId:21,
                                                Identity:1@mcp-keycustodian-service-cp-5bc9567c9f-zj92h,
                                                MeteringMetadata:{NonfirstLocalActivityExecutionAttempts:0}}
  23  WorkflowExecutionCompleted                {Result:[TlBZAHFqAm9rEw],
                                                WorkflowTaskCompletedEventId:22}
```


## Steps to Reproduce the Problem

  1.  run workflow list multiple times ... occasionally seeing a set of apparently completed workflows
 ```
tctl --ns prd workflow list --pdt --query="ExecutionStatus=1"
```

## Specifications

  - Version: 1.22.4 
  - Platform: kubernetes without elasticsearch extended visibility


#### Comments (1)

<details>
<summary><strong>tlalfano</strong> commented on 2025-10-31 14:40:03.000 UTC</summary>

Let's close this since tctl is deprecated. cc @bergundy for confirmation to close

</details>


---

### #4151: Allow update timeout/retry config for started activities with retry

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4151 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2023-04-10 20:36:43.000 UTC (2y 8m ago) |
| **Updated** | 2024-03-11 13:08:42.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 1 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

## Is your feature request related to a problem? Please describe 
There are two use cases that this feature will be super helpful for Temporal applications:
### Use case 1: Making async activity completion more reliable and efficient. 
Async activity completion is very handy/powerful/useful in certain scenario, when a workflow want to invoke an external action, and then wait for a response from external. This could be done by activity + signal, however, async activity completion could be more natural, less code to write, and more efficient (less events in history).

However, the current async activity completion has a potential reliability issue. 

The activity needs to pass down the activity taskToken(or at least the activityId) to external system so that the activity task can be completed by sending the token/taskId to Temporal service. So the async activity has be  be implemented this below sequence.
(see [doc](https://docs.temporal.io/application-development/features#asynchronous-activity-completion) )
```golang 
activityMethodForAsyncCompletion(ctx Context ){
    // Retrieve the activity information needed to asynchronously complete the activity.
    activityInfo := cadence.GetActivityInfo(ctx)
    taskToken := activityInfo.TaskToken

    // Send the taskToken to the external service that will complete the activity.
    ...

    // Return from the activity a function indicating that Cadence should wait for an async completion
    // message.
    return "", activity.ErrResultPending
}
``` 
But anything could happen in `Send the taskToken to the external service that will complete the activity`. There could be a timeout, or a worker sudden disruption. Note that right now SDK doesn't do anything to `activity.ErrResultPending` (just n[ot respond anything to server](https://github.com/temporalio/sdk-go/blob/1b62656a17d39001201bace9a01be646663dea49/internal/internal_task_pollers.go#L1070) )

When the failure happens, if the start to close timeout is very long, then the external system will not receive the token until next retry. 

As a workaround, application has to set very short start to close timeout, and then keep retrying on the activity. This leads to issues:
a. If the external system uses a async mechanism like message queue to receive the token, then it will receive a lot of duplicates
b. If the external system use a database to store it, then additional logic must be implemented with atomic/traditional operation for adding the token. 
The workaround not along cause a lot of complication, but also very in-efficient to do. Because in reality , this is a very extremely rare case (but we can't ignore it for some critical system, and it's against the Temporal's strong consistency guarantee). 

One solution to this is to let activity set a smaller heartbeat timeout initially, and then update the heartbeat timeout to infinite()) after successfully sending the token to external system. 

### Use case 2: Allow using different backoff retry/timeout strategy for different errors
Right now the backoff retry and policy is set as a global for all types of errors. This is inconvenient for some use cases that some retriable error can do a much bigger backoff than others. 

If allowing update the timeout config, then SDK can return back a different config based on different errors returned from the application(activity results).

## Describe the solution you'd like**
The started activities with retry have the timeout/retry config stored in mutable state, it should be straightforward to introduce an API to allow updating it. Also, the activity retry won't write down started even until closing so it should cause any problem for history replay. 

## Describe alternatives you've considered**
For use case 1(async activity), an alternative could be letting SDK send back the `activity.ErrResultPending` to server, so that server can stop retry for the activity. Right now SDK doesn't do anything to `activity.ErrResultPending` (just n[ot respond anything to server](https://github.com/temporalio/sdk-go/blob/1b62656a17d39001201bace9a01be646663dea49/internal/internal_task_pollers.go#L1070) )

## Additional context**
https://temporalio.slack.com/archives/CTQU95E84/p1681159067543939


#### Comments (1)

<details>
<summary><strong>airhorns</strong> commented on 2024-03-11 13:08:18.000 UTC</summary>

If I could add another use case in here it'd be making activities retry only after some deadline an external system hands them. When making calls to 3rd party APIs that are rate limited, if you exceed your rate limit, the service often replies with a `Retry-After` header. We know any more calls to the API before that time won't work, so it is inefficient to retry on a fixed schedule until then. Better would be having the activity adjust the retry options (somehow) to say "don't retry again until this point in time" at which point it may succeed or may fail again. 

</details>


---

### #4038: partitioned activity taskQueue based on workflowId, for better caching performance in activity workers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4038 |
| **State** | OPEN |
| **Author** | p4p4 (Patrick Klampfl) |
| **Created** | 2023-03-09 20:27:01.000 UTC (2y 9m ago) |
| **Updated** | 2023-09-02 01:26:25.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 1 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Tasks on an activityTaskqueue are randomly fetched by competing activity workers reading from the same queue.
In case of subsequent activity calls on different activity workers, caching per pod does not work effectively.

For Workflow workers, temporal takes care for their cache performance using [sticky exectution](https://docs.temporal.io/tasks#sticky-execution). For activity workers, a similar concept does not exist without custom code.

The typical workflows that I usually come up with consist of multiple activities which all operate on one entity (e.g. order workflow with activities operating on 1 ecommerce order, user workflow on 1 user, uber driver workflow on 1 driver entity).

The problem exists in the following scenario


* there is a set of activities on the same task queue, all operating on the same entity (e.g. Order)
* when the state of the entity is not fully owned by the workflow history alone (but instead e.g. in some data base, and temporal is only passing identifiers)
* and when there‚Äôs a node(/pod)-specific entity cache in place (which is probably not the case for smaller services), for example [Ehcache](https://www.ehcache.org/)

**Describe the solution you'd like**
Partition activity task queues based on workflowId, and assign partitions to specific worker instances (similar to partition assignment within a kafka consumer group).

The number of partitions could be either fixed, or dynamic. 
A limitation  to have smaller or equal number of workers than queue partitions is not strictly required. If the number of consumer exceeds the number of partitions, it would still improve caching if let‚Äôs say only 2 worker instances are competing for tasks on the same partition, as opposed to all worker instances read from all partitions.

Such a taskQueue feature should be only optional, and not the default, as only systems with a cache would benefit from it. Otherwise, random or round-robin dispatching would lead to a better load-balancing across the workers

**Describe alternatives you've considered**
 set custom task queue names in workflow code using activity options, like in the [fileprocessing](https://github.com/temporalio/samples-java/tree/main/src/main/java/io/temporal/samples/fileprocessing) example, which requires to have custom logic in


1. worker setup (to start the activity worker on  aunique taskqueue)
1. workflow code (to set the acivityOptions in workflowmethod dynamically, instead of when creating the Workflow worker)
1. activity code (to return a unique taskqueue)



#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2023-09-02 01:26:25.000 UTC</summary>

It seems what you needed is https://docs.temporal.io/workers#worker-session

</details>


---

### #3502: Add metric for number of open Workflow Executions

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3502 |
| **State** | OPEN |
| **Author** | lorensr (Loren ‚ò∫Ô∏è) |
| **Created** | 2022-10-17 22:02:39.000 UTC (3y 2m ago) |
| **Updated** | 2025-12-23 18:47:12.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

User request


#### Comments (5)

<details>
<summary><strong>ksapchuk</strong> commented on 2022-10-17 23:00:35.000 UTC</summary>

We would greatly benefit from this feature. We often have bursts of workflows and want to know how far behind we are in processing them. The task schedule latency provides us a rough indication that we are behind but with the metric having a 10 second maximum we have no idea how behind we are. 

We could query the number of open workflows periodically to emit our own metric but having one available in the client or server metrics would remove the need of manually emitting one. 

Reactions: üëç 1

</details>

<details>
<summary><strong>yiminc</strong> commented on 2022-10-21 21:53:36.000 UTC</summary>

Current recommendation is to do count query on visibility.
We will have to think about a long term solution.

</details>

<details>
<summary><strong>yinsidij</strong> commented on 2024-04-19 07:44:15.000 UTC</summary>

@yiminc any updates? The number of open workflow (could be emitted as a server metric) would be very helpful and straightforward in the dashboard

</details>

<details>
<summary><strong>falvarado-maven</strong> commented on 2025-04-28 14:42:39.000 UTC</summary>

We would also like to request this metric to be added to cloud available metrics https://docs.temporal.io/production-deployment/cloud/metrics/reference#available-metrics
This would be the clearest indication of temporal activity pressure on our backends, other metrics provide only indirect visibility.

</details>

<details>
<summary><strong>dustin-temporal</strong> commented on 2025-12-23 18:47:12.000 UTC</summary>

A generated metric for this is now available as part of Temporal Cloud metrics: https://docs.temporal.io/production-deployment/cloud/metrics/openmetrics/metrics-reference#temporal_cloud_v1_namespace_open_workflows

For self-hosted, the recommendation is still to query visibility to get this value.

</details>


---

### #2891: Document configuration (development.yaml, dynamicconfig)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2891 |
| **State** | OPEN |
| **Author** | kennedyjustin (Justin Kennedy) |
| **Created** | 2022-05-23 18:01:11.000 UTC (3y 7m ago) |
| **Updated** | 2024-08-20 08:25:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
There is almost no documentation on what the development.yaml file is, how it relates to dynamicconfig, etc. This is the closest thing: https://docs.temporal.io/server/configuration, and it is severely lacking a high-level explanation.

**Describe the solution you'd like**
A page dedicated to what this file is, how it relates to dynamicconfig, where is this file expected to be in a production deployment or docker-compose setup, etc.

**Describe alternatives you've considered**
N/A

**Additional context**
I am running through my local and production setup for the first time (I am a new Temporal user).


#### Comments (5)

<details>
<summary><strong>yiminc</strong> commented on 2022-05-27 21:51:32.000 UTC</summary>

We recently updated the document at https://docs.temporal.io/server/configuration, could you check again.

</details>

<details>
<summary><strong>kennedyjustin</strong> commented on 2022-05-28 22:00:34.000 UTC</summary>

For me the most confusing part of setting up temporal was the configuration, and now that I'm on the other side of it I think the biggest part missing was documentation on how to use temporal with out-of-the-box docker images (something I have become used to through software products from HashiCorp, Elastic, Postgres, etc.).

Most out-of-the-box docker images can be configured via command arguments or environment variables, and I think this is the way I was expecting to use temporal (and indeed it is how I am using it now). The existence of https://github.com/temporalio/temporal/blob/master/docker/config_template.yaml and https://github.com/temporalio/docker-builds/tree/main/docker helped me understand exactly how its possible to setup temporal via docker images, but I didn't find this documented anywhere, and had rely on combing through the repos to understand it all.

I think a blog post in the same style as https://docs.temporal.io/blog/auto-setup/, but about the topic of building temporal as a binary yourself with a `development.yaml` contrasted with using a pre-built docker image (`temporalio/server`) and environment variables would be a huge help to those that want to consume temporal through docker images like me.

With that all said it wasn't clear to me whether blessing a pre-built docker image setup for production use is something that is on the roadmap or if the team strongly recommends building yourself for the medium or long-term future.

So to conclude I guess what I was hoping for was some documentation about:
1. If one were to go down the path of setting up temporal through the pre-built images, how does configuration work at a high level.
2. Some discussion about whether pre-build docker images for production use (and potentially even 100% parity with `development.yaml` through env vars or command options) is something that is on the roadmap or being thought about on the team.

I also want to say that I am new to the community, so I apologize if this is something that has already been hashed out.

Reactions: üëç 14

</details>

<details>
<summary><strong>sharadregoti</strong> commented on 2022-10-21 04:47:00.000 UTC</summary>

I am still facing this issue, cannot understand the difference between development.yaml & dynamiconfig

Reactions: üëç 1

</details>

<details>
<summary><strong>leofdgit</strong> commented on 2022-11-29 10:20:40.000 UTC</summary>

+1, the docs on configuration are a bad part of what is otherwise good documentation.

Reactions: üëç 1

</details>

<details>
<summary><strong>od82078</strong> commented on 2024-08-20 08:25:00.000 UTC</summary>

Has there been any progress here? I'm also a new temporal user looking to set it up for my team and this still seems to be a problem 2 years later...

</details>


---

### #2630: DNS name in BroadcastAddress 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2630 |
| **State** | OPEN |
| **Author** | ai-zelenin (ARTEM ZELENIN) |
| **Created** | 2022-03-22 09:36:23.000 UTC (3y 9m ago) |
| **Updated** | 2023-03-03 20:20:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | jbreiding |
| **Milestone** | None |

#### Description

I try deploy temporal cluster on docker swarm(need concept proof(scalability/limitations) before we will use it in prod).
Kuber+helm too cumbersome for my laptop(and i have not too much expertise in it (i am developer not a devops))
We face problems with BIND_ON_IP (it sets only on one interface and cannot see requests from others) and
TEMPORAL_BROADCAST_ADDRESS(we can not set a dns name as address(validation failure ringpop.go:75) can not understand reason for this restriction) 

**Probable solution**
TEMPORAL_BROADCAST_ADDRESS(BroadcastAddress) can be dns name
BIND_ON_IP is 0.0.0.0 by default


**Alternatives solution**
Maybe you can add a swarm config in docker-compose repo for concept proof purpose.
Single node deployment not enough to be sure in scalability and see bottlenecks. 

Sorry for bothering))


#### Comments (5)

<details>
<summary><strong>jbreiding</strong> commented on 2022-03-25 22:36:31.000 UTC</summary>

I think its a good idea to have a sample for docker-compose in swarm mode.

These two settings have different meanings:

BIND_ON_IP is the local IP address to bind on.
0.0.0.0 represents all interfaces and is needed for containerized services.
localhost in the context of a container process is not accessible to other containers, generally not exposed.

TEMPORAL_BROADCAST_ADDRESS is the address that is reachable by other temporal services.

Together these settings enable the membership coordination.

In Kubernetes this is the pod IP address, I am not sure of the analog in docker swarm mode.
This is the IP that would be provided and showing the network the containers are connected to.

I can see how an example would help make this clearer.
I will have to dig around docker docs and see what I can come up with.

</details>

<details>
<summary><strong>jbreiding</strong> commented on 2022-03-28 16:00:36.000 UTC</summary>

This is the functionality from Kubernetes that enables this to work.
https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/

I so far haven't been able to locate the docker equivalent. A DNS name is not going to be sufficient as this value needs to be unique per replica, this represents the IP reachable by other pods in the docker network.

You could allocate static IPs for each replica in the docker network, and then configure this per replica.

To be honest it is going to be more complicate than just using Kubernetes and helm.

Have you given [k3d.io](https://k3d.io/v5.4.0/) a try?



</details>

<details>
<summary><strong>ai-zelenin</strong> commented on 2022-03-29 09:48:32.000 UTC</summary>

Make unique dns names much more simpler.
If broadcast address can accept a dns name i can just launch it without containers and set a bunch of names in /etc/hosts
I think an application can not require for k8 or any other orchestrator for launching in local environment
Developers rarely have such expertise. And i can not blame them)) k8 is not very developer friendly

</details>

<details>
<summary><strong>jbreiding</strong> commented on 2022-03-29 14:52:59.000 UTC</summary>

Without containers and on a local system using `127.0.0.1` with different unique ports in the services section.

Docker in swarm mode is where the problem occurs.

You do not `need` Kubernetes.
Right now we have helm for helping simplify running temporal with multiple replicas in a production scenario.

Temporal is a go binary and can be run without containers.

</details>

<details>
<summary><strong>Sanil2108</strong> commented on 2022-05-20 10:59:25.000 UTC</summary>

@ai-zelenin I am also looking into similar issues, adding my two cents for adding DNS names for docker configuration if it helps anyone.

You can write a bit of code to resolve DNS names at container startup and set the appropriate value for `TEMPORAL_BROADCAST_ADDRESS`.

For example, I added these couple of lines of code in `entrypoint.sh`
```
: "${TEMPORAL_BROADCAST_ADDRESS:=$(hostname -i)}"
export TEMPORAL_BROADCAST_ADDRESS
```

Reactions: ‚ù§Ô∏è 2

</details>


---

### #2320: Thrift version update and unification

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2320 |
| **State** | OPEN |
| **Author** | andrey-dubnik (cloudwiz) |
| **Created** | 2021-12-22 18:06:14.000 UTC (4 years ago) |
| **Updated** | 2023-03-03 20:21:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

Hi

I have a question which came out from the security scan we did recently. Is there a reason on why thrift (github.com/apache/thrift@v0.0.0-20161221203622-b2a4d4ae21c7) is pinned? It does look like this one is from version 0.10. There is another one used by the ringpop-go (actually 2) one from 0.9.3 and another one from the no longer maintained repo. Latest thrift is 0.15 and beyond 0.13 have some active CVEs. Could you please consider reviewing the thrift versions in those 2 packages?

Here is some dependency graph

```
go.temporal.io/server github.com/apache/thrift@v0.0.0-20161221203622-b2a4d4ae21c7
go.temporal.io/server github.com/temporalio/ringpop-go@v0.0.0-20211012191444-6f91b5915e95
github.com/temporalio/ringpop-go@v0.0.0-20211012191444-6f91b5915e95 github.com/apache/thrift@v0.0.0-20150905105024-5bc8b5a3a5da
github.com/temporalio/ringpop-go@v0.0.0-20211012191444-6f91b5915e95 github.com/samuel/go-thrift@v0.0.0-20191111193933-5165175b40af
```

**Describe the solution you'd like**

Minimum thrift library version of 0.13, better 0.15

**Describe alternatives you've considered**

None available


#### Comments (5)

<details>
<summary><strong>tminusplus</strong> commented on 2021-12-23 19:42:11.000 UTC</summary>

For context there has been some effort put into this outside of the Temporal team. Happy to continue pushing along my patches if they are still interested. The update is not trivial and includes breaking changes.

- PR pending on forked tchannel-go for update to v0.13.0 at https://github.com/temporalio/tchannel-go/pull/1.
- Opened an issue with uber/tchannel-go asking them to revisit updating Thrift at https://github.com/uber/tchannel-go/issues/844.
- Have a patch ready to update the forked ringpop-go to support v0.13.0 once tchannel-go is updated.

Happy to change my patches to support whatever version the Temporal team would like to use. Likely, v0.13.0 is the most compatible with other Go dependencies, which is useful for mono-repos. But, updating to v0.14.0+ fixes a potential DOS [CVE-2020-13949](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13949). This CVE does not seem too dangerous but will leave judgement calls up to the team.

I will also be testing these patches on a three-node cluster in the next week or so.

</details>

<details>
<summary><strong>yiminc</strong> commented on 2022-01-07 22:12:32.000 UTC</summary>

cc @mcbryde

</details>

<details>
<summary><strong>tminusplus</strong> commented on 2022-01-18 02:37:00.000 UTC</summary>

I added a branch to update to v0.15.0, the latest. I will open the PR for review once I have tested the ringpop-go changes, and then tested Temporal in a production environment with it.

https://github.com/temporalio/tchannel-go/pull/2

</details>

<details>
<summary><strong>tminusplus</strong> commented on 2022-01-24 22:02:16.000 UTC</summary>

PR is opened https://github.com/temporalio/tchannel-go/pull/2

</details>

<details>
<summary><strong>tminusplus</strong> commented on 2022-08-19 22:53:28.000 UTC</summary>

This should be done now with https://github.com/temporalio/temporal/pull/3250

</details>


---

### #1895: Use "nobody" account to run binaries in the docker

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1895 |
| **State** | OPEN |
| **Author** | jrsmiley (John Smiley) |
| **Created** | 2021-09-07 21:21:51.000 UTC (4y 3m ago) |
| **Updated** | 2023-05-30 16:31:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | dnr, underrun, alexshtin |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
The linked PR is intended to allow temporal to run in Docker as a non-root user.  

**Describe the solution you'd like**
Looking at the changes, it appears that uid 1000 and gid 1000 were used for the temporal user id and group id. Under some implementations of Docker, the user is required to supply their own uid:gid to run the container, which may conflict with the 1000:1000 chosen. May I suggest using the ‚Äúnobody‚Äù user to run temporal under?

**Additional context**
Docker implementations that require uid:gid to be passed as arguments as part of a non-root runtime environment.


#### Comments (5)

<details>
<summary><strong>underrun</strong> commented on 2021-09-08 21:45:25.000 UTC</summary>

nobody isn't a great fit because we need filesystem permissions for the user - 

but i'm not sure i understand why it's a problem to run the container with 1000:1000 if your user is 1000:1000 - can you explain why this is a problem for your use case?

</details>

<details>
<summary><strong>jrsmiley</strong> commented on 2021-09-09 18:03:37.000 UTC</summary>

Would you be available for a call to discuss in real-time?


On Wed, Sep 8, 2021 at 4:45 PM Derek Wilson ***@***.***>
wrote:

> nobody isn't a great fit because we need filesystem permissions for the
> user -
>
> but i'm not sure i understand why it's a problem to run the container with
> 1000:1000 if your user is 1000:1000 - can you explain why this is a problem
> for your use case?
>
> ‚Äî
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/temporalio/temporal/issues/1895#issuecomment-915594934>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF5UZUKPXQYSPP6R55SLT2LUA7KQBANCNFSM5DTG3MEQ>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>


</details>

<details>
<summary><strong>dnr</strong> commented on 2021-09-10 17:40:59.000 UTC</summary>

I'm not sure I understand the problem either. Can you be specific about the container implementation you're referring to so that we can look into it?

The point of `nobody` is that nothing should ever run as that uid, so I don't think that makes sense here.

</details>

<details>
<summary><strong>jrsmiley</strong> commented on 2021-09-13 13:51:06.000 UTC</summary>

I'm not sure I entirely understand either.  I'm quoting a Docker expert
that I'm working with.  He has offered to join a call to explain.

On Fri, Sep 10, 2021 at 12:41 PM David Reiss ***@***.***>
wrote:

> I'm not sure I understand the problem either. Can you be specific about
> the container implementation you're referring to so that we can look into
> it?
>
> The point of nobody is that nothing should ever run as that uid, so I
> don't think that makes sense here.
>
> ‚Äî
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/temporalio/temporal/issues/1895#issuecomment-917089472>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF5UZUNW6JP6OM3QDJOZBS3UBI7LNANCNFSM5DTG3MEQ>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>


</details>

<details>
<summary><strong>plaisted</strong> commented on 2023-05-30 16:31:39.000 UTC</summary>

We have a similar requirement for passing security scans that require UID/GID > 10000 (https://kubesec.io/basics/containers-securitycontext-runasuser/). I know there are other ways to mitigate this risk but this is a pretty common requirement for vulnerability scanning of containers.


</details>


---

### #1882: support manual retries

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1882 |
| **State** | OPEN |
| **Author** | philosophicus |
| **Created** | 2021-09-02 10:36:19.000 UTC (4y 4m ago) |
| **Updated** | 2023-03-03 20:21:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 5 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
No.

**Describe the solution you'd like**
There are ExponentialRetryPolicy and TwoPhaseRetryPolicy as far as I know. I wonder whether it's possible to have a ManualRetryPolicy which enables users to determine when to retry.

**Describe alternatives you've considered**


**Additional context**



#### Comments (5)

<details>
<summary><strong>yiminc</strong> commented on 2021-09-02 16:01:53.000 UTC</summary>

You are asking for a full flow control on retry. You can achieve this by using signals. In your workflow, you have a signal channel, and depending on when and what signal it received, the workflow react on the signal. For example, you can define 2 type of signals, one to continue retry activity, the other to abort the activity. Your user could decide if (and when) they want to continue retry, as well as when to abort.  

</details>

<details>
<summary><strong>philosophicus</strong> commented on 2021-09-03 10:50:06.000 UTC</summary>

@yiminc Thanks a lot for the guidance. I'll definitely test it myself.
Will the heartbeat details recorded in the initial activity still be available in the retried activities?

</details>

<details>
<summary><strong>yiminc</strong> commented on 2021-09-03 15:58:07.000 UTC</summary>

@philosophicus , good question. I'm afraid that currently it won't be available in the client side retries. The heartbeat only available in the automatic server side initiated retries. I think it is good to expose the heartbeat result in ActivityFailedEvent/ActivityTimeoutEvent. cc @mfateev any comments?

</details>

<details>
<summary><strong>yiminc</strong> commented on 2021-09-03 20:16:48.000 UTC</summary>

I take my above statement back.
It turns out that the last heartbeat is already exposed to ActivityTimeoutEvent via FailureInfo. See https://github.com/temporalio/api/blob/760fa1278ece08707f05b8937cc0b9c4c926eb6c/temporal/api/history/v1/message.proto#L221 https://github.com/temporalio/api/blob/760fa1278ece08707f05b8937cc0b9c4c926eb6c/temporal/api/failure/v1/message.proto#L88 
https://github.com/temporalio/api/blob/760fa1278ece08707f05b8937cc0b9c4c926eb6c/temporal/api/failure/v1/message.proto#L45

For failure case, you could pass whatever data you want to return in the NewApplicationError() https://github.com/temporalio/sdk-go/blob/041f5c7e3e2647bf704cb2e4d6e2c8279eeae12a/internal/error.go#L266

So you should be able to do this already.

</details>

<details>
<summary><strong>philosophicus</strong> commented on 2021-09-04 17:00:30.000 UTC</summary>

@yiminc That's fantastic. Many thanks for your detailed reply.

</details>


---

### #1739: Return more accurate error codes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1739 |
| **State** | OPEN |
| **Author** | alexshtin (Alex Shtin) |
| **Created** | 2021-07-17 19:47:02.000 UTC (4y 5m ago) |
| **Updated** | 2023-03-03 20:21:29.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 1 |
| **Priority Score** | 5 |
| **Labels** | enhancement |
| **Assignees** | alexshtin |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
In many places better error code can be returned from different APIs. One of the example is `NOT_FOUND` error code is returned when requesting a termination and workflow is already completed. It makes it impossible to differentiate this from the case when workflow is actually doesn't exists.  `FAILED_PRECONDITION` seems to be a better error code according to the https://github.com/googleapis/googleapis/blob/master/google/rpc/code.proto.

**Describe the solution you'd like**
All public APIs need to be reviews holistically and proper adjustment should be made.

**Additional context**
https://community.temporal.io/t/terminating-a-completed-run-grpc-error-code-is-not-found-why-not-aborted-or-failed-precondition/2565

#### Comments (1)

<details>
<summary><strong>javiercanillas</strong> commented on 2023-02-28 15:10:49.000 UTC</summary>

Is this issue being prioritized? 

I did an utility method to determine if this exception is caused because execution was completed or not:
```java
  public static boolean isWorkflowCompleted(@NonNull final WorkflowNotFoundException exception) {
    return Optional.of(exception)
        .map(Exception::getCause)
        .filter(StatusRuntimeException.class::isInstance)
        .map(e -> (StatusRuntimeException) e)
        .map(StatusRuntimeException::getStatus)
        .filter(
            status ->
                Status.Code.NOT_FOUND.equals(status.getCode())
                    && "workflow execution already completed".equals(status.getDescription()))
        .isPresent();
  }
```


</details>


---

### #469: Limit depth of child workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/469 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-06-18 23:41:18.000 UTC (5y 6m ago) |
| **Updated** | 2023-03-03 20:22:59.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 1 |
| **Priority Score** | 5 |
| **Labels** | enhancement, API, P2 |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently there is no ability to limit number of levels of child workflow creation. It can potentially lead to situations when due to programming mistake workflow keeps creating children recursively forever.

**Describe the solution you'd like**
Allow configurable depth limit for child workflow creation.



#### Comments (1)

<details>
<summary><strong>sunshineo</strong> commented on 2020-06-19 00:29:32.000 UTC</summary>

Thank you! Limit depth is valuable but should we have circle detection or something about DAG?

Reactions: üëç 1

</details>


---

### #8581: [Scheduled Actions] - Update Schedule memo

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8581 |
| **State** | OPEN |
| **Author** | tlalfano (Tasha Alfano) |
| **Created** | 2025-10-31 14:41:53.000 UTC (2 months ago) |
| **Updated** | 2025-12-11 22:12:11.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 2 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
Users would like the ability to update the Schedules memo so that they can more effectively reconcile their Schedules. 

**Describe the solution you'd like**
Enable updating of the Schedule memo.

#### Comments (2)

<details>
<summary><strong>gow</strong> commented on 2025-11-13 23:42:41.000 UTC</summary>

We are looking into this and work on this in early 2026

</details>

<details>
<summary><strong>gow</strong> commented on 2025-12-11 22:12:11.000 UTC</summary>

This was closed by mistake (due to a faulty integration). Reopening it.

</details>


---

### #8202: CrashLoopBackOff on GKE cluster.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8202 |
| **State** | OPEN |
| **Author** | aunjaffery (AunOx) |
| **Created** | 2025-08-17 13:35:31.000 UTC (4 months ago) |
| **Updated** | 2025-12-11 22:15:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior


## Actual Behavior
I'm using temporal on my GKE cluster. with following values. it works perfectly but i have an issue. whenever there is an gke update and node gets replaced. temporal crashes and goes in CrashLoopBackOff and cannot recover. complains about no usable database connection. I have to manual redeploy with helm.
```
server:
  config:
    namespaces:
      create: true
      namespace:
        - name: default
          retention: 1d
    persistence:
      default:
        driver: "sql"
        sql:
          driver: "postgres12"
          host: my-postgresql.microservices.svc.cluster.local
          port: 5432
          database: temporal
          user: postgres
          password: "mypass"
          maxConns: 20
          maxIdleConns: 20
          maxConnLifetime: "1h"
      visibility:
        driver: "sql"

        sql:
          driver: "postgres12"
          host: my-postgresql.microservices.svc.cluster.local
          port: 5432
          database: temporal_visibility
          user: postgres
          password: "mypass"
          maxConns: 20
          maxIdleConns: 20
          maxConnLifetime: "1h"
cassandra:
  enabled: false

mysql:
  enabled: false

postgresql:
  enabled: true

prometheus:
  enabled: false

grafana:
  enabled: false

elasticsearch:
  enabled: false

schema:
  createDatabase:
    enabled: true
  setup:
    enabled: true
  update:
    enabled: true
```

## Specifications

  - Version: temporal-0.65.0
  - App-Version: 1.28.1
  - Platform: GKE


#### Comments (4)

<details>
<summary><strong>bergundy</strong> commented on 2025-10-02 23:19:01.000 UTC</summary>

Sorry for the delayed response. We are looking into it but no timeline for when we would be able to properly investigate.

</details>

<details>
<summary><strong>yycptt</strong> commented on 2025-11-13 21:43:49.000 UTC</summary>

We have some connection [refresh](https://github.com/temporalio/temporal/blob/d59c5d7eb38f749e05044b937555dfc06f8407e6/common/persistence/sql/sqlplugin/db_handle.go#L165) logic implemented for certain error types returned from driver. 

I am guessing in this case the error returned is not recognized by the logic [here](https://github.com/temporalio/temporal/blob/d59c5d7eb38f749e05044b937555dfc06f8407e6/common/persistence/sql/sqlplugin/postgresql/driver/interface.go#L26) so didn't trigger the refresh and requires a service deployment/restart to re-establish the connection.  

Are you able to access server logs and see what's error returned when the update happens? Also see if there's any warn/error message that starts with `sql handle: `

</details>

<details>
<summary><strong>aunjaffery</strong> commented on 2025-11-13 21:51:02.000 UTC</summary>

Because of this crash problem, I had GKE auto-upgrade tuned off. I saved the error in my notes.
```
[Fx] Error returned: received non-nil error from function "go.temporal.io/server/temporal".ServerOptionsProvider                                                                                                 ‚îÇ
‚îÇ     /home/runner/work/docker-builds/docker-builds/temporal/temporal/fx.go:182:                                                                                                                                   ‚îÇ
‚îÇ sql schema version compatibility check failed: unable to read DB schema version keyspace/database: temporal error: no usable database connection found                                                           ‚îÇ
‚îÇ [Fx] ERROR        Failed to initialize custom logger: could not build arguments for function "go.uber.org/fx".(*module).constructCustomLogger.func2                                                              ‚îÇ
‚îÇ     /home/runner/go/pkg/mod/go.uber.org/fx@v1.23.0/module.go:294:                                                                                                                                                ‚îÇ
‚îÇ failed to build fxevent.Logger:                                                                                                                                                                                  ‚îÇ
‚îÇ could not build arguments for function "go.temporal.io/server/temporal".init.func7                                                                                                                               ‚îÇ
‚îÇ     /home/runner/work/docker-builds/docker-builds/temporal/temporal/fx.go:1008:                                                                                                                                  ‚îÇ
‚îÇ failed to build log.Logger:                                                                                                                                                                                      ‚îÇ
‚îÇ received non-nil error from function "go.temporal.io/server/temporal".ServerOptionsProvider                                                                                                                      ‚îÇ
‚îÇ     /home/runner/work/docker-builds/docker-builds/temporal/temporal/fx.go:182:                                                                                                                                   ‚îÇ
‚îÇ sql schema version compatibility check failed: unable to read DB schema version keyspace/database: temporal error: no usable database connection found                                                           ‚îÇ
‚îÇ Unable to create server. Error: could not build arguments for function "go.uber.org/fx".(*module).constructCustomLogger.func2 (/home/runner/go/pkg/mod/go.uber.org/fx@v1.23.0/module.go:294): failed to build fx ‚îÇ
‚îÇ stream closed EOF for microservices/temporaltest-worker-656b4fbbb5-kl6tx (temporal-worker)
```

</details>

<details>
<summary><strong>gow</strong> commented on 2025-12-11 22:15:20.000 UTC</summary>

This was closed by mistake (due to a faulty integration). Reopening it.

</details>


---

### #8109: Support for Naming Timers in Temporal Workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8109 |
| **State** | OPEN |
| **Author** | SlawomirA |
| **Created** | 2025-07-29 10:13:05.000 UTC (5 months ago) |
| **Updated** | 2025-07-29 10:13:05.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, timers created in Temporal workflows are assigned system-generated IDs (UUIDs) which are not human-readable. This makes debugging, monitoring, and managing workflows harder, especially when multiple timers are involved. For example, in the Temporal Web UI, timers appear only with generic IDs, making it difficult to identify their purpose at a glance.

**Describe the solution you'd like**
It would be very helpful to have an option to assign custom, descriptive names or labels to timers when creating them via the SDK (e.g., Workflow.newTimer(Duration, String timerName)). These names would then be visible in the Temporal Web UI and logs, improving workflow observability and troubleshooting.

**Describe alternatives you've considered**
-     Using custom search attributes or workflow metadata to tag timers indirectly.
-     Adding logging messages with timer names during workflow execution.
-     Naming workflows or task queues meaningfully to partially compensate.

However, these alternatives do not solve the core issue of timers lacking explicit, human-readable names in the system.

**Additional context**
<img width="790" height="126" alt="Image" src="https://github.com/user-attachments/assets/2bc82ece-2e33-4d6c-bbab-e68be2495f59" />



---

### #7947: CLI/WebUI option to list out dynamic configs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7947 |
| **State** | OPEN |
| **Author** | hokadiri (Hussein Kadiri) |
| **Created** | 2025-06-23 04:40:49.000 UTC (6 months ago) |
| **Updated** | 2025-06-23 04:40:49.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Getting a list of dynamic configs could be simpler, the current way as described in the [doc](https://docs.temporal.io/references/dynamic-configuration) is to check the code. But one needs to
- identify version of the code they're running
-  identify the corresponding git sha of that version 
-  Then get the https://github.com/temporalio/temporal/blob/<SOME GIT SHA>/common/dynamicconfig/constants.go .

> For the complete list of dynamic configuration keys, see https://github.com/temporalio/temporal/blob/main/common/dynamicconfig/constants.go. Ensure that you check server release notes for any changes to these keys and values.

This is a cumbersome process.

**Describe the solution you'd like**
A CLI/Web UI option that prints out all dynamic configs KV pairs would make things much easier.

**Describe alternatives you've considered**
Alternatives is doing what the doc recommends. i'e cross referencing against release notes.




---

### #7200: temporal cli works but hit deadline-exceeded error for sample code

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7200 |
| **State** | OPEN |
| **Author** | binchenX (Bin Chen) |
| **Created** | 2025-01-31 04:43:29.000 UTC (11 months ago) |
| **Updated** | 2025-02-20 23:55:54.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 2 |
| **Priority Score** | 4 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

## Expected Behavior

offical [golang sample code](https://github.com/temporalio/samples-go/tree/main/helloworld) can run successfully.

Notes: already read https://docs.temporal.io/troubleshooting/deadline-exceeded-error. Can't make sense where to look since the cli works, was expecting the sample works too.

## Actual Behavior

offical golang sample code failed with context deadline exceeded.

```
‚ûú  helloworld git:(main) go run worker/main.go
2025/01/31 15:08:44 INFO  No logger configured for temporal client. Created default one.
2025/01/31 15:08:49 Unable to create client failed reaching server: context deadline exceeded
exit status 1
```

## Steps to Reproduce the Problem

  
1. Deployed to kubernetes following [offical helm chart](https://github.com/temporalio/helm-charts). 

2. Port forwarding to local, ensure cluster is health and temporal cli works

```bash
‚ûú  kubectl port-forward services/temporal-frontend-headless 7233:7233               
Forwarding from 127.0.0.1:7233 -> 7233
Forwarding from [::1]:7233 -> 7233
Handling connection for 7233
Handling connection for 7233
Handling connection for 7233
Handling connection for 7233
Handling connection for 7233
Handling connection for 7233


‚ûú  temporal operator cluster health --address 127.0.0.1:7233
SERVING

‚ûú  temporal operator namespace create --namespace default

‚ûú  temporal operator namespace list
  NamespaceInfo.Name                    default
  NamespaceInfo.Id                      3524a90c-ad2a-416e-aa0e-647520983665
  NamespaceInfo.Description
  NamespaceInfo.OwnerEmail
  NamespaceInfo.State                   Registered
  NamespaceInfo.Data                    map[]
  Config.WorkflowExecutionRetentionTtl  72h0m0s
  ReplicationConfig.ActiveClusterName   active
  ReplicationConfig.Clusters            [{"clusterName":"active"}]
  Config.HistoryArchivalState           Disabled
  Config.VisibilityArchivalState        Disabled
  IsGlobalNamespace                     false
  FailoverVersion                       0
  FailoverHistory                       []
  Config.HistoryArchivalUri
  Config.VisibilityArchivalUri

  NamespaceInfo.Name                    temporal-system
  NamespaceInfo.Id                      32049b68-7872-4094-8e63-d0dd59896a83
  NamespaceInfo.Description             Temporal internal system namespace
  NamespaceInfo.OwnerEmail              temporal-core@temporal.io
  NamespaceInfo.State                   Registered
  NamespaceInfo.Data                    map[]
  Config.WorkflowExecutionRetentionTtl  168h0m0s
  ReplicationConfig.ActiveClusterName   active
  ReplicationConfig.Clusters            [{"clusterName":"active"}]
  Config.HistoryArchivalState           Disabled
  Config.VisibilityArchivalState        Disabled
  IsGlobalNamespace                     false
  FailoverVersion                       0
  FailoverHistory                       []
  Config.HistoryArchivalUri
  Config.VisibilityArchivalUri
```

3. Run the [sample hello world code](https://github.com/temporalio/samples-go/tree/main/helloworld) and failed.

```bash
‚ûú  helloworld git:(main) go run worker/main.go
2025/01/31 15:08:44 INFO  No logger configured for temporal client. Created default one.
2025/01/31 15:08:49 Unable to create client failed reaching server: context deadline exceeded
exit status 1
```

## Specifications

```
  ‚ûú  ‚úó temporal -v
temporal version 1.2.0 (Server 1.26.2, UI 2.34.0)

  ‚ûú  temporal operator cluster describe
  ClusterName  PersistenceStore  VisibilityStore
  active       cassandra         elasticsearch

  - Platform: Kubernetes/EKS 
```

## Other

- Was in running in cooperation so involves firewall and proxy. but again since the temporal cli works, was expecting the client program be no different. Would love to know extra config for the client such timeout and proxy if that exists and deems to be issue.  

#### Comments (2)

<details>
<summary><strong>giuliohome</strong> commented on 2025-02-11 08:52:49.000 UTC</summary>

We‚Äôre encountering a similar issue. In our case [^1][^2], it‚Äôs not the sample code, but we get the same `"failed reaching server: context deadline exceeded"` in very similar Temporal client code.

```go
c, err := client.Dial(client.Options{
        HostPort:  os.Getenv("TEMPORAL_HOSTPORT"),
        Namespace: os.Getenv("TEMPORAL_NAMESPACE"),
    })
    if err != nil {
        log.Error("Unable to create client", "error", err)
        panic(err)
    }
```

> {"time":"2025-02-10T17:25:39.557547051Z","level":"ERROR","msg":"Unable to create client","myapp":{"activity":"results","error":"failed reaching server: context deadline exceeded"}}

[^1]: Platform: Kubernetes/AKS.
[^2]: Our panic (at least the most recent one observed) appears to be caused by temporary resource throttling and/or a concurrent Kubernetes deployment.

</details>

<details>
<summary><strong>bergundy</strong> commented on 2025-02-20 23:55:53.000 UTC</summary>

You should try to run the sample with grpc debugging enabled, there may be some useful information in there:

`GRPC_GO_LOG_VERBOSITY_LEVEL=99` `GRPC_GO_LOG_SEVERITY_LEVEL=info`.

</details>


---

### #6977: Addressing a lot of security vulnerabilities in the Temporalio/admin-tools release 1.25.2-tctl-1.18.1-cli-1.1.2

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6977 |
| **State** | OPEN |
| **Author** | LauVietVan (LauVietVan) |
| **Created** | 2024-12-12 09:34:54.000 UTC (1 years ago) |
| **Updated** | 2024-12-12 09:34:54.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

## Expected Behavior
No more CVEs found.
## Actual Behavior
There are a lot of CVEs found from the latest Temporal image:
`temporalio/admin-tools:1.25.2-tctl-1.18.1-cli-1.1.2`
## Steps to Reproduce the Problem
Pull the latest image `temporalio/admin-tools:1.25.2-tctl-1.18.1-cli-1.1.2` from Dockerhub
Scan the image with any vulnerability scanner
```shell
Scan results for: image temporalio/admin-tools:1.25.2-tctl-1.18.1-cli-1.1.2 sha256:70c966b9022bd1574036d32179e07a777258d1a16e1387beaf923835d488023b
Vulnerabilities
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
|       CVE        | SEVERITY | CVSS |           PACKAGE            |                VERSION                |             STATUS              | PUBLISHED  | DISCOVERED |                    DESCRIPTION                     |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-6345    | high     | 8.80 | setuptools                   | 65.5.0                                | fixed in 70.0.0                 | > 5 months | < 1 hour   | A vulnerability in the package_index module of     |
|                  |          |      |                              |                                       | > 4 months ago                  |            |            | pypa/setuptools versions up to 69.1.1 allows for   |
|                  |          |      |                              |                                       |                                 |            |            | remote code execution via its download functions.  |
|                  |          |      |                              |                                       |                                 |            |            | Thes...                                            |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| PRISMA-2022-0168 | high     | 7.80 | pip                          | 24.0                                  | open                            | > 2 years  | < 1 hour   | An issue was discovered in pip (all versions)      |
|                  |          |      |                              |                                       |                                 |            |            | because it installs the version with the highest   |
|                  |          |      |                              |                                       |                                 |            |            | version number, even if the user had intended to   |
|                  |          |      |                              |                                       |                                 |            |            | obtain...                                          |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-7348    | high     | 7.50 | postgresql16                 | 16.3-r0                               | fixed in 16.4-r0                | > 4 months | < 1 hour   | Time-of-check Time-of-use (TOCTOU) race condition  |
|                  |          |      |                              |                                       | 41 days ago                     |            |            | in pg_dump in PostgreSQL allows an object creator  |
|                  |          |      |                              |                                       |                                 |            |            | to execute arbitrary SQL functions as the user     |
|                  |          |      |                              |                                       |                                 |            |            | run...                                             |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-9681    | medium   | 6.50 | curl                         | 8.9.1-r2                              | fixed in 8.11.0-r0              | 36 days    | < 1 hour   | When curl is asked to use HSTS, the expiry time    |
|                  |          |      |                              |                                       | 35 days ago                     |            |            | for a subdomain might overwrite a parent domain\'s |
|                  |          |      |                              |                                       |                                 |            |            | cache entry, making it end sooner or later than    |
|                  |          |      |                              |                                       |                                 |            |            | oth...                                             |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2022-40897   | medium   | 5.90 | setuptools                   | 65.5.0                                | fixed in 65.5.1                 | > 1 years  | < 1 hour   | Python Packaging Authority (PyPA) setuptools       |
|                  |          |      |                              |                                       | > 1 years ago                   |            |            | before 65.5.1 allows remote attackers to cause a   |
|                  |          |      |                              |                                       |                                 |            |            | denial of service via HTML in a crafted package or |
|                  |          |      |                              |                                       |                                 |            |            | custo...                                           |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-2689    | medium   | 0.00 | go.temporal.io/server        | v1.18.1-0.20230217005328-b313b7f58641 | fixed in 1.20.5, 1.21.6, 1.22.7 | > 6 months | < 1 hour   | Denial of Service in Temporal Server prior to      |
|                  |          |      |                              |                                       | > 6 months ago                  |            |            | version 1.20.5, 1.21.6, and 1.22.7 allows an       |
|                  |          |      |                              |                                       |                                 |            |            | authenticated user who has permissions to interact |
|                  |          |      |                              |                                       |                                 |            |            | with wor...                                        |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-3485    | low      | 3.60 | go.temporal.io/server        | v1.18.1-0.20230217005328-b313b7f58641 | fixed in 1.20.0                 | > 3 months | < 1 hour   | Insecure defaults in open-source Temporal Server   |
|                  |          |      |                              |                                       | > 1 years ago                   |            |            | before version 1.20 on all platforms allows an     |
|                  |          |      |                              |                                       |                                 |            |            | attacker to craft a task token with access to a    |
|                  |          |      |                              |                                       |                                 |            |            | namesp...                                          |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-9287    | low      | 0.00 | python3                      | 3.12.6-r0                             | fixed in 3.12.8-r0              | 50 days    | < 1 hour   | A vulnerability has been found in the CPython      |
|                  |          |      |                              |                                       | 5 days ago                      |            |            | `venv` module and CLI where path names provided    |
|                  |          |      |                              |                                       |                                 |            |            | when creating a virtual environment were not       |
|                  |          |      |                              |                                       |                                 |            |            | quoted prop...                                     |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-9143    | low      | 0.00 | openssl                      | 3.3.2-r0                              | fixed in 3.3.2-r1               | 56 days    | < 1 hour   | Issue summary: Use of the low-level GF(2^m)        |
|                  |          |      |                              |                                       | 52 days ago                     |            |            | elliptic curve APIs with untrusted explicit values |
|                  |          |      |                              |                                       |                                 |            |            | for the field polynomial can lead to out-of-bounds |
|                  |          |      |                              |                                       |                                 |            |            | memo...                                            |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-8096    | low      | 0.00 | curl                         | 8.9.1-r2                              | fixed in 8.10.0-r0              | > 3 months | < 1 hour   | When curl is told to use the Certificate Status    |
|                  |          |      |                              |                                       | > 3 months ago                  |            |            | Request TLS extension, often referred to as OCSP   |
|                  |          |      |                              |                                       |                                 |            |            | stapling, to verify that the server certificate is |
|                  |          |      |                              |                                       |                                 |            |            | va...                                              |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-51744   | low      | 0.00 | github.com/golang-jwt/jwt/v4 | v4.5.0                                | fixed in 4.5.1                  | 37 days    | < 1 hour   | golang-jwt is a Go implementation of JSON Web      |
|                  |          |      |                              |                                       | 37 days ago                     |            |            | Tokens. Unclear documentation of the error         |
|                  |          |      |                              |                                       |                                 |            |            | behavior in `ParseWithClaims` can lead to          |
|                  |          |      |                              |                                       |                                 |            |            | situation where use...                             |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-50602   | low      | 0.00 | expat                        | 2.6.3-r0                              | fixed in 2.6.4-r0               | 46 days    | < 1 hour   | An issue was discovered in libexpat before 2.6.4.  |
|                  |          |      |                              |                                       | 33 days ago                     |            |            | There is a crash within the XML_ResumeParser       |
|                  |          |      |                              |                                       |                                 |            |            | function because XML_StopParser can stop/suspend   |
|                  |          |      |                              |                                       |                                 |            |            | an uns...                                          |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-12254   | low      | 0.00 | python3                      | 3.12.6-r0                             | fixed in 3.12.8-r1              | 5 days     | < 1 hour   | Starting in Python 3.12.0, the                     |
|                  |          |      |                              |                                       | 5 days ago                      |            |            | asyncio._SelectorSocketTransport.writelines()      |
|                  |          |      |                              |                                       |                                 |            |            | method would not "pause" writing and signal to the |
|                  |          |      |                              |                                       |                                 |            |            | Protocol to drain  th...                           |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-10979   | low      | 0.00 | postgresql16                 | 16.3-r0                               | fixed in 16.5-r0                | 27 days    | < 1 hour   | Incorrect control of environment variables in      |
|                  |          |      |                              |                                       | 26 days ago                     |            |            | PostgreSQL PL/Perl allows an unprivileged database |
|                  |          |      |                              |                                       |                                 |            |            | user to change sensitive process environment       |
|                  |          |      |                              |                                       |                                 |            |            | variable...                                        |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-10978   | low      | 0.00 | postgresql16                 | 16.3-r0                               | fixed in 16.5-r0                | 27 days    | < 1 hour   | Incorrect privilege assignment in PostgreSQL       |
|                  |          |      |                              |                                       | 26 days ago                     |            |            | allows a less-privileged application user to view  |
|                  |          |      |                              |                                       |                                 |            |            | or change different rows from those intended.  An  |
|                  |          |      |                              |                                       |                                 |            |            | attac...                                           |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-10977   | low      | 0.00 | postgresql16                 | 16.3-r0                               | fixed in 16.5-r0                | 27 days    | < 1 hour   | Client use of server error message in PostgreSQL   |
|                  |          |      |                              |                                       | 26 days ago                     |            |            | allows a server not trusted under current SSL or   |
|                  |          |      |                              |                                       |                                 |            |            | GSS settings to furnish arbitrary non-NUL bytes to |
|                  |          |      |                              |                                       |                                 |            |            | t...                                               |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-10976   | low      | 0.00 | postgresql16                 | 16.3-r0                               | fixed in 16.5-r0                | 27 days    | < 1 hour   | Incomplete tracking in PostgreSQL of tables        |
|                  |          |      |                              |                                       | 26 days ago                     |            |            | with row security allows a reused query to view    |
|                  |          |      |                              |                                       |                                 |            |            | or change different rows from those intended.      |
|                  |          |      |                              |                                       |                                 |            |            | CVE-2023-24...                                     |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+

Vulnerabilities found for image temporalio/admin-tools:1.25.2-tctl-1.18.1-cli-1.1.2: total - 17, critical - 0, high - 3, medium - 3, low - 11
Vulnerability threshold check results: PASS

Compliance Issues
+----------+------------------------------+
| SEVERITY |         DESCRIPTION          |
+----------+------------------------------+
| high     | Private keys stored in image |
+----------+------------------------------+

Compliance found for image temporalio/admin-tools:1.25.2-tctl-1.18.1-cli-1.1.2: total - 1, critical - 0, high - 1, medium - 0, low - 0
Compliance threshold check results: PASS
```
## Specifications

  - Version: `temporalio/admin-tools:1.25.2-tctl-1.18.1-cli-1.1.2`
  - Platform:



---

### #6976: Addressing a lot of security vulnerabilities in the Temporalio/server release 1.25.2.0

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6976 |
| **State** | OPEN |
| **Author** | LauVietVan (LauVietVan) |
| **Created** | 2024-12-12 09:09:59.000 UTC (1 years ago) |
| **Updated** | 2024-12-12 09:09:59.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

## Expected Behavior
No more CVEs found.
## Actual Behavior
There are a lot of CVEs found from the latest Temporal image:
`temporalio/server:1.25.2.0`
## Steps to Reproduce the Problem

1. Pull the latest image `temporalio/server:1.25.2.0` from Dockerhub
2. Scan the image with any vulnerability scanner
``` shell
Scan results for: image temporalio/server:1.25.2.0 sha256:efb68cf7ec1e22ccb1244d4e3f25561c8c71c56aa201d70387adb7e747cb85d3
Vulnerabilities
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
|       CVE        | SEVERITY | CVSS |           PACKAGE            |                VERSION                |             STATUS              |  PUBLISHED  | DISCOVERED |                          DESCRIPTION                          |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-6197    | high     | 7.50 | curl                         | 8.5.0-r0                              | fixed in 8.9.0-r0               | > 4 months  | < 1 hour   | libcurl\'s ASN1 parser has this utf8asn1str()                 |
|                  |          |      |                              |                                       | > 4 months ago                  |             |            | function used for parsing an ASN.1 UTF-8 string.              |
|                  |          |      |                              |                                       |                                 |             |            | Itcan detect an invalid field and return error.               |
|                  |          |      |                              |                                       |                                 |             |            | Unfortu...                                                    |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-9681    | medium   | 6.50 | curl                         | 8.5.0-r0                              |                                 | 35 days     | < 1 hour   | When curl is asked to use HSTS, the expiry time               |
|                  |          |      |                              |                                       |                                 |             |            | for a subdomain might overwrite a parent domain\'s            |
|                  |          |      |                              |                                       |                                 |             |            | cache entry, making it end sooner or later than               |
|                  |          |      |                              |                                       |                                 |             |            | oth...                                                        |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| PRISMA-2023-0056 | medium   | 6.20 | github.com/sirupsen/logrus   | v1.4.2                                | fixed in v1.9.3                 | > 1 years   | < 1 hour   | The github.com/sirupsen/logrus module of all                  |
|                  |          |      |                              |                                       | > 1 years ago                   |             |            | versions is vulnerable to denial of service.                  |
|                  |          |      |                              |                                       |                                 |             |            | Logging more than 64kb of data in a single entry              |
|                  |          |      |                              |                                       |                                 |             |            | without new...                                                |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2023-6992    | medium   | 5.50 | zlib                         | 1.3.1-r0                              |                                 | > 11 months | < 1 hour   | Cloudflare version of zlib library was found                  |
|                  |          |      |                              |                                       |                                 |             |            | to be vulnerable to memory corruption issues                  |
|                  |          |      |                              |                                       |                                 |             |            | affecting the deflation algorithm implementation              |
|                  |          |      |                              |                                       |                                 |             |            | (deflate.c)...                                                |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2023-42366   | medium   | 5.50 | busybox                      | 1.36.1-r15                            | fixed in 1.36.1-r16             | > 1 years   | < 1 hour   | A heap-buffer-overflow was discovered in BusyBox              |
|                  |          |      |                              |                                       | > 6 months ago                  |             |            | v.1.36.1 in the next_token function at awk.c:1159.            |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2023-42365   | medium   | 5.50 | busybox                      | 1.36.1-r15                            | fixed in 1.36.1-r19             | > 1 years   | < 1 hour   | A use-after-free vulnerability was discovered in              |
|                  |          |      |                              |                                       | > 6 months ago                  |             |            | BusyBox v.1.36.1 via a crafted awk pattern in the             |
|                  |          |      |                              |                                       |                                 |             |            | awk.c copyvar function.                                       |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2023-42364   | medium   | 5.50 | busybox                      | 1.36.1-r15                            | fixed in 1.36.1-r19             | > 1 years   | < 1 hour   | A use-after-free vulnerability in BusyBox v.1.36.1            |
|                  |          |      |                              |                                       | > 6 months ago                  |             |            | allows attackers to cause a denial of service                 |
|                  |          |      |                              |                                       |                                 |             |            | via a crafted awk pattern in the awk.c evaluate               |
|                  |          |      |                              |                                       |                                 |             |            | funct...                                                      |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2023-42363   | medium   | 5.50 | busybox                      | 1.36.1-r15                            | fixed in 1.36.1-r17             | > 1 years   | < 1 hour   | A use-after-free vulnerability was discovered                 |
|                  |          |      |                              |                                       | > 6 months ago                  |             |            | in xasprintf function in xfuncs_printf.c:344 in               |
|                  |          |      |                              |                                       |                                 |             |            | BusyBox v.1.36.1.                                             |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-0853    | medium   | 5.30 | curl                         | 8.5.0-r0                              | fixed in 8.6.0-r0               | > 10 months | < 1 hour   | curl inadvertently kept the SSL session ID for                |
|                  |          |      |                              |                                       | > 4 months ago                  |             |            | connections in its cache even when the verify                 |
|                  |          |      |                              |                                       |                                 |             |            | status (*OCSP stapling*) test failed. A subsequent            |
|                  |          |      |                              |                                       |                                 |             |            | transf...                                                     |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-6874    | medium   | 4.30 | curl                         | 8.5.0-r0                              | fixed in 8.9.0-r0               | > 4 months  | < 1 hour   | libcurl\'s URL API function                                   |
|                  |          |      |                              |                                       | > 4 months ago                  |             |            | [curl_url_get()](https://curl.se/libcurl/c/curl_url_get.html) |
|                  |          |      |                              |                                       |                                 |             |            | offers punycode conversions, to and from IDN. Asking to       |
|                  |          |      |                              |                                       |                                 |             |            | conv...                                                       |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-2689    | medium   | 0.00 | go.temporal.io/server        | v1.18.1-0.20230217005328-b313b7f58641 | fixed in 1.20.5, 1.21.6, 1.22.7 | > 6 months  | < 1 hour   | Denial of Service in Temporal Server prior to                 |
|                  |          |      |                              |                                       | > 6 months ago                  |             |            | version 1.20.5, 1.21.6, and 1.22.7 allows an                  |
|                  |          |      |                              |                                       |                                 |             |            | authenticated user who has permissions to interact            |
|                  |          |      |                              |                                       |                                 |             |            | with wor...                                                   |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2023-3485    | low      | 3.60 | go.temporal.io/server        | v1.18.1-0.20230217005328-b313b7f58641 | fixed in 1.20.0                 | > 3 months  | < 1 hour   | Insecure defaults in open-source Temporal Server              |
|                  |          |      |                              |                                       | > 1 years ago                   |             |            | before version 1.20 on all platforms allows an                |
|                  |          |      |                              |                                       |                                 |             |            | attacker to craft a task token with access to a               |
|                  |          |      |                              |                                       |                                 |             |            | namesp...                                                     |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-9143    | low      | 0.00 | openssl                      | 3.1.4-r5                              | fixed in 3.1.7-r1               | 56 days     | < 1 hour   | Issue summary: Use of the low-level GF(2^m)                   |
|                  |          |      |                              |                                       | 52 days ago                     |             |            | elliptic curve APIs with untrusted explicit values            |
|                  |          |      |                              |                                       |                                 |             |            | for the field polynomial can lead to out-of-bounds            |
|                  |          |      |                              |                                       |                                 |             |            | memo...                                                       |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-6119    | low      | 0.00 | openssl                      | 3.1.4-r5                              | fixed in 3.1.7-r0               | > 3 months  | < 1 hour   | Issue summary: Applications performing certificate            |
|                  |          |      |                              |                                       | > 3 months ago                  |             |            | name checks (e.g., TLS clients checking server                |
|                  |          |      |                              |                                       |                                 |             |            | certificates) may attempt to read an invalid                  |
|                  |          |      |                              |                                       |                                 |             |            | memory ...                                                    |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-5535    | low      | 0.00 | openssl                      | 3.1.4-r5                              | fixed in 3.1.6-r0               | > 5 months  | < 1 hour   | Issue summary: Calling the OpenSSL API function               |
|                  |          |      |                              |                                       | > 5 months ago                  |             |            | SSL_select_next_proto with an empty supported                 |
|                  |          |      |                              |                                       |                                 |             |            | client protocols buffer may cause a crash or                  |
|                  |          |      |                              |                                       |                                 |             |            | memory cont...                                                |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-51744   | low      | 0.00 | github.com/golang-jwt/jwt/v4 | v4.5.0                                | fixed in 4.5.1                  | 37 days     | < 1 hour   | golang-jwt is a Go implementation of JSON Web                 |
|                  |          |      |                              |                                       | 37 days ago                     |             |            | Tokens. Unclear documentation of the error                    |
|                  |          |      |                              |                                       |                                 |             |            | behavior in `ParseWithClaims` can lead to                     |
|                  |          |      |                              |                                       |                                 |             |            | situation where use...                                        |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-4741    | low      | 0.00 | openssl                      | 3.1.4-r5                              | fixed in 3.1.6-r0               | 28 days     | < 1 hour   | Issue summary: Calling the OpenSSL API function               |
|                  |          |      |                              |                                       | > 5 months ago                  |             |            | SSL_free_buffers may cause memory to be accessed              |
|                  |          |      |                              |                                       |                                 |             |            | that was previously freed in some situations                  |
|                  |          |      |                              |                                       |                                 |             |            | Impact ...                                                    |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-4603    | low      | 0.00 | openssl                      | 3.1.4-r5                              | fixed in 3.1.5-r0               | > 6 months  | < 1 hour   | Issue summary: Checking excessively long DSA                  |
|                  |          |      |                              |                                       | > 6 months ago                  |             |            | keys or parameters may be very slow.  Impact                  |
|                  |          |      |                              |                                       |                                 |             |            | summary: Applications that use the functions                  |
|                  |          |      |                              |                                       |                                 |             |            | EVP_PKEY_param_...                                            |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-25629   | low      | 0.00 | c-ares                       | 1.24.0-r1                             | fixed in 1.27.0-r0              | > 9 months  | < 1 hour   | c-ares is a C library for asynchronous DNS                    |
|                  |          |      |                              |                                       | > 8 months ago                  |             |            | requests. `ares__read_line()` is used to                      |
|                  |          |      |                              |                                       |                                 |             |            | parse local configuration files such as                       |
|                  |          |      |                              |                                       |                                 |             |            | `/etc/resolv.conf`, `/etc/...                                 |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-2511    | low      | 0.00 | openssl                      | 3.1.4-r5                              | fixed in 3.1.4-r6               | > 8 months  | < 1 hour   | Issue summary: Some non-default TLS server                    |
|                  |          |      |                              |                                       | > 8 months ago                  |             |            | configurations can cause unbounded memory growth              |
|                  |          |      |                              |                                       |                                 |             |            | when processing TLSv1.3 sessions  Impact summary:             |
|                  |          |      |                              |                                       |                                 |             |            | An attac...                                                   |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-2466    | low      | 0.00 | curl                         | 8.5.0-r0                              | fixed in 8.7.1-r0               | > 8 months  | < 1 hour   | libcurl did not check the server certificate of               |
|                  |          |      |                              |                                       | > 4 months ago                  |             |            | TLS connections done to a host specified as an IP             |
|                  |          |      |                              |                                       |                                 |             |            | address, when built to use mbedTLS.  libcurl would            |
|                  |          |      |                              |                                       |                                 |             |            | w...                                                          |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-2398    | low      | 0.00 | curl                         | 8.5.0-r0                              | fixed in 8.7.1-r0               | > 8 months  | < 1 hour   | When an application tells libcurl it wants to                 |
|                  |          |      |                              |                                       | > 4 months ago                  |             |            | allow HTTP/2 server push, and the amount of                   |
|                  |          |      |                              |                                       |                                 |             |            | received headers for the push surpasses the                   |
|                  |          |      |                              |                                       |                                 |             |            | maximum allowed ...                                           |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-2379    | low      | 0.00 | curl                         | 8.5.0-r0                              | fixed in 8.7.1-r0               | > 8 months  | < 1 hour   | libcurl skips the certificate verification for                |
|                  |          |      |                              |                                       | > 4 months ago                  |             |            | a QUIC connection under certain conditions,                   |
|                  |          |      |                              |                                       |                                 |             |            | when built to use wolfSSL. If told to use an                  |
|                  |          |      |                              |                                       |                                 |             |            | unknown/bad ci...                                             |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+
| CVE-2024-2004    | low      | 0.00 | curl                         | 8.5.0-r0                              | fixed in 8.7.1-r0               | > 8 months  | < 1 hour   | When a protocol selection parameter option                    |
|                  |          |      |                              |                                       | > 4 months ago                  |             |            | disables all protocols without adding any then                |
|                  |          |      |                              |                                       |                                 |             |            | the default set of protocols would remain in the              |
|                  |          |      |                              |                                       |                                 |             |            | allowed set...                                                |
+------------------+----------+------+------------------------------+---------------------------------------+---------------------------------+-------------+------------+---------------------------------------------------------------+

Vulnerabilities found for image temporalio/server:1.25.2.0: total - 24, critical - 0, high - 1, medium - 10, low - 13
Vulnerability threshold check results: PASS
```

## Specifications

  - Version: `temporalio/server:1.25.2.0`
  - Platform:



---

### #6845: Support full, structured failures for queries

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6845 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-11-20 14:13:05.000 UTC (1y 1m ago) |
| **Updated** | 2025-10-06 16:12:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

Today we only support a single string for query failure: https://github.com/temporalio/api/blob/c5bab058fa7ffb52c7137cdbaabd37ac0c41537f/temporal/api/query/v1/message.proto#L56. This means users can't get stack traces or set any of the details they expect to be able to set when using errors in other parts of the platform (workflows, activities, updates, etc). Also, users expect to be able to "encode" failures (i.e. move the message to a payload so it is encryptable, see [here](https://github.com/temporalio/api/blob/c5bab058fa7ffb52c7137cdbaabd37ac0c41537f/temporal/api/failure/v1/message.proto#L107-L122)), but queries not supporting our proper failure structures does not allow them to do this for queries.

Support a full `temporal.api.failure.v1.Failure` on query result (can be mutually exclusive with the string today)


#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2024-11-21 13:45:15.000 UTC</summary>

Proposed API:

* Add `temporal.api.failure.v1.Failure failure` to `temporal.api.query.v1.WorkflowQueryResult`. This will be set by SDKs _in addition_ to the existing `error_message`.
* Add `temporal.api.failure.v1.Failure failure` to `temporal.api.errordetails.v1.QueryFailedFailure`. This will be populated by server with the `failure` from above

</details>

<details>
<summary><strong>ashish111333</strong> commented on 2025-10-04 16:35:39.000 UTC</summary>

hey chad can I take this ?

</details>

<details>
<summary><strong>cretz</strong> commented on 2025-10-06 14:12:22.000 UTC</summary>

This may have already been done in https://github.com/temporalio/api/pull/503 and #6947 by @bergundy. @bergundy - can you confirm we can close this issue?

</details>

<details>
<summary><strong>ashish111333</strong> commented on 2025-10-06 16:12:53.000 UTC</summary>

@cretz  Np I was looking for something interesting to work on , if you can point in the right direction (anywhere where the team needs help )   I would be happy to work on it.

</details>


---

### #6611: Expose anonymous HTTP/HTTPS health endpoint on Frontend

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6611 |
| **State** | OPEN |
| **Author** | alexku7 (alexk) |
| **Created** | 2024-10-06 20:36:19.000 UTC (1y 2m ago) |
| **Updated** | 2024-10-06 20:36:19.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**

When the frontend which is installed on kubernetes is exposed to Internet via ingress load balancer, the load balancer requires a health endpoint for the health-checks.  When the client authentication is enabled by mTLS the health check fails and the load balancer marks the frontend pods as failed. Therefore the setup is not functional.
Most cloud load balancers today don"t support the client certificate auth. For example we use GKE.



**Describe the solution you'd like**
Enable a health endpoint without auth so the load balancer can use it for health checks, 





---

### #6338: error in prometheus reporter: AccessHistoryNew counter

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6338 |
| **State** | OPEN |
| **Author** | Tadimsky (Jonno Schmidt) |
| **Created** | 2024-07-25 15:40:52.000 UTC (1y 5m ago) |
| **Updated** | 2025-07-29 05:32:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

No warning logs output by Temporal.

## Actual Behavior

Temporal is logging this warning:
```
time=2024-07-25T15:31:08.764 level=WARN msg="error in prometheus reporter" error="a previously registered descriptor with the same fully-qualified name as Desc{fqName: \"AccessHistoryNew\", help: \"AccessHistoryNew counter\", constLabels: {}, variableLabels: {service_name,operation}} has different label names or a different help string"
```

## Steps to Reproduce the Problem

  1. Run `temporal server start-dev` 
  1. Run workflows on server
  1. View logs

## Specifications

  - Version: `temporalio/admin-tools:1.24.2-tctl-1.18.1-cli-0.13.2`
  - Platform: macOS 14.5

#### Comments (4)

<details>
<summary><strong>Tadimsky</strong> commented on 2024-07-25 15:42:23.000 UTC</summary>

I couldn't find in the codebase where this metric was coming from - but it looks like it's maybe differing in what labels it's being emitted with.

</details>

<details>
<summary><strong>ltlombardi</strong> commented on 2024-10-18 15:50:21.000 UTC</summary>

Same happening to me while running this tutorial https://learn.temporal.io/getting_started/dotnet/first_program_in_dotnet/

Windows 11, temporal version 0.13.2 (server 1.24.1) (ui 2.28.0)


</details>

<details>
<summary><strong>webchick</strong> commented on 2025-07-29 05:16:13.000 UTC</summary>

Seeing the same when attempting to run the hello world example from https://github.com/temporal-community/openai-agents-demos:

```
time=2025-07-28T21:55:46.436 level=WARN msg="error in prometheus reporter" error="a previously registered descriptor with the same fully-qualified name as Desc{fqName: \"task_schedule_to_start_latency\", help: \"task_schedule_to_start_latency histogram\", constLabels: {}, variableLabels: {service_name,operation,taskqueue,namespace}} has different label names or a different help string"
```

(Slightly different label compared to OP; mine is "task_schedule_to_start_latency" vs. "AccessHistoryNew")

- Version: temporal version 1.1.2 (Server 1.25.2, UI 2.32.0)
- MacOS: Sonoma Version 14.7.5 (23H527)

</details>

<details>
<summary><strong>webchick</strong> commented on 2025-07-29 05:25:35.000 UTC</summary>

In my case, grepping the project for this line leads me to https://github.com/temporalio/sdk-core/blob/d2426f75c70528fc6b4cdb341e666c161fcf195b/core/src/telemetry/metrics.rs#L629C11-L629C62

```
/// The string name (which may be prefixed) for this metric
pub const WORKFLOW_TASK_SCHED_TO_START_LATENCY_HISTOGRAM_NAME: &str =
    "workflow_task_schedule_to_start_latency";
```



</details>


---

### #6201: tdbg diagnose command for stuck workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6201 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2024-06-27 00:09:27.000 UTC (1y 6m ago) |
| **Updated** | 2024-06-27 00:09:27.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

We need a tdbg command that could help dump all relevant information and do some basic analysis.
`tdbg workflow diagnose`

Information includes:
1) Mutable state
2) Workflow History
3) Shard info (ack level, read level for all queues)
4) Task queue info (including all partitions, backlog size etc)
5) Namespace info

Diagnose part:
Check if there should have workflow task scheduled or started, but didn't happen.
Check if there is any timer that should already fired but did not happen.
Check if there is worker polling the right task queue.



---

### #6100: fr: add support for multi-az postgres with read-only replication

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6100 |
| **State** | OPEN |
| **Author** | joshua-auchincloss (Joshua Auchincloss) |
| **Created** | 2024-06-08 17:37:25.000 UTC (1y 6m ago) |
| **Updated** | 2024-10-22 17:34:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | gow |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

Yes, current implementation assumes all postgres data stores have static read-write configurations (i.e. will always be read-write for the lifecycle of the temporal instance). 

**Describe the solution you'd like**

Add support for postgres configurations such that:
- Nodes may utilize *dynamic* read-only/read-write (HA) postgres servers between multiple instances.
- Where write transactions are required, fall back to available read-write (master) nodes to mitigate issues with HA compatibility.
- Default queries only utilize read-only nodes (optional but bonus points if so).

**Describe alternatives you've considered**

No alternatives, if HA is enabled between multiple postgres servers (e.g temporal does not check `pg_is_in_recovery`), temporal will eventually fail upon (postgres) instance recovery / failover.

**Additional context**

Postgres configured such that:
- 1 master / read-write instance
- 1 or more read-only instance(s)
- Nodes may failover from read-write to read-only (and vice versa) at any point.

Happy to provide docker configurations for reproducible context for the feature request.

#### Comments (4)

<details>
<summary><strong>gow</strong> commented on 2024-07-16 17:21:21.000 UTC</summary>

Hi @joshua-auchincloss I'm evaluating the feasibility of this feature. This is a very good feature to have. But not sure about the complexity it introduces.
Could you please provide the docker setup so that I can try it out?

</details>

<details>
<summary><strong>gow</strong> commented on 2024-09-10 16:49:49.000 UTC</summary>

Unfortunately the Temporal server application cannot clearly distinguish between purely read-only transactions and read-write transactions. So it's not possible for the server to route the queries appropriately.


</details>

<details>
<summary><strong>joshua-auchincloss</strong> commented on 2024-09-12 15:49:35.000 UTC</summary>

> Unfortunately the Temporal server application cannot clearly distinguish between purely read-only transactions and read-write transactions. So it's not possible for the server to route the queries appropriately.
> 
> 

Hey @gow, thanks for the response - apologies for the delayed response on docker configs, will get to this once there is time to do so.

Regarding distinguishing read-only vs read-write transactions, the client could distinguish at the minimum between writable database hosts at runtime and reroute all queries to that database. Per transaction level is a nice-to-have, but not required. IMO, priority would be the former where all queries are routed to the 'master' write clusters to avoid e.g errors against read-only instances. This approach would be a simple [`select pg_is_in_recovery()`](https://postgresql.org/docs/current/functions-admin.html#FUNCTIONS-RECOVERY-CONTROL)

</details>

<details>
<summary><strong>gow</strong> commented on 2024-10-22 16:57:03.000 UTC</summary>

Hi @joshua-auchincloss looks like the sql connections are all treated as read-write. There is no concept of read-only connections at the moment.
A quick temporary fix for this problem could be that when we initiate a connection, we perform pg_is_in_recovery() check and drop that connection. However, this would not work in 2 cases
1. PG goes into recovery mode after the connection is established.
2. There are no other hosts available to connect and we end up not having any DB connections.


</details>


---

### #5881: Continue-as-new with a different WorkflowID

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5881 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2024-05-08 18:13:42.000 UTC (1y 7m ago) |
| **Updated** | 2024-05-08 18:13:42.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Workflows that call continue-as-new frequently can lead to hot shards.

**Describe the solution you'd like**
Many workflows rely on continue-as-new to process large datasets. Many of those workflows do not receive any external signals that require addressing them by WorkflowID. 

The proposal is to support an option to continue-as-new to change WorkflowID. The whole chain of workflows still can be navigated through correspondent ids in the events as well listed using visibility queries.




---

### #5873: WorkflowContext provides whether the current workflow task is from a reset

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5873 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2024-05-07 18:47:41.000 UTC (1y 7m ago) |
| **Updated** | 2024-05-28 21:10:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

It's not possible to know whether or not the current task is from a  reset. It would be much simpler to build a logic in workflow to fail on certain case (to stop all the activities), then I will reset it later to resume.

### Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. SCREENSHOTS OR CODE SAMPLES ARE VERY HELPFUL -->

Expose a boolean flag in WorkflowContext to indicate whether the current workflow task is from a reset?(basically if the previous task is workflow task failed with reason of Reset)

#### Comments (4)

<details>
<summary><strong>cretz</strong> commented on 2024-05-07 20:13:32.000 UTC</summary>

This is a server-side feature. If this information is deemed important, it will have to be provided on each task in history. Transferring to server repo...

</details>

<details>
<summary><strong>bergundy</strong> commented on 2024-05-07 20:33:19.000 UTC</summary>

As discussed internally, the SDK has the information already, it uses it to reseed the RNG.

@longquanzheng, can you show an example of how you'd expect this to be used? A short sample would be nice so we can understand what you have in mind.

Also, what would this flag do if a workflow was reset multiple times?

</details>

<details>
<summary><strong>longquanzheng</strong> commented on 2024-05-07 23:18:47.000 UTC</summary>


> @longquanzheng, can you show an example of how you'd expect this to be used? A short sample would be nice so we can understand what you have in mind.
> 


Yeah I need to implement a "soft timeout" for a workflow. It will fail the workflow on the timeout. But later on I will let a human to manually restart it by resetting the workflow back to the last workflow task. If I have this boolean, I can just let it create another timer again as an "extended timeout". 

This also sounds like a workaround of a missing feature of "soft timeout" that Maxim pointed out. https://github.com/temporalio/temporal/issues/1412 

> Also, what would this flag do if a workflow was reset multiple times?

I would expect the flag is only true for the first workflow task after reset. So if a history:
```
WF task failed by reset
WF task scheduled
WF task started <-- flag will be true
...
WF task scheduled
WF task started <-- flag will be false
...
WF task failed by reset
WF task scheduled
WF task started <-- flag will be true
...
WF task scheduled
WF task started <-- flag will be false
...

```


</details>

<details>
<summary><strong>bergundy</strong> commented on 2024-05-28 21:10:29.000 UTC</summary>

I would recommend that you create a timer and cancel the root workflow scope for a soft timeout for now.
I agree that soft timeout would be a great feature to have in the platform.

</details>


---

### #4845: Add ability to reject signals by type

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4845 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2023-09-06 22:39:27.000 UTC (2y 3m ago) |
| **Updated** | 2023-09-06 22:39:27.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Sometimes a workflow needs the ability to reject signals that it is not listening to. For example a workflow that was canceled doesn't want to accept signals while it is performing cleanup.

**Describe the solution you'd like**
Add commands to enable/disable signal types. The signal type which is not enabled will be rejected synchronously.

BTW the same can apply to updates.

**Describe alternatives you've considered**
An update can be rejected, but it requires workers to run. This feature will generate rejection even if workers are down.




---

### #4715: Unable to start 1.21.4 cluster with archival disabled

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4715 |
| **State** | OPEN |
| **Author** | vanhtuan0409 (Tu·∫•n V∆∞∆°ng) |
| **Created** | 2023-08-02 07:08:12.000 UTC (2y 5m ago) |
| **Updated** | 2023-08-11 21:43:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

Able to upgrade temporal cluster from 1.20.4 to 1.21.4 with archival disabled


## Actual Behavior

Server crash after upgrading to 1.21.4

## Steps to Reproduce the Problem

  1. Start cluster 1.20 with the bellow config
  2. Let's the cluster run for a while
  3. Upgrade cluster to 1.21.4

```
archival:
  history:
    state: "disabled"
  visibility:
    state: "disabled"
```

## Specifications

  - Version: 1.21.4
  - Platform: Linux
 
Suspect related issue: https://github.com/temporalio/temporal/issues/4638


#### Comments (4)

<details>
<summary><strong>yiminc</strong> commented on 2023-08-08 16:24:28.000 UTC</summary>

@vanhtuan0409 did you have archival enabled on your v1.20.4 server before upgrade to 1.21?

</details>

<details>
<summary><strong>vanhtuan0409</strong> commented on 2023-08-08 16:25:26.000 UTC</summary>

@yiminc no I dont have

</details>

<details>
<summary><strong>yiminc</strong> commented on 2023-08-08 18:33:09.000 UTC</summary>

@vanhtuan0409 could you paste the error logs you get from server.

</details>

<details>
<summary><strong>yycptt</strong> commented on 2023-08-11 21:41:41.000 UTC</summary>

Oh sorry, just saw that archival is not enable before. Plz ignore my commit below

~~This is expected if the cluster previously has archival queue enabled since the behavior will be very confusing and undefined.~~
~~See more discussion here: https://github.com/temporalio/temporal/pull/4634~~

~~Archival can still be disabled on namespace level.~~

</details>


---

### #4542: Support activity caching through sticky routing by user provided key

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4542 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2023-06-26 15:58:56.000 UTC (2y 6m ago) |
| **Updated** | 2023-06-26 15:58:56.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Users want to cache activity results to improve performance. Currently there is no built-in way to provide activity routing to the same host by user provided id. For example Kafka consumers get this out of the box.

**Describe the solution you'd like**
Support worker options to enable this feature. Support activity invocation option to specify a routing key.





---

### #4094: [Bug] Version info upgrade notification does not get cleared after cluster version upgrade

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4094 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2023-03-23 21:22:11.000 UTC (2y 9m ago) |
| **Updated** | 2025-07-14 12:34:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | bug, good first issue, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Sample output from `tctl admin cluster describe`:

```
"versionInfo": {
  "current": {
   "version": "1.20.0",
   "releaseTime": "2023-02-17T21:48:00Z"
  },
  "recommended": {
   "version": "1.20.0",
   "releaseTime": "2023-02-17T21:48:00Z"
  },
  "alerts": [
   {
    "message": ":ringed_planet: A new release is available!",
    "severity": "Low"
   }
  ],
  "lastUpdateTime": "2023-03-22T08:54:54.838392383Z"
 }
```

Note that the `lastUpdateTime` is after the cluster was upgraded.

#### Comments (4)

<details>
<summary><strong>smcgivern</strong> commented on 2023-09-18 10:25:26.000 UTC</summary>

We're seeing something similar:

```json
{
  "supportedClients": {
    "temporal-cli": "\u003c2.0.0",
    "temporal-go": "\u003c2.0.0",
    "temporal-java": "\u003c2.0.0",
    "temporal-php": "\u003c2.0.0",
    "temporal-server": "\u003c2.0.0",
    "temporal-typescript": "\u003c2.0.0",
    "temporal-ui": "\u003c3.0.0"
  },
  "serverVersion": "1.22.0",
  // snip
  "versionInfo": {
    "current": {
      "version": "1.20.3",
      "releaseTime": "2023-05-16T04:25:00Z"
    },
    "recommended": {
      "version": "1.21.2",
      "releaseTime": "2023-07-15T02:00:00Z"
    },
    "alerts": [
      {
        "message": "ü™ê A new release is available!",
        "severity": "Low"
      }
    ],
    "lastUpdateTime": "2023-09-08T14:24:09.898917704Z"
  },
  "failoverVersionIncrement": "10",
  "initialFailoverVersion": "1"
}
```

Per https://community.temporal.io/t/cluster-upgrade-from-1-14-4-1-9-0-serverversion-changes-but-current-version-stays-1-14-4/6691/2, we should consider `serverVersion` to be accurate, but the cluster's `versionInfo` hasn't updated, which is confusing.

</details>

<details>
<summary><strong>g4ze</strong> commented on 2025-01-23 18:23:12.000 UTC</summary>

hey @bergundy ! is this issue still up? if yes, could you pls elaborate a bit more on this issue?


</details>

<details>
<summary><strong>vaibhavyadav-dev</strong> commented on 2025-04-23 14:03:41.000 UTC</summary>

@yycptt @bergundy I'm Taking this one.
Thanks!

</details>

<details>
<summary><strong>AadiDev005</strong> commented on 2025-07-14 12:34:58.000 UTC</summary>

Hi @bergundy, @vaibhavyadav-dev, and Temporal team! I‚Äôm new to contributing to Temporal and interested in helping with this bug to ensure the version upgrade notification clears correctly in the Go-based server codebase. I see @vaibhavyadav-dev submitted a fix in PR #7650. Is there still work needed, like testing, refinements, or additional changes? I‚Äôd love to dive into the version info logic and assist. Any guidance on next steps or the dev setup would be great! Excited to contribute.

</details>


---

### #3769: Any plan to support MS SQL Server?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3769 |
| **State** | OPEN |
| **Author** | raymondsze (Sze Ka Wai Raymond) |
| **Created** | 2023-01-03 13:58:55.000 UTC (2y 12m ago) |
| **Updated** | 2024-03-23 17:03:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Temporal is awesome!
I read the documentation and tried the typescript examples. It do solve the current issue I am facing on distributed transaction.
However, I found that Temporal officially support MySQL and Postgres. 
We are working on on-premise product and some customers require MS SQL Server for the persistence.
I would like to ask if there is any plan to support MS SQL Server. I found https://gitlab.com/lercher/temporal-sqls, but it seems not official.



#### Comments (4)

<details>
<summary><strong>raymondsze</strong> commented on 2023-01-12 10:26:27.000 UTC</summary>

any update?

</details>

<details>
<summary><strong>adeal</strong> commented on 2023-05-08 20:50:09.000 UTC</summary>

My org (Salesforce) is also interested in support for Sql Server as the underlying persistence store.

Is this being tracked as a feature request? We're open to ideas of making the initial contribution to the Temporal server.

</details>

<details>
<summary><strong>samarabbas</strong> commented on 2023-05-10 02:27:59.000 UTC</summary>

Hey @adeal currently there is no plan to support MS SQL Server is the persistence plugin.  Initial contribution of the plugin is one concern but the bigger problem is ongoing cost of maintaining the plugin.  Ideally we would first refactor the persistence which allows these plugins to reside in separate repos and injected at runtime.  We also need to refactor the testing infrastructure where these plugins can be tested in a generic fashion.
Once we have this setup, then we can have external contributors provide and independently support plugins for other data sources.

</details>

<details>
<summary><strong>plaisted</strong> commented on 2024-03-23 17:03:33.000 UTC</summary>

I've been tinkering with temporal persistence as a way to understand how things work a little better and was wondering if adding a built in grpc persistence implementation would be possible. It appears the storage is already abstracted into a set of interfaces. General idea would be to add a built in "grpc" persistence where configuration would be supplying an endpoint (or endpoints) + TLS information. The endpoint would then be responsible for implementing all the required grpc calls based on the existing persistence interfaces.

This would serve the same purpose as a golang plugin architecture but allow the standard temporal distribution (containers, binaries) to use custom persistence without modification (just add configuration). Obviously this comes at some performance cost (serialization, network hop) but thought I'd mention and see what people thought.

</details>


---

### #2905: Access and configure underlying gRPC connection for core components

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2905 |
| **State** | OPEN |
| **Author** | eritscher (Evan) |
| **Created** | 2022-05-26 15:10:15.000 UTC (3y 7m ago) |
| **Updated** | 2023-03-03 20:20:16.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Core Temporal components connect to each other over a gRPC connection configured within the Temporal server codebase. An individual instance of Temporal Server does not have access to and cannot configure the underlying connection. 

Without the ability to configure/intercept that connection when a Temporal server component is started, we cannot add things like custom tracing or `grpc.WithPerRPCCredentials.`

 These are necessary so we can properly identify services when they reach out Temporal frontend
 
**Describe the solution you'd like**
Temporal server components would expose access to their underlying gRPC connection during initialization. A user can set custom dial options and/or interceptors on the underlying connection. 

**Describe alternatives you've considered**
As far as I've been able to tell, no implementation exists in Temporal Server for this level of control.



---

### #2698: Internode TLS certificates dual used as both client and server certificates

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2698 |
| **State** | OPEN |
| **Author** | sahilvv (Sahil Vazirani) |
| **Created** | 2022-04-01 18:45:23.000 UTC (3y 9m ago) |
| **Updated** | 2023-03-03 20:20:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
- Ability to feed separate certificates for clients (eg: matching client, history client) and their respective server certificates for each service
- Ability to feed different server names for each component (Frontend, matching, history)
## Actual Behavior
- Assumption in code to use single certificate for dual purpose. This puts a security risk with how mTLS is implemented. Based on discussions with temporal team it seems like self-signed certs were used during implementation of this feature causing this to fail.
- Another assumption in code to use same server name in SAN for all services (frontend, matching, history). This presents weak security as 1 microservice can impersonate another. 

## Steps to Reproduce the Problem

  1. Create a server certificate with the following in the certificate:
           X509v3 Extended Key Usage:
                TLS Web Server Authentication
  2. Use the cert/key pair in the TLS/internode section and bring up FE, Matching, History, Worker service
  3. Use tctl to query get-search-attributes
```
       Error: Unable to get search attributes.
Error Details: rpc error: code = Unavailable desc = unable to get temporal-sys-add-search-attributes-workflow workflow state: context deadline exceeded
('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
```

If the above steps are repeated with a client certificate containing the following:
           X509v3 Extended Key Usage:
                TLS Web Client Authentication
Error observed:
`{"level":"error","ts":"2022-04-01T01:15:45.045Z","msg":"matching client encountered error","service":"history","error":"last connection error: connection error: desc = \"transport: authentication handshake failed: x509: certificate specifies an incompatible key usage\"","service-error-type":"serviceerror.Unavailable","logging-call-at":"metricClient.go:259","stacktrace":"..."}
`
## Specifications

  - Version: 1.15.0
  - Platform: Linux


#### Comments (4)

<details>
<summary><strong>sahilvv</strong> commented on 2022-04-01 18:46:29.000 UTC</summary>

@sergeybykov FYI

</details>

<details>
<summary><strong>sergeybykov</strong> commented on 2022-04-01 20:29:32.000 UTC</summary>

Thank you, @sahilvv, for opening this and for your help with narrowing down the problem!

</details>

<details>
<summary><strong>yiminc</strong> commented on 2022-04-16 04:25:58.000 UTC</summary>

[ClientTLS](https://github.com/temporalio/temporal/blob/67e34a34209c278dc22312629c42f1363d59c79f/common/config/config.go#L173-L177)

Need to add CertFile/CertData and KeyFile/KeyData to ClientTLS. 

</details>

<details>
<summary><strong>jbreiding</strong> commented on 2022-04-16 06:27:27.000 UTC</summary>

I believe I understand the core issue here, we should be able to use certs which are restricted by key usage for their purpose.

This should be simple to reproduce given correctly generated certs.

The issue with the SAN will remain because of how internode connections are made and the TLS configuration available.

For something more strict a service mesh side car is going to be the recommended path to achieve a strict network policy like being described here.

</details>


---

### #2685: [Interpreter Proposal] New way to write workflow using Temporal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2685 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2022-03-30 19:18:17.000 UTC (3y 9m ago) |
| **Updated** | 2023-03-03 20:20:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Background

The main problem of this technology is that it requires developers to write code to be replayable. This is against the existing pattern of human programming. 
Most people don't know their code must be replayable. On the other hand, software must be soft. We have to change code all the time for business changes and new requirements. 
These two factors together have made things extremely difficult for engineers to use Temporal. 

## Proposal

Instead of writing the Temporal workflow, we let users write workflow in code by defining a set of **WorkflowState**. 
Each **WorkflowState** defines two things: **requestPrecondition** and **decideNextStates** 
**requestPrecondition** will request for signals, timers, activity, etc 
When all/any precondition is met, the **decideNextStates** is invoked to get the next states to jump into. 

To make this idea work, we build one Temporal workflow for everyone so that they don't need to write their Temporal workflow. 
This workflow will call **requestPrecondition** and **decideNextStates** using Temporal activities, so that those user workflow code is always replay safe. 

In another way to think about it, this is a Codable version of StepFunctions which is built on top of Temporal. 

It preserves all most the full power of Temporal like 
* Reset workflow 
* Search Attributes
* Writing unit tests for workflow code(which is super hard for StepFunctions) 
* etc

And there is no need to worry about non-deterministic errors. 

## Limitation

* We may lose some readability of the workflow code
* We will lose some flexibility of mutating internal workflow states compared to native Temporal workflow code. 


#### Comments (4)

<details>
<summary><strong>ccl0326</strong> commented on 2022-07-25 04:27:06.000 UTC</summary>

hi @longquanzheng , i totally agree this proposal. Is any progress now?

</details>

<details>
<summary><strong>longquanzheng</strong> commented on 2022-07-25 04:41:26.000 UTC</summary>

@ccl0326 yeah I have started the project https://docs.google.com/document/d/1BpJuHf67ibaOWmN_uWw_pbrBVyb6U1PILXyzohxA5Ms/edit

</details>

<details>
<summary><strong>longquanzheng</strong> commented on 2022-10-01 02:42:47.000 UTC</summary>

@ccl0326 

Some updates 
Here is the [slides](https://docs.google.com/presentation/d/1CpsroSf6NeVce_XyUhFTkd9bLHN8UHRtM9NavPCMhj8/edit#slide=id.gfe2f455492_0_26) of a comparison

A design [doc](https://docs.google.com/document/d/1BpJuHf67ibaOWmN_uWw_pbrBVyb6U1PILXyzohxA5Ms/edit).

We have implemented the first version with Temporal: https://github.com/cadence-oss/iwf

And a Java [SDK](https://github.com/cadence-oss/iwf-java):
It could be easily extended to support any other languages as the [API schema](https://github.com/cadence-oss/iwf-idl) is very lightweight

</details>

<details>
<summary><strong>longquanzheng</strong> commented on 2022-11-08 22:11:55.000 UTC</summary>

the project is now moved to IndeedEng: https://github.com/indeedeng/iwf and just released the first version

</details>


---

### #1904: Create visibility reconciler workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1904 |
| **State** | OPEN |
| **Author** | alexshtin (Alex Shtin) |
| **Created** | 2021-09-09 01:44:43.000 UTC (4y 3m ago) |
| **Updated** | 2023-03-03 20:21:41.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement, visibility |
| **Assignees** | alexshtin |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Due to various reasons visibility records might get in-sync with main database. It is possible to reconstruct entire visibility database from the main database. 

**Describe the solution you'd like**
System workflow which is started with `tctl` command would be the best solution.



---

### #1844: Taskqueue scavenger emits persistence errors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1844 |
| **State** | OPEN |
| **Author** | skrul (Steve Krulewitz) |
| **Created** | 2021-08-23 20:30:11.000 UTC (4y 4m ago) |
| **Updated** | 2023-03-03 20:21:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | potential-bug |
| **Assignees** | dnr, MichaelSnowden |
| **Milestone** | None |

#### Description

## Expected Behavior

Taskqueue scavenger should run without emitting an excessive amount of persistence error metrics and not cause a dip in persistence availability.

## Actual Behavior

Every time the taskqueue scavenger runs, it emits quite a few `persistence_errors` metrics with operations `gettasks`, `deletetaskqueue`, `listtaskqueue` and `completetaskslessthan`, enough to affect a dip in our persistence availability metric (persistence_errors vs. persistence_requests).

The scavenger does appear to be making progress and successfully deleting queues, so it is unclear what all these errors are about. Unfortunately the code does not log error messages.

Are these errors expected? Should I be excluding worker persistence metrics from my overall persistence availability metric?

## Specifications

  - Version: 1.8
  - Platform:


#### Comments (4)

<details>
<summary><strong>yiminc</strong> commented on 2021-08-24 00:47:44.000 UTC</summary>

@skrul , could you paste a few example of the error logs emitted? 
== update ==
Ok, now I see that you mention there is no error logs. Could you paste the metrics query that you use to associate the issue with taskqueue scavenger.

</details>

<details>
<summary><strong>skrul</strong> commented on 2021-08-24 02:45:54.000 UTC</summary>

The query is something like:

```
sum:temporal_server.persistence_errors{app:temporal-worker-myapp-production} by {operation,app}.as_count()
```

In our case the app tag has the role of the server so in this case I know that it is coming from a worker. This was during a run that deleted ~5k taskqueues:

![image](https://user-images.githubusercontent.com/372141/130546970-7757c406-9026-4ee8-9ea1-731f189178a3.png)

During this time the database wasn't under any particularly high load but it did get hit pretty hard with reads:

![image](https://user-images.githubusercontent.com/372141/130547473-9c4809ba-bf61-46cf-a2a4-9b97c32dbea2.png)

And a bunch of new connections:

![image](https://user-images.githubusercontent.com/372141/130547536-23ce519f-b810-4899-8df1-cb435beb22e4.png)

Now I'm thinking this could be due to the fact that we don't have a max connection pool set on the worker.


</details>

<details>
<summary><strong>yiminc</strong> commented on 2021-08-24 21:05:33.000 UTC</summary>

@skrul how do you get to know that run deleted 5K task queues? Do you know how many tasks were there in those queues? Because inorder to delete a queue, the scavenger need to make sure all tasks in the queue are expired. If those queue has lots of tasks in them and no one is polling from those tasks, it would cause the scavenger to read through all of them before it can be deleted. 

There are a few metrics we can read to better understand the situation:
https://github.com/temporalio/temporal/blob/fe05751305b1cb50b68efa23f8aa5f1b34f45bc5/service/worker/scanner/taskqueue/scavenger.go#L186
https://github.com/temporalio/temporal/blob/fe05751305b1cb50b68efa23f8aa5f1b34f45bc5/service/worker/scanner/taskqueue/scavenger.go#L194-L197

Also, the task queue scavenger is a cron workflow that runs every 12 hours. It has only one activity that would spin up 32 executor concurrently each to scan through one task queue.

</details>

<details>
<summary><strong>aliceabe</strong> commented on 2021-10-29 23:49:28.000 UTC</summary>

Just wanted to update this thread with our latest findings. We know the run deletes about 5k task queues by looking at the log lines "taskqueue deleted". Unfortunately the job doesn't log when it encounters persistence errors, it just infinitely retries. Ideally, we should make this job more configurable, for example allow users to configure the task queue batch size:

https://github.com/temporalio/temporal/blob/fe05751305b1cb50b68efa23f8aa5f1b34f45bc5/service/worker/scanner/taskqueue/scavenger.go#L79

Or at least pass and use the max persistence QPS setting in the task queue scavenger, similar to how it's used in the history scavenger:

https://github.com/temporalio/temporal/blob/fe05751305b1cb50b68efa23f8aa5f1b34f45bc5/service/worker/scanner/history/scavenger.go#L116-L118

</details>


---

### #1821: Document the Elasticsearch permissions required by Temporal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1821 |
| **State** | OPEN |
| **Author** | cmaron (Chad Maron) |
| **Created** | 2021-08-13 16:09:12.000 UTC (4y 4m ago) |
| **Updated** | 2023-03-03 20:21:34.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement |
| **Assignees** | alexshtin, flossypurse |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
We were recently debugging an issue in our QA environment where our workflows were failing trying to execute a `ListWorkflowExecutionsRequest` (in java). It turns out the Elasticsearch cluster we had setup for temporal had restrictive permissions which was leading to 403's when temporal tried to query Elasticsearch. We temporarily resolved the issue by opening up permissions between Temporal and ES.

**Describe the solution you'd like**
We do not want to leave the ES permission open, so a documentation update as to which permissions are required by Temporal would be amazing.

**Describe alternatives you've considered**
Currently our fix was to make ES open for the Temporal cluster, but this is not sustainable for us.

**Additional context**
Original slack thread: https://temporalio.slack.com/archives/CTRCR8RBP/p1627509370447100



---

### #1700: Simple way to handle a single concurrent workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1700 |
| **State** | OPEN |
| **Author** | mytototo |
| **Created** | 2021-07-01 07:20:30.000 UTC (4y 6m ago) |
| **Updated** | 2023-03-03 20:22:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement, API, devexp |
| **Assignees** | mfateev |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
I'm looking to create a task queue that needs to handle at most 1 workflow of a specific workflow type across workers.

**Describe the solution you'd like**
I think something like this could work for one worker:
```go
we.RegisterWorkflowWithOptions(MyWorkflow, workflow.RegisterOptions{
  MaxConcurrentWorkflowsInWorker: 1,
  MaxConcurrentWorkflowsAcrossWorkers: 1,
})
```

But I'm not sure this would work across all workers. Any idea?

**Describe alternatives you've considered**
I took a look at [this](https://github.com/temporalio/samples-go/tree/master/mutex), [this](https://community.temporal.io/t/management-of-mutex-workflow/190), and [this](https://community.temporal.io/t/limit-workflow-based-concurrent-executions/633).

I think these solutions are too complex to implement in comparison to the solution described.

#### Comments (4)

<details>
<summary><strong>samarabbas</strong> commented on 2021-07-04 21:25:29.000 UTC</summary>

@mfateev any thoughts on this feature request?

</details>

<details>
<summary><strong>yiminc</strong> commented on 2021-07-16 17:00:50.000 UTC</summary>

Concurrent limit of 1 is special case, you can rely on the unique workflow ID feature of temporal. If you make sure your workflow always use a const workflow ID, you are guaranteed to have only one running workflow for this ID. 

</details>

<details>
<summary><strong>mytototo</strong> commented on 2021-07-18 18:18:41.000 UTC</summary>

@yiminc Sorry if I were not clear. I'm not looking to handle 1 concurrent workflow ID across workers at a time. Instead I'm looking to ensure a workflow type is executing a single workflow at a time, where the workflow IDs are not const. These workflows access a single resource in a database and I need to ensure only one is active when reading / writing these said resource.

</details>

<details>
<summary><strong>sync-by-unito</strong> commented on 2023-03-02 02:22:55.000 UTC</summary>

‚û§ Maxim Fateev commented:

An ability to limit the number of parallel workflow executions (of a given type or by some other group like batchId) across all workers is a very frequently requested feature. There is a similar feature to support limiting number of parallel activities. 

Designing such a limit might be nontrivial at scale. So we need to weight it against other feature requests.

</details>


---

### #960: Support for in memory db and setup for running temporal with limited resources

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/960 |
| **State** | OPEN |
| **Author** | morenohernan (Hernan Moreno) |
| **Created** | 2020-11-07 23:18:50.000 UTC (5y 1m ago) |
| **Updated** | 2021-07-04 20:11:49.000 UTC |
| **Upvotes** | 2 |
| **Comments** | 0 |
| **Priority Score** | 4 |
| **Labels** | enhancement, packaging, devexp |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 2 |

#### Description

My organization has a use case (IoT, Edge) where We need our app to be reliable running under limited resources (hardware with limited capacity, for instance: CPU with 2 cores and 2 Gb ram). We consider temporal as the right choice, to build our apps, however, we realized that the current default setup of temporal might need adjustments, documentation on how to run properly under such scenarios. Also, we consider that perhaps an in-memory DB along with simple binary (temporal) would be a better choice for executing workflows with good throughput. After discussing with **Wenquan Xing**, this looks feasible.  



---

### #778: Add ScheduleToStart timeout to WorkflowTaskScheduledEvent

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/778 |
| **State** | OPEN |
| **Author** | mastermanu |
| **Created** | 2020-07-20 22:47:07.000 UTC (5y 5m ago) |
| **Updated** | 2021-07-03 23:37:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | good first issue, API, devexp, difficulty: easy, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |

#### Description

![image](https://user-images.githubusercontent.com/5552381/87993729-1be82c00-caa0-11ea-943f-034f3001724c.png)

See image above, which indicates "10" seconds, however the server is hardcoded to 5 seconds and the task actually does timeout in 5 seconds, which can be confusing to the user:

![image](https://user-images.githubusercontent.com/5552381/87993777-3ae6be00-caa0-11ea-8c43-da2ba3e8dea0.png)


#### Comments (4)

<details>
<summary><strong>mfateev</strong> commented on 2020-08-05 01:25:14.000 UTC</summary>

This is actually correct, while confusing. 
The 5 second timeout is the ScheduleToStart timeout. And the WorkflowTaskTimeout is StartToClose timeout for the workflow task.

</details>

<details>
<summary><strong>mfateev</strong> commented on 2020-08-05 01:26:48.000 UTC</summary>

Non sticky tasks have infinite (more precisely workflowRunTimeout) ScheduleToStart timeout. But the sticky list ones have short 5 second (by default) timeout to ensure that they are rerouted to another worker quickly.


</details>

<details>
<summary><strong>mastermanu</strong> commented on 2020-08-05 01:35:21.000 UTC</summary>

This makes sense. I wonder if it is worth printing the ScheduleToStart timeout on the WorkflowTaskScheduled just to make this clearer or perform some other UX changes just to make this less confusing to a user?

</details>

<details>
<summary><strong>mfateev</strong> commented on 2020-08-05 01:39:27.000 UTC</summary>

I think UI should show only what's present on the event. I don't see a problem in adding ScheduleToStart timeout to the  WorkflowTaskScheduledEvent.

</details>


---

### #674: Heartbeat WorkflowTask during history pagination

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/674 |
| **State** | OPEN |
| **Author** | samarabbas (Samar Abbas) |
| **Created** | 2020-08-13 17:47:27.000 UTC (5y 4m ago) |
| **Updated** | 2023-08-08 20:00:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 4 |
| **Priority Score** | 4 |
| **Labels** | enhancement, difficulty: medium, operations |
| **Assignees** | yiminc |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
For workflow executions with long history it could take a longer than workflow task timeout 
to paginate through history events during replay.  This would result in workflow task timeout
which would be very bad for the system as now this task would be retried on a different host
which would again result in replaying the history from beginning.  

**Describe the solution you'd like**
One potential solution is to introduce a new concept of workflow task heartbeat while client
is paginating through history during replay.  This way we can keep on renewing the workflow 
task timeout while client is paginating through history events during replay.



#### Comments (4)

<details>
<summary><strong>bergundy</strong> commented on 2021-11-17 21:01:49.000 UTC</summary>

@yiminc this is exactly what we'd talked about today :)

</details>

<details>
<summary><strong>yiminc</strong> commented on 2021-11-17 23:59:20.000 UTC</summary>

The same workflow task heartbeat mechanism that is used by worker for local activity to extend workflow task timeout could be used here. From server's point of view, there is no difference. 

</details>

<details>
<summary><strong>bergundy</strong> commented on 2021-11-18 03:50:00.000 UTC</summary>

@Sushisource looks like a reasonable solution, wdyt, should we add to the backlog in core?

</details>

<details>
<summary><strong>mfateev</strong> commented on 2023-08-08 20:00:21.000 UTC</summary>

I don't think workflow task timeout is the best solution. I would make this the service responsibility to extend the timeout when history is paginated. All the needed information can be included into the pagination token.

</details>


---

### #8764: Listing workflows and activities from temporal sdk

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8764 |
| **State** | OPEN |
| **Author** | albertom-pirovano-mck (Alberto Mario Pirovano) |
| **Created** | 2025-12-05 19:30:23.000 UTC (26 days ago) |
| **Updated** | 2025-12-12 00:08:56.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
I would like to use temporal python sdk to list workflows and activities registered in the system

**Describe the solution you'd like**
see above

**Describe alternatives you've considered**
I built an internal registry but I'd like to leverage temporal to manage the registry

**Additional context**
None


#### Comments (1)

<details>
<summary><strong>ShahabT</strong> commented on 2025-12-12 00:08:56.000 UTC</summary>

@albertom-pirovano-mck thanks for the feature request.

As of now server does not keep registry of Workflow and Activity Types. But this is something that maybe we want to consider in the future.

Another approach would be to get the info via server APIs from the workers. But that requires 1) workers be running 2) user gives task queue name in the request so server knows which worker to get the info from.

Given the limitations of the second approach do you think it's going to still be helpful for your use case?

</details>


---

### #8611: Allowing workflow to specify max retry attempts / duration before an activity starts throwing non-BENIGN errors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8611 |
| **State** | OPEN |
| **Author** | albertyfwu (Albert) |
| **Created** | 2025-11-07 16:09:46.000 UTC (1 months ago) |
| **Updated** | 2025-12-11 22:12:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Sometimes, a workflow will call an activity that polls a service. We don't want error logs and metrics for the first, say, N activity retry attempts, but we want it thereafter. Recent changes allowed activities to return BENIGN errors, which suppress error logs and metrics. However, this is controlled by the activity.

Is it possible to build something to allow the developer to specify at the instantiation site of the activity (alongside specification of retry policy in `ActivityOptions`) the exhaustion thresholds?

**Describe the solution you'd like**
I'd like for the developer to be able to specify at the instantiation site of the activity the benign exhaustion thresholds alongside the retry policy. Perhaps there is an interceptor on the activity side that reads in the settings, and maps exceptions to BENIGN when under the thresholds but passes exceptions through directly without transformation once the thresholds are breached.

**Describe alternatives you've considered**
Only solution I can think of is:
1. Use `ContextPropagator` and `WorkflowThreadLocal` to pass along benign retry policy config from the workflow to activity. Benign retry policy would be written immediately before scheduling activity and cleared immediately after activity returns.
2. Interceptor in activity side reads out the benign retry policy config and transforms exceptions from activity if under benign threshold by setting `BENIGN` error category.

Is this the best solution?

**Additional context**
Here is the previous change for allowing activities to throw BENIGN: https://github.com/temporalio/sdk-java/issues/704


#### Comments (3)

<details>
<summary><strong>Quinn-With-Two-Ns</strong> commented on 2025-11-10 17:26:29.000 UTC</summary>

The Temporal server manages activity retries, all ways to control activity retries are already present in `ActivityOptions`. A user could partially emulate using an interceptor as you pointed out, but to handle timeouts and some other failures this would need to be managed by the Temporal server.

</details>

<details>
<summary><strong>Quinn-With-Two-Ns</strong> commented on 2025-11-10 17:27:18.000 UTC</summary>

Transferring to the server repo as this is not a Java specific issue and the majority of the work is on the sever. 

</details>

<details>
<summary><strong>gow</strong> commented on 2025-12-11 22:12:05.000 UTC</summary>

This was closed by mistake (due to a faulty integration). Reopening it.

</details>


---

### #8432: Set log level of dynamic config updated message in file_based_client to warn instead of info

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8432 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2025-10-06 16:11:34.000 UTC (2 months ago) |
| **Updated** | 2025-10-08 02:17:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

https://github.com/temporalio/temporal/blob/main/common/dynamicconfig/file_based_client.go#L193

Currently users have to set log level in static config to "info" to see this message. Message can be pretty 
important to have in logs to know that dynamic config changes have been applied (and when according to 
timestamp)

Request is to change this log message to Warn so that users don't have to set their log level to info and get bombarded with tons more logs, but still can see this one.

#### Comments (3)

<details>
<summary><strong>mfateev</strong> commented on 2025-10-06 18:47:53.000 UTC</summary>

I believe warn should be used only for messages that represent some potential problem. Applying dynanmic config changes doesn't fit this definition.

</details>

<details>
<summary><strong>mfateev</strong> commented on 2025-10-06 18:48:30.000 UTC</summary>

We should add the ability to inspect dynamic config from a service through an API.

Reactions: üëç 1

</details>

<details>
<summary><strong>hiteshwadhwani</strong> commented on 2025-10-08 02:17:50.000 UTC</summary>

I agree with @mfateev that using warn could dilute its meaning and that creating an endpoint would be much more intuitive. As an alternative we can keep the global log level and have a way to override levels for specific components.

</details>


---

### #8153: Address security vulnerability CVE-2025-22871 for golang:net/http/internal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8153 |
| **State** | OPEN |
| **Author** | Pavithra-Reddy (Pavithra Reddy) |
| **Created** | 2025-08-06 09:37:44.000 UTC (4 months ago) |
| **Updated** | 2025-09-18 07:45:53.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

We see that golang:net/http/internal has vulnerability [CVE-2025-22871](https://nvd.nist.gov/vuln/detail/CVE-2025-22871). Please help us with a docker image for both server and admin tools by fixing these vulnerabilities.

#### Comments (1)

<details>
<summary><strong>subrata71</strong> commented on 2025-09-18 07:45:53.000 UTC</summary>

Any update on this?

</details>


---

### #8113: Add Activity support for Worker Versioning

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8113 |
| **State** | OPEN |
| **Author** | jbrody-nexxa |
| **Created** | 2025-07-29 18:56:53.000 UTC (5 months ago) |
| **Updated** | 2025-11-29 04:58:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
My team has Workflows and Activities developed on different release cycles. We use Worker Versioning to either pin or use latest version of a Workflow. However, when a Workflow calls an Activity, even if the Activity Worker uses Worker Versioning, there is no way for the Workflow to specify the version of the Activity to use. We have resorted to baking the Activity version into the task queue name, which is not ideal as our Workers are already using Deployments and Versioning.

**Describe the solution you'd like**
Add `versioningOverride` to `ActivityOptions`, and have the Temporal Service use the `versioningOverride` (as well as the worker defaults specified in `WorkerDeploymentOptions`) to resolve the scheduled Activity to a Worker Deployment.

**Describe alternatives you've considered**
For now, we are baking the Activity Version into the task queue name, which forces all Activity calls to be pinned to a specific version.

**Additional context**
This would mostly leverage the work already done for Workflow Versioning.


#### Comments (3)

<details>
<summary><strong>ShahabT</strong> commented on 2025-10-10 21:03:29.000 UTC</summary>

@jbrody-nexxa I'd like to understand more about your use case.

> My team has Workflows and Activities developed on different release cycles. 

In this case, first thing I'd think is "workflow <-> activity interface should remain compatible across versions of these separate deployments" so, for example, when you upgrade the workflow worker, both old and new workflow workers can work with the current activity worker version (and vice versa).

I assume something is preventing you from taking that approach. Can I know what it is?

Generally, during the design of this feature we've been trying to not involve the caller in worker versioning (and deployment cycles of the workers) and it's up to the user needs to keep the interface compatible. But I don't think there is any big reason preventing us to add this feature if there are compelling use cases for it.

</details>

<details>
<summary><strong>jbrody-nexxa</strong> commented on 2025-10-22 17:27:56.000 UTC</summary>

> In this case, first thing I'd think is "workflow <-> activity interface should remain compatible across versions of these separate deployments" so, for example, when you upgrade the workflow worker, both old and new workflow workers can work with the current activity worker version (and vice versa).
> 
> I assume something is preventing you from taking that approach. Can I know what it is?

Sure, if you can be 100% confident that the interface (and any external side effects of the implementation) will never change, you could take this approach. I would say about 90% of the time, that's not the case for us. Interfaces do change, and even when they don't, sometimes implementations change in a way that has external side effects. In those cases, we want to increment a version and make sure the calling workflow is intentionally upgrading to it. Of course, one could achieve the same outcome by changing the activity name rather than versioning it. Effectively, that's what we've done by baking the version number into the activity task queue. But that's not activity versioning.

If you think of an activity the same as a block of code or a library, you could ask the same of any other code or library... why doesn't npm or maven or a REST API simply always serve up the latest version and rely on the implementation to guarantee backwards compatibility? Because in practice things do change, and if the framework doesn't support versioning, it means you'll end up with a proliferation of artifacts when the implementation inevitably does change.

> Generally, during the design of this feature we've been trying to not involve the caller in worker versioning (and deployment cycles of the workers) and it's up to the user needs to keep the interface compatible. But I don't think there is any big reason preventing us to add this feature if there are compelling use cases for it.

This is surprising to hear because the workflow client has the ability to specify the version and strategy (pinned, auto-upgrade) when calling a workflow. This works great for us, as we explicitly version workflows and workflow dependencies due to the reasons above. All we need is the same interface between workflows and activities, for the same reasons.

</details>

<details>
<summary><strong>ShahabT</strong> commented on 2025-11-29 04:58:31.000 UTC</summary>

@jbrody-nexxa we discussed this internally. We want to support it, but it is right now in backlog behind some other more important items.

</details>


---

### #8087: [Scheduled Actions] Skipped Action Metric

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8087 |
| **State** | OPEN |
| **Author** | lina-temporal (Lina Jodoin) |
| **Created** | 2025-07-22 21:24:56.000 UTC (5 months ago) |
| **Updated** | 2025-07-23 00:09:36.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement, feature-request, schedules |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
We'd like to have a metric for whenever schedules skip a scheduled run for any reason (either by overlap policy, missing the catchup window, or from a buffer overrun). 

**Describe the solution you'd like**
- A new metric is introduced to keep track of skipped scheduled runs.

**Describe alternatives you've considered**
- Changes to record skipped schedules in a schedule's visibility record and/or the memo would still necessitate polling to record and alarm on their values.

**Additional context**
- Customers would like to alarm on when a scheduled run was missed or skipped.


#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2025-07-23 00:09:36.000 UTC</summary>

And they need to be able to identify which schedules has that missing/skipped run after they are paged. 

Reactions: üëç 1

</details>


---

### #7882: Support for selecting "Workflow Id Conflict Policy" for child workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7882 |
| **State** | OPEN |
| **Author** | jcmonteiro (Jo√£o C. Monteiro) |
| **Created** | 2025-06-07 04:58:53.000 UTC (6 months ago) |
| **Updated** | 2025-10-08 00:31:21.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
I have a setup where an orchestrator workflow calls child workflows. The parent workflow defines the "top-level" logic (much like a flowchart) and delegates the specifics to the children. The pattern works well in my use case since the goal is to isolate the parent workflow from the implementation details of each step.

In case the parent must be reset, I would like its children to be reused with a "Use Existing" reuse policy. The challenge is that this option is not supported when starting a child workflow.

**Describe the solution you'd like**
Support for selecting the "Workflow Id Conflict Policy" when starting a child workflow (in Python).

**Describe alternatives you've considered**
- Redesigning our use pattern with workflows and child workflows. 
- Selecting the "Workflow Id Reuse Policy" to "Terminate if Running." Not ideal since the child Workflow could still be healthy.

**Additional context**
I am using the Python SDK, but noticed the option is also not present in the other SDKs.


#### Comments (1)

<details>
<summary><strong>mfateev</strong> commented on 2025-10-08 00:31:21.000 UTC</summary>

This is a reasonable request. Note that this assumes each child has an explicitly assigned WorkflowId, which remains unchanged after the reset. By default, the random seed changes after the reset, leading to different child workflow IDs.

</details>


---

### #7866: AWS STS for connecting S3 for archival

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7866 |
| **State** | OPEN |
| **Author** | Mahee777 |
| **Created** | 2025-06-04 08:17:36.000 UTC (7 months ago) |
| **Updated** | 2025-08-08 16:14:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi Team,

**Problem Statement**
We are using a self-hosted Temporal service and attempting to enable the archival feature. However, we are unable to access AWS S3 from the Temporal service because it does not support AWS STS. The Temporal server requires actual access_key and secret, but due to infrastructure restrictions, we cannot use actual AWS credentials. Our infrastructure supports STS, but the Temporal server does not. Is there an alternative solution that can work in this scenario?

**Expected Behavior**
temporal server should be able to connect with S3 bucker and archive.

**Actual Behavior**
getting 403 


#### Comments (3)

<details>
<summary><strong>Protozet</strong> commented on 2025-06-04 10:26:46.000 UTC</summary>

@Mahee777 - Were you able to get it running using the actual AWS access key and secret? Can you share your archival values ?

</details>

<details>
<summary><strong>Mahee777</strong> commented on 2025-06-04 11:48:07.000 UTC</summary>

@Protozet its working with actual AWS access and secret key. 

Here is the archival values

history:
        enableRead: true
        provider:
            filestore: null
            gstorage: null
            s3store:
                endpoint: null
                logLevel: 0
                region: us-east-1
                s3ForcePathStyle: false
        state: enabled
    visibility:
        enableRead: true
        provider:
            filestore: null
            gstorage: null
            s3store:
                endpoint: null
                logLevel: 0
                region: us-east-1
                s3ForcePathStyle: false
        state: enabled
namespaceDefaults:
    archival:
        history:
            URI: s3://dev-workflow-temporal-archive-bucket
            state: enabled
        visibility:
            URI: s3://dev-workflow-temporal-archive-bucket
            state: enabled

</details>

<details>
<summary><strong>erka</strong> commented on 2025-08-08 16:14:57.000 UTC</summary>

@Mahee777 You need a role for the Temporal and then set that role to environment variable `AWS_ROLE_ARN`. It really depends on the your setup how it should be configured. EC2, ECS, EKS, etc will have own steps. Like https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html. 

Also you will need access key and secret to assume into the role with STS in any case. 

</details>


---

### #7741: Workflow Update causing WFT Failure with `Premature end of stream`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7741 |
| **State** | OPEN |
| **Author** | gauravthadani (Gaurav Thadani) |
| **Created** | 2025-05-09 06:33:06.000 UTC (7 months ago) |
| **Updated** | 2025-11-18 12:52:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
Expect no Workflow Task failure.


## Actual Behavior
- There is WF Task failure
- We know this is a transient and benign error, subsequent WFT fetches entire history and resumes well
- Monitoring on Workflow Task Failures causing alerts at Workers

## Steps to Reproduce the Problem

  1. Updates sent to Worfklows in cache after some time (approx. 3 hours)
  1.
  1.

## Specifications

  - Version: latest 1.27.2
  - Platform:


#### Comments (3)

<details>
<summary><strong>yycptt</strong> commented on 2025-05-29 23:47:23.000 UTC</summary>

Hi @gauravthadani Thanks for reporting!

Yes, we are aware of this issue and currently working on it. There will be a PR for this soon.
cc @alexshtin @stephanos 

</details>

<details>
<summary><strong>stephanos</strong> commented on 2025-06-26 23:21:02.000 UTC</summary>

Note that [the fix was merged](https://github.com/temporalio/temporal/pull/7732) but [subsequently reverted again](https://github.com/temporalio/temporal/pull/7878). Taking another stab at this soon.

Reactions: üëç 1

</details>

<details>
<summary><strong>sethlurietfg</strong> commented on 2025-11-18 12:52:53.000 UTC</summary>

Hey there. We are still getting this problem. Any chance of trying to get this fixed again?

</details>


---

### #7640: Unable to use passthrough:/// prefix in temporal operator command

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7640 |
| **State** | OPEN |
| **Author** | SakshiMehta |
| **Created** | 2025-04-22 06:22:16.000 UTC (8 months ago) |
| **Updated** | 2025-05-13 16:21:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
We are setting up multi-cluster replication in Temporal, which involves configuring clusterMetadata and setting the rpcAddress for each remote cluster. In our setup, we want to explicitly specify the passthrough:/// prefix in the rpcAddress, so the gRPC client uses the passthrough resolver behavior intentionally.

This is a valid gRPC URI format supported by grpc-go, and we expect Temporal to handle addresses with the passthrough:/// prefix in clusterInformation.rpcAddress without issue.

## Actual Behavior
When the rpcAddress is set to a value like passthrough:///host:port, operations that use this value ‚Äî such as temporal operator cluster upsert ‚Äî it still goes to default dns resolver

We think this is happening because the implementation of CreateRemoteFrontendGRPCConnection uses net.SplitHostPort to extract the hostname from the rpcAddress.
We observed that using the same prefix was working for temporal health command because it was not splitting the URI
Relevant code snippet from Temporal:

```
hostname, _, err2 := net.SplitHostPort(rpcAddress)
if err2 != nil {
    d.logger.Fatal("Invalid rpcAddress for remote cluster", tag.Error(err2))
}
```
The net.SplitHostPort function expects a basic host:port format and fails when the address includes a URI-style scheme like passthrough:///.

## Steps to Reproduce the Problem
Run `temporal operator cluster upsert` to connect to the remote cluster using address as `passthrough:///host:port`

The operation still resolves to default dns

#### Comments (3)

<details>
<summary><strong>bergundy</strong> commented on 2025-05-01 23:02:27.000 UTC</summary>

FTR the code is located here:

https://github.com/temporalio/temporal/blob/bb507313febe5a2cb509f5aa46918d9587b086f4/common/rpc/rpc.go#L218.

I'm not sure what other locations in the code would need to change to accept `passthrough:///` URLs.
We won't prioritize this work in the immediate future but would be happy to accept a contribution.

</details>

<details>
<summary><strong>SakshiMehta</strong> commented on 2025-05-09 10:35:09.000 UTC</summary>

Created a [PR](https://github.com/temporalio/temporal/pull/7744), please review

</details>

<details>
<summary><strong>SakshiMehta</strong> commented on 2025-05-13 16:21:13.000 UTC</summary>

@bergundy please review the changes for supporting url with prefix 

</details>


---

### #7637: Add a dedicated metric for corrupted workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7637 |
| **State** | OPEN |
| **Author** | phuongdnguyen (D.Phuong Nguyen) |
| **Created** | 2025-04-20 02:00:53.000 UTC (8 months ago) |
| **Updated** | 2025-07-25 00:49:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
In our environment when there millions of workflow are running, when there are corrupted workflows due to database layer, it's hard to know which workflows are corrupted.

For the status quo of temporal, corrupted workflows are either:
- Logged & counted in namespace-level metric ( `ConditionFailedError `, `CurrentWorkflowConditionFailedError`, `WorkflowConditionFailedError`)
- Logged only without workflow info (for `DataLoss` error)

Temporal already have a protection for this case by [trimming](https://github.com/temporalio/temporal/blob/d851d4a92e36bb9ad3be949daa59efa3562a08cf/common/persistence/execution_manager.go#L241) history branch to last valid node. However, when corruption happens on database side, trimming history node won't work (we have case where workflows are corrupted because of `ConditionFailedError`, current execution record is gone from C*)
**Describe the solution you'd like**
Adding a new metrics (eg: `workflow_corrupted`) with label of workflowID and runID. This is more efficient to detect problematic workflows rather than running a full db scan using something like `tdbg`.

I can help to come up with a PR if you guys think it's useful.

**Describe alternatives you've considered**
Parsing log, which i think need more compute power than proactively expose information in Temporal server itself.

**Additional context**
N/A


#### Comments (3)

<details>
<summary><strong>yycptt</strong> commented on 2025-07-10 19:28:12.000 UTC</summary>

Looks like there's some misunderstanding re. the error type. `ConditionFailedError`, `CurrentWorkflowConditionFailedError`, `WorkflowConditionFailedError` are expected errors and don't mean workflows are corrupted. They can happen during history shard movement or trying to start an existing workflow. 

DataLoss errors yes, signals one corruption type that we are able to detect & well known, in this case the corruption of history event batches. Other types of corruption, for example resurrected workflows/activity/child workflow info, missing current workflow record etc. can't even be detected without introducing a lot more operations to the normal hot code path and that's not something we can afford.
Putting it another way, if we can detect the corrupted workflows (especially on the workflow state side), we can go ahead and fix those corruptions (by replaying the history events) and not emit the corruption metric.

Due to cardinality reasons, we also can't emit metric with workflowID & runID tags, they are too expensive for metric backends.
For now, error log is the probably the best way to figure out workflowID & runIDs.

</details>

<details>
<summary><strong>phuongdnguyen</strong> commented on 2025-07-11 03:06:25.000 UTC</summary>

Hi @yycptt , thanks for the response.
It's been a while and we've go ahead to add metrics in our internal fork.
So we found several combinations indicator of corruption:
- `ConditionFailedError` when Temporal do a workflow execution update (the api in persistence layer)
- DataLoss Error as you've mentioned.
About `CurrentWorkflowConditionFailedError` and `WorkflowConditionFailedError`, yes we see it's a transient error that usually related to new workflow start or when a lot of workflows doing continueasnew

Moving forward, do you think we need a system workflow (like the existing ones registered for internal worker) to do scanning for corruptions? Something like what `tdbg admin db scan` do but in a more controlled way


</details>

<details>
<summary><strong>yycptt</strong> commented on 2025-07-25 00:49:24.000 UTC</summary>

hmm it's probably true today that for the `ExecutionManager/Store` that `ConditionFailedError` signals corruption, but we won't be able to guarantee that and may use `ConditionFailedError` in other transient cases in the future. Matching task store and queueStore are already doing that.

> tdbg admin db scan

I guess you mean `tctl admin db scan`? `tctl` is being deprecated and I don't think that command is available in tdbg right now.    I believe we have the cli script version first and later moved to a proper system workflow implementation (the more controlled way) for doing the scanning, so the tctl command shouldn't be used. 
Since that scanner workflow is also a workflow, I believe you can simply trigger it with normal workflow start cli command and use `temporal-system` as the namespace. I don't recall if we automatically start those scanner workflows on system worker start by default, likely no.

</details>


---

### #7623: Feature: Delayed schedule for activity tasks

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7623 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2025-04-16 23:07:43.000 UTC (8 months ago) |
| **Updated** | 2025-04-17 22:11:53.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

Ability to define a delay in scheduling activity task.

Why
Currently alternatives like using workflow.Sleep add extra actions

#### Comments (1)

<details>
<summary><strong>yycptt</strong> commented on 2025-04-17 22:11:52.000 UTC</summary>

One approach to support this is reuse today's activity retry timer for the first attempt as well if there's a delay.

</details>


---

### #7560: Default JWT claim mapper support for permissions parsing using regular expression

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7560 |
| **State** | OPEN |
| **Author** | adamko147 (Adam Horacek) |
| **Created** | 2025-04-03 05:21:20.000 UTC (9 months ago) |
| **Updated** | 2025-05-29 17:31:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
We're currently using self hosted temporal server with custom JWT auth plugin as described at https://docs.temporal.io/self-hosted-guide/security#plugins
The only reason we need to maintain custom auth plugin is missing claim mapper configuration to parse permissions from claims. Unfortunately we don't have an option to configure JWT issuer to pass permissions in form `namespace:role` as expected by default JWT claim mapper. 

**Describe the solution you'd like**
Default JWT authorization included in temporal server is almost sufficient for our use case (including issuer and audience validation), except for configuration how to parse permissions from JTW claims. An option for configuration `permissionPattern` in form of regular expression with named groups would cover our use case (as well as others potentially) and avoid overhead related to maintaining customer auth plugin and therefore significantly simplifying our setup.

**Suggested solution**
Add `PermissionsPattern` into `Authorization` Config. If configured, it would be used to parse the claim in defaultJWTClaimMapper.extractPermissions. If not configured, fall back to original code. An example of such pattern could be 
- `prefix-i-cannot-change\.(?P<namespace>\w+)\.(?P<role>\w+)`, or 
- `(?P<namespace>\w+)\.(?P<role>\w+)` to use `.` as separator (instead of `:` because that character is not allowed by JWT issuer), etc...

I'm pretty ok with go and I would like to contribute by raising a PR with the new code if the owners/community agree this is viable option.

**Describe alternatives you've considered**
As mentioned above, authorization plugin is an alternative, but it requires maintaining custom temporal build, additional CI/CD steps to build the code, create docker image, etc...

Thank you,
Adam

PS: And yes, for our use case, I'm ok to sacrifice performance of strings.Split vs regex.FindStringSubmatch for the benefit of not maintaining our temporal build :)


#### Comments (3)

<details>
<summary><strong>yycptt</strong> commented on 2025-04-03 22:46:45.000 UTC</summary>

Hi @adamko147, thanks for the proposal!

I will bring this issue to team and get relevant folks involved to review the approach described. 

</details>

<details>
<summary><strong>adamko147</strong> commented on 2025-04-04 22:17:53.000 UTC</summary>

> Hi [@adamko147](https://github.com/adamko147), thanks for the proposal!
> 
> I will bring this issue to team and get relevant folks involved to review the approach described.

Thank you @yycptt. Meanwhile I've created #7574 with initial implementation. Early feedback from community/maintainers is very welcome! I'm also looking around the code base for places to adjust (e.g. helm charts) and trying to bring the change into our self hosted environment to verify with functional testing (in addition to unit tests included in the pull request)

</details>

<details>
<summary><strong>adamko147</strong> commented on 2025-05-29 17:31:06.000 UTC</summary>

Hi @yycptt did you have a chance to discuss this issue with team already? Thanks in advance for any update on this.

</details>


---

### #7551: Atomic workflow reset & update

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7551 |
| **State** | OPEN |
| **Author** | boonsuen (Boonsuen Oh) |
| **Created** | 2024-12-07 14:36:29.000 UTC (1 years ago) |
| **Updated** | 2025-10-15 19:51:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
- The use case is to reset the workflow to a previous point and immediately sends a update request to update the state. 
- Currently, reset & update cannot be applied together. Update can happen milliseconds~second later after reset. There could be a race that when the code rely on the updated state after reset immediately, but the update request is not guaranteed to be applied atomically with reset, resulting in the reset-ed workflow execution continues with the "old" state. 

**Describe the solution you'd like**
Provide a API, preferably something like ResetWithUpdate(...) to reset to a point of event with the update request fulfilled.

**Describe alternatives you've considered**
Using workflow.Await but the approach comes with extra flags to control the logic flow.

**Additional context**
- Slack source: https://temporalio.slack.com/archives/CTDTU3J4T/p1733236562254059


#### Comments (3)

<details>
<summary><strong>Quinn-With-Two-Ns</strong> commented on 2024-12-17 11:43:59.000 UTC</summary>

Moving to features since this would apply to all SDK's and would also require server support

</details>

<details>
<summary><strong>Sushisource</strong> commented on 2025-04-01 17:28:33.000 UTC</summary>

Since this is a large request that would require substantial server work, transferring to server for triage

</details>

<details>
<summary><strong>ichinna</strong> commented on 2025-10-15 19:50:55.000 UTC</summary>

Any update on this?
@Sushisource @Quinn-With-Two-Ns 

</details>


---

### #6633: [Feature Request] Support exponential/native histograms in Temporal Server/SDKs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6633 |
| **State** | OPEN |
| **Author** | gregbrowndev (Greg Brown) |
| **Created** | 2024-10-09 18:58:04.000 UTC (1y 2m ago) |
| **Updated** | 2024-12-20 14:21:01.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**

Temporal Server is fairly expensive to monitor in self-hosted environments due to the volume of metric series generated. Observability platforms, such as AWS CloudWatch Metrics, Grafana Cloud, etc. charge per active metric series so the costs quickly add up.

Prometheus has experimental support for native histograms, and stability is improving daily. One of the main advantages of native histograms over Prometheus' classic histograms is that they can store the same data with fewer metric series and higher accuracy/resolution.

The presenter in the YouTube video "Prometheus Native Histograms in Production - Bj√∂rn Rabenstein, Grafana Labs" at 17:30 states: "bottom line is you get 10x the resolution at half the price". That infographic also shows the number of series is ~16k compared to ~1k for classic vs native histograms in his example, respectively. Because you only need a single series to store the whole histogram (for a given set of labels).

For teams deploying a new Temporal installation, having the option to export exponential histograms would be great, as we can save costs and we since don't have extensive dashboards/alerting/SRE based on the old metric names, we can quickly build out the SRE on the new native histograms.

**Describe alternatives you've considered**

I'm using Grafana Alloy specifically to scrape the Temporal Server metrics. When I enable the option to scrape native histograms, no native histograms are scraped. The SDK metrics emitted using the OTel config are emitted as classic histograms. I believe this needs to be updated in the Server/SDK code.

**Additional context**
Add any other context or screenshots about the feature request here.


#### Comments (1)

<details>
<summary><strong>gregbrowndev</strong> commented on 2024-12-20 14:21:00.000 UTC</summary>

For future reference, there is another discussion on this topic on Slack:

https://temporalio.slack.com/archives/CTT84RS0P/p1734356128801839
 

</details>


---

### #6546: Allow to disable workflow deadlock detector

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6546 |
| **State** | OPEN |
| **Author** | itayd (Itay Donanhirsh) |
| **Created** | 2024-09-24 01:52:52.000 UTC (1y 3m ago) |
| **Updated** | 2024-10-02 07:39:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

We need to run some function that might block for a while (up to 10s, usually) during a Temporal workflow. That function cannot receive a Temporal workflow context. It also has to run every time the workflow runs, including replays. Further operations during this workflow rely on this function being run first.

Currently when we do this, the deadlock detection mechanism panics.

**Describe the solution you'd like**

Allow to disable the deadlock detection for a specific scope. Possible just making https://github.com/temporalio/sdk-go/blob/master/internal/workflow_deadlock.go#L51 and its friend public.

**Describe alternatives you've considered**

We tried running the function in a separate go routine, and while it's running sleeping on Temporal. The problem is that this is non-deterministic, and we don't know how much Sleep we need to get.

**Additional context**

I initially really really really didn't want to open this issue. I really think that the deadlock detection is a Good Thing. I discussed this with @mfateev and Chad (no idea what's his alias here), and at the end came to a conclusion that there's no avoiding this.

@mfateev suggested (ab)using the `DataConverterWithoutDeadlockDetection`, but I'd rather do it in a more polite fashion. 

#### Comments (3)

<details>
<summary><strong>itayd</strong> commented on 2024-09-24 02:09:29.000 UTC</summary>

Just for fun, this is what I can do to abuse the data convertor:

```go

import (
	"context"

	commonpb "go.temporal.io/api/common/v1"
	"go.temporal.io/sdk/workflow"
)

type deadlockDetectorAbuser func()

func (deadlockDetectorAbuser) ToPayload(interface{}) (*commonpb.Payload, error)      { return nil, nil }
func (deadlockDetectorAbuser) FromPayload(*commonpb.Payload, interface{}) error      { return nil }
func (deadlockDetectorAbuser) ToPayloads(...interface{}) (*commonpb.Payloads, error) { return nil, nil }
func (deadlockDetectorAbuser) FromPayloads(*commonpb.Payloads, ...interface{}) error { return nil }
func (deadlockDetectorAbuser) ToString(*commonpb.Payload) string                     { return "" }

func (dd deadlockDetectorAbuser) ToStrings(*commonpb.Payloads) []string {
	dd()
	return nil
}

func WithoutDeadlockDetection(ctx workflow.Context, f func())  {
	cvt := workflow.DataConverterWithoutDeadlockDetection(deadlockDetectorAbuser(f))
	cvt = cvt.(workflow.ContextAware).WithWorkflowContext(ctx)
	_ = cvt.ToStrings(nil)
}
```

edit: need to supply workflow context to the converter.

</details>

<details>
<summary><strong>itayd</strong> commented on 2024-09-24 02:14:26.000 UTC</summary>

tbh, it's so ugly i kinda like it.

</details>

<details>
<summary><strong>itayd</strong> commented on 2024-09-24 03:45:46.000 UTC</summary>

Proposed https://github.com/temporalio/sdk-go/pull/1647.

</details>


---

### #6375: CompleteUpdate message is sometimes not honored when in same WFT completion as ContinueAsNew

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6375 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-08-06 16:25:17.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-12 00:23:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
CompleteUpdate command should be honored when submitted in the same workflow task completion as ContinueAsNew.

## Actual Behavior

Occasionally, there is a server error and it is not honored, and the update is applied to the post-ContinueAsNew run.

I have a workflow that CANs in the same WFT as an update is completed.
I ensure that the update is sent in the first WFT and, most of the time, I see what I expect:

1. worker receives WFT: `[doUpdate, startWorkflow]`
2. worker sends WFT completion: `[accept&completeUpdate, CAN]`
3. update caller gets successful update response
4. worker receives WFT after CAN: `[startWorkflow]`
5. workflow completes

However, sometimes (~1/20) I see

1. \<as above\>
2. \<as above>
3. server error when handling UpdateWorkflowExecution: [`unable to locate current workflow execution`](https://github.com/temporalio/temporal/blob/99f15c4dabf9a22763aaa13653c83a832bd0fffa/service/history/statemachine_environment.go#L167) (update caller does not get a response)
4. WFT after CAN contains the update for a second time, in addition to startWorkflow

I assume what's happening is that the server error causes an internal retry (FE => History) of the UpdateWorkflowExecution, and this retry lands on the 2nd run, after CAN. But I haven't yet dug into why the server error happens
```
	// for close workflow we need to check if it is still the current run
	// since it's possible that the workflowID has a newer run before it's locked 

wcache.GetCurrentRunID(...) != wfContext.GetWorkflowKey().RunID
```


## Steps to Reproduce the Problem

This was discovered while writing tests for `sdk-typescript`. In lieu of a simpler repro, it can be repro'd as follows

<details>
<summary>steps to repro</summary>

```
git clone git@github.com:temporalio/sdk-typescript.git
cd sdk-typescript
git fetch origin  1459-server-error-repro
git checkout 1459-server-error-repro
git submodule update --init --recursive

# Build temporal CLI against server 99f15c4dabf9a22763aaa13653c83a832bd0fffa
# Look at last commit, modify to use path to your temporal CLI executable

rustup update
npm install
npm run build
for i in `seq 1 100`; do (cd packages/test && npm run test -- -m 'unfinished update handler with continue-as-new waiting for all handlers to finish') || break; done
```
</details>





<details>
<summary>Log in happy case</summary>

```
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(a12a0761-d96e-4111-973b-1e54027baaed)] üü† job: doUpdate {"doUpdate":{"id":"update-id","protocolInstanceId":"update-id","name":"unfinishedHandlersWorkflowTermina
tionTypeUpdate","meta":{"updateId":"update-id","identity":"57045@dan-2.local"},"runValidator":true}}
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(a12a0761-d96e-4111-973b-1e54027baaed)] üü† job: startWorkflow {"startWorkflow":{"workflowType":"runUnfinishedHandlersWorkflowTerminationTypeWorkflow","workflowId
":"a12a0761-d96e-4111-973b-1e54027baaed","arguments":[{"metadata":{"encoding":"anNvbi9wbGFpbg=="},"data":"ImNvbnRpbnVlLWFzLW5ldyI="},{"metadata":{"encoding":"anNvbi9wbGFpbg=="},"data":"IndhaXQtYWxsLWhhbmRsZXJzLWZpb
mlzaGVkIg=="}],"randomnessSeed":"3270138216709017763","identity":"57045@dan-2.local","workflowTaskTimeout":{"seconds":"10"},"lastCompletionResult":{},"firstExecutionRunId":"8a25effa-1499-46b6-be24-0a3d23c1fb66","at
tempt":1,"cronScheduleToScheduleInterval":{},"memo":{},"startTime":{"seconds":"1722604980","nanos":463150000}}}
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(a12a0761-d96e-4111-973b-1e54027baaed)] üü¢ activationCompletion: [{"updateResponse":{"protocolInstanceId":"update-id","accepted":{}}},{"updateResponse":{"protoco
lInstanceId":"update-id","completed":{"metadata":{"encoding":{"0":106,"1":115,"2":111,"3":110,"4":47,"5":112,"6":108,"7":97,"8":105,"9":110}},"data":{"0":34,"1":117,"2":112,"3":100,"4":97,"5":116,"6":101,"7":45,"8"
:114,"9":101,"10":115,"11":117,"12":108,"13":116,"14":34}}}},{"continueAsNewWorkflowExecution":{"workflowType":"runUnfinishedHandlersWorkflowTerminationTypeWorkflow","arguments":[{"metadata":{"encoding":{"0":106,"1
":115,"2":111,"3":110,"4":47,"5":112,"6":108,"7":97,"8":105,"9":110}},"data":{"0":34,"1":114,"2":101,"3":116,"4":117,"5":114,"6":110,"7":34}}],"headers":{},"taskQueue":"unfinished_update_handler_with_continue-as-ne
w_waiting_for_all_handlers_to_finish","versioningIntent":0}}]
‚úÖ caller got update result
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(a12a0761-d96e-4111-973b-1e54027baaed)] üü† job: startWorkflow {"startWorkflow":{"workflowType":"runUnfinishedHandlersWorkflowTerminationTypeWorkflow","workflowId
":"a12a0761-d96e-4111-973b-1e54027baaed","arguments":[{"metadata":{"encoding":"anNvbi9wbGFpbg=="},"data":"InJldHVybiI="}],"randomnessSeed":"17192061066528283883","workflowRunTimeout":{},"workflowTaskTimeout":{"seco
nds":"10"},"continuedFromExecutionRunId":"8a25effa-1499-46b6-be24-0a3d23c1fb66","continuedInitiator":"CONTINUE_AS_NEW_INITIATOR_WORKFLOW","lastCompletionResult":{},"firstExecutionRunId":"8a25effa-1499-46b6-be24-0a3
d23c1fb66","attempt":1,"cronScheduleToScheduleInterval":{"nanos":501851000},"memo":{},"startTime":{"seconds":"1722604980","nanos":938805000}}}
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(a12a0761-d96e-4111-973b-1e54027baaed)] üü¢ activationCompletion: [{"completeWorkflowExecution":{"result":{"metadata":{"encoding":{"0":98,"1":105,"2":110,"3":97,"
4":114,"5":121,"6":47,"7":110,"8":117,"9":108,"10":108}}}}}]
```

</details>


<details>
<summary>Log in unhappy case</summary>

```
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(f1f37cd3-1c11-4aa7-ba58-f12f8880f594)] üü† job: doUpdate {"doUpdate":{"id":"update-id","protocolInstanceId":"update-id","name":"unfinishedHandlersWorkflowTermina
tionTypeUpdate","meta":{"updateId":"update-id","identity":"57075@dan-2.local"},"runValidator":true}}
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(f1f37cd3-1c11-4aa7-ba58-f12f8880f594)] üü† job: startWorkflow {"startWorkflow":{"workflowType":"runUnfinishedHandlersWorkflowTerminationTypeWorkflow","workflowId
":"f1f37cd3-1c11-4aa7-ba58-f12f8880f594","arguments":[{"metadata":{"encoding":"anNvbi9wbGFpbg=="},"data":"ImNvbnRpbnVlLWFzLW5ldyI="},{"metadata":{"encoding":"anNvbi9wbGFpbg=="},"data":"IndhaXQtYWxsLWhhbmRsZXJzLWZpb
mlzaGVkIg=="}],"randomnessSeed":"3129644504916450612","identity":"57075@dan-2.local","workflowTaskTimeout":{"seconds":"10"},"lastCompletionResult":{},"firstExecutionRunId":"d47f00b8-1462-4a45-821d-5e72c853f631","at
tempt":1,"cronScheduleToScheduleInterval":{},"memo":{},"startTime":{"seconds":"1722604985","nanos":101629000}}}
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(f1f37cd3-1c11-4aa7-ba58-f12f8880f594)] üü¢ activationCompletion: [{"updateResponse":{"protocolInstanceId":"update-id","accepted":{}}},{"updateResponse":{"protoco
lInstanceId":"update-id","completed":{"metadata":{"encoding":{"0":106,"1":115,"2":111,"3":110,"4":47,"5":112,"6":108,"7":97,"8":105,"9":110}},"data":{"0":34,"1":117,"2":112,"3":100,"4":97,"5":116,"6":101,"7":45,"8"
:114,"9":101,"10":115,"11":117,"12":108,"13":116,"14":34}}}},{"continueAsNewWorkflowExecution":{"workflowType":"runUnfinishedHandlersWorkflowTerminationTypeWorkflow","arguments":[{"metadata":{"encoding":{"0":106,"1
":115,"2":111,"3":110,"4":47,"5":112,"6":108,"7":97,"8":105,"9":110}},"data":{"0":34,"1":114,"2":101,"3":116,"4":117,"5":114,"6":110,"7":34}}],"headers":{},"taskQueue":"unfinished_update_handler_with_continue-as-ne
w_waiting_for_all_handlers_to_finish","versioningIntent":0}}]
time=2024-08-02T09:23:05.598 level=ERROR msg="service failures" operation=UpdateWorkflowExecution wf-namespace=default error="unable to locate current workflow execution"
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(f1f37cd3-1c11-4aa7-ba58-f12f8880f594)] üü† job: doUpdate {"doUpdate":{"id":"update-id","protocolInstanceId":"update-id","name":"unfinishedHandlersWorkflowTermina
tionTypeUpdate","meta":{"updateId":"update-id","identity":"57075@dan-2.local"},"runValidator":true}}
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(f1f37cd3-1c11-4aa7-ba58-f12f8880f594)] üü† job: startWorkflow {"startWorkflow":{"workflowType":"runUnfinishedHandlersWorkflowTerminationTypeWorkflow","workflowId
":"f1f37cd3-1c11-4aa7-ba58-f12f8880f594","arguments":[{"metadata":{"encoding":"anNvbi9wbGFpbg=="},"data":"InJldHVybiI="}],"randomnessSeed":"17700601131409760754","workflowRunTimeout":{},"workflowTaskTimeout":{"seco
nds":"10"},"continuedFromExecutionRunId":"d47f00b8-1462-4a45-821d-5e72c853f631","continuedInitiator":"CONTINUE_AS_NEW_INITIATOR_WORKFLOW","lastCompletionResult":{},"firstExecutionRunId":"d47f00b8-1462-4a45-821d-5e7
2c853f631","attempt":1,"cronScheduleToScheduleInterval":{"nanos":566360000},"memo":{},"startTime":{"seconds":"1722604985","nanos":513977000}}}
[runUnfinishedHandlersWorkflowTerminationTypeWorkflow(f1f37cd3-1c11-4aa7-ba58-f12f8880f594)] üü¢ activationCompletion: [{"updateResponse":{"protocolInstanceId":"update-id","accepted":{}}},{"completeWorkflowExecution
":{"result":{"metadata":{"encoding":{"0":98,"1":105,"2":110,"3":97,"4":114,"5":121,"6":47,"7":110,"8":117,"9":108,"10":108}}}}}]
```

</details>



## Specifications

  - Version: Server built from latest `main` 99f15c4dabf9a22763aaa13653c83a832bd0fffa


#### Comments (3)

<details>
<summary><strong>dandavison</strong> commented on 2024-08-09 03:42:43.000 UTC</summary>

It seems that there are a total of 3 `UpdateWorkflowRequests` made to History service.

- The first request is delivered to the worker, but is ultimately failed as it is completing, and is retried by the Frontend service.
- The second fails immediately on entry to History service and is retried by the Frontend service.
- The final request lands on the post-CAN run and thus is treated as a distinct update (executed a second time by a worker).

```
UpdateWorkflowExecution
namespace_id:"98d00113-d1ae-4623-8b71-f24f5aac51cd"  request:{namespace:"default"  workflow_execution:{workflow_id:"f95ace75-8b2e-4c78-933c-8b813e4c7b48"}  first_execution_run_id:"d0442481-44d1-451b-9f14-50ad7d63b2bb"  wait_policy:{lifecycle_stage:UPDATE_WORKFLOW_EXECUTION_LIFECYCLE_STAGE_COMPLETED}  request:{meta:{update_id:"update-id"  identity:"66459@dan-2.local"}  input:{header:{}  name:"unfinishedHandlersWorkflowTerminationTypeUpdate"  args:{}}}}
<nil>, Workflow Update was aborted.

UpdateWorkflowExecution
namespace_id:"98d00113-d1ae-4623-8b71-f24f5aac51cd"  request:{namespace:"default"  workflow_execution:{workflow_id:"f95ace75-8b2e-4c78-933c-8b813e4c7b48"}  first_execution_run_id:"d0442481-44d1-451b-9f14-50ad7d63b2bb"  wait_policy:{lifecycle_stage:UPDATE_WORKFLOW_EXECUTION_LIFECYCLE_STAGE_COMPLETED}  request:{meta:{update_id:"update-id"  identity:"66459@dan-2.local"}  input:{header:{}  name:"unfinishedHandlersWorkflowTerminationTypeUpdate"  args:{}}}}
<nil>, unable to locate current workflow execution

UpdateWorkflowExecution
namespace_id:"98d00113-d1ae-4623-8b71-f24f5aac51cd"  request:{namespace:"default"  workflow_execution:{workflow_id:"f95ace75-8b2e-4c78-933c-8b813e4c7b48"}  first_execution_run_id:"d0442481-44d1-451b-9f14-50ad7d63b2bb"  wait_policy:{lifecycle_stage:UPDATE_WORKFLOW_EXECUTION_LIFECYCLE_STAGE_COMPLETED}  request:{meta:{update_id:"update-id"  identity:"66459@dan-2.local"}  input:{header:{}  name:"unfinishedHandlersWorkflowTerminationTypeUpdate"  args:{}}}}
<nil>, workflow execution already completed <-- this happens because of the nature of the Typescript test being used.
```

</details>

<details>
<summary><strong>mfateev</strong> commented on 2024-08-09 17:42:11.000 UTC</summary>

Wouldn't the same happen with a signal?

</details>

<details>
<summary><strong>alexshtin</strong> commented on 2024-08-12 00:23:44.000 UTC</summary>

It is kinda similar observable effect but probability to face it in real life are different, and I have hard times to understand which one is more likely.

Signals can be delivered twice when event was successfully written into the history by history service, **and** then it crashed w/o replying to frontend, or frontend service lost connection to history service, and workflow, while receiving and processing this signal replies with CAN, and then frontend, or API caller retries and 2nd attempt lands on 2nd run. Something really bad should happen on the server to trigger it.

Updates are different in a sense that **any** error with workflow (in the @dandavison's case it was matching tried to start workflow task 2nd time) leads to lost update, and trigger frontend retry. Retries for updates are part of design: server relies on retries to recreate update after those errors. This sounds like much more likely to happen. With one caveat though: updates are mostly delivered with speculative workflow task, and this task is also lost with any error. It means that server wouldn't process update acceptance for the 1st run. Update can go on normal workflow task only if it piggybacks existing workflow task created by something else (this is exactly what happened in @dandavison`s case where update was delivered on first workflow task). This fact, seems to me, significantly reduce chances of facing this issue in real life.

</details>


---

### #5877: Address force completion when make a request through CompleteByID with a failure.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5877 |
| **State** | OPEN |
| **Author** | alexseedkou (Jingcheng Kou) |
| **Created** | 2024-05-08 01:20:17.000 UTC (1y 7m ago) |
| **Updated** | 2024-05-08 15:36:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
This is a follow-up feature of this [issue](https://github.com/temporalio/temporal/issues/987).

For an async call, the activity may fail due to a bug or design drawback after a call to an external server. However, temporal server may receive a request from `CompleteByID` API by other servers while the new attempt for a retryable error of the activity is not started yet. In this case, if the request is to complete the activity, we would complete the activity even the new attempt has not started yet so that we can unblock the workflow (refer this [PR](https://github.com/temporalio/temporal/pull/5724)). However, if the request is to fail the activity, we may think an appropriate way to handle such cases as we are not sure the failure is to fail the activity or it is a transit error and we want to attempt the activity again.

**Describe the solution you'd like**
For the request to fail an activity:
1) If the request is to force fail an activity, we should fail the activity if the attempt for a retryable error has not started yet.
2) If the request is to fail an activity due to a non-retryable error, we should fail the activity.

**Describe alternatives you've considered**
We may introduce a separate API or a flag for a client to tell the server that it would like a request to force fail the activity.

**Additional context**
Add any other context or screenshots about the feature request here.


#### Comments (3)

<details>
<summary><strong>bergundy</strong> commented on 2024-05-08 13:38:44.000 UTC</summary>

Let me take a stab at the explanation.

[This PR](https://github.com/temporalio/temporal/pull/5724) now allows completing an activity by ID when an activity is backing off between attempts.

There are two other APIs for resolving an activity that we considered changing the behavior for while an activity is backing off: `RespondActivityTaskFailedById` and `RespondActivityTaskCanceledById`.

For `RespondActivityTaskCanceledById` there's no need to do anything because the server immediately resolves the activity as canceled.
For `RespondActivityTaskFailedById`, there are a couple of different cases:
- The failure is non-retryable (e.g. a non retryable ApplicationFailure, a failure that is matched in the retry policy's non retryable error types, and a failure reported in the last permitted attempt). In this case, we may want to allow unblocking the workflow and resolving the activity as failed.
- The failure is retryable. This call should IMHO be a noop to avoid wasting an activity attempt without having a chance to get started and triggering the next retry backoff.

As @alexseedkou we may want to consider an explicit flag to "force" the activity to fail, bypassing the retry policy, but I think using a non retryable application failure can already be used as that marker.

Reactions: üëç 2

</details>

<details>
<summary><strong>mjameswh</strong> commented on 2024-05-08 14:52:09.000 UTC</summary>

Just a note that once this done, we'll need to update API and SDKs documentation to reflect the fact that `RespondActivityTask*` and `RespondActivityTask*ById` have different behaviors in regard to in-backoff activities.

</details>

<details>
<summary><strong>mfateev</strong> commented on 2024-05-08 15:36:15.000 UTC</summary>

> The failure is retryable. This call should IMHO be a noop to avoid wasting an activity attempt without having a chance to get started and triggering the next retry backoff.

There is a feature request to retry activity immediately when waiting for the next retry. So we either introduce another method to force immediate retry or assume such a `Respond call` is such a request. 

</details>


---

### #5768: Metric specific to workflow retries per namespace

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5768 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2024-04-19 18:44:22.000 UTC (1y 8m ago) |
| **Updated** | 2024-05-06 20:42:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently we seem to only have workflow_retry_backoff_timer
available to check for count of workflow retries. 

Feature request is to add workflow_retried counter metric that would be able to be filtered by namespace.

Thanks.

#### Comments (3)

<details>
<summary><strong>yiminc</strong> commented on 2024-04-26 21:49:09.000 UTC</summary>

@tsurdilo how are user going to use this metrics? Do we need to tag the workflowID on the metrics? That will have cardinality issue.

</details>

<details>
<summary><strong>yiminc</strong> commented on 2024-04-26 21:49:39.000 UTC</summary>

The attempt count should also be available from within workflow. I think workflow.GetInfo() should return it already.

</details>

<details>
<summary><strong>clayzermk1</strong> commented on 2024-05-06 20:42:34.000 UTC</summary>

Hi folks, @tsurdilo was kind enough to make this issue from a [conversation we were having on Slack](https://temporalio.slack.com/archives/C05JRT1GKEE/p1713545472469399?thread_ts=1713534467.692749&cid=C05JRT1GKEE). Basically, we are in a situation where we manage the control plane, but not the workflows themselves. We would like to know the global retry count, ideally by namespace, at a given time.

</details>


---

### #5475: temporal frontend unable to connect to elasticsearch visibilitystore

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5475 |
| **State** | OPEN |
| **Author** | prashanthzen |
| **Created** | 2024-03-01 19:10:24.000 UTC (1y 10m ago) |
| **Updated** | 2024-10-08 14:45:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I have a temporal setup as following:
defaultstore: postgres
advancedVisibilityStore: es-visibility (the database behind es-visiblity is opensearch cluster)
I ran some workflows and I see the data going into the opensearch. But the temporal UI does not show any data. Neither do I see any error logs from the temporal pods.
The configmap for the temporal services look as following:
(.venv) temporal prashanth$ kubectl get cm ft1-mm-temporal-frontend-config -o yaml
apiVersion: v1
data:
  config_template.yaml: |-
    log:
      stdout: true
      level: "debug,info"

    persistence:
      defaultStore: default
      advancedVisibilityStore: es-visibility
      numHistoryShards: 512
      datastores:
        default:
          sql:
            pluginName: "postgres12"
            driverName: "postgres12"
            databaseName: "temporal"
            connectAddr: "db-console-pg.ft1.dev.xxx.com:5432"
            connectProtocol: "tcp"
            user: temporal
            password: "{{ .Env.TEMPORAL_STORE_PASSWORD }}"
            maxConnLifetime: 1h
            maxConns: 20
            secretName: ""
        visibility:
          sql:
            pluginName: "postgres12"
            driverName: "postgres12"
            databaseName: "temporal"
            connectAddr: "db-console-pg.ft1.dev.xxx.com:5432"
            connectProtocol: "tcp"
            user: "temporal"
            password: "{{ .Env.TEMPORAL_VISIBILITY_STORE_PASSWORD }}"
            maxConnLifetime: 1h
            maxConns: 20
            secretName: ""
        es-visibility:
            elasticsearch:
                version: "v7"
                url:
                    scheme: "https"
                    host: "es.mgmt.dev.xxx.com:443"
                username: "temporal_visibility"
                password: "<placeholder>"
                logLevel: "error"
                indices:
                    visibility: "temporal-visibility"

    global:
      membership:
        name: temporal
        maxJoinDuration: 30s
        broadcastAddress: {{ default .Env.POD_IP "0.0.0.0" }}

      pprof:
        port: 7936

      metrics:
        tags:
          type: frontend
        prometheus:
          timerType: histogram
          listenAddress: "0.0.0.0:9090"

    services:
      frontend:
        rpc:
          grpcPort: 7233
          membershipPort: 7933
          bindOnIP: "0.0.0.0"

      history:
        rpc:
          grpcPort: 7234
          membershipPort: 7934
          bindOnIP: "0.0.0.0"

      matching:
        rpc:
          grpcPort: 7235
          membershipPort: 7935
          bindOnIP: "0.0.0.0"

      worker:
        rpc:
          grpcPort: 7239
          membershipPort: 7939
          bindOnIP: "0.0.0.0"
    clusterMetadata:
      enableGlobalDomain: false
      failoverVersionIncrement: 10
      masterClusterName: "active"
      currentClusterName: "active"
      clusterInformation:
        active:
          enabled: true
          initialFailoverVersion: 1
          rpcName: "temporal-frontend"
          rpcAddress: "127.0.0.1:7933"
    dcRedirectionPolicy:
      policy: "noop"
      toDC: ""
    archival:
      status: "disabled"

    publicClient:
      hostPort: "ft1-mm-temporal-frontend:7233"

    dynamicConfigClient:
      filepath: "/etc/temporal/dynamic_config/dynamic_config.yaml"
      pollInterval: "10s"
UI:
![Screenshot 2024-02-29 at 3 39 19 PM](https://github.com/temporalio/temporal/assets/40897631/fd6e159d-9afd-4dba-8a37-cf47f9cb7d49)



#### Comments (3)

<details>
<summary><strong>yiminc</strong> commented on 2024-03-02 01:08:12.000 UTC</summary>

Can you try to remove all filters? Or try using temporal/CLI to see if you can list workflow from using the CLI. 


</details>

<details>
<summary><strong>prashanthzen</strong> commented on 2024-03-04 19:07:21.000 UTC</summary>

After changing the index name from temporal-visibility to temporal_visibility_v1_dev in the es-visibility visibilityStore configuration, the UI started working as expected.

</details>

<details>
<summary><strong>phuongdnguyen</strong> commented on 2024-10-08 14:45:42.000 UTC</summary>

@yiminc We've encountered this issue to when trying to plug 2 different temporal cluster on a same elasticsearch cluster with different index name. It's because pattern for template index is [fixed](https://github.com/temporalio/temporal/blob/06a8e35a96a349ab84f61571d74682a3f9817edd/schema/elasticsearch/visibility/versioned/v7/index_template_v7.json#L3) to match index with prefix of `temporal_visibility_v1`. Changing index to have name prefix of `temporal_visibility_v1` fix the issue for us. However, seems like it's a good chance to have this implication mentioned somewhere.

</details>


---

### #5436: 503 in api/v1/namespaces/<your_namespace>/search-attributes? sometime

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5436 |
| **State** | OPEN |
| **Author** | granescb (Sergey Anishenko) |
| **Created** | 2024-02-19 13:27:51.000 UTC (1y 10m ago) |
| **Updated** | 2025-10-10 03:39:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

200 http status
## Actual Behavior
http 503 sometime
![image](https://github.com/temporalio/temporal/assets/5940224/83778c75-a248-44cc-97fc-b6fcce5004dd)

## Steps to Reproduce the Problem

  1. Open "api/v1/namespaces/<your_namespace>/search-attributes?" in browser
  2. Open browser console
  3. Update page 20-30 times to get some errors 


## Specifications

  - Version: 1.22.4

The error from the component **temporal-frontend**:
`{"level":"error","ts":"2024-02-19T13:17:16.265Z","msg":"service failures","operation":"OperatorListSearchAttributes","wf-namespace":"les","error":"Unable to get namespace info with error: les","logging-call-at":"telemetry.go:341","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:341\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:174\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept\n\t/home/builder/temporal/service/frontend/redirection_interceptor.go:180\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/authorization.(*interceptor).Interceptor\n\t/home/builder/temporal/common/authorization/interceptor.go:158\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/metrics.NewServerMetricsContextInjectorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.42.0/interceptor.go:344\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_logger.go:84\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_validator.go:111\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1195\ngo.temporal.io/server/common/rpc.ServiceErrorInterceptor\n\t/home/builder/temporal/common/rpc/grpc.go:145\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1186\ngo.temporal.io/api/operatorservice/v1._OperatorService_ListSearchAttributes_Handler\n\t/go/pkg/mod/go.temporal.io/api@v1.24.1-0.20231003165936-bb03061759c8/operatorservice/v1/service.pb.go:302\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1376\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:1753\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.58.2/server.go:998"}`


#### Comments (3)

<details>
<summary><strong>granescb</strong> commented on 2024-02-19 13:50:00.000 UTC</summary>

Probably, the error is rising here: https://github.com/temporalio/temporal/blob/b670c79114d2dacd07bb53156281abe10e96473e/service/frontend/operator_handler.go#L518-L526
And there is no information about which exactly error happens in the logs, only namespace name.

</details>

<details>
<summary><strong>granescb</strong> commented on 2024-03-01 08:25:28.000 UTC</summary>

Fixed typo here https://github.com/temporalio/temporal/pull/5473

</details>

<details>
<summary><strong>Mahee777</strong> commented on 2025-10-10 03:39:22.000 UTC</summary>

@granescb this issues still there ?


</details>


---

### #5171: Timeout on complete when invalid task token passed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5171 |
| **State** | OPEN |
| **Author** | jontro (Jonas Trollvik) |
| **Created** | 2023-03-14 21:16:56.000 UTC (2y 9m ago) |
| **Updated** | 2023-11-29 15:29:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
The client should throw an exception immidiatley

## Actual Behavior
Calling activityCompletionClient.complete with an invalid task token, gives a timeout after a while instead of an error. 


## Steps to Reproduce the Problem

1.
```
    workflowClient.newActivityCompletionClient().complete(
        "bogus".toByteArray(), ""
    )
```
2. Timeout occurs after a while with the following exception:
`Caused by: io.grpc.StatusRuntimeException: UNKNOWN: unexpected EOF`


## Specifications

  - Version: [v1.20.0](https://github.com/temporalio/temporal/releases/tag/v1.20.0)
  - Platform: amd64


#### Comments (3)

<details>
<summary><strong>phongcao</strong> commented on 2023-05-30 15:46:05.000 UTC</summary>

This is not a bug of the temporal server. When we debugged, the server realized the invalid task token and returned the error immediately to the client. However, by design [the `grpcRetryer` implementation on the sdk-java](https://github.com/temporalio/sdk-java/blob/master/temporal-sdk/src/main/java/io/temporal/internal/client/external/ManualActivityCompletionClientImpl.java#LL111C20-L111C20) catches this error, keeps retrying until expiration and finally prints out the error after a couple minutes.
This implementation doesn't exist in other sdks such as: `sdk-go`, `sdk-typescript` and `sdk-python`.

</details>

<details>
<summary><strong>Quinn-With-Two-Ns</strong> commented on 2023-10-22 02:05:52.000 UTC</summary>

Checking the Go SDK it also returns a timeout so this is not a Java excl. If the server is returning an `UNKNOWN` status code then most SDKs should consider that retry-able. 

</details>

<details>
<summary><strong>Quinn-With-Two-Ns</strong> commented on 2023-10-22 22:58:32.000 UTC</summary>

Confirmed all SDKs consider `UNKNOWN` retry-able. So all SDKs will timeout on an invalid task token.

https://github.com/temporalio/sdk-typescript/blob/4dcc82aa48fc285119978bd2a93acc3a0e9fd231/packages/client/src/grpc-retry.ts#L100

https://github.com/temporalio/sdk-core/blob/45d2bc997fd25bf24d347b04d519e7279851aea4/client/src/retry.rs#L22

I think the simplest fix is for server to return a different status code like `INVALID_ARGUMENTS` in this case.

</details>


---

### #5135: Support more direct/immediate notification of a cancelled activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5135 |
| **State** | OPEN |
| **Author** | ghaskins (Gregory Haskins) |
| **Created** | 2023-11-19 16:45:35.000 UTC (2y 1m ago) |
| **Updated** | 2023-11-19 22:00:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

We have long-running activities that need to be canceled on occasion.  When the need arises, we typically confirm that the activity has been fully canceled before proceeding to another step related to the side effects that this activity may produce, such as performing IO to an external system.  Because of this dynamic, the cancellation has real-time synchronous implications on the orchestrating workflow, visible to a human driving a UI.

Because of this real-time dynamic, we use a relatively aggressive heartbeat strategy (currently 5-second heartbeats with 20-second heartbeat timeouts).  If the Temporal system delivered the ActivityCancelled exception at the first heartbeat post request, this would probably be acceptable latency.  However, things are compounded by the internal heartbeat suppression algorithm such that we only get the notification once we hit the 80% timeout threshold, in this case, around 15-20 seconds.

We could tune the heartbeat/timeout to be more aggressive (e.g., 1/5 seconds).  However, this is a lot of overhead for something that may run months at a time and only very rarely needs to be canceled. The feature proposal is to offer some mechanism for delivering cancelation notifications to an Activity.  This could be as simple as a call like "isCanceled()" that is not subject to the heartbeat optimizations, to some callback being registered that is invoked by the framework.



#### Comments (3)

<details>
<summary><strong>ghaskins</strong> commented on 2023-11-19 16:48:22.000 UTC</summary>

Slack reference: https://temporalio.slack.com/archives/CTT84KXK9/p1700401249875499

</details>

<details>
<summary><strong>Quinn-With-Two-Ns</strong> commented on 2023-11-19 17:21:47.000 UTC</summary>

>This could be as simple as a call like "isCanceled()" that is not subject to the heartbeat optimizations

This suggestion sounds equivalent to lowering your heartbeat timeout, you'd still have to call this `isCanceled` API rapidly to achieve your desired low latency, and pay for the overhead that would cause.  

I think, if the desire is low latency and low overhead on the worker, the server needs to somehow notify the worker that the activity is cancelled proactively without waiting for the worker to send a heartbeat request.

</details>

<details>
<summary><strong>Quinn-With-Two-Ns</strong> commented on 2023-11-19 22:00:47.000 UTC</summary>

Moving out of the Java SDK issues since this there isn't anything Java specific about this feature. Moving to the server issues since I think the primary effort would be to build some sort of push based notification from the server to the worker to notify it an activity it is working on is cancelled.

Reactions: üëç 1

</details>


---

### #4887: Prepopulate run_id in schedule future_action_times

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4887 |
| **State** | OPEN |
| **Author** | nnoto (Nicholas Noto) |
| **Created** | 2023-09-21 18:09:08.000 UTC (2y 3m ago) |
| **Updated** | 2023-10-02 14:57:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
I would like to track workflow executions via some unique identifier such as the run_id, including future workflow executions defined by a schedule. 

**Describe the solution you'd like**
Schedule information currently only provides a run_id for a workflow execution that has already begun. workflow_id isn't guaranteed to be unique when factoring in scheduled and on-demand workflows. Temporal documentation mentions a unique identifier of a workflow execution is the namespace + workflow_id + run_id.  Since future_action_times are provided for some portion of future schedule executions, it would be helpful to prepopulate the run_id as well is possible.

**Describe alternatives you've considered**
Including the schedule information (if applicable) that led to a workflow execution in the workflow description via list_workflows is helpful, but does not provide context for future workflow executions.

**Additional context**
Existing implementation: 
```
{  
    recent_actions {
        schedule_time {
            seconds: 1695312900
        }
        actual_time {
            seconds: 1695312900
            nanos: 405395930
        }
        start_workflow_result {
            workflow_id: "your-workflow-id-2023-09-21T16:15:00Z"
            run_id: "bae2a0b4-37b8-48f2-8a63-b6afc7d01b0f"
        }
    }
    future_action_times {
        seconds: 1695330900
    }
}
```
Proposed change:
```
{  
    recent_actions {
        schedule_time {
            seconds: 1695312900
        }
        actual_time {
            seconds: 1695312900
            nanos: 405395930
        }
        start_workflow_result {
            workflow_id: "your-workflow-id-2023-09-21T16:15:00Z"
            run_id: "bae2a0b4-37b8-48f2-8a63-b6afc7d01b0f"
        }
    }
    future_actions {
        workflow_id: <workflow_id>
        run_id: <workflow run_id>
        schedule_time {
            seconds: 1695330900
        }
    }
}
```


#### Comments (3)

<details>
<summary><strong>dnr</strong> commented on 2023-09-27 16:59:01.000 UTC</summary>

Currently `run_id` is generated at start time and I don't think we want to change that, so we can't predict the `run_id` ahead of time.

Can you use the workflow id (prefix + `-` + start time) or the value of the `TemporalScheduledStartTime` search attribute? The catch with using those is that those will use the time before jitter is added, while the time in `future_action_time` has the "actual" time including jitter. We could add a new field parallel to `future_action_time` that has the time without jitter so it'll match the workflow id 

</details>

<details>
<summary><strong>nnoto</strong> commented on 2023-09-29 18:42:50.000 UTC</summary>

@dnr Yes, using the workflow id is possible. The field parallel to future_action_time that has the time without jitter would be much appreciated if possible.

Has generating the run_ids based on the same method used for scheduled workflow ids (workflow prefix + - + start time) when the scheduled workflow executes been considered? Currently the run_ids seem to be some UUID regardless of whether the workflow was run on-demand or scheduled.
The way I understand workflows is that the workflow id is essentially just the identifier for the action/routine comprising a set of activities and can be used for many executions of the workflow, so the workflow prefix should suffice for the workflow id even in the context of scheduled workflows. Since run ids are unique and represent an execution of a workflow, using the start time suffix seems more appropriate for the run_ids. 

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-10-02 14:57:49.000 UTC</summary>

Run ids are explicitly represented as UUIDs in various database schemas so I'm afraid changing that isn't possible now. If it were a free-form string then yes, the timestamp would make sense. I think client-provided UUID run ids (where you could embed a timestamp, for example) could also violate certain assumptions in the rest of the system about uniqueness, so I don't think we can go there either.

The reason we put the timestamp in the workflow id itself instead of letting scheduled workflows reuse the workflow id is that schedules can be configured to allow scheduled workflows to run concurrently instead of enforcing exclusion. Only one workflow execution with the same workflow id can be running at once, and that's a basic invariant of Temporal.

That said, in configurations where we do enforce exclusion, we could reuse the workflow id (still with random run ids). That's requested in #4795.

</details>


---

### #4778: I want to be able to print log files separately by date

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4778 |
| **State** | OPEN |
| **Author** | guozongkang |
| **Created** | 2023-08-17 02:06:40.000 UTC (2y 4m ago) |
| **Updated** | 2023-10-05 23:42:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | potential-bug |
| **Assignees** | alexshtin |
| **Milestone** | None |

#### Description

At present, the logs are all printed in one file, which is difficult to handle

#### Comments (3)

<details>
<summary><strong>alexshtin</strong> commented on 2023-08-18 21:49:57.000 UTC</summary>

This can be helpful: https://dhwaneetbhatt.com/blog/time-based-log-file-rotation-with-zap. Would you like to contribute and send a PR?

</details>

<details>
<summary><strong>locustbaby</strong> commented on 2023-09-22 15:55:58.000 UTC</summary>

@alexshtin Hi, Alex, may I know if there is any PR about this issue, I'd like to try it out

</details>

<details>
<summary><strong>yiminc</strong> commented on 2023-10-05 23:42:48.000 UTC</summary>

@locustbaby there isn't any PR on this issue yet. Contribution is welcomed. 

</details>


---

### #4463: Cache event history from query tasks

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4463 |
| **State** | OPEN |
| **Author** | taonic (Tao Guo) |
| **Created** | 2023-06-09 01:47:23.000 UTC (2y 6m ago) |
| **Updated** | 2023-07-04 01:45:40.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, if a query task is delivered to a worker through a normal queue, the [event history](https://github.com/temporalio/api/blob/cfa1a15b960920a47de8ec272873a4ee4db574c4/temporal/api/workflowservice/v1/request_response.proto#L269) is discarded after handling the query task since [RespondQueryTaskCompletedRequest](https://github.com/temporalio/api/blob/master/temporal/api/workflowservice/v1/request_response.proto#L794-L801) does not carry any [sticky attributes](https://github.com/temporalio/api/blob/master/temporal/api/workflowservice/v1/request_response.proto#L301).

For query heavy workflows with large history, especially when codec is involved, discarding the payload seems to be wasteful, and paying latency penalty unnecessarily.

**Describe the solution you'd like**
I'd like to see sticky attributes to be added to RespondQueryTaskCompletedRequest or in any other ways to cache the event history on worker in order to handle subsequent queries more efficiently.

#### Comments (1)

<details>
<summary><strong>taonic</strong> commented on 2023-07-04 01:45:39.000 UTC</summary>

Just realized one workaround of the issue is to set a repeating Workflow timer with a short enough interval for reviving the workflow, which in turn rehydrates the cache and re-establishes the sticky queue.

</details>


---

### #4193: High frequency query mysql

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4193 |
| **State** | OPEN |
| **Author** | jixiezhixin |
| **Created** | 2023-04-20 03:02:45.000 UTC (2y 8m ago) |
| **Updated** | 2024-10-15 08:00:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description


Why does temporal call mysql so frequently, calling mysql more than 2000 times in 30 seconds, and it is under no-load conditions

Will high-frequency operations cause excessive pressure on mysql?

## Expected Behavior
nomral operate mysql

## Actual Behavior
Data are as follows
SELECT count(user_host) as count_user,user_host from mysql.general_log group by user_host order by count_user DESC;

¬±-----------¬±--------------------------------------------+
| count_user | user_host |
¬±-----------¬±--------------------------------------------+
| 36772 | temporal[temporal] @ [10.131.136.10] |
| 2625 | n9e_hbs[n9e_hbs] @ [10.131.136.10] |
| 788 | [temporal] @ [10.131.136.10] |
| 567 | root[root] @ localhost  |
| 406 | keystone[keystone] @ [10.131.136.20] |
| 306 | root[root] @ [10.131.136.10] |
| 220 | n9e_mon[n9e_mon] @ [10.131.136.10] |
| 112 | region_mgmt[region_mgmt] @ [10.131.136.10] |
| 55 | [root] @ localhost  |
| 19 | skyops[skyops] @ [10.131.136.20] |
| 17 | atreus[atreus] @ [10.131.136.10] |
| 14 | skyops[skyops] @ [10.131.136.10] |
| 12 | atreus[atreus] @ [10.131.136.20] |
| 7 | [keystone] @ [10.131.136.20] |
| 3 | [skyops] @ [10.131.136.10] |
| 2 | kingcobra[kingcobra] @ [10.131.136.10] |
| 2 | [atreus] @ [10.131.136.20] |
| 2 | [atreus] @ [10.131.136.10] |
| 1 | [kingcobra] @ [10.131.136.10] |
| 1 | [skyops] @ [10.131.136.20] |
¬±-----------¬±--------------------------------------------+


## Steps to Reproduce the Problem
SELECT count(user_host) as count_user,user_host from mysql.general_log group by user_host order by count_user DESC;

## Specifications

  - Version: docker build 1.18.0
  - Platform:


#### Comments (3)

<details>
<summary><strong>yiminc</strong> commented on 2023-04-21 21:15:29.000 UTC</summary>

How many shard do you have?
Each shard will have some task pump to read DB peroidically for tasks to be processed. So if your shard count is too large it would have considerable overhead cost. 
Could you share some of the query statements to help understand what they are.

</details>

<details>
<summary><strong>jixiezhixin</strong> commented on 2024-10-15 03:32:05.000 UTC</summary>

> How many shard do you have? Each shard will have some task pump to read DB peroidically for tasks to be processed. So if your shard count is too large it would have considerable overhead cost. Could you share some of the query statements to help understand what they are.

I'm sorry for taking so long to reply.
tctl admin cl describe
{
"supportedClients": {
"temporal-cli": "\u003c2.0.0",
"temporal-go": "\u003c2.0.0",
"temporal-java": "\u003c2.0.0",
"temporal-php": "\u003c2.0.0",
"temporal-server": "\u003c2.0.0",
"temporal-typescript": "\u003c2.0.0",
"temporal-ui": "\u003c3.0.0"
},
"serverVersion": "1.22.7",
"membershipInfo": {
"currentHost": {
"identity": "10.244.2.129:7233"
},
"reachableMembers": [
"10.244.2.193:6934",
"10.244.1.71:6933",
"10.244.1.123:6935",
"10.244.1.88:6939",
"10.244.2.146:6935",
"10.244.2.129:6933",
"10.244.1.53:6934",
"10.244.2.178:6935",
"10.244.2.170:6933",
"10.244.2.192:6934",
"10.244.2.150:6939",
"10.244.1.58:6939"
],
"rings": [
{
"role": "frontend",
"memberCount": 3,
"members": [
{
"identity": "10.244.1.71:7233"
},
{
"identity": "10.244.2.129:7233"
},
{
"identity": "10.244.2.170:7233"
}
]
},
{
"role": "history",
"memberCount": 3,
"members": [
{
"identity": "10.244.1.53:7234"
},
{
"identity": "10.244.2.193:7234"
},
{
"identity": "10.244.2.192:7234"
}
]
},
{
"role": "matching",
"memberCount": 3,
"members": [
{
"identity": "10.244.2.178:7235"
},
{
"identity": "10.244.2.146:7235"
},
{
"identity": "10.244.1.123:7235"
}
]
},
{
"role": "worker",
"memberCount": 3,
"members": [
{
"identity": "10.244.1.88:7239"
},
{
"identity": "10.244.1.58:7239"
},
{
"identity": "10.244.2.150:7239"
}
]
}
]
},
"clusterId": "6e34652b-4564-4202-be62-cf1680157ec5",
"clusterName": "active",
"historyShardCount": 512,
"persistenceStore": "mysql",
"visibilityStore": "mysql,elasticsearch",
"failoverVersionIncrement": "10",
"initialFailoverVersion": "1"
}

</details>

<details>
<summary><strong>jixiezhixin</strong> commented on 2024-10-15 08:00:43.000 UTC</summary>

same as https://community.temporal.io/t/large-number-of-database-calls-by-temporal-to-datastax-cassandra/6789/3

</details>


---

### #4020: Unexpected lag between TimerStarted and TimerFired when switching a namespace's active cluster

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4020 |
| **State** | OPEN |
| **Author** | emmercm (Christian Emmer) |
| **Created** | 2023-03-07 01:30:53.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-21 15:14:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | potential-bug |
| **Assignees** | yux0 |
| **Milestone** | None |

#### Description

## Expected Behavior
When:

* Making use of `Workflow.sleep()`
* Using multi-cluster replication
* Switching the active cluster for a namespace

Then the time between `TimerStarted` and `TimerFired` should be minimal.

## Actual Behavior
I've observed the time between `TimerStarted` and `TimerFired` to be more than 10 minutes.

## Steps to Reproduce the Problem

1. Have two Temporal clusters, `cluster-a` and `cluster-b`, with multi-cluster replication enabled
1. Have one Java service with two SDK clients, one for each cluster
1. Make a workflow with the following steps:
  a. Execute an activity that returns immediately
  b. `Workflow.sleep(500)`
  c. Execute an activity that returns immediately
  d. `Workflow.sleep(500)`
  e. Execute an activity that returns immediately
  f. `Workflow.sleep(500)`
  g. Execute an activity that returns immediately
  h. `Workflow.sleep(500)`
  i. Execute an activity that returns immediately
  j. `Workflow.sleep(500)`
1. Schedule that workflow on a short cron:

    ```shell
    tctl --namespace sandbox workflow start --taskqueue sandbox --workflow_type DummyWorkflow --cron "@every 10s"
    ```

1. Change the active cluster for the namespace in the middle of workflow execution:    

    ```shell
    tctl --namespace sandbox namespace update --active_cluster cluster-b
    ```

1. Repeat the previous step until the workflow's event history appears to be stuck waiting for `TimerFired` (the last event in history is `TimerStarted`). Only repeat the step every ~60sec so the thrash isn't crippling.
1. Wait ~10 minutes
1. Observe that `TimerFired` did eventually fire

## Specifications

* Version: local `temporal/auto-setup:1.19.0` with Java SDK v1.18.2
* Platform: macOS Ventura v13.2.1 Intel, Docker v20.10.23

I'm hoping there is a simple answer to this behavior, such as a timeout I'm missing. I'm not setting explicit timeouts in the above `tctl` commands, and I'm not setting explicit activity timeouts in the workflow code. The UI doesn't show a timeout for timer tasks in the way it does for workflow tasks, so I'm not positive I can affect this behavior.



#### Comments (3)

<details>
<summary><strong>yux0</strong> commented on 2023-03-13 17:45:20.000 UTC</summary>

The repeated 10s cron with 500 milliseconds sleeps is very aggressive. Could you double check if the e2e timer latency is high? (metrics: task_latency_queue_bucket with operation="TimerActive*" or "TimerStandby*"). I guess there is backlog in timer queue.

</details>

<details>
<summary><strong>emmercm</strong> commented on 2023-03-20 15:35:51.000 UTC</summary>

It's definitely aggressive, and it's not realistic. I wanted to create a scenario that was putting a constant load on Temporal, and chose to use one workflow instead of many.

I will check out those metrics now!

</details>

<details>
<summary><strong>emmercm</strong> commented on 2023-03-21 15:14:39.000 UTC</summary>

I set up Grafana with a local Docker Compose to visualize this, but I'm less familiar with Grafana, I usually have a Datadog agent scrape Prometheus, and then I visualize metrics in the Datadog UI. Let me know if I'm missing anything below.

Reproducing the scenario where the workflow is stuck after `TimerStarted`, waiting on `TimerFired`, I see:

- `histogram_quantile(0.95, sum by(le, operation) (rate(task_latency_queue_bucket{operation=~"TimerActive.+|TimerStandby.+"}[$__rate_interval])))` stops plotting points, `task_latency_queue_bucket{operation=~"TimerActive.+|TimerStandby.+"}` stays flat for the ~10min the problem happens for
- Using [Tihomirs's dashboards](https://github.com/tsurdilo/my-temporal-dockercompose) I see:
  - Workflow and activity task scheduled, started, and completed all halt (expected)
  - State transition halts (expected)
  - No significant change to memory or goroutines
  - Changes to `services_requests`:
    - Some stayed flat/consistent (in order of most to the least requests): `PollActivityTaskQueue`, `GetReplicationMessages`, `PollWorkflowTaskQueue`, `GetNamespaceReplicationMessages` decreased to near zero if not zero.
    - Some increased during the problem (in no particular order): `DescribeTaskQueue`, `DescribeWorkflowExecution`, `GetMutableState`, `GetWorkflowExecutionHistory`, `GetWorkflowExecutionHistoryReverse`, `PollMutableState`
  - Changes to `persistence_requests`:
    - Some stayed flat/consistent (in no particular order): `UpdateTaskQueue`, `GetAckLevel`, `UpdateAckLevel`, `GetMetadata`, `UpsertClusterMembership`, `AssertShardOwnership`, `ListClusterMetadata`, `ListNamespaces`, `ReadQueueMessages`, `UpdateShard`, `UpdateTaskQueue`
    - Only one increased during the problem: `ReadHistoryBranchReverse`

And if it's helpful:

```shell
$ docker ps

CONTAINER ID   IMAGE                           COMMAND                  CREATED          STATUS          PORTS                                                                                              NAMES
a4ddb403fdff   temporalio/admin-tools:1.19.0   "tail -f /dev/null"      29 minutes ago   Up 29 minutes                                                                                                      temporal-admin-tools-b
4dbc4e652f85   temporalio/ui:2.11.1            "./start-ui-server.sh"   29 minutes ago   Up 29 minutes   0.0.0.0:8081->8080/tcp                                                                             temporal-ui-b
d02517932c2d   temporalio/ui:2.11.1            "./start-ui-server.sh"   29 minutes ago   Up 29 minutes   0.0.0.0:8080->8080/tcp                                                                             temporal-ui-a
37ac6cb260a3   temporalio/admin-tools:1.19.0   "tail -f /dev/null"      29 minutes ago   Up 29 minutes                                                                                                      temporal-admin-tools-a
68ee231ac1cf   grafana/grafana:latest          "/run.sh"                29 minutes ago   Up 29 minutes   0.0.0.0:3000->3000/tcp                                                                             temporal-grafana
67ed30f2940a   temporalio/auto-setup:1.19.0    "/etc/temporal/entry‚Ä¶"   29 minutes ago   Up 29 minutes   6933-6935/tcp, 6939/tcp, 7234-7235/tcp, 7239/tcp, 0.0.0.0:8001->8001/tcp, 0.0.0.0:8233->7233/tcp   temporal-b
44071ca264e5   temporalio/auto-setup:1.19.0    "/etc/temporal/entry‚Ä¶"   29 minutes ago   Up 29 minutes   6933-6935/tcp, 6939/tcp, 7234-7235/tcp, 0.0.0.0:7233->7233/tcp, 7239/tcp, 0.0.0.0:8000->8000/tcp   temporal-a
4d19a4e4099d   prom/prometheus:latest          "/bin/prometheus --c‚Ä¶"   29 minutes ago   Up 29 minutes   0.0.0.0:9090->9090/tcp                                                                             temporal-prometheus
32a784d644ee   mysql:8                         "docker-entrypoint.s‚Ä¶"   29 minutes ago   Up 29 minutes   33060/tcp, 0.0.0.0:3308->3306/tcp                                                                  temporal-mysql-b
859f9317ca5f   elasticsearch:7.10.1            "/tini -- /usr/local‚Ä¶"   29 minutes ago   Up 29 minutes   9300/tcp, 0.0.0.0:9202->9200/tcp                                                                   temporal-elasticsearch-b
604ba2ebe913   mysql:8                         "docker-entrypoint.s‚Ä¶"   29 minutes ago   Up 29 minutes   33060/tcp, 0.0.0.0:3307->3306/tcp                                                                  temporal-mysql-a
bd3df70b564a   elasticsearch:7.10.1            "/tini -- /usr/local‚Ä¶"   29 minutes ago   Up 29 minutes   9300/tcp, 0.0.0.0:9201->9200/tcp                                                                   temporal-elasticsearch-a
```

</details>


---

### #3863: Pin a particular workflow run to prevent it from getting purged

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3863 |
| **State** | OPEN |
| **Author** | utkarshpaliwal9 (Utkarsh Paliwal) |
| **Created** | 2023-01-30 13:32:01.000 UTC (2y 11m ago) |
| **Updated** | 2023-03-03 18:02:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Problem Description**
Coming from the fact there are some unusual/rare events that happen in a run (like some race condition, for example, which are difficult to reproduce) that needs to be looked into at a later point in time. And the devs have to race against time to solve the issue before the run gets purged.

**Describe the solution you'd like**
Temporal can provide a button to mark/pin/save a run and this makes sure the run would not get purged. Later, this workflow run can be referenced with a link or otherwise. This run stored in the DB should not get deleted unless explicitly deleted.

**Alternatives considered**
Downloading workflow history json probably would work as a stopgap for saving it locally. But, some better way from the Temporal UI would be really helpful such that others can also look at it.



#### Comments (3)

<details>
<summary><strong>yiminc</strong> commented on 2023-02-03 22:38:39.000 UTC</summary>

This is for closed workflow? I think you are asking for ability to dynamic retention per workflow. 

</details>

<details>
<summary><strong>yiminc</strong> commented on 2023-02-03 22:38:57.000 UTC</summary>

This is currently not possible, could be a future feature to consider.

</details>

<details>
<summary><strong>utkarshpaliwal9</strong> commented on 2023-02-06 09:49:31.000 UTC</summary>

Yes @yiminc. This feature ask indeed is for a closed workflow to prevent it from getting purged eventually.

</details>


---

### #3665: Update workflow search attributes outside of workflow context 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3665 |
| **State** | OPEN |
| **Author** | calum-stripe |
| **Created** | 2022-11-25 04:08:54.000 UTC (3y 1m ago) |
| **Updated** | 2024-02-14 15:21:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | rodrigozhou |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
My use case is wanting to tag workflows with metadata based on an external system. For example, I want to run a cronjob (or another workflow) every minute which will check if a workflow has taken longer than X amounts of minutes time to finish running (the X value here is not available in the workflow or worker and only in this third party system so we couldn't just use a timer or search attributes in the workflow itself). I have access to the workflow ids because we can query Temporal with the client library. 

**Describe the solution you'd like**
I would like the ability to update a workflows search attributes outside of the workflow context. Preferably with the ability to be able to update a set of workflows based on a query. For example we would like the ability to provide a query `ExecutionStatus='Failed'` and to update all the workflows with this. 

**Describe alternatives you've considered**
I've considered storing the workflow id and the search attributes in another datastore but this feels wasteful when search attributes are so useful and accessible.

https://temporalio.slack.com/archives/CTRCR8RBP/p1669332033386459
- updating the ElasticSearch index manually using the ElasticSearch API directly.

**Additional context**


#### Comments (3)

<details>
<summary><strong>calum-stripe</strong> commented on 2022-11-25 04:36:01.000 UTC</summary>

Also just curious if this does get implemented how this will be impacted on cloud (will it be classed an action per workflow updated or per operation). 

</details>

<details>
<summary><strong>bergundy</strong> commented on 2022-11-29 21:00:44.000 UTC</summary>

@calum-stripe do you need to be able to update closed workflows as well as open ones?

</details>

<details>
<summary><strong>Chrisbattarbee</strong> commented on 2024-02-14 15:21:01.000 UTC</summary>

I'd like to be able to update closed workflows.

When I add a new search attribute I'd like to migrate past workflows by setting the search attribute on the existing workflows

</details>


---

### #3532: Remove unused methods in ClientBean

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3532 |
| **State** | OPEN |
| **Author** | yux0 (Yu Xia) |
| **Created** | 2022-10-24 21:44:35.000 UTC (3y 2m ago) |
| **Updated** | 2023-10-23 20:25:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | yux0 |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
1. Remove unused methods in ClientBean
2. Add new method GetAdminClient to align with Frontend Client

**Describe the solution you'd like**
1. Remove unused methods in ClientBean
2. Add new method GetAdminClient to align with Frontend Client

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.


#### Comments (3)

<details>
<summary><strong>alexshtin</strong> commented on 2022-10-28 23:17:35.000 UTC</summary>

I would remove entire `ClientBean` and replace it with proper `fx` injections.

</details>

<details>
<summary><strong>yiminc</strong> commented on 2023-10-22 04:35:29.000 UTC</summary>

https://github.com/temporalio/temporal/pull/5000

</details>

<details>
<summary><strong>dnr</strong> commented on 2023-10-23 20:25:13.000 UTC</summary>

#5000 is for the persistence client bean only, not the rpc one that this issue is about

</details>


---

### #3104: Surprising clusterMetadata override

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3104 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-07-14 23:25:06.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:19:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | bug |
| **Assignees** | yux0 |
| **Milestone** | None |

#### Description

## Expected Behavior
If DB is not initialized, static config of cluster metadata should be 
* either respected & persisted into DB
* or deprecated completely

## Actual Behavior
```
clusterMetadata:
  enableGlobalNamespace: true
  failoverVersionIncrement: 10
  masterClusterName: "cluster-a"
  currentClusterName: "cluster-a"
  clusterInformation:
    cluster-a:
      enabled: true
      initialFailoverVersion: 3
      rpcName: "frontend"
      rpcAddress: "localhost:7233"
    cluster-b:
      enabled: true
      initialFailoverVersion: 2
      rpcName: "frontend"
      rpcAddress: "localhost:8233"
```      
Using above static config & clean DB & after 1s ish, the `cluster-b` section will be deleted 

## Steps to Reproduce the Problem
See above

## Specifications

  - Version: 1.16.x+
  - Platform: Mac & Intel


#### Comments (3)

<details>
<summary><strong>yux0</strong> commented on 2022-07-15 21:50:45.000 UTC</summary>

The add cluster from config is deprecated and a warning log will show if you figure it via config.

`{"level":"warn","ts":"2022-07-15T14:38:43.425-0700","msg":"ClusterInformation in ClusterMetadata config is deprecated. Please use TCTL admin tool to configure remote cluster connections","component":"metadata-initializer","key":"clusterInformation","cluster-name":"standby","ignored-value":{"Enabled":true,"InitialFailoverVersion":2,"RPCAddress":"localhost:8233"},"logging-call-at":"fx.go:615"}`

</details>

<details>
<summary><strong>yux0</strong> commented on 2022-07-15 21:55:22.000 UTC</summary>

We cannot deprecate the clusterMetadata because we need to know the current cluster and load it from DB. I agree we can do better to refine this cluster metadata. But there will be questions about how to enable global namespace? How to deal with the primary cluster?  How to set initialFailoverVersion and failoverVersionIncrement in first cluster initialization? How to be backward-compatible? Before solving those, the clusterMetadata will stay.

</details>

<details>
<summary><strong>wxing1292</strong> commented on 2022-07-15 23:44:22.000 UTC</summary>

warn log says ClusterInformation is deprecated, but it did not say anything about ignoring cluster-b (non current cluster)

Silent failure is misleading, either complete deprecate the cluster metadata by using a new structure or improve the logging 

</details>


---

### #3015: Possibility to exclude metrics tags when emitting metrics?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3015 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-06-23 07:21:19.000 UTC (3y 6m ago) |
| **Updated** | 2023-03-03 20:19:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

[tagsToMap](https://github.com/temporalio/temporal/blob/43b1049/common/metrics/tally_metric_provider.go#L106) is invoked everything when emitting counter / timer e.g.
* [example 1](https://github.com/temporalio/temporal/blob/43b1049/common/metrics/tally_metric_provider.go#L79)
* [example 2](https://github.com/temporalio/temporal/blob/43b1049/common/metrics/tally_metric_provider.go#L86)
* [example 3](https://github.com/temporalio/temporal/blob/43b1049/common/metrics/tally_metric_provider.go#L93)
* [example 4](https://github.com/temporalio/temporal/blob/43b1049/common/metrics/tally_metric_provider.go#L100)

consider exclude the unwanted tags when exporting the metrics to e.g. prometheus, instead of during metrics generation, like ^

#### Comments (3)

<details>
<summary><strong>yiminc</strong> commented on 2022-06-23 15:25:16.000 UTC</summary>

The tagsToMap would exclude the tags listed in excludeTags by replacing them as const value. Not sure what the suggestion here is? The excludeTags also has whitelist of values that can be included, not sure how useful it is.

</details>

<details>
<summary><strong>wxing1292</strong> commented on 2022-06-23 15:47:31.000 UTC</summary>

Can we execute the exclusion logic when exporting the metrics to e.g. premetheus? instead of doing <- this logic every time when increasing a counter.

</details>

<details>
<summary><strong>wxing1292</strong> commented on 2022-06-25 00:00:48.000 UTC</summary>

sample pic which shows that tagsToMap function may create too many objects
<img width="1516" alt="image" src="https://user-images.githubusercontent.com/8762893/175749789-9fa64e1d-91b6-4791-b1ad-00aa487b87e3.png">


</details>


---

### #2598: Allow configurable time for history/visibility archiving

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2598 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2022-03-10 15:15:12.000 UTC (3y 9m ago) |
| **Updated** | 2025-05-28 09:44:26.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

Currently it's not possible to configure times when inline archiving of visibility and history data is triggered.
Archival can sometimes take a large number of resources, and it happens at unpredictable times at wf retention time. It would be helpful if users had the ability to configure a specific time or a time duration when archival should be performed (could be for example certain times when least production loads are expected). 



#### Comments (1)

<details>
<summary><strong>kkcmadhu-IBM</strong> commented on 2025-05-28 09:44:26.000 UTC</summary>

file system based archival should be supported ( currently its non production grade according to documents).
s3 archival limitation of doing only = search should be addressed.

</details>


---

### #1338: Workflow Reset: add logic of determining the reset point to the service

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1338 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2021-03-01 16:25:54.000 UTC (4y 10m ago) |
| **Updated** | 2025-05-23 07:18:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement, good first issue, refactoring, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
`tctl workflow reset` supports `reset_type` argument. The ResetWorkflowExecution gRPC API accepts only `workflow_task_finish_event_id`. So all the logic of finding reset point resides in the tctl. This makes the logic not reusable when SDKs invoke reset operation directly.

**Describe the solution you'd like**
Move logic of finding reset point to the service by adding reset_type argument to ResetWorkflowExecutionRequest.



#### Comments (3)

<details>
<summary><strong>bisakhmondal</strong> commented on 2021-07-29 07:15:40.000 UTC</summary>

Hey, @mfateev would you mind, if I work on this issue? I have been using temporal.io for quite a long time and want to contribute something : )
Thanks.

</details>

<details>
<summary><strong>sstro</strong> commented on 2023-03-24 13:21:19.000 UTC</summary>

Hello,

In case there are parallel branches in a workflow it can be difficult to find the "reset point" event corresponding to the branch one wishes to restart. In the Java SDK, we haven't found any correlation between the "workflow task started" event and activity events that follow.

If the logic of determining the reset point were moved to the service, ideally one could just invoke an API to e.g. "reset a workflow to the point preceding the first failure".

Thanks

</details>

<details>
<summary><strong>Shridhar2104</strong> commented on 2025-05-23 07:18:49.000 UTC</summary>

## Auto‚Äêselect reset point when not provided

### Problem
By default the Reset API requires clients to supply a `WorkflowTaskFinishEventId`, forcing callers to inspect history and pick an internal event ID themselves. This makes resets brittle and user‚Äêunfriendly.

### Solution
If the client omits `WorkflowTaskFinishEventId` (i.e. it‚Äôs zero), the service will:

1. Read the workflow‚Äôs history branch up to `(NextEventID ‚Äì 1)`.
2. Scan the returned `HistoryEvents` for the last `WORKFLOW_TASK_COMPLETED` event.
3. Set `WorkflowTaskFinishEventId` to that event‚Äôs ID, so the reset will roll back to just before it completed.
4. Proceed with the existing validation and reset logic.

### Key Code Snippet

```go
// in service/history/api/resetworkflow/api.go ‚Üí Invoke(...)
baseMutableState := baseLease.GetMutableState()

// 1) Auto‚Äêselect finish ID if caller omitted it
if req.GetWorkflowTaskFinishEventId() == 0 {
  // read history up to last event
  resp, err := shardCtx.GetExecutionManager().ReadHistoryBranch(ctx, &persistence.ReadHistoryBranchRequest{
    BranchToken: baseMutableState.GetExecutionInfo().GetCurrentBranchToken(),
    MinEventID:  common.FirstEventID,
    MaxEventID:  baseMutableState.GetNextEventID() - 1,
    PageSize:    defaultPageSize,
  })
  if err != nil {
    return nil, serviceerror.NewInternal("fetching history for reset: " + err.Error())
  }

  // 2) find last WorkflowTaskCompleted
  var lastComplete int64
  for _, ev := range resp.HistoryEvents {
    if ev.GetEventType() == enumspb.EVENT_TYPE_WORKFLOW_TASK_COMPLETED {
      lastComplete = ev.GetEventId()
    }
  }
  if lastComplete <= common.FirstEventID {
    return nil, serviceerror.NewInvalidArgument("no completed workflow task found to reset to")
  }

  // 3) use that as the finish‚Äêevent ID
  req.WorkflowTaskFinishEventId = lastComplete
}

// 4) existing validation now passes, then core reset logic runs...


</details>


---

### #1311: Make workflow state (not status) available via API

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1311 |
| **State** | OPEN |
| **Author** | robholland (Rob Holland) |
| **Created** | 2021-02-22 14:52:42.000 UTC (4y 10m ago) |
| **Updated** | 2025-05-29 18:25:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement, good first issue, API, devexp, difficulty: easy |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently the web UI will show "Running" for cron workflows which are created but not yet started as they are not yet due. This isn't ideal for developers as you can't tell without clicking through to the workflow whether it is actually running.

**Describe the solution you'd like**
It would be better if the web UI was able to refer to the state rather than just the status. Temporal knows that the workflow is created but not yet started, the persistence record has "created" as the state. The state field isn't currently exposed via the Temporal APIs at all, which means this information is not available via tctl either.

#### Comments (3)

<details>
<summary><strong>namit-chandwani</strong> commented on 2024-05-25 07:00:45.000 UTC</summary>

Hey @robholland @yiminc @dnr @samarabbas,  I would like to get started with some beginner issue to get familiar with the temporal codebase.

Can you please assign this issue to me if it hasn't been taken up by anyone yet?

</details>

<details>
<summary><strong>dnr</strong> commented on 2024-05-28 20:32:42.000 UTC</summary>

This issue is pretty much obsolete since it's about the old "cron" feature. Instead, you could try to get started with some easy schedules bugs. The equivalent bug for schedules might be:

- Schedules with no future action times should show as "Idle" instead of "Running" in the web UI

That one is actually just in the UI though, it can just look at the future action times list and show Idle if it's empty.

In the server, there's one required change to make that work better:

- Schedule "future action times" should respect limited actions

(i.e. if the schedule has one action remaining, it should only list one future action time)

I'm not sure if there are issues filed for those yet but feel free to file them or just send a PR.

</details>

<details>
<summary><strong>vaibhavyadav-dev</strong> commented on 2025-05-29 18:25:32.000 UTC</summary>

Hey @dnr, second one, ```Schedule "future action times" should respect limited actions```, already fixed, I've tried it using cli and found expected results.

</details>


---

### #1024: Add a CLI and tab in the web dashboard to display all taskQueues list / pollers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1024 |
| **State** | OPEN |
| **Author** | lerminou (Nicolas DUBUT) |
| **Created** | 2020-11-24 13:47:06.000 UTC (5y 1m ago) |
| **Updated** | 2023-06-26 13:58:26.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement, API |
| **Assignees** | dnr |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**

Actually, we can get pollers for a taskQueues only in the web dashboard when we click on a workflow detail and then click on => taskQueue
final url: /namespaces/default/task-queues/User
![image](https://user-images.githubusercontent.com/3818051/100102045-58b79b80-2e63-11eb-9120-a6c0f3352ddc.png)
results:
![image](https://user-images.githubusercontent.com/3818051/100102316-b4822480-2e63-11eb-8213-2189713e7762.png)


**Describe the solution you'd like**
I think it can be great to have a tab "taskqueues" just before "settings"
and a CLI command in the TCTL tool

this list will list all taskqueues, (with maybe some informations ?) and if we click on one item, then all pollers for the taskqueue are displayed.

because taskQueues are dynamically registered in Temporal, it can be a great debugger option to check the actual taskQueues


#### Comments (1)

<details>
<summary><strong>Fl0ux</strong> commented on 2023-06-26 13:58:26.000 UTC</summary>

It would be really helpful to list all workers actually listening to all tasks queues.

Reactions: üëç 1

</details>


---

### #996: [request] API needs for Temporal Web

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/996 |
| **State** | OPEN |
| **Author** | swyxio (swyx.io) |
| **Created** | 2020-11-16 19:38:20.000 UTC (5y 1m ago) |
| **Updated** | 2021-12-15 23:20:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 3 |
| **Priority Score** | 3 |
| **Labels** | enhancement, API, difficulty: easy, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |

#### Description

logging our future internal needs on github issue as discussed with @wxing1292 and @feedmeapples 

**Is your feature request related to a problem? Please describe.**

there are a range of small things we want, which may involve the same underlying code:

- (small) ability to query OPEN + CLOSED workflows in a single call instead of currently double call
- (nice to have) backwards pagination of a query i/o just forward pagination
- API-side FILTER of event types in History (currently doing clientside) 
- Given a parent workflow, find all the child workflows 
  - (We want to make it easy to find the child workflows of a parent workflow in Temporal Web.)
  - having both pending and complete workflows would be best, but we can settle for just pending first.
  - Wenquan says implementation may be doable in mutableState if we only hold ID's of children
  - more info: https://www.notion.so/PRD-for-Parent-Child-workflows-on-Temporal-Web-69c3ed9654a9471e819c13398766b0a9

**Describe the solution you'd like**

we are unopnionated on solution but recognize that we may need to wait a while for the underlying code for this to be in place to offer these APIs.




#### Comments (3)

<details>
<summary><strong>wxing1292</strong> commented on 2020-11-16 20:26:47.000 UTC</summary>

> 1. (small) ability to query OPEN + CLOSED workflows in a single call instead of currently double call
> 2. (nice to have) backwards pagination of a query i/o just forward pagination

Probably will not support this features, at least not in the near future.

</details>

<details>
<summary><strong>wxing1292</strong> commented on 2020-11-16 20:28:12.000 UTC</summary>

> 3. API-side FILTER of event types in History (currently doing clientside)
> 4. Given a parent workflow, find all the child workflows

3. can be done fairly easily, but pending some perf analysis
4. this may requires a little bit of design work

Reactions: ‚ù§Ô∏è 1

</details>

<details>
<summary><strong>swyxio</strong> commented on 2021-12-15 23:20:03.000 UTC</summary>

was discussed today at onsite. we agree on parent -> child thing with Yimin.

</details>


---

### #787: Frequent timeouts while archiving to S3

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/787 |
| **State** | OPEN |
| **Author** | rfwagner (Richard Wagner) |
| **Created** | 2020-10-02 17:16:29.000 UTC (5y 3m ago) |
| **Updated** | 2021-07-04 07:12:57.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | potential-bug, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

## Expected Behavior

In our setup we are archiving to an S3 compliant object store. Latency to the S3 store is fairly low. I expect the vast majority of S3 calls to succeed w/o timeout.

## Actual Behavior

We are seeing very frequent timeouts in the S3 archival calls. Some sample logs included below. From the setting of `defaultBlobstoreTimeout` in `s3store/historyArchiver.go`, it seems like the general intent is to enforce a timeout of one minute on the S3 API calls. I put some print statements in `ensureContextTimeout` and I can see that in many cases the deadline in place at the time of the S3 calls is just under 60s. But in other cases, the deadline in the context being used for the S3 calls is under 1s. Presumably the timeouts are happening when this is the case, though I haven't been able to verify that for sure.

In any case it seems like it would be beneficial to ensure that a consistent deadline is enforced on all S3 API calls.

## Steps to Reproduce the Problem

  1. Configure a namespace for S3 archiving.
  1. Set retention period to 0 so archiving happens immediately and can be easily tested.
  1. Execute several workflows and observe a large number of timeouts on the S3 calls.

## Specifications

  - Version: 0.29
  - Platform: MacOS

Here are a few logs:
```
{"level":"error","ts":"2020-10-02T10:02:23.350-0700","msg":"Archive method encountered an non-retryable error.","service":"history","archival-request-namespace-id":"c52608e8-2d17-468c-b940-369bba914c91","archival-request-namespace":"rtest","archival-request-workflow-id":"deb097c6-f862-415d-82e2-4763a2f139ca-host0","archival-request-run-id":"874a451a-4112-44f1-8ed8-759ab22e9768","archival-request-workflow-type":"upgradeSingleHost","archival-request-close-timestamp":"2020-10-02T17:02:23.050Z","archival-request-status":"Completed","archival-URI":"s3://testb1","archival-archive-fail-reason":"failed to write history to s3","error":"RequestCanceled: request context canceled\ncaused by: context deadline exceeded","logging-call-at":"visibilityArchiver.go:120","stacktrace":"go.temporal.io/server/common/log/loggerimpl.(*loggerImpl).Error\n\t/Users/rwagner/go/src/github.com/temporalio/temporal/common/log/loggerimpl/logger.go:138\ngo.temporal.io/server/common/archiver/s3store.(*visibilityArchiver).Archive.func1\n\t/Users/rwagner/go/src/github.com/temporalio/temporal/common/archiver/s3store/visibilityArchiver.go:120\ngo.temporal.io/server/common/archiver/s3store.(*visibilityArchiver).Archive\n\t/Users/rwagner/go/src/github.com/temporalio/temporal/common/archiver/s3store/visibilityArchiver.go:149\ngo.temporal.io/server/service/worker/archiver.(*client).archiveVisibilityInline\n\t/Users/rwagner/go/src/github.com/temporalio/temporal/service/worker/archiver/client.go:250"}
{"level":"info","ts":"2020-10-02T10:02:23.384-0700","msg":"failed to perform visibility archival inline","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc00101b000","archival-caller-service-name":"history","archival-archive-attempted-inline":true,"archival-request-namespace-id":"c52608e8-2d17-468c-b940-369bba914c91","archival-request-namespace":"rtest","archival-request-workflow-id":"deb097c6-f862-415d-82e2-4763a2f139ca-host0","archival-request-run-id":"874a451a-4112-44f1-8ed8-759ab22e9768","archival-URI":"s3://testb1","error":"RequestCanceled: request context canceled\ncaused by: context deadline exceeded","logging-call-at":"client.go:235"}
{"level":"error","ts":"2020-10-02T10:02:23.366-0700","msg":"Archive method encountered an non-retryable error.","service":"history","archival-request-namespace-id":"c52608e8-2d17-468c-b940-369bba914c91","archival-request-namespace":"rtest","archival-request-workflow-id":"deb097c6-f862-415d-82e2-4763a2f139ca-host1","archival-request-run-id":"3253bb9b-507c-4814-83d4-d556e2cd0303","archival-request-workflow-type":"upgradeSingleHost","archival-request-close-timestamp":"2020-10-02T17:02:23.071Z","archival-request-status":"Completed","archival-URI":"s3://testb1","archival-archive-fail-reason":"failed to write history to s3","error":"RequestCanceled: request context canceled\ncaused by: context deadline exceeded","logging-call-at":"visibilityArchiver.go:120","stacktrace":"go.temporal.io/server/common/log/loggerimpl.(*loggerImpl).Error\n\t/Users/rwagner/go/src/github.com/temporalio/temporal/common/log/loggerimpl/logger.go:138\ngo.temporal.io/server/common/archiver/s3store.(*visibilityArchiver).Archive.func1\n\t/Users/rwagner/go/src/github.com/temporalio/temporal/common/archiver/s3store/visibilityArchiver.go:120\ngo.temporal.io/server/common/archiver/s3store.(*visibilityArchiver).Archive\n\t/Users/rwagner/go/src/github.com/temporalio/temporal/common/archiver/s3store/visibilityArchiver.go:149\ngo.temporal.io/server/service/worker/archiver.(*client).archiveVisibilityInline\n\t/Users/rwagner/go/src/github.com/temporalio/temporal/service/worker/archiver/client.go:250"}
```


#### Comments (1)

<details>
<summary><strong>alexshtin</strong> commented on 2020-10-29 19:31:51.000 UTC</summary>

Thanks for reporting this and making partial investigation. `ensureContextTimeout` actually sets timeout to 1 minute only if it is not set at all. If it is set by caller then it stays as is. Logs you see are coming from "inline" archival feature when `Archive` is called right from timer and transfer task queue processor. Timeout values are set to [1 second](https://github.com/temporalio/temporal/blob/d280cc1dbf9cb9db7c13f631e8419c2aa5c08f87/service/history/service.go#L323) and [200 milliseconds](https://github.com/temporalio/temporal/blob/d280cc1dbf9cb9db7c13f631e8419c2aa5c08f87/service/history/service.go#L340) there. And I believe these error messages can be safely ignored because background worker will eventually archive workflow.

Noisy error messages need to be fixed though. For now you can try to increase timeouts using dynamic config settings.

</details>


---

### #518: Add support for passing large arguments between activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/518 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-07-09 05:09:19.000 UTC (5y 5m ago) |
| **Updated** | 2023-03-03 20:23:04.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement, API, architecture, devexp |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
From Slack:

> I tried to pass a 30MB JSON from an activity as return to workflow and have workflow pass it to the next activity. It did not work. Save it as a blob somewhere and pass back the url would be the workaround I go for, but I do wonder if Temporal can have this built in. Let the activity call Temporal to save the blob in Cassandra/MySQL, returns the id of the blob to the workflow that can pass it on. Unify how large blob is passed from activity to activity, so user of Temporal do not have to think about it and do their own implementations





#### Comments (1)

<details>
<summary><strong>sunshineo</strong> commented on 2020-07-09 05:25:30.000 UTC</summary>

Thank you @mfateev !

</details>


---

### #503: Record activity started and failed event for activity in retry on workflow completion

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/503 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-07-07 18:50:28.000 UTC (5y 5m ago) |
| **Updated** | 2023-03-03 20:23:03.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 1 |
| **Priority Score** | 3 |
| **Labels** | enhancement, good first issue, difficulty: easy, planning, operations, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 üëÄ 2 |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, start and failure events for an activity that is constantly failing and in a retry loop are not recorded into the history until ScheduleToClose timeout is exhausted. If a workflow completes or continues as new before the status of the activity execution is essentially lost.  

**Describe the solution you'd like**
Record started and failed events for all activities that are in a retry loop before completing the workflow.



#### Comments (1)

<details>
<summary><strong>edmondop</strong> commented on 2022-11-30 21:11:23.000 UTC</summary>

Is this still useful? Would be happy to look into that

</details>


---

### #8606: Schedules - allow TriggerImmediatelyRequest to define arguments

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8606 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2025-11-07 11:46:35.000 UTC (1 months ago) |
| **Updated** | 2025-11-13 23:56:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

https://github.com/temporalio/api/blob/master/temporal/api/schedule/v1/message.proto#L278

Currently not possible to trigger schedule with a different payload than what is specified within the start action.
Ask is to allow arguments to be specified on trigger request

Use case - users wanting to trigger one-off schedule run with diff arguments than whats defined in start action

#### Comments (2)

<details>
<summary><strong>gow</strong> commented on 2025-11-13 23:47:41.000 UTC</summary>

@tsurdilo what is the usecase for this? Can we not use start-with-delay?

</details>

<details>
<summary><strong>bergundy</strong> commented on 2025-11-13 23:56:35.000 UTC</summary>

Just adding a drive by test comment.

</details>


---

### #8072: Adding sql metric stats

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8072 |
| **State** | OPEN |
| **Author** | vanhtuan0409 (Tu·∫•n V∆∞∆°ng) |
| **Created** | 2025-07-21 09:15:33.000 UTC (5 months ago) |
| **Updated** | 2025-07-22 03:56:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

Excessive connection created to PG

**Describe the solution you'd like**

Expose more SQL connection metrics from temporal, provide more insight when troubleshooting and config tunning


#### Comments (2)

<details>
<summary><strong>vanhtuan0409</strong> commented on 2025-07-21 09:16:31.000 UTC</summary>

ref: https://github.com/temporalio/temporal/pull/8071

</details>

<details>
<summary><strong>vanhtuan0409</strong> commented on 2025-07-22 03:56:50.000 UTC</summary>

@tdeebswihart I'm not sure if I can ask you for a review of this pr

</details>


---

### #8013: List workflows by custom search attributes sometimes returns wrong value

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8013 |
| **State** | OPEN |
| **Author** | genek96 (–ï–≤–≥–µ–Ω–∏–π –ò—à—É—Ç–∏–Ω) |
| **Created** | 2025-07-04 11:40:38.000 UTC (6 months ago) |
| **Updated** | 2025-07-04 11:45:58.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

## Expected Behavior
When a workflow is created with a custom search attribute, and the ListWorkflows method is called using this attribute, we expect the result to either include the matching workflow or return no results (if the visibility task has not yet been processed).


## Actual Behavior
In some cases, the method returns a workflow that does not contain the requested search attribute. This workflow was created at the same time (within the same second) as the expected one.


## Steps to Reproduce the Problem

  1. Start a workflow using the .NET SDK with TypedSearchAttributes, like this:
```c#
var handle = await client.StartWorkflowAsync(
    workflowRunCall,
    new(options.WorkflowId.ToString(), GetQueueName<TWorkflow>())
    {
        TypedSearchAttributes = GetSearchAttributes(options)
    }
);
```
Here, the method GetSearchAttributes returns a SearchAttributeCollection. It includes a single attribute: orderId, of type Guid (UUIDv4).

 2. Start another workflow of the same type with the orderId attribute set to a different UUIDv4.
 3. After about 2 seconds, call ListWorkflowsAsync with the desired orderId value:

```
var query = $"orderId='{orderId}'";
var workflows = client.ListWorkflowsAsync(query);
```

 4. The result contains one workflow, but it is not the expected one. Instead, it returns the workflow that was created at the same second as the expected one.
 5. After a while (we have checked after 15 minutes) the index became consistent and search works correctly.


## Additional Notes
This behavior appears approximately once every 1,000‚Äì2,000 workflow runs.

While we understand that the visibility store is eventually consistent, it is unexpected for the result to return an incorrect workflow, rather than the correct one or an empty result.

## Specifications

  - Version: 1.26
  - Persistence storage: Cassandra 4.1.5
  - Visibility storage: postgreSQL 16
  - Platform: all



---

### #7821: [Bug]  workflow list takes differnt query for hot and archieved

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7821 |
| **State** | OPEN |
| **Author** | kkcmadhu-IBM |
| **Created** | 2025-05-28 12:59:16.000 UTC (7 months ago) |
| **Updated** | 2025-05-29 22:18:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | bug |
| **Assignees** | jmbarzee |
| **Milestone** | None |

#### Description

### What are you really trying to do?

uniform way of querying  archived and hot workflows
### Describe the bug

` temporal workflow list   -q 'WorkflowType = "MyWorkflowType"'  -n mynamespace`
works

i expected

` temporal workflow list   -q 'WorkflowType = "MyWorkflowType"'  -n mynamespace  --archived to work`

but it fails with error  "failed listing workflows: unknown filter name: WorkflowType"


but this works 

` temporal workflow list   -q 'WorkflowTypeName = "MyWorkflowType"'  -n mynamespace  --archived `



#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2025-05-28 13:36:57.000 UTC</summary>

This appears to be a server limitation. Transferring there...

</details>

<details>
<summary><strong>yycptt</strong> commented on 2025-05-29 22:18:42.000 UTC</summary>

We should be able to fix/translate the filter name. But the query capability on different backends are very different, so even if syntax are the same, some querier/operators may only work on hot workflows or certain archival backends.

</details>


---

### #7625: Additional tuning configs for scavenger jobs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7625 |
| **State** | OPEN |
| **Author** | justinrixx (Justin Ricks) |
| **Created** | 2025-04-17 17:37:40.000 UTC (8 months ago) |
| **Updated** | 2025-05-22 17:31:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement, difficulty: easy, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

My (self-hosted) cluster's persistence DB gets overwhelmed (100% cpu, high latency for ~20 mins at a time) on a cron schedule like clockwork, every 12 hours. I believe this is due to the scavenger jobs to clean up the DB. Unfortunately the only configs I have to tune this seems to be the various `persistenceMaxQPS` settings, which is too blunt of a tool.

The various scavengers also have inconsistent dynamic configuration available: [the executions scanner has a per-shard qps setting](https://github.com/temporalio/temporal/blob/v1.27.2/service/worker/scanner/workflow.go#L194-L195), but [the history scanner has no such configuration](https://github.com/temporalio/temporal/blob/v1.27.2/service/worker/scanner/history/scavenger.go#L106); its `rps` [comes from persistenceMaxQPS](https://github.com/temporalio/temporal/blob/v1.27.2/service/worker/scanner/workflow.go#L137)

**Describe the solution you'd like**

I'd like to add a new dynamic configuration field for `historyScannerRPS` to tune this job. This will allow me to set a (low) limit for these heavy requests that won't overwhelm the underlying persistence DB, while keeping a higher limit for other persistence requests (so that the cluster's throughput isn't artificially limited just for these cleanup windows).

Another note: it may be useful to make the cron schedule configurable as well, defaulting to its current value. I don't _need_ the ability to change this as the current cleanups happen mostly outside regular hours, but if i were to deploy to a different region where this cleanup schedule coincided with peak traffic, I'd have no options for mitigation.

**Describe alternatives you've considered**

The main alternative I've considered is lowering the `persistenceMaxQPS` setting. I've been careful about lowering this though as I don't want the cluster's throughput to suffer outside of the cleanup time. I've had to take this setting pretty low to see any difference in the load pattern on my DB.

**Additional context**

I've never made a contribution to this project, but would be willing to submit a PR if the project is inclined to accept one. I'm also open to alternative ideas; maybe there's something I'm overlooking and I could solve this in some other way.

#### Comments (2)

<details>
<summary><strong>bergundy</strong> commented on 2025-05-01 22:44:23.000 UTC</summary>

@justinrixx we do not plan on working on this in the immediate future but contributions are welcome.

</details>

<details>
<summary><strong>justinrixx</strong> commented on 2025-05-22 17:31:40.000 UTC</summary>

@bergundy thanks

i'm waiting on some internal stuff at my company to sign off on the CLA so haven't gotten around to this yet. it looks like somebody else opened a PR as well. if that falls through by the time i get CLA approval i'll take a crack at this.

</details>


---

### #6946: PostgreSQL password in config file

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6946 |
| **State** | OPEN |
| **Author** | cloudmaster39 (Pavel Denisov) |
| **Created** | 2024-12-06 08:31:57.000 UTC (1 years ago) |
| **Updated** | 2024-12-09 06:29:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hello!

Thank you very much for such an amazing program!

Could you please help me with the following question?

I am running Temporal using systemctl with a connection to PostgreSQL. I have configured everything according to the documentation, and it works perfectly!

However, I have one concern. I really don‚Äôt like storing the database password in the configuration YAML file.  
How can I configure Temporal, when running it via a systemd unit, so that it retrieves the PostgreSQL password from an environment variable? I‚Äôve tried all possible options but haven‚Äôt had any success so far.

I would greatly appreciate your help. Thank you!


#### Comments (2)

<details>
<summary><strong>yiminc</strong> commented on 2024-12-06 17:47:03.000 UTC</summary>

@cloudmaster39, please see discussion here: https://community.temporal.io/t/temporal-with-custom-postgres-db/4787/3

</details>

<details>
<summary><strong>cloudmaster39</strong> commented on 2024-12-09 06:29:58.000 UTC</summary>

> @cloudmaster39, please see discussion here: https://community.temporal.io/t/temporal-with-custom-postgres-db/4787/3

Thank you very much for the quick reply, but my question is not answered there. I have studied all the documentation and have not found the answer.

I'm interested in how to specify the password value from the environment variable in a configuration file like this one

/etc/temporal/temporal-server.yaml 

log:
  stdout: true
  level: info

persistence:
  defaultStore: postgres-default
  visibilityStore: postgres-visibility
  numHistoryShards: 4
  datastores:
    postgres-default:
      sql:
        pluginName: "postgres12"
        connectAddr: "127.0.0.1:5432"
        connectProtocol: "tcp"
        databaseName: "temporal"
        user: "user_temporal"
        password: "password_temporal"


</details>


---

### #6834: Support start delay in continue-as-new options

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6834 |
| **State** | OPEN |
| **Author** | raymondji (Raymond Ji) |
| **Created** | 2024-11-16 16:57:34.000 UTC (1y 1m ago) |
| **Updated** | 2024-12-07 20:23:32.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
The same as https://github.com/temporalio/features/issues/515: "The use case here is to prevent from having to put a long timer (months) at the top of a workflow and force maintaining history compatibility for as long as that workflow lives."

Workflows that implement some type of loop-with-sleep pattern could benefit from this feature. Currently they would need to be long lived workflows (with the accompanying history compatibility constraints), but adding a start delay to continue-as-new options would allow them to be represented as a series of short-lived workflows.

More generally, any workflow that needs to perform work after sleeping could potentially be rewritten in a "recursive" way using this feature. They would then benefit from being a short-lived workflow and e.g. could better take advantage of worker versioning.

**Describe the solution you'd like**
Support start delay in continue-as-new options.

**Describe alternatives you've considered**
The alternatives today are:
- Sleep within the workflow, but this requires you to maintain history compatibility for the duration of the sleep
- Start a new workflow within an activity, but that loses the parent-child relationship and history.

**Additional context**
https://restate.dev/blog/code-that-sleeps-for-a-month/#control-loops describes a similar approach for a different durable execution tool.



---

### #6827: Support Environment Variable Substitution for clusterMetadata in Temporal Configuration Templates

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6827 |
| **State** | OPEN |
| **Author** | mhmtszr (Mehmet Sezer) |
| **Created** | 2024-11-15 20:50:46.000 UTC (1y 1m ago) |
| **Updated** | 2024-11-15 20:50:46.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

Currently, Temporal‚Äôs template configuration does not support substituting environment variables in the clusterMetadata section. This limitation makes it difficult to dynamically configure multi-cluster setups or enable/disable features like enableGlobalNamespace without manually modifying the configuration file.

Expected Behavior: 
- Support for environment variables within the clusterMetadata section.
- Ability to set dynamic cluster configurations (e.g., enable global namespaces, multi-cluster addresses) through environment variables.

Example:
```
clusterMetadata:
  enableGlobalNamespace: {{ env "TEMPORAL_CLUSTER_METADATA_ENABLE_GLOBAL_NAMESPACE" | default "false" }}
```



---

### #6758: API for setting RPS limits for activity tasks dispatch

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6758 |
| **State** | OPEN |
| **Author** | Shivs11 (Shivam) |
| **Created** | 2024-11-05 20:22:43.000 UTC (1y 1m ago) |
| **Updated** | 2024-11-07 16:45:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | Shivs11 |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, users are able to control the activity task dispatch RPS (rate per second) as a configuration option before instantiating a worker. After a worker has started and polls for tasks, this RPS value can only be changed by re-deploying the workers. This has proved to be a pain-point for some users

**Describe the solution you'd like**
I shall be working on implementing an API which shall allow customers to set this RPS limit. My implementation shall also prevent re-deploying of new/existing workers for changing the RPS value.

**Describe alternatives you've considered**
NA - This seems to be a good solution to solve the problem at hand.

**Additional context**
This is how customers set this limit, as of today:
```
w := worker.New(c, "hello-world", worker.Options{
		TaskQueueActivitiesPerSecond: 100, // default value: 100K
	})
```



#### Comments (2)

<details>
<summary><strong>spsoto</strong> commented on 2024-11-06 16:17:40.000 UTC</summary>

What happens to `TaskQueueActivitiesPerSecond` when the value is set via API? Does it get it ignored?

</details>

<details>
<summary><strong>Shivs11</strong> commented on 2024-11-07 16:44:09.000 UTC</summary>

The idea is that the API set values would have the highest preference for these limits. So yes, `TaskQueueActivitiesPerSecond` will be ignored if you would have set a value using the API. This will effectively stop/remove the flapping behaviour thats present with the worker option.

Care will also be taken to have an "unset" feature for the API for those users who prefer to use the worker option as a way for setting the API limit. In that case, when unsetting is done, the value will be set the way it's being set as of today.



</details>


---

### #6754: Adding a custom label to Temporal out-of-the-box metric

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6754 |
| **State** | OPEN |
| **Author** | goldtigerstone |
| **Created** | 2024-11-05 17:38:40.000 UTC (1y 1m ago) |
| **Updated** | 2024-11-05 17:38:40.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
The default out-of-the-box metrics have been very helpful. Meanwhile, I thought adding custom labels (dimensions) to these metrics would offer more precise insights for troubleshooting.

For example, we are submitting thousands of WorkflowA instances, each with multiple subtype values. While `temporal_workflow_failed` is a useful metric for tracking failures, we would have better insights if we could add subtype as a custom label in the metric, such as `temporal_workflow_failed{..., subtype=...}`.

**Describe the solution you'd like**
When adding a custom Search Attribute, it would be helpful if Temporal provided an option to include these attributes as metric labels. This would make it easier to centralize and use custom Search Attributes directly in metric labels.

**Describe alternatives you've considered**
If it were possible to reference and copy the default out-of-the-box metrics within workflow or activity code, we could modify these metrics to include additional labels.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #6724: More batch reset types

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6724 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2024-10-28 19:00:14.000 UTC (1y 2m ago) |
| **Updated** | 2025-11-04 20:12:39.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**

Currently only batch reset to first workflow task is useful. In many cases, reset to last workflow task will just complete or fail the workflow. If the activity task fail and we wanted to reset, we should reset to last workflow task -1. 


**Describe the solution you'd like**
Provide more reset types, maybe an offset based on the workflow first/last workflow task. Or by time

**Describe alternatives you've considered**
NA. No easy workaround

**Additional context**
NA



---

### #6561: Simple configuration structure

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6561 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2024-09-26 00:47:43.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-27 15:44:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

The current server configuration takes a directory and an environment name, supports merging multiple files and requires separate files for static and dynamic config.

This complexity hurts the developer experience and is the reason we don't expose server configuration in the Temporal CLI.

**Describe the solution you'd like**

Come up with a clean config structure that can be contained in a single file.

**Additional context**

Extra care should be taken if we migrate to a new format to avoid breaking existing users.

#### Comments (2)

<details>
<summary><strong>dnr</strong> commented on 2024-09-26 21:42:22.000 UTC</summary>

Straw-man proposal for dynamic config part:

- Assume that static config has been collapsed to only use one file (I'm not proposing details for that part).
- If the static config file contains a top-level key `dynamicConfig`, then the value of that key is used exactly the same way the dynamic config file is used now, including reloading on change. (Of course, changes to any other keys in the file will not take effect without restart.)
- If the config also has a value for `dynamicConfigClient.filepath`, then an error is raised on startup.
- The value of `dynamicConfigClient.pollInterval` is still respected.
- Currently `dynamicConfigClient.pollInterval` is required, with a minimum of 5s. This would be changed so it's not required anymore, defaults to infinite (do not reload), same minimum.


</details>

<details>
<summary><strong>bergundy</strong> commented on 2024-09-27 15:44:57.000 UTC</summary>

Overall SGTM. I'd maybe just call the top level key `dynamic` because you're already in the context of `config` but that's mostly nitpicking.

</details>


---

### #6481: Support wait for signal in execution history API

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6481 |
| **State** | OPEN |
| **Author** | frankmu |
| **Created** | 2024-09-04 22:26:16.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-04 22:26:16.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
Originated in this post from temporal community: https://community.temporal.io/t/await-signal-in-workflow-execution-history-api/11705

**Describe the solution you'd like**
We'd like to:
1. Support wait for signal in execution history API
2. Show this in the Temporal UI 

**Describe alternatives you've considered**
N/A

**Additional context**
N/A



---

### #6479: GKE Web UI 404 Archival Using bucket

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6479 |
| **State** | OPEN |
| **Author** | Nyuuk (Nyuuk) |
| **Created** | 2024-09-04 14:02:26.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-04 14:39:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Web UI 404 when accessing detail of workflow id from archival
![image](https://github.com/user-attachments/assets/d93e3b8f-5847-4578-810e-dcd0080bf06b)

Here is the English translation of your issue:

---

I am using Helm to deploy to GKE with the `values.archival.gcloud.yaml` as follows:

```yaml
server:
  archival:
    history:
      state: "enabled"
      enableRead: true
      provider:
        gstorage:
          credentialsPath: "/etc/credentials/credentials.json"
    visibility:
      state: "enabled"
      enableRead: true
      provider:
        gstorage:
          credentialsPath: "/etc/credentials/credentials.json"

  namespaceDefaults:
    archival:
      history:
        state: "enabled"
        URI: "gs://kp-temporal-archival/temporal_archival"
      visibility:
        state: "enabled"
        URI: "gs://kp-temporal-archival/temporal_visibility"
```

For persistence, I am using PostgreSQL with the following `values.postgresql.yaml`:

```yaml
server:
  config:
    persistence:
      default:
        driver: "sql"

        sql:
          driver: "postgres12"
          host: postgres.example.dev
          port: 5432
          database: temporal
          user: temporal
          password: temporal
          maxConns: 20
          maxConnLifetime: "1h"

      visibility:
        driver: "sql"

        sql:
          driver: "postgres12"
          host: postgres.example.dev
          port: 5432
          database: temporal_visibility
          user: temporal
          password: temporal
          maxConns: 20
          maxConnLifetime: "1h"

cassandra:
  enabled: false

mysql:
  enabled: false

postgresql:
  enabled: true

prometheus:
  enabled: false

grafana:
  enabled: false
  
elasticsearch:
  enabled: true

schema:
  createDatabase:
    enabled: false
  setup:
    enabled: true
  update:
    enabled: true
```

Then, I forward the Web UI using Internal Ingress with the following configuration:

```yaml
web:
  enabled: true
  replicaCount: 1
  image:
    repository: temporalio/ui
    tag: 2.28.0
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8080
    annotations: {}
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.regional-static-ip-name: ingress-temporal-web
      kubernetes.io/ingress.allow-http: "true"
      kubernetes.io/ingress.class: "gce-internal"
    hosts:
      - "temporal.example.dev"
    tls:
      - secretName: temporal
        hosts:
          - temporal.example.dev
```

The archival and visibility files are already in the bucket.
![image](https://github.com/user-attachments/assets/164719ba-f231-4632-8156-bace700a6377)
![image](https://github.com/user-attachments/assets/ad638eaf-a003-49f2-824c-9054a0a07c69)
---
and this is my version server `1.24.2`

#### Comments (2)

<details>
<summary><strong>Akiyaaaaaa</strong> commented on 2024-09-04 14:18:13.000 UTC</summary>

I am encountering the same issue when accessing the details of a temporal workflow. Here's the result from the browser network:
![Here's the result from the browser network: ](https://github.com/user-attachments/assets/9d23602a-6d72-4956-abc8-1308965b089d)



</details>

<details>
<summary><strong>Nyuuk</strong> commented on 2024-09-04 14:39:12.000 UTC</summary>

> I am encountering the same issue when accessing the details of a temporal workflow. Here's the result from the browser network: ![Here's the result from the browser network: ](https://private-user-images.githubusercontent.com/107343416/364414620-9d23602a-6d72-4956-abc8-1308965b089d.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU0NjA4NzYsIm5iZiI6MTcyNTQ2MDU3NiwicGF0aCI6Ii8xMDczNDM0MTYvMzY0NDE0NjIwLTlkMjM2MDJhLTZkNzItNDk1Ni1hYmM4LTEzMDg5NjViMDg5ZC5qcGc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwOTA0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDkwNFQxNDM2MTZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hYzgwNDBkNDdmNzZiNmFiYWE1MjU5MWE4NDU0MDkyMjMxMDE5ZmJlZGFmMGQxNmRjYWQ3NTAyM2JkYmI5YmFjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.s8ZiXpJslVcgWy9dPE-RyUcGHaQT0iGL1b3RcTTF8PM)

what's your environment do you use? docker, local, or kubernetes?

</details>


---

### #6467: CVE-2024-24790

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6467 |
| **State** | OPEN |
| **Author** | m-barthelemy (Matthieu Barthelemy) |
| **Created** | 2024-08-30 01:06:07.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-30 01:06:07.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

The latest temporalio/admin-tools, temporalio/server and temporalio/ui images are using a version of Go that is impacted by CVE-2024-24790.

Updating Go to 1.21.11 or newer would fix the issue.



---

### #6333: Do not consume rate limit token on invalid tasks

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6333 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2024-07-24 19:21:38.000 UTC (1y 5m ago) |
| **Updated** | 2025-04-08 05:14:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

When setting maxTaskQueueActivitiesPerSecond to enable rate limit on task queue, the token is consumed when task is attempted to be delivered. However, the task could be invalid (expired or workflow already closed), which lead to waste of rate limit token.

We should change this to only consume token when task is delivered. 

#### Comments (2)

<details>
<summary><strong>dnr</strong> commented on 2024-07-24 21:08:57.000 UTC</summary>

I think if you do this in the obvious way, it'll create more problems. Like if you block on the rate limiter after a task is assigned to a poller, and then the poller times out, you have to put the task on the end of the queue, so you'd get a lot more cycling and maybe fail to deliver entirely for low rate limits.

Two other approaches could be:
1. If you fail to deliver the task for any reason after it comes from the matcher, return the token that was taken (so another task can go immediately). This would probably look like storing a reservation in the task and canceling it in finish if there was an error.
2. Make the task validator more aggressive so these tasks are more likely to be thrown away before they get matched.


Reactions: üëç 1

</details>

<details>
<summary><strong>dnr</strong> commented on 2024-08-27 18:41:31.000 UTC</summary>

For 1 (returning the token after delivery failure), it turns out that the rate limiter doesn't support canceling a reservation after its time has passed, so the logic would have to be more complicated.

</details>


---

### #6273: PostgreSQL schema update fails on busy Temporal instances

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6273 |
| **State** | OPEN |
| **Author** | yurkeen (Yury Evtikhov) |
| **Created** | 2024-07-11 15:36:54.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-11 15:38:01.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

We run schema update jobs during our deployment process. The command looks like the following
```yaml
command: ["temporal-sql-tool", "--plugin", "postgres", "update-schema", "-d", "{{ $storeConfig.schemaPath }}"]
```

## Expected Behavior
In the testing/staging environment where our Temporal instances don't have much workload and where the persistency and visibility databases are both relatively small, migrations run without any issues with the desired effect:

```
2024-07-11T13:38:03.670Z	INFO	UpdateSchemaTask started	{"config": {"DBName":"","TargetVersion":"","SchemaDir":"/etc/temporal/schema/postgresql/v12/visibility/versioned","SchemaName":"","IsDryRun":false}, "logging-call-at": "updatetask.go:104"}
2024-07-11T13:38:03.675Z	DEBUG	Schema Dirs: [v1.6]	{"logging-call-at": "updatetask.go:212"}
2024-07-11T13:38:03.675Z	INFO	Processing schema file: v1.6/fix_root_workflow_info.sql	{"logging-call-at": "updatetask.go:256"}
2024-07-11T13:38:03.675Z	DEBUG	running 1 updates for current version 1.5	{"logging-call-at": "updatetask.go:138"}
2024-07-11T13:38:03.675Z	DEBUG	---- Executing updates for version 1.6 ----	{"logging-call-at": "updatetask.go:157"}
2024-07-11T13:38:03.675Z	DEBUG	DROP INDEX by_root_workflow_id;	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.068Z	DEBUG	DROP INDEX by_root_run_id;	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.794Z	DEBUG	ALTER TABLE executions_visibility DROP COLUMN root_workflow_id;	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.798Z	DEBUG	ALTER TABLE executions_visibility DROP COLUMN root_run_id;	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.799Z	DEBUG	ALTER TABLE executions_visibility ADD COLUMN root_workflow_id VARCHAR(255) NOT NULL DEFAULT '';	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.802Z	DEBUG	ALTER TABLE executions_visibility ADD COLUMN root_run_id VARCHAR(255) NOT NULL DEFAULT '';	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.804Z	DEBUG	CREATE INDEX by_root_workflow_id ON executions_visibility (namespace_id, root_workflow_id, (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);	{"logging-call-at": "updatetask.go:159"}
...
2024-07-11T13:38:49.703Z	INFO	UpdateSchemaTask done	{"logging-call-at": "updatetask.go:127"}
```
## Actual Behavior
For our production instances, however, which are quite active and where databases are in constant use (~1TB for persistence and ~120 Gb visibility), index creation fails constantly.
Part of the equation is that we have deadlock detection mechanism and our PostgreSQL instances would terminate certain blocking queries running over a configured time limit. This is often the case when manipulating indices referencing busy tables:

```
2024-07-11T13:38:03.670Z	INFO	UpdateSchemaTask started	{"config": {"DBName":"","TargetVersion":"","SchemaDir":"/etc/temporal/schema/postgresql/v12/visibility/versioned","SchemaName":"","IsDryRun":false}, "logging-call-at": "updatetask.go:104"}
2024-07-11T13:38:03.675Z	DEBUG	Schema Dirs: [v1.6]	{"logging-call-at": "updatetask.go:212"}
2024-07-11T13:38:03.675Z	INFO	Processing schema file: v1.6/fix_root_workflow_info.sql	{"logging-call-at": "updatetask.go:256"}
2024-07-11T13:38:03.675Z	DEBUG	running 1 updates for current version 1.5	{"logging-call-at": "updatetask.go:138"}
2024-07-11T13:38:03.675Z	DEBUG	---- Executing updates for version 1.6 ----	{"logging-call-at": "updatetask.go:157"}
2024-07-11T13:38:03.675Z	DEBUG	DROP INDEX by_root_workflow_id;	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.068Z	DEBUG	DROP INDEX by_root_run_id;	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.794Z	DEBUG	ALTER TABLE executions_visibility DROP COLUMN root_workflow_id;	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.798Z	DEBUG	ALTER TABLE executions_visibility DROP COLUMN root_run_id;	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.799Z	DEBUG	ALTER TABLE executions_visibility ADD COLUMN root_workflow_id VARCHAR(255) NOT NULL DEFAULT '';	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.802Z	DEBUG	ALTER TABLE executions_visibility ADD COLUMN root_run_id VARCHAR(255) NOT NULL DEFAULT '';	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:04.804Z	DEBUG	CREATE INDEX by_root_workflow_id ON executions_visibility (namespace_id, root_workflow_id, (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);	{"logging-call-at": "updatetask.go:159"}
2024-07-11T13:38:41.793Z	ERROR	Unable to update SQL schema.	{"error": "error executing statement: driver: bad connection", "logging-call-at": "handler.go:78"}
```
The `error executing statement: driver: bad connection` error  is just an indication that query killer was engaged to terminate the query which blocks longer than the allowed limit.


## Specifications

  - Version: 1.24.2 (and earlier)
  - Platform: N/A

## Proposed Solution

The proposed solution for the issue should be creating the indices concurrently, so the `CREATE INDEX` requests are not blocking. For example `schema/postgresql/v12/visibility/versioned/v1.6/fix_root_workflow_info.sql` would have the following changes:

```diff
--- a/schema/postgresql/v12/visibility/versioned/v1.6/fix_root_workflow_info.sql
+++ b/schema/postgresql/v12/visibility/versioned/v1.6/fix_root_workflow_info.sql
ALTER TABLE executions_visibility ADD COLUMN root_workflow_id VARCHAR(255) NOT NULL DEFAULT '';
ALTER TABLE executions_visibility ADD COLUMN root_run_id      VARCHAR(255) NOT NULL DEFAULT '';
- CREATE INDEX by_root_workflow_id  ON executions_visibility (namespace_id, root_workflow_id, (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);
- CREATE INDEX by_root_run_id       ON executions_visibility (namespace_id, root_run_id,      (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);
+ CREATE INDEX CONCURRENTLY by_root_workflow_id  ON executions_visibility (namespace_id, root_workflow_id, (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);
+ CREATE INDEX CONCURRENTLY by_root_run_id       ON executions_visibility (namespace_id, root_run_id,      (COALESCE(close_time, '9999-12-31 23:59:59')) DESC, start_time DESC, run_id);

```



---

### #6255: Add support for a "final workflow state retention" on successful exit optimization

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6255 |
| **State** | OPEN |
| **Author** | ghaskins (Gregory Haskins) |
| **Created** | 2024-07-09 02:08:37.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-09 20:59:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

Many of our workflows process a decent amount of activity data and conclude with a short final status.  We like the retention window to preserve de-duplication but don't need the full history for successfully exited workflows.  However, Temporal naturally stores the full history for the entire retention window, wasting space in Cassandra.

### Describe the solution you'd like

One solution to the above dilemma would be offering the ability to configure the system (perhaps on a per-workflow definition or invocation basis) where the developer indicates that the full history is unnecessary for successful workflows.  When enabled, the workflow ID and final state would remain available for de-duplication and final-state retrieval, but the rest of the history (or at least the larger parts, such as data payloads) could be optimized out of storage.

### Per-SDK Tickets

<!-- Add links here once the tickets are created (no need to create them immediately). -->

- [ ] Go - 
- [ ] Java - 
- [ ] Core - 
- [ ] TypeScript - 
- [ ] Python - 
- [ ] .NET - 
- [ ] PHP - 
- [ ] Temporal CLI - 


#### Comments (2)

<details>
<summary><strong>cretz</strong> commented on 2024-07-09 13:34:39.000 UTC</summary>

This seems like a server/platform request as opposed to using the issue template for a cross-SDK request in the features repo. I am transferring to the server repository. If something like this is built and there does need to be some kind of SDK exposure of the option, we can open a separate issue then.

</details>

<details>
<summary><strong>yiminc</strong> commented on 2024-07-09 20:59:46.000 UTC</summary>

This would be similar to https://github.com/temporalio/temporal/issues/487

</details>


---

### #6239: Allow SDKs to change workflow task max retry interval in WorkerOptions

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6239 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2024-07-04 17:02:32.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-04 17:02:32.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

Currently users change workflow task timeout via WorkerOptions. In some use cases users want to also 
change the max retry interval for workflow tasks which is 10min. 

Feature request is to add ability to set this via WorkerOptions in SDKs


---

### #6226: "Flag provided but not defined" when trying to specify config path

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6226 |
| **State** | OPEN |
| **Author** | axfelix (Alex Garnett) |
| **Created** | 2024-07-03 15:43:26.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-03 16:10:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

`temporal-server start -c /path/to/config` would load a config path


## Actual Behavior

`Incorrect Usage: flag provided but not defined: -c`

This happens when passing `--config` as well, and it also happens with the new ui-server binaries.


## Steps to Reproduce the Problem

  1. Try to path a config path to the binary at runtime

## Specifications

  - Version: newest release (1.24.2)
  - Platform: bash on Ubuntu 22.04


#### Comments (2)

<details>
<summary><strong>dnr</strong> commented on 2024-07-03 16:03:03.000 UTC</summary>

The config flags are weird for historical reasons. `-c` is a global flag (i.e. has to be given before `start`), but it just specifies the directory. For the file within the directory, add `-e file`, but without the `.yaml` extension.

</details>

<details>
<summary><strong>axfelix</strong> commented on 2024-07-03 16:10:12.000 UTC</summary>

Ah, gotcha. We should probably change the CLI help messages then, since this isn't clear at all otherwise.

</details>


---

### #6173: Schedule "StartAt" not used when calculating intervals?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6173 |
| **State** | OPEN |
| **Author** | nunofgs (Nuno Sousa) |
| **Created** | 2024-06-19 17:59:22.000 UTC (1y 6m ago) |
| **Updated** | 2024-06-21 20:04:47.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | lina-temporal |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

Today is _June 19th, 2024_. If you create a schedule and specify "every 365 days" the result will be that the first run is on `2024-12-18 00:00:00`. **That's in 6 months**. I've tried passing `StartAt: 2025-06-19` but I get `2025-12-18 00:00:00` as a result.

I understand this may be known and as-designed behavior but it struck me as unexpected to ask for a 365 day interval, yet the first run is within the next 6 months.

I also don't see any way to achieve my desired result. Am I missing anything?

_EDIT_: I also can't use the _offset_ to control this behavior because I don't know what the final calculation will be until the schedule is created.

## Expected Behavior

In the example above, I'd prefer to have the schedule's first run be at my specified "StartAt". So, if I say:

"Start this workflow every 365 days with start at 2025-02-03 04:05:06", then the upcoming runs would look like:

* 2025-02-03 04:05:06
* 2026-02-03 04:05:06
* 2027-02-03 04:05:06
* etc.

## Actual Behavior

When I pick 365 days as the interval, I seem to be getting a date 6 months in the future.


## Steps to Reproduce the Problem

  1. Go to temporal UI and add a schedule.
  1. Choose 365 days and save.
  1. Check the schedule's upcoming runs.

## Specifications

  - Version:
  - Platform:



---

### #5785: deleted schedule showing

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5785 |
| **State** | OPEN |
| **Author** | jesse-triplewhale |
| **Created** | 2024-04-24 09:23:06.000 UTC (1y 8m ago) |
| **Updated** | 2024-04-30 18:21:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

version: 1.22.4
self-hosted

I deleted a schedule in the UI, but it is still returning in the web UI and via
```
$ temporal schedule list
  ScheduleId    WorkflowType   Paused  Notes  NextRunTime  LastRunTime  
...
```

When I try to delete
```
$ temporal schedule delete ...
Error: unable to delete schedule: workflow execution already completed
Stack trace:
goroutine 1 [running]:
runtime/debug.Stack()
        /opt/hostedtoolcache/go/1.20.11/x64/src/runtime/debug/stack.go:24 +0x65
runtime/debug.PrintStack()
        /opt/hostedtoolcache/go/1.20.11/x64/src/runtime/debug/stack.go:16 +0x19
github.com/temporalio/cli/app.HandleError(0x24d5b60?, {0x320df40, 0xc00065a2e0})
        /home/runner/work/cli/cli/app/app.go:73 +0x15b
github.com/urfave/cli/v2.(*App).handleExitCoder(0xc0008fe160?, 0xc000225400?, {0x320df40?, 0xc00065a2e0?})
        /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/app.go:452 +0x38
github.com/urfave/cli/v2.(*Command).Run(0xc0008fe160, 0xc000699bc0, {0xc0000af770, 0x5, 0x5})
        /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/command.go:276 +0xa18
github.com/urfave/cli/v2.(*Command).Run(0x4744c20, 0xc000699a80, {0xc0000a5aa0, 0x6, 0x6})
        /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/command.go:267 +0xc4d
github.com/urfave/cli/v2.(*Command).Run(0xc000900c60, 0xc000699940, {0xc000050070, 0x7, 0x7})
        /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/command.go:267 +0xc4d
github.com/urfave/cli/v2.(*App).RunContext(0xc0007b25a0, {0x322f340?, 0xc00005a028}, {0xc000050070, 0x7, 0x7})
        /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/app.go:332 +0x616
github.com/urfave/cli/v2.(*App).Run(0x60?, {0xc000050070?, 0x0?, 0x0?})
        /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.25.7/app.go:309 +0x38
main.main()
        /home/runner/work/cli/cli/cmd/temporal/main.go:14 +0x33
```

Thank you!

#### Comments (2)

<details>
<summary><strong>dnr</strong> commented on 2024-04-25 19:48:12.000 UTC</summary>

That looks like the change to the status of the schedule workflow hasn't made it to visibility. Which main and visibility backends are you using? Have you noticed any other visibility issues, any other situations where the result of ListWorkflows isn't in sync with the status of running workflows? Any visibility-related errors in logs or metrics, i.e. for visibility task processing or ES bulk processor (if using ES)?

</details>

<details>
<summary><strong>jesse-triplewhale</strong> commented on 2024-04-30 18:21:33.000 UTC</summary>

Hi, thanks for the response - we're using postgres + Elasticsearch. I haven't noticed any other behaviors like this (although I think I have seen this behavior multiple times). I ended up deleting and recreating the namespace to clean it up

</details>


---

### #5729: Temporal fails to connect to Google Cloud SQL Postgres when password contains certain characters

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5729 |
| **State** | OPEN |
| **Author** | NAjustin (Justin Beasley) |
| **Created** | 2024-04-15 22:15:39.000 UTC (1y 8m ago) |
| **Updated** | 2024-08-30 17:58:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
Temporal should connect correctly regardless of what characters are contained in the password.

## Actual Behavior
Temporal actually has mixed results when certain characters are present:
1. The `temporal-sql-tool` appears to run correctly. For example, it creates the `temporal` and `temporal_visibility` databases, as well as creates/updates the schemas. **This also proves that the password is, in fact, _valid_.**
2. But that's where success ends. Once the creation/update is handled, connections begin to error: `sql schema version compatibility check failed: pq: password authentication failed for user "example-user"`

Here's the full section of logs containing errors:
```
[Fx] ERROR  Failed to initialize custom logger: could not build arguments for function "go.uber.org/fx".(*App).constructCustomLogger.func2
/go/pkg/mod/go.uber.org/fx@v1.18.2/app.go:414:
failed to build fxevent.Logger:
could not build arguments for function "go.temporal.io/server/temporal".glob..func8
/home/builder/temporal/temporal/fx.go:1025:
failed to build log.Logger:
received non-nil error from function "go.temporal.io/server/temporal".ServerOptionsProvider
/home/builder/temporal/temporal/fx.go:159:
sql schema version compatibility check failed: pq: password authentication failed for user "example-user"
Unable to create server. Error: could not build arguments for function "go.uber.org/fx".(*App).constructCustomLogger.func2 (/go/pkg/mod/go.uber.org/fx@v1.18.2/app.go:414): failed to build fxevent.Logger: could not build arguments for function "go.temporal.io/server/temporal".glob..func8 (/home/builder/temporal/temporal/fx.go:1025): failed to build log.Logger: received non-nil error from function "go.temporal.io/server/temporal".ServerOptionsProvider (/home/builder/temporal/temporal/fx.go:159): sql schema version compatibility check failed: pq: password authentication failed for user "example-user".
```

## Steps to Reproduce the Problem

  1. Use special characters in your password. Verified (failed) example: `y{0zzu2p3\t*az<g`
  2. Deploy Temporal (I used Helm)
  3. Enjoy the errors

Changing the password to not include the above characters makes the identical setup work correctly (and also fixes the surrounding errors). I didn't have the time to hunt down the offending character sequence, but my money is on the `\t` getting turned into a tab somewhere in the execution (e.g. before or after the password gets turned into a DSN)‚Äîbut because the `temporal-sql-tool` scripts run in the same deployment, I think it's safe to rule out environmental factors. It may just be that `net/url.QueryEscape` isn't the right tool for the job to always generate a valid DSN, or it may be somewhere else in that code flow.

(It doesn't appear that this code flow has changed in more recent versions than what's being supplied in my upstream.)

## Specifications

  - Version: `1.20.1` (tested via `temporalio/auto-setup:1.20.1`)
  - Platform: GKE Autopilot (password passed via env variable from Kubernetes Secret) using the Postgres plugin/DB provider to connect to Google Cloud SQL (Postgres 13).


#### Comments (2)

<details>
<summary><strong>jaylin-sf</strong> commented on 2024-07-04 23:59:23.000 UTC</summary>

I have the same issue with 1.24.2, the databases in PostgreSQL (Azure Flexible Server) have been created successfully. But it failed to build log.Logger for some reason.

Unable to create server. Error: could not build arguments for function "go.uber.org/fx".(*module).constructCustomLogger.func2 (/home/runner/go/pkg/mod/go.uber.org/fx@v1.21.1/module.go:292): failed to build fxevent.Logger: could not build arguments for function "go.temporal.io/server/temporal".glob..func8 (/home/runner/work/docker-builds/docker-builds/temporal/temporal/fx.go:1009): failed to build log.Logger: received non-nil error from function "go.temporal.io/server/temporal".ServerOptionsProvider (/home/runner/work/docker-builds/docker-builds/temporal/temporal/fx.go:183): sql schema version compatibility check failed: pq: no pg_hba.conf entry for host "20.227.4.75", user "temporal_pg_test", database "temporal", no encryption.

BTW: I passed in CA file in environment variables with the latest docker image (temporalio/auto-setup):
POSTGRES_TLS_ENABLED = true
POSTGRES_TLS_CA_FILE = {path from container volume}

DB = postgres12

</details>

<details>
<summary><strong>raymondregrello</strong> commented on 2024-08-30 17:58:30.000 UTC</summary>

Ran into the issue as well, I believe it'll get fixed by https://github.com/temporalio/helm-charts/pull/551

</details>


---

### #5624: Archival does not show list of archived workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5624 |
| **State** | OPEN |
| **Author** | js-michaels (Joseph Michaels) |
| **Created** | 2024-03-28 18:20:25.000 UTC (1y 9m ago) |
| **Updated** | 2024-05-02 07:55:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
Viewing the Archival tab for a namespace which has archived workflows should display them.

## Actual Behavior
The Archival tab is displaying the message "No Workflows running in this Namespace" even though there are archived workflows present in the configured archival S3 bucket. The GET call to `api/v1/namespaces/<namespace>/archived-workflows?query=` is failing with an HTTP 400.

## Steps to Reproduce the Problem

  1. Create an S3 bucket for Archival.
  2. Ensure that Temporal's K8s service account has the below IAM permissions, changing the bucket name here to match.
```
{
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetBucketLocation",
                "s3:PutObject",
                "s3:PutObjectAcl",
                "s3:GetObject",
                "s3:GetObjectAcl",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::temporal-workflow-archive-bucket",
                "arn:aws:s3:::temporal-workflow-archive-bucket/*"
            ]
        }
    ]
}
```
  4. Add the below configuration to the Temporal Helm chart, changing the bucket name here to match.
```
config:
    archival:
      history:
        state: enabled
        enableRead: true
        provider:
          s3store:
            region: <aws region>
      visibility:
        state: enabled
        enableRead: true
        provider: 
          s3store:
            region: <aws region>

    namespaceDefaults:
      archival:
        history:
          state: enabled
          URI: s3://temporal-workflow-archive-bucket
        visibility:
          state: enabled
          URI: s3://temporal-workflow-archive-bucket
```
  5. Turn on archival and visibility for a namespace that has workflows running, and wait for the retention period.
  6. View the archival bucket and observe that there are files present for those archived workflows.
  7. Go to the Archival tab for that namespace and observe the message "No Workflows running in this Namespace" and the failed call to `api/v1/namespaces/<namespace>/archived-workflows?query=`.

## Specifications

  - Version: 1.22.5
  - Platform: AWS, EKS, amd64


#### Comments (2)

<details>
<summary><strong>js-michaels</strong> commented on 2024-03-30 22:59:06.000 UTC</summary>

Some more information about this issue:

The call to `api/v1/namespaces/banking/archived-workflows?query=` which returns an HTTP 400 is giving the following error:
```json
{
  "code": 3,
  "message": "Cluster is not configured for reading archived visibility records."
}
```

This error message is defined here:
```golang
errClusterIsNotConfiguredForReadingArchivalVisibility = serviceerror.NewInvalidArgument("Cluster is not configured for reading archived visibility records.")
```
https://github.com/temporalio/temporal/blob/main/service/frontend/errors.go#L62

And seems to be only used here:
```golang
if !wh.archivalMetadata.GetVisibilityConfig().ReadEnabled() {
    return nil, errClusterIsNotConfiguredForReadingArchivalVisibility
}
```
https://github.com/temporalio/temporal/blob/main/service/frontend/workflow_handler.go#L2131

It looks like ultimately the value of `wh.archivalMetadata.GetVisibilityConfig().ReadEnabled()` comes from the configuration loaded from the config yaml, since `cfg.Archival.Visibility.EnableRead` maps to the yaml key path `archival.visibility.enableRead`:
```golang
func ArchivalMetadataProvider(dc *dynamicconfig.Collection, cfg *config.Config) archiver.ArchivalMetadata {
	return archiver.NewArchivalMetadata(
		dc,
		cfg.Archival.History.State,
		cfg.Archival.History.EnableRead,
		cfg.Archival.Visibility.State,
		cfg.Archival.Visibility.EnableRead,
		&cfg.NamespaceDefaults.Archival,
	)
}
```
https://github.com/temporalio/temporal/blob/main/common/resource/fx.go#L350

However, I can see from the temporal-frontend Pod's logs that it's loading config/docker.yaml:
```
2024/03/29 15:52:04 Loading config; env=docker,zone=,configDir=config
2024/03/29 15:52:04 Loading config files=[config/docker.yaml]
```

And if I shell into the container, I can see the config file does have the correct configuration:
```
temporal-frontend-f56454dff-7dr26:/etc/temporal$ cat config/docker.yaml
<...>
archival:
  history:
    state: "enabled"
    enableRead: true
    provider:
      s3store:
        logLevel: 0
        region: #######
  visibility:
    state: "enabled"
    enabledRead: true
    provider:
      s3store:
        logLevel: 0
        region: #######

namespaceDefaults:
  archival:
    history:
      URI: #######
      state: enabled
    visibility:
      URI: #######
      state: enabled
<...>
```

So why is temporal-frontend acting like this is not configured?

Reactions: üëÄ 1

</details>

<details>
<summary><strong>gad26032</strong> commented on 2024-05-01 07:49:56.000 UTC</summary>

Same issue. But i use local file system as a backend.
```
  archival:
    history:
      state: "enabled"
      enableRead: true
      provider:
        filestore:
          fileMode: "0666"
          dirMode: "0766"
    visibility:
      state: "enabled"
      enableRead: true
      provider:
        filestore:
          fileMode: "0666"
          dirMode: "0766"

  namespaceDefaults:
    archival:
      history:
        state: "enabled"
        URI: "file:///home/temporal/temporal_archival/data"
      visibility:
        state: "enabled"
        URI: "file:///home/temporal/temporal_vis_archival/data"
```
describe namespace
```
temporal-admintools-94844b8c5-v5957:/etc/temporal$ temporal operator namespace describe dev
  NamespaceInfo.Name                    dev                                               
  NamespaceInfo.Id                      c975b631-f109-41f7-865a-04b341fbe922              
  NamespaceInfo.Description                                                               
  NamespaceInfo.OwnerEmail                                                                
  NamespaceInfo.State                   Registered                                        
  NamespaceInfo.Data                    map[]                                             
  Config.WorkflowExecutionRetentionTtl  24h0m0s                                           
  ReplicationConfig.ActiveClusterName   active                                            
  ReplicationConfig.Clusters            [&ClusterReplicationConfig{ClusterName:active,}]  
  Config.HistoryArchivalState           Enabled                                           
  Config.VisibilityArchivalState        Enabled                                           
  IsGlobalNamespace                     false                                             
  FailoverVersion                                                                      0  
  FailoverHistory                       []                                                

```

on temporal-history pod

```
temporal-history-6c4ffd4497-gdjzn:~$ tree 
.
‚îú‚îÄ‚îÄ temporal_archival
‚îÇ   ‚îî‚îÄ‚îÄ data
‚îÇ       ‚îú‚îÄ‚îÄ 9432711404182916402104093526875229053638092447613532531101_0.history
‚îÇ       ‚îú‚îÄ‚îÄ 94327114041829164021084163317621728861618148438391857267770_0.history
‚îÇ       ‚îú‚îÄ‚îÄ 94327114041829164021091822365132548957316345918779004043580_0.history
...
‚îÇ       ‚îú‚îÄ‚îÄ 943271140418291640294944933450628752312303702040985760010_0.history
‚îÇ       ‚îú‚îÄ‚îÄ 943271140418291640295710937541081983910480885637690598829_0.history
‚îÇ       ‚îî‚îÄ‚îÄ 9432711404182916402970473396858808043215067898247463723775_0.history
‚îî‚îÄ‚îÄ temporal_vis_archival
    ‚îî‚îÄ‚îÄ data
        ‚îî‚îÄ‚îÄ c975b631-f109-41f7-865a-04b341fbe922
            ‚îú‚îÄ‚îÄ 1714523543567043851_3107041130708393426.visibility
            ‚îú‚îÄ‚îÄ 1714523665551876245_8603905498755678542.visibility
            ‚îú‚îÄ‚îÄ 1714523666297386886_281047780464377597.visibility
...
            ‚îú‚îÄ‚îÄ 1714526336198869418_5221394463637855399.visibility
            ‚îú‚îÄ‚îÄ 1714527023940092587_4563104817072754649.visibility
            ‚îú‚îÄ‚îÄ 1714527023998615275_17488082703656383857.visibility
            ‚îî‚îÄ‚îÄ 1714527261355949256_15312822523284306755.visibility

5 directories, 64 files
```
But when i try to get history, i get this

```
temporal-admintools-94844b8c5-v5957:/etc/temporal$ temporal workflow list --archived dev
Error: unable to list archived workflow executions: Namespace is not configured for visibility archival.
('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
```

Web UI just doesn't show anything on the Archived page


## UPD 2024-04-02:

I've decide to redeploy temporal from scratch. 
After that logs don't show any errors but history is still not sows on Archive page.
Request from admin-tools container now works without errors but shows nothing
```
temporal-admintools-94844b8c5-pptl4:/etc/temporal$ temporal workflow list --archived dev
temporal-admintools-94844b8c5-pptl4:/etc/temporal$ 
```
History container do have files with history
```
temporal-history-6c4ffd4497-7jxfk:~$ tree
.
‚îú‚îÄ‚îÄ temporal_archival
‚îÇ   ‚îî‚îÄ‚îÄ data
‚îÇ       ‚îú‚îÄ‚îÄ 3153029508883409284100556632954027255031624317270243527795_0.history
‚îÇ       ‚îú‚îÄ‚îÄ 315302950888340928410077844079520042907189990798703046629_0.history
‚îÇ       ‚îú‚îÄ‚îÄ 31530295088834092841014179372472256450213867944691256176036_0.history
...      ...
‚îÇ       ‚îú‚îÄ‚îÄ 315302950888340928494255717878218597641732674283183740556_0.history
‚îÇ       ‚îú‚îÄ‚îÄ 31530295088834092849742622859503433869401773985681693035_0.history
‚îÇ       ‚îî‚îÄ‚îÄ 3153029508883409284992368164740503790016816529257862930874_0.history
‚îî‚îÄ‚îÄ temporal_vis_archival
    ‚îî‚îÄ‚îÄ data
        ‚îî‚îÄ‚îÄ 71c27c7d-edca-4ed2-b986-1bbcd9aa94c5
            ‚îú‚îÄ‚îÄ 1714567282922524643_8163139250068973546.visibility
            ‚îú‚îÄ‚îÄ 1714568230253421245_10586275668407886897.visibility
            ‚îú‚îÄ‚îÄ 1714568884468711750_1733191977649475147.visibility
             ...
            ‚îú‚îÄ‚îÄ 1714613295491251227_14703381710616799526.visibility
            ‚îú‚îÄ‚îÄ 1714613318262705922_1624317270243527795.visibility
            ‚îî‚îÄ‚îÄ 1714613512563957127_16827496660219090821.visibility

5 directories, 74 files
temporal-history-6c4ffd4497-7jxfk:~$ 
```

Also i've noticed that `temporal-system` namespace has weird workflows that seems stuck
![Selection_144](https://github.com/temporalio/temporal/assets/11608884/18e69694-6065-4918-8283-d1ba3a1a3a85)

Is it proper behavior or I have some miss-configuration.

Any clues?


</details>


---

### #5105: Add support to create Elasticsearch rolling indexes in visibility

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5105 |
| **State** | OPEN |
| **Author** | jeacott1 |
| **Created** | 2023-11-12 07:24:16.000 UTC (2y 1m ago) |
| **Updated** | 2023-11-22 11:28:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | rodrigozhou |
| **Milestone** | None |

#### Description

I would like to be able to keep an Elasticsearch cluster performant,  manageable, and affordable by having the ES visibility storage provider roll to a new indexname periodically. 
ie per month index names might be {namespace}-202301, {namespace}-202302, etc, or per year {namespace}-2023, {namespace}-2024. A configurable user defined pattern that allows custom field values as target indexnames would also be a nice add. There should then be a means to search across a range of indexes (with the default range being configurable, or all?)

This would allow searches to target the indexes by date range (or custom field etc) thereby being much more efficient. This would also allow automatic ES migration of older content to cheaper ES tiers, or archiving them.

**Describe alternatives you've considered**
I am not aware of any real alternative. 


#### Comments (2)

<details>
<summary><strong>rodrigozhou</strong> commented on 2023-11-21 18:14:21.000 UTC</summary>

We don't have support for this at this moment. But I wonder if you created index aliases, maybe it might work.

</details>

<details>
<summary><strong>jeacott1</strong> commented on 2023-11-22 01:07:16.000 UTC</summary>

@rodrigozhou I'm not sure how aliases would help with creating the index names. 
ES will create an index based on whatever name you write to - afaik there's no option to make this dynamic with temporal right now. This would be a small change that would at least relieve the single index problem.
As for reading across the multiple generated indexes, yes you could use an alias in the short term, but ideally you generate a list of index names covering the timespan you care about and query with those. Otherwise with a simple alias pattern of my-index-* you'll always be querying the ever growing list of all the indexes which will eventually break. It also means you cant move old indexes to cold nodes because they'll constantly be hit.

edit -  ES does have a feature where you can write to an index with a construct like index-{now/d{YYYYMMDD}
but it's not at all clear to me if it accepts real timestamps instead of now.
In order for this feature to be useful and idempotent, the date needs to come from an actual document field, not just now.


</details>


---

### #5026: When I tested with more than 200 workers connectedÔºåToo high memory usage on matching service

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5026 |
| **State** | OPEN |
| **Author** | DSkyrim |
| **Created** | 2023-10-24 07:58:31.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-27 21:47:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

When I tested with more than 200 workers connected, I encountered some issues; my server memory was full,Too high memory usage on matching service  and subsequently, I discovered the disk IO was also maxed out. I deployed Temporal using Helm and utilized a standalone MySQL. My k8s node has 2 CPU cores and 2GB of memory. I want to know how should I configure my cluster if there are tens of thousands of workers connected.

#### Comments (2)

<details>
<summary><strong>DSkyrim</strong> commented on 2023-10-24 08:06:11.000 UTC</summary>

Too high memory usage on matching service

</details>

<details>
<summary><strong>yiminc</strong> commented on 2023-10-27 21:47:42.000 UTC</summary>

Your server might be too small for that many workers. Usually you don't need that many worker for small setup.

</details>


---

### #4947: Add fields to `ListBatchOperations` and `DescribeBatchOperation` API responses

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4947 |
| **State** | OPEN |
| **Author** | rossedfort (Ross Edfort) |
| **Created** | 2023-10-09 17:35:05.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-09 17:44:40.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
After building pages in the Web UI to visualize Batch Operations, I observed some improvements that could be made to the `ListBatchOperations` and `DescribeBatchOperation` APIs. Specifically, when listing Batch Operations, only 4 fields are returned. This limits users in trying to differentiate their Batch Operations when listed in a tabular format, as the only uniquely identifiable field that is returned is the Job ID. Additionally, there is no way for a user to associate the Batch Operation back to a list of Workflows that were affected by the Batch Operation.

**Describe the solution you'd like**
I'd like to suggest that the following additions are made:
- operation type is added to the `ListBatchOperations` response. 
- the visibility query (or the list of workflow executions when a batch operation was created with the `executions` parameter) is returned on the `DescribeBatchOperation` response.

**Describe alternatives you've considered**
N/a

**Additional context**
Add any other context or screenshots about the feature request here.
Currently the information we receive in the `ListBatchOperations` API response is quite limited and does not provide a lot of meaningful context to the user.
<img width="970" alt="Screenshot 2023-10-09 at 11 31 50 AM" src="https://github.com/temporalio/temporal/assets/11775628/ed4ab9a7-dc18-4ec4-9eba-d738ebda7c2b">



---

### #4823: Bearer token not passed to remote cluster when adding or upserting remote cluster

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4823 |
| **State** | OPEN |
| **Author** | sonrel |
| **Created** | 2023-08-31 12:21:48.000 UTC (2y 4m ago) |
| **Updated** | 2023-09-01 21:38:40.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | dnr |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

Set up:
2 seperate running clusters, Cluster-1 and Cluster-2. Both are configured with custom authorizers with OAuth authorization.

## Expected Behavior

1.  _Request-1_: _addOrUpsertRemoteCluster_ request to _Cluster-1_ with bearer token.
2. _Request-1_ passes authorization check on _Cluster-1_
3. Frontend service from _Cluster-1_ makes a _Request-2_ to _Cluster-2_ with propagated token from _Request-1_ 
4. _Request-2_ passes authorization check on _Cluster-2_

## Actual Behavior

1.  _Request-1_: _addOrUpsertRemoteCluster_ request to _Cluster-1_ with bearer token.
2. _Request-1_ passes authorization check on _Cluster-1_
3. Frontend service from _Cluster-1_ makes a _Request-2_ to _Cluster-2_ does not propagate token from _Request-1_ 
4. _Request-2_ fails authorization check on _Cluster-2_

## Steps to Reproduce the Problem
  1. Run 2 seperate clusters with OAuth authorizers (set them up to deny request with emtpy tokens)
  2. Make a request _addOrUpsertRemoteCluster_ :
  `temporal operator cluster upsert --frontend-address 'remote.address' --grpc-meta 'authorization=bearer TOKEN'`


## Specifications

  - Version: 1.20.3
  - Platform: kubernetes, docker compose, any.



---

### #4802: Increase the number of pre-allocated custom search attributes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4802 |
| **State** | OPEN |
| **Author** | lorensr (Loren ‚ò∫Ô∏è) |
| **Created** | 2023-08-24 21:09:38.000 UTC (2y 4m ago) |
| **Updated** | 2024-09-16 06:50:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | rodrigozhou |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

Unable to add more than the small [pre-allocated number](https://github.com/temporalio/temporal/blob/46bbbf74317769eb911f455749e00c1d4fa402de/schema/sqlite/v3/visibility/schema.sql#L50-L52) of custom search attributes when using the CLI server: 

https://github.com/temporalio/cli/issues/318

The main idea of the CLI server is an easy-to-use dev version of the server. This hard (unconfigurable) upper limit makes the CLI server unusable in dev for some use cases.

**Describe the solution you'd like**

Have a large upper limit‚Äîone that's much less likely to be run into.

**Workaround**

Going back to docker compose (worse DX and at this point much less discoverable, since we changed our docs/tutorials/samples to reference the CLI).

#### Comments (2)

<details>
<summary><strong>yiminc</strong> commented on 2023-08-26 00:34:57.000 UTC</summary>

If it works for dev environment, but does not work for your prod environment that is not what we wanted. 

</details>

<details>
<summary><strong>Varun-L</strong> commented on 2024-09-16 06:50:17.000 UTC</summary>

Docker version and Prod environment does support more than 3 KeywordList search attributes but this temporal server CLI version is not supporting this and making us move from a very stable and custom built CI/CD environment to Docker compose one. 

</details>


---

### #4389: Do not record activity input in the workflow history

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4389 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2023-05-24 14:27:27.000 UTC (2y 7m ago) |
| **Updated** | 2023-11-08 00:19:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Activity inputs consume history space and are not used when recovering workflow state through the replay.

**Describe the solution you'd like**
Do not persist activity inputs; in case of failures, reconstruct them by querying or replaying workflow. Assuming that workflow code is deterministic, the inputs can be reconstructed at any time. The same mechanism also can be used to show inputs in UI.

**Describe alternatives you've considered**
Store inputs separately from the history and don't return them by default when replaying workflows.

**Additional context**
Customer using query called by each activity as a workaround:

> We have a workflow where we pass the JSON (Object) in all those activities (assume 20) to perform operations.  It increases the DB size as well as the history cache. If in the activities we can get it using the stub query it will not be the part of the every activity registered with workflow and inside activity we can query from workflow. Is that the right approach to go with ?

#### Comments (2)

<details>
<summary><strong>robholland</strong> commented on 2023-06-12 14:27:24.000 UTC</summary>

I'm not sure how the UI would be able to derive the inputs, it cannot see or run the workflow code.

</details>

<details>
<summary><strong>mfateev</strong> commented on 2023-11-08 00:19:45.000 UTC</summary>

Workflow can implement a special query to produce these inputs through replay.

</details>


---

### #4201: Allow to configure max concurrency of a workflow with the same id

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4201 |
| **State** | OPEN |
| **Author** | StarpTech (Dustin Deus) |
| **Created** | 2023-04-21 14:01:39.000 UTC (2y 8m ago) |
| **Updated** | 2025-03-11 19:56:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

Hi, I'm curious if someone else run into that use case. We want to ensure that only one workflow with the same ID runs at the same time and that subsequent workflow runs are queued with a concurrency of 1. We are aware that this can be solved with `SignalWithStartWorkflow` and draining the signal channel but it is very cumbersome to implement. We need to build a queue our-self. We think this mechanism should be provided by temporal natively because it manages the task queue.
Also this approach does not create a new workflow run but extends the history. This is bad for traceability, We want to see a workflow run per job. Is there anything on the roadmap to cover this issue? Therefore are they other approaches to implement this result?

**Describe the solution you'd like**

Extending `client.StartWorkflowOptions` with a property e.g. `TaskQueueConcurrency` to control the desired concurrency.

```golang
	workflowOptions := client.StartWorkflowOptions{
		ID:                   "projects/my-branch"
		TaskQueue:            CreateDeploymentTaskQueueName,
		WorkflowIDConcurrencyPolicy: enums.WORKFLOW_CONCURRENCY_POLICY_UNLIMITED | WORKFLOW_CONCURRENCY_POLICY_SERIAL,
	}
	we, err := c.ExecuteWorkflow(context.Background(), workflowOptions, CreateDeploymentWorkflow, options)
```

- `WORKFLOW_CONCURRENCY_POLICY_UNLIMITED`: (default): Runs are scheduled without limits.
- `WORKFLOW_CONCURRENCY_POLICY_SERIAL`: Runs with the same ID  are queued as separate runs and scheduled when the current workflow run is completed.

**Describe alternatives you've considered**

Using `SignalWithStartWorkflow`.

**Additional context**

https://temporalio.slack.com/archives/CTDTU3J4T/p1682069065643249


#### Comments (2)

<details>
<summary><strong>yiminc</strong> commented on 2023-10-21 20:12:05.000 UTC</summary>

Relevant issue: https://github.com/temporalio/temporal/issues/4386

</details>

<details>
<summary><strong>rushabh-harness</strong> commented on 2025-03-11 19:56:50.000 UTC</summary>

I have a very similar usecase : https://temporalio.slack.com/archives/CTT84KXK9/p1741636085855259



</details>


---

### #4179: Header propagation on activity/workflow results

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4179 |
| **State** | OPEN |
| **Author** | AndreaWozzup |
| **Created** | 2023-04-18 07:07:19.000 UTC (2y 8m ago) |
| **Updated** | 2023-05-11 06:57:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
On our Temporal projects we use context propagators to pass cross-cutting parameters across the "temporal stack", i.e. the user that is running a workflow etc.
This works fine, anyway a use case arised where we need to set/change some of these variables within an activity (but it may be also within a workflow) and have it "propagated" when the result of the activity is returned to the calling workflow.

**Describe the solution you'd like**
We would like to be able to change contxt propagated vars within activity/workflow. Please refer also to https://community.temporal.io/t/context-propagator-java-sdk/2189/6?u=andreacolombo

**Describe alternatives you've considered**
None.

**Additional context**
None.


#### Comments (2)

<details>
<summary><strong>yiminc</strong> commented on 2023-04-28 21:51:46.000 UTC</summary>

This seems like an SDK feature where extra header info can be encoded into the result and that can be decoded out of result inside workflow context.

Reactions: üëç 1

</details>

<details>
<summary><strong>AndreaWozzup</strong> commented on 2023-05-11 06:57:37.000 UTC</summary>

That's correct Yimin, do you think it can be implemented?

</details>


---

### #3885: LIKE is not working as expected in visibility query

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3885 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2023-02-02 06:03:06.000 UTC (2y 11m ago) |
| **Updated** | 2023-03-03 18:02:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | rodrigozhou |
| **Milestone** | None |

#### Description

Report from community: 

I think there may be a bug with the advanced search on the temporal UI page. When querying using LIKE, the ‚Äú%‚Äù characters aren‚Äôt being properly escaped, which throws a client side exception: URIError: URI malformed

this might also be because the field being searched (WorkflowID) is a keyword not a string so can‚Äôt do submatch? but unsure

thinky_face: it‚Äôs pretty unclear from the documentation then (looking at https://docs.temporal.io/visibility)
At the top it says:
Wildcards ('*', '%', etc.) are supported for string-type List Filter Search Attributes. Use wildcards with the LIKE operator to query possible values:
But under the Types section there‚Äôs no String type. For Keyword it says: To have the whole string considered as a searchable keyword, use the Keyword type



#### Comments (2)

<details>
<summary><strong>Alex-Tideman</strong> commented on 2023-02-13 16:02:59.000 UTC</summary>

For more context:

URIError: URI malformed error is a front end bug. 

However after fixing it locally, LIKE does not appear to be working as expected. I tried with WorkflowType and no results were returned. I just want to understand the expected behavior for LIKE before fixing the client side exception so user's aren't confused when it doesn't return results.

<img width="1684" alt="Screen Shot 2023-02-13 at 9 57 13 AM" src="https://user-images.githubusercontent.com/7967403/218508481-1f473baf-2c51-4454-be91-4f3764a6fcb1.png">
<img width="1686" alt="Screen Shot 2023-02-13 at 9 58 26 AM" src="https://user-images.githubusercontent.com/7967403/218508506-4715e9ea-6176-4167-aa56-aae3266d475d.png">
<img width="1684" alt="Screen Shot 2023-02-13 at 10 00 08 AM" src="https://user-images.githubusercontent.com/7967403/218508520-5267d4a1-01a4-4d00-8222-29f6778f2652.png">


</details>

<details>
<summary><strong>antmendoza</strong> commented on 2023-02-27 09:23:25.000 UTC</summary>

@Alex-Tideman @yiminc wildcards are not supported right now, the documentation has been updated https://docs.temporal.io/visibility#supported-operators



</details>


---

### #3633: Cassandra schema should use default SizeTieredCompactionStrategy 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3633 |
| **State** | OPEN |
| **Author** | bschoening (Brad Schoening) |
| **Created** | 2022-11-21 18:13:24.000 UTC (3y 1m ago) |
| **Updated** | 2024-03-15 08:12:28.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

The Cassandra schema uses LevelTieredCompaction (LCS) but likely should use default SizeTieredCompactionStrategy (STCS) instead.  All of the tables seem to be configured with LCS but this is not a good general purpose configuration.

Level compaction (LCS) can involve significant write magnification, where data is re-written multiple times. In performance testing, reports show up to 13-fold write amplification with LCS.  This blog article by ScyllaDB - a Cassandra compatible database -discusses the many downsides to using LCS:

- https://www.scylladb.com/2018/01/31/compaction-series-leveled-compaction/

The default STCS is preferred and works well for most workloads.

#### Comments (2)

<details>
<summary><strong>jaffarsadikk</strong> commented on 2022-11-22 10:09:01.000 UTC</summary>

Hi Brad,
It  looks Temporal by default creating table using SizeTieredCompactionStrategy (STCS) ie., schema_version & schema_update_history tables, rest of them manually specifing compaction strategy is LevelTieredCompaction (LCS).

https://github.com/temporalio/temporal/blob/master/schema/cassandra/temporal/schema.cql#L53

</details>

<details>
<summary><strong>vikingUnet</strong> commented on 2024-03-15 08:12:27.000 UTC</summary>

Hello again! Thanks for answer, but why rest of tables indeed have old LevelTieredCompaction (LCS) instead more common and robust compaction type SizeTieredCompactionStrategy (STCS) ? Is this a secret meaning in that, or we can change compaction type of all tables on Size Tired compaction type by ourselfs?


</details>


---

### #3614: Cannot trigger/run multiple schedule actions immediately

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3614 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-11-17 16:44:44.000 UTC (3y 1m ago) |
| **Updated** | 2023-10-22 04:46:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | potential-bug, schedules |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

## Expected Behavior

With either buffer or allow-all, we should support people triggering multiple actions

## Actual Behavior

Silently some aren't triggered.

This has been discussed before and it is a known limitation of the system currently. We only allow 1 workflow per second and we can't fail a trigger is within that. This bug is track this known issue in case others see it.

Ideally we should/could change workflow ID to have a unique value put on the end if it duplicates (so no uniqueness value if it doesn't, but maybe starting at `-2` if it does). This can be done via an attempted start followed by a counter on already-exists (it's obviously unreasonable to track which ones have already run and we can't guarantee time is monotonic).

Another concern is what if my system clock sync moves me back some seconds to fix skew? Could this cause a duplicate workflow ID?

#### Comments (2)

<details>
<summary><strong>paulnpdev</strong> commented on 2022-11-17 16:58:08.000 UTC</summary>

- isn't the 1-per-second limit a configurable limit?  I assumed it was only there to avoid accidental duplication.
- I don't understand your request to generate (e.g. appending an ordinal) non-duplicate keys.  Are you suggesting the server do this generation or the Worker?
- wrt your question about skew creating duplicate wf IDs, that won't happen unless your workflow is configured to *allow* duplicates.
- 

</details>

<details>
<summary><strong>cretz</strong> commented on 2022-11-17 17:37:52.000 UTC</summary>

> isn't the 1-per-second limit a configurable limit? I assumed it was only there to avoid accidental duplication.

Not that I am aware of. We actually set workflow IDs to their second-resolution timestamp from what I'm seeing and of course Temporal can't have more than one workflow ID running at a time.

> I don't understand your request to generate (e.g. appending an ordinal) non-duplicate keys. Are you suggesting the server do this generation or the Worker?

Server. Server already appends timestamp to user-configured workflow ID, it's just at a second resolution.

> wrt your question about skew creating duplicate wf IDs, that won't happen unless your workflow is configured to allow duplicates.

It can happen for manually triggering. If I trigger a workflow that starts at `01:02:03` (with the workflow ID that starts at that), then two seconds later at `01:02:05` I trigger a workflow but the server clock has been reset back `-2` seconds, this same uniqueness piece would be hit. The appended unique counter would alleviate this.

</details>


---

### #3212: Add query param to ListNamespacesRequest

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3212 |
| **State** | OPEN |
| **Author** | lorensr (Loren ‚ò∫Ô∏è) |
| **Created** | 2022-08-11 00:05:13.000 UTC (3y 4m ago) |
| **Updated** | 2023-03-03 20:18:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

User requested feature

#### Comments (2)

<details>
<summary><strong>yiminc</strong> commented on 2022-08-12 21:40:54.000 UTC</summary>

What would the query looks like?

</details>

<details>
<summary><strong>lorensr</strong> commented on 2022-08-12 23:12:22.000 UTC</summary>

My guess is just `name` substring would be good enough for most: `{ query: 'foo' }` or `{ query: { name: 'foo' } }` matching namespace named `foobar`, but could do fuzzy or add other fields

```
message NamespaceInfo {
    string name = 1;
    temporal.api.enums.v1.NamespaceState state = 2;
    string description = 3;
    string owner_email = 4;
    // A key-value map for any customized purpose.
    map<string, string> data = 5;
    string id = 6;
```

</details>


---

### #3086: Do not emit metrics when acquire shard r/w lock

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3086 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-07-12 00:49:27.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:19:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement, performance |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
CPU profiling shows that logic probably should not emitting metrics during / before acquisition of shard r/w lock.
<img width="1188" alt="image" src="https://user-images.githubusercontent.com/8762893/178384429-43ceca5f-af84-441f-964a-fa85a918f016.png">


**Describe the solution you'd like**
Either do not emit metrics when acquiring shard r/w lock or at least do sampling

**Describe alternatives you've considered**
N/A

**Additional context**
N/A

#### Comments (2)

<details>
<summary><strong>wxing1292</strong> commented on 2022-07-12 00:51:32.000 UTC</summary>

cc @tusharroy25 @paulnpdev 

</details>

<details>
<summary><strong>paulnpdev</strong> commented on 2022-07-13 19:42:16.000 UTC</summary>

cc @yiminc 

</details>


---

### #2929: Update namespace replication task cleanup logic with connected clusters

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2929 |
| **State** | OPEN |
| **Author** | yux0 (Yu Xia) |
| **Created** | 2022-06-01 17:35:34.000 UTC (3y 7m ago) |
| **Updated** | 2023-03-03 20:20:18.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | yux0 |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

## Expected Behavior
calculate namespace replication ack level by active cluster connections

NS replication metadata:
Cluster A (disconnected): 100
Cluster B (connected): 1000
Cluster C (connected): 500

minimum ack level: 500, delete all tasks before 500.

## Actual Behavior

NS replication metadata:
Cluster A (disconnected): 100
Cluster B (connected): 1000
Cluster C (connected): 500

minimum ack level: 100

## Specifications
In 1.16, we allow remove clusters from namespace cluster list. We should update this logic accordingly.



---

### #2695: Workflow ID size check mismatch

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2695 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-04-01 00:02:56.000 UTC (3y 9m ago) |
| **Updated** | 2024-06-26 00:18:54.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | duplicate, config, close-after-30-days |
| **Assignees** | jbreiding |
| **Milestone** | None |

#### Description

frontend / history business logic workflow ID check allow max size of 1000
https://github.com/temporalio/temporal/blob/v1.15.2/service/frontend/service.go#L155

but mysql / postgresql only allow 255
https://github.com/temporalio/temporal/blob/v1.15.2/schema/mysql/v57/temporal/schema.sql#L49
https://github.com/temporalio/temporal/blob/v1.15.2/schema/postgresql/v96/temporal/schema.sql#L49


#### Comments (2)

<details>
<summary><strong>jbreiding</strong> commented on 2022-04-01 21:42:28.000 UTC</summary>

The default must be set via dynamic config.

That was the choice for fixing last time.
https://github.com/temporalio/temporal/issues/2122#event-6226633790

As shown here:
https://github.com/temporalio/temporal/pull/2568/files#diff-a9477a219eecb612dc53e68339912f86ec3daf62730cf3fb65ef5037dee3125eR28

</details>

<details>
<summary><strong>uritig</strong> commented on 2024-06-25 01:03:43.000 UTC</summary>

@jbreiding I have the same problem. I've left the default setting of `limit.maxIDLength = 1000` but observing the error `createOrUpdateCurrentExecution failed. Failed to insert into current_executions table. Error: pq: value too long for type character varying(255)`. Makes sense since the actual table field size is 255. It is not clear to me as to how the database field in the postgresql is supposed to get updated to 1000.

Please let me know if I am missing something obvious. Thank you.

</details>


---

### #2688: Support SignalExternalWorkflowExecutionWithStart

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2688 |
| **State** | OPEN |
| **Author** | mastermanu |
| **Created** | 2022-03-30 21:47:43.000 UTC (3y 9m ago) |
| **Updated** | 2023-03-03 20:20:03.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

There is a Workflow Command for Signaling an External Workflow Execution, but that typically means that the author of the WF has to write code that ensures the workflow is already running and/or start the workflow separately.

SignalExternalExecutionWithStart would make this experience a lot easier


---

### #2526: Metric on workflows not progressing because of repetitive Workflow Task Timeouts / Failures

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2526 |
| **State** | OPEN |
| **Author** | Spikhalskiy (Dmitry Spikhalsky) |
| **Created** | 2022-02-22 01:49:39.000 UTC (3y 10m ago) |
| **Updated** | 2023-03-03 20:20:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëÄ 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
One specific Temporal workflow may be completely stuck because something causes Workflow Tasks to repetitively fail or timeout. 
SDKs don't report Workflow Task Timeout and generally do their best to avoid them. Hence, when Workflow Task Timeout happens, something went wrong on the SDK side and SDK shouldn't be trusted in this situation. Temporal Server enforces the timeout and should be responsible for producing the metric.

**Describe the solution you'd like**
Temporal Server already has LOGGING about some workflows reaching a threshold of workflow task timeouts in a row: 
https://github.com/temporalio/temporal/blob/61b9b65744e455d4a85681b117e8315287911621/service/history/workflow/workflow_task_state_machine.go#L763
This should be exposed as a metric reporting a number of workflows that have more than N failed workflow tasks in a row to simplify detection of this situation by the users.
This may also be exposed as a histogram where x is an amount of failed workflow tasks in a row and y is number of workflows to don't bring some artificial static system-wide limit that has to be reached before the metric is reported.


#### Comments (2)

<details>
<summary><strong>yycptt</strong> commented on 2022-02-22 21:43:14.000 UTC</summary>

The # of attempts for a workflow task is emitted as a distribution on L759 in the linked file. Is that something you are looking for?

</details>

<details>
<summary><strong>yycptt</strong> commented on 2022-02-25 22:49:39.000 UTC</summary>

The number of workflows can probably be estimated by the rate of a certain histogram bucket. It's not exact but should be very close.

</details>


---

### #2491: Throttled logger should not throttle errors that happens rarely

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2491 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2022-02-11 06:32:51.000 UTC (3y 10m ago) |
| **Updated** | 2023-03-03 20:20:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |

#### Description

[Throttled logger ](https://github.com/temporalio/temporal/blob/61dac76ed46f90697b63a948e7b5ae1a93471b38/common/log/throttle_logger.go#L46) will rate limit all log lines equally. 
We can do much better job to throttle only frequently repeat log lines.
Current throttled logger likely will miss important error log lines if they happen rarely while hit by rps limit.

#### Comments (2)

<details>
<summary><strong>paulnpdev</strong> commented on 2022-02-11 22:38:52.000 UTC</summary>

I'm a big fan but I'd like to see this done as part of a more comprehensive plan for our logging setup.

</details>

<details>
<summary><strong>imskr</strong> commented on 2022-02-16 20:29:46.000 UTC</summary>

Can I work on this?

</details>


---

### #2482: Ability to disable dynamic config in auto-setup docker image

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2482 |
| **State** | OPEN |
| **Author** | e-zhydzetski (Eugene Zhydzetski) |
| **Created** | 2022-02-09 21:11:52.000 UTC (3y 10m ago) |
| **Updated** | 2023-03-03 20:20:38.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | alexshtin |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

I tried to launch auto-setup docker image without `DYNAMIC_CONFIG_FILE_PATH` env var and got an error.

## Expected Behavior
Server started without dynamic configuration features.

## Actual Behavior
Server startup failed with `Unable to create dynamic config client. Error: unable to read dynamic config: dynamic config file: /etc/temporal/config/dynamicconfig: read /etc/temporal/config/dynamicconfig: is a directory` error.

## Specifications
Image: temporalio/auto-setup:1.14.4

## PS
Looks like a bug in `config_template.yaml`: segment `dynamicConfigClient` should be wrapped with `{{- if .Env.DYNAMIC_CONFIG_FILE_PATH }}`. 
Ready for PR.



---

### #2470: Temporal assumes the credentials of storage service used for archival will stay the same through out the life cycle of the application.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2470 |
| **State** | OPEN |
| **Author** | VikasNS (Vikas NS) |
| **Created** | 2022-02-05 12:56:38.000 UTC (3y 11m ago) |
| **Updated** | 2023-03-03 20:20:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
1. We use AWS S3 to store archived workflows.
2. Authentication is via AWS Keys. The keys are pulled every 8 hours and written to the credential file. This is done as our keys are valid only for 8 hours.
3. I expect temporal to use the latest keys from the credential file each and every time when connecting to AWS S3.


## Actual Behavior
1. Archiver object which does the archiving (which containers the AWS session object) is created only once for the first call and is cached. There after for subsequent calls, the same archiver object is used.
2. This means that the AWS Keys present at the first call (at the start of the application) is used for the whole life cycle.
3. This will result in archival failure after 8 hours and the keys expire after 8 hours.

I feel the root of the problem is that temporal assumes that the credentials will stay the same for the whole lifecycle of the application which isn't true.


## Steps to Reproduce the Problem

  1. Setup AWS S3 archival
  2. Configure the AWS Keys to expire , say 4 hours from creation.
  3. Archival will stop working after 4 hours.

This has been discussed here -> https://community.temporal.io/t/aws-keys-updated-every-10-hours-in-credential-file/3612

## Specifications

  - Version: 1.1.12
  - Platform: Java


#### Comments (2)

<details>
<summary><strong>yiminc</strong> commented on 2022-02-11 22:39:46.000 UTC</summary>

This is experimental feature, we will address this once we have resource to work on productionize this.

</details>

<details>
<summary><strong>jontro</strong> commented on 2022-06-28 16:07:28.000 UTC</summary>

Not sure that this use case is supported by the aws go sdk yet. See this discussion for more context and a possible workaround: https://github.com/aws/aws-sdk-go/issues/1993

</details>


---

### #2405: Use sticky task queue on querying close workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2405 |
| **State** | OPEN |
| **Author** | yux0 (Yu Xia) |
| **Created** | 2022-01-21 21:03:54.000 UTC (3y 11m ago) |
| **Updated** | 2023-03-03 20:20:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Use sticky task queue on querying close workflow

**Describe the solution you'd like**
A detail discussion on motivation and solution.

The close workflow will cleanup the stickyness. It is worth to keep the stickyness for frequent query workflow.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.


#### Comments (2)

<details>
<summary><strong>yiminc</strong> commented on 2022-01-21 22:36:01.000 UTC</summary>

but SDK would evict closed workflow from their sticky cache.

</details>

<details>
<summary><strong>yiminc</strong> commented on 2022-01-28 18:17:12.000 UTC</summary>

From server side, there is no change needed, the sticky flag is still there and so server could dispatch the query task to the right sticky worker.
But from SDK side, we would need to stop evict close workflow and only rely on LRU to kick in to evict.

</details>


---

### #2342: Add API to allow operator to add custom system workflows to worker role

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2342 |
| **State** | OPEN |
| **Author** | rfwagner (Richard Wagner) |
| **Created** | 2022-01-04 21:09:00.000 UTC (3y 12m ago) |
| **Updated** | 2023-03-03 20:21:14.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | samarabbas |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

Operators of Temporal often have their own system-level workflows they run in order to support various integrations with the underlying infrastructure or org-specific enterprise systems. We can always spin up our own worker apps, but in some cases it would be simpler to just piggyback on top of the existing temporal-server worker role.

The Temporal server code already supports many ways to customize the server code via ServerOptions (or dependency injection in the latest versions?). I'm thinking it would be nice to allow for a new customization API that would allow the operator to specify system level workflows and corresponding workers that should be run within the worker role.

We would probably need the ability to specify the workflow id of these custom workflows. We would also need to be able to specify some of the same sets of options as are currently specified in the Config struct in the worker package.



---

### #1893: Include tarball with versioned schema in release

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1893 |
| **State** | OPEN |
| **Author** | bouk (Bouke van der Bijl) |
| **Created** | 2021-09-07 13:34:04.000 UTC (4y 3m ago) |
| **Updated** | 2023-03-03 20:21:39.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | alexshtin |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**

Hi there, I'm trying to set up a Temporal server manually and I got to the step where I have to apply the versioned schema. The schemas don't seem to be included in the release however, so I'm not sure what the best way is to acquire them.

**Describe the solution you'd like**

In addition to the compiled binaries, include the schemas that are tied to a version.

**Describe alternatives you've considered**

Downloads the schemas from the github tag



---

### #1847: [xdc] Workers connected to standby cluster not picking up tasks after  failover 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1847 |
| **State** | OPEN |
| **Author** | kkcmadhu |
| **Created** | 2021-08-24 10:16:30.000 UTC (4y 4m ago) |
| **Updated** | 2023-03-03 20:21:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | potential-bug |
| **Assignees** | samarabbas |
| **Milestone** | None |

#### Description

## Expected Behavior
Workers connected to standby cluster should start picking up tasks when proted as teh active cluster

## Actual Behavior
No taks get dispatched to the worker connected to standby cluster


## Steps to Reproduce the Problem

  1. Create an xdc setup in [docker compose](https://github.com/kkcmadhu/docker-compose/blob/patch-1/docker-compose-mysql-xdc.yml)   bring up the cluster using 
   `docker compose -f docker-compose-mysql-xdc.yaml up`
  2. create a global namespace named xdc with clusters active and standby 
  3. create some workflow in xdc namespace
  4.  bring down the active cluster (i.e temporal-active, temporal-active-admin-tools, temporal-active-web) (kill specific containers)
  5.  log on to temporal-standby-admin-tools and  fail over to standby cluser using tctl -ns xdc n u --ac standby
  

The workers  (java sdk) connected to the standby cluster is still not  getting any workflows/tasks dispatched.

this behaviour is with java sdk based worker/clients

#### Comments (2)

<details>
<summary><strong>kkcmadhu</strong> commented on 2021-08-24 10:29:06.000 UTC</summary>

please let me know if i am missing something!

</details>

<details>
<summary><strong>kkcmadhu</strong> commented on 2021-08-25 05:24:41.000 UTC</summary>

thie behaviour is highly undetermistic. This  is what i observe :

a) At times the new worker never pickup anything (old workflows/ newly submited workflows)
b) Some other times,  i see that the standby cluster, which was made active is able to process any newly created workfows, howver do not pick/contine processiong workfows which were in progress and were processed by the active cluster prior to fail over.

On a side note:

when i submit new workflows to standby cluster, they do get submitted, but when i do the same in active cluster after fail over
i get  "cluster is not active " action, is this because i have 

`dcRedirectionPolicy:
  policy: "selected-apis-forwarding"
  toDC: ""` set in the standby cluster?

</details>


---

### #1423: Make output streams of logger configurable perfectly

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1423 |
| **State** | OPEN |
| **Author** | shiwano (Shogo Iwano) |
| **Created** | 2021-04-01 03:11:35.000 UTC (4y 9m ago) |
| **Updated** | 2023-03-03 20:22:41.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement, operations |
| **Assignees** | alexshtin, Ardagan |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**

Google Cloud Platform collects logs output to `stderr` as error (means that Severity is `ERROR` ) and those output to `stdout` as non-error. When the temporal server outputs the log, whether the log level is `INFO` or `ERROR` , it outputs to the same output stream, so everything gets mixed up.

Because of this, I want the feature that configures `OutputPaths` and `ErrorOutputPaths` of loggers.

**Describe the solution you'd like**

Add `OutputPaths` and `ErrorOutputPaths` fields to this:
https://github.com/temporalio/temporal/blob/a04944a0a174a22745f52e64128ca7c1331e792a/common/log/config.go#L29-L36

And set those to the logger config:
https://github.com/temporalio/temporal/blob/a04944a0a174a22745f52e64128ca7c1331e792a/common/log/zap_logger.go#L199-L200

**Describe alternatives you've considered**

None.

**Additional context**

We may want to consider what to do with existing configuration fields such as `Stdout` and `OutputFile`.
I think we should leave it at that for compatibility.


---

### #1018: Add CANCEL_REQUESTED status

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1018 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-11-20 22:01:56.000 UTC (5y 1m ago) |
| **Updated** | 2023-03-03 20:23:06.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement, API, difficulty: easy, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
DescribeWorkflowExecution result doesn't indicate if workflow cancel was requested.

**Describe the solution you'd like**
Add CANCEL_REQUESTED status or some additional property to the DescribeWorkflowExecutionResult.

**Additional context**
https://community.temporal.io/t/workflow-status-canceled-vs-canceling/1050


---

### #845: Generate a signed token for each workflow activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/845 |
| **State** | OPEN |
| **Author** | randomswdev |
| **Created** | 2020-10-13 17:22:37.000 UTC (5y 2m ago) |
| **Updated** | 2021-11-12 22:02:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | mfateev |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
I would like to let temporal workers access external systems using their own identity, that certifies what activity/workflow a worker is executing, fro mwhich task list and so on. Very similar to what a GitLab job is able to do:

https://docs.gitlab.com/ee/ci/examples/authenticating-with-hashicorp-vault/

**Describe the solution you'd like**
I would like to have Cadence generate such an identity and provide it to a worker when the worker extracts some work to do from a task list. The cadence server can generate the signed token (or call a configurable plugin to generate it) and thus certify a set of information about the activity/workflow.

If you think this is something valuable we can discuss the design more in depth and then I can work to contribute the required code.

#### Comments (2)

<details>
<summary><strong>samarabbas</strong> commented on 2021-07-04 07:03:31.000 UTC</summary>

@mfateev what do you think about this feature ask?

</details>

<details>
<summary><strong>randomswdev</strong> commented on 2021-11-12 22:02:30.000 UTC</summary>

Any update about this? We need such a feature and we can contribute code if you are interested.

</details>


---

### #773: session and childworkflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/773 |
| **State** | OPEN |
| **Author** | linvon (Linvon) |
| **Created** | 2020-09-28 11:51:34.000 UTC (5y 3m ago) |
| **Updated** | 2021-07-04 18:21:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement |
| **Assignees** | mfateev |
| **Milestone** | None |

#### Description

Will the session take effect for all activities and activities in the childworkflowÔºü
The scenario is that I create a session in a large workflow and create ctx of child workflow based on it. However, the activities in child workflow are not bound to be executed in the same worker. Is the session data lost when transferring ctx to the new childworkflow? Should I pass the session taskQueue to the child workflow and reset ctx whit it?
Thank you

#### Comments (2)

<details>
<summary><strong>samarabbas</strong> commented on 2021-07-04 07:15:52.000 UTC</summary>

@mfateev any thoughts on this one?

</details>

<details>
<summary><strong>mfateev</strong> commented on 2021-07-04 17:13:13.000 UTC</summary>

It is possible to reopen a session through [RecreateSession](https://github.com/temporalio/sdk-go/blob/9778ef5e148d747e953dc88a8dd654d6074ad417/workflow/session.go#L114). But a session cannot currently spawn multiple workflows. 

It would require pretty significant refactoring to accommodate your scenario of sharing a session across multiple workflows. 
I'm going to convert this issue to a feature request.

The workaround is to use child workflows to keep the whole workflow state in a single workflow execution. One variation of such design is to have a session keeper workflow that other workflows use to request activity executions through signals.

</details>


---

### #705: Support reset with pending child workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/705 |
| **State** | OPEN |
| **Author** | samarabbas (Samar Abbas) |
| **Created** | 2020-08-31 19:47:34.000 UTC (5y 4m ago) |
| **Updated** | 2024-10-18 22:31:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement, difficulty: hard |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ‚ù§Ô∏è 14 |

#### Description

**Is your feature request related to a problem? Please describe.**
Reset is not allowed if the workflow execution has any pending child workflow executions.

**Describe the solution you'd like**
Support reset with pending children


#### Comments (2)

<details>
<summary><strong>snackattas</strong> commented on 2024-09-09 16:34:58.000 UTC</summary>

Any movement on this one? I see its been documented in 2020...but I am running into this now

Reactions: üëç 3

</details>

<details>
<summary><strong>drewhoskins-temporal</strong> commented on 2024-10-18 22:31:46.000 UTC</summary>

@snackattas (and to other lurkers) we are working on it!  Please DM me at @ Drew Hoskins in the Temporal Community Slack, so I can make sure we cover your use case.

</details>


---

### #607: Add proper unit / integration tests for parentclosepolicy workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/607 |
| **State** | OPEN |
| **Author** | mastermanu |
| **Created** | 2020-07-24 23:45:31.000 UTC (5y 5m ago) |
| **Updated** | 2025-08-19 11:54:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | good first issue, potential-bug, testing |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Context: We had a nil ref bug in the workflow where the activity was not passing in a context, which meant that this Workflow was always failing. Although we fixed the bug, there is a lack of unit/integration testing that should have protected us against this (it was caught by stress testing, but that is far too late to catch such a basic issue)

#### Comments (2)

<details>
<summary><strong>vaibhavyadav-dev</strong> commented on 2025-05-01 19:35:27.000 UTC</summary>

can I take this one @samarabbas ?

</details>

<details>
<summary><strong>endo0911engineer</strong> commented on 2025-08-19 11:54:15.000 UTC</summary>

https://github.com/temporalio/sdk-go/pull/2023

Hi, 
I have created a PR that addresses this issue.
The PR adds a test for NilContextWorkflow to ensure safety when executing activities with a nil context.

Thanks for your patience!

</details>


---

### #515: Expose zap logger encoding to config

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/515 |
| **State** | OPEN |
| **Author** | skrul (Steve Krulewitz) |
| **Created** | 2020-07-08 23:19:44.000 UTC (5y 5m ago) |
| **Updated** | 2023-03-03 20:23:03.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement, difficulty: easy, operations, up-for-grabs |
| **Assignees** | Ardagan |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
Temporal is hard coded to always output logs in json. Our infrastructure handles json-encoded logs but in a specific way that has certain constraints (e.g. low cardinality of field names), therefore these logs are somewhat incompatible with our infra.

**Describe the solution you'd like**
Would like to be able to use the zap "console" encoder which I assume outputs regular log lines.

**Describe alternatives you've considered**
It may be possible to get our infra to treat the json encoded logs as plain text but they are not very user friendly.

**Additional context**
None.



---

### #455: If a temporal service fails to start due to a missing config key, log the missing key

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/455 |
| **State** | OPEN |
| **Author** | markmark206 (Mark Markaryan) |
| **Created** | 2020-06-15 17:55:50.000 UTC (5y 6m ago) |
| **Updated** | 2023-03-03 20:22:58.000 UTC |
| **Upvotes** | 1 |
| **Comments** | 0 |
| **Priority Score** | 2 |
| **Labels** | enhancement, difficulty: easy, operations, up-for-grabs, P2 |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëç 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]
When creating a kubernetes deployment of temporal, a customer ran into what looks like a  configuration problem, which manifested itself in the service simply exiting, with the following log:

```
2020-06-15T15:12:00.852003855Z + bash /start-temporal.sh E 
2020-06-15T15:12:00.854018920Z + dockerize -template /etc/temporal/config/config_template.yaml:/etc/temporal/config/docker.yaml E 
2020-06-15T15:12:00.857719185Z + exec temporal-server --root /etc/temporal --env docker start --services=frontend E 
2020-06-15T15:12:00.875540906Z 2020/06/15 15:12:00 Loading config; env=docker,zone=,configDir=/etc/temporal/config E 
2020-06-15T15:12:00.875686932Z 2020/06/15 15:12:00 Loading configFiles=[/etc/temporal/config/docker.yaml] E 
2020-06-15T15:12:00.876097429Z 2020/06/15 15:12:00 Config file corrupted.yaml: line 123: did not find expected key E
```

debugging the condition was tricky, and could be greatly simplified if only the log included the name of the missing key.

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

When validating configs and encountering a missing key, please include the. name of the key in the log.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

The obvious (default) alternative is to do nothing. which is also fine, but makes problems harder to troubleshoot.

**Additional context**
Add any other context or screenshots about the feature request here.

This problem was brought to my attention by Ringo in Temporal slack chat:  https://temporalio.slack.com/archives/CTTJCPZQE/p1592235043052000


---

### #109: Docker: Docker image with schema loaded

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/109 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-02-02 20:11:09.000 UTC (5y 11m ago) |
| **Updated** | 2023-10-12 17:21:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement, devexp, difficulty: easy, up-for-grabs, P2 |
| **Assignees** | None |
| **Milestone** | None |

#### Description

The initial execution of the `docker-compose up` takes a long time. Mostly because it has to create the DB schema. Consider shipping a docker container with a schema already populated to speed up the startup.

Any other ideas on how to speed up `docker-compose up` are welcome.

#### Comments (2)

<details>
<summary><strong>hazcod</strong> commented on 2020-04-09 05:52:21.000 UTC</summary>

Perhaps a nice idea aswel to reduce size would be opting for a docker `scratch` image with using `confd` to render the configuration from templates + args/env vars.
I always feel a lot safer without a shell in my container. :-)

</details>

<details>
<summary><strong>yiminc</strong> commented on 2022-10-29 17:42:12.000 UTC</summary>

We are addressing the developer experience on this pain point with single binary (temporalite). 

Reactions: üëç 1

</details>


---

### #108: Make temporal docker to not emit noisy logs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/108 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-02-02 19:55:12.000 UTC (5y 11m ago) |
| **Updated** | 2023-11-09 14:08:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 2 |
| **Priority Score** | 2 |
| **Labels** | enhancement, docker, devexp, difficulty: easy, up-for-grabs, P2 |
| **Assignees** | None |
| **Milestone** | Initial Temporal Release |

#### Description

Currently temporal docker emits a lot of logs with 99.9% of them being useless. It also doesn't have clear indication when the service is ready serve requests. My proposal is to emit something like:

`Temporal Service Starting...`
`Temporal Service Started`

and then emit only fatal errors. For troubleshooting provide an option to enable logs.  

#### Comments (2)

<details>
<summary><strong>rylandg</strong> commented on 2020-04-27 20:58:24.000 UTC</summary>

I personally feel this pain when using Temporal as a non-advanced user. Especially when running Temporal via docker-compose, it's very difficult to discern meaningful information. It seemed that Cassandra might be a big culprit.

</details>

<details>
<summary><strong>vwkd</strong> commented on 2023-11-09 12:29:21.000 UTC</summary>

This seems to be still valid for temporalite. `temporal start server-dev` debug logs JSON to the console by default.

It would be good if debug log is opt-in instead of default. Also, it should default to a human-friendly format, and to a machine-friendly format on opt-in. It's hard on the eyes to parse screen filling JSON messages in real time...

</details>


---

### #8909: Deprecation error log in v1.29.1 with Elasticsearch 8.19.2

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8909 |
| **State** | OPEN |
| **Author** | andropler |
| **Created** | 2025-12-29 08:19:57.000 UTC (2 days ago) |
| **Updated** | 2025-12-30 05:12:01.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
The Temporal Server should be fully compatible with Elasticsearch 8.x without generating excessive error logs for deprecated fields. Deprecation warnings from Elasticsearch should ideally be handled internally or logged at a warn or info level, rather than error, to avoid log noise in production environments.

## Actual Behavior
The frontend service repeatedly logs an error level message when interacting with Elasticsearch 8.19.2. The error message indicates that a deprecated field include_upper is being used in a CountWorkflowExecutions request.

Log Snippet:

"msg": "Deprecation warning: 299 Elasticsearch-8.19.2-c1c00e18ef14acd650768ff01f037eaede0c1f7b \"Deprecated field [include_upper] used, this field is unused and will be removed entirely\""
"level": "error"
"logging-call-at": "temporal/common/persistence/visibility/store/elasticsearch/client/logger.go:24"
Even though the server version is 1.29.1, it seems the internal olivere/elastic/v7 client is still sending legacy parameters that trigger this warning from the ES 8.x server.

## Steps to Reproduce the Problem

  1. Deploy Temporal Server v1.29.1.
  2. Connect it to an Elasticsearch cluster version 8.19.2 or similar 8.x version.
  3. Trigger a workflow execution count (e.g., by opening the Temporal Web UI or calling CountWorkflowExecutions via CLI/SDK).
  4. Check the frontend service logs in the temporal namespace.

## Specifications

  - Version: Temporal Server v1.29.1
  - Platform: Kubernetes


#### Comments (1)

<details>
<summary><strong>andropler</strong> commented on 2025-12-30 05:12:01.000 UTC</summary>

Note that I'm running ES 8.19 and used the most recent APIs for index creation. Please refer to the stack trace attached below.

go.temporal.io/server/common/log.(*zapLogger).Error
	/home/runner/work/docker-builds/docker-builds/temporal/common/log/zap_logger.go:151
go.temporal.io/server/common/persistence/visibility/store/elasticsearch/client.(*errorLogger).Printf
	/home/runner/work/docker-builds/docker-builds/temporal/common/persistence/visibility/store/elasticsearch/client/logger.go:24
github.com/olivere/elastic/v7.(*Client).errorf
	/home/runner/go/pkg/mod/github.com/olivere/elastic/v7@v7.0.32/client.go:840
github.com/olivere/elastic/v7.(*Client).PerformRequest
	/home/runner/go/pkg/mod/github.com/olivere/elastic/v7@v7.0.32/client.go:1479
github.com/olivere/elastic/v7.(*CountService).Do
	/home/runner/go/pkg/mod/github.com/olivere/elastic/v7@v7.0.32/count.go:364
go.temporal.io/server/common/persistence/visibility/store/elasticsearch/client.(*clientImpl).Count
	/home/runner/work/docker-builds/docker-builds/temporal/common/persistence/visibility/store/elasticsearch/client/client_v7.go:140
go.temporal.io/server/common/persistence/visibility/store/elasticsearch.(*VisibilityStore).CountWorkflowExecutions
	/home/runner/work/docker-builds/docker-builds/temporal/common/persistence/visibility/store/elasticsearch/visibility_store.go:375
go.temporal.io/server/common/persistence/visibility.(*visibilityManagerImpl).CountWorkflowExecutions
	/home/runner/work/docker-builds/docker-builds/temporal/common/persistence/visibility/visibility_manager_impl.go:147
go.temporal.io/server/common/persistence/visibility.(*visibilityManagerRateLimited).CountWorkflowExecutions
	/home/runner/work/docker-builds/docker-builds/temporal/common/persistence/visibility/visibility_manager_rate_limited.go:126
go.temporal.io/server/common/persistence/visibility.(*visibilityManagerMetrics).CountWorkflowExecutions
	/home/runner/work/docker-builds/docker-builds/temporal/common/persistence/visibility/visiblity_manager_metrics.go:137
go.temporal.io/server/service/frontend.(*WorkflowHandler).CountWorkflowExecutions
	/home/runner/work/docker-builds/docker-builds/temporal/service/frontend/workflow_handler.go:2582
go.temporal.io/api/workflowservice/v1._WorkflowService_CountWorkflowExecutions_Handler.func1
	/home/runner/go/pkg/mod/go.temporal.io/api@v1.53.0/workflowservice/v1/service_grpc.pb.go:3113
go.temporal.io/server/common/rpc/interceptor.(*RetryableInterceptor).Intercept.func1
	/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/retry.go:38
go.temporal.io/server/common/backoff.ThrottleRetryContext
	/home/runner/work/docker-builds/docker-builds/temporal/common/backoff/retry.go:65
go.temporal.io/server/common/rpc/interceptor.(*RetryableInterceptor).Intercept
	/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/retry.go:42
google.golang.org/grpc.getChainUnaryHandler.func1.getChainUnaryHandler.1
	/home/runner/go/pkg/mod/google.golang.org/grpc@v1.72.2/server.go:1217
go.temporal.io/server/common/rpc/interceptor.(*SlowRequestLoggerInterceptor).Intercept
	/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/slow_request_logger.go:51
google.golang.org/grpc.getChainUnaryHandler.func1
	/home/runner/go/pkg/mod/google.golang.org/grpc@v1.72.2/server.go:1217
go.temporal.io/server/common/rpc/interceptor.(*CallerInfoInterceptor).Intercept
	/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/caller_info.go:40
google.golang.org/grpc.getChainUnaryHandler.func1.getChainUnaryHandler.1
	/home/runner/go/pkg/mod/google.golang.org/grpc@v1.72.2/server.go:1217
go.temporal.io/server/common/rpc/interceptor.(*SDKVersionInterceptor).Intercept
	/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/sdk_version.go:44
google.golang.org/grpc.getChainUnaryHandler.func1
	/home/runner/go/pkg/mod/google.golang.org/grpc@v1.72.2/server.go:1217
go.temporal.io/server/common/rpc/interceptor.(*RateLimitInterceptor).Intercept
	/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/rate_limit.go:64
google.golang.org/grpc.getChainUnaryHandler.func1.getChainUnaryHandler.1
	/home/runner/go/pkg/mod/google.golang.org/grpc@v1.72.2/server.go:1217
go.temporal.io/server/common/rpc/interceptor.(*NamespaceRateLimitInterceptorImpl).Intercept
	/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/namespace_rate_limit.go:75
google.golang.org/grpc.getChainUnaryHandler.func1
	/home/runner/go/pkg/mod/google.golang.org/grpc@v1.72.2/server.go:1217
go.temporal.io/server/common/rpc/interceptor.(*ConcurrentRequestLimitInterceptor).Intercept
	/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/concurrent_request_limit.go:83
google.golang.org/grpc.getChainUnaryHandler.func1.getChainUnaryHandler.1
	/home/runner/go/pkg/mod/google.golang.org/grpc@v1.72.2/server.go:1217
go.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).StateValidationIntercept
	/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/namespace_validator.go:210
google.golang.org/grpc.getChainUnaryHandler.func1
	/home/runner/go/pkg/mod/google.golang.org/grpc@v1.72.2/server.go:1217
go.temporal.io/server/common/rpc/interceptor.(*HealthInterceptor).Intercept
	/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/health.go:42
google.golang.org/grpc.getChainUnaryHandler.func1.getChainUnaryHandler.1
	/home/runner/go/pkg/mod/google.golang.org/grpc@v1.72.2/server.go:1217
go.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept

</details>


---

### #8698: [Security] Review & Fix Multiple CVEs affecting Go standard libraries

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8698 |
| **State** | OPEN |
| **Author** | jigar-shah-acquia |
| **Created** | 2025-11-25 12:26:43.000 UTC (1 months ago) |
| **Updated** | 2025-12-29 10:26:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## üîê Security Vulnerability Review ‚Äì Go Standard Library CVEs

Several CVEs affecting Go standard libraries have been identified. We are requesting confirmation on whether Temporal is impacted, and if any patches or mitigations are already planned or required.

---

### üìå List of CVEs

| CVE ID | Description |
|--------|-------------|
| **CVE-2025-47912** | `Parse()` incorrectly allows non-IPv6 values inside square brackets in URL host (RFC 3986 violation). |
| **CVE-2025-58185** | Malicious DER payloads may allocate excessive memory, leading to memory exhaustion. |
| **CVE-2025-58186** | Unlimited cookie parsing may result in high memory usage when many small cookies are sent. |
| **CVE-2025-58187** | Name constraint checking scales non-linearly with certificate size ‚Üí potential performance issue. |
| **CVE-2025-58188** | DSA-based certificate validation may cause program panic due to unsafe interface casting. |
| **CVE-2025-58189** | `Conn.Handshake` may leak attacker-controlled ALPN protocol values without escaping. |
| **CVE-2025-61724** | `Reader.ReadResponse()` uses repeated string concatenation, leading to high CPU usage. |
| **CVE-2025-61725** | `ParseAddress()` repeatedly concatenates domain literals, causing CPU performance impact. |

---

### üß™ Suggested Code Areas to Review

The **installed version is `1.25.0`**, and as per the patch advisory, it should be **upgraded to `1.25.3`** to address the issue.


#### Comments (1)

<details>
<summary><strong>shriacquia</strong> commented on 2025-12-29 10:26:27.000 UTC</summary>

New CVEs identified:
<google-sheets-html-origin><style type="text/css"><!--td {border: 1px solid #cccccc;}br {mso-data-placement:same-cell;}--></style>
CVE ID | CVSS Severity
-- | --
CVE-2025-47910 | MEDIUM
CVE-2025-47912 | HIGH
CVE-2025-47914 | MEDIUM
CVE-2025-58181 | MEDIUM
CVE-2025-58183 | LOW
CVE-2025-58185 | MEDIUM
CVE-2025-58186 | MEDIUM
CVE-2025-58187 | MEDIUM
CVE-2025-58188 | HIGH
CVE-2025-58189 | MEDIUM
CVE-2025-61723 | MEDIUM
CVE-2025-61724 | MEDIUM
CVE-2025-61725 | MEDIUM
CVE-2025-61727 | MEDIUM
CVE-2025-61729 | MEDIUM



</details>


---

### #8655: Addressing security vulnerabilities in the Temporalio/admin-tools:1.29.1

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8655 |
| **State** | OPEN |
| **Author** | thle40 |
| **Created** | 2025-11-17 09:42:51.000 UTC (1 months ago) |
| **Updated** | 2025-12-16 11:21:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
No more CVEs found

## Actual Behavior
There are some CVEs found from the latest Temporal image:
temporalio/admin-tools:1.29.1*

## Steps to Reproduce the Problem

> Pull the latest image temporalio/admin-tools:1.29.1* from Dockerhub
> Scan the image with any vulnerability scanner

CVE | SEVERITY | CVSS | PACKAGE | VERSION | FIX IN
-- | -- | -- | -- | -- | --
CVE-2025-22870, CWE-918 | HIGH | 8.8 | golang.org/x/net/http/httpproxy | v0.34.0 | 0.36.0
CVE-2023-47108, CWE-770, GHSA-8pgv-569h-w5rw | HIGH | 7.5 | go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc | v0.36.4 | 0.46.0
CVE-2024-44337, CWE-835 | MEDIUM | 6.9 | github.com/gomarkdown/markdown/parser | v0.0.0-20250311123330-531bef5e742b | N/A
CVE-2024-2689, CWE-20, GHSA-wmxc-v39r-p9wf | MEDIUM | 4.4 | go.temporal.io/server/common | v1.18.1-0.20230217005328-b313b7f58641 | 1.20.5
CVE-2025-9086 | LOW | None | curl | 8.14.1-r0 | 8.14.1-r2
CVE-2025-10148 | LOW | None | curl | 8.14.1-r0 | 8.14.1-r2
CVE-2025-4575 | LOW | None | openssl | 3.5.0-r0 | 3.5.1-r0
CVE-2025-9232 | LOW | None | openssl | 3.5.0-r0 | 3.5.4-r0
CVE-2025-9230 | LOW | None | openssl | 3.5.0-r0 | 3.5.4-r0
CVE-2025-9231 | LOW | None | openssl | 3.5.0-r0 | 3.5.4-r0
CVE-2025-8715 | LOW | None | 	postgresql | 17:17.5-r0 |	17.6-r0
CVE-2025-8714 | LOW | None | 	postgresql | 17:17.5-r0 |	17.6-r0
CVE-2025-8713  | LOW | None | 	postgresql | 17:17.5-r0 |	17.6-r0
CVE-2025-12817  | LOW | None | 	postgresql | 17:17.5-r0 |	17.7-r0
CVE-2025-12818 	 | LOW | None | 	postgresql | 17:17.5-r0 |	17.7-r0
CVE-2025-59375, CWE-770 | LOW | None | 	expat | 2.7.1-r0 | 2.7.2-r0
CVE-2025-49014, CWE-416 	 | LOW | None | 	jq | 1.8.0-r0 |	1.8.1-r0
CVE-2025-6965 	| CRITICAL 	| 9.8| sqlite | 3.49.2-r0 	| 3.49.2-r1

#### Comments (1)

<details>
<summary><strong>Ujjwaljain16</strong> commented on 2025-12-16 11:21:30.000 UTC</summary>

I can confirm this is a valid security issue. I've analyzed the codebase and found:

**Vulnerable Dependencies in go.mod:**
- `golang.org/x/net v0.39.0` ‚Üí Should update to `v0.48.0` (latest)
- `go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.59.0` ‚Üí Should update to `v0.64.0` (latest)

**Vulnerable Base Image:**
- `admin-tools.Dockerfile` uses `alpine:3.22` which contains sqlite 3.49.2-r0 with **CRITICAL CVE-2025-0224 (CVSS 9.8)**
- Should update to `alpine:3.23` or later

**Impact:**
- The CRITICAL sqlite vulnerability affects the admin-tools container which is used for database migrations and administrative operations
- The HIGH severity golang.org/x/net vulnerability affects network communications across all services
- These CVEs expose production deployments to potential security breaches

**Proposed Fix:**
1. Update Go dependencies in go.mod to latest secure versions
2. Update Alpine base image in admin-tools.Dockerfile to 3.23+
3. Run dependency audit and vulnerability scan
4. Update go.sum accordingly

I'd be happy to submit a PR with these fixes if the maintainers agree this should be addressed. I can have it ready for review shortly.

</details>


---

### #8654: Unable to view workflows in UI

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8654 |
| **State** | OPEN |
| **Author** | saatvik-chugh |
| **Created** | 2025-11-17 06:18:39.000 UTC (1 months ago) |
| **Updated** | 2025-12-11 23:22:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
Workflows should be visible on temporal UI

## Actual Behavior
Workflows are not visible on temporal UI.

Through tctl 
I'm able to describe the workflow using 
tctl workflow show --workflow_id <workflow-id>

but get an empty output through
tctl workflow list

## Steps to Reproduce the Problem
  1. Using the Temporal EKS chart version: 0.71.0 and app version: 1.29.0
  2. Using AWS RDS mysql  for persistence (able to view all workflows in the database in the executions table)
  3. Using Elasticsearch for visibility, version 8.19.1. (docs are being written to elasticsearch too, verified that)

## Specifications

Temporal
  - Chart Version: 0.71.0
  - App Version: 1.29.0
  - Platform: EKS
  
MySQL Database
  - Version: 8.0.42
  - Platform: AWS

Elasticsearch
  - Version: 8.19.1
  - Platform: Elasticcloud

#### Comments (1)

<details>
<summary><strong>gow</strong> commented on 2025-12-11 23:22:23.000 UTC</summary>

@saatvik-chugh Could you please confirm the following.
1. which workflow were you describing (show). We hide some system workflows from visibility. So you can describe them individually but they won't show up in list view. Just want to make sure this is not the case.
2. Can you see any workflows in the web UI? You mentioned in description that you used tctl to list. Can you confirm that you don't see the workflow list in the web UI as well.

Once you confirm, we would be able to narrow down our investigation and forward this to the right person to investigate further.

</details>


---

### #8608: Support opt-in ability to get workflow input and output (if closed) on describe workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8608 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-11-07 20:44:49.000 UTC (1 months ago) |
| **Updated** | 2025-12-11 22:11:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

Some users need workflow input/output without crawling history. This is something that is being added for Nexus operations and standalone activities. We should add for workflows too.

#### Comments (1)

<details>
<summary><strong>gow</strong> commented on 2025-12-11 22:11:47.000 UTC</summary>

This was closed by mistake (due to a faulty integration). Reopening it.

Reactions: ‚ù§Ô∏è 1

</details>


---

### #8537: Add first execution run ID for already-started start workflow results

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8537 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-10-24 14:59:28.000 UTC (2 months ago) |
| **Updated** | 2025-10-30 22:50:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

SDK needs `StartWorkflowExecutionResponse.first_execution_run_id` for on-conflict-use-existing situations, and `WorkflowExecutionAlreadyStartedFailure.first_execution_run_id` for other already-started situations. A lot of the time this will be populated with the same value as `run_id`.

Today, when SDK gets `StartWorkflowExecutionResponse` with `started` as `false` or `WorkflowExecutionAlreadyStartedFailure`, it doesn't include the first execution run ID that we can bind successive calls to. It only provides the run ID. If you start a workflow today successfully, SDK uses the run ID as the first execution run ID on successive calls like cancel and signal and such. But if you start a workflow today w/ conflict policy of use existing and it doesn't start, that run ID is not acceptable for first execution run ID.

#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2025-10-30 22:50:39.000 UTC</summary>

It's a bit involved but not too much of a heavy lift, there's some information that we don't have at the time when we handle the request that we would need to store in the current run persistence record.

</details>


---

### #8431: Docker deployment TEMPORAL_ADMINTOOLS_VERSION

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8431 |
| **State** | OPEN |
| **Author** | beddows (Michael Beddows) |
| **Created** | 2025-10-04 15:54:52.000 UTC (2 months ago) |
| **Updated** | 2025-12-16 11:10:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Using the docker compose [deployment instructions](https://docs.temporal.io/self-hosted-guide/deployment), the .env contains TEMPORAL_ADMINTOOLS_VERSION=1.29.0, but the [temporalio/admin-tools image ](https://hub.docker.com/r/temporalio/admin-tools)only has tag 1.29 and docker compose fails:
` ‚úò temporal-admin-tools Error manifest for temporalio/admin-tools:1.29.0 not found: manifest unknown: manifest unknown`

Either TEMPORAL_ADMINTOOLS_VERSION should be 1.29 or the 1.29.0 tag should be included with the image.

#### Comments (1)

<details>
<summary><strong>Ujjwaljain16</strong> commented on 2025-12-16 11:07:15.000 UTC</summary>

Hi @beddows,

Thanks for reporting this! I've investigated the issue and found that the problem is actually in a different repository.

The `TEMPORAL_ADMINTOOLS_VERSION=1.29.0` you're referring to is in the [temporalio/docker-compose](https://github.com/temporalio/docker-compose) repository's `.env` file, not in this repository (temporalio/temporal). 

This repository (`temporalio/temporal`) builds and publishes the Docker images, which are currently tagged using `major.minor` format (e.g., `1.29`), not `major.minor.patch` format (e.g., `1.29.0`).

**The fix should be made in the docker-compose repository** by updating the `.env` file from:
TEMPORAL_ADMINTOOLS_VERSION=1.29.0
to
TEMPORAL_ADMINTOOLS_VERSION=1.29

Could you please open an issue (or move this one) to https://github.com/temporalio/docker-compose instead? That's where the actual configuration error exists.

Alternatively, if the Temporal team wants to change the Docker tagging strategy to include patch versions, that would require changes to the CI/CD workflows in this repository.


</details>


---

### #8349: Support relative time in the visibility store

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8349 |
| **State** | OPEN |
| **Author** | Alex-Tideman (Alex Tideman) |
| **Created** | 2025-09-18 20:39:51.000 UTC (3 months ago) |
| **Updated** | 2025-10-16 20:27:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | awln-temporal |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Right now querying workflows requires absolute timestamps, which is frustrating when I want to quickly search based on relative time. I want to be able to find workflows started in the last hour without needing to figure out the absolute timestamp myself before sending the query.

**Describe the solution you'd like**
I'd like to be able to query with < '5m' or >= '1 day' where it's a relative time and the absolute timestamp is calculated at the time of query execution.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.


#### Comments (1)

<details>
<summary><strong>awln-temporal</strong> commented on 2025-10-16 17:56:54.000 UTC</summary>

Feature implementation requires a signed Duration value to compare with the current timestamp. Otherwise, all DateTime search attributes (System / Predefined / Custom) support relative timestamp queries as requested. Feel free to leave comments and suggestions.

Pull request: https://github.com/temporalio/temporal/pull/8498

Edit: Pending further design discussion before making changes.

</details>


---

### #8322: Don't log a warning when task queue kind is unspecified

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8322 |
| **State** | OPEN |
| **Author** | jvilk-stripe (John Vilk) |
| **Created** | 2025-09-12 21:07:06.000 UTC (3 months ago) |
| **Updated** | 2025-09-12 21:12:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

The temporal server logs the following log line _every time a task runs_ without the task queue `kind` set:

```
level=WARN msg="Unspecified task queue kind" service=frontend wf-task-queue-name=devbox-temporal-server:pay-server:shared wf-namespace=dev>
```

As a result, it dominates server log output. [`kind` is well-documented to default to "Normal"](https://github.com/temporalio/api/blob/ae312b0724003957b96fb966e3fe25a02abaade4/temporal/api/taskqueue/v1/message.proto#L47), so it seems odd to warn about this behavior.

The Ruby SDK we are using does not pass `kind`, so every Ruby task we run generates this log line. This seems like a high amount of logs for a non-issue!

Source of log line: https://github.com/temporalio/temporal/blob/5c26e48486a412d212e88a6faba14d3439627111/service/frontend/workflow_handler.go#L859

**Describe the solution you'd like**

Don't emit this log line by default. Maybe move it to a verbose option if it's useful for some.

**Describe alternatives you've considered**

We may try to patch our Ruby SDK to pass `kind` to silence this log line, but I believe the community would benefit from less log chunder.

**Additional context**
None.

#### Comments (1)

<details>
<summary><strong>jvilk-stripe</strong> commented on 2025-09-12 21:12:00.000 UTC</summary>

It looks like this was reported before and should've been fixed in 1.24:
https://github.com/temporalio/temporal/issues/5903

But I appear to be using a version with that change, and still see the warning:

```
$ /pay/deploy/devbox-temporal-server/current/temporal --version
temporal version 1.3.0 (Server 1.27.1, UI 2.36.0)
```

</details>


---

### #8298: Make Nexus work out of the box with zero config

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8298 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2025-09-08 17:32:46.000 UTC (3 months ago) |
| **Updated** | 2025-09-11 17:57:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | chaptersix |
| **Milestone** | None |

#### Description

## Start with

- Allow `temporal://system` URLs always to be attached as callbacks.
- Dispatch `temporal://system` callback URLs internally, automatically (see link below).

## Then

- Add a token [here](https://github.com/temporalio/api/blob/0286a6924ef7bd2d31ef1a1141d5e50bd43bb3c6/temporal/api/common/v1/message.proto#L171).
 
## Then update the SPEC and Nexus SDKs

- In the Nexus SPEC.md, call out that the `Nexus-Callback-Token` is a standard header and that it is required as part of the `StartOperation` request.
- In the Nexus SDKs (start with Go), add a `CallbackToken []byte (or string TBD)` field in `StartOpertionOptions` (called details, context) in the other SDKs.

## Once the server can handle these new URLs, switch to using them

- Behind a dynamic config, which can be deprecated in a few releases.
- Default the server's callback URL to `temporal://system`.
  - The callback URL template, must be provided when making "external" calls but "worker" endpoints should use the `temporal://system` callback URL.
- Figure out how to make this work in Cloud.

## Reference
https://github.com/temporalio/temporal/blob/16b1e29b2a86eaf1a95e026e49e16f31f8c414aa/components/callbacks/fx.go#L45
https://github.com/temporalio/temporal/blob/16b1e29b2a86eaf1a95e026e49e16f31f8c414aa/components/nexusoperations/fx.go#L102-L111

#### Comments (1)

<details>
<summary><strong>chaptersix</strong> commented on 2025-09-11 17:57:33.000 UTC</summary>

PR: #8308 for first task.

</details>


---

### #8235: GetWorkflow doesn't find archived one

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8235 |
| **State** | OPEN |
| **Author** | loicsaintroch (Lo√Øc Saint-Roch) |
| **Created** | 2021-08-06 08:22:53.000 UTC (4y 4m ago) |
| **Updated** | 2025-08-25 15:40:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

When passing both a workflow and run ID, the SDK should return the desired run for the given workflow, even though this workflow has been archived, right?

So this code should work:
```go
wr := c.GetWorkflow(context.Background(), WorkflowId, RunId)
err = wr.Get(context.Background(), &ptr)
if err != nil {
  // ...
}
```

## Actual Behavior

But when executing this, I have the following error:
```bash
Workflow executionsRow not found.  WorkflowId: <id>, RunId: <id>
```

It looks like the error comes from [temporalio/temporal/blob/master/common/persistence/sql/execution.go#L257](https://github.com/temporalio/temporal/blob/master/common/persistence/sql/execution.go#L257)

## Steps to Reproduce the Problem

  1. Run a workflow, and wait for it to be archived.
  2. With the SDK, get the workflow by passing its ID and a run ID.

(I use the file storage for archival, which works fine.)

## Specifications

  - Version: SDK v1.8.0, Server v1.11.2
  - Platform: `darwin`


#### Comments (1)

<details>
<summary><strong>loicsaintroch</strong> commented on 2021-08-25 14:23:28.000 UTC</summary>

In case anyone else has issue related to this one, here is a little snippet of the workaround I use for now:

```go
// Try to find the desired workflow, with no run ID. If no workflow is returned,
// continue so we can try to find if from the archival store.
w := c.GetWorkflow(context.Background(), workflowID, "")
err = w.Get(context.Background(), &result)
if err != nil {
  if _, notFound := err.(*serviceerror.NotFound); !notFound {
    // return ...
  }
}

// Build the search query for the archival store.
search := &workflowservice.ListArchivedWorkflowExecutionsRequest{
  Namespace: "",
  Query:     fmt.Sprintf("WorkflowId = '%s'", workflowID),
}

// Try to find the workflow from the archived ones.
workflows, err := c.ListArchivedWorkflow(context.Background(), search)
if err != nil {
  // return ...
}

// Save the workflow ID and run ID from the executions returned by the
// archival store. In my case I just need the first one returned.
workflowID = workflows.Executions[0].Execution.WorkflowId
runID := workflows.Executions[0].Execution.RunId

// Iter over events found for the given workflow ID and run ID.
iter := c.GetWorkflowHistory(context.Background(), workflowID, runID, false, 1)
events := []*history.HistoryEvent{}
for iter.HasNext() {
  event, err := iter.Next()
  if err != nil {
    // return ...
  }

  // Only add the events we desire. In this case when a workflow is started (2),
  // completed (3), or failed (4).
  if event.EventType == 2 || event.EventType == 3 || event.EventType == 4 {
    events = append(events, event)
  }
}

// ...

// Try to unmarshal the data of the payload found.
err = json.Unmarshal(events[0].GetWorkflowExecutionCompletedEventAttributes().Result.Payloads[0].Data, &result)
if err != nil {
  // return ...
}
```

Not sure if this is best / idiomatic way to achieve this, but it fits perfectly for my use case.

</details>


---

### #8220: Addressing security vulnerabilities in the Temporalio/server v1.28.1

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8220 |
| **State** | OPEN |
| **Author** | thle40 |
| **Created** | 2025-08-20 09:13:05.000 UTC (4 months ago) |
| **Updated** | 2025-08-21 22:38:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
No more CVEs found

## Actual Behavior
There are some CVEs found from the latest Temporal image:
temporalio/server:1.28.1

## Steps to Reproduce the Problem

> Pull the latest image temporalio/server:1.28.1  from Dockerhub
> Scan the image with any vulnerability scanner

CVE | SEVERITY | CVSS | PACKAGE | VERSION | FIX IN
-- | -- | -- | -- | -- | --
CVE-2025-22870 | HIGH | 8.8 | golang.org/x/net/http/httpproxy | v0.34.0 | 0.36.0
CVE-2023-47108 |MEDIUM | 7.5 | go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc | v0.36.4 | 0.46.0 	
CVE-2024-44337 | MEDIUM | 6.9 	| github.com/gomarkdown/markdown/parser | v0.0.0-20241105142532-d03b89096d81 | N/A
CVE-2024-2689 | medium   | 4.40 | go.temporal.io/server | v1.18.1-0.20230217005328-b313b7f58641 | 1.20.5, 1.21.6, 1.22.7
CVE-2025-4575 | LOW | None 	| [CVE-2025-4575](https://security.snyk.io/vuln/SNYK-ALPINE322-OPENSSL-10597997) | openssl:3.5.0-r0 | 3.5.1-r0




#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2025-08-21 22:38:13.000 UTC</summary>

Going over the list of CVEs, here's where it is used:
- `openssl` is a dependency of curl, not used in the server binary and planned to be removed from the image in the future
- `otelgrpc` is from the `tctl` binary which is deprecated and will be removed from the image in a future release
- `gomarkdown/markdown/parser` is coming from Temporal CLI, needs to be upgraded, will open an issue there
- `go.temporal.io/server` is from the `tctl` binary which is deprecated and will be removed from the image in a future release
- `golang.org/x/net/http/httpproxy` is from `dockerize` which is also going to be removed from the image in a future release

We will be taking action on all of these.

</details>


---

### #8219: Addressing security vulnerabilities in the Temporalio/admin-tools:1.28.1-tctl-1.18.4-cli-1.4.1

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8219 |
| **State** | OPEN |
| **Author** | thle40 |
| **Created** | 2025-08-20 09:08:00.000 UTC (4 months ago) |
| **Updated** | 2025-09-04 22:33:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
No more CVEs found

## Actual Behavior
There are some CVEs found from the latest Temporal image:
temporalio/admin-tools:1.28.1-tctl-1.18.4-cli-1.4.1

## Steps to Reproduce the Problem

> Pull the latest image temporalio/admin-tools:1.28.1  from Dockerhub
> Scan the image with any vulnerability scanner

CVE | SEVERITY | CVSS | PACKAGE | VERSION | FIX IN
-- | -- | -- | -- | -- | --
CVE-2025-6965 | LOW | None 	| sqlite:3.49.2-r0 	| 3.49.2-r1
CVE-2025-22870, CWE-918 | HIGH | 8.8 	| golang.org/x/net/http/httpproxy | v0.35.0 	| 0.36.0
CVE-2023-47108, CWE-770, GHSA-8pgv-569h-w5rw | HIGH | 7.5 | go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc | v0.36.4 	| 0.46.0
CVE-2024-44337, CWE-835 | MEDIUM | 6.9 	| github.com/gomarkdown/markdown/parser | v0.0.0-20241105142532-d03b89096d81 | N/A
CVE-2024-2689  | medium   | 4.40 | go.temporal.io/server        | v1.18.1-0.20230217005328-b313b7f58641 | 1.20.5, 1.21.6, 1.22.7
CVE-2025-4575 | LOW | None 	| openssl:3.5.0-r0 	| 3.5.1-r0
CVE-2025-8715 | LOW 	None 	| postgresql17 | 17.5-r0 	| 17.6-r0
CVE-2025-8714 | LOW 	None 	| postgresql17 | 17.5-r0 	| 17.6-r0
CVE-2025-8713 | LOW 	None 	| postgresql17 | 17.5-r0 	| 17.6-r0








#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2025-09-04 22:33:03.000 UTC</summary>

We are aware, we have an initiative to clean up these images that we are trying to start.
I hope we can kick this off soon.

Reactions: üëç 2

</details>


---

### #8101: Improve record activity started implementation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8101 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2025-07-25 23:11:43.000 UTC (5 months ago) |
| **Updated** | 2025-10-07 07:51:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
RecordActivityStarted implementation today loads the ActivityScheduledEvent while holding the workflow lock. Also the event is not added to the events cache when the activity is being scheduled. This increases the time workflow is locked unnecessarily (because history is immutable and can be accessed without workflow being locked). 

When workflow schedules large number of activities at the same time, the transfer task for dispatching the activity (and RecordActivityStarted will be invoked on sync match) will compete with each other for the workflow lock and this additional locking makes the contention worse and task will start to fail, backoff and retry. 

Of course we can improve the task processing part to reduce/avoid lock contention, but fixing the RecordActivityStarted implementation is very straightforward and a low hanging fruit.

**Describe the solution you'd like**
- Read ActivityScheduledEvent outside workflow lock
- Cache ActivityScheduledEvent

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.


#### Comments (1)

<details>
<summary><strong>yycptt</strong> commented on 2025-07-31 00:01:35.000 UTC</summary>

we did put activityScheduledEvent into the cache: https://github.com/temporalio/temporal/blob/9746b5f72ed6bddb512c136a59f7e614de76dcd3/service/history/workflow/mutable_state_impl.go#L3487

so loading them inside workflow lock is probably not an big deal. Need some extra data on event cache hit rate to confirm if the change will make any real difference or not.

</details>


---

### #7916: Allow multiple task queues to a worker - enables throttling of specific activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7916 |
| **State** | OPEN |
| **Author** | iuliananz (Iulian Stefanica) |
| **Created** | 2025-06-13 07:07:54.000 UTC (6 months ago) |
| **Updated** | 2025-06-26 23:12:01.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
We need to throttle specific activities across all worker instances to not run more than a number per second. That's to not overload downstream.

Temporal task queues already can be configured to have a limit of activities run per second, so if we could register several task queues (with different limits), we could use specific queues for some activities that need throttling.

**Describe the solution you'd like**
Ability to attach several task queues to a worker. Each task queue can have it's TaskQueueActivitiesPerSecond configured independently.

```
worker.New(temporalClient, "your-task-queue-name", worker.Options{})
```
 would now looks something like:

```
queues := []Queue{{
    "task-queue-fast",
    QueueOptions {
        TaskQueueActivitiesPerSecond: 100000,
    }},
    "task-queue-slow",
    QueueOptions {
        TaskQueueActivitiesPerSecond: 10,
    }},
}

worker.New(temporalClient, queues, worker.Options{})
```
then calling an activity, you can select what queue to send it to
```
newOptions := workflow.ActivityOptions{
	TaskQueue:          "task-queue-slow,
}
newCtx := workflow.WithActivityOptions(ctx, newOptions)
err := workflow.ExecuteActivity(newCtx, ThrottledActivity, name).Get(ctx, &result)
```

**Describe alternatives you've considered**
Alternatives available:
- have copies of the same worker register to the same task queue - requires several instances of the worker
- do the throttling using an external system - e.g. redis/db etc

**Additional context**
This is a very common setup which all projects calling downstream API endpoints would be able to use.

I am happy to give it a go at raising a PR for it if you think this is something you'd like to include.


#### Comments (1)

<details>
<summary><strong>dnr</strong> commented on 2025-06-26 23:12:01.000 UTC</summary>

Thanks for the suggestion. For these sorts of features, the general direction we're going in is to make a single task queue more powerful and configurable instead of requiring the use of multiple task queues.

In this case I'm thinking we might eventually want a way to attach one or more tags to activities (which could just be the activity type) and then specify rate limits for those tags. That would fit pretty well into the roadmap for priority and fairness features for task queues.

In the meantime, you could implement the multiple task queue solution without explicit support, just run multiple workers in the same process, one for each task queue. That's fairly lightweight. You'd have to manage resource sharing among those workers yourself, but any feature to support listening on multiple task queues in the worker would run into most of the same issues, which is why it's better to do it all within the task queue.

</details>


---

### #7869: temporal frontend unable to connect to elasticsearch visibilitystore

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7869 |
| **State** | OPEN |
| **Author** | adwait-vk |
| **Created** | 2025-06-05 10:07:09.000 UTC (6 months ago) |
| **Updated** | 2025-06-26 22:56:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I am trying to deploy temporal with visibility set as elastic search... but I am getting this error.
Error: INSTALLATION FAILED: execution error at (temporal/templates/server-deployment.yaml:37:28): Please specify cassandra port for visibility store.


Thus is my values.custom.yaml file:
server:
  enabled: true
  config:
    persistence:
      defaultStore: default
      advancedVisibilityStore: es-visibility
      datastores: {}

      default:
        driver: "sql"
        sql:
          driver: "mysql"
          host: "10.24.40.101"
          port: 4000
          database: "temporal"
          user: "readwrite"
          password: "2UQs4+iw0U"
          maxConns: 500
          maxConnLifetime: "1h"

      es-visibility:
        driver: "elasticsearch"
        elasticsearch:
          version: "v8"
          logLevel: "error"
          url:
            scheme: "http"
            host: "chanakya-temporal-playground-es-http.ekl-cl-chanakya-playground.svc.cluster.local:9200"
          indices:
            visibility: "temporal_visibility"

cassandra:
  enabled: false
  config:
    ports:
      cql: 9042

mysql:
  enabled: false # We're using external MySQL

postgresql:
  enabled: false

elasticsearch:
  enabled: false # Using external Elasticsearch
  external: true
  version: "v8"
  scheme: "http"
  host: "chanakya-temporal-playground-es-http.ekl-cl-chanakya-playground.svc.cluster.local"
  port: "9200"
  username: ""
  password: ""
  visibilityIndex: "temporal_visibility_v1"
  logLevel: "error"

prometheus:
  enabled: false

grafana:
  enabled: false

schema:
  setup:
    enabled: false # Schema already set up
  update:
    enabled: false # Schema already set up

I am not even using cassandra enywhere but still getting this issue.

#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2025-06-26 22:56:39.000 UTC</summary>

Can you please provide the Temporal server version and helm chart version?
Also @rodrigozhou have you seen this before?

</details>


---

### #7800: SSO and system worker

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7800 |
| **State** | OPEN |
| **Author** | fabianboerner |
| **Created** | 2025-05-24 00:31:29.000 UTC (7 months ago) |
| **Updated** | 2025-05-28 22:52:56.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi,

if i enable SSO with JWT authorization the system worker cannot access the frontend.

msg":"error starting temporal-sys-history-scanner-workflow workflow","service":"worker","error":"Request unauthorized.","logging-call-at":"/home/runner/work/docker-builds/docker-builds/temporal/service/worker/scanner/scanner.go:292","stacktrace":"[go.temporal.io/server/common/log.(*zapLogger).Error](http://go.temporal.io/server/common/log.(*zapLogger).Error)\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/log/zap_logger.go:154\[ngo.temporal.io/server/service/worker/scanner.(*Scanner).startWorkflow](http://ngo.temporal.io/server/service/worker/scanner.(*Scanner).startWorkflow)\n\t/home/runner/work/docker-builds/docker-builds/temporal/service/worker/scanner/scanner.go:292\[ngo.temporal.io/server/service/worker/scanner.(*Scanner).startWorkflowWithRetry.func1](http://ngo.temporal.io/server/service/worker/scanner.(*Scanner).startWorkflowWithRetry.func1)\n\t/home/runner/work/docker-builds/docker-builds/temporal/service/worker/scanner/scanner.go:262\[ngo.temporal.io/server/common/backoff.ThrottleRetryContext](http://ngo.temporal.io/server/common/backoff.ThrottleRetryContext)\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/backoff/retry.go:89\[ngo.temporal.io/server/service/worker/scanner.(*Scanner).startWorkflowWithRetry](http://ngo.temporal.io/server/service/worker/scanner.(*Scanner).startWorkflowWithRetry)\n\t/home/runner/work/docker-builds/docker-builds/temporal/service/worker/scanner/scanner.go:261"}
{"level":"fatal","ts":"2025-05-23T23:14:20.986Z","msg":"error starting scanner","service":"worker","error":"Request unauthorized.","logging-call-at":"/home/runner/work/docker-builds/docker-builds/temporal/service/worker/service.go:340","stacktrace":"[go.temporal.io/server/common/log.(*zapLogger).Fatal](http://go.temporal.io/server/common/log.(*zapLogger).Fatal)\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/log/zap_logger.go:178\[ngo.temporal.io/server/service/worker.(*Service).startScanner](http://ngo.temporal.io/server/service/worker.(*Service).startScanner)\n\t/home/runner/work/docker-builds/docker-builds/temporal/service/worker/service.go:340\[ngo.temporal.io/server/service/worker.(*Service).Start](http://ngo.temporal.io/server/service/worker.(*Service).Start)\n\t/home/runner/work/docker-builds/docker-builds/temporal/service/worker/service.go:255\[ngo.uber.org/fx/internal/lifecycle.Wrap[...].func1](http://ngo.uber.org/fx/internal/lifecycle.Wrap[...].func1)\n\t/home/runner/go/pkg/mod/go.uber.org/fx@v1.23.0/internal/lifecycle/lifecycle.go:80\[ngo.uber.org/fx/internal/lifecycle.(*Lifecycle).runStartHook](http://ngo.uber.org/fx/internal/lifecycle.(*Lifecycle).runStartHook)\n\t/home/runner/go/pkg/mod/go.uber.org/fx@v1.23.0/internal/lifecycle/lifecycle.go:256\[ngo.uber.org/fx/internal/lifecycle.(*Lifecycle).Start](http://ngo.uber.org/fx/internal/lifecycle.(*Lifecycle).Start)\n\t/home/runner/go/pkg/mod/go.uber.org/fx@v1.23.0/internal/lifecycle/lifecycle.go:216\[ngo.uber.org/fx.(*App).start-fm.(*App).start.func1](http://ngo.uber.org/fx.(*App).start-fm.(*App).start.func1)\n\t/home/runner/go/pkg/mod/go.uber.org/fx@v1.23.0/app.go:704\[ngo.uber.org/fx.(*App).withRollback](http://ngo.uber.org/fx.(*App).withRollback)\n\t/home/runner/go/pkg/mod/go.uber.org/fx@v1.23.0/app.go:686\[ngo.uber.org/fx.(*App).start](http://ngo.uber.org/fx.(*App).start)\n\t/home/runner/go/pkg/mod/go.uber.org/fx@v1.23.0/app.go:703\[ngo.uber.org/fx.withTimeout.func1](http://ngo.uber.org/fx.withTimeout.func1)\n\t/home/runner/go/pkg/mod/go.uber.org/fx@v1.23.0/app.go:803"}


This is self hosted in kubernetes, is there a way to make that work without custom programming?

#### Comments (1)

<details>
<summary><strong>dnr</strong> commented on 2025-05-28 22:52:55.000 UTC</summary>

You probably want to use internal-frontend. This isn't documented that well, see the release notes here: https://github.com/temporalio/temporal/releases/tag/v1.20.0 under "Internal frontend"

</details>


---

### #7780: Support for HTTPS metrics endpoint on OSS Temporal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7780 |
| **State** | OPEN |
| **Author** | gkarthiks (Karthikeyan Govindaraj) |
| **Created** | 2025-05-15 16:40:23.000 UTC (7 months ago) |
| **Updated** | 2025-06-26 22:51:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

We are self-hosting OSS Temporal in our Kubernetes cluster. We are required to expose only secure APIs, including metrics APIs, from any container running in our cluster.

**Describe the solution you'd like**
A feature that allows to load a dedicated certificate different from the Temporal's inter-node and frontend certificate for the metrics endpoint. 

For example, we would like to run other services within the mesh and run self-hosted Temporal with native TLS, and with that model, Prometheus is already loaded with the service-mesh certificate to scrape from other containers. So, if Temporal can serve the metrics endpoint with the same certificate, it would be straightforward for Prometheus to access the HTTPS metrics endpoint.

**Describe alternatives you've considered**
Alternatives are really tough, tried to put the temporal inside the service mesh, didn't work well. 

**Additional context**
-N/A-


#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2025-06-26 22:51:33.000 UTC</summary>

We don't currently have plans to work on this but you're welcome to contribute.

</details>


---

### #7658: Allow Configurable Prefixes for Temporal Schema Table Names

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7658 |
| **State** | OPEN |
| **Author** | Dox33 |
| **Created** | 2025-04-24 11:21:58.000 UTC (8 months ago) |
| **Updated** | 2025-05-01 23:12:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Current Situation:
Temporal Server currently uses fixed table names, causing naming conflicts when multiple Temporal deployments or environments share a single database schema.

## Proposed Solution:
Add an optional configuration setting to allow specifying a custom prefix for all Temporal tables during schema initialization. 
Example: -- add_prefix "dev_" results in tables like "dev_executions", "dev_schema_version". Temporal Server should support these prefixed tables, ensuring all existing operations remain functional.

 ## Benefits:
Prevent naming conflicts in shared schemas.
Easier management of multiple environments.
Improved flexibility and compliance with naming conventions.

Thank you!


#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2025-05-01 23:12:56.000 UTC</summary>

This is low priority for us since most users can use separate schemas for their different deployments. Happy to take a contribution if this is important to you.

</details>


---

### #7657: Rename Visibility schema's version table to avoid conflict with Temporal schema

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7657 |
| **State** | OPEN |
| **Author** | Dox33 |
| **Created** | 2025-04-24 07:08:50.000 UTC (8 months ago) |
| **Updated** | 2025-05-01 23:14:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description


## Current Situation:
Temporal and Temporal Visibility schemas currently can't coexist in the same database because both utilize the same table name `schema_version`. This naming conflict causes versioning conflicts and blocks seamless schema upgrades.

## Proposed Solution:
Rename the Visibility schema‚Äôs versioning table, e.g., from `schema_version` to something distinct such as `visibility_version`. 

## Benefits:
This would allow both schemas to coexist without conflicts, enabling smoother upgrades and coexistence of multiple schema versions.

## Additional Context (Optional):
This suggestion arose from a discussion in https://community.temporal.io/t/allow-temporal-and-visibility-schemas-in-a-single-database-by-renaming-schema-version-table/17151.

Thank you!

#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2025-05-01 23:14:08.000 UTC</summary>

Related issue: #7658 

</details>


---

### #7591: 1.27.1 - Multiple ChildWorkflowStarted events written for same child workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7591 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2025-04-08 15:55:41.000 UTC (8 months ago) |
| **Updated** | 2025-10-14 12:10:56.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Community thread https://temporalio.slack.com/archives/CTRCR8RBP/p1744049342207489

TS SDK, user starting a number of child workflows async, in scenario where workflow wants to continueasnew
and server fails it with UnhandledCommand, server also seems to record a second ChildWorkflowExecutionStarted (seems buffered / resurrected) event to event history after workflow task failure. 


Sample event history:  

```
 {
      "eventId": "4746",
      "eventTime": "2025-04-08T13:27:43.099430181Z",
      "eventType": "EVENT_TYPE_START_CHILD_WORKFLOW_EXECUTION_INITIATED",
      ...
    }
    {
      "eventId": "4751",
      "eventTime": "2025-04-08T13:27:43.115638093Z",
      "eventType": "EVENT_TYPE_CHILD_WORKFLOW_EXECUTION_STARTED",
      "taskId": "24810703",
      "childWorkflowExecutionStartedEventAttributes": {
        "initiatedEventId": "4746",
        "workflowExecution": {
          "workflowId": "example-9205",
          "runId": "01961595-d504-72b6-ac7d-b5ca3fdb6489"
        },
        ...
    }
    {
      "eventId": "4855",
      "eventTime": "2025-04-08T13:27:43.115638093Z",
      "eventType": "EVENT_TYPE_CHILD_WORKFLOW_EXECUTION_STARTED",
      "taskId": "24810959",
      "childWorkflowExecutionStartedEventAttributes": {
        "initiatedEventId": "4746",
        "workflowExecution": {
          "runId": "01961595-d504-72b6-ac7d-b5ca3fdb6489"
      ...
    }
    {
      "eventId": "4907",
      "eventTime": "2025-04-08T13:28:23.518183956Z",
      "eventType": "EVENT_TYPE_CHILD_WORKFLOW_EXECUTION_COMPLETED",
        "workflowExecution": {
          "runId": "01961595-d504-72b6-ac7d-b5ca3fdb6489"
        },
        "initiatedEventId": "4746",
        "startedEventId": "4855"
      ...
    }
```

#### Comments (1)

<details>
<summary><strong>tsurdilo</strong> commented on 2025-04-08 17:31:44.000 UTC</summary>

issue seems related to Cassandra 5 which is not supported as of yet

</details>


---

### #7577: Add retries for Temporal having connectivity failures for persistence database

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7577 |
| **State** | OPEN |
| **Author** | jatin27chopra (JatinChopra) |
| **Created** | 2025-04-05 21:47:44.000 UTC (9 months ago) |
| **Updated** | 2025-05-23 20:50:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Temporal server needs to retry transient 5xx errors from visibility datastore especially when it fails to acquire database connections temporarily.

I tried raising database resources such as CPU, memory but it didn't help. 


It's the same error as reported in this 

https://github.com/pingcap/tidb/issues/52653


Or somehow expose connectAttributes and the variable "connect_timeout=XYZ" in temporal server configuration

#### Comments (1)

<details>
<summary><strong>jc2707</strong> commented on 2025-05-23 20:50:51.000 UTC</summary>

Can temporal officially support this? https://github.com/pingcap/tidb/issues/52653#issuecomment-2905221537 
We would like to not manually fork and update a gigantic codebase so that good new features and updates to further updates in temporal stay with us too (instead of a divergence). Thanks 

</details>


---

### #7515: Retry interval is not always respected

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7515 |
| **State** | OPEN |
| **Author** | gleason-m (Michael Gleason) |
| **Created** | 2025-03-21 22:15:54.000 UTC (9 months ago) |
| **Updated** | 2025-03-26 23:58:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | ychebotarev |
| **Milestone** | None |

#### Description

## Expected Behavior
With following retry policy, we expect 60 attempts with 1min between each attempt (assuming the activity always hits a retriable error)
```go
temporal.RetryPolicy{
	InitialInterval:        time.Minute,
	BackoffCoefficient:     1,
	MaximumAttempts:        60,
}
```

## Actual Behavior
Some attempts have less than 1 minute between them. In the logs below, attempts 6, 17, and 53 occur less than 1 second after the log for the preceding event.

> 2025/03/21 19:25:39 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 1 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:26:40 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 2 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:27:40 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 3 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:28:41 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 4 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:29:41 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 5 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:29:42 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 6 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:30:42 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 7 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:31:43 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 8 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:32:43 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 9 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:33:44 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 10 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:34:45 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 11 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:35:45 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 12 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:36:46 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 13 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:37:46 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 14 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:38:47 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 15 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:39:47 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 16 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:39:48 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 17 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:40:48 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 18 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:41:49 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 19 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:42:49 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 20 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:43:50 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 21 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:44:51 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 22 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:45:51 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 23 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:46:52 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 24 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:47:53 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 25 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:48:53 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 26 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:49:54 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 27 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:50:54 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 28 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:51:55 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 29 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:52:56 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 30 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:53:56 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 31 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:54:56 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 32 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:55:57 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 33 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:56:57 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 34 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:57:58 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 35 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:58:58 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 36 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 19:59:59 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 37 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:00:59 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 38 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:02:00 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 39 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:03:00 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 40 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:04:01 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 41 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:05:01 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 42 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:06:02 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 43 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:07:02 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 44 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:08:03 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 45 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:09:04 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 46 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:10:04 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 47 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:11:04 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 48 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:12:05 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 49 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:13:06 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 50 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:14:06 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 51 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:15:07 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 52 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:15:08 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 53 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:16:08 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 54 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:17:09 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 55 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:18:10 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 56 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:19:11 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 57 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:20:11 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-9vkgw@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 58 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:21:11 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-8m4x8@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 59 Error otdr not complete, need to retry (type: RetriableError, retryable: true)
> 2025/03/21 20:22:12 ERROR Activity error. Namespace temporal-system TaskQueue perm-task-queue WorkerID 1@azfiber-workflow-worker-deployment-79fc667848-nvgd9@ WorkflowID RunFiberCutWorkflow-1742584915037-test RunID a701d998-0606-4443-b0fc-5dc9dfe02fe2 ActivityType RunPollOtdr Attempt 60 Error otdr not complete, need to retry (type: RetriableError, retryable: true)


## Steps to Reproduce the Problem

I don't have an exact repro, but we are running with 3 replicas of the temporal server and 3 replicas of our worker. That seems to be relevant as I am unable to repro locally using the temporal server shipped with the CLI.

## Specifications

  - Version: 1.26.2
  - Platform:


#### Comments (1)

<details>
<summary><strong>gleason-m</strong> commented on 2025-03-26 23:58:56.000 UTC</summary>

slack thread with some additional context
https://temporalio.slack.com/archives/CTDTU3J4T/p1742508774924669

</details>


---

### #7506: Start child with WorkflowID longer than 255 with SQL as persistence would stuck

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7506 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2025-03-20 23:08:35.000 UTC (9 months ago) |
| **Updated** | 2025-04-03 22:58:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Due to DB column size limit, the workflow ID length limit is 255 when using SQL as persistence layer. 

If a workflow try to start a child with workflow ID longer than 255, the workflow would see a child initiated, and then it would stuck. Server log indicate that it is stuck in the start child transfer with error like `"msg":"Operation failed with internal error.","error":"createOrUpdateCurrentExecution failed. Failed to insert into current_executions table. Error: pq: value too long for type character varying(255)","error-type":"serviceerror.Unavailable"`

This is bad from user experience POV. At least it should fail the child sooner with clear error.

#### Comments (1)

<details>
<summary><strong>yycptt</strong> commented on 2025-04-03 22:58:09.000 UTC</summary>

We have `limit.maxIDLength` but that's a dynamic config and people may not have the right config for SQL causing this issue to happen. 

I think either the maxIDLength should not be a dynamic config, but hardcoded valued based on the persistence implementation used. Or we need to enforce a max limit based on the persistence impl used.

</details>


---

### #7503: Does temporal support sharing database?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7503 |
| **State** | OPEN |
| **Author** | lt-cha (lt) |
| **Created** | 2025-03-20 11:54:32.000 UTC (9 months ago) |
| **Updated** | 2025-03-20 22:39:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

hiÔºåthere
I'm planing to deploy self-hosting temporal, but my workflow executes 20 million times a day. 
In our usual business we use 100 database with sharding keyÔºåmy question is does temporal support sharing database to support so much workflow executions.

#### Comments (1)

<details>
<summary><strong>yycptt</strong> commented on 2025-03-20 22:39:01.000 UTC</summary>

Hello there. Temporal server doesn't support multiple databases at this time. 

Please join our community forum or slack channel (link in repo README) which are actually monitored by our dev success team, and they can help give suggestions for your use cases. 

</details>


---

### #7480: Addressing a lot of security vulnerabilities in the Temporalio/server release v1.27.1

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7480 |
| **State** | OPEN |
| **Author** | thle40 |
| **Created** | 2025-03-18 07:44:14.000 UTC (9 months ago) |
| **Updated** | 2025-03-25 03:52:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
No more CVEs found

## Actual Behavior
There are some CVEs found from the latest Temporal image:
temporalio/server:1.27.1

## Steps to Reproduce the Problem

> Pull the latest image temporalio/server:1.27.1  from Dockerhub
> Scan the image with any vulnerability scanner

CVE | SEVERITY | CVSS | PACKAGE | VERSION | FIX IN
-- | -- | -- | -- | -- | --
CVE-2024-2689 | medium   | 4.40 | go.temporal.io/server | v1.18.1-0.20230217005328-b313b7f58641 | 1.20.5, 1.21.6, 1.22.7
CVE-2023-3485 | low      | 3.00 | go.temporal.io/server | v1.18.1-0.20230217005328-b313b7f58641 | 1.20.0 
CVE-2025-22870(https://security.snyk.io/vuln/SNYK-GOLANG-GOLANGORGXNETHTTPHTTPPROXY-9058601) | HIGH | 8.8 | golang.org/x/net/http/httpproxy | v0.34.0 | 0.36.0
CVE-2025-22868 (https://security.snyk.io/vuln/SNYK-GOLANG-GOLANGORGXOAUTH2JWS-8749594) | HIGH 	|8.7 	| golang.org/x/oauth2/jws | v0.25.0 | 0.27.0
CVE-2025-27144, GHSA-c6gw-w398-hv78  | MEDIUM | 6.9 | github.com/go-jose/go-jose/v4 | v4.0.4 | 4.0.5
CVE-2024-44337 | MEDIUM | 6.9 	| github.com/gomarkdown/markdown/parser | v0.0.0-20241105142532-d03b89096d81 | N/A
CVE-2024-51744, GHSA-29wx-vh33-7x7r | LOW | 2.3 | github.com/golang-jwt/jwt | v3.2.2+incompatible | N/A




#### Comments (1)

<details>
<summary><strong>hansliu</strong> commented on 2025-03-25 03:52:52.000 UTC</summary>

We are also seeing these security vulnerabilities in temporalio/server:1.27.1, what is the current status?

</details>


---

### #7405: strange workflow task timeout

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7405 |
| **State** | OPEN |
| **Author** | hebrd (hebrd) |
| **Created** | 2025-02-28 01:00:51.000 UTC (10 months ago) |
| **Updated** | 2025-03-06 22:48:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I've set the activity timeout to 3600 seconds in the settings. Most tasks complete normally, but some are not polled and executed by the worker until several hours later.

Below is the event history‚ÄîI'm unsure what happened. Could anyone help diagnose this issue?

Thanks in advance!

![Image](https://github.com/user-attachments/assets/b28e6067-89b1-408d-b433-d869666dcab6)

#### Comments (1)

<details>
<summary><strong>yycptt</strong> commented on 2025-03-06 22:48:17.000 UTC</summary>

That typically means the workflow task is keep failing (or timing out) and retrying. We do not record all those retries as history events to prevent history from exploding when there are too many retires. Only the next successful workflow task will recorded in history. 

If workflow task is failing, there should be some error logs on the sdk worker side about the actual failure. You might also want to check if your worker is overloaded and taking too long to process the workflow task. 

</details>


---

### #7390: Feature: Integrate Needle RAG API as a component.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7390 |
| **State** | OPEN |
| **Author** | JANHMS (Jan Heimes) |
| **Created** | 2025-02-26 12:16:09.000 UTC (10 months ago) |
| **Updated** | 2025-02-26 19:03:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëÄ 1 |

#### Description

##  Describe the solution you'd like
Integrate Needle's RAG API as a native Temporal workflow component. Enable users to create collections, perform semantic searches, and pass results to downstream workflow steps without leaving the Temporal environment.

## Describe alternatives you've considered
Custom HTTP activities (requires boilerplate code)
Third-party middleware (adds complexity)
Separate plugins (breaks workflow continuity)

## Additional context
Needle has successful integrations with Zapier, n8n, and Langflow. Documentation: https://docs.needle-ai.com/docs/api-reference/collections/

#### Comments (1)

<details>
<summary><strong>mfateev</strong> commented on 2025-02-26 19:03:28.000 UTC</summary>

Would you DM me at [Temporal Slack](https://join.slack.com/t/temporalio/shared_invite/zt-30lkd47g6-Z3DlUpAHljqEqLik7y4Zjw)? I would like to understand your proposal in more detail over Zoom.

</details>


---

### #7367: Do not truncate activity failure info if there is not many of them

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7367 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2025-02-20 18:18:42.000 UTC (10 months ago) |
| **Updated** | 2025-02-20 23:32:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently server would truncate activity failure if it exceeds 4KB (default threshold). This make sense if there is many pending activities (like thousands), but it does not make sense if there is only one activity. 

A better solution is to only truncate if the aggregated failure from all pending activities exceeds some lager threshold. 


#### Comments (1)

<details>
<summary><strong>yycptt</strong> commented on 2025-02-20 23:32:57.000 UTC</summary>

Some context: 

We enforce a fixed 2kb activity failure size limit for each activity. This has a couple of issues:
- The limit maybe too low if a workflow only have 1 or 2 activities. We have received requests from cluster before saying we need a higher limit.
- The limit is too high when workflow has lots of pending activities (max 2k pending activities), causing entire mutable state size to reach limit and workflow get terminated.
- When combined with other things like buffered events,  the total mutable state size may reach the limit and get workflow terminated.



Some ideas:
- A total failure size limit across activities. 
- When ms size reaches the limit, before directly terminating the workflow, see if we can flush buffered events or truncate activity failure message. 

</details>


---

### #6819: Why is the default value of the SDK client grpc "MaxSendMsgSize" inconsistent with the server grpc "MaxRecvMsgSize"?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6819 |
| **State** | OPEN |
| **Author** | smile-yi (Áéã‰∏≠Ëâ∫) |
| **Created** | 2024-11-15 02:27:16.000 UTC (1y 1m ago) |
| **Updated** | 2025-02-22 03:15:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

The server grpc "MaxRecvMsgSize" iuses the default value of 4M and is not rewritten, but the client sdk grpc "MaxSendMsgSize" is rewritten to 128M. What is the reason for this?

server: 
```
func NewService(
	server *grpc.Server,
	serviceConfig *configs.Config,
	visibilityMgr manager.VisibilityManager,
	handler *Handler,
	logger log.Logger,
	grpcListener net.Listener,
	membershipMonitor membership.Monitor,
	metricsHandler metrics.Handler,
	healthServer *health.Server,
) *Service {
	return &Service{
		server:            server,
		handler:           handler,
		visibilityManager: visibilityMgr,
		config:            serviceConfig,
		logger:            logger,
		grpcListener:      grpcListener,
		membershipMonitor: membershipMonitor,
		metricsHandler:    metricsHandler,
		healthServer:      healthServer,
	}
}
```

client: 
```
defaultMaxPayloadSize = 128 * mb
...
opts = append(opts, securityOptions...)
opts = append(opts, grpc.WithDefaultCallOptions(grpc.MaxCallSendMsgSize(maxPayloadSize)))
opts = append(opts, grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(maxPayloadSize)))
```


#### Comments (1)

<details>
<summary><strong>AlbertZheng</strong> commented on 2025-02-22 03:15:25.000 UTC</summary>

Same question! and this caused an critical error "io.grpc.StatusRuntimeException: RESOURCE_EXHAUSTED: grpc: received message larger than max (5214441 vs. 4194304)" in our prod envs.

I suggest if the server codes can be improved to use the same grpc "MaxRecvMsgSize" (= 128M)  with the client sdk grpc "MaxSendMsgSize", this problem will be resolved right away!

</details>


---

### #6647: Make option for UpsertSearchAttribute command to fail if SA does not exists

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6647 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2024-10-11 00:21:12.000 UTC (1y 2m ago) |
| **Updated** | 2024-11-09 03:39:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, if workflow try to upsert SA from within the workflow and it does not exists, the workflow task will fail and keep retrying forever.

**Describe the solution you'd like**
Give option for workflow to fail the command if SA does not exists

**Describe alternatives you've considered**
Work around is to use activity/localActivity to check if SA exists before call upsertSA.
But really the author should make sure the SA exists before the use case go online. Create SA should be handed as one time job and should not be created dynamicly. If you find that you need to create SA dynamicly, you probably doing it wrong. The number of SA is very limited.

**Additional context**
Add any other context or screenshots about the feature request here.


#### Comments (1)

<details>
<summary><strong>drewhoskins-temporal</strong> commented on 2024-11-09 02:46:08.000 UTC</summary>

Given that there's a workaround, we have put this on the backlog for now and will not get to it any time soon.  Please bump this issue if you'd like to vote for it.

</details>


---

### #6527: WorkflowCache inconsistency upon workflow (force) deletion

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6527 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2024-09-16 23:44:17.000 UTC (1y 3m ago) |
| **Updated** | 2024-10-23 09:31:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
- Cached workflow mutate state should always match state in DB.


## Actual Behavior
- If workflow is deleted, it's still cached in memory but already deleted in DB.

In most cases this is not an issue, the normal workflow deleted command is async anyway. 
It's confusing for tdbg workflow deleted though because it's sync delete.


## Steps to Reproduce the Problem

  1. 
  1.
  1.

## Specifications

  - Version:
  - Platform:


#### Comments (1)

<details>
<summary><strong>guoziqian96</strong> commented on 2024-10-23 09:31:49.000 UTC</summary>

After deleting the workflow, it will still be displayed in the Temporal WEB UI list, but clicking on it will result in a 404 error

</details>


---

### #6525: Rare test failure in CLI: "Current branch token and request branch token doesn't match."

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6525 |
| **State** | OPEN |
| **Author** | josh-berry (Josh Berry) |
| **Created** | 2024-09-16 18:16:19.000 UTC (1y 3m ago) |
| **Updated** | 2024-10-07 18:32:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

The Temporal CLI tests hit a couple rare failures in CI after updating to Server 1.25 that we (Dan and I) believe to be a Server issue‚Äîwe see spurious errors of the form: `Current branch token and request branch token doesn't match.`

These seem very rare because neither Dan nor I were able to repro them on our local machines (despite ~100 attempts each), and when we re-ran the failing CI job, the tests succeeded.

## Expected Behavior

Successful CLI test run (no such errors reported)

## Actual Behavior

The following two CLI tests failed ‚Äî full log output here: https://github.com/temporalio/cli/actions/runs/10885937402/job/30204717954

In TestSharedServerSuite/TestWorkflow_Reset_ReapplyExclude:

```
=== FAIL: temporalcli TestSharedServerSuite/TestWorkflow_Reset_ReapplyExclude (0.02s)
    commands.workflow_reset_test.go:287: 
        	Error Trace:	/home/runner/actions-runner/_work/cli/cli/temporalcli/commands.workflow_reset_test.go:287
        	Error:      	Received unexpected error:
        	            	Current branch token and request branch token doesn't match.
        	Test:       	TestSharedServerSuite/TestWorkflow_Reset_ReapplyExclude
```

In TestSharedServerSuite/TestActivity_Complete:

```
=== FAIL: temporalcli TestSharedServerSuite/TestActivity_Complete (0.12s)
    commands_test.go:182: Calling: activity complete --activity-id dev-activity-id --workflow-id bb9450c4-9cf0-4ee6-b0f5-c5e1c1390db5 --result "complete-activity-result" --identity MyIdentity --address 127.0.0.1:37747
    commands.activity_test.go:27: 
        	Error Trace:	/home/runner/actions-runner/_work/cli/cli/temporalcli/commands.activity_test.go:27
        	Error:      	Received unexpected error:
        	            	Current branch token and request branch token doesn't match.
        	Test:       	TestSharedServerSuite/TestActivity_Complete
```

[TestActivity_Complete](https://github.com/temporalio/cli/blob/main/temporalcli/commands.activity_test.go#L12) is particularly interesting because there are no workflow resets performed during this test. This test is quite simple; here's what it does:

1. Launch a workflow
2. Wait for the workflow to launch an activity that never completes
3. Try to force-complete the activity using the CLI command: `temporal activity complete [‚Ä¶] --result [‚Ä¶] --identity [‚Ä¶]`

I think this is a server-side issue because nothing in the test actually does a workflow reset, and so there should be no reason why the workflow history has diverging branches. (We are also not using clustering or MRN or anything like that; this is just a single-process dev-server, so replication doesn't come into play at all.)

## Steps to Reproduce the Problem

  1. Check out `temporalio/cli`
  1. `go test -v -count 1 -run '^TestSharedServerSuite/(TestWorkflow_Reset_ReapplyExclude|TestActivity_Complete)$' github.com/temporalio/cli/temporalcli`

## Specifications

  - Version: 1.25
  - Platform: ubuntu-arm


#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2024-10-07 18:31:35.000 UTC</summary>

This same issue is happening in new Ruby SDK CI when fetching history:

```
time=2024-10-07T18:24:25.428 level=ERROR msg="service failures" operation=PollWorkflowExecutionHistory wf-namespace=default error="Current branch token and request branch token doesn't match."
time=2024-10-07T18:24:25.428 level=ERROR msg="service failures" operation=PollWorkflowExecutionHistory wf-namespace=default wf-id=wf-72884a17-e275-4ce9-807b-e49efed0537a wf-run-id=dd5e4b6f-141d-4804-8d79-3afda01f96e5 error="Current branch token and request branch token doesn't match."
E, [2024-10-07T18:24:25.443796 #9419] ERROR -- : Block failure (beginning worker shutdown)
E, [2024-10-07T18:24:25.443853 #9419] ERROR -- : Current branch token and request branch token doesn't match. (Temporalio::Error::RPCError)
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/client/connection/service.rb:35:in `rescue in invoke_rpc'
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/client/connection/service.rb:24:in `invoke_rpc'
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/client/connection/workflow_service.rb:160:in `get_workflow_execution_history'
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/internal/client/implementation.rb:161:in `block (2 levels) in fetch_workflow_history_events'
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/internal/client/implementation.rb:160:in `loop'
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/internal/client/implementation.rb:160:in `block in fetch_workflow_history_events'
/Users/runner/hostedtoolcache/Ruby/3.1.6/arm64/lib/ruby/gems/3.1.0/gems/rake-13.2.1/lib/rake/rake_test_loader.rb:in `each'
/Users/runner/hostedtoolcache/Ruby/3.1.6/arm64/lib/ruby/gems/3.1.0/gems/rake-13.2.1/lib/rake/rake_test_loader.rb:in `each'
Full cause chain:
Exception: RuntimeError - Neutered Exception Temporalio::Error::RPCError: Current branch token and request branch token doesn't match.
Backtrace:
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/client/connection/service.rb:35:in `rescue in invoke_rpc'
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/client/connection/service.rb:24:in `invoke_rpc'
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/client/connection/workflow_service.rb:160:in `get_workflow_execution_history'
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/internal/client/implementation.rb:161:in `block (2 levels) in fetch_workflow_history_events'
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/internal/client/implementation.rb:160:in `loop'
/Users/runner/work/sdk-ruby/sdk-ruby/temporalio/lib/temporalio/internal/client/implementation.rb:160:in `block in fetch_workflow_history_events'
/Users/runner/hostedtoolcache/Ruby/3.1.6/arm64/lib/ruby/gems/3.1.0/gems/rake-13.2.1/lib/rake/rake_test_loader.rb:in `each'
/Users/runner/hostedtoolcache/Ruby/3.1.6/arm64/lib/ruby/gems/3.1.0/gems/rake-13.2.1/lib/rake/rake_test_loader.rb:in `each'
```

</details>


---

### #6459: Excessive new connections to Postgres 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6459 |
| **State** | OPEN |
| **Author** | DonForbes (Donald Forbes) |
| **Created** | 2024-08-28 13:30:08.000 UTC (1y 4m ago) |
| **Updated** | 2025-04-03 04:29:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
Under load the number of connections to the Postgres database backend remains fairly consistent over time.

## Actual Behavior
When under load it has been observed that there are many new connections being made to the Postgres database being used as the DB backing for the history service.  (200+ new connections per second during load test). The expectation is that the number of connections may rise to handle the load but it should achieve a steady state and relatively few connections killed and re-established.

One suggestion is that the method get may be being called frequently and for some reason the refcount is not incremented so remains at 0 and many new connections returned.  To look into the problem further to see if this theory is valid.

https://github.com/temporalio/temporal/blob/b383ffffcbbeacdfce2fe021c30f093bab64b5d9/common/persistence/sql/factory.go#L195

## Steps to Reproduce the Problem

  1. Install self-hosted platform using Postgres as the DB
  2. Setup montoring for the number of connections/new connections being made to DB
  3. Run load test

## Specifications

  - Version: 1.24
  - Platform: Kubernetes via Helm charts


#### Comments (1)

<details>
<summary><strong>raymondregrello</strong> commented on 2025-04-03 04:29:38.000 UTC</summary>

Out of curiosity, in the helm chart `values.yaml` under `server.config.persistence.default.sql`, have you set `maxIdleConns`, `maxConns` and `maxConnLifetime` to sufficiently high values?

I believe the defaults for `maxIdleConns` are pretty low; we initially saw a lot of connection thrashing when load testing until we set it to a higher value.

</details>


---

### #6378: unknown method GetWorkflowExecutionHistoryReverse for service temporal.server.api

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6378 |
| **State** | OPEN |
| **Author** | trustnote-wang |
| **Created** | 2024-08-07 02:44:28.000 UTC (1y 4m ago) |
| **Updated** | 2024-10-21 22:27:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

A portion of the workflow history records that have been executed are displayed as 501. Note that not all of them are. Why is this happening? I have checked the documentation and it says that the version is incorrect, but I am using the latest version. The temporal server version is 1.24.2 and the Temporal UI Version is 2.28.0

![image](https://github.com/user-attachments/assets/fa183060-0dee-48ad-9a80-296317cb65d0)

![image](https://github.com/user-attachments/assets/56e9ab51-b239-43ec-a321-d88475ae0157)


#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2024-10-21 22:27:21.000 UTC</summary>

What is the setup of your server? Is this dev environment with temporal/CLI? Or is this a prod environment? 
Do you have proxy in between the UI and the server? 

</details>


---

### #6329: Ability to attach a local debugger to Local Temporal Worker

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6329 |
| **State** | OPEN |
| **Author** | abhishek-parative |
| **Created** | 2024-07-23 17:02:11.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-23 17:03:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
When testing a Temporal Worker locally, I would like the ability to expose a port and attach a debugger (either in a JetBrains product or VSCode) that allows me to trace each step the Worker takes. The debugging functionality provided by the Temporal Web UI is not sufficient in the slightest to trace where/why the application bugged out.



#### Comments (1)

<details>
<summary><strong>abhishek-parative</strong> commented on 2024-07-23 17:03:01.000 UTC</summary>

If this functionality exists, please send me the documentation and I can close out this feature request. I spent many hours combing through the Temporal documentation and could not find the functionality. 

</details>


---

### #6319: Schedule list with query times out on server 1.25.0-rc.0

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6319 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2024-07-22 18:28:16.000 UTC (1y 5m ago) |
| **Updated** | 2025-02-20 23:24:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Upgrading the server in the Go SDK test suite, causes a test that lists schedules with a visibility query to time out.

https://github.com/temporalio/sdk-go/commit/1350fb4bd58fa0782c774ab198d75e8cafb1fcff#diff-8118e44ac15fb5ab2e2ece2ab52fd23062a10b5cc9c55ee5b3620a1242cff27cR4938

#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2025-02-20 23:24:22.000 UTC</summary>

May no longer be relevant, requires reproduction.

</details>


---

### #6188: CreateSchedule with empty workflowID should not be allowed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6188 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2024-06-21 21:31:13.000 UTC (1y 6m ago) |
| **Updated** | 2025-04-12 05:04:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
The call should fail

## Actual Behavior
It is created with `- Timestampe`
![image](https://github.com/temporalio/temporal/assets/84411014/ce2fe620-ea0c-45fa-b01b-998365d21db1)

This cause some of them to succeed, but others to fail with workflow ID conflict if you call it multiple times in 1s.

## Steps to Reproduce the Problem

  1.
  1.
  1.

## Specifications

  - Version:
  - Platform:


#### Comments (1)

<details>
<summary><strong>phuongdnguyen</strong> commented on 2025-04-12 05:04:05.000 UTC</summary>

@yiminc , is this issue still valid? I can take it

</details>


---

### #6000: How to ensure that activities belonging to the same workflow can be prioritized and executed to avoid hunger when a batch of tasks is initiated for a workflow containing multiple activities.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6000 |
| **State** | OPEN |
| **Author** | HarvestWu (Âê¥ÈáëÁßã) |
| **Created** | 2024-05-27 03:05:11.000 UTC (1y 7m ago) |
| **Updated** | 2024-05-28 20:40:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

How to ensure that activities belonging to the same workflow can be prioritized and executed to avoid hunger when a batch of tasks is initiated for a workflow containing multiple activities.

#### Comments (1)

<details>
<summary><strong>dnr</strong> commented on 2024-05-28 20:40:36.000 UTC</summary>

Depending on your application, you might consider using local activities for some of them. In general though, this isn't really possible right now. We're starting to work on related issues so we might have a better answer in a while.

</details>


---

### #5796: Application Error with a Suggested Retry Interval Duration

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5796 |
| **State** | OPEN |
| **Author** | Albert-Coding (Albert) |
| **Created** | 2024-04-25 19:40:12.000 UTC (1y 8m ago) |
| **Updated** | 2024-05-17 21:40:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | gow |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Given an activity that calls a third-party service to perform a rate limited action, we return an application error when encountering a rate-limit response.  Sometimes, the third-party rate limit suggestion is to NOT make another call for X-period of time which is dynamic (e.g. 5s to 90s).

**Describe the solution you'd like**
It would be nice if the application error could also hold a `retry interval` value so that the Temporal server would not schedule the activity retry until such time has elapse.

**Describe alternatives you've considered**
1- Set the activity's `RetryPolicy.InitialInterval` to the max X-period expected.  This is bad because the activity is waiting too long before retrying.
or
2- Use a non-retryable application error and add details to it to then in the workflow make a timer and then retry the call.  This bad for many reasons (not listed here because they are probably obvious).
or
3- Don't return an error.  Have the activity handle the retrying by waiting and then trying again.  This is bad because we're reinventing a feature that's a first-class citizen in Temporal.  Additionally, it will take up an active activity slot.

#### Comments (1)

<details>
<summary><strong>gow</strong> commented on 2024-05-17 21:40:52.000 UTC</summary>

PR to allow workflow tasks to specify a retry delay interval - https://github.com/temporalio/temporal/pull/5946
We still need to update the SDK to send this option before we can make this feature complete. Working on that.

Reactions: ‚ù§Ô∏è 1

</details>


---

### #5763: Limit failure cause depth

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5763 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-04-19 15:53:21.000 UTC (1y 8m ago) |
| **Updated** | 2024-04-26 21:57:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

Users can set very deep failure causes, and once it gets past 30 or so it breaks some protobuf parsers (any based on `upb` like Python and maybe Ruby/C/C++). We should limit the depth to something like 10/15. Can discuss implementation design on how this limitation occurs (not sure we should reject the failure but up to implementer). I can also provide more exact numbers on how deep before Python fails if that information is needed.

#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2024-04-26 21:57:23.000 UTC</summary>

Server is not crack open those failure and just pass it along. It would be hard for server to arbitrary determine what is the cutoff depth. 

</details>


---

### #5372: trigger one workflow from another workflow 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5372 |
| **State** | OPEN |
| **Author** | mmoya91 (Maria Moya) |
| **Created** | 2024-01-30 21:38:33.000 UTC (1y 11m ago) |
| **Updated** | 2024-02-05 14:14:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I was wondering if it was possible to trigger one workflow using another workflow? For instance: can workflow A trigger workflow B and can workflow B access outputs from workflow A's activity?

We want to build two different workflows for two different teams that share the same temporal server

#### Comments (1)

<details>
<summary><strong>theleeeo</strong> commented on 2024-02-05 14:14:08.000 UTC</summary>

Yes, it is possible to trigger workflows from within a workflow. Accessing activity results is slightly more difficult but still possible.
If you are wondering from a security perspective, you should definitely use two different servers to keep everything separate. If you only want the two teams to not accidentally interfere with each others activity- and workflow names using two namespaces will likely be enough.

</details>


---

### #5324: access from one temporal namespace to another

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5324 |
| **State** | OPEN |
| **Author** | mmoya91 (Maria Moya) |
| **Created** | 2024-01-19 16:18:43.000 UTC (1y 11m ago) |
| **Updated** | 2024-01-26 22:43:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

hello, I was wondering if there any additional considerations when referencing resources from one namespace to another?

For instance: 
lets say I workflow_1 that contains activity_1 and resides in temporal namespace_1
and I also have namespace_2 which contains workflow_2 and activity_2
and both temporal namespaces, namespace_1 and namespace_2, exist in the same temporal server

would we run into any issues if the inputs for activity_2 depends on outputs from activity_1? I'm not sure if temporal namespaces behave like k8s namespaces as far as access

#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2024-01-26 22:43:44.000 UTC</summary>

Cross namespace invocation will be supported by nexus service. See more: https://github.com/temporalio/proposals/tree/master/nexus

</details>


---

### #4816: Elasticsearch legacy index template 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4816 |
| **State** | OPEN |
| **Author** | felrivero |
| **Created** | 2023-08-29 10:20:23.000 UTC (2y 4m ago) |
| **Updated** | 2023-09-01 21:51:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | rodrigozhou |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
There is a problem when using temporal with elasticsearch in which index templates are created in a new format https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-templates.html

**Describe the solution you'd like**
Convert index templates to a new format https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-templates.html

**Additional context**
Temporal uses deprecated index templates to work with Elasticsearch ( https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-templates-v1.html ) And does not combine the settings from the index template of the new version


#### Comments (1)

<details>
<summary><strong>rodrigozhou</strong> commented on 2023-09-01 21:51:26.000 UTC</summary>

Can you provide more details what's the new format? What's the difference?


</details>


---

### #4692: Could taskReader persistAckLevel check last persist ackLevel avoid persist duplicate ackLevel

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4692 |
| **State** | OPEN |
| **Author** | zedongh (zedongh) |
| **Created** | 2023-07-27 07:32:38.000 UTC (2y 5m ago) |
| **Updated** | 2023-07-28 21:38:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Some workflow worker or actvitiy worker just startup wait for task coming but maybe hold for long time. Here are may no task coming for a long while. Persist acklevel cause effortless database request per updateAckTimer.

**Describe the solution you'd like**
`taskReader` store last persist ack level and do `persistAckLevel` only when current ack level not match last persist ack level.

**Describe alternatives you've considered**
No

**Additional context**
No

#### Comments (1)

<details>
<summary><strong>dnr</strong> commented on 2023-07-27 18:12:18.000 UTC</summary>

That's correct. Actually there are more optimizations that could be done in that area, e.g. not trying to read tasks from the database periodically either. I've been looking into this area recently so there may be some changes here soon.

Reactions: üëç 1

</details>


---

### #4612: Matching task dispatch busy loop when history service is not available

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4612 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2023-07-11 00:59:27.000 UTC (2y 5m ago) |
| **Updated** | 2023-07-21 22:11:28.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug, matching |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
If history service is not running, matching service ideally should not generate lots of DB requests.

## Actual Behavior
When task in matching tasks table failed to be dispatched, the loaded task will be acked and a new task will be created. This is essentially a busy loop and consume tons of persistence resources.

This logic exists today to prevent one bad task from blocking the entire task queue I think.

## Steps to Reproduce the Problem

  1.
  1.
  1.

## Specifications

  - Version:
  - Platform:


#### Comments (1)

<details>
<summary><strong>alexshtin</strong> commented on 2023-07-21 22:11:28.000 UTC</summary>

Current work around is to start services in the following sequence: history, matching, frontend, worker.

</details>


---

### #4600: Use different task queue for activity and workflow task retries

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4600 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2023-07-07 18:50:35.000 UTC (2y 5m ago) |
| **Updated** | 2023-07-07 18:52:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Some applications require different retry behavior based on error codes. For example ([source](https://community.temporal.io/t/custom-retry-policy-per-error-type-for-activity/8772)):

> Hi, we are currently trying to design retry policy for our activity. The activity is calling http server and we want to retry based on the http status code.
> 
> The default for 5xx, it will be retryable with shorter backoff of e.g. 2 sec as these are mostly intermittent.
> For 429 (rate limiter) error we want to retry with longer backoff e.g. 1 min.
> For 400 error, this might happen because a resource in the server are currently being ‚Äòpaused‚Äô by human operator for maintenance. This maintenance is expected to be done < 10 mins, after which the resource is available again. We want to auto retry this request with long backoff e.g. 10 mins.
> Other 4xx will NOT be retryable.

Providing different retry options will partially help, but for rate limit error changing backoff of an individual activity is not going to help as the aggregate rate across all of them can still be high.

**Describe the solution you'd like**
Use a different rate limited (possibly with a dynamic rate limiter) task queue to retry activities in certain scenarios. This way retries of activities that failed with a specific error code would be scheduled in a separate task queue. This allows other activities to execute and retry without being limited.

The queue might not be directly exposed to a user and be an implementation detail.


#### Comments (1)

<details>
<summary><strong>mfateev</strong> commented on 2023-07-07 18:52:58.000 UTC</summary>

This can also solve the problem of workflow tasks that are constantly failing, affecting healthy workflows.

</details>


---

### #4502: Support data references to avoid storing the same Payload multiple times

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4502 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2023-06-15 22:16:52.000 UTC (2y 6m ago) |
| **Updated** | 2025-09-24 18:06:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ‚ù§Ô∏è 9 |

#### Description

**Is your feature request related to a problem? Please describe.**
The same value can be an argument of many activities. For example, one activity returns a document, and then 1k activities are invoked, each of them receiving a document as one of the arguments. In this case, the document is going to be stored 1k+1 times in the history which is wasteful. 

**Describe the solution you'd like**
My strawman is to introduce a special command (or reuse Marker command) that would store a value (as Payloads) in the history. Add support for activity/child workflow inputs and outputs reference that value (by eventId) instead of including it directly into the corresponding schedule commands.

Another option is to support referencing Payloads directly inside other events like WorkflowExecutionStarted.

**Additional context**
From Slack:

> To make this more concrete, i'm implementing a DSL on top of a Temporal workflow.
> The DSL workflow definition has some things that need to be extracted and consumed throughout, sometimes.
> Currently i'm passing around my DSL string to all functions as a "dsl workflow context" 







#### Comments (1)

<details>
<summary><strong>Pavan-Samtani</strong> commented on 2025-09-24 18:06:22.000 UTC</summary>

Any updates to this feature request?

Reactions: üëç 1

</details>


---

### #4348: Replication tasks referencing archived workflow executions can't be processed, blocking all replication

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4348 |
| **State** | OPEN |
| **Author** | emmercm (Christian Emmer) |
| **Created** | 2023-05-16 18:58:35.000 UTC (2y 7m ago) |
| **Updated** | 2023-05-19 21:56:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | yux0 |
| **Milestone** | None |

#### Description

_Carried over from this community thread: https://community.temporal.io/t/what-is-the-correct-way-to-disable-re-enable-multi-cluster-replication/8216?u=emmercm_

## Expected Behavior

When two clusters are replicating to each other, and one is taken offline for an extended period of time (longer than namespace retention windows), then when the cluster is brought back online it should catch up on replication.

## Actual Behavior

No workflow history replication is occurring, including workflows newly started after the secondary cluster was brought back online.

The trio of error logs that I see constantly coming from the primary cluster's history service are, in order:

- ```json
  {
    "msg": "Persistent fetch operation Failure",
    "wf-run-id": "...",
    "store-operation": "get-wf-execution",
    "shard-id": 383,
    "address": "...:7234",
    "wf-namespace-id": "...",
    "stacktrace": "go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:144\ngo.temporal.io/server/service/history/workflow.getWorkflowExecution\n\t/home/builder/temporal/service/history/workflow/transaction_impl.go:423\ngo.temporal.io/server/service/history/workflow.(*ContextImpl).LoadMutableState\n\t/home/builder/temporal/service/history/workflow/context.go:263\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).processReplication\n\t/home/builder/temporal/service/history/replication/ack_manager.go:582\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).generateHistoryReplicationTask\n\t/home/builder/temporal/service/history/replication/ack_manager.go:431\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).toReplicationTask\n\t/home/builder/temporal/service/history/replication/ack_manager.go:356\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).getTasks\n\t/home/builder/temporal/service/history/replication/ack_manager.go:280\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).GetTasks\n\t/home/builder/temporal/service/history/replication/ack_manager.go:224\ngo.temporal.io/server/service/history/api/replication.GetTasks\n\t/home/builder/temporal/service/history/api/replication/get_tasks.go:60\ngo.temporal.io/server/service/history.(*historyEngineImpl).GetReplicationMessages\n\t/home/builder/temporal/service/history/historyEngine.go:750\ngo.temporal.io/server/service/history.(*Handler).GetReplicationMessages.func1\n\t/home/builder/temporal/service/history/handler.go:1417",
    "level": "error",
    "wf-id": "...",
    "error": "context canceled",
    "logging-call-at": "transaction_impl.go:423",
    "ts": "2023-05-16T17:39:45.746Z"
  }
  ```
- ```json
  {
    "msg": "replication task reader encounter error, return earlier",
    "component": "replicator-queue-processor",
    "shard-id": 383,
    "address": "...:7234",
    "stacktrace": "go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:144\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).getTasks\n\t/home/builder/temporal/service/history/replication/ack_manager.go:281\ngo.temporal.io/server/service/history/replication.(*ackMgrImpl).GetTasks\n\t/home/builder/temporal/service/history/replication/ack_manager.go:224\ngo.temporal.io/server/service/history/api/replication.GetTasks\n\t/home/builder/temporal/service/history/api/replication/get_tasks.go:60\ngo.temporal.io/server/service/history.(*historyEngineImpl).GetReplicationMessages\n\t/home/builder/temporal/service/history/historyEngine.go:750\ngo.temporal.io/server/service/history.(*Handler).GetReplicationMessages.func1\n\t/home/builder/temporal/service/history/handler.go:1417",
    "level": "error",
    "error": "context canceled",
    "logging-call-at": "ack_manager.go:281",
    "ts": "2023-05-16T17:39:45.746Z"
  }
  ```
- ```json
  {
    "msg": "Failed to retrieve replication messages.",
    "shard-id": 383,
    "address": "...:7234",
    "stacktrace": "go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:144\ngo.temporal.io/server/service/history/api/replication.GetTasks\n\t/home/builder/temporal/service/history/api/replication/get_tasks.go:66\ngo.temporal.io/server/service/history.(*historyEngineImpl).GetReplicationMessages\n\t/home/builder/temporal/service/history/historyEngine.go:750\ngo.temporal.io/server/service/history.(*Handler).GetReplicationMessages.func1\n\t/home/builder/temporal/service/history/handler.go:1417",
    "level": "error",
    "error": "context canceled",
    "logging-call-at": "get_tasks.go:66",
    "ts": "2023-05-16T17:39:45.746Z"
  }
  ```

The `Persistent fetch operation Failure` error seems to be the root problem. I would have expected `shard.Context.GetWorkflowExecution()` to return `serviceerror.NotFound` if the old workflows couldn't be found, though, so I'm confused by that.

Some other metrics, carried over from the linked community thread:

- The primary cluster's:
  - Metric `persistence_error_with_type{operation="getreplicationtasks"}` with `error_type="serviceerrorunavailable"` is emitting at a fairly constant rate
  - Metric `replication_tasks_fetched` is a flat zero
  - Table `replication_tasks` is only being `INSERT`ed to, never `DELETE`d from. It has >2.3mil rows.
  - DB has no obvious errors or timeouts.

I'm happy to gather any other metrics that would help debug the issue.

Given the `ORDER BY` on `SELECT task_id, data, data_encoding FROM replication_tasks WHERE shard_id = ? AND task_id >= ? AND task_id < ? ORDER BY task_id LIMIT ?`, I don't think this will ever resolve on its own. 

## Steps to Reproduce the Problem

1. Have two Temporal clusters:
   1. With 512 history shards
   2. In multi-cluster replication, and observe it is working as expected
2. Have all namespaces with:
   1. A default 72h retention period
   2. The default 4 task queue partitions 
3. Have all namespaces active in the "primary" cluster, none active in the "secondary cluster"
4. Scale the secondary cluster down to zero replicas
5. Wait an extended period of time, e.g. 2 weeks
   1. During this time, the primary cluster is still processing workflows, at a rate of ~240/hour for a total of ~140k completed while the secondary cluster is offline
6. Scale the secondary cluster back above zero replicas
7. Observe that no replication is occurring, based on the metrics above

## Specifications

- Version: Temporal server v1.19.1
- Platform: Kubernetes & Docker `temporalio/server:1.19.1`, `docker.io/temporalio/server@sha256:c8a5cdb7c78d26c9d611ce19abb62733dfe5480e02d40a39968bd9b2ab8b45c2`
- Persistence store: MySQL v8 via Vitess


#### Comments (1)

<details>
<summary><strong>emmercm</strong> commented on 2023-05-19 17:50:24.000 UTC</summary>

I was able to reproduce this locally with Docker Compose. The steps were:

1. Have two `temporalio/auto-setup:1.19.1` containers, both backed by MySQL
2. Set up multi-cluster replication between the two
3. Start a workflow worker application that is connected to the primary cluster only
4. Start 100 workflows in the primary cluster, have the worker application process & complete them
5. Observe the workflow history is replicated to the secondary cluster, via the secondary cluster's web UI
6. Start triggering 60k workflows in the primary cluster
7. Wait ~10sec, then observe that some of the workflows that have been started so far have been replicated to the secondary cluster
8. Stop the secondary cluster's container
9. Wait until every workflow has been completed or timed out, as observed in the primary cluster's web UI
10. Stop the primary cluster's container (in order to purge any kind of in-memory caches)
11. Observe that the primary cluster's MySQL has ~282k `replication_tasks` rows and ~39k `executions` rows (some of the workflows timed out)
12. Delete every row in the primary cluster's `executions` and `current_executions` tables
13. Re-start the primary cluster's container
14. Re-start the secondary cluster's container
15. After waiting >10min, continue to observe:
  1. The primary cluster's MySQL still has ~282k `replication_tasks` rows
  2. The primary cluster is emitting logs described in the original post

Then, to see what would happen with newly started workflows with both clusters running, I did:

1. Start another 100 workflows in the primary cluster
2. Keep both cluster containers running
3. Observe that the workflows were all processed & completed by the worker application that was never stopped
4. Observe that the workflows were never replicated to the secondary cluster

</details>


---

### #4247: Add firstExecutionRunId to system search attribute

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4247 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2023-04-27 22:09:23.000 UTC (2y 8m ago) |
| **Updated** | 2023-04-28 21:44:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | rodrigozhou |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Right now runId is always changed when :
* ContinueAsNew
* Reset
* Cron
* WorkflowRetry

However, user have to rely on runId to distinguish different executions when using IdReusePolicy. For example, user starts two different runs with IdReusePolicy, and they are both completed after certain reset or continueAsNew.

Now user wants to query the **final** workflow results for different runs. 

**Describe the solution you'd like**
User can provide the firstExecutionRunId to find out the last runId (filter by completed or running status) to find out the last runId. And then use the final runId to query the workflows

**Describe alternatives you've considered**
User can do search attribute themselves, but it's repeated work.

**Additional context**
NA


#### Comments (1)

<details>
<summary><strong>alexshtin</strong> commented on 2023-04-28 21:44:05.000 UTC</summary>

Similar issue: https://github.com/temporalio/temporal/issues/351

</details>


---

### #4246: Consider adding Heartbeat Details to other Failure modes of an Activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4246 |
| **State** | OPEN |
| **Author** | mastermanu |
| **Created** | 2023-04-27 21:28:28.000 UTC (2y 8m ago) |
| **Updated** | 2023-04-28 21:40:58.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | samanbarghi |
| **Milestone** | None |

#### Description

Timeout failure info has heartbeat details (https://github.com/temporalio/api/blob/master/temporal/api/failure/v1/message.proto#L43)
Would be great, if this can be surfaced on other items (such as Canceled and ApplicationFailure) as that can make it easier to start a new activity and resume from when a potentially previous attempt got stuck and required starting a new one.

#### Comments (1)

<details>
<summary><strong>cretz</strong> commented on 2023-04-27 21:29:31.000 UTC</summary>

I think it can just be on `ActivityFailure`

</details>


---

### #4131: Issue with MariaDB 10.6.9.  (Galera cluster)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4131 |
| **State** | OPEN |
| **Author** | niv0 (Vladimir Nisevic) |
| **Created** | 2023-03-31 08:09:08.000 UTC (2y 9m ago) |
| **Updated** | 2023-03-31 08:11:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
We are using Temporal 1.20 with MariaDB 10.2.30 and now we moved to new MariaDB 10.6.9 running in clustered mode (using Galera https://galeracluster.com/) 

## Actual Behavior

Testing with new Version we observe issues on some workflows, workflow stays in status "Running", activity is scheduled but nothing happens.

![image](https://user-images.githubusercontent.com/2017626/229062797-f142b461-f100-4293-848f-f222a7c288aa.png)


## Steps to Reproduce the Problem

  1. use MariaDB 10.6.9
  2. launch e.g. 100 workflows 
  3. some workflows stay in status RUNNING forever

## Specifications

  - Version: 1.20
  - Platform: K8S


#### Comments (1)

<details>
<summary><strong>niv0</strong> commented on 2023-03-31 08:11:33.000 UTC</summary>

Also I see in https://github.com/temporalio/temporal/blob/master/go.mod#L14 you use mysql driver in version 1.5, meanwhile there is a version 1.7 out there.

Support for go 1.17 and MariaDB 10.6 is done with this PR https://github.com/go-sql-driver/mysql/pull/1253 which is obviously later then used version 1.5

So easy fix could be to upgrade to latest mysql go driver?

</details>


---

### #4021: Expiration policy for signal requestIDs in mutable state

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4021 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2023-03-07 02:21:34.000 UTC (2y 10m ago) |
| **Updated** | 2023-04-20 00:14:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement, P0 |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Today if workflow is signal directly via the Signal api (not workflow-to-workflow signal), signal requestID will be store in workflow mutable state forever, which will result in an increase in mutable state size, which could lead to performance issue when say there're 30K signal requestIDs.

**Describe the solution you'd like**

* Simply rely on the existing max signal count limit.
* Give up the existing guarantee we have on signal deduplication and remove those requestIDs before workflow is closed.
  * Expire requestID based on time (still need to enforce the max signal count limit)
  * Expire requestID based on count 

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



#### Comments (1)

<details>
<summary><strong>yycptt</strong> commented on 2023-04-20 00:14:30.000 UTC</summary>

Same as https://github.com/temporalio/temporal/issues/4191

</details>


---

### #3987: Named Timers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3987 |
| **State** | OPEN |
| **Author** | Albert-Coding (Albert) |
| **Created** | 2023-02-23 14:09:49.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-08 21:36:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
When unit testing timers, one uses:


```
SetOnTimerScheduledListener(listener func(timerID string, duration time.Duration))
```

which returns the string `1`, `2`, etc.  The numbers correspond to the timer creation order (i.e. first, second, third, etc...).

This means unit tests are brittle because the placement of a timer in the code will mean the position expectations will break.

**Describe the solution you'd like**
A solution would be to allow for named timers similar to named goroutines and gochannels.  Then pass that name into something like


```
SetOnNamedTimerScheduledListener(listener func(timerID string, name string, duration time.Duration))
```

**Describe alternatives you've considered**
Alternatives are:
1) don't make unit tests to test timer firing order (non-ideal)
2) make unit tests and accept they will be brittle and developers will learn to fear changing the ordering of timers in the code
3) make support workflow code to introduce named timers at the workflow level

**Additional context**
https://community.temporal.io/t/testing-a-timer-fired/7371/2



#### Comments (1)

<details>
<summary><strong>mfateev</strong> commented on 2023-03-08 21:36:59.000 UTC</summary>

There was a similar request for activities from a customer. We don't want to use activityId for this purpose:

> Good morning. I have a workflow that spans a large number of activities, and I was planning on manually setting up the activity Id, to help navigate the web UI. Doing a search here though yield this advice from a Temporal resource:
> Activity ID, while technically settable, really shouldn't be manually set.
> What is the best way to trace one activity amongst many for a workflow? Particularly because I will need to chain activities. Is there a way in the web UI to query for an activity Id and see all the related events? I know Temporal integrates with Datadog and other platform, etc.. but I am focusing on a POC to demonstrate the capabilities for now.



</details>


---

### #3983: Unused `GetHistoryTask` persistence API

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3983 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2023-02-22 03:33:38.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-03 18:02:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | refactoring |
| **Assignees** | yycptt |
| **Milestone** | None |

#### Description

`GetHistoryTask` is not used anywhere, shall we remove it from the codebase? 

Or we plan to build some admin api around it?



#### Comments (1)

<details>
<summary><strong>yycptt</strong> commented on 2023-03-02 23:42:50.000 UTC</summary>

Will remove this API.

The only use case I can see is use it as an admin tool to dump information for a certain task, which GetHistoryTasks  (the batch read version) can also do.

The only case batch read can‚Äôt cover is when there‚Äôre multiple tasks with same visibility timestamp (1ms precision) in a scheduled queue, all tasks with that timestamp will be returned. But I don‚Äôt think for a given shard there‚Äôll be tons of tasks with the same timestamp so that should be ok.

</details>


---

### #3682: Taskqueue scavenger does not work for cassandra

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3682 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2022-12-01 22:44:29.000 UTC (3y 1m ago) |
| **Updated** | 2023-03-03 20:17:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | MichaelSnowden |
| **Milestone** | None |

#### Description

Task queue scavenger is not working for cassandra. 
This is mostly because the ListTaskQueue persistence API is not implemented by Cassandra persistence.
https://github.com/temporalio/temporal/blob/79a572de6bb649d31539ce67c8885c5cec6bacf9/common/persistence/cassandra/matching_task_store.go#L286

Need to fix this so stale task queue can be cleaned up.

#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2022-12-02 02:11:48.000 UTC</summary>

https://github.com/temporalio/temporal/blob/21ed2aa5a1088bb0228d0d18883617ebb2f41ba2/service/worker/scanner/taskqueue/handler.go#L145-L153
https://github.com/temporalio/temporal/issues/1021

</details>


---

### #3667: Temporal Server returns incorrect RetryState in some cases of scheduleToClose timeout

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3667 |
| **State** | OPEN |
| **Author** | Spikhalskiy (Dmitry Spikhalsky) |
| **Created** | 2022-11-28 00:11:36.000 UTC (3y 1m ago) |
| **Updated** | 2023-03-03 20:17:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug |
| **Assignees** | Spikhalskiy |
| **Milestone** | None |

#### Description

## Required context

There are two distinct situations that lead to firing `ScheduleToClose` timeout of an activity. Let's call them "Speculative" (not a precise term) and "Factual".

"Factual" mode: The last Activity attempt is scheduled or started, but not finished yet. `ScheduleToClose` timeout timer fires on the Server. Temporal Server fails the Activity Execution with an `ActivityFailure` caused by `ScheduleToClose` timeout, because it was actually reached.

"Speculative" mode: After an activity retry is failed, Temporal Server calculates how long is it until the next attempt of this activity. If the next attempt should happen after `ScheduleToClose` deadline is reached, there is no reason to wait for the next attempt, because it will never happen. Temporal Server proactively fails the Activity Execution with an `ActivityFailure` caused by `ScheduleToClose` timeout, because `ScheduleToClose` timeout will happen earlier than the next attempt. 

## Expected Behavior

I expect the situations above to be aligned in the way how they are returned to the SDK. 
I would expect to receive `ActivityFailure(retryState=Timeout)` and `TimeoutFailureInfo(timeoutType=ScheduleToClose)` as a cause.

## Actual Behavior
"Factual" mode returns as `ActivityFailure(retryState=Timeout)` with `TimeoutFailureInfo(timeoutType=ScheduleToClose)` as expected.

"Speculative" mode returns as `ActivityFailure(retryState=NonRetryableError)` with `TimeoutFailureInfo(timeoutType=ScheduleToClose)`.

This does not look right. 

`RetryState=NonRetryableError` happens and should happen when an activity attempt is failed with a type explicitly set as non-retryable by the user and only in this situation. Otherwise, it significantly complicates the way for users to check for situations if one of the explicitly stated NonRetryableErrors has been thrown. 

On the other hand, `ActivityFailure(retryState=Timeout)` is reserved specifically for server-enforced timeouts and currently happens only with `ScheduleToClose` timeout. `StartToClose` timeout always returns with `MaximumAttemptsReached`, which makes sense, because an actual reason for timing out is that there are no attempts left, not the `StartToClose` itself.

## Additional Context

ScheduleToStart timeout also arrives with NonRetryableFailure, which also doesn't look right.
Tests covering all of the timeout modes and specifically the "Factual" mode:
 https://github.com/temporalio/sdk-java/blob/c5f0ebbe8a5cd52ee018809ecdd466379b5a7339/temporal-sdk/src/test/java/io/temporal/workflow/activityTests/ActivityTimeoutTest.java#L594
https://github.com/temporalio/sdk-java/blob/c5f0ebbe8a5cd52ee018809ecdd466379b5a7339/temporal-sdk/src/test/java/io/temporal/workflow/activityTests/ActivityTimeoutTest.java#L650

#### Comments (1)

<details>
<summary><strong>mfateev</strong> commented on 2022-11-28 02:06:31.000 UTC</summary>

If I were designing it now without thinking about backward compatibility:

I think we should return all the available information about failure:

* The last activity failure as a cause. This can be either ApplicationFailure or TimeoutFailure for StartToCloseTimeout.
* The information about why retry is not possible

TimeoutFailureInfo should be returned only in case of StartToClose timeout.

ActivityFailure
    retryState=..., 
    lastHeartbeatDetails=...
    cause=    ApplicationFailure | TimeoutFailure (only if the last failure was StartToClose timeout)

</details>


---

### #3666: Add ability to invoke the ParentClosePolicy on chain completion not the run completion.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3666 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2022-11-25 20:38:04.000 UTC (3y 1m ago) |
| **Updated** | 2023-11-18 19:16:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
When a parent calls continue-as-new the child behaves according to its ParentClosePolicy. A common scenario is to keep children running until the whole chain of parent continue-as-new calls is complete. In this case the only option is to set ParentClosePolicy to ABANDON. But it disables the ability to terminate all the children automatically when the parent is terminated.

**Describe the solution you'd like**
Add a flag that specifies if ParentClosePolicy applies when the chain is done or the run is done. Besides not being backward compatible, I propose to default it to when the whole chain is done. The strawman for the flag is `parentClosePolicyScope` with RUN, WORKFLOW_CHAIN (or just WORKFLOW). 

If parentClosePolicyScope is set to WORKFLOW_CHAIN then terminating the last run of a workflow will terminate all the children started by the whole chain. This obviously should obey the parent retention interval as information about the past children started by deleted due to retention runs might not be available. 

#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2023-11-18 19:16:05.000 UTC</summary>

When parent continue as new, it does not carry over child, the new run does not consider previous run's child as their child. 
We cannot keep carry over child to next run as will become unbounded. 
The possible solution for this new parent_close_policy is for the system workflow that handles the policy to traversal the chain backwards if the scope of the policy is to the chain. 
The scope also needs to be passed down when doing ContinueAsNew. 

</details>


---

### #3417: deleteWorkflowExecution failure response leaks implementation details

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3417 |
| **State** | OPEN |
| **Author** | Spikhalskiy (Dmitry Spikhalsky) |
| **Created** | 2022-09-20 21:16:56.000 UTC (3y 3m ago) |
| **Updated** | 2023-10-12 17:20:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | API, P1 |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Actual Behavior

Calling DeleteWorkflowExecution in java-sdk for already deleted execution causes a response with the following message:
`io.grpc.StatusRuntimeException: NOT_FOUND: operation GetWorkflowExecution encountered not found`

Temporal server leaks implementation details of workflow execution deletion by oversharing that "GetWorkflowExecution encountered not found" while we didn't call GetWorkflowExecution, we called DeleteWorkflowExecution.

Reproduces on Temporal Cloud at 2022-09-20. Probably Temporal Server v1.17.6.


#### Comments (1)

<details>
<summary><strong>mindaugasrukas</strong> commented on 2022-09-26 19:56:30.000 UTC</summary>

Reproduced using tctl:
```
% ./tctl workflow delete --workflow-id=hello_world_workflowID
Delete workflow succeeded
% ./tctl workflow delete --workflow-id=hello_world_workflowID
Error: unable to delete workflow: operation GetCurrentExecution encountered not found
('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
```

</details>


---

### #3177: Add input validation that request ID size is below threshold

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3177 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-08-03 16:45:39.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:18:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | rodrigozhou |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Validate that request ID size does not exceed limit in business logic layer.
Currently, the validation is done by DB: e.g. https://github.com/temporalio/temporal/blob/v1.17.1/schema/cassandra/temporal/schema.cql#L44


#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2022-08-05 21:34:55.000 UTC</summary>

Input validation for request_ID has to be uuid as it is required by persistence schema. 


</details>


---

### #3165: Sync search attributes and ES schema

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3165 |
| **State** | OPEN |
| **Author** | rodrigozhou (Rodrigo Zhou) |
| **Created** | 2022-07-29 16:15:00.000 UTC (3y 5m ago) |
| **Updated** | 2025-10-07 21:18:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | rodrigozhou |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
If I delete the Elasticsearch index, and rebuild it with `make install-schema-es`, it doesn't add the existing search attributes to the ES mappings, ie., it doesn't recreate the indexes for them.

**Describe the solution you'd like**
Add a command to `tctl` to sync search attributes and ES schema, and auto-sync when running `make install-schema-es`.

**Describe alternatives you've considered**
Maybe also do checks in visibility when adding new documents to make sure the search attributes are indexed?

**Additional context**
N/A


#### Comments (1)

<details>
<summary><strong>colinbjohnson</strong> commented on 2025-10-07 21:18:55.000 UTC</summary>

I feel like the temporal attribute to elasticsearch mapping sync tool is needed - I would have used today.

I did explore using Elasticsearch's [Elasticsearch's "Dynamic Mapping"](https://www.elastic.co/docs/manage-data/data-store/mapping/dynamic-mapping) to keep temporal attributes and elasticsearch mappings in sync - that configuration doesn't meet my requirement for simplicity/predictability because the Dynamic Mapping feature _does not_ guarantee that a mapping's "data type" will be set as you would desire.

</details>


---

### #3134: Do not block shard ownership assertion if `acquireShards` is blocked

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3134 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-07-22 20:41:14.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:19:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement, potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Do not block shard ownership assertion if `acquireShards` is blocked
v1.17.x logic:

shard controller will start a background thread periodically asserting shard ownership
```go
for {
	select {
	...
	case <-acquireTicker.C:
		c.acquireShards()
	...
	}
}
```

if acquireShards is blocked due to whatever reason, then no further acquireShards will be executed.

OSS team should consider the following logic
```go
var (
	ShardController struct {
		...

		shardAssertionInProgressMutex sync.Mutex
		shardAssertionInProgress map[int32]struct{}
		shardIDChan chan int32
		...
	}
)

func (sc *ShardController) eventLoop() {
	acquireTicker := time.NewTicker(c.config.AcquireShardInterval())
	defer acquireTicker.Stop()

	for {
		select {
		case <-c.shutdownCh:			
			return
		case <-acquireTicker.C:
			for i = 0; i < totalShardIDs; i++ {
				shardAssertionInProgressMutex.Lock()
				_, exist = sc.shardAssertionInProgress[i]
				if !exist {
					sc.shardAssertionInProgress[i] = struct{}{}
				}
				shardAssertionInProgressMutex.Unlock()
				if !exist {
					sc.shardIDChan <- i
				}
			}
		}
	}
}

func (sc *ShardController) acquireShard() {
	for shardID := range shardIDChan {
		...
		// shard ownership assertion logic here
		...
		
		shardAssertionInProgressMutex.Lock()
		delete(sc.shardAssertionInProgress, shardID)
		shardAssertionInProgressMutex.Unlock()
	}
}
```


#### Comments (1)

<details>
<summary><strong>dnr</strong> commented on 2022-07-27 01:58:51.000 UTC</summary>

I like that idea, I agree there's no reason to wait for one "wave" to finish before starting the next one. But also look at #3108, which will make `acquireShards` not block on individual shard rwlocks anymore, which might be enough so that this doesn't matter.

</details>


---

### #3065: i get taskqueue history metics,it has abort all  80000 activities metrics.it is to large for promethues to pull

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3065 |
| **State** | OPEN |
| **Author** | SuperLight-007 (guangzhi.Zhu) |
| **Created** | 2022-07-06 09:28:40.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:19:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
i get taskqueue history metics,it has abort all  80000 activities metrics.it is to large for promethues to pull

## Actual Behavior
i get taskqueue history metics,it has abort all  80000 activities metrics.it is to large for promethues to pull

## Steps to Reproduce the Problem

  1.
  1.
  1.

## Specifications

  - Version:
  - Platform:


#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2022-07-08 21:36:54.000 UTC</summary>

Could you provide more details.

</details>


---

### #3008: Continue workflow as new and signal in a single transaction

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3008 |
| **State** | OPEN |
| **Author** | mmxmb (Max) |
| **Created** | 2022-06-21 03:04:13.000 UTC (3y 6m ago) |
| **Updated** | 2023-03-03 20:19:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement, feature-request |
| **Assignees** | None |
| **Milestone** | None |

#### Description

_I will be using Go SDK naming here. But I believe this is a server-side feature request._

**Is your feature request related to a problem? Please describe.**

It is possible to start a new workflow and send a signal to it in a signal transaction using `SignalWithStartWorkflow`. However, it is not possible to continue a workflow as new (`NewContinueAsNewError`) and send a signal to it in a single transaction.

A workflow example that demonstrates why this might be useful:

* The workflow is running "an infinite loop" by returning `NewContinueAsNewError` at the end of each execution. 
* During each iteration of this workflow it runs some activities given an input. This input can be updated while the workflow is running. The workflow will finish the current iteration using the old input. The updated input is used on the next iteration.
*  In fact, the input can be updated multiple times during the single iteration. The next iteration must use the most recent input. All other input is considered stale and is discarded.
* The workflow sleeps at the end of each iteration if no new input was provided and starts the next iteration using the old input.
* If new input was provided while the workflow was running activities/during sleep, it skips sleep/stop sleeping and starts the next iteration using the new input right away.

This description suggests that it's best to provide input as a signal.

Pseudocode to implement this workflow:

1. Start workflow/signal already running workflow using `SignalWithStartWorkflow`
2. Receive input signal
3. Long running activities that use input
4. Sleep unless new input is received:
4.1. No new input. Sleep until timer finishes. Return `NewContinueAsNewError` and signal old input in the same transaction.
4.2 New input was signalled (maybe several signals). Drain signal channel to get the most recently provided input. Return `NewContinueAsNewError` and signal new input in a single transaction.

**Describe the solution you'd like**

Add new error type that is like `NewContinueAsNewError` but allows signalling the new workflow in a single transaction like `SignalWithStartWorkflow`.

**Describe alternatives you've considered**


Right now I wrote my workflow to accept the same input struct using both workflow function argument and signal. I need to accept input as signals because of the requirement to be able to update input for the next iteration while the current iteration is running. I need to be able to provide input as argument since this is currently the only way to provide input to the workflow that is started using `NewContinueAsNewError`.

Providing the same input type using both workflow argument and signal is awkward, thus my feature request.

**Additional context**

In case my use case description and pseudocode don't make sense, here's the skeleton implementation of the workflow that I currently have: https://gist.github.com/mmxmb/d3a2b490855c98f0a28f3f6d30a005e2

#### Comments (1)

<details>
<summary><strong>mmxmb</strong> commented on 2023-03-03 19:23:57.000 UTC</summary>

Any reason why this was closed?

</details>


---

### #2829: Support starting abandoned child after parent workflow is closed 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2829 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2022-05-10 17:38:56.000 UTC (3y 7m ago) |
| **Updated** | 2023-11-25 00:31:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently after parent workflow is closed, if child is not started yet (meaning child started event not recorded), child start will be skipped even if the parent close policy is abandon. Due to this reason, user needs to explicitly wait for the started event in their workflow before completing it.

**Describe the solution you'd like**
https://github.com/temporalio/temporal/blob/master/service/history/transferQueueActiveTaskExecutor.go#L687
https://github.com/temporalio/temporal/blob/master/service/history/transferQueueStandbyTaskExecutor.go#L415

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**



#### Comments (1)

<details>
<summary><strong>mfateev</strong> commented on 2023-11-25 00:31:51.000 UTC</summary>

This applies to signalExternalWorkflow as well.

</details>


---

### #2707: Admin CLI to update mutable state directly

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2707 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-04-08 04:30:29.000 UTC (3y 8m ago) |
| **Updated** | 2023-03-03 20:20:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

* e.g. update workflow task timeout
* e.g. update workflow task queue

#### Comments (1)

<details>
<summary><strong>wxing1292</strong> commented on 2022-04-08 04:32:28.000 UTC</summary>

@bergundy

</details>


---

### #2687: Populate identity for child workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2687 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-03-30 21:28:41.000 UTC (3y 9m ago) |
| **Updated** | 2023-03-03 20:20:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement, difficulty: easy, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

`temporal.api.history.v1.WorkflowExecutionStartedEventAttributes.identity` is populated with the client identity when started from a client, but not populated with a value when it's a child workflow.

**Describe the solution you'd like**

Use `temporal.api.workflowservice.v1.RespondWorkflowTaskCompletedRequest.identity` for the child workflow identity. SDKs set this as the same identity they set on `temporal.api.workflowservice.v1.StartWorkflowExecutionRequest.identity`. So a top-level workflow and a child-workflow started from the same SDK client will have the same value.

#### Comments (1)

<details>
<summary><strong>sumit-maurya-maersk</strong> commented on 2022-03-31 03:24:21.000 UTC</summary>

Below is the child workflow history. You can clearly  see that `identity=""` for `event id=1` and for other  events is showing `identity="MyIdentity"` that is correct. The issue is only with `event id=1`.

```json
[
  {
    "eventId": "1",
    "eventTime": "2022-03-30T05:31:30.480372Z",
    "eventType": "WorkflowExecutionStarted",
    "version": "0",
    "taskId": "1048593",
    "workflowExecutionStartedEventAttributes": {
      "workflowType": {
        "name": "SampleChildWorkflow"
      },
      "parentWorkflowNamespace": "default",
      "parentWorkflowExecution": {
        "workflowId": "parent-workflow_7ec4cd96-0b6f-44a2-b084-e266ccd5f81d",
        "runId": "09e80459-93ca-4022-8d6b-aa546c71bb94"
      },
      "parentInitiatedEventId": "5",
      "taskQueue": {
        "name": "child-workflow-continue-as-new",
        "kind": "Normal"
      },
      "input": {
        "payloads": [
          0,
          5
        ]
      },
      "workflowExecutionTimeout": "0s",
      "workflowRunTimeout": "0s",
      "workflowTaskTimeout": "10s",
      "continuedExecutionRunId": "",
      "initiator": "Unspecified",
      "continuedFailure": null,
      "lastCompletionResult": null,
      "originalExecutionRunId": "d48ff871-9016-4d5f-bf12-cf79a72d3a91",
      "identity": "",
      "firstExecutionRunId": "d48ff871-9016-4d5f-bf12-cf79a72d3a91",
      "retryPolicy": null,
      "attempt": 1,
      "workflowExecutionExpirationTime": null,
      "cronSchedule": "",
      "firstWorkflowTaskBackoff": "0s",
      "memo": null,
      "searchAttributes": null,
      "prevAutoResetPoints": null,
      "header": {
        "fields": {}
      }
    },
    "id": "1",
    "attributes": {
      "type": "workflowExecutionStartedEventAttributes",
      "workflowType": {
        "name": "SampleChildWorkflow"
      },
      "parentWorkflowNamespace": "default",
      "parentWorkflowExecution": {
        "workflowId": "parent-workflow_7ec4cd96-0b6f-44a2-b084-e266ccd5f81d",
        "runId": "09e80459-93ca-4022-8d6b-aa546c71bb94"
      },
      "parentInitiatedEventId": "5",
      "taskQueue": {
        "name": "child-workflow-continue-as-new",
        "kind": "Normal"
      },
      "input": {
        "payloads": [
          0,
          5
        ]
      },
      "workflowExecutionTimeout": "0s",
      "workflowRunTimeout": "0s",
      "workflowTaskTimeout": "10s",
      "continuedExecutionRunId": "",
      "initiator": "Unspecified",
      "continuedFailure": null,
      "lastCompletionResult": null,
      "originalExecutionRunId": "d48ff871-9016-4d5f-bf12-cf79a72d3a91",
      "identity": "",
      "firstExecutionRunId": "d48ff871-9016-4d5f-bf12-cf79a72d3a91",
      "retryPolicy": null,
      "attempt": 1,
      "workflowExecutionExpirationTime": null,
      "cronSchedule": "",
      "firstWorkflowTaskBackoff": "0s",
      "memo": null,
      "searchAttributes": null,
      "prevAutoResetPoints": null,
      "header": {
        "fields": {}
      }
    }
  },
  {
    "eventId": "2",
    "eventTime": "2022-03-30T05:31:30.482071Z",
    "eventType": "WorkflowTaskScheduled",
    "version": "0",
    "taskId": "1048601",
    "workflowTaskScheduledEventAttributes": {
      "taskQueue": {
        "name": "child-workflow-continue-as-new",
        "kind": "Normal"
      },
      "startToCloseTimeout": "10s",
      "attempt": 1
    },
    "id": "2",
    "attributes": {
      "type": "workflowTaskScheduledEventAttributes",
      "taskQueue": {
        "name": "child-workflow-continue-as-new",
        "kind": "Normal"
      },
      "startToCloseTimeout": "10s",
      "attempt": 1
    }
  },
  {
    "eventId": "3",
    "eventTime": "2022-03-30T05:31:30.483875Z",
    "eventType": "WorkflowTaskStarted",
    "version": "0",
    "taskId": "1048607",
    "workflowTaskStartedEventAttributes": {
      "scheduledEventId": "2",
      "identity": "MyIdentity",
      "requestId": "1ef00e16-4310-4eee-bdb3-0b261b93c72a"
    },
    "id": "3",
    "attributes": {
      "type": "workflowTaskStartedEventAttributes",
      "scheduledEventId": "2",
      "identity": "MyIdentity",
      "requestId": "1ef00e16-4310-4eee-bdb3-0b261b93c72a"
    }
  },
  {
    "eventId": "4",
    "eventTime": "2022-03-30T05:31:30.486503Z",
    "eventType": "WorkflowTaskCompleted",
    "version": "0",
    "taskId": "1048612",
    "workflowTaskCompletedEventAttributes": {
      "scheduledEventId": "2",
      "startedEventId": "3",
      "identity": "MyIdentity",
      "binaryChecksum": "ae42ab60b0d4526ee63b5719d3a2d805"
    },
    "id": "4",
    "attributes": {
      "type": "workflowTaskCompletedEventAttributes",
      "scheduledEventId": "2",
      "startedEventId": "3",
      "identity": "MyIdentity",
      "binaryChecksum": "ae42ab60b0d4526ee63b5719d3a2d805"
    }
  },
  {
    "eventId": "5",
    "eventTime": "2022-03-30T05:31:30.486628Z",
    "eventType": "WorkflowExecutionContinuedAsNew",
    "version": "0",
    "taskId": "1048613",
    "workflowExecutionContinuedAsNewEventAttributes": {
      "newExecutionRunId": "034a7ff8-49dd-421d-961d-62e497c39b88",
      "workflowType": {
        "name": "SampleChildWorkflow"
      },
      "taskQueue": {
        "name": "child-workflow-continue-as-new",
        "kind": "Normal"
      },
      "input": {
        "payloads": [
          1,
          4
        ]
      },
      "workflowRunTimeout": "0s",
      "workflowTaskTimeout": "10s",
      "workflowTaskCompletedEventId": "4",
      "backoffStartInterval": null,
      "initiator": "Unspecified",
      "failure": null,
      "lastCompletionResult": null,
      "header": {
        "fields": {}
      },
      "memo": null,
      "searchAttributes": null
    },
    "id": "5",
    "attributes": {
      "type": "workflowExecutionContinuedAsNewEventAttributes",
      "newExecutionRunId": "034a7ff8-49dd-421d-961d-62e497c39b88",
      "workflowType": {
        "name": "SampleChildWorkflow"
      },
      "taskQueue": {
        "name": "child-workflow-continue-as-new",
        "kind": "Normal"
      },
      "input": {
        "payloads": [
          1,
          4
        ]
      },
      "workflowRunTimeout": "0s",
      "workflowTaskTimeout": "10s",
      "workflowTaskCompletedEventId": "4",
      "backoffStartInterval": null,
      "initiator": "Unspecified",
      "failure": null,
      "lastCompletionResult": null,
      "header": {
        "fields": {}
      },
      "memo": null,
      "searchAttributes": null
    }
  }
]
```

</details>


---

### #2615: auto-setup does not use DB_PORT for mysql and cassandra 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2615 |
| **State** | OPEN |
| **Author** | diddowill |
| **Created** | 2022-03-16 08:21:13.000 UTC (3y 9m ago) |
| **Updated** | 2023-03-03 20:20:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | config, potential-bug |
| **Assignees** | jbreiding |
| **Milestone** | None |

#### Description

## Expected Behavior
When a customized DB_PORT is specified, for example 3307 for mysql, it would report error and stop the server:
`ERROR	Unable to create SQL database.	{"error": "dial tcp 10.168.168.221:3306: connect: connection refused", "logging-call-at": "handler.go:98"}` 

auto-setup.sh misses the DB_PORT parameter in mysql and cassandra connection:
` SCHEMA_DIR=${TEMPORAL_HOME}/schema/mysql/v57/temporal/versioned
    if [[ ${SKIP_DB_CREATE} != true ]]; then
        temporal-sql-tool --ep "${MYSQL_SEEDS}" -u "${MYSQL_USER}" "${MYSQL_CONNECT_ATTR[@]}" create --db "${DBNAME}"
    fi
    temporal-sql-tool --ep "${MYSQL_SEEDS}" -u "${MYSQL_USER}" "${MYSQL_CONNECT_ATTR[@]}" --db "${DBNAME}" setup-schema -v 0.0
    temporal-sql-tool --ep "${MYSQL_SEEDS}" -u "${MYSQL_USER}" "${MYSQL_CONNECT_ATTR[@]}" --db "${DBNAME}" update-schema -d "${SCHEMA_DIR}"

    VISIBILITY_SCHEMA_DIR=${TEMPORAL_HOME}/schema/mysql/v57/visibility/versioned
    if [[ ${SKIP_DB_CREATE} != true ]]; then
        temporal-sql-tool --ep "${MYSQL_SEEDS}" -u "${MYSQL_USER}" "${MYSQL_CONNECT_ATTR[@]}" create --db "${VISIBILITY_DBNAME}"
    fi
    temporal-sql-tool --ep "${MYSQL_SEEDS}" -u "${MYSQL_USER}" "${MYSQL_CONNECT_ATTR[@]}" --db "${VISIBILITY_DBNAME}" setup-schema -v 0.0
    temporal-sql-tool --ep "${MYSQL_SEEDS}" -u "${MYSQL_USER}" "${MYSQL_CONNECT_ATTR[@]}" --db "${VISIBILITY_DBNAME}" update-schema -d "${VISIBILITY_SCHEMA_DIR}"`


## Actual Behavior
use the specified DB_PORT and connect to database correctly 

## Specifications

  - Version: 15.2
  - Platform: Linux


#### Comments (1)

<details>
<summary><strong>jbreiding</strong> commented on 2022-04-11 15:44:30.000 UTC</summary>

Thanks for the issue, we are going to track this under a larger effort to refactor all of the different ways to configure temporal server.

</details>


---

### #2565: Consider adding an option for bidirectional gRPC streaming instead of unary RPC

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2565 |
| **State** | OPEN |
| **Author** | underrun (J Derek Wilson) |
| **Created** | 2022-03-03 05:17:40.000 UTC (3y 10m ago) |
| **Updated** | 2023-03-03 20:20:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Some traffic management solutions or connection policies may benefit from having flexibility here

**Describe the solution you'd like**
Ideally an option to use gRPC bidirectional streaming for sending / receiving multiple requests rather than using long polling with unary RPC

**Describe alternatives you've considered**
An alternative would be to shift entirely to streaming, but flexibility would be nice to have.

#### Comments (1)

<details>
<summary><strong>dnr</strong> commented on 2022-03-07 20:52:27.000 UTC</summary>

Is this mainly for worker‚Üífrontend connections? There's been some discussion about using grpc streaming for matching, but we're probably not going to pursue it in the near future.

How might traffic management/connection policies be improved with streaming? My general impression was that for traffic management, smaller shorter requests are preferable so you can load balance at a finer granularity. Of course, some things are definitely better with streaming, like connection overhead (especially authentication).

</details>


---

### #2403: import of github.com/cactus/go-statsd-client is ambiguous in go.temporal.io/server

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2403 |
| **State** | OPEN |
| **Author** | Danprot |
| **Created** | 2022-01-21 08:49:39.000 UTC (3y 11m ago) |
| **Updated** | 2023-03-03 20:20:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug |
| **Assignees** | Ardagan |
| **Milestone** | None |

#### Description

## Expected Behavior
updating packages for embedded temporal , from go.temporal.io/server v1.9.2 to  go.temporal.io/server v1.14.3
go mod tidy works fine

## Actual Behavior
go mod tidy fails
temp_test imports
        go.temporal.io/server/common/authorization imports
        go.temporal.io/server/common/metrics imports
        github.com/cactus/go-statsd-client/statsd loaded from github.com/cactus/go-statsd-client/statsd@v0.0.0-20200423205355-cb0885a1018c,
        but go 1.16 would fail to locate it:
        ambiguous import: found package github.com/cactus/go-statsd-client/statsd in multiple modules:
        github.com/cactus/go-statsd-client v3.1.1+incompatible (.../go/pkg/mod/github.com/cactus/go-statsd-client@v3.1.1+incompatible/statsd)
        github.com/cactus/go-statsd-client/statsd v0.0.0-20200423205355-cb0885a1018c (../go/pkg/mod/github.com/cactus/go-statsd-client/statsd@v0.0.0-20200423205355-cb0885a1018c)

## Steps to Reproduce the Problem

  1. use go.temporal.io/server v1.9.2 in code
  sample:
      package main
      
      import (
	      "go.temporal.io/server/common/authorization"
	      "go.temporal.io/server/common/config"
	      "log"
      )
      
      func main() {
	      cfg := config.Authorization{}
	      _, err := authorization.GetAuthorizerFromConfig(&cfg)
	      if err != nil {
		      log.Println(err)
	      }
      }
  2. go get -u go.temporal.io/server
  3. go mod tidy

## Specifications

  - Version: go version go1.17.5
  - Platform: ubuntu 20 (same on windows)


#### Comments (1)

<details>
<summary><strong>Danprot</strong> commented on 2022-01-21 08:50:13.000 UTC</summary>

The issue can be fixed, using go mod tidy -compat=1.17 , but this is undesirable

</details>


---

### #2283: Add well known indicator/label/tag to log output for temporal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2283 |
| **State** | OPEN |
| **Author** | underrun (J Derek Wilson) |
| **Created** | 2021-12-10 15:00:56.000 UTC (4 years ago) |
| **Updated** | 2023-03-03 20:21:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | Ardagan |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
When looking at mixed logs it's not always easy to clearly see what lines are from temporal services and what isn't .

**Describe the solution you'd like**
I would like to see a tag, label or other consistent indicator that a log line originated from temporal.

**Describe alternatives you've considered**
None

**Additional context**
None


#### Comments (1)

<details>
<summary><strong>sjmtan</strong> commented on 2021-12-13 23:17:59.000 UTC</summary>

Oh - one thing I just realized, this might need to be in sdk-go. We're on the cloud service, so we only consume it via the SDK.

</details>


---

### #2252: Improve error message(s) returned when failing to meet database constraints

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2252 |
| **State** | OPEN |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2021-12-02 21:54:23.000 UTC (4y 1m ago) |
| **Updated** | 2023-03-03 20:21:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Many of the error messages returned when something can't be done b/c of database schema are very opaque.

In particular, [one user ran into this message](https://temporalio.slack.com/archives/C01DKSMU94L/p1638478211328000):
```
createOrUpdateCurrentExecution failed. Failed to insert into current_executions table. Error: pq: value too long for type character varying(255)
```

Because their workflow type/name was too long. There are numerous other such failure paths that result in similarly opaque error messages.

**Describe the solution you'd like**
These messages should be actionable by users without understanding the database schema / temporal internals. The solution for this one is relatively easy to guess if you know what's going on here, but if you don't it's pretty impossible to understand.

I can see how this isn't necessarily easy to do since these messages will vary depending on the persistence backend and different backends may have slightly varying constraints, but perhaps some adapter that can interpret errors from the DB and turn them into something actionable is reasonable.

For this particular problem, ideally we'd see something like:
```
createOrUpdateCurrentExecution failed. The workflow type name is too long. The Postgres backend limits type names to 255 characters.
```

**Describe alternatives you've considered**
Validation could be done client side but this could easily drift and presents a maintainability/coupling problem.

**Additional context**
Add any other context or screenshots about the feature request here.


#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2021-12-03 17:35:32.000 UTC</summary>

dup of #2122 

</details>


---

### #2101: History server should emit cache size / capacity metrics

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2101 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2021-10-27 18:23:42.000 UTC (4y 2m ago) |
| **Updated** | 2023-03-03 20:21:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

*No description provided.*

#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2021-10-29 21:39:39.000 UTC</summary>

for mutable cache and history event cache.
current size and cache capacity.

</details>


---

### #2054: Expose disableInitialHostLookup for temporal-cassandra-tool 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2054 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2021-10-14 17:25:09.000 UTC (4y 2m ago) |
| **Updated** | 2023-03-03 20:21:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

We expose "disableInitialHostLookup " in the server config for cassandra configuration: https://github.com/temporalio/temporal/blob/master/common/config/config.go#L271

Would be great to expose a similar flag for the temporal-cassandra-tool as well

#### Comments (1)

<details>
<summary><strong>ramielrowe</strong> commented on 2021-10-14 18:43:16.000 UTC</summary>

I'm working on getting approval from my employer's legal team for the CLA, but I've got a patch to add this config and to add another to also disable the host event based auto discovery as well. That is seemingly another path to get these bad connections.

</details>


---

### #1979: Worker identity from task heartbeats is not exposed anyhow in API

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1979 |
| **State** | OPEN |
| **Author** | Spikhalskiy (Dmitry Spikhalsky) |
| **Created** | 2021-09-26 23:51:10.000 UTC (4y 3m ago) |
| **Updated** | 2023-03-03 20:21:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

# Background

`PendingActivityInfo` has a field called `lastWorkerIdentity`.
Right now this field is populated only from a worker response when it picks up the activity task when it was started.
See: https://github.com/temporalio/temporal/blob/ff069578a44dac7f39d7454771e88a989f62fe61/service/history/historyEngine.go#L1196

`RecordActivityTaskHeartbeatRequest` and `RecordActivityTaskHeartbeatByIdRequest` from heartbeating API both have `identity` field that right now is not used and exposed in `describeExecution` API even if worker didn't report an identity during actviity task pickup, but did report it later in a heartbeat.

# Proposal

Either
1. Identity reported in heartbeating should override `PendingActivityInfo#lastWorkerIdentity` (if heartbeating identity is not empty).

Or
2.  Identity reported in heartbeating should be exposed in a new `PendingActivityInfo` field.

#### Comments (1)

<details>
<summary><strong>Spikhalskiy</strong> commented on 2021-09-26 23:59:53.000 UTC</summary>

Some relevant usecases:
Worker 1 picked the task, Worker 2 heartbeats for this task.
Especially relevant with async completion of activities.

</details>


---

### #1898: Add pid file configuration option

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1898 |
| **State** | OPEN |
| **Author** | bouk (Bouke van der Bijl) |
| **Created** | 2021-09-08 09:10:26.000 UTC (4y 3m ago) |
| **Updated** | 2023-03-03 20:21:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

I'd like to run the Temporal server process as a daemon locally and it would be useful if the server could write its PID to a file.

**Describe the solution you'd like**

I imagine it working as follows:

1. Specify path to the pid file in the configuration
2. The server writes its PID on start
3. The server deletes the PID file on shutdown

**Describe alternatives you've considered**

I could also implement this myself with another tool, but it would be convenient if it was built-in.

**Additional context**

We don't use Docker for local development but rather run dependent servers as processes directly, so now I have to figure out some daemonization process myself. I'm trying to figure out if there's a way in which Temporal server could make this easier.

#### Comments (1)

<details>
<summary><strong>dnr</strong> commented on 2021-09-10 17:47:39.000 UTC</summary>

Can you use systemd or another process manager that doesn't use pid files? Pid files are a really archaic mechanism and prone to race conditions and other issues. Note that even if there's a pid file, Go binaries like the Temporal server are never going to support forking into the background like classic daemons, so you're going to want to run them under a process manager anyway.

</details>


---

### #1891: Worker heartbeat

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1891 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2021-09-05 07:51:09.000 UTC (4y 3m ago) |
| **Updated** | 2023-03-03 20:21:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

An issue that has come up a few times in the community slack is auto-activity heartbeats.
A lot of the time heartbeats are used just to verify that the activity is still running on a Worker or there might not be a simple way to heartbeat from the activity logic in a timely manner so heartbeats are implemented "externally" as follows:

```ts
async function autoheartbeat(fn) {
  async function heartbeat() {
    const cx = activity.Context.current();
    for (;;) {
      await cx.sleep(cx.info.heartbeatTimeout / 2);
      cx.heartbeat();
    }
  }
  return (...args) => Promise.race([heartbeat(), fn(...args)]);
}

export const myActivity = autoheartbeat(async (arg1, arg2) => {
  // activity logic
});
```

**Describe the solution you'd like**

Add support for Worker heartbeat so activities automatically failover to a new Worker if the current executing Worker goes down.

**Describe alternatives you've considered**

A trivial alternative is to ask users to wrap their activity function in an "autoheartbeat" wrapper as shown above or add SDK support for auto activity heartbeat.

Per-activity heartbeat is noisier than Worker heartbeat so it'd be better if we supported Worker heartbeats.


#### Comments (1)

<details>
<summary><strong>sevein</strong> commented on 2021-11-02 07:55:01.000 UTC</summary>

Relates to https://github.com/temporalio/sdk-go/issues/126.

</details>


---

### #1714: Activity dispatching optimization

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1714 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2021-07-08 22:56:48.000 UTC (4y 5m ago) |
| **Updated** | 2023-03-03 20:21:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

copying issue from cadence: https://github.com/uber/cadence/issues/2965

#### Comments (1)

<details>
<summary><strong>bergundy</strong> commented on 2022-07-18 18:30:42.000 UTC</summary>

From the original issue:
> before writing the remaining un-started activity (activity with retry policy only, since these are at least once execution) task to db, try pushing these task to matching service for sync match (e.g. wait for 1 to 2 second), if sync matching is a success, the corresponding transfer task can be omitted.

Leaving this here to be considered, the rest has already been implemented.

</details>


---

### #1468: Add ActivityTaskStarted event immediately if RetryOptions.maximumAttempts is set to 1.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1468 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2021-04-16 18:15:34.000 UTC (4y 8m ago) |
| **Updated** | 2023-03-03 20:22:01.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement |
| **Assignees** | wxing1292, feedmeapples |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
The ActivityTaskStarted event is added to the history only after an activity is completed to avoid polluting history on activity retries. This is confusing especially when the retries are turned off by setting `RetryOptions.maximumAttempts` to 1.

**Describe the solution you'd like**
Add `ActivityTaskStarted` immediately to the history if retries are disabled by setting `RetryOptions.maximumAttempts` to 1.

Consider adding `ActivityTaskStarted` immediately for the first attempt even if the activity has a retry options with non 1 maximumAttempts. In case of failure write ActivityTaskFailed only for the first attempt and then continue with the current logic. I would also add some field to ActivityTaskFailed/TimedOut that indicates that retries are going to happen.

**Describe alternatives you've considered**
Make UI only changes to surface that activity is in retry.



#### Comments (1)

<details>
<summary><strong>feedmeapples</strong> commented on 2021-05-04 03:14:48.000 UTC</summary>

Started showing Pending activities as Activity Task Started history event. In PR https://github.com/temporalio/web/pull/318

</details>


---

### #1057: On first start Temporal should not accept connections until Default namespace is setup

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1057 |
| **State** | OPEN |
| **Author** | shaunco (Shaun) |
| **Created** | 2020-12-08 11:13:03.000 UTC (5 years ago) |
| **Updated** | 2023-03-03 20:23:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug, packaging, devexp |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

## Expected Behavior
When Temporal accepts a client connection it should be in a fully operable state.

## Actual Behavior
On initial startup (using https://github.com/temporalio/temporal/blob/master/docker/docker-compose.yml), Temporal accepts connections from clients, but then fails when a worker attempts to `Run()` (via the sdk-go) with `Namespace default does not exist.`

The Temporal server log looks something like this:
```log
1  : + echo 'waiting for cassandra to start up'
2  : + sleep 1
3  : + temporal-cassandra-tool --ep cassandra validate-health
4  : 2020/12/08 10:24:33 gocql: unable to dial control conn 172.28.0.2: dial tcp 172.28.0.2:9042: connect: connection refused
5  : 2020/12/08 10:24:33 unable to establish CQL session:gocql: unable to create session: control: unable to connect to initial hosts: dial tcp 172.28.0.2:9042: connect: connection refused
6  :
7  : ...
8  :
9  : + temporal-cassandra-tool --ep cassandra validate-health
10 : + echo 'cassandra started'
11 : + '[' '' '!=' true ']'
12 : + setup_schema
13 : + '[' cassandra == mysql ']'
14 : cassandra started
15 : + '[' cassandra == postgresql ']'
16 : + echo 'setup cassandra schema'
17 : + setup_cassandra_schema
18 : setup cassandra schema
19 :
20 : ...
21 :
22 : + temporal-cassandra-tool --ep cassandra validate-health
23 : + echo 'cassandra started'
24 : + '[' '' '!=' true ']'
25 : + setup_schema
26 : + '[' cassandra == mysql ']'
27 : cassandra started
28 : + '[' cassandra == postgresql ']'
29 : + echo 'setup cassandra schema'
30 : + setup_cassandra_schema
31 : setup cassandra schema
32 :
33 : ...
34 :
35 : {"level":"info","ts":"2020-12-08T10:24:49.675Z","msg":"Created gRPC listener","service":"history","address":"172.28.0.3:7234","logging-call-at":"rpc.go:134"}
36 : ...
37 : {"level":"info","ts":"2020-12-08T10:24:49.942Z","msg":"Created gRPC listener","service":"matching","address":"172.28.0.3:7235","logging-call-at":"rpc.go:134"}
38 : ...
39 : {"level":"info","ts":"2020-12-08T10:24:50.105Z","msg":"Created gRPC listener","service":"frontend","address":"172.28.0.3:7233","logging-call-at":"rpc.go:134"}
40 : ...
41 : {"level":"info","ts":"2020-12-08T10:24:50.263Z","msg":"Created gRPC listener","service":"worker","address":"172.28.0.3:7239","logging-call-at":"rpc.go:134"}
42 :
43 : ...
44 :
45 : + echo 'Registering default namespace: default'
46 : + tctl --ns default namespace describe
47 :
48 : Registering default namespace: default
49 : {"level":"info","ts":"2020-12-08T10:24:54.470Z","msg":"temporal-sys-history-scanner-workflow workflow successfully started","service":"worker","logging-call-at":"scanner.go:196"}
50 : {"level":"info","ts":"2020-12-08T10:24:54.475Z","msg":"Get dynamic config","name":"system.advancedVisibilityWritingMode","value":"off","default-value":"off","logging-call-at":"config.go:79"}
51 : {"level":"info","ts":"2020-12-08T10:24:54.475Z","msg":"Get dynamic config","name":"history.historyVisibilityOpenMaxQPS","value":"300","default-value":"300","logging-call-at":"config.go:79"}
52 : Error: Operation DescribeNamespace failed.
53 : Error Details: rpc error: code = NotFound desc = Namespace default does not exist.
54 : ('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
55 : + echo 'Default namespace default not found. Creating...'
57 : + sleep 1
58 : Default namespace default not found. Creating...
59 : + tctl --ns default namespace register --rd 1 --desc 'Default namespace for Temporal Server'
60 : {"level":"info","ts":"2020-12-08T10:24:55.518Z","msg":"Get dynamic config","name":"history.enableAdminProtection","value":"false","default-value":"false","logging-call-at":"config.go:79"}
61 : {"level":"info","ts":"2020-12-08T10:24:55.519Z","msg":"Get dynamic config","name":"system.visibilityArchivalState","value":"enabled","default-value":"enabled","logging-call-at":"config.go:79"}
62 : {"level":"info","ts":"2020-12-08T10:24:55.552Z","msg":"Register namespace succeeded","service":"frontend","wf-namespace":"default","wf-namespace-id":"d13ba738-3b3c-4eb3-b60b-7dcbe75b275f","logging-call-at":"handler.go:278"}
63 : Namespace default successfully registered.
64 : + tctl --ns default namespace describe
65 : Name: default
66 : Id: d13ba738-3b3c-4eb3-b60b-7dcbe75b275f
67 : Description: Default namespace for Temporal Server
68 : OwnerEmail:
69 : NamespaceData: map[string]string(nil)
70 : State: Registered
71 : RetentionInDays: 24h0m0s
72 : ActiveClusterName: active
73 : Clusters: active
74 : HistoryArchivalState: Disabled
75 : VisibilityArchivalState: Disabled
76 : Bad binaries to reset:
77 : +-----------------+----------+------------+--------+
78 : | BINARY CHECKSUM | OPERATOR | START TIME | REASON |
79 : +-----------------+----------+------------+--------+
80 : +-----------------+----------+------------+--------+
81 : Default namespace registration complete.
82 : + echo 'Default namespace registration complete.'
```

From the client it looks like this:
- Lines 1-38:
   ```
   health check error: last connection error: connection error: desc = \"transport: Error while dialing dial tcp 172.28.0.3:7233: connect: connection refused
   ```
- Lines 39-80 (as can be seen from the server on lines 52/54 above):
   ```
   unable to start Temporal worker error="Namespace default does not exist."
   ```
- Lines 81+: All is good

## Steps to Reproduce the Problem

  1. Clone the https://github.com/temporalio/hello-world-project-template-go repo
  1. Since the sample code exits on first failure, we can put in some retries... Replace `worker/main.go` with:
     ```golang
     func main() {
         // Create the client object just once per process
         var err error
         c, err = client.NewClient(client.Options{})
         if err != nil {
             // Retry up to 600 times, 1 second delay each time (5 minutes)
             const maxRetries = 600
             for i := 0; i < maxRetries; i++ {
                 time.Sleep(time.Second)
                 c, err = client.NewClient(topts)
                 if err == nil {
                     break
                 }
 
                 // Only log after the first failed retry
                 log.Warn("Temporal not available, retrying...")
             }
         }

		 // If we still failed, exit
         c, err := client.NewClient(client.Options{})
         if err != nil {
             log.Fatalln("unable to create Temporal client", err)
         }
         defer c.Close()
         // This worker hosts both Worker and Activity functions
         w := worker.New(c, app.GreetingTaskQueue, worker.Options{})
         w.RegisterWorkflow(app.GreetingWorkflow)
         w.RegisterActivity(app.ComposeGreeting)
         // Start listening to the Task Queue
         err = w.Run(worker.InterruptCh())
         if err != nil {
             log.Fatalln("unable to start Worker", err)
         }
     }
     ```
  1. `go build worker`
  1. Get `curl -O -J https://raw.githubusercontent.com/temporalio/temporal/master/docker/docker-compose.yml`
  1. `docker-compose pull`
  1. At roughly the same time in two different shells:
     1. `docker-compose up`
     1. `./worker/worker`
     
  As described above, worker will eventually connect, and then fail when `w.Run` is called because the default namespace hasn't been created.
  
## Additional Notes
While I could put a similar retry loop around the `w.Run` call, I would rather fail fast if the worker is misconfigured and using the wrong namespace or something like that. For connections a retry loop makes sense, and the Temporal client's internal grpc client will automatically reestablish the connection if it fails after the initial connect, but the initial connect seems to have no such logic.

Ideally, **sdk-go** would handle retries on both the `client.NewClient` call and the `w.Run` call, but there are no retry settings in `ConnectionOptions` or `WorkerOptions`. Happy to open a separate issue in that repo, but didn't want to spam the same team.

Seems like the easiest fix, for now, is to simply not accept connections until the initial setup is 100% complete.

## Specifications

  - Version: Temporal v1.3.2, sdk-go v1.2.0
  - Platform: Ubuntu


#### Comments (1)

<details>
<summary><strong>marniks7</strong> commented on 2022-04-21 00:10:55.000 UTC</summary>

Hi, 
is there any plans to improve it?

I i am trying to run Temporal as a part of integration tests with TestContainers, so i do need namespace created and ready right away.
I use `temporalio/auto-setup:1.15.2` and `DEFAULT_NAMESPACE=mynamespace`.
Namespace is created in the end, but it was required to apply WA, otherwise i got error that it is NOT found.
It takes up to `~10s` additional seconds for namespace to be ready.

WA:
```java
public boolean isNamespaceExist(String namespace) {
  try {
      DescribeNamespaceResponse response = workfloServiceStubs.blockingStub().describeNamespace(
              DescribeNamespaceRequest.newBuilder().setNamespace(namespace).build());
     return true;
  } catch(StatusRuntimeException ex) {
       return false;
  }
}
```

```java
private static final int NAMESPACE_CHECK_SLEEP_MILLIS = 200;
private static final int NAMESPACE_CHECK_MAXIMUM_WAIT_MILLIS = 10000;
 
private void waitForNamespaceToBeReady(String temporalNamespace) {
        int millis = 0;
        while (true) {
            final boolean namespaceExist = isNamespaceExist(temporalNamespace);
            if (namespaceExist) {
                break;
            } else if (millis >= NAMESPACE_CHECK_MAXIMUM_WAIT_MILLIS) {
                throw new RuntimeException("Unable to init app cause namespace " + temporalNamespace + " is not found in temporal");
            } else {
                try {
                    millis += NAMESPACE_CHECK_SLEEP_MILLIS;
                    Thread.sleep(NAMESPACE_CHECK_SLEEP_MILLIS);
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
            }
        }
    }
```
Another similar issue about that: https://github.com/temporalio/temporal/issues/1941

</details>


---

### #746: Workflow loses cron when also using NewContinueAsNewError

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/746 |
| **State** | OPEN |
| **Author** | AAAstorga (Austin Astorga) |
| **Created** | 2020-09-21 04:41:03.000 UTC (5y 3m ago) |
| **Updated** | 2021-12-19 16:10:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | potential-bug, CRON, planning |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

## Expected Behavior

Even when returning NewContinueAsNewError in a workflow that has a CronSchedule, the CronSchedule should still execute


## Actual Behavior

After returning NewContinueAsNewError, the CronSchedule is erased from the workflow settings and thus the workflow will stop once you return something other than NewContinueAsNewError.


## Steps to Reproduce the Problem

  1. Create a workflow that returns NewContinueAsNewError
  1. Invoke workflow with CronSchedule
  1. Notice how CronSchedule is removed and will not ever run again

## Specifications

  - Version: 0.30.0
  - Platform:


#### Comments (1)

<details>
<summary><strong>eladman7</strong> commented on 2021-12-19 16:10:42.000 UTC</summary>

Hi,
was this issue solved?

</details>


---

### #266: Matching service degradation due to large number of task lists

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/266 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-03-31 00:17:54.000 UTC (5y 9m ago) |
| **Updated** | 2025-02-17 18:39:26.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | bug, operations |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

Customer reported:

> Seeing significant degradation in the matching service after being exposed to many task lists.
> And the pressure went away when the process bounced. The test was creating a task list per workflow every 20 seconds. It is more than just memory.  Some timer is operating on the DB because of them.  Calling UpdateTaskList.

#### Comments (1)

<details>
<summary><strong>peixotoleonardo</strong> commented on 2025-02-17 18:39:25.000 UTC</summary>

Is this problem still happening @yiminc? I would like to help with this.

</details>


---

### #19: Clear indication that service is up and running

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/19 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2019-11-15 01:05:59.000 UTC (6y 1m ago) |
| **Updated** | 2023-03-03 20:23:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement, docker, devexp, difficulty: easy, P2 |
| **Assignees** | None |
| **Milestone** | None |

#### Description

docker-compose up and other ways to start a service should produce a clear message into the log that indicates that service is up and running. Something like:
```
 _____                                    _  
|_   _|__ _ __ ___  _ __   ___  _ __ __ _| | 
  | |/ _ \ '_ ` _ \| '_ \ / _ \| '__/ _` | | 
  | |  __/ | | | | | |_) | (_) | | | (_| | | 
  |_|\___|_| |_| |_| .__/ \___/|_|  \__,_|_| 
                   |_|                       
 ____  _             _           _ 
/ ___|| |_ __ _ _ __| |_ ___  __| |
\___ \| __/ _` | '__| __/ _ \/ _` |
 ___) | || (_| | |  | ||  __/ (_| |
|____/ \__\__,_|_|   \__\___|\__,_|
```

#### Comments (1)

<details>
<summary><strong>shawnhathaway</strong> commented on 2020-03-25 21:10:15.000 UTC</summary>

Docker Configuration link: https://docs.docker.com/config/containers/logging/configure/

</details>


---

### #13: Multi-phase activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/13 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2019-11-12 04:09:32.000 UTC (6y 1m ago) |
| **Updated** | 2023-03-03 20:23:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 1 |
| **Priority Score** | 1 |
| **Labels** | enhancement, discussion, architecture, devexp |
| **Assignees** | None |
| **Milestone** | None |

#### Description

A lot of real life activities have multiple phases that require different timeouts. For example a human task needs to be inserted into an external system with a pretty short timeout and then be picked up by a human within another timeout. The third timeout specifies how long the activity can be worked up after it was claimed.

Current solution to the above use case is to use an activity to insert the task into the external system. And then signal workflow about each task state change. While this workaround works it significantly complicates workflow code especially having that timeouts should be enforced in the business logic through timers.

The proposal is to model an activity execution as a list of phases with each phase having its own timeout. 

There are multiple ways to achieve this. Here are two options:

1) Specify list of (phase, timeout, retryPolicy) triples when scheduling an activity. Allow retrieving the current value of the triple when retrying an activity. Add an additional API to complete a phase or augment CompleteActivityTask with additional fields to use it for phase completion.

2) Treat phases as an internal activity detail. In this case an activity can override heartbeat timeout as part of a heartbeat. This way it can store whatever information needed in the progress field and change heartbeat timeout to conform with the current phase requirements. For example in the case of human task the initial heartbeat timeout is going to be as small as needed to insert the task into the external system, then it is going to be extended to the maximum queue time, and so on.

#### Comments (1)

<details>
<summary><strong>yiminc</strong> commented on 2022-10-29 17:34:48.000 UTC</summary>

This sounds like a pretty specific use case. Adding such fundamental concept to the core component means pretty significant change and complexity to the system.

Could this be implemented as multiple activities? In your example, it would be 3 activities, with second activity be async completion type with proper timeouts. 

The second option in this proposal is interesting, which is to change heartbeat timeout of a running activity. That could be a useful feature for other use cases as well. We could create new issues to request that as a new feature and close this one.


</details>


---

### #8902: History service memory usage upward trend

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8902 |
| **State** | OPEN |
| **Author** | hafiz-qasim (Hafiz Qasim) |
| **Created** | 2025-12-24 10:03:38.000 UTC (7 days ago) |
| **Updated** | 2025-12-24 10:05:25.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi Temporal team and community,

I‚Äôm trying to understand whether it‚Äôs realistically possible to run Temporal on Kubernetes in a way that keeps the history service memory usage stable and within limits, without eventually hitting OOM kills.

Environment
	‚Ä¢	Temporal version: 1.29.1
	‚Ä¢	Persistence: PostgreSQL
	‚Ä¢	Deployment: Kubernetes
	‚Ä¢	History service replicas: 1
	‚Ä¢	Shards: tried multiple values
	‚Ä¢	Monitoring: Grafana community dashboard for Temporal server

Load model
	‚Ä¢	Execute 100 workflows per minute, evenly distributed
	‚Ä¢	After each minute of execution: 1 minute sleep
	‚Ä¢	After 15 cycles, there is a 30-minute idle period
	‚Ä¢	This pattern repeats

What I‚Äôve tried, adjusting:
	‚Ä¢	Number of shards
	‚Ä¢	History and events cache settings
	‚Ä¢	Database connection pool sizes
	‚Ä¢	Verifying via Grafana that traffic drops during idle periods

Observed behavior
	‚Ä¢	History pod memory usage continuously accumulates over time
	‚Ä¢	Memory does not stabilize or decrease, even during idle periods
	‚Ä¢	Eventually the history pod is OOM killed
	‚Ä¢	I don‚Äôt see a clear plateau in memory usage in Grafana

Main question
Has anyone successfully deployed Temporal on Kubernetes such that the history service memory remains stable over time (especially under cyclical or bursty workloads), without relying on restarts or OOM recovery?

If so:
	‚Ä¢	What configuration patterns made the difference?
	‚Ä¢	Is multiple history replicas required for memory stability?
	‚Ä¢	Are there known limitations or expected behavior around cache eviction / GC in this scenario?

I‚Äôd appreciate any guidance, references, or confirmation of whether this is expected behavior or a misconfiguration on my side.

Thanks in advance!



---

### #8901: Skip Re-executing Completed Activities on Workflow Retry (Replay-on-Retry Option)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8901 |
| **State** | OPEN |
| **Author** | Tomlord1122 (Hsiu-Chi Liu (Tomlord)) |
| **Created** | 2025-12-24 09:43:00.000 UTC (7 days ago) |
| **Updated** | 2025-12-24 09:43:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

Yes. When a Workflow fails after some Activities have completed successfully, and we want to retry the entire Workflow via Workflow-level Retry policy, Temporal re-executes all Activities from the beginning, including those that already succeeded.

  This is problematic for systems with non-reversible side effects:

1. Our Use Case: We have a workflow where: 

   - `CreateCaseActivity` successfully creates a case record in an external database 

   - `xxxActivity` fails 3 times (Activity-level retry exhausted)

   - Workflow fails and triggers Workflow-level Retry

   - Problem: Workflow Retry attempts to create the same case again ‚Üí duplicate/conflict errors

2. Why We Can't Simply Delete and Retry:

   - The external case record is not reversible (once created, it shouldn't be deleted)

   - Other teams' systems query and depend on these records

   - Deleting records to enable retry is unreliable and error-prone

3. Current Workaround (unsatisfactory):

   - Manually detect failed Workflows

   - Delete case records from external storage 

   - Wait for Workflow Retry - This is dangerous, especially when other teams are actively using the same data

**Describe the solution you'd like**


Support an option in Workflow execution to replay Event History during Workflow Retry, similar to how Temporal handles Worker failover recovery:

```go
  options := client.StartWorkflowOptions{
      ID:        "aifp-case-001",
      TaskQueue: "aifp_queue",
      RetryPolicy: &temporal.RetryPolicy{
          MaximumAttempts: 10,
          InitialInterval: 10 * time.Second,
          // New feature: Skip completed activities on retry
          ReplayOnRetry: true,  // üëà Proposed flag
      },
  }
```

  Behavior when ReplayOnRetry: true:

- Workflow Retry uses the existing Event History

- Already-completed Activities are skipped (using their historical results)

- Execution resumes from the point of last failure

- This prevents duplicate side effects in external systems



**Describe alternatives you've considered**

1. Manual Reset via SDK (`client.ResetWorkflowExecution()`)

   - Requires external monitoring and manual API calls

   - Adds operational complexity

   - Not suitable for automatic recovery scenarios

2. Delete and Retry Pattern

   - Currently, we must delete case records from external storage before retry 

   - Risk: Other teams' queries may fail during deletion window

   - Unreliable and dangerous in production

3. `ContinueAsNew` with State Persistence

   - Could save completion checkpoints and call `ContinueAsNew` on failure

   - Adds significant complexity to Workflow code

   - Doubles the size of Event History

4. Manual Idempotency + Retry

   - Make every Activity check if it's already been done before re-executin

   - Requires external state tracking (defeats the purpose of Temporal)

   - Adds unnecessary retry latency

5. Saga Pattern with Compensation 

   - We already use Saga for cleanup on failure. However, Saga compensation (deleting records) is not always safe or desired. For our use case, we'd rather skip re-execution than compensate

***Additional Context***

I‚Äôm new to Temporal and may be misinterpreting some aspects. Thank you for your patience. If there‚Äôs a recommended approach for this use case, I‚Äôd truly appreciate your guidance so I can handle it the right way.


---

### #8889: Enhancing Temporal PR Review and Release Confidence with Hikaflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8889 |
| **State** | OPEN |
| **Author** | RomirJ |
| **Created** | 2025-12-20 08:03:21.000 UTC (11 days ago) |
| **Updated** | 2025-12-20 08:03:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hi Temporal team,

I‚Äôm impressed by the power of Temporal as a scalable workflow engine for orchestrating complex long-running business processes. With workflows and activities defined in code, ensuring determinism and correct execution across retries and failure recovery is critical. As the Temporal codebase evolves‚Äîincluding the core service, SDKs, and samples‚Äîchanges to workflow execution semantics, scheduling, and state management can have far-reaching effects. Reviewing pull requests to understand which components and workflows are impacted often requires manually reading diffs and tracing through code to see how a change could affect workflow routing, history, persistence, and visibility.

I‚Äôm building Hikaflow, an AI-powered engineering assistant that integrates with GitHub to help teams manage these complexities. Hikaflow provides:

- On-demand code expertise: ask questions about Temporal‚Äôs codebase, modules, dependencies, and recent changes in scoped threads.
- Auto commit summaries: structured summaries for each commit, highlighting features, dependency updates, refactors, and infra changes.
- Issue detection: automatic scanning for security vulnerabilities, code smells, duplication, and complexity, with options to ignore or disable rules.
- Auto pull request analysis: PR dashboard with summaries, contributor insights, complexity metrics, and AI-generated comments and prompts for fixes.
- Contributor analytics: performance and quality dashboards across team members.
- Impact analysis: maps changed files to workflows, activities, persistence layers, and core services, assigning risk levels to each impacted flow and generating targeted test cases.
- Intelligent documentation: automatically generates and maintains documentation and diagrams for workflow and activity flows, APIs, and architectural components.
- Full-code audits and engineering intelligence: scans entire repositories for issues, provides metrics on review latency and churn.
- Seamless integration: runs as a GitHub app with no changes to your workflow.

I‚Äôd love to offer a two-week trial of Hikaflow on Temporal to see if these features can help streamline PR reviews, surface regressions early, and improve release confidence. Let me know if you‚Äôre interested or have specific pain points that Hikaflow could address.

‚Äî Romir


---

### #8866: Address security vulnerability CVE-2025-61729 for golang:crypto/x509

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8866 |
| **State** | OPEN |
| **Author** | roshchha (Roshni Chhabria) |
| **Created** | 2025-12-18 12:17:09.000 UTC (13 days ago) |
| **Updated** | 2025-12-18 12:17:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

We see that golang:crypto/x509 has vulnerability [CVE-2025-61729](https://nvd.nist.gov/vuln/detail/CVE-2025-61729). Please help us with a docker image for both server and admin tools by fixing these vulnerabilities.


---

### #8865: Address security vulnerability CVE-2025-61727 for golang:crypto/x509

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8865 |
| **State** | OPEN |
| **Author** | roshchha (Roshni Chhabria) |
| **Created** | 2025-12-18 12:15:27.000 UTC (13 days ago) |
| **Updated** | 2025-12-18 12:15:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

We see that golang:crypto/x509 has vulnerability [CVE-2025-61727](https://nvd.nist.gov/vuln/detail/CVE-2025-61727). Please help us with a docker image for both server and admin tools by fixing these vulnerabilities.


---

### #8864: Workflow cannot run with the dedicated cloud PostgreSQL: "Unable to query workflow due to Workflow Task in failed state."

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8864 |
| **State** | OPEN |
| **Author** | vy-kauppinen (Vy Kauppinen) |
| **Created** | 2025-12-18 09:00:33.000 UTC (13 days ago) |
| **Updated** | 2025-12-18 09:00:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Describe the bug

I'm trying to deploy Temporal with our persistence PostgreSQL on our cloud service. I managed to set up the databases and schema for Temporal. However, when I run the worker and set up a Temporal Schedule, the workflow fails, and the UI continues to return the error message: "Unable to query workflow due to Workflow Task in failed state." I'm not sure what's causing this issue. I tried switching to MySQL, and after some time, the issue also occurred.

#### values.postgresql.yaml

```yaml
server:
  config:
    persistence:
      default:
        driver: "sql"

        sql:
          driver: "postgres12"
          host: MY_HOST_IP
          port: 5432
          database: temporal
          user: temporal
          password: PASSWORD
          # for a production deployment use this instead of `password` and provision the secret beforehand e.g. with a sealed secret
          # it has a single key called `password`
          # existingSecret: temporal-default-store
          maxConns: 20
          maxIdleConns: 20
          maxConnLifetime: "1h"
          # tls:
          #  enabled: true
          #  enableHostVerification: true
          #  serverName: _HOST_ # this is strictly required when using serverless CRDB offerings
          #  caFile: /path/to/certs/<CA-file> # Here we assumed that caFile, certFile, keyFile are stored in one secret mounted as 'secret-with-certs' (see: server.additionalVolumes and server.additionalVolumeMounts sections).
          #  certFile: /path/to/certs/<client-cert-file>
          #  keyFile: /path/to/certs/<client-key-file>

      visibility:
        driver: "sql"

        sql:
          driver: "postgres12"
          host: MY_HOST_IP
          port: 5432
          database: temporal_visibility
          user: temporal
          password: MY_PASSWORD
          # for a production deployment use this instead of `password` and provision the secret beforehand e.g. with a sealed secret
          # it has a single key called `password`
          # existingSecret: temporal-visibility-store
          maxConns: 20
          maxIdleConns: 20
          maxConnLifetime: "1h"
          # tls:
          #  enabled: true
          #  enableHostVerification: true
          #  serverName: _HOST_ # this is strictly required when using serverless CRDB offerings
          #  caFile: /path/to/certs/<CA-file> # Here we assumed that caFile, certFile, keyFile are stored in one secret mounted as 'secret-with-certs' (see: server.additionalVolumes and server.additionalVolumeMounts sections).
          #  certFile: /path/to/certs/<client-cert-file>
          #  keyFile: /path/to/certs/<client-key-file>

  # additionalVolumes:
  #   - name: secret-with-certs
  #     secret:
  #       secretName: secret-with-certs
  # additionalVolumeMounts:
  #   - name: secret-with-certs
  #     mountPath: /path/to/certs/

cassandra:
  enabled: false

mysql:
  enabled: false

postgresql:
  enabled: true

prometheus:
  enabled: false

grafana:
  enabled: false
  
elasticsearch:
  enabled: false

schema:
  createDatabase:
    enabled: true
  setup:
    enabled: false
  update:
    enabled: false

```

<img width="1682" height="182" alt="Image" src="https://github.com/user-attachments/assets/9e07d378-1478-4822-8174-8e4b179b53aa" />

####Response

```json
{
  "code": 9,
  "message": "Unable to query workflow due to Workflow Task in failed state.",
  "details": [
    {
      "@type": "type.googleapis.com/temporal.api.errordetails.v1.WorkflowNotReadyFailure"
    }
  ]
}

```

### Minimal Reproduction

# Create and initialize database
# in https://github.com/temporalio/temporal git repo dir
export SQL_PLUGIN=postgres12
export SQL_HOST=postgresql_host
export SQL_PORT=5432
export SQL_USER=temporal
export SQL_PASSWORD=postgresql_password

make temporal-sql-tool

./temporal-sql-tool --database temporal create-database
SQL_DATABASE=temporal ./temporal-sql-tool setup-schema -v 0.0
SQL_DATABASE=temporal ./temporal-sql-tool update -schema-dir schema/postgresql/v12/temporal/versioned

./temporal-sql-tool --database temporal_visibility create-database
SQL_DATABASE=temporal_visibility ./temporal-sql-tool setup-schema -v 0.0
SQL_DATABASE=temporal_visibility ./temporal-sql-tool update -schema-dir schema/postgresql/v12/visibility/versioned


# I had to grant permission for temporal user for schema for each database
GRANT ALL ON SCHEMA public TO temporal;
GRANT ALL ON ALL TABLES IN SCHEMA public TO temporal;
GRANT ALL ON ALL SEQUENCES IN SCHEMA public TO temporal;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO temporal;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO temporal;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO temporal;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO temporal;


helm install -f ./temporal/values/values.postgresql.yaml temporal ./temporal --timeout 900s

kubectl --namespace=default get pods -l "app.kubernetes.io/instance=temporal"


kubectl port-forward services/temporal-frontend 7233:7233

kubectl port-forward services/temporal-web 8080:8080

### Environment/Versions

<!-- Please complete the following information where relevant. -->

- OS and processor: Linux
- Temporal Version: 1.29.1
- Are you using Docker or Kubernetes or building Temporal from source? - Kubernetes




---

### #8833: Scheduler does not list workflows runned when using "triggerImmediately"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8833 |
| **State** | OPEN |
| **Author** | AlexMog (AlexMog) |
| **Created** | 2025-12-16 16:56:49.000 UTC (15 days ago) |
| **Updated** | 2025-12-16 17:02:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
Temporal's `client.schedule.getHandle(scheduleId).describe()` should return created running workflows in `info.runningActions` even when using `triggerImmediately`

## Actual Behavior
`info.runningActions` is empty when using the option `triggerImmediately`, the workflow is correctly created, but does not seems to be linked to the scheduler.

## Steps to Reproduce the Problem
Here's a reproduction code:
  ```typescript
await client.schedule.create({
        ...,
        state: {
          triggerImmediately: true
        }
});

// Wait for the schedule to be triggered and the workflow to run
await sleep(5000);

// Test the running workflows, runningActions should not be empty, but it is!
console.log((await client.schedule.getHandle(scheduleId).describe()).info.runningActions);
```

By triggering manually, it works fine:
  ```typescript
const handler = await client.schedule.create({
        ...,
});

await handler.trigger();

// Wait for the schedule to be triggered and the workflow to run
await sleep(5000);

// This time, runningActions is populated correctly
console.log((await client.schedule.getHandle(scheduleId).describe()).info.runningActions);
```

## Specifications

  - Version: 1.9.0
  - Platform: Kubernetes



---

### #8790: history_node table keeps growing

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8790 |
| **State** | OPEN |
| **Author** | xtyinbiao (Â∞πÂΩ™) |
| **Created** | 2025-12-10 10:43:07.000 UTC (21 days ago) |
| **Updated** | 2025-12-10 10:48:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëÄ 1 |

#### Description

temporal version: 2.22.3
db: mysql

set Retention Period 3 days:
<img width="3184" height="916" alt="Image" src="https://github.com/user-attachments/assets/158a83b3-0853-489c-8b4b-93da7f6d242f" />


I see workflows total count is :
<img width="3500" height="1360" alt="Image" src="https://github.com/user-attachments/assets/fbcb1eb5-8f9c-4ee6-be5a-8d4e4459072e" />

and before 3 days just  7274Ôºå I think retention works.
<img width="3488" height="1284" alt="Image" src="https://github.com/user-attachments/assets/6ec9b286-626f-4270-8f90-651319a79462" />

but , history_node table keep growing, now has 19402165 count


---

### #8652: Documenting the current state of ScyllaDB as a Cassandra-compatible Backend

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8652 |
| **State** | OPEN |
| **Author** | andrii-kysylevskyi (Andrii Kysylevskyi) |
| **Created** | 2025-11-16 16:55:29.000 UTC (1 months ago) |
| **Updated** | 2025-11-16 16:55:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëÄ 1 |

#### Description

**Is your feature request related to a problem? Please describe.**
Temporal supports Cassandra as a storage backend, and ScyllaDB is often presented as a drop-in Cassandra alternative. It‚Äôs currently not mentioned in the documentation, and it's unclear whether using ScyllaDB with Temporal is recommended, unsupported, or simply unverified.

While looking into this, I found a few previous discussions that touched on ScyllaDB behaviour or compatibility questions:

- https://github.com/temporalio/temporal/issues/2683
- https://github.com/temporalio/temporal/issues/3511#issuecomment-1287535816
- Possibly related: https://github.com/temporalio/temporal/issues/1267

This issue is mostly to clarify the current state and consolidate conversations in one place.

**Describe the solution you'd like**
Some clarification in the docs or guidance, such as:

- ScyllaDB is expected to work, but has not been officially tested, or
- Compatibility may be revisited in the future if there is interest.

No changes requested to core code right now - just a clearer direction for users evaluating storage options.

**Describe alternatives you've considered**

- Relying on scattered Slack/GitHub discussions.
- Community blog posts or notes (easy to get outdated).

**Additional context**
Opening this issue to track the topic and give a single reference point if ScyllaDB or Temporal contributors decide to explore this further. Happy to add more details if helpful. I would also be happy to add the note about ScyllaDB support (in whichever way the team prefers)

Thanks a lot in advance!


---

### #8648: Add ExecutionStatus to visibility columns for archived executions

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8648 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2025-11-14 18:25:50.000 UTC (1 months ago) |
| **Updated** | 2025-11-14 18:25:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently none of the out of box archival providers have ExecutionStatus as visibility column and users have to resort to look at event history to find this out and cannot 
search archived workflows by execution status. 

Ask is to please add ExecutionStatus as one of the exported search attributes for archival 


---

### #8631: Trying to create search attribute that already exists should return error

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8631 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2025-11-13 14:13:11.000 UTC (1 months ago) |
| **Updated** | 2025-11-17 16:05:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

How issue was found:
Try adding same search attribute multiple times via cli - cli always gives "ok" response, for example try running this command multiple times:

          temporal operator search-attribute create --name MyCustomSA --type Keyword
          Search attributes have been added

When adding SA server has this loop where it creates aliasToFieldMap - https://github.com/temporalio/temporal/blob/4c2403736409e6f38972df3be4de480026a5b6fa/service/frontend/operator_handler.go#L279

In this loop server logs the warn message with errSearchAttributeAlreadyExistsMessage
but because it does not add anything to map it just returns nil - https://github.com/temporalio/temporal/blob/4c2403736409e6f38972df3be4de480026a5b6fa/service/frontend/operator_handler.go#L334

instead of serviceerror
this then in turn return nil to cli as response. cli does check for already exists error, but since server returns nil, error is never shown to user.

Ask is to change this code on server to return serviceError instead that tells cli/sdk/clients that they are trying to create a SA that already exists   


---

### #8579: Addressing security vulnerabilities in the Temporalio/server:1.29 1.29.1

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8579 |
| **State** | OPEN |
| **Author** | thle40 |
| **Created** | 2025-10-31 02:06:03.000 UTC (2 months ago) |
| **Updated** | 2025-11-17 09:35:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
No more CVEs found

## Actual Behavior
There are some CVEs found from the latest Temporal image:
temporalio/server:1.29

## Steps to Reproduce the Problem

> Pull the latest image temporalio/server:1.29 from Dockerhub
> Scan the image with any vulnerability scanner

CVE | SEVERITY | CVSS | PACKAGE | VERSION | FIX IN
-- | -- | -- | -- | -- | --
CVE-2025-22870, CWE-918 | HIGH | 8.8 | golang.org/x/net/http/httpproxy | v0.34.0 | 0.36.0
CVE-2023-47108, CWE-770, GHSA-8pgv-569h-w5rw | HIGH | 7.5 | go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc | v0.36.4 | 0.46.0
CVE-2024-44337, CWE-835 | MEDIUM | 6.9 | github.com/gomarkdown/markdown/parser | v0.0.0-20250311123330-531bef5e742b | N/A
CVE-2024-2689, CWE-20, GHSA-wmxc-v39r-p9wf | MEDIUM | 4.4 | go.temporal.io/server/common | v1.18.1-0.20230217005328-b313b7f58641 | 1.20.5
CVE-2025-9086 | LOW | None | curl | 8.14.1-r0 | 8.14.1-r2
CVE-2025-10148 | LOW | None | curl | 8.14.1-r0 | 8.14.1-r2
CVE-2025-4575 | LOW | None | openssl | 3.5.0-r0 | 3.5.1-r0
CVE-2025-9232 | LOW | None | openssl | 3.5.0-r0 | 3.5.4-r0
CVE-2025-9230 | LOW | None | openssl | 3.5.0-r0 | 3.5.4-r0
CVE-2025-9231 | LOW | None | openssl | 3.5.0-r0 | 3.5.4-r0


---

### #8538: Provide user identity on user-initiated events

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8538 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-10-24 15:51:34.000 UTC (2 months ago) |
| **Updated** | 2025-12-17 20:52:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

Need something like `HistoryEvent.user_identity` that is set for any events that are initiated by users (so signals and cancellation requests and such, but not timer firing). Up to implementer on details like field name and whether it's optional on top-level events or copied to many specific events. This should only be settable by server as the literal user identity which is assumed to be in context and work in a common way for OSS authorizers and cloud users. This is completely unrelated to existing `identity` (aka "client identity" or "worker identity") fields.


---

### #8097: Carryover instead of reject incoming signals upon continueAsNew

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8097 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2025-07-25 00:32:16.000 UTC (5 months ago) |
| **Updated** | 2025-07-25 00:32:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Right now we reject new incoming signals if workflow attempts to close/continueAsNew but fails due to buffered signals (or other events) to prevent an infinite loop.

This makes sense for completing/failing the workflow but not so much for continueAsNew.

**Describe the solution you'd like**
Upon continueAsNew, we can simply flush buffered signals to the new run instead of current run, and we don't have to fail & retry the workflow task

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #8074: [FR] - ResetWithSignal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8074 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2025-07-21 15:03:20.000 UTC (5 months ago) |
| **Updated** | 2025-07-21 15:03:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Request is to be able to do reset + signal which would reset execution at specific reset point
and also apply a signal to the workflow task after the one that is failed by reset command. 

Users should still be able to do reapply-signals. 

Use case would be to be able to add some extra information with reset with this signal payload.


---

### #8039: Expose activity last failure info via PollActivityTaskQueueResponse

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8039 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2025-07-14 15:49:03.000 UTC (5 months ago) |
| **Updated** | 2025-07-14 15:49:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Request is to expose last activity failure info via 
https://github.com/temporalio/api/blob/master/temporal/api/workflowservice/v1/request_response.proto#L449

Would allow users to check last failure info within their activity code and able to make decisions based on it.


---

### #8006: Is it possible to dynamially customize what labels are shown for activities in the Event History diagram?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/8006 |
| **State** | OPEN |
| **Author** | nhungpmtus |
| **Created** | 2025-07-03 20:01:54.000 UTC (6 months ago) |
| **Updated** | 2025-07-03 20:01:54.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Let's say I have a workflow with 100 calls to the same activity. I want each activity to have a custom name so that when I look at the event history diagram, I can quickly identify what call it corresponds to. Is this possible? Right now, it only displays the activity function name.


---

### #7940: Allow auth headers to be sent to HTTP on localhost

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7940 |
| **State** | OPEN |
| **Author** | mrsimonemms (Simon Emms) |
| **Created** | 2025-06-19 13:42:11.000 UTC (6 months ago) |
| **Updated** | 2025-06-19 13:42:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

When developing a codec server, it's always frustrating to have to run my localhost version through ngrok to get an HTTPS address. Many OAuth providers (eg, GitHub) have an exception that allow `localhost` to be run on HTTP.

![Image](https://github.com/user-attachments/assets/39a78b41-4417-44ac-b9d2-272f5d2bbd39)

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

Allow `http://localhost`

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

Use ngrok

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #7933: Workflow reset: add ability to set workflow run/execution timeout

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7933 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2025-06-18 17:47:27.000 UTC (6 months ago) |
| **Updated** | 2025-06-18 17:47:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Add ability to set workflow run/execution timeout when resetting a running / completed execution.
This would either override the currently set one(s) or set them if none were defined. 


---

### #7867: Golang Upgrade

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7867 |
| **State** | OPEN |
| **Author** | shivangi660 (Shivangi Sharma) |
| **Created** | 2025-06-04 12:45:53.000 UTC (7 months ago) |
| **Updated** | 2025-06-04 12:45:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Please upgrade the golang version to 1.23.5 as we have bugs reported for the open source version
Following are the CVE id's for ref
[CVE-2024-45341](https://nvd.nist.gov/vuln/detail/CVE-2024-45341), [CVE-2025-22866](https://nvd.nist.gov/vuln/detail/CVE-2025-22866), [CVE-2024-45336](https://nvd.nist.gov/vuln/detail/CVE-2024-45336), [CVE-2024-9355](https://nvd.nist.gov/vuln/detail/CVE-2024-9355)


---

### #7845: for enabling SSL in temporal engine will below properties are sufficent

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7845 |
| **State** | OPEN |
| **Author** | munagasaikrishna1234 |
| **Created** | 2025-06-01 10:36:32.000 UTC (7 months ago) |
| **Updated** | 2025-06-01 10:36:32.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

In config_template.yml

  tls:
                    enabled: {{ default .Env.SQL_TLS_ENABLED "true" }}
                    caFile: {{ default .Env.SQL_CA "" }}
                    certFile: {{ default .Env.SQL_CERT "" }}
                    keyFile: {{ default .Env.SQL_CERT_KEY "" }}
                    enableHostVerification: {{ default .Env.SQL_HOST_VERIFICATION "true" }}
                    serverName: {{ default .Env.SQL_HOST_NAME "" }}

Before both default .Env.SQL_TLS_ENABLED  and .Env.SQL_HOST_VERIFICATION are false and i changed it to true and in
docker-compose.yml
i added 
environment:
      - SSL=true
      - SQL_TLS=true
      - SQL_TLS_ENABLED=true
      - SQL_TLS_DISABLE_HOST_VERIFICATION=true


---

### #7721: Expose API version to API users

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7721 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2025-05-07 15:12:07.000 UTC (7 months ago) |
| **Updated** | 2025-05-07 15:12:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

We need a way to expose the version of https://github.com/temporalio/api currently in use to users. This is helpful for companies with proxies and in general is a good practice when using versioned API. It is especially important for us because we not only add things, but we make incompatible changes in non-GA situations.

The first step here is to research possibilities and get consensus on how to do this. There are two problems to tackle:

1) How to get the API version in code at runtime? This could be via manual const in api-go, build-flag that sets var in api-go, or maybe even obtained at runtime via `debug.ReadBuildInfo` if we don't strip debug info on build. Or maybe there are other options. Please research and get consensus on how to obtain this.

2) How to expose the API version via the API itself? This cannot be via get system info since in some environments like cloud, the same "system" can have have different API versions depending upon which namespace is in use. It could be via response header, but do we want to add a new header to _every_ response. It could be via `DescribeNamespace` but that only has partial cloud support since we encourage the cloud-ops-api namespace getter for proper namespace information. Please research and get consensus on how to expose this.


---

### #7708: Dev Mode Improvements (Insights from AWS Hackathon)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7708 |
| **State** | OPEN |
| **Author** | winmoee (Win Moe) |
| **Created** | 2025-05-03 01:35:14.000 UTC (8 months ago) |
| **Updated** | 2025-05-03 01:35:14.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I do not like restarting 3 terminals in dev mode. Either there should be an auto workflow/worker restart feature whenever a change is made kind of like npm or uv in python. Also when the workflow and workers are killed and restarted, if they are not done in order, they do not connect sometimes.

Solution Suggestion: kill switch in dev mode UI, and automatic restart of processes specific to dev mode.

**Describe alternatives you've considered**
Right now I just kill all terminals. Sometimes when they don't connect I have to restart all.

**Additional context**
Issues were from the hackathon repo https://github.com/temporal-community/temporal-ai-agent/tree/main


---

### #7576: security vulnarablity issues in 1.27.2 (latest version) as of 05 apr 2025

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7576 |
| **State** | OPEN |
| **Author** | kkcmadhu-IBM |
| **Created** | 2025-04-05 07:14:48.000 UTC (9 months ago) |
| **Updated** | 2025-04-05 07:14:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

hi there are few security vulnarablites which are critical/severe in nature and this is stopping us from rolling temporal to prod, it will be great if these can be addresed in an upcoming release.

<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:x="urn:schemas-microsoft-com:office:excel"
xmlns="http://www.w3.org/TR/REC-html40">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content="Microsoft Excel 15">

</head>

<body link="#467886" vlink="#96607D">


image | osDistro | osDistroRelease | osDistroVersion | architecture | issueType | severity | severityCHML | cvss | cve | hasFix | status | packageType | packageName | packageVersion
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
temporalio/ui:latest | alpine | 3.19.1 | 3.19.1 | amd64 | vulnerability | high | H | 7.5 | CVE-2024-6197 | Y | fixed in 8.9.0-r0 | os | curl | 8.5.0-r0
temporalio/ui:latest | alpine | 3.19.1 | 3.19.1 | amd64 | vulnerability | high | H | 7.5 | CVE-2025-30204 | N | open | go | github.com/golang-jwt/jwt | v3.2.2
temporalio/ui:latest | alpine | 3.19.1 | 3.19.1 | amd64 | vulnerability | critical | C | 9.8 | CVE-2024-24790 | Y | fixed in 1.21.11, 1.22.4 | go | net/netip | 1.22.1
temporalio/server:latest | alpine | 3.21.3 | 3.21.3 | amd64 | vulnerability | high | H | 7.5 | CVE-2025-30204 | N | open | go | github.com/golang-jwt/jwt | v3.2.2
temporalio/admin-tools:latest | alpine | 3.21.3 | 3.21.3 | amd64 | vulnerability | high | H | 0 | CVE-2024-45338 | Y | fixed in 0.33.0 | go | golang.org/x/net/html | v0.31.0
temporalio/admin-tools:latest | alpine | 3.21.3 | 3.21.3 | amd64 | vulnerability | high | H | 7.5 | CVE-2025-30204 | N | open | go | github.com/golang-jwt/jwt | v3.2.2



</body>

</html>
















---

### #7566: A workflow task following ShutdownWorker should not time out if no workers are available

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7566 |
| **State** | OPEN |
| **Author** | lina-temporal (Lina Jodoin) |
| **Created** | 2025-04-03 20:20:57.000 UTC (9 months ago) |
| **Updated** | 2025-04-03 20:20:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
When a Workflow Task (WFT) is scheduled, it will wait indefinitely (modulus workflow timeout parameters) for a worker to come online without timing itself out. This is working as expected when the workflow has no sticky worker set.

## Actual Behavior
After and despite `ShutdownWorker` being called to indicate graceful worker shutdowns, if no workers are available at the time of WFT scheduling (such as after a timer firing event), a WFT failure with the now-shutdown sticky queue may be observed. After the WFT failure, the workflow will show a WFT being scheduled on the normal queue, which will wait indefinitely for a worker to become available, as usual.

The bug here is mostly cosmetic, as if there *are* other workers available on the normal queue, they will immediately receive the pending WFT. Still, a WFT failure should not be observable in a workflow's history if all workers are shut down gracefully. 

## Steps to Reproduce the Problem

  1. Start a single worker for a target task queue.
  1. Start a workflow that creates a timer firing after >10s. 
  1. Kill the worker after the workflow's "Timer Started" event has been written (but before the timer fires event).
  1. Observe that, following the "Timer Fired" event, a failed WFT that appears to have been destined for the shutdown sticky queue will be observable in workflow history.




---

### #7487: grpc Status response to query errors contains malformed details

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7487 |
| **State** | OPEN |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2025-03-19 00:41:22.000 UTC (9 months ago) |
| **Updated** | 2025-03-19 00:41:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

I went to try to finish https://github.com/temporalio/sdk-core/issues/867 real quick only to find it's very difficult to inspect the full Failure responses in anything other than Go.

When you get back from a client query call looks like this:
```
[core/../tests/integ_tests/queries_tests.rs:273:5] &q_resp = Status {
    code: InvalidArgument,
    message: "oh no broken",
    details: b"\x08\x03\x12\x0coh no broken\x1a\\\nCtype.googleapis.com/temporal.api.errordetails.v1.QueryFailedFailure\x12\x15\n\x13\n\x0coh no broken\x12\x03src",
    metadata: MetadataMap {
        headers: {
            "content-type": "application/grpc",
        },
    },
    source: None,
}
```
There are a few issues here:
https://github.com/temporalio/api/blob/master/temporal/api/workflowservice/v1/service.proto#L551 's comment should say something about how the encoded failure is packed into the Status details field, which ostensibly, should be that it follows the guidance here: https://google.aip.dev/193#statusdetails and attaches some an `Any` containing an `ErrorInfo` followed by an `Any` Wrapping a `QueryFailedFailure`
AFAICT it's not doing that, though. I can't actually tell how this binary string is constructed. Luckily no SDK is using this yet so we can fix it. If it's not following the guidance (maybe it is and somehow I'm just not unpacking it right, but I don't think so), we should adhere to that. 

From my reading of the binary it looks like it's just directly serializing a https://github.com/temporalio/api-go/blob/master/serviceerror/query_failed.go#L35 somehow, which maybe is OK for api-go users, but isn't for anyone else.


---

### #7481: Addressing a lot of security vulnerabilities in the Temporalio/admin-tools release temporalio/admin-tools:1.27.1

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7481 |
| **State** | OPEN |
| **Author** | thle40 |
| **Created** | 2025-03-18 08:09:33.000 UTC (9 months ago) |
| **Updated** | 2025-03-18 08:09:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
No CVE found

## Actual Behavior
There are several CVEs found from the latest Temporal image:
temporalio/admin-tools:1.27.1-tctl-1.18.2-cli-1.3.0 

## Steps to Reproduce the Problem

> Scan results for: image axonhub.azurecr.us/temporalio/admin-tools:1.27.1-tctl-1.18.2-cli-1.3.0 sha256:8ee113eb55b60642680baaf268e590955f05925f2232abb722ec43e11aee4ed0

  Vulnerabilities
```|      CVE       | SEVERITY | CVSS |        PACKAGE        |                VERSION                |             STATUS              | PUBLISHED  | DISCOVERED |                    DESCRIPTION                     |
+----------------+----------+------+-----------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-45338 | high     | 0.00 | golang.org/x/net/html | v0.31.0                               | fixed in 0.33.0                 | 89 days    | < 1 hour   | An attacker can craft an input to the Parse        |
|                |          |      |                       |                                       | 89 days ago                     |            |            | functions that would be processed non-linearly     |
|                |          |      |                       |                                       |                                 |            |            | with respect to its length, resulting in extremely |
|                |          |      |                       |                                       |                                 |            |            | slow par...                                        |
+----------------+----------+------+-----------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-2689  | medium   | 4.40 | go.temporal.io/server | v1.18.1-0.20230217005328-b313b7f58641 | fixed in 1.20.5, 1.21.6, 1.22.7 | > 9 months | < 1 hour   | Denial of Service in Temporal Server prior to      |
|                |          |      |                       |                                       | > 9 months ago                  |            |            | version 1.20.5, 1.21.6, and 1.22.7 allows an       |
|                |          |      |                       |                                       |                                 |            |            | authenticated user who has permissions to interact |
|                |          |      |                       |                                       |                                 |            |            | with wor...                                        |
+----------------+----------+------+-----------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2023-3485  | low      | 3.00 | go.temporal.io/server | v1.18.1-0.20230217005328-b313b7f58641 | fixed in 1.20.0                 | > 6 months | < 1 hour   | Insecure defaults in open-source Temporal Server   |
|                |          |      |                       |                                       | > 1 years ago                   |            |            | before version 1.20 on all platforms allows an     |
|                |          |      |                       |                                       |                                 |            |            | attacker to craft a task token with access to a    |
|                |          |      |                       |                                       |                                 |            |            | namesp...                                          |
+----------------+----------+------+-----------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2025-21490 | low      | 0.00 | mariadb               | 11.4.4-r1                             | fixed in 11.4.5-r0              | 55 days    | < 1 hour   | Vulnerability in the MySQL Server product of       |
|                |          |      |                       |                                       | 21 days ago                     |            |            | Oracle MySQL (component: InnoDB).  Supported       |
|                |          |      |                       |                                       |                                 |            |            | versions that are affected are 8.0.40 and prior,   |
|                |          |      |                       |                                       |                                 |            |            | 8.4.3 and p...                                     |
+----------------+----------+------+-----------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2025-1094  | low      | 0.00 | postgresql17          | 17.2-r0                               | fixed in 17.4-r0                | 32 days    | < 1 hour   | Improper neutralization of quoting syntax in       |
|                |          |      |                       |                                       | 24 days ago                     |            |            | PostgreSQL libpq functions PQescapeLiteral(),      |
|                |          |      |                       |                                       |                                 |            |            | PQescapeIdentifier(), PQescapeString(), and        |
|                |          |      |                       |                                       |                                 |            |            | PQescapeStringC...                                 |
+----------------+----------+------+-----------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2024-8176  | low      | 0.00 | expat                 | 2.6.4-r0                              | fixed in 2.7.0-r0               | 3 days     | < 1 hour   | A stack overflow vulnerability exists in the       |
|                |          |      |                       |                                       | 1 days ago                      |            |            | libexpat library due to the way it handles         |
|                |          |      |                       |                                       |                                 |            |            | recursive entity expansion in XML documents. When  |
|                |          |      |                       |                                       |                                 |            |            | parsing an X...                                    |
+----------------+----------+------+-----------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+
| CVE-2025-22870  | high    | 8.80 | golang.org/x/net/http/httpproxy | v0.35.0                     | fixed in 0.36.0                 | 3 days     | < 1 hour   |        |
+----------------+----------+------+-----------------------+---------------------------------------+---------------------------------+------------+------------+----------------------------------------------------+

```



---

### #7164: Confirm aborted update retries schedule responsible for robustness of update to CAN

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7164 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2025-01-27 11:31:26.000 UTC (11 months ago) |
| **Updated** | 2025-01-28 23:32:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

[temporalio/features:features/continue_as_new/updates_do_not_block_continue_as_new](https://github.com/temporalio/features/blob/main/features/continue_as_new/updates_do_not_block_continue_as_new) is a feature test that confirms that, when an update is admitted while a WFT is in-flight, and that WFT CANs, that the update lands on the next run.

This behavior relies on the update being aborted, and retried (our intention is that these retries are done by gRPC interceptors and/or history client in the Frontend service). However, currently, the error responses are being returned to the SDK caller; the desired behavior occurs because the SDK client retries the `ResourceExhaustedFailure`. So:

- We need to make the server retry this internally (at least, that was out original intention)
- I think that we want to confirm that retries will be spread out in time such that races with the CAN handling are very unlikely.


---

### #7048: Handle Permission Errors in getConfigFiles Function

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7048 |
| **State** | OPEN |
| **Author** | os-a2 |
| **Created** | 2025-01-06 05:07:45.000 UTC (11 months ago) |
| **Updated** | 2025-01-06 05:07:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

The getConfigFiles function doesn't handle permission errors properly. Right now, if it can't read a file because of permission issues, it just skips it without letting us know.

https://github.com/temporalio/temporal/blob/2d8afd44c413a25c53f5fe8eecff3cbbbb782fba/common/config/loader.go#L135C1-L140C3


---

### #7011: Support Markdown in Result panel

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/7011 |
| **State** | OPEN |
| **Author** | semics-tech (SEMICS) |
| **Created** | 2024-12-19 08:12:13.000 UTC (1 years ago) |
| **Updated** | 2024-12-19 08:12:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Results are not as good as they could be.

**Describe the solution you'd like**
It would be really good if you could output results as markdown. It will mean that at a glance you could see a report on what went on in the workflow.

**Describe alternatives you've considered**
Just outputting markdown in the raw text and copy and pasting into a markdown converter.

**Additional context**
I think this would be simple to implement and be a very nice feature


---

### #6961: Provide already-existing run ID on StartChildWorkflowExecutionFailedEventAttributes if it failed for "already exists"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6961 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-12-09 20:16:45.000 UTC (1 years ago) |
| **Updated** | 2024-12-09 20:16:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

Need the already-exists run ID on `StartChildWorkflowExecutionFailedEventAttributes` when the child start fails because it already exists. This already is present on `WorkflowExecutionAlreadyStartedFailure` for clients. No opinion on what to call the field, can just be `existing_run_id` so as not to confuse people into thinking it's the actual child's run ID (which would be impossible since it failed to start).


---

### #6954: IsTerminatedByResetter should not be used for checking if child workflow should report to parent

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6954 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2024-12-07 02:01:03.000 UTC (1 years ago) |
| **Updated** | 2024-12-07 02:01:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
- A reset operation involves three runs: base run, current run and reset (new) run. If current run is running when processing a reset, it will be terminated. If current run is also a child workflow, then there's a question if it should report to its parent after the termination. 
- If current run is base run, then we should not report to parent, as we will have a new run for the child.
- If current run is not base run, then we should report to parent as the new run is not a continuation of the current run. 

## Actual Behavior
- In the `replyToParentWorkflow` [logic](https://github.com/temporalio/temporal/blob/a4849620e82d25ae0499164c58f72204fe7a8e7d/service/history/transfer_queue_active_task_executor.go#L355), we are checking if the current run is terminated by reset or not, which is not checking if current run is the base run.

We should use the new `ResetRunId` field in WorkflowExecutionInfo, which is only set when the workflow is the base run of a reset. However, note that with today's event-based replication stack, this ResetRunID field won't be replicated upon reset. So if we switch to that field now, we will temporary make things worse (if failover happens before the closeExecution task is processed) until state-based replication goes live. 

To fix the issue before state-based replication, we can:
1. Change existing replication logic and instead of generating a historyReplicationTask when workflow is closed, generate a SyncWorkflowState task (yes we have this sync state task in event-based world as well, but only works for closed workflow)
2. Add some new fields to the Terminated event to indicate if the workflow is base run.


## Steps to Reproduce the Problem

  1.
  1.
  1.

## Specifications

  - Version:
  - Platform:



---

### #6952: Extend activity retry expiration time upon reset

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6952 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2024-12-07 01:38:10.000 UTC (1 years ago) |
| **Updated** | 2024-12-07 01:38:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
- Activity retry expiration time should be re-calculated based on the reset time

## Actual Behavior
- Activity retry expiration time is based calculated using the base workflow's activityScheduleEvent time.
- Then activity scheduled time is overwritten to the reset time, causing activity scheduled time to be later than retry expiration time. https://github.com/temporalio/temporal/blob/main/service/history/ndc/workflow_resetter.go#L562


## Steps to Reproduce the Problem

  1.
  1.
  1.

## Specifications

  - Version:
  - Platform:



---

### #6924: Support custom search attributes of type Duration

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6924 |
| **State** | OPEN |
| **Author** | darshan-au (Darshan Pandhi) |
| **Created** | 2024-12-03 20:48:54.000 UTC (1 years ago) |
| **Updated** | 2024-12-03 20:48:54.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Yes, I'm trying to add a custom search attribute that capture duration between a collection of steps in the workflow execution. These are the subset of steps that define a business usecase in my workflow execution and hence being able to filter on workflows that exceeded a threshold is important. I'm trying to create a custom search attribute that behaves similar to the default ExecutionDuration filter. Currently temporal supports creating custom search attributes of type `Int` or `Datetime`. Neither of them support my usecase of being able to apply a filter using duration strings.

**Describe the solution you'd like**
Add support for creating custom search attributes of type `Duration` that allow users to filter using duration strings like `1m`, `1h` or `1d` etc.

**Describe alternatives you've considered**
Currently I'm limited to creating a custom search attribute of the type `Int` and using numeric filters. Using `Int` as the type here is limiting users from applying the correct filters.
For example: If UserA creates a custom search attribute of type Int named `myCustomDuration` and stores milliseconds in it. UserB won't know that they have to provide milliseconds as values for applying the numeric filter. 

Being able to use search attributes of type `Duration` would be very useful in such cases.



---

### #6896: Include command info when command fails validation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6896 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2024-11-27 14:29:08.000 UTC (1y 1m ago) |
| **Updated** | 2024-11-27 14:43:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently when handling schedule activity task command from workflow task completion
we check payload size limit, but do not log the command info (activity type would be great) as part of the error info, for example:

           return nil, nil, handler.terminateWorkflow(enumspb.WORKFLOW_TASK_FAILED_CAUSE_BAD_SCHEDULE_ACTIVITY_ATTRIBUTES, err)

here err does not include any info about the offending activity, would be great to have this info if possible as would make debugging easier


---

### #6844: Temporal 1.24.1 <- 1.23.1 upgrade error

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6844 |
| **State** | OPEN |
| **Author** | harish4459 |
| **Created** | 2024-11-20 07:00:15.000 UTC (1y 1m ago) |
| **Updated** | 2024-11-20 07:01:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

Application should work normally after upgrading

## Actual Behavior

Application is crashing after upgrading to version 1.24.1


## Steps to Reproduce the Problem

  postgreSQL Schema upgrade to PostgreSQL schema v1.12 from v1.11  is successfully done.
  Application upgrade to version 1.24.1 from older version was successful .

 Our setup is we are using single db schema (core) and not using visibility schema but there are few tables from visibility schema in our core db.
 As per logs error seems to be it is hitting visibility schema tables but not sure what is causing this ?

## Specifications

  - Version: 1.24.1 with postgresSQL 
  - Platform: AWS EKS version 1.29

![image](https://github.com/user-attachments/assets/267808ed-11e2-408d-bd48-2cb704407acc)


temporal-frontend logs-

`{"level":"error","ts":"2024-11-20T06:57:21.362Z","msg":"service failures","operation":"CountWorkflowExecutions","wf-namespace":"default","error":"CountWorkflowExecutions operation failed. Query failed: pq: column \"temporalnamespacedivision\" does not exist","logging-call-at":"telemetry.go:411","stacktrace ‚îÇ
‚îÇ ":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/log/zap_logger.go:156\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/te ‚îÇ
‚îÇ lemetry.go:411\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/telemetry.go:202\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/ ‚îÇ
‚îÇ server.go:1186\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation.func2\n\t/home/runner/work/docker-builds/docker-builds/temporal/service/frontend/redirection_interceptor.go:245\ngo.temporal.io/server/service/frontend.(*NoopRedirectionPolicy).WithNamespaceRedirect ‚îÇ
‚îÇ \n\t/home/runner/work/docker-builds/docker-builds/temporal/service/frontend/dc_redirection_policy.go:125\ngo.temporal.io/server/service/frontend.(*RedirectionInterceptor).handleRedirectAPIInvocation\n\t/home/runner/work/docker-builds/docker-builds/temporal/service/frontend/redirection_interceptor.go:242\ng ‚îÇ
‚îÇ o.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/service/frontend/redirection_interceptor.go:202\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo ‚îÇ
‚îÇ .temporal.io/server/common/authorization.(*Interceptor).Intercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/authorization/interceptor.go:181\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/ser ‚îÇ
‚îÇ ver/service/frontend.GrpcServerOptionsProvider.NewServerMetricsContextInjectorInterceptor.func2\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo ‚îÇ
‚îÇ .opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/home/runner/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.51.0/interceptor.go:315\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/pkg/ ‚îÇ
‚îÇ mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/namespace_logger.go:85\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/runner/go/ ‚îÇ
‚îÇ pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/interceptor/namespace_validator.go:135\ngoogle.golang.org/grpc.getChainUnaryHa ‚îÇ
‚îÇ ndler.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/common/utf8validator.(*Validator).Intercept\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/utf8validator/validate.go:182\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/home/ ‚îÇ
‚îÇ runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1186\ngo.temporal.io/server/service/frontend.GrpcServerOptionsProvider.NewServiceErrorInterceptor.func1\n\t/home/runner/work/docker-builds/docker-builds/temporal/common/rpc/grpc.go:178\ngoogle.golang.org/grpc.NewServer.chainUnaryServerInterceptors. ‚îÇ
‚îÇ chainUnaryInterceptors.func1\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1177\ngo.temporal.io/api/workflowservice/v1._WorkflowService_CountWorkflowExecutions_Handler\n\t/home/runner/go/pkg/mod/go.temporal.io/api@v1.32.1/workflowservice/v1/service_grpc.pb.go:2217\ngoogle.golang.org/g ‚îÇ
‚îÇ rpc.(*Server).processUnaryRPC\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1369\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/home/runner/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1780\ngoogle.golang.org/grpc.(*Server).serveStreams.func2.1\n\t/home/runner/go/pkg/mod/go ‚îÇ
‚îÇ ogle.golang.org/grpc@v1.63.2/server.go:1019"}`





---

### #6843: Add small jitter timeout to the long poll empty response 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6843 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2024-11-19 21:35:24.000 UTC (1y 1m ago) |
| **Updated** | 2024-11-19 21:35:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

When matching service restarts, it kills all long poll connections, and SDK would retry them, the new polling request would come almost at the same time, and they would timeout at 60s if there is no tasks. This would result in long poll request rate oscillating at interval of 60s. We could add some short jitter to the long poll timeout to make the requests smooth out faster.


---

### #6800: Support activity task queue `max_tasks_per_second` even when `request_eager_execution` is set

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6800 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-11-12 18:47:34.000 UTC (1y 1m ago) |
| **Updated** | 2024-11-12 18:47:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

Today, if `TaskQueueMetadata.max_tasks_per_second` is set for an activity task queue, it is ignored if `ScheduleActivityTaskCommandAttributes.request_eager_execution` is true. SDKs have to work around this limitation and/or inform customers, especially since SDKs try to set `request_eager_execution` as true by default.

We should support both of these working together. In the meantime, SDKs will continue to educate users on their incompatibility.


---

### #6760: Include in-memory (sync match) backlog in `ApproximateBacklogCount`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6760 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2024-11-05 23:57:27.000 UTC (1y 1m ago) |
| **Updated** | 2024-11-05 23:57:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

There are some tasks that will never go to the durable backlog, such as queries and Nexus tasks.
In those cases, and in cases where there's a high sync match rate, it's useful to know how many tasks are in the sync match backlog in order to autoscale workers.



---

### #6695: disable XDC cache when not using multi-cluster replication

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6695 |
| **State** | OPEN |
| **Author** | jasonmchan (Jason Chan) |
| **Created** | 2024-10-22 20:35:35.000 UTC (1y 2m ago) |
| **Updated** | 2024-10-22 20:35:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
As far as I understand the XDC cache introduced by https://github.com/temporalio/temporal/pull/4379 is only pertinent when using multi-cluster replication.

This is frustrating because like the other caches in the history service, entries aren't evicted according to their TTL, so the cache keeps growing. If operators don't tune the XDC cache size limit (and it's not apparent that they should, especially if they're not using multi-cluster replication) the history service will OOM.

**Describe the solution you'd like**
Minimize XDC cache overheads when not using multi-cluster replication

**Describe alternatives you've considered**
N/A

**Additional context**
Ran a workload of executing a few 1000 simple workflows and the XDC cache accumulates in size (see pprof screenshot):
![image](https://github.com/user-attachments/assets/3dae4c00-5113-4526-8e8a-f44286644dab)




---

### #6680: Coordinate CI step timeout-minutes with `go test -timeout` flag

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6680 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2024-10-21 13:26:03.000 UTC (1y 2m ago) |
| **Updated** | 2024-10-21 13:26:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

_Originally posted by @dnr in https://github.com/temporalio/temporal/pull/6668#discussion_r1807078651_
            


---

### #6618: Cassandra 5 support

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6618 |
| **State** | OPEN |
| **Author** | credmond-git (Colin Redmond) |
| **Created** | 2024-10-07 21:35:04.000 UTC (1y 2m ago) |
| **Updated** | 2024-10-07 21:35:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Cassandra 5 is now GA https://cassandra.apache.org/_/blog/Apache-Cassandra-5.0-Announcement.html

It would be great to use the latest version of Cassandra 5 with Temporal. 

Thanks!



---

### #6617: hybrid architecture for Temporal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6617 |
| **State** | OPEN |
| **Author** | mohankurali (mohan) |
| **Created** | 2024-10-07 19:53:15.000 UTC (1y 2m ago) |
| **Updated** | 2024-10-07 19:53:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, temporal design focuses on ensuring strong guarantees for workflow execution, which requires each event or activity result to be written immediately to the persistence store. For this reason, Temporal does not batch activities for bulk writes to Cassandra. To handle this, one needs to go for bigger persistence store cluster adding to the cost.

**Describe the solution you'd like**

1. Redis for Real-Time Workflow Execution State:

Purpose: Redis can be used as a cache for workflow execution state and hot data (i.e., frequently accessed or recently updated data). Since Redis is an in-memory database, it provides low-latency reads and writes, making it ideal for workflows where immediate responsiveness is required.

Use Case: Whenever a workflow is being executed, its current state, progress, and results of activities can be stored in Redis. Redis acts as a working memory, keeping active workflow states in memory for fast access during execution.

Persistence in Redis: Redis persistence methods such as RDB snapshots or AOF can be used to periodically back up data to disk, but Redis itself would not be relied upon for full persistence.

Eviction Policy: Since Redis is memory-constrained, an eviction policy can be applied where old or less frequently used workflow data is offloaded to Cassandra after a certain threshold or TTL (time-to-live) period.

2. Kafka for Event Streaming and Replay:

Purpose: Kafka can be used to stream all workflow events and activity results for asynchronous processing and durable backup. If Redis crashes or the system needs to replay workflow histories, Kafka can be used to recover or reconstruct the exact event sequence.

Use Case: Every change in workflow state (activity completion, task scheduling, etc.) is also streamed to Kafka as an event log. Kafka topics can be partitioned based on workflows or workflow shards to allow parallel processing.

Replay Capability: Kafka acts as a reliable event log for replaying events in case Redis or Cassandra fails. If Redis loses the in-memory state, Temporal can replay workflow events from Kafka to restore the most recent workflow state in Redis.

Fault Tolerance: Kafka provides durable message storage and eventual consistency, ensuring that workflow states can be reconstructed by replaying the Kafka log, even after failures.

3. Cassandra for Long-Term, Durable Storage:

Purpose: Cassandra can be used as the durable, long-term persistence store for workflow history and state. This is critical for fault tolerance, event sourcing, and ensuring that even workflows that last for long periods can be recovered.

Use Case: While Redis holds the real-time working state, Cassandra is responsible for storing the entire history of workflow execution, including every event that happened during the workflow lifecycle.

Batch Writes: Cassandra can handle batch writes of workflow events and state transitions that are periodically written from Redis and Kafka. Redis acts as the in-memory cache, while Cassandra ensures that once a workflow completes or reaches certain checkpoints, its state is persisted for long-term storage.

Scalability: Cassandra provides horizontal scalability, allowing it to store huge amounts of workflow data, making it suitable for storing the event history and workflow results after Redis evicts data or when workflows are completed.





---

### #6600: Standardized Documentation Format a la Swagger

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6600 |
| **State** | OPEN |
| **Author** | alexdavis24 (Alex Davis) |
| **Created** | 2024-10-04 13:59:16.000 UTC (1y 2m ago) |
| **Updated** | 2024-10-04 13:59:27.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Coming from a a world of Swagger/OpenAPI, there is a clearly defined documentation template around exposing an API. We would like to expose Temporal Workflows as part of a larger service, where a Temporal Workflow plays the role of a "black box" service call, and accordingly the front-end dev should know only what input parameters/return values are expected for a particular workflow, with the rest of the workflow abstracted away.

Swagger/OpenAPI has a clearly defined structure to share between end users:
[https://swagger.io/docs/specification/v3_0/basic-structure/](url)

**Describe the solution you'd like**
Is there a possibility of Temporal 
1. defining a consistent structure to use across projects and 
2. Creating tooling that can read in the consistent structure to generate a human-friendly documentation page, etc.?


**Describe alternatives you've considered**
Custom yaml files that are shared across teams - this requires teams to agree on their own internal standards and makes reuse of documentation across projects difficult

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #6585: Record a history event indicating that a Nexus operation request was delivered to the handler

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6585 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2024-10-01 21:45:33.000 UTC (1y 3m ago) |
| **Updated** | 2024-10-01 21:45:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Required to support the WAIT_CANCELLATION_REQUESTED CancellationType in the SDKs (see the equivalent functionality for child workflows here: https://javadoc.io/doc/io.temporal/temporal-sdk/latest/io/temporal/workflow/ChildWorkflowCancellationType.html).


---

### #6555: Add `EdDSA` support to default JWT ClaimMapper

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6555 |
| **State** | OPEN |
| **Author** | mxk (Maxim Khitrov) |
| **Created** | 2024-09-25 15:39:55.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-25 15:39:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
We would like to use `EdDSA` JWTs with the default ClaimMapper, but it currently only supports [`RS256` and `ES256`](https://github.com/temporalio/temporal/blob/8ab9531593670f2dabac4edce49100bd17439b07/common/authorization/default_token_key_provider.go#L115) tokens.

**Describe the solution you'd like**
Add support for `EdDSA` using `Ed25519` curve.



---

### #6542: temporal-history panic

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6542 |
| **State** | OPEN |
| **Author** | roytamirzs (Roy Tamir) |
| **Created** | 2024-09-23 10:38:30.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-23 10:39:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
temporal-history pod should not panic and exit.

## Actual Behavior

```
"2024-09-23T09:07:36.195Z, "Fail to process task""2024-09-23T09:07:36.195Z, "Critical error processing task, retrying."
"2024-09-23T09:08:19.217Z, "panic: runtime error: slice bounds out of range [-30:]"
"2024-09-23T09:08:19.217Z, "goroutine 150888820 [running]:"
"2024-09-23T09:08:19.217Z, "go.temporal.io/api/common/v1.(*WorkflowExecution).MarshalToSizedBuffer(0x403a217260, {0x403f2be000, 0x47, 0x2fa?})"
"2024-09-23T09:08:19.217Z, "/go/pkg/mod/go.temporal.io/api@v1.24.0/common/v1/message.pb.go:1712 +0x20c"
"2024-09-23T09:08:19.217Z, "go.temporal.io/api/workflow/v1.(*WorkflowExecutionInfo).MarshalToSizedBuffer(0x40163aea20, {0x403f2be000, 0x2ca, 0x2fa})"
"2024-09-23T09:08:19.217Z, "/go/pkg/mod/go.temporal.io/api@v1.24.0/workflow/v1/message.pb.go:1761 +0xaf0"
"2024-09-23T09:08:19.217Z, "go.temporal.io/server/api/historyservice/v1.(*DescribeWorkflowExecutionResponse).MarshalToSizedBuffer(0x403acb79f0, {0x403f2be000, 0x2fa, 0x2fa})"
"2024-09-23T09:08:19.218Z, "/home/builder/temporal/api/historyservice/v1/request_response.pb.go:13966 +0x134"
"2024-09-23T09:08:19.218Z, "go.temporal.io/server/api/historyservice/v1.(*DescribeWorkflowExecutionResponse).Marshal(0x401d86f630?)"
"2024-09-23T09:08:19.218Z, "/home/builder/temporal/api/historyservice/v1/request_response.pb.go:13907 +0x50"
"2024-09-23T09:08:19.218Z, "google.golang.org/protobuf/internal/impl.legacyMarshal({{}, {0x26d7548, 0x401d86f630}, {0x0, 0x0, 0x0}, 0x0})"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/google.golang.org/protobuf@v1.31.0/internal/impl/legacy_message.go:402 +0x84"
"2024-09-23T09:08:19.218Z, "google.golang.org/protobuf/proto.MarshalOptions.marshal({{}, 0xc8?, 0x0, 0x0}, {0x0, 0x0, 0x0}, {0x26d7548, 0x401d86f630})"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/google.golang.org/protobuf@v1.31.0/proto/encode.go:166 +0x1b8"
"2024-09-23T09:08:19.218Z, "google.golang.org/protobuf/proto.MarshalOptions.MarshalAppend({{}, 0xa0?, 0x17?, 0x1?}, {0x0, 0x0, 0x0}, {0x26a7ac0?, 0x401d86f630?})"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/google.golang.org/protobuf@v1.31.0/proto/encode.go:125 +0x70"
"2024-09-23T09:08:19.218Z, "github.com/golang/protobuf/proto.marshalAppend({0x0, 0x0, 0x0}, {0xffff781c1fc8?, 0x403acb79f0?}, 0x0?)"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/github.com/golang/protobuf@v1.5.3/proto/wire.go:40 +0x78"
"2024-09-23T09:08:19.218Z, "github.com/golang/protobuf/proto.Marshal(...)"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/github.com/golang/protobuf@v1.5.3/proto/wire.go:23"
"2024-09-23T09:08:19.218Z, "google.golang.org/grpc/encoding/proto.codec.Marshal({}, {0x20117a0, 0x403acb79f0})"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/google.golang.org/grpc@v1.57.0/encoding/proto/proto.go:45 +0x54"
"2024-09-23T09:08:19.218Z, "google.golang.org/grpc.encode({0xffff78ca0ba8?, 0x3b1ec18?}, {0x20117a0?, 0x403acb79f0?})"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/google.golang.org/grpc@v1.57.0/rpc_util.go:633 +0x40"
"2024-09-23T09:08:19.218Z, "google.golang.org/grpc.(*Server).sendResponse(0x4000a741e0, {0x26cf060, 0x40092c1ba0}, 0x402ce2bb00, {0x20117a0?, 0x403acb79f0}, {0x0, 0x0}, 0xffffffffffffffff?, {0x0, ...})"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1123 +0x160"
"2024-09-23T09:08:19.218Z, "google.golang.org/grpc.(*Server).processUnaryRPC(0x4000a741e0, {0x26cf060, 0x40092c1ba0}, 0x402ce2bb00, 0x4000be1890, 0x3abd138, 0x0)"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1407 +0xd44"
"2024-09-23T09:08:19.218Z, "google.golang.org/grpc.(*Server).handleStream(0x4000a741e0, {0x26cf060, 0x40092c1ba0}, 0x402ce2bb00, 0x0)"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:1737 +0x80c"
"2024-09-23T09:08:19.218Z, "google.golang.org/grpc.(*Server).serveStreams.func1.1()"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:982 +0x84"
"2024-09-23T09:08:19.218Z, "created by google.golang.org/grpc.(*Server).serveStreams.func1"
"2024-09-23T09:08:19.218Z, "/go/pkg/mod/google.golang.org/grpc@v1.57.0/server.go:980 +0x164"
```

## Steps to Reproduce the Problem
n/a

## Specifications

  - Version: 1.20
  - Platform: eks v1.29.8-eks-a737599, Amazon Linux 2(arm64)
 
Issue might be related to https://github.com/temporalio/temporal/issues/4159
 



---

### #6541: getAnyClient with cachingRedirector problem

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6541 |
| **State** | OPEN |
| **Author** | ndtretyak (Nikolay Tretyak) |
| **Created** | 2024-09-22 08:59:59.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-22 08:59:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
`ListQueues` (and other methods using `getAnyClient`) handles history hosts failures.

## Actual Behavior

`cachingRedirector` handles history failures in its `redirectLoop` and removes failed hosts from the cache
https://github.com/temporalio/temporal/blob/86817057833b6c44d47c8da6920b7af783348355/client/history/caching_redirector.go#L110-L113

However, `ListQueues` does not use `redirectLoop` and calls corresponding client's method directly.
https://github.com/temporalio/temporal/blob/86817057833b6c44d47c8da6920b7af783348355/client/history/client.go#L298-L308

That causes a broken host not to be removed from the cache.

## Steps to Reproduce the Problem

  1. Use `cachingRedirector`
  1. Turn off a history host
  1. Call `ListQueues`

## Specifications

  - Version:
  - Platform:



---

### #6529: Certificate-filters in self-hosted version of Temporal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6529 |
| **State** | OPEN |
| **Author** | slavb18 (slavb18) |
| **Created** | 2024-09-17 07:55:02.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-17 07:55:02.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**  
Currently, certificate-filters only work with the [cloud version of Temporal](https://docs.temporal.io/cloud/tcld/namespace#import). It would be beneficial to have this functionality available in the self-hosted version as well, as it's frustrating for users running on-prem environments to lack similar security features.

**Describe the solution you'd like**  
A potential solution could be implementing a custom [myClaimMapper.go](https://github.com/temporalio/samples-server/blob/main/extensibility/authorizer/myClaimMapper.go), which would read from a certificate-filters file and use it as a claim mapper to enforce restrictions based on certificate attributes.

**Describe alternatives you've considered**  
An alternative approach could involve using an nginx gRPC proxy that applies certificate-based access control at the network level, though this might add additional complexity compared to a native Temporal solution.

**Additional context**  
Providing a solution that works similarly in both cloud and self-hosted environments will help maintain feature parity and improve security for self-hosted users.


---

### #6521: Optional Activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6521 |
| **State** | OPEN |
| **Author** | ilijaNL (ilija) |
| **Created** | 2024-09-15 07:49:44.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-15 07:50:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently workflows are hard to easily extend when adding new activities calls without any meaningful activity result for the workflow. This is because a workflow needs to be deterministic thus adding/removing activities will break determinism. 

**Describe the solution you'd like**
We would like to have an ability to mark an activity call as "optional", meaning that if we add or remove activity invocation from a workflow the workflow replay wont cause any determinism issue. This is only possible when the workflow is not depending on the result of the activity. This is especially useful when you have fire&forget kind of activities (like publishing messages, notifying other services, fan-outs, etc).

An potential example could be:
```ts
const activitiesFn = [
   (payload: string) => activityA(payload),
   (payload: string) => activityB(payload),
   (payload: string) => activityC(payload),
   // possibility to just add additional activityFn without breaking determinism
   // (payload: string) => activityD(payload),
];

export async function activitiesWorkflow() {
  const someEvent = 'abc';
  await Promise.all(activitiesFn.map((fn) => fn(someEvent)));
}
```

**Describe alternatives you've considered**
Publishing an single event and create own listeners which listen to the published event and react upon. This however requires own infra/queue/listeners setup to do fanout. 

**Additional context**
Slack: https://temporalio.slack.com/archives/CTRCR8RBP/p1725981611435449



---

### #6512: Update latency increases with number of updates sent to workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6512 |
| **State** | OPEN |
| **Author** | dandavison (Dan Davison) |
| **Created** | 2024-09-12 21:27:37.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-12 22:16:00.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Latency experiments (see https://github.com/dandavison/temporal-latency-experiments) show that update latency increases with the number of updates sent to the workflow. Example results:

<img width="362" alt="image" src="https://github.com/user-attachments/assets/c383e4c3-e265-4bf2-98ec-da712ad798d9">


<img width="353" alt="image" src="https://github.com/user-attachments/assets/1ce28e33-56da-4812-9f9c-17de0986a37c">

The above measurements were collected in a `us-west-2` EC2 instance. The "localhost" version was using `temporal server start-dev` (v1.25) and the "cloud" variant was using cloud namespace `sdk-ci.a2dd6.tmprl.cloud` (also in `us-west-2`). The latency measured is from sending an update to receiving the result. Go SDK. The update handler mutates local workflow state but does not block on anything. The experiments send n=2000 updates since this is the maximum allowed per workflow.

In contrast, neither signal latency nor query latency show the same trend:

<img width="365" alt="image" src="https://github.com/user-attachments/assets/616ab490-3d8b-44dc-8e01-fd0534056e84">

<img width="359" alt="image" src="https://github.com/user-attachments/assets/b5901ba8-6779-4166-85fc-47adc1331726">



---

### #6501: Access original run id in mutable state

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6501 |
| **State** | OPEN |
| **Author** | yux0 (Yu Xia) |
| **Created** | 2024-09-09 21:02:43.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-09 21:02:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Allow access original run id in mutable state

**Describe the solution you'd like**
Added a new field in the mutable state to record the original run id.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
[Add any other context or screenshots about the feature request here.](https://github.com/temporalio/temporal/pull/6499)



---

### #6475: Support registering search attributes as an option/config

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6475 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-09-03 21:42:23.000 UTC (1y 3m ago) |
| **Updated** | 2024-09-03 21:42:23.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

In CLI we foolishly used operator service to register search attributes immediately after start, but users just check whether port is available to see if started and therefore it may race against operator service.

What we'd like is the ability to set search attributes. Basically something like `go.temporal.io/server/temporal.WithSearchAttributes` that accepts a `map[string]enums.IndexedValueType` (or any other structure) and does not start the server until those are confirmed set with that value type. I think the best way is to register if attr name not there, do nothing if attr name there and the proper type, and fail startup if attr there but wrong type. It's possible this can be used for general server config too.


---

### #6471: Angie testing something (to delete)

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6471 |
| **State** | OPEN |
| **Author** | webchick-test |
| **Created** | 2024-08-30 18:57:49.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-30 18:57:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üöÄ 1 |

#### Description

## Expected Behavior


## Actual Behavior


## Steps to Reproduce the Problem

  1.
  1.
  1.

## Specifications

  - Version:
  - Platform:



---

### #6468: GenerateReplicationTasks activity of ForceReplicationWorkflow should only log not found errors opposed to returning the error as well

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6468 |
| **State** | OPEN |
| **Author** | hferentschik (Hardy Ferentschik) |
| **Created** | 2024-08-30 10:29:50.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-30 10:29:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

When generating replication tasks in [GenerateReplicationTasks](https://github.com/temporalio/temporal/blob/0836b0fff0bbd6637ea1723f7e5a675519029354/service/worker/migration/activities.go#L427), not found errors from the underlying `historyClient.GenerateLastHistoryReplicationTasks` should only be logged.

Workflows can for example be archived between being listed and then processed in `GenerateReplicationTasks`. A not found history should just be logged. Returning the error keep retrying the activity

## Actual Behavior

Due to returning the error the activity gets retried and does not make any progress anymore.

```Go
	for i := startIndex; i < len(request.Executions); i++ {
		we := request.Executions[i]
		if err := a.generateWorkflowReplicationTask(ctx, rateLimiter, definition.NewWorkflowKey(request.NamespaceID, we.WorkflowId, we.RunId)); err != nil {
			if !isNotFoundServiceError(err) {
				a.logger.Error("force-replication failed to generate replication task", tag.WorkflowNamespaceID(request.NamespaceID), tag.WorkflowID(we.WorkflowId), tag.WorkflowRunID(we.RunId), tag.Error(err))
				return err
			}
		}

		activity.RecordHeartbeat(ctx, i)
	}
```


## Steps to Reproduce the Problem

  1. Execute a ForceReplicationWorkflow run for a given workflow which will archive directly once completed
  1. Complete the workflow in between `ListWorkflows` and `GenerateReplicationTasks`

## Specifications

  - Version: `main`
  - Platform:



---

### #6411: Error: Cannot query workflow due to Workflow Task in failed state 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6411 |
| **State** | OPEN |
| **Author** | talha-naeem1 |
| **Created** | 2024-08-19 10:25:59.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-19 10:25:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Problem Statement:
 I am using temporal with another open source tool i.e. airbyte. While benchmarking airbyte, I sent  1000 parallel requests, but temporal is limiting the progress. And throwing this error in the logs  `Error: Cannot query workflow due to Workflow Task in failed state `
Can someone  help me understand the reason for this error?

Complete Error Log:

```
{"level":"error","ts":"2024-08-12T18:00:53.205Z","msg":"service failures","operation":"QueryWorkflow","wf-namespace":"default","error":"Cannot query workflow due to Workflow Task in failed state.","logging-call-at":"telemetry.go:295","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:150\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).handleError\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:295\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:166\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.54.0/server.go:1164\ngo.temporal.io/server/common/metrics.NewServerMetricsTrailerPropagatorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:113\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.54.0/server.go:1164\ngo.temporal.io/server/common/metrics.NewServerMetricsContextInjectorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.54.0/server.go:1164\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.36.4/interceptor.go:341\ngoogle.golang.org/grpc.getChainUnaryHandler.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.54.0/server.go:1164\ngo.temporal.io/server/common/rpc.ServiceErrorInterceptor\n\t/home/builder/temporal/common/rpc/grpc.go:137\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.54.0/server.go:1155\ngo.temporal.io/server/api/historyservice/v1._HistoryService_QueryWorkflow_Handler\n\t/home/builder/temporal/api/historyservice/v1/service.pb.go:1697\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.54.0/server.go:1345\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.54.0/server.go:1722\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.2\n\t/go/pkg/mod/google.golang.org/grpc@v1.54.0/server.go:966"}
```

## Specifications

  - Version: temporalio/auto-setup:1.20.1
  - Platform: Kubernetes
  
PS: I am using default configs of temporal.



---

### #6403: Introducing quarkus-temporal

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6403 |
| **State** | OPEN |
| **Author** | rmanibus (Lo√Øc Hermann) |
| **Created** | 2024-08-14 13:42:59.000 UTC (1y 4m ago) |
| **Updated** | 2024-08-14 13:42:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description


Good morning,

This is just to inform you that we started to build a temporal / quarkus integration here:

https://docs.quarkiverse.io/quarkus-temporal/dev/index.html


---

### #6364: Add TemporalScheduledStartTime and TemporalScheduledById to the StartWorkflow Event message

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6364 |
| **State** | OPEN |
| **Author** | jlacefie (Jonathan Lacefield) |
| **Created** | 2024-07-31 22:17:39.000 UTC (1y 5m ago) |
| **Updated** | 2024-12-12 15:39:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
A user would like to validate the originating starter for WFs started by a schedule.  Using Search Attributes is problematic. The ideal solution is to add a similar field for cron, cron_schedule, for Schedules.

**Describe the solution you'd like**
We would like to add TemporalScheduledStartTime and TemporalScheduledById to the start event message as opposed to just search attributes.

**Describe alternatives you've considered**
The user tried using Search Attributes but found them problematic for 2 reasons:
- it's feasible that the schedule Search Attributes could be spoofed
- the WFID in the Schedule Search Attributes includes a timestamp which breaks client logic

**Additional context**




---

### #6339: Update/trigger/etc activities via client

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6339 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-07-25 18:30:02.000 UTC (1y 5m ago) |
| **Updated** | 2024-08-02 18:44:28.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | ychebotarev |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

Support updating activity options and/or triggering a backing-off activity from a client. This is a concept Temporal is actively considering and still designing, so details are not present yet. This is merely an issue for public issue visibility.


---

### #6336: Expose start delay in child workflow options

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6336 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2024-07-24 23:13:12.000 UTC (1y 5m ago) |
| **Updated** | 2024-08-02 18:44:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | lina-temporal |
| **Milestone** | None |

#### Description

See https://github.com/temporalio/features/issues/515 for more information


---

### #6328: Unit test error in v1.24.2

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6328 |
| **State** | OPEN |
| **Author** | BrianKopp (Brian Kopp) |
| **Created** | 2024-07-23 15:36:49.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-23 15:36:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

Unit tests should pass

## Actual Behavior

Unit tests do not pass

## Steps to Reproduce the Problem

  1. Checkout tag `v1.24.2` or `release/v1.24.x`
  1. Run unit tests, either via `make unit-test-coverage` or `go test ./service/history/workflow/...`
  1. Observe failure

```
=== Failed
=== FAIL: service/history/workflow TestMutableStateSuite/TestTotalEntitiesCount (0.00s)
2024-07-22T22:38:14.867Z	info	Task key range updated	{"number": 1048576, "next-number": 2097152, "logging-call-at": "task_key_generator.go:177"}
    controller.go:231: missing call(s) to *cluster.MockMetadata.ClusterNameForFailoverVersion(is equal to false (bool), is equal to 1234 (int64)) /home/runner/work/temporal/temporal/service/history/workflow/mutable_state_impl_test.go:1087
    controller.go:231: missing call(s) to *cluster.MockMetadata.GetCurrentClusterName() /home/runner/work/temporal/temporal/service/history/workflow/mutable_state_impl_test.go:1091
    controller.go:231: aborting test due to missing call(s)

=== FAIL: service/history/workflow TestMutableStateSuite (0.05s)

DONE 4541 tests, 7 skipped, 2 failures in 665.189s
make: *** [Makefile:405: unit-test-coverage] Error 1
```
## Specifications

  - Version: v1.24.2
  - Platform: 
 ```
Current runner version: '2.317.0'
Operating System
  Ubuntu
  [2](https://github.com/bigeyedata/temporal/actions/runs/10049193855/job/27775132542?pr=19#step:1:2)2.04.4
  LTS
Runner Image
  Image: ubuntu-22.04
  Version: 20240714.1.0
  Included Software: https://github.com/actions/runner-images/blob/ubuntu22/20240714.1/images/ubuntu/Ubuntu2204-Readme.md
  Image Release: https://github.com/actions/runner-images/releases/tag/ubuntu22%2F20240714.1
Runner Image Provisioner
  2.0.[3](https://github.com/bigeyedata/temporal/actions/runs/10049193855/job/27775132542?pr=19#step:1:3)70.1
```


---

### #6320: Multiple versioning related flakes in Go SDK test suite after upgrading to server 1.25.0-rc.0

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6320 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2024-07-22 20:28:13.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-22 20:28:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

While upgrading the dev server in the Go SDK to test Nexus, I've encountered flakes that don't occur with a 1.24.

Tests detailed here:

https://github.com/temporalio/sdk-go/pull/1555/files#diff-ab31ca891f17d88fce76f80500e533d5db2a1723dbfe88449169270bcbbaf781

The cause needs to be investigated and server code fixed before we cut an official 1.25.0 release.


---

### #6275: Is there a way to detect when workflow is in a Failed State but is still "Running"

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6275 |
| **State** | OPEN |
| **Author** | bijeebuss (Michael Welnick) |
| **Created** | 2024-07-11 18:07:58.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-11 18:09:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Is your feature request related to a problem? Please describe.
If I do this https://github.com/temporalio/samples-typescript/compare/main...bijeebuss:samples-typescript:main
and then start the workflow it will show up as "Running" but it's actually in a sort of failed state. It also does this in other cases like when you forget to export the workflow. 

Things I tried

1. Try/Catch (does not catch the error)
1. Custom Logger: All I see in the logs is "workflow started" and "workflow completed" and the meta has no way to detect that it actually failed.
1. setting retry: { maximumAttempts: 1 }
when starting the workflow does not cause the workflow to enter an actual "Failed" state

Describe the solution you'd like
A way to detect when a workflow enters a state like this. Maybe it already exists but I can't find anything


---

### #6259: Support validated/authenticated payloads

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6259 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-07-10 15:22:51.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-10 15:35:18.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

Users send encrypted payloads, but server accepts all forms of payloads. Many users want to restrict payloads to only ones they create.

This can be done a few ways. To me the most obvious/simplest approach is that we have server/namespace-level public keys from a user (w/ full CRUD API, RSA or ECDSA only at first probably) with an identifier for each key and if any have been configured, use https://pkg.go.dev/go.temporal.io/api/proxy#VisitPayloads via interceptor to ensure that every non-search-attribute payload has a `keyId` payload metadata entry for a known key, and a `signature` payload metadata entry that uses that public key to validate that signature against the `data` contents. Cloud can have its own way to configure server/namespace-level keys.


---

### #6253: Expand configuration for Elastic to support serverless

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6253 |
| **State** | OPEN |
| **Author** | thegedge (Jason Gedge) |
| **Created** | 2024-07-08 18:03:37.000 UTC (1y 5m ago) |
| **Updated** | 2024-07-08 18:03:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

We were looking to use Elastic's serverless option for our visibility store, but to connect we would need to set an [`Authorization`](https://www.elastic.co/docs/current/serverless/elasticsearch/get-started#test-connection) header, which currently doesn't appear to be supported by the config for the ES client: https://github.com/temporalio/temporal/blob/66ab754e7e8d60421678457df1914fa7ddae9dc3/common/persistence/visibility/store/elasticsearch/client/config.go#L45-L60

**Describe the solution you'd like**

I'd like to be able to add a configuration like the following:

```yaml
elasticsearch:
  # ...
  headers:
    Authorization: "ApiKey {{ .Env.TEMPORAL_ES_VISIBIILITY_STORE_PASSWORD }}"
```

**Describe alternatives you've considered**

Another option would be to have this kind of a format:

```yaml
elasticsearch:
  # ...

  auth:
    basic:
      username: "..."
      password: "..."
    # or
    elastic:
      apiKey: "..."
```

But it feels far less flexible and a bit more complicated to implement.
 
Other than that, I can't really think of any alternative, besides having to set up a proxy that adds the header. Ideally, we avoid the operational overhead of another proxy and just do it straight on the client.


---

### #6237: Enable wildcard namespace definition when setting namespace RPS

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6237 |
| **State** | OPEN |
| **Author** | kelkawi-a (Ali Kelkawi) |
| **Created** | 2024-07-04 08:53:04.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-04 08:53:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
It seems there is a way of setting namespace RPS limits as [outlined here](https://community.temporal.io/t/rate-limiting-by-namespace/1335/7). To enable a more dynamic approach, it would be useful to be able to define wildcard rules that get applied to namespaces (e.g. `stg-*` means any namespaces matching this rule such as `stg-test` will have this rule applied to it).

**Describe the solution you'd like**
Current functionality:
```yaml
frontend.namespacerps:
- value: 2000
- value: 100
  constraints:
    namespace: "namespaceA"
- value: 500
  constraints:
    namespace: "namespaceB"
```

Requested functionality:
```yaml
frontend.namespacerps:
- value: 1000
- value: 500
  constraints:
    namespace: "stg-*"
- value: 600
  constraints:
    namespace: "prod-*"
```

**Describe alternatives you've considered**
The alternative here is updating the server config every time a new namespace is added to which we want to apply RPS limits. The proposed approach can allow a more generic configuration of RPS limits to namespaces.

**Additional context**
N/A



---

### #6227: Metric for Complete Workflow/Activity Failure

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6227 |
| **State** | OPEN |
| **Author** | TheHiddenLayer |
| **Created** | 2024-07-03 17:45:48.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-03 18:05:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | üëÄ 1 |

#### Description

**Is your feature request related to a problem? Please describe.**

I'd like to be able to clearly understand how many Workflows suffered a complete failure _after_ exhausting all retries. (See Additional Context section).

**Describe the solution you'd like**
A metric representing the failure of a workflow/activity after any and all retries have been exhausted. 

**Describe alternatives you've considered**
N/A?

**Additional context**
Sometimes our Temporal service goes down, and during the outage, various metrics show "failures" (`temporal_workflow_failed`, `temporal_activity_execution_failure`, etc. etc.). There are client-side retries, so would be good to know when there's been the "final" failure after all client side retries have been exhausted and the WF / Activity has "actually" failed for real and won't be re-attempted.

If my feature request doesn't make sense, then let me present our larger scenario for context: Quite reasonably, we want to "assess impact" of the service outage by knowing how many activities or workflows "actually, permanently failed" (i.e. all forms of retires are exhausted, while the service was down and they didn't get to run ever again). How can we do this?



---

### #6212: Allow configurable root path for HTTP API

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6212 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-07-01 16:17:21.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-01 16:17:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

Even though our server API only listens to `/namespaces`, `/cluster`, and `/system-info`, some users may still want that at a deeper level (as opposed to their proxies trimming it off), so allow users to specify a path prefix.


---

### #6211: Flaky versioning issue when querying since 1.24

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6211 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-07-01 14:43:27.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-01 14:44:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

https://github.com/temporalio/sdk-dotnet/blob/a87116418c526bb76e0cc11f951d6f8648b71e01/tests/Temporalio.Tests/Worker/WorkerVersioningTests.cs#L53-L55 works with pre-1.24 servers without flake.

## Actual Behavior

https://github.com/temporalio/sdk-dotnet/blob/a87116418c526bb76e0cc11f951d6f8648b71e01/tests/Temporalio.Tests/Worker/WorkerVersioningTests.cs#L53-L55 now flakes. Querying a workflow with versioning for some reason sometimes now times out (i.e. gets "Timeout expired" RPC failure). This only occurs on slower GitHub runners (I wonder if there is some issue with `UpdateWorkerBuildIdCompatibility` not being strongly consistent). Marking that test skipped and opening this issue at @ShahabT's request.


---

### #6210: Lots of log noise on restarting server with sqlite persistence

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6210 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-07-01 14:16:32.000 UTC (1y 6m ago) |
| **Updated** | 2024-07-01 14:16:32.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

No log noise.

## Actual Behavior

On 1.24.1 (i.e. latest CLI as of this writing). Doing a simple `temporal server start-dev --db-filename my-db.db`, interrupting it (does graceful shutdown), then running that command again gives lots of logs. This is in addition to the log noise server already gives on startup/shutdown of CLI.


---

### #6192: Support externally-provided unique async completion token for async activities

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6192 |
| **State** | OPEN |
| **Author** | criemen (Cornelius Riemenschneider) |
| **Created** | 2024-06-25 10:00:20.000 UTC (1y 6m ago) |
| **Updated** | 2024-06-25 10:00:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
I want to connect GitHub webhooks to temporal. In particular, I want to run a GH actions workflow as an activity in my temporal workflow.
This is not modelled really well with signals in the workflow: From receiving a [workflow_run](https://docs.github.com/en/webhooks/webhook-events-and-payloads#workflow_run) event, it's usually unclear which temporal workflow triggered this run.
Also, mixing control flow and signals is imo not very nice. I'd rather have this abstracted inside an activity (with timeouts, retry, etc).

So, I want to (and am) using an async activity.
The problem with that is that for that, I am _getting_ a unique ID (task token) from temporal, and I'm getting another unique ID from GitHub (workflow_run_id). I now have to manually correlate those two (in my case: By using a database table on the side that is accessible to both the temporal worker that starts the actions workflow, and the temporal client, that receives the GH webhooks).
This works well, but I'd rather not maintain a database myself.


**Describe the solution you'd like**
Provide the same async activity mechanism, but with an API where I hand temporal a unique ID (I have to promise uniqueness as part of the API contract, otherwise temporal is expected to throw an error), instead of getting a unique Id from temporal.
I think that would lend itself neatly to all sorts of webhook-receiving problems, where the system you're integrating against hands out unique IDs.
I believe in many cases, async activities will model flows in external systems much better than signal handlers in workflows, as activities compose much better (and are easier to be reused as well!) than signal handlers, that are rather workflow-specific.


**Describe alternatives you've considered**
See above, I can manually map the IDs between the different systems. For that to be easier, a general-purpose KV-store exposed by temporal would also be appreciated, as then I don't need to manually maintain the database+database table+migration myself.


**Additional context**
I'm happy to answer any more questions about our use case.


---

### #6144: ActivityStateReplicator should trigger history resend when mutable state is no found

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6144 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2024-06-14 22:20:03.000 UTC (1y 6m ago) |
| **Updated** | 2024-06-14 22:20:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
- Sync activity replication task should create the workflow if not found in the target cluster. 

## Actual Behavior
- Sync activity replication task will be dropped.
https://github.com/temporalio/temporal/blob/a29cc498bfb13771b41a2eeaeda41f4622ae0a8f/service/history/ndc/activity_state_replicator.go#L222

## Solution
- Return a RetryReplication error instead off nil to trigger history resend. History resender can handler the case where the workflow no longer existing in the source cluster.



---

### #6104: Include Linux ARM, macOS x64/ARM, and Windows x64 in CI

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/6104 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2024-06-11 12:24:46.000 UTC (1y 6m ago) |
| **Updated** | 2024-06-21 20:13:54.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | carlydf |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

Would like test suite to run in more supported platforms than just the one Linux x64.


---

### #5853: Cross-replicated search attributes are not persistent

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5853 |
| **State** | OPEN |
| **Author** | joshua-auchincloss (Joshua Auchincloss) |
| **Created** | 2024-05-06 15:11:44.000 UTC (1y 7m ago) |
| **Updated** | 2024-05-06 15:11:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
Expected to be able to add search attributes in global namespaces


## Actual Behavior
No effect / change to search attributes

## Steps to Reproduce the Problem

  1. Set up a cross-replicated global namespace between two clusters
  2. Execute `temporal operator search-attribute create --namespace my-ns --name AbcSearch --type Text` against active cluster (cluster-a for example purposes)
  3. Execute the same against non-active cluster (no matter how many times this is run, it will always say `search attributes have been added`, expect to run once and all subsequent executions should raise conflict errors)
  4. Execute `temporal operator search-attribute list --namespace my-ns` against active cluster (will maybe appear)
  5. Execute the same against non-active cluster (will not appear)
  6. Initiate active cluster failover using `temporal namespace update --active-cluster cluster-b my-ns`
  7. Re-execute 3 & 5, search attributes will not update

## Specifications

  - Version: 1.23.0
  - Platform: Debian


---

### #5743: DLQv2 operator tooling should print categoryID, source cluster, and target cluster instead of internal queue name

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5743 |
| **State** | OPEN |
| **Author** | tdeebswihart (Tim Deeb-Swihart) |
| **Created** | 2024-04-17 17:55:43.000 UTC (1y 8m ago) |
| **Updated** | 2024-04-26 21:57:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, operations |
| **Assignees** | prathyushpv |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

Right now our DLQ tooling dumps the internal DLQ queue name which is not only an implementation detail but _not_ what you need to use the tool. For example:

```
$ tdbg dlq --dlq-version v2 list
          QUEUENAME          | MESSAGECOUNT
  1_cluster1_cluster2_ETC |            0
```

Right now this is in the form `{dlq-type}_{source-cluster}_{target-cluster}_BLAH` but we shouldn't expect our operators to know that.

**Describe the solution you'd like**

To use our DLQ tooling you need to know:
- The DLQ's type (1 in the above example).
- The source cluster (cluster1)
- The target cluster (cluster2)

Those are the values that should be dumped, so I'd expect something like
```
DLQ TYPE | SOURCE CLUSTER | TARGET CLUSTER | MESSAGECOUNT
1        | cluster1       | cluster2       | 0
```

**Describe alternatives you've considered**

I didn't.



---

### #5642: dynamic-config-value invalid JSON value for key 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5642 |
| **State** | OPEN |
| **Author** | suityou01 (Charlie Benger-Stevenson) |
| **Created** | 2024-04-02 08:30:01.000 UTC (1y 9m ago) |
| **Updated** | 2024-04-02 08:36:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

When I launch the temporal server from the command line and supply a valid key/value for the dynamic-config-value flag the server starts and applies this config value

https://docs.temporal.io/cli/cmd-options#dynamic-config-value

## Actual Behavior

The server fails to start and gives the error

"Error: invalid JSON value for key "persistence.defaultStore"

## Steps to Reproduce the Problem

  1. Install temporal
 ```bash
curl -sSf https://temporal.download/cli.sh | sh
```
  2. Launch with the following dynamic config flags

```bash
temporal server start-dev --ip 0.0.0.0 --dynamic-config-value \
	 frontend.workerVersioningDataAPIs=true --dynamic-config-value \
	 persistance.numHistoryShards=2 --dynamic-config-value \
	 persistence.defaultStore="default" --dynamic-config-value \
	 persistence.visibilityStore="default" --dynamic-config-value \
	 persistence.secondaryVisibilityStore="default" --dynamic-config-value \
	 persistence.datastores.default.sql.user="root" --dynamic-config-value \
	 persistence.datastores.default.sql.password="root" --dynamic-config-value \
         persistence.datastores.default.sql.pluginName="mysql" --dynamic-config-value \
         persistence.datastores.default.sql.databaseName="temporal" --dynamic-config-value \
         persistence.datastores.default.sql.connectAddr="mysql" --dynamic-config-value \
         persistence.datastores.default.sql.connectProtocol="tcp"
```

## Specifications

  - Version: temporal version 0.11.0 (server 1.22.4) (ui 2.21.3)
  - Platform: Docker container running debian based image



---

### #5634: Use the new CLI's rewrite branch when building temporal images

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5634 |
| **State** | OPEN |
| **Author** | tdeebswihart (Tim Deeb-Swihart) |
| **Created** | 2024-04-01 18:48:31.000 UTC (1y 9m ago) |
| **Updated** | 2024-10-11 15:55:32.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, docker |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**
Our admin-tools and other official images should use the new [pre-release CLI branch](https://github.com/temporalio/cli/tree/cli-rewrite) once it has a release rather than cli v0.11.0

**Additional context**

We discussed this briefly in https://github.com/temporalio/docker-builds/pull/190



---

### #5604: `temporalio/server` image 1.23.0 reports RC version

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5604 |
| **State** | OPEN |
| **Author** | danieljoos (Daniel Joos) |
| **Created** | 2024-03-26 16:37:36.000 UTC (1y 9m ago) |
| **Updated** | 2024-03-26 16:37:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

The version string of a release version of `temporal-server` should not include `-rc`.

## Actual Behavior

The `temporal-server` binary in the official `temporalio/server@1.23.0` image on Docker reports a `-rc` version:

```
docker run -it temporalio/server:1.23.0 -- bash
$ temporal-server --version
temporal version 1.23.0-rc16
```

## Steps to Reproduce the Problem

  1. Load the official docker image `temporalio/server@1.23.0` from docker hub. (digest `sha256:c97983007672fd402c4e4c460a80d0056821bc8338732ede2e1fafd4f86d3ce2`)
  1. Start a container and run `temporal-server --version`
  1. Alternatively, use the temporal-ui to show the server version.

## Specifications

  - Version: 1.23.0
  - Platform: tried amd64 and arm64 images



---

### #5589: command missing in new Temporal CLI

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5589 |
| **State** | OPEN |
| **Author** | sonrel |
| **Created** | 2024-03-25 12:07:08.000 UTC (1y 9m ago) |
| **Updated** | 2024-03-25 12:07:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
I have not found a command replacing this behaviour in the new cli, where you update a namespace to contain multiple clusters:
`tctl  --namespace "myNamespace" namespace update --clusters clusterA clusterB`

for context: https://community.temporal.io/t/running-into-problems-testing-multi-cluster-replication-locally-via-docker/5920/8

it is marked as the solution in thread above.

**Describe the solution you'd like**
`temporal operator namespace update  --cluster 'clusterA clusterB' "myNamespace"`
Have the cluster accept a list of clusters and not only a single one. 

**Describe alternatives you've considered**
1. Use the old tctl, but at some point it will be deprecated.

2. I cannot create it from scratch since we already have namespaces in use.




---

### #5587: dev-server crash on AddOrUpdateRemoteCluster RPC with invalid frontend address

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5587 |
| **State** | OPEN |
| **Author** | josh-berry (Josh Berry) |
| **Created** | 2024-03-22 22:09:35.000 UTC (1y 9m ago) |
| **Updated** | 2024-03-22 22:09:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

Server rejects the RPC with an error, but continues to run.

## Actual Behavior

Server crashes (that is, `temporal server start-dev` exits with error code 1).

Error log message: `{"level":"fatal",...,"msg":"Invalid rpcAddress for remote cluster","error":"address asdf: missing port in address","logging-call-at":"rpc.go:191",` ...

Stack trace:

```
go.temporal.io/server/common/log.(*zapLogger).Fatal
        go.temporal.io/server@v1.22.4/common/log/zap_logger.go:180
go.temporal.io/server/common/rpc.(*RPCFactory).CreateRemoteFrontendGRPCConnection
        go.temporal.io/server@v1.22.4/common/rpc/rpc.go:191
go.temporal.io/server/client.(*rpcClientFactory).NewRemoteAdminClientWithTimeout
        go.temporal.io/server@v1.22.4/client/clientfactory.go:195
go.temporal.io/server/service/frontend.(*OperatorHandlerImpl).AddOrUpdateRemoteCluster
        go.temporal.io/server@v1.22.4/service/frontend/operator_handler.go:607
go.temporal.io/api/operatorservice/v1._OperatorService_AddOrUpdateRemoteCluster_Handler.func1
        go.temporal.io/api@v1.26.0/operatorservice/v1/service.pb.go:347
go.temporal.io/server/common/rpc/interceptor.(*RetryableInterceptor).Intercept.func1
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/retry.go:63
go.temporal.io/server/common/backoff.ThrottleRetryContext
        go.temporal.io/server@v1.22.4/common/backoff/retry.go:145
go.temporal.io/server/common/rpc/interceptor.(*RetryableInterceptor).Intercept
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/retry.go:67
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/rpc/interceptor.(*CallerInfoInterceptor).Intercept
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/caller_info.go:80
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/rpc/interceptor.(*SDKVersionInterceptor).Intercept
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/sdk_version.go:69
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/rpc/interceptor.(*RateLimitInterceptor).Intercept
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/rate_limit.go:88
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/rpc/interceptor.(*NamespaceRateLimitInterceptor).Intercept
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/namespace_rate_limit.go:93
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/rpc/interceptor.(*ConcurrentRequestLimitInterceptor).Intercept
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/concurrent_request_limit.go:121
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).StateValidationIntercept
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/namespace_validator.go:194
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).UnaryIntercept
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/telemetry.go:165
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/service/frontend.(*RedirectionInterceptor).Intercept
        go.temporal.io/server@v1.22.4/service/frontend/redirection_interceptor.go:180
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/authorization.(*interceptor).Interceptor
        go.temporal.io/server@v1.22.4/common/authorization/interceptor.go:158
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/service/frontend.GrpcServerOptionsProvider.NewServerMetricsContextInjectorInterceptor.func1
        go.temporal.io/server@v1.22.4/common/metrics/grpc.go:66
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1
        go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.42.0/interceptor.go:344
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/namespace_logger.go:84
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).NamespaceValidateIntercept
        go.temporal.io/server@v1.22.4/common/rpc/interceptor/namespace_validator.go:111
google.golang.org/grpc.getChainUnaryHandler.func1
        google.golang.org/grpc@v1.59.0/server.go:1163
go.temporal.io/server/common/rpc.ServiceErrorInterceptor
        go.temporal.io/server@v1.22.4/common/rpc/grpc.go:145
google.golang.org/grpc.NewServer.chainUnaryServerInterceptors.chainUnaryInterceptors.func1
        google.golang.org/grpc@v1.59.0/server.go:1154
go.temporal.io/api/operatorservice/v1._OperatorService_AddOrUpdateRemoteCluster_Handler
        go.temporal.io/api@v1.26.0/operatorservice/v1/service.pb.go:349
google.golang.org/grpc.(*Server).processUnaryRPC
        google.golang.org/grpc@v1.59.0/server.go:1343
google.golang.org/grpc.(*Server).handleStream
        google.golang.org/grpc@v1.59.0/server.go:1737
google.golang.org/grpc.(*Server).serveStreams.func1.1
        google.golang.org/grpc@v1.59.0/server.go:986
```

## Steps to Reproduce the Problem

  1. `temporal server start-dev`
  1. `temporal operator cluster upsert --frontend-address=asdf`
  1. Server process exits with error code 1.

(I'm not sure if this is repro'able on an actual server, but the crash is coming from within server code, so I would assume so‚Äîhence why I'm filing the issue here instead of against the CLI.)

## Specifications

  - Version: `temporal version 0.11.0 (server 1.22.4) (ui 2.21.3)`
  - Platform: Mac ARM



---

### #5553: Update license check script to verify current year is used in new files

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5553 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2024-03-18 17:36:45.000 UTC (1y 9m ago) |
| **Updated** | 2024-03-18 17:55:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | None |
| **Assignees** | None |
| **Milestone** | None |

#### Description

_Originally posted by @tdeebswihart in https://github.com/temporalio/temporal/pull/5523#discussion_r1525640564_
            


---

### #5544: Use different error when signalName is empty than errSignalNameTooLong

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5544 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2024-03-15 15:02:35.000 UTC (1y 9m ago) |
| **Updated** | 2024-03-15 15:02:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Feature request is to add a new error message when signalName is empty:
Currently used is https://github.com/temporalio/temporal/blob/main/service/frontend/workflow_handler.go#L1629-L1631

SignalNameTooLong 

which is a bit confusing

Thanks.


---

### #5543: Remove indexes from schema Temporal in cassandra

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5543 |
| **State** | OPEN |
| **Author** | vikingUnet (Maxim Sherstuk) |
| **Created** | 2024-03-15 08:46:55.000 UTC (1y 9m ago) |
| **Updated** | 2024-03-15 08:46:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Hello!

I [found](https://github.com/temporalio/temporal/blob/main/schema/cassandra/temporal/schema.cql#L185) creating indexes in schema description:
```
CREATE INDEX cm_lastheartbeat_idx on cluster_membership (last_heartbeat);
CREATE INDEX cm_sessionstart_idx on cluster_membership (session_start);
```

As i know, indexes are not very good works in cassandra for big clusters because have a lot of [restrictions](https://docs.datastax.com/en/cql-oss/3.3/cql/cql_using/useWhenIndex.html#Whennottouseanindex).

Have you considered to create invert tables instead of using indexes - like tables cluster_membership_by_last_heartbeat and cluster_membership_by_session_start to detect correct membership_partition value and query with this to cluster_membership table?

```
CREATE TABLE cluster_membership_by_last_heartbeat
(
    last_heartbeat timestamp,
    membership_partition tinyint,
    PRIMARY KEY (last_heartbeat)
) WITH COMPACTION = {
    'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  };

CREATE TABLE cluster_membership_by_session_start
(
    session_start timestamp,
    membership_partition tinyint,
    PRIMARY KEY (session_start)
) WITH COMPACTION = {
    'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  };
```

Thanks a lot!



---

### #5541: Support for nestjs

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5541 |
| **State** | OPEN |
| **Author** | iampato (Patrick waweru) |
| **Created** | 2024-03-15 06:01:35.000 UTC (1y 9m ago) |
| **Updated** | 2024-03-15 06:01:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
As a backend engineer who loves nestjs and uses it I would love to experiment with Temporal

**Describe the solution you'd like**
They're community/individual efforts for example: https://github.com/KurtzL/nestjs-temporal are there plans to support such endeavors it would be a risk to our department If we invested then the pkg is not updated

**Describe alternatives you've considered**
Writing plain nodejs but the company has policies and practices (NestJs only)





---

### #5510: Ability to update schedule memo via Python SDK

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5510 |
| **State** | OPEN |
| **Author** | nnoto (Nicholas Noto) |
| **Created** | 2024-03-11 21:42:17.000 UTC (1y 9m ago) |
| **Updated** | 2024-03-11 21:42:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Client,create_schedule allows a memo to be created on a schedule. Most schedule fields can be updated by calling ScheduleHandle.update, but it does not seem to allow the update of a memo with ScheduleUpdateInput/ScheduleUpdate. 
Memos may contain reference information and/or metadata for the schedule that could change depending on other changes to the schedule configuration as a result of the ScheduleHandle.update call. In order to keep the memo consistent with the updated configuration, the ability to update the memo is necessary as well.

**Describe the solution you'd like**
Allow an updated memo object to be passed in the ScheduleHandle.update call in the ScheduleUpdate object. 

**Describe alternatives you've considered**
There do not seem to be other ways to track metadata associated with a schedule apart from the memo field, so this seems to be the only solution. 

**Additional context**
N/A


---

### #5493: Automatically split history event batches when size of reapplied events are too large

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5493 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2024-03-05 22:18:02.000 UTC (1y 10m ago) |
| **Updated** | 2024-06-10 22:38:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
When doing reset, signals and updates after the reset point will be reapplied (cherry-picked) to the new run. However all those reapplied events are grouped into one batch. Our persistence layer has a validation that basically says each event batch can't be exceeded 4MB size limit (each batch is a separate call to persistence). This means if the size of reapplied events is larger than 4MB, the reset can't be done.  

This issue from my understanding only applies to reset, where events from more than one event batches in the base workflow can be picked. The events reapply logic during conflict resolution is triggered by the replication task of a single batch of event, so we won't run into the situation. 

**Describe the solution you'd like**
Reset with more than 4MB reapplied events should be supported.

Approach 1:
- In history builder code, automatically track current event batch size and create a new batch if size limit is exceeded.

Approach 2:
- Make this specific to reset logic. Track the size of reapplied events and call an method on history build to create a new batch.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #5489: SecondaryVisbilityStore validation failed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5489 |
| **State** | OPEN |
| **Author** | vanhtuan0409 (Tu·∫•n V∆∞∆°ng) |
| **Created** | 2024-03-05 07:58:26.000 UTC (1y 10m ago) |
| **Updated** | 2024-03-05 07:58:26.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

I want to migrate the visibility store from ES to Postgres using Dual Visibility Store feature. Expected behavior to be able to dual write to both ES and Postgres. Temporal config:

```
  visibilityStore: es-visibility
  secondaryVisibilityStore: pg-visibility
  datastores:
    pg-visibility:
      SQL:
    es-visibility:
      elasticsearch:
```

## Actual Behavior

Upon starting the server, I got the following error `persistence config: cannot set secondaryVisibilityStore when visibilityStore is setting elasticsearch datastore`

## Steps to Reproduce the Problem

  1. Setup the config as above
  2. Start temporal server

## Specifications

  - Version: 1.22.3
  - Platform: Linux



---

### #5461: Add programmatic way to extract metrics port when passing `0` in config

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5461 |
| **State** | OPEN |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2024-02-28 17:30:40.000 UTC (1y 10m ago) |
| **Updated** | 2024-02-28 17:30:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
When using the repo as a library, as in the cli, there's no good way (that I could find) to extract the bound metrics port if you pass in `0` in the config. This makes it hard to figure out (log, in my case) what port was actually chosen. I had to find a free port first to work around that, even though the code would do that for me.

**Describe the solution you'd like**
Have some easy way to fetch the chosen metrics port after instantiating / starting a dev server




---

### #5454: Identifiers length validation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5454 |
| **State** | OPEN |
| **Author** | ndtretyak (Nikolay Tretyak) |
| **Created** | 2024-02-27 10:02:46.000 UTC (1y 10m ago) |
| **Updated** | 2024-02-27 10:02:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
The PostgreSQL schema limits identifier lengths to 255 characters. So I set  `limit.maxIDLength` to 255 as well for server-side validation because I don't want to have any errors on the database level. However, `len(someString)` in Go returns byte size, not character count, causing issues with multi-byte characters in identifiers.

**Describe the solution you'd like**
`limit.maxIDLength` in symbols rather than bytes.



---

### #5439: Worker cache handover

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5439 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2024-02-20 05:06:51.000 UTC (1y 10m ago) |
| **Updated** | 2024-02-23 22:34:57.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
When histories are large, a worker restart invalidates all the cached histories. This leads to increased latency for all the workflows that were cached on that worker.

**Describe the solution you'd like**
When a worker gracefully shuts down (or even crashes), the workflows that are cached on it are recovered on other workers to get cached before any tasks are generated for them.

**Additional context**
This is how a user requested it:

> Moving workflows that have a high history size from one worker to another when you have to kill the worker (to update for example) can be painful. It would be great if there was a way to mark workflows as "require preemptive cache alocation" or something that ensures that the cache is coordinated when killing the worker (or some similar strategy to make killing such a worker less painful)




---

### #5352: Refinement of ResourceExhausted reasons

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5352 |
| **State** | OPEN |
| **Author** | ndtretyak (Nikolay Tretyak) |
| **Created** | 2024-01-25 12:52:22.000 UTC (1y 11m ago) |
| **Updated** | 2024-01-26 22:39:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | tdeebswihart |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, there are three `ResourceExhausted` errors with the `RESOURCE_EXHAUSTED_CAUSE_BUSY_WORKFLOW` reason.

1. `ErrWorkflowClosing`, in my opinion, does not necessarily mean exhaustion.
https://github.com/temporalio/temporal/blob/aec2c100b6fae91a53f0f4ea3befe8ea683f0d9c/service/history/consts/const.go#L71-L72

2. `ErrConsistentQueryBufferExceeded` indicates that there are too many queries but `BUSY_WORKFLOW` might be too ambiguous in this case. 
https://github.com/temporalio/temporal/blob/aec2c100b6fae91a53f0f4ea3befe8ea683f0d9c/service/history/consts/const.go#L77-L78

3. `ErrResourceExhaustedBusyWorkflow` explicitly includes "busy workflow" in its name. So, this error looks fine :) However, the comment indicate that this error `should not be retried`, but resource exhaustion usually means "retry later, please".
https://github.com/temporalio/temporal/blob/aec2c100b6fae91a53f0f4ea3befe8ea683f0d9c/service/history/consts/const.go#L101-L102

These errors are indistinguishable in metrics, so I cannot exclude `ErrWorkflowClosing` (which is valid in my workload) from alerts.

**Describe the solution you'd like**
More granular reasons for `service_errors_resource_exhausted`.





---

### #5310: Allow(optionally) omit activity argument/input in the activity task scheduled event to save the storage cost

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5310 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2024-01-17 18:23:12.000 UTC (1y 11m ago) |
| **Updated** | 2024-01-17 18:25:40.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
The activity input is not really being used for replay. It's mainly for debugging(could be useful for cluster replication as well but can be eliminated/improved if designed properly).

**Describe the solution you'd like**
Allow workflow to not include argument/input in the actiivty task scheudled event. Just like local activity options: https://github.com/temporalio/sdk-java/blob/806eab71f2b65f762cdd1c4079d3fbad0b62ae1d/temporal-sdk/src/main/java/io/temporal/activity/LocalActivityOptions.java#L58

**Describe alternatives you've considered**
Using local activity could do that. However, local activity could run into sideffect when it has high failure rate -- the history will get blowed up with many timer events for retry management. (see https://community.temporal.io/t/local-activity-vs-activity/290/3 )

**Additional context**
This needs server to support because the input is needed to pass to activity task, to activity worker.



---

### #5306: Defining a storage class on S3 archival

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5306 |
| **State** | OPEN |
| **Author** | frucher08 |
| **Created** | 2024-01-17 10:01:33.000 UTC (1y 11m ago) |
| **Updated** | 2024-01-17 10:01:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
We have a policy of using only STANDARD_IA storage class. Currently, there is no ability to specify the storage class to use in S3 Archival configurations. This is blocking us from using S3 archival.

**Describe the solution you'd like**
Support additional S3 connection "kwargs" which will all just be passed to the connection or add `storage_class` as an option.

**Describe alternatives you've considered**
We are considering using filestore archival instead. Either that or implementing our own archiver.



---

### #5303: Provide WorkflowStatus in when signing a workflow that is closed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5303 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2024-01-16 19:04:21.000 UTC (1y 11m ago) |
| **Updated** | 2024-01-16 19:04:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently signaling a closed workflow will throw error, but there are several cases:

* Workflow really doesn't exist (or deleted after retention)
* Workflow is completed/terminated/etc (closed)
As an application, we sometimes need to handle the two cases differently(error handling).



**Describe the solution you'd like**
The error should should be able to return "WorkflowStatus" inside.

**Describe alternatives you've considered**
Currently we have to make a DescribeWorkflowExecution API call to achieve this. This requires more code, more error handling, and higher latency .

**Additional context**
https://github.com/temporalio/sdk-java/issues/1969



---

### #5299: Check DB Connectivity in Liveness probe

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5299 |
| **State** | OPEN |
| **Author** | F1bonacc1 (Eugene Berger) |
| **Created** | 2024-01-15 11:18:06.000 UTC (1y 11m ago) |
| **Updated** | 2024-01-15 11:19:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
In K8s environment there are situations when DB passwords and certificates are renewed.
When that happens the current `worker`, `frontend`, `matching` and `history` behavior is to fail silently and log the error.
The passwords are stored in K8s secrets and loaded as environment variables, in other words, a pod restart would resolve the issue.

**Describe the solution you'd like**
Check DB connectivity in liveness probe.
If the DB password is changed the temporal pods would restart and load the new secret.

**Describe alternatives you've considered**
Using `tctl namespace list` as liveness probe exec command, but it can't connect to the localhost even if the correct port is specified.
Plus, replacing the existing liveness probe feels hacky.

**Additional context**
An example of the `tctl n l` failure when run inside the history pod.
```
temporal-history-67f566466-qz8n8:/etc/temporal$ tctl --address localhost:7234 n l
Error: Error when list namespaces info
Error Details: rpc error: code = Unavailable desc = connection error: desc = "transport: authentication handshake failed: tls: first record does not look like a TLS handshake"
('export TEMPORAL_CLI_SHOW_STACKS=1' to see stack traces)
```



---

### #5298: Can Child workflows inherit search attributes from a parent workflow?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5298 |
| **State** | OPEN |
| **Author** | MysticalMount (MysticalMount) |
| **Created** | 2024-01-14 10:03:09.000 UTC (1y 11m ago) |
| **Updated** | 2024-01-14 10:03:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Im using Go sdk, starting Child workflows with search attributes is working fine, but when that workflow then spawns another workflow, I was hoping the SearchAttributes would be inherited.

Im using this to prepare the context for execution of the child workflow:
```
	cwo := workflow.GetChildWorkflowOptions(ctx)
	ctx = workflow.WithChildOptions(ctx, cwo)
```

But the executed workflow using that context doesnt seem to have the search attributes attached.

I doubt this is a bug,¬†just a lack of understanding on my part, any assistance appreciated, it wasnt easy to find an answer via search engines.


---

### #5249: Retry Policy should be copied to the continued as new workflow.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5249 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2023-12-28 17:19:37.000 UTC (2 years ago) |
| **Updated** | 2023-12-28 19:01:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
When a retry policy is not set in the `ContinueAsNewWorkflowExecutionCommandAttributes` continued workflow should have the same policy as the previous workflow run.

## Actual Behavior
The continued workflow doesn't have the retry policy set.

## Steps to Reproduce the Problem

  1. Set retry policy on a workflow.
  1. Call continue as new
  1. Verify that the next workflow doesn't have retry policy set.

## The code that should perform the merge:

https://github.com/temporalio/temporal/blob/a6fb92999a23b35f14c06157934580351d3b2162/service/history/workflow/mutable_state_impl.go#L1630


---

### #5245: SchedulesClientListOptions is missing SearchAttributes param

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5245 |
| **State** | OPEN |
| **Author** | dheraskou (Dmitry Heraskou) |
| **Created** | 2023-12-26 22:05:10.000 UTC (2 years ago) |
| **Updated** | 2024-02-09 19:15:59.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
 https://pkg.go.dev/go.temporal.io/sdk@v1.25.1/internal#ScheduleOptions has a description for the field SearchAttributes:
 "SearchAttributes - Indexed info that can be used in query of List schedules APIs". But the[ List API of a https://pkg.go.dev/go.temporal.io/sdk@v1.25.1/internal#ScheduleClient](https://pkg.go.dev/go.temporal.io/sdk@v1.25.1/internal#ScheduleClient.List) take https://pkg.go.dev/go.temporal.io/sdk@v1.25.1/internal#ScheduleListOptions param, which has only PazeSize and no SearchAttributes.  

A clear and concise description of what the problem :
Either the SearchAttributes field description is incorrect or (what is more likely) it should be one of the parameters in ScheduleListOptions.SearchAttributes field.

**Describe the solution you'd like**
Add ScheduleListOptions and do the filtering based on it in ScheduleClient.List API.

**Additional context**
Am I missing any other List schedules API?



---

### #5133: Failed workflow list for batch operations

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5133 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2023-11-18 00:16:08.000 UTC (2y 1m ago) |
| **Updated** | 2023-11-18 00:16:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
When running batch operations, especially batch reset. Operator will like to know the status of the job and if the operation fails for any workflow. And if the operation does fail, a list of those failed workflows.

**Describe the solution you'd like**
A way for operator to get the list of workflows on with the batch operation has failed. 

Today we log the failed workflowID, maybe that's enough. But if not, we need to another way:
- tag workflows with some search attributes, so that we can filter visibility later and get the list.
- Return failed workflow list as a result of the workflow.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #5063: Workflow Task Timeout not always respected

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5063 |
| **State** | OPEN |
| **Author** | Sushisource (Spencer Judge) |
| **Created** | 2023-11-01 18:04:00.000 UTC (2y 2m ago) |
| **Updated** | 2023-11-17 22:46:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | carlydf |
| **Milestone** | None |

#### Description

## Expected Behavior
When setting https://github.com/temporalio/api/blob/master/temporal/api/workflowservice/v1/request_response.proto#L158 I expect workflow tasks to always time out with the specified duration.

## Actual Behavior
Sometimes the timeout may not be respected. In particular, if the task is being processed on a sticky queue. I have attached a history where the execution started event clearly shows a 1s task timeout, but there is a task timeout that doesn't happen for 10 seconds: 
[a20228ec-d12b-43dc-91e7-f00350e3f2dc_events.json](https://github.com/temporalio/temporal/files/13230283/a20228ec-d12b-43dc-91e7-f00350e3f2dc_events.json)


## Steps to Reproduce the Problem
I want to say that I've seen this work properly on sticky queues before, but in my case I had to explicitly call reset on the sticky queue to get things to proceed quickly. In my test, there is an outstanding workflow update while the task times out. Perhaps it's a contributing factor.

This diff can be applied to the test added [here](https://github.com/temporalio/sdk-core/pull/624) to repro
```
diff --git a/tests/integ_tests/update_tests.rs b/tests/integ_tests/update_tests.rs
index 26254d40e..9eea536df 100644
--- a/tests/integ_tests/update_tests.rs
+++ b/tests/integ_tests/update_tests.rs
@@ -814,6 +814,7 @@ async fn task_failure_after_update() {
 async fn worker_restarted_in_middle_of_update() {
     let wf_name = "worker_restarted_in_middle_of_update";
     let mut starter = CoreWfStarter::new(wf_name);
+    starter.workflow_options.task_timeout = Some(Duration::from_secs(1));
     let mut worker = starter.worker().await;
     let client = starter.get_client().await;

@@ -886,10 +887,10 @@ async fn worker_restarted_in_middle_of_update() {
         // Allow it to start again, the second time
         BARR.wait().await;
         // Poke the workflow off the sticky queue to get it to complete faster than WFT timeout
-        client
-            .reset_sticky_task_queue(wf_id.clone(), run_id.clone())
-            .await
-            .unwrap();
+        // client
+        //     .reset_sticky_task_queue(wf_id.clone(), run_id.clone())
+        //     .await
+        //     .unwrap();
     };
     let run = async {
         // This run attempt will get shut down
```         

## Specifications

  - Version:
  - Platform:



---

### #5047: Config option to turn off "ignoring permission in unexpected format" error message

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5047 |
| **State** | OPEN |
| **Author** | andreclaro (Andr√©) |
| **Created** | 2023-10-28 18:53:38.000 UTC (2y 2m ago) |
| **Updated** | 2023-11-10 13:41:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

I would like to have an option to turn off the following warning message in the `default_jwt_claim_mapper.go`:
"ignoring permission in unexpected format..."

https://github.com/temporalio/temporal/blob/89c1b5fdfdcfd8da217623ddebb5e654efd7b831/common/authorization/default_jwt_claim_mapper.go#L111

**Describe the solution you'd like**
Add a configuration option to turn off this message as this error is expected in our setup.

Other option is to change it to debug level message.




---

### #5005: Schedules: Allow overriding search attributes when backfilling

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/5005 |
| **State** | OPEN |
| **Author** | tomasfarias (Tom√°s Far√≠as Santana) |
| **Created** | 2023-10-19 20:39:16.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-21 19:01:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, schedules |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

When backfilling a schedule, there is no way to track which Workflow Executions correspond to the backfill. This can make it confusing to understand whether the runs I'm seeing in the UI are part of a backfill or regularly scheduled runs, especially when overriding the ScheduleOverlapPolicy by setting `ALLOW_ALL` when issuing a ScheduleBackfill request.

**Describe the solution you'd like**

A potential solution to this problem would be to extend the ScheduleBackfill request to support overriding search attributes of the Workflow Executions that are part of the backfill. This way, I can create a bool `IsBackfill` search attribute, set it to `False` in the Schedule's action, but override it to `True` in the ScheduleBackfill request. I could go beyond that, develop my own backfill abstraction on top of Temporal*, and set a `BackfilledById` keyword search attribute when issuing a backfill.

[*]: This is exactly what we have done to deal with the Temporal buffer limit, which is a separate topic. We backfill _a lot_.

**Describe alternatives you've considered**

As far as I can tell, there is no other way to identify Workflow Executions that are part of a Schedule Backfill. A very imprecise solution could be built around tracking the start time of the backfill, but with an `ALLOW_ALL` ScheduleOverlapPolicy, I can't see how this would work.

**Additional context**

I **think** (haven't dug to much into the code) this could be implemented by extending the `addStart` function [here](https://github.com/temporalio/temporal/blob/068bd38be15cc9af896680e4012f988086a33a78/service/worker/scheduler/workflow.go#L904) to take a mapping of search attributes to add to `s.State.BufferedStarts`. Then it's just a matter of passing that all the way down to the [`startWorkflow` function](https://github.com/temporalio/temporal/blob/main/service/worker/scheduler/workflow.go#L1052). Basically extending this [`BufferedStart` message](https://github.com/temporalio/temporal/blob/068bd38be15cc9af896680e4012f988086a33a78/proto/internal/temporal/server/api/schedule/v1/message.proto#L38) to support search attributes, exactly as how overriding the overlap policy is supported. 

I may be off the mark, but happy to give it a shot at implementing this if 1) it's deemed a valid feature, 2) my initial intuition is correct.

Thank you!


---

### #4984: Schedules: Make it possible to list buffered executions

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4984 |
| **State** | OPEN |
| **Author** | mjameswh (James Watkins-Harvey) |
| **Created** | 2023-10-16 16:44:13.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-21 19:01:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, schedules |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

A user creates a schedules and sets `policy.overlap` to `ScheduleOverlapPolicy.BUFFER_ALL`. At some point in time, user now want to "see" the list of buffered executions (ie. executions that have been queued to be executed, one at a time, after the current workflow execution completes).

As it is now, buffered executions appears neither in `info.futureActionTimes`, `info.recentActions` nor `info.runningWorkflows`, and no other field hints that some executions are currently buffered.

**Describe the solution you'd like**

- Add a field `buffered_actions` to the `ScheduleInfo` ([code](https://github.com/temporalio/api/blob/135691242e9b4ed6214a7b5e1231c1c9930ff6c8/temporal/api/schedule/v1/message.proto#L325)) structure returned by the `DescribeSchedule` API.

  In its simplest form, that field could be defined as:
  ```
   repeated google.protobuf.Timestamp buffered_actions = 9 [(gogoproto.stdtime) = true];
  ```
  Arguably, it could be marginally more useful to make that field a collection of some new protobuf object, containing both the original schedule time of the buffered execution and a flag indicating how that execution got queued up (eg. the execution got queued up because the schedule time was reached, because of a backfill request, or because of a "trigger immediately" request).

- Similarly add a field `buffered_actions` to the `ScheduleListInfo` structure, returned by the `ListSchedules` API.


---

### #4979: Support waiting for Admitted stage on UpdateWorkflowExecution

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4979 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2023-10-13 20:26:06.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-13 20:26:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Describe the solution you'd like**

When setting `UpdateWorkflowExecutionRequest.wait_policy.lifecycle_stage` to `UPDATE_WORKFLOW_EXECUTION_LIFECYCLE_STAGE_ADMITTED`, the server errors with "Admitted is not implemented". We should support returning once the update is "admitted" (i.e. will not be lost but may not have reached worker yet)


---

### #4958: GetMutableStateRequest is used in poll mutable state 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4958 |
| **State** | OPEN |
| **Author** | yux0 (Yu Xia) |
| **Created** | 2023-10-11 17:50:12.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-11 17:50:12.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
GetMutableStateRequest is used in poll mutable state. 
code: https://github.com/temporalio/temporal/blob/747b30b454fedeb55fdbc52ce8ad9bfe43e99adb/service/history/api/get_workflow_util.go#L48

**Describe the solution you'd like**
GetMutableState should just return the current version of mutable state, no branch token nor version related data is required in the request.
PollMutableState contains a token which may include the version history item data and the corresponding branch token

**Describe alternatives you've considered**

**Additional context**



---

### #4957: CurrentBranchChanged Error should use event id + version as branch id

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4957 |
| **State** | OPEN |
| **Author** | yux0 (Yu Xia) |
| **Created** | 2023-10-11 16:15:13.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-13 21:48:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
CurrentBranchChanged Error should use event id + version as branch id

## Actual Behavior
CurrentBranchChanged now use branch token bytes as branch id



---

### #4934: The Rate and Burst functions of DynamicRateLimiterImpl are unsafe

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4934 |
| **State** | OPEN |
| **Author** | MichaelSnowden (Michael Snowden) |
| **Created** | 2023-10-04 16:04:51.000 UTC (2y 2m ago) |
| **Updated** | 2023-10-04 16:04:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
When you use `DynamicRateLimiter` as just a `RateBurst`, its rate and burst quotas should update dynamically after the refresh interval. 

## Actual Behavior
They don't.

## Steps to Reproduce the Problem

  1. Create a dynamic rate limiter
  1. Update its rate
  1. Wait the refresh duration
  1. Call Rate() or Burst() (notice that they don't update)

## Specifications
More context here: https://github.com/temporalio/temporal/pull/4933

  - Version: 1.22 and below
  - Platform: all



---

### #4870: Allow static JWKS for `jwtKeyProvider`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4870 |
| **State** | OPEN |
| **Author** | smndtrl (Simon) |
| **Created** | 2023-09-13 06:55:15.000 UTC (2y 3m ago) |
| **Updated** | 2023-09-29 21:43:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
In our internal setup, temporal is isolated and cannot reach out to our externally hosted IDP and get the keys necessary for authz.

**Describe the solution you'd like**
Add a `jwkStaticKeySource` string that can contain the keys and would be equal to the response of the JWKS endpoint. `jwkStaticKeySource` could/should be loaded via dynamic config as well so that key rotation can be implemented easily



---

### #4749: Support listening on both IPv4 and IPv6 networks in dual stack environments

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4749 |
| **State** | OPEN |
| **Author** | underrun (J Derek Wilson) |
| **Created** | 2023-08-09 16:20:19.000 UTC (2y 4m ago) |
| **Updated** | 2023-08-09 16:20:19.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Migrating to ipv6 is hard but will (probably?) eventually happen. To make this transition easier for operators, it would be advantageous for operators to be able to use either/both ipv4 and ipv6 to connect to temporal server services at the same time.

**Describe the solution you'd like**
when starting temporal server services, by default, if an external ipv4 and ipv6 address are available, bind to both using the same port.

Alter services rpc config `BindOnLocalHost` to bind on both `127.0.0.1` and `::1` if both are available.

Add some method to services rpc config to allow specifying multiple IPs to bind to (could be one v4 and one v6, or just N addresses to support multihomed environments).

**Describe alternatives you've considered**
It may be possible to run multiple of the same service on a host with both IPv4 and IPv6, one bound to IPv4 and one bound to IPv6, though that would probably be more resource intensive than enabling one process listening on multiple addresses.

**Additional context**
N/A


---

### #4745: Replicate search attributes together with namespace

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4745 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2023-08-08 23:01:05.000 UTC (2y 4m ago) |
| **Updated** | 2023-08-08 23:07:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
When an existing namespace is replicated to a new cluster (by updating namespace's clusters list), search attributes registered for that namespace on the current cluster won't be replicated/created on the new cluster. 

**Describe the solution you'd like**
Replicate custom search attributes as part of namespace replication.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #4703: Add a list of datetimes to `ScheduleSpec`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4703 |
| **State** | OPEN |
| **Author** | lorensr (Loren ‚ò∫Ô∏è) |
| **Created** | 2023-07-28 21:42:55.000 UTC (2y 5m ago) |
| **Updated** | 2023-10-21 19:09:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, schedules |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Been requested by a few users, and I think is on internal roadmap. Opening this issue for people to follow.

https://api-docs.temporal.io/#temporal.api.schedule.v1.ScheduleSpec


---

### #4638: History Queue Deletion & Life Cycle

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4638 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2023-07-17 22:22:56.000 UTC (2y 5m ago) |
| **Updated** | 2023-07-17 22:22:56.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

Our history queue logic supports creating new history queue by simply define a new category and task execution logics. However, there's no good story for deleting or stop using an existing queue due to the fact that there might always be some backlogs in the database.

Today, simply skipping task category registration on service start up will result in an undefined behavior:
- In 1.21, the server will panic if shard info sees a task categoryID but can't find the corresponding task category definition. (This is actually preferred now since it won't result in task loss and will be more explicit signal on mis-configuration)
- In master branch, server won't panic but queue won't start and the data will be left in shard info metadata.

We should have a proper design for history queue deprecation. This could be useful for disabling archival queue and in the future if more history queues needs to be dynamically created and deleted.

**Describe the solution you'd like**


**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #4435: Deprecate old archival logic

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4435 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2023-06-02 21:08:13.000 UTC (2y 7m ago) |
| **Updated** | 2023-06-02 21:32:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | refactoring |
| **Assignees** | MichaelSnowden |
| **Milestone** | None |

#### Description

Durable archival is enabled by default in 1.20, we should be able to deprecate the old code path (workflow archival).


---

### #4428: Support fixed deadline timers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4428 |
| **State** | OPEN |
| **Author** | scmorse (Stephen Morse) |
| **Created** | 2023-06-01 12:44:30.000 UTC (2y 7m ago) |
| **Updated** | 2023-06-02 21:37:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, feature-request |
| **Assignees** | None |
| **Milestone** | None |

#### Description

### Is your feature request related to a problem? Please describe.

If a temporal client schedules an action to occur 10s later, e.g. `Workflow.await('10s')`, but there is 1s of delay in network travel time and/or persistence lag, then the net effect is an 11s delay when the client only specified 10s. This effect isn't terribly noticeable until network delay or persistence lag spikes.

### Describe the solution you'd like

I would like to be able to specify the deadline of a timer as an absolute timestamp / ISO string, rather than a relative delay. Then the 1s lost in network transit / persistence lag would not affect the time that the workflow is scheduled to be resumed.

**_This is actually a win for the developer experience as well_**, since in some situations we start by knowing the target time we would like to resume our workflow, and the calculation to determine the delay between now() and the target time is unnecessary / cumbersome.

### Describe alternatives you've considered

None.

### Additional context

Originally discussed in this Temporal [Slack thread üßµ](https://temporalio.slack.com/archives/CTT84KXK9/p1685563043149169).



---

### #4171: Add metrics when history service writes activity task failed to even history

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4171 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2023-04-14 21:03:08.000 UTC (2y 8m ago) |
| **Updated** | 2023-04-14 21:03:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

https://github.com/temporalio/temporal/blob/master/service/history/api/respondactivitytaskfailed/api.go#L124-L131


It would be imo useful to add counter metric here so that user can track how often service has failed activity for retries exhausted


---

### #4149: ‚Äã Version v1.20.1 had slow query in temporal_visibility

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4149 |
| **State** | OPEN |
| **Author** | Huangsir |
| **Created** | 2023-04-10 03:37:10.000 UTC (2y 8m ago) |
| **Updated** | 2023-04-10 03:48:45.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

After upgrading temporal to version 1.20.1, I found that there was a slow log when querying temporal_visibility.
After running for a while, "temporal-history" started to frequently report the error "context deadline exceeded", andMySQL started to report SlowLogs.

```
{"level":"error","ts":"2023-04-07T07:12:33.752Z","msg":"Operation failed with an error.","error":"context deadline exceeded","logging-call-at":"visiblity_manager_metrics.go:258","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/home/builder/temporal/common/log/zap_logger.go:150\ngo.temporal.io/server/common/persistence/visibility.(*visibilityManagerMetrics).updateErrorMetric\n\t/home/builder/temporal/common/persistence/visibility/visiblity_manager_metrics.go:258\ngo.temporal.io/server/common/persistence/visibility.(*visibilityManagerMetrics).RecordWorkflowExecutionClosed\n\t/home/builder/temporal/common/persistence/visibility/visiblity_manager_metrics.go:102\ngo.temporal.io/server/service/history.(*visibilityQueueTaskExecutor).recordCloseExecution\n\t/home/builder/temporal/service/history/visibilityQueueTaskExecutor.go:443\ngo.temporal.io/server/service/history.(*visibilityQueueTaskExecutor).processCloseExecution\n\t/home/builder/temporal/service/history/visibilityQueueTaskExecutor.go:392\ngo.temporal.io/server/service/history.(*visibilityQueueTaskExecutor).Execute\n\t/home/builder/temporal/service/history/visibilityQueueTaskExecutor.go:120\ngo.temporal.io/server/service/history/queues.(*executableImpl).Execute\n\t/home/builder/temporal/service/history/queues/executable.go:211\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).executeTask.func1\n\t/home/builder/temporal/common/tasks/fifo_scheduler.go:231\ngo.temporal.io/server/common/backoff.ThrottleRetry.func1\n\t/home/builder/temporal/common/backoff/retry.go:175\ngo.temporal.io/server/common/backoff.ThrottleRetryContext\n\t/home/builder/temporal/common/backoff/retry.go:199\ngo.temporal.io/server/common/backoff.ThrottleRetry\n\t/home/builder/temporal/common/backoff/retry.go:176\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).executeTask\n\t/home/builder/temporal/common/tasks/fifo_scheduler.go:241\ngo.temporal.io/server/common/tasks.(*FIFOScheduler[...]).processTask\n\t/home/builder/temporal/common/tasks/fifo_scheduler.go:217"}

```

This SQL statement was obtained from ‚Äúshow processlist‚Äù.

```sql
SELECT ev.namespace_id, ev.run_id, ev.workflow_type_name, ev.workflow_id, ev.start_time, ev.execution_time, ev.status, ev.close_time, ev.history_length, ev.memo, ev.encoding, ev.task_queue, ev.search_attributes FROM executions_visibility ev LEFT JOIN custom_search_attributes USING (namespace_id, run_id) WHERE namespace_id = 'f458436c-e0b6-47d6-ac5d-a633ef7d84bb' AND TemporalNamespaceDivision is null ORDER BY coalesce(close_time, cast('9999-12-31 23:59:59' as datetime)) DESC, start_time DESC, run_id LIMIT 1000
```

There are approximately 3 million rows of data under this namespace.


## Expected Behavior
‚Äã
Based on the index involvement in the table structure, MySQL should use the "default_idx" index. and this is the execution result after I forced the use of the "default_idx" index.

```sql
mysql> explain analyze SELECT ev.namespace_id, ev.run_id, ev.workflow_type_name, ev.workflow_id, ev.start_time, ev.execution_time, ev.status, ev.close_time, ev.history_length, ev.memo, ev.encoding, ev.task_queue, ev.search_attributes FROM executions_visibility ev USE INDEX(default_idx) LEFT JOIN custom_search_attributes USING (namespace_id, run_id) WHERE namespace_id = 'f458436c-e0b6-47d6-ac5d-a633ef7d84bb' AND TemporalNamespaceDivision is null ORDER BY coalesce(close_time, cast('9999-12-31 23:59:59' as datetime)) DESC, start_time DESC, run_id LIMIT 1000\G;
*************************** 1. row ***************************
EXPLAIN: -> Limit: 1000 row(s)  (cost=115154.45 rows=1000) (actual time=0.073..18.884 rows=1000 loops=1)
    -> Nested loop left join  (cost=115154.45 rows=154992) (actual time=0.072..18.796 rows=1000 loops=1)
        -> Filter: ((ev.namespace_id = 'f458436c-e0b6-47d6-ac5d-a633ef7d84bb') and (ev.TemporalNamespaceDivision is null))  (cost=60907.21 rows=154992) (actual time=0.057..12.063 rows=1000 loops=1)
            -> Index lookup on ev using default_idx (namespace_id='f458436c-e0b6-47d6-ac5d-a633ef7d84bb')  (cost=60907.21 rows=1549921) (actual time=0.054..11.499 rows=1000 loops=1)
        -> Filter: (custom_search_attributes.namespace_id = 'f458436c-e0b6-47d6-ac5d-a633ef7d84bb')  (cost=0.25 rows=1) (actual time=0.007..0.007 rows=0 loops=1000)
            -> Single-row covering index lookup on custom_search_attributes using PRIMARY (namespace_id='f458436c-e0b6-47d6-ac5d-a633ef7d84bb', run_id=ev.run_id)  (cost=0.25 rows=1) (actual time=0.006..0.006 rows=0 loops=1000)

1 row in set (0.02 sec)


mysql> explain SELECT ev.namespace_id, ev.run_id, ev.workflow_type_name, ev.workflow_id, ev.start_time, ev.execution_time, ev.status, ev.close_time, ev.history_length, ev.memo, ev.encoding, ev.task_queue, ev.search_attributes FROM executions_visibility ev USE INDEX(default_idx) LEFT JOIN custom_search_attributes USING (namespace_id, run_id) WHERE namespace_id = 'f458436c-e0b6-47d6-ac5d-a633ef7d84bb' AND TemporalNamespaceDivision is null ORDER BY coalesce(close_time, cast('9999-12-31 23:59:59' as datetime)) DESC, start_time DESC, run_id LIMIT 1000\G;
*************************** 1. row ***************************
           id: 1
  select_type: SIMPLE
        table: ev
   partitions: NULL
         type: ref
possible_keys: default_idx
          key: default_idx
      key_len: 256
          ref: const
         rows: 1549921
     filtered: 10.00
        Extra: Using where
*************************** 2. row ***************************
           id: 1
  select_type: SIMPLE
        table: custom_search_attributes
   partitions: NULL
         type: eq_ref
possible_keys: PRIMARY,by_bool_01,by_bool_02,by_bool_03,by_datetime_01,by_datetime_02,by_datetime_03,by_double_01,by_double_02,by_double_03,by_int_01,by_int_02,by_int_03,by_keyword_01,by_keyword_02,by_keyword_03,by_keyword_04,by_keyword_05,by_keyword_06,by_keyword_07,by_keyword_08,by_keyword_09,by_keyword_10,by_keyword_list_01,by_keyword_list_02,by_keyword_list_03
          key: PRIMARY
      key_len: 512
          ref: const,temporal_visibility.ev.run_id
         rows: 1
     filtered: 100.00
        Extra: Using where; Using index
2 rows in set, 1 warning (0.01 sec)
```

## Actual Behavior

MySQL did not use the "default_idx" index as I forced, instead, it used the PrimaryKey as index

```sql

mysql> explain SELECT ev.namespace_id, ev.run_id, ev.workflow_type_name, ev.workflow_id, ev.start_time, ev.execution_time, ev.status, ev.close_time, ev.history_length, ev.memo, ev.encoding, ev.task_queue, ev.search_attributes FROM executions_visibility ev LEFT JOIN custom_search_attributes USING (namespace_id, run_id) WHERE namespace_id = 'f458436c-e0b6-47d6-ac5d-a633ef7d84bb' AND TemporalNamespaceDivision is null ORDER BY coalesce(close_time, cast('9999-12-31 23:59:59' as datetime)) DESC, start_time DESC, run_id LIMIT 1000\G;
*************************** 1. row ***************************
           id: 1
  select_type: SIMPLE
        table: ev
   partitions: NULL
         type: ref
possible_keys: PRIMARY,default_idx,by_execution_time,by_workflow_id,by_workflow_type,by_status,by_history_length,by_task_queue,by_temporal_change_version,by_binary_checksums,by_batcher_user,by_temporal_scheduled_start_time,by_temporal_scheduled_by_id,by_temporal_schedule_paused,by_temporal_namespace_division
          key: PRIMARY
      key_len: 256
          ref: const
         rows: 1549921
     filtered: 10.00
        Extra: Using where; Using filesort
*************************** 2. row ***************************
           id: 1
  select_type: SIMPLE
        table: custom_search_attributes
   partitions: NULL
         type: eq_ref
possible_keys: PRIMARY,by_bool_01,by_bool_02,by_bool_03,by_datetime_01,by_datetime_02,by_datetime_03,by_double_01,by_double_02,by_double_03,by_int_01,by_int_02,by_int_03,by_keyword_01,by_keyword_02,by_keyword_03,by_keyword_04,by_keyword_05,by_keyword_06,by_keyword_07,by_keyword_08,by_keyword_09,by_keyword_10,by_keyword_list_01,by_keyword_list_02,by_keyword_list_03
          key: PRIMARY
      key_len: 512
          ref: const,temporal_visibility.ev.run_id
         rows: 1
     filtered: 100.00
        Extra: Using where; Using index
2 rows in set, 1 warning (0.01 sec)

mysql> explain analyze SELECT ev.namespace_id, ev.run_id, ev.workflow_type_name, ev.workflow_id, ev.start_time, ev.execution_time, ev.status, ev.close_time, ev.history_length, ev.memo, ev.encoding, ev.task_queue, ev.search_attributes FROM executions_visibility ev LEFT JOIN custom_search_attributes USING (namespace_id, run_id) WHERE namespace_id = 'f458436c-e0b6-47d6-ac5d-a633ef7d84bb' AND TemporalNamespaceDivision is null ORDER BY coalesce(close_time, cast('9999-12-31 23:59:59' as datetime)) DESC, start_time DESC, run_id LIMIT 1000\G;
*************************** 1. row ***************************
EXPLAIN: -> Limit: 1000 row(s)  (cost=210064.29 rows=1000) (actual time=20665.083..20675.554 rows=1000 loops=1)
    -> Nested loop left join  (cost=210064.29 rows=1549921) (actual time=20665.081..20675.468 rows=1000 loops=1)
        -> Sort: coalesce(close_time,cast(_utf8mb4'9999-12-31 23:59:59' as datetime)) DESC, ev.start_time DESC, ev.run_id  (cost=16324.17 rows=1549921) (actual time=20665.033..20665.540 rows=1000 loops=1)
            -> Filter: ((ev.namespace_id = 'f458436c-e0b6-47d6-ac5d-a633ef7d84bb') and (ev.TemporalNamespaceDivision is null))  (actual time=0.038..8314.438 rows=3205388 loops=1)
                -> Index lookup on ev using PRIMARY (namespace_id='f458436c-e0b6-47d6-ac5d-a633ef7d84bb')  (actual time=0.035..6668.868 rows=3205388 loops=1)
        -> Filter: (custom_search_attributes.namespace_id = 'f458436c-e0b6-47d6-ac5d-a633ef7d84bb')  (cost=0.25 rows=1) (actual time=0.010..0.010 rows=0 loops=1000)
            -> Single-row covering index lookup on custom_search_attributes using PRIMARY (namespace_id='f458436c-e0b6-47d6-ac5d-a633ef7d84bb', run_id=ev.run_id)  (cost=0.25 rows=1) (actual time=0.010..0.010 rows=0 loops=1000)

1 row in set (20.96 sec)

```

It took 20 seconds to execute the SQL query.

## Steps to Reproduce the Problem

  1. Upgrade to v1.20.1
  1. About 300w rows in temporal_visibility
  1. Run temporal
  2. Execute `show processlist` in MySQL 

## Specifications

  - Version: Temporal: v1.20.1,  MySQL: 8.0.29
  - Platform: K8S



---

### #4117: Take additional factors into account for suggesting continue-as-new

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4117 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2023-03-29 16:06:21.000 UTC (2y 9m ago) |
| **Updated** | 2023-03-29 16:06:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

The recently added mechanism for the server to suggest workflow continue-as-new is based on history event length and byte size.
Users can hit other limits such as number of signals received and scheduled activities, the server suggestion should take those factors into account to help users out.

See for reference: https://github.com/temporalio/api/blob/ae312b0724003957b96fb966e3fe25a02abaade4/temporal/api/history/v1/message.proto#L180


---

### #4059: System search attributes to find workflows that are backoff retrying an activity 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4059 |
| **State** | OPEN |
| **Author** | longquanzheng (Quanzheng Long) |
| **Created** | 2023-03-16 15:07:05.000 UTC (2y 9m ago) |
| **Updated** | 2023-03-16 15:07:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
It‚Äôs hard to find out those workflows . The only way is through logs but sometimes logs are too noisy to read and search when production is running in outrage.

**Describe the solution you'd like**
Provide system search attributes like historyLength, eg retryingActivityType to help searching in webUI

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #4052: Non-expired JWT rejected as expired by Temporal Frontend

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4052 |
| **State** | OPEN |
| **Author** | sstro |
| **Created** | 2023-03-14 08:50:24.000 UTC (2y 9m ago) |
| **Updated** | 2023-03-14 08:50:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

JWT having valid iat/nbf/exp is accepted by the Temporal Frontend server and the request is authenticated.

## Actual Behavior

JWT having valid iat/nbf/exp is rejected by the server with the ‚ÄúToken is expired‚Äù message.

Log excerpt:

'temporal-frontend {‚Äúlevel‚Äù:‚Äúerror‚Äù,‚Äúts‚Äù:‚Äú2023-02-22T08:12:38.955Z‚Äù,‚Äúmsg‚Äù:‚ÄúAuthorization error‚Äù,‚Äúerror‚Äù:‚ÄúToken is expired‚Äù,‚Äúlogging-call-at‚Äù:‚Äúinterceptor.go:169‚Äù,‚Äústacktrace‚Äù:‚Äú[go.temporal.io/server/common/log.(*zapLogger).Error](http://go.temporal.io/server/common/log.(*zapLogger).Error)\n\t/home/builder/temporal/common/log/zap_logger.go:144\ngo.temporal.io/server/common/authorization.(*interceptor).logAuthError\n\t/home/builder/temporal/common/authorization/interceptor.go:169\ngo.temporal.io/server/common/authorization.(*interceptor).Interceptor\n\t/home/builder/temporal/common/authorization/interceptor.go:115\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:1165\ngo.temporal.io/server/common/rpc/interceptor.(*TelemetryInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/telemetry.go:142\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:1165\ngo.temporal.io/server/common/metrics.NewServerMetricsContextInjectorInterceptor.func1\n\t/home/builder/temporal/common/metrics/grpc.go:66\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:1165\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc.UnaryServerInterceptor.func1\n\t/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.36.1/interceptor.go:352\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:1165\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceLogInterceptor).Intercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_logger.go:84\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:1165\ngo.temporal.io/server/common/rpc/interceptor.(*NamespaceValidatorInterceptor).LengthValidationIntercept\n\t/home/builder/temporal/common/rpc/interceptor/namespace_validator.go:103\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:1165\ngo.temporal.io/server/common/rpc.ServiceErrorInterceptor\n\t/home/builder/temporal/common/rpc/grpc.go:137\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1.1\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:1165\ngoogle.golang.org/grpc.chainUnaryInterceptors.func1\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:1167\ngo.temporal.io/api/workflowservice/v1._WorkflowService_ListNamespaces_Handler\n\t/go/pkg/mod/go.temporal.io/api@v1.13.1-0.20221110200459-6a3cb21a3415/workflowservice/v1/service.pb.go:1410\ngoogle.golang.org/grpc.(*Server).processUnaryRPC\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:1340\ngoogle.golang.org/grpc.(*Server).handleStream\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:1713\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.2\n\t/go/pkg/mod/google.golang.org/grpc@v1.50.1/server.go:965‚Äù}'

## Steps to Reproduce the Problem

  1. Deploy Temporal Server with TEMPORAL_AUTH_AUTHORIZER=default, TEMPORAL_AUTH_CLAIM_MAPPER= default.
  2. Run a worker and initiate connection to the Temporal Server, sending JWT with iat=nbf=<5 minutes back in the past from now>, exp=<24 hours + now> in the Authorization header.

Interestingly, a token with iat=nbf=<5 minutes back in the past from now>, exp=<*1 hour* + now> is accepted.

Apparently, the ‚Äúlong‚Äù token validity of 24 hours is what confuses the validator. Unfortunately, we do not control validity of the token (set by auth provider - Azure Active Directory).




---

### #4045: Expose current worker identity for started pending activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4045 |
| **State** | OPEN |
| **Author** | zedongh (zedongh) |
| **Created** | 2023-03-11 06:04:51.000 UTC (2y 9m ago) |
| **Updated** | 2023-03-11 06:04:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
No.

**Describe the solution you'd like**
Expose more info from server internal pending activty info (StartedIdentity).
For long running task, it's nice to know which worker picked current task.

**Describe alternatives you've considered**
No.

**Additional context**
[same cadence feature request](https://github.com/uber/cadence/issues/5058)


---

### #4030: Query at a specific WorkflowTaskStarted eventId

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4030 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2023-03-08 16:11:54.000 UTC (2y 9m ago) |
| **Updated** | 2023-03-08 18:36:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
For debugging purposes, it is useful to know a query result at a specific point of workflow execution.

As a separate feature, It would be nice to be able to get a query result at each WorkflowTaskStarted event as a single API call. This would be very helpful in a certain troubleshooting scenarios.

**Describe the solution you'd like**
Add "WorkflowTaskStartedEventId" argument to query request. The workflow should be replayed up to that event id and a query should be executed against it. 

Add a new API that returns a list of query results. One result per WorkflowTaskStarted eventId.

**Additional context**
This would allow querying terminated workflows safely.




---

### #4029: Clear signal requestID on workflow close

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4029 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2023-03-08 02:15:00.000 UTC (2y 9m ago) |
| **Updated** | 2023-03-10 22:50:24.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, discussion, up-for-grabs |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Signal requestID is for dedupping signal request. After workflow close, all signal requests will be rejected, so we don't really need signal requestIDs for dedupping.

**Describe the solution you'd like**

* Clear signal requestID on workflow close
* However, we may need it for https://github.com/temporalio/temporal/issues/4028. If that's the case, we can persist the requestID as part of the signaled event, so that upon reset, the signal requestID map can be rebuilt.

**Describe alternatives you've considered**

* We don't have to do it if we believe we won't benefit much from making this change. 

**Additional context**
Add any other context or screenshots about the feature request here.




---

### #4028: Signal requestID dropped upon workflow reset

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/4028 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2023-03-08 02:10:31.000 UTC (2y 9m ago) |
| **Updated** | 2023-08-10 22:25:28.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | dandavison |
| **Milestone** | None |

#### Description

## Expected Behavior
I am not sure if this is expected or not, but at least existing behavior looks weird to me

* On workflow reset, signal got picked to the new run together with corresponding signal requestID so that dedup can still work.

## Actual Behavior
* After reset, signal event got picked but not signal request ID.

## Steps to Reproduce the Problem
1. Change tctl code to include a requestID when sending signals
1. Start a workflow
1. Use the modified tctl to signal it several time.
1. Terminate the workflow started in 2
1. Run `tctl admin desc` command, `signalRequestedIds` field is shown.
1. Reset the workflow to first workflow task (any reset type will do I think, but I haven't verified)
1. Run `tctl admin desc` command for the new run, there's no `signalRequestedIds` field.

## Specifications
* Version: master
* Platform:




---

### #3990: Blacklisting workers by version and identity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3990 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2023-02-25 00:27:55.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-03 18:02:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Sometimes due to a deployment mistake old worker can connect to a production cluster. This can cause all sorts of issues.
Sometimes it can take a long time to locate and shutdown such a worker. 

**Describe the solution you'd like**
Provide an operator API to blacklist workers that have a specific version from connecting, as well as blacklist specific workers by their identity string. We should consider other properties like IP address ranges.




---

### #3988: Add query result caching

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3988 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2023-02-23 18:27:18.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-03 18:02:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
In some scenarios, many users need frequently query a current workflow state. For example, 100 developers are watching for a CI/CD pipeline status.
Most of the query results are the same, so there is no need to recalculate them unless workflow returns a different value. 

**Describe the solution you'd like**
Add caching of query results. Update the cache only if the query result is updated as part of the last workflow task.

**Describe alternatives you've considered**
There is a proposal to create a long poll query. But I can see the need for query optimization when a long poll is not possible. I think these features can be implemented or at least designed together.




---

### #3912: Matching with Cassandra encounters lots of tombstone for expired tasks

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3912 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2023-02-07 18:18:17.000 UTC (2y 10m ago) |
| **Updated** | 2023-03-03 18:02:12.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

Sample stack encountered by user:

```
temporal-frontend-prod-69b84468bf-vlfwl {
  "level": "error",
  "ts": "2023-02-05T16:44:30.785Z",
  "msg": "uncategorized error",
  "service": "frontend",
  "operation": "StartWorkflowExecution",
  "wf-namespace": "tempo-payment-v4",
  "error": "operation CreateWorkflowExecution encounter Operation timed out - received only 1 responses.",
  "l ‚îÇ
  ‚îÇ
  ogging-call-at
  ":"
  telemetry.go: 190
  ","
  stacktrace
  ":"
  "go.temporal.io/server/common/log"
}
```

Slack thread: https://temporalio.slack.com/archives/CTTJCPZQE/p1675744969688159




---

### #3700: Archival - add ExecutionStatus to archival visibility syntax

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3700 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2022-12-08 20:47:28.000 UTC (3 years ago) |
| **Updated** | 2023-03-03 20:17:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently s3, gcloud archivers state following visibility is possible:
WorkflowType String
WorkflowID String
StartTime Date
CloseTime Date
SearchPrecision String - Day, Hour, Minute, Second

Please add ExecutionStatus to this so finding archived wf status does not have to be done by scanning workflow history.



---

### #3688: Read only workflow worker

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3688 |
| **State** | OPEN |
| **Author** | kkcmadhu |
| **Created** | 2022-12-03 06:14:48.000 UTC (3 years ago) |
| **Updated** | 2023-03-03 20:17:21.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
There are times when i want my sysetm not to process workflows/signals (say during maintenance period)
However, i would  like workflow queries to still be processed during this maintenace period

**Describe the solution you'd like**
I am thinking of  a read only worker which can process /respond to queryes but will not process activity/siginals/workflows.

**Describe alternatives you've considered**
Custom code, in my workflow , keeping all workflows in a timer event usin workflow.await()


I had some queries regarding this in community and @mfateev  advised to raise a Feature Request.
https://community.temporal.io/t/read-only-workflow-worker/6635



---

### #3597: Add a Cancellation Request status to DescribeWorkflowExecution API

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3597 |
| **State** | OPEN |
| **Author** | rossedfort (Ross Edfort) |
| **Created** | 2022-11-15 17:35:30.000 UTC (3y 1m ago) |
| **Updated** | 2023-03-03 20:17:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, P1 |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Cancelling a workflow is a non-deterministic operation that could take quite a while to complete - or possibly never complete at all.  Currently when I create a Cancel request for a given workflow, that status is not reflected in the workflow itself. 

**Describe the solution you'd like**
A new value in the existing `workflowExecutionInfo.status` field that reflects whether a cancel request has been created

**Describe alternatives you've considered**
Polling the workflow from the UI is not feasible as we don't know when the operation will complete, if at all.

**Additional context**


---

### #3590: GRPC_SSL_CIPHER_SUITES not respected

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3590 |
| **State** | OPEN |
| **Author** | badideasforsale |
| **Created** | 2022-11-14 19:33:48.000 UTC (3y 1m ago) |
| **Updated** | 2023-10-12 17:20:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug, P1 |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
When `GRPC_SSL_CIPHER_SUITES` environment variable is set, those are the only ciphers the server will use.


## Actual Behavior
The default ciphers are still used

## Steps to Reproduce the Problem

  1. Set `GRPC_SSL_CIPHER_SUITES=TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-CHACHA20-POLY1305`
  2. Run `sslscan` against the gRPC endpoint
  3. Observe results

## Specifications

  - Version: 1.18.4
  - Platform:

[See user report](https://community.temporal.io/t/temporal-frontend-identified-with-weak-vulnerable-ciphers/6441)



---

### #3543: Better memory management for zap logger

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3543 |
| **State** | OPEN |
| **Author** | nagl-temporal (Nathan Glass) |
| **Created** | 2022-10-29 01:02:35.000 UTC (3y 2m ago) |
| **Updated** | 2023-03-03 20:18:05.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | yiminc |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
The `zap` framework can consume an unbounded amount of memory, and often consumes more memory than it "needs to" due to naive use of `sync.Pool`.

For context, see [this golang issue](https://github.com/golang/go/issues/23199) for a discussion of best (and worst!) practices around `sync.Pool`. Zap follows the bad practice of [putting variable-sized buffers in its pool](https://github.com/uber-go/zap/issues/1130).

I've seen this cause bad behavior in temporal when it is frequently logging small lines but infrequently logging large lines. This pattern leads to an unnecessarily large zap buffer pool (in practice, I've seen up to 2.5gb). The golang issue discusses this scenario specifically [here](https://github.com/golang/go/issues/23199#issuecomment-353193866).

**Describe the solution you'd like**
See below - it's nothing but alternatives!

**Describe alternatives you've considered**
There's a lot we could do about this. Off the top of my head:
* Dedup recent logs with long lines (e.g. ones with stack traces)
* Commit any/several of the suggestions from the golang issue to `zap`:
  * Sometimes don't return large buffers to the pool.
  * Use a tiered pool (small vs. large).
  * Implement a real upper bound on pool size (and don't return buffers over that bound to the pool).


---

### #3536: Allow Admin DeleteWorkflowExecution Delete by namespace ID

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3536 |
| **State** | OPEN |
| **Author** | yux0 (Yu Xia) |
| **Created** | 2022-10-26 19:03:03.000 UTC (3y 2m ago) |
| **Updated** | 2023-03-03 20:18:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Admin DeleteWorkflowExecution currently takes in namespace. After a namespace is deleted, we don't have a way to delete data if there is leftover data. 

**Describe the solution you'd like**
Admin DeleteWorkflowExecution takes in namespace ID to delete workflow data

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #3453: Build docker images with integer UID/GID and use the same one on all images

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3453 |
| **State** | OPEN |
| **Author** | skandragon (Michael Graff) |
| **Created** | 2022-10-04 01:57:56.000 UTC (3y 2m ago) |
| **Updated** | 2023-10-12 17:20:47.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug, P1 |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

Be able to deploy in kubernetes in a restricted environment without a lot of work.

## Actual Behavior

It's a bit of work.

## Steps to Reproduce the Problem

In a restricted world, I need to define some characteristics of the container.  Kubernetes can query the image and detect if it runs as non-root, for instance, but not if the UID is a username and not a UID.

My current workaround is more verbose than needed:

```
securityContext:
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  capabilities:
    drop: ["ALL"]
  seccompProfile:
    type: RuntimeDefault
  runAsUser: 1000
  runAsGroup: 1000
```

If the UIDs were not 0, I could leave off at least the last two items.

Also, the web container uses 5000 for the temporal user...  which is likely an unneeded difference.

## Specifications

  - Version:  1.18.0
  - Platform: Any (kubernetes)



---

### #3436: Integration test for metrics

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3436 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2022-09-28 01:02:44.000 UTC (3y 3m ago) |
| **Updated** | 2023-03-03 20:17:56.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | yux0 |
| **Milestone** | None |

#### Description

Our current integration test cannot verify certain metrics are emitted as expected. 
We need to build that ability and add integration test to cover some key metrics. For example, prevent regression that cause "action" metrics to disappear.


---

### #3435: Add API for worker to signal that it is shutting down

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3435 |
| **State** | OPEN |
| **Author** | bergundy (Roey Berman) |
| **Created** | 2022-09-27 21:21:33.000 UTC (3y 3m ago) |
| **Updated** | 2023-03-03 20:17:55.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently the main thing workers would use this API for is to reset the sticky task queue for any tasks on that queue.
While it is understand it might be a considerable effort and is unlikely to be picked up anytime soon, I'd like to leave this here for future reference.


---

### #3381: Causal consistency guarantee in replication

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3381 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2022-09-14 15:39:22.000 UTC (3y 3m ago) |
| **Updated** | 2023-03-03 20:17:52.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | yux0 |
| **Milestone** | None |

#### Description

Current replication is done at shard level, and each shard replicate independently. This could result in causal inconsistency issue after failover. To address that, replication need to be aware of all cross shard events, and guarantee their order before apply  events.


---

### #3367: Single active timer task per workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3367 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2022-09-12 15:59:59.000 UTC (3y 3m ago) |
| **Updated** | 2023-03-03 20:17:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently, we have multiple timer tasks per one workflow:
* one for user timers
* one for activities
* one for workflow task
* one for workflow execution

Ideally, we should only create one next timer for any kind, and create next in line after current one fires.
Also need the ability to delete timer if new timer created is earlier than current one.



---

### #3296: Ensure ordering of child-workflow-sent signal and child workflow complete to the same workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3296 |
| **State** | OPEN |
| **Author** | cretz (Chad Retz) |
| **Created** | 2022-09-01 21:11:22.000 UTC (3y 4m ago) |
| **Updated** | 2023-03-03 20:18:49.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | yycptt |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

We don't have any guarantees that a signal sent from a child to its parent happens before child workflow completion event.

**Describe the solution you'd like**

Provide a guarantee.


---

### #3288: Add API for setting task queue rate limiting

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3288 |
| **State** | OPEN |
| **Author** | lorensr (Loren ‚ò∫Ô∏è) |
| **Created** | 2022-09-01 03:09:32.000 UTC (3y 4m ago) |
| **Updated** | 2023-03-03 20:18:48.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

A user wants to dynamically change `maxTaskQueueActivitiesPerSecond`. Currently, in order to do so, they need to redeploy all Workers that are on that queue:

![image](https://user-images.githubusercontent.com/251288/187823547-0a77e3b1-5232-4ec7-94cf-38b61094fe81.png)

**Describe the solution you'd like**

A gRPC method for setting a new value (takes task queue name and a number). Then a user can not set it on the Workers, and only use gRPC/tctl/SDK.



---

### #3284: Server returns status messages over metadata limit leading to connection drops and cryptic errors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3284 |
| **State** | OPEN |
| **Author** | Spikhalskiy (Dmitry Spikhalsky) |
| **Created** | 2022-08-30 18:54:12.000 UTC (3y 4m ago) |
| **Updated** | 2023-03-30 20:29:22.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior

If a worker reports an unexpected sequence of commands, a worker should get a gRPC error like this: 

> io.grpc.StatusRuntimeException: INVALID_ARGUMENT: invalid command sequence: [<sequence>], command CompleteWorkflowExecution must be the last command.

irrespectively of the size of the number of commands that were sent. 

The server should return an error message that is aligned with the announced receiver limit or at least under the default gRPC-over-HTTP/2 header limit.

## Actual Behavior

Reproduction:
https://github.com/Spikhalskiy/java-sdk/commit/74e6e3f2af363afd061cd9ec9f2c0d9b31ea5b21#diff-e566b269374183dd0f8a9068200b3c417dff37dce93f64021fe8cd40e71dee21R80

This reproduction generates a large (but manageable and under all the gRPC limits) and incorrect sequence of commands on the workflow task completion
[CompleteWorkflowExecution, RecordMarker \<xManyTimes\>]
The worker receives `RST_STREAM` frame instead of the normal server response and gets a connection closed.
`io.grpc.StatusRuntimeException: INTERNAL: RST_STREAM closed stream. HTTP/2 error code: INTERNAL_ERROR`.
Another manifestation of this problem on smaller sizes is
`io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2Exception$HeaderListSizeException: Header size exceeded max allowed size (10240)`
This leads to a cryptic log and there is no way for an application developer to understand that it's an incorrect sequence of commands causing it.

## Workaround

Client-side can temporary set a large limit on incoming headers (maxInboundMetadataSize) like it's shown here:
https://github.com/Spikhalskiy/java-sdk/commit/74e6e3f2af363afd061cd9ec9f2c0d9b31ea5b21#diff-dbd68ad479d675fe5c67f7fe0141d1078d9009fb27f6cd52e5ed6c42bf10d46aR179

## Root cause analysis

The server uses a raw sequence of commands as a part of the error message. The server doesn't respect either the SDK settings of maxInboundMetadataSize (that should be announced in the `SETTINGS_MAX_HEADER_LIST_SIZE` field of `SETTINGS` HTTP/2 frame) or a standard HTTP/2 limit for headers of [8192 bytes](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md).

## Proposed solution

The Server should preprocess all the variable-sized portions of error messages to make sure that it fits into
- a header limit announced by the counterpart (better)
- some reasonable default max length that is lower than the default gRPC metadata limit of 8Kb (acceptable)

It's better to receive an error message without specific commands or with a trimmed list than to get a broken connection and no useful error message at all

## Related HTTP/2 specs

> [6.5.2](https://datatracker.ietf.org/doc/html/rfc7540#section-6.5.2).  Defined SETTINGS Parameters
>    SETTINGS_MAX_HEADER_LIST_SIZE (0x6):  This advisory setting informs a
>       peer of the maximum size of header list that the sender is
>       prepared to accept, in octets.  The value is based on the
>       uncompressed size of header fields, including the length of the
>       name and value in octets plus an overhead of 32 octets for each
>       header field.
> 
>       For any given request, a lower limit than what is advertised MAY
>       be enforced.  The initial value of this setting is unlimited.


> [10.5.1](https://datatracker.ietf.org/doc/html/rfc7540#section-10.5.1).  Limits on Header Block Size
> 
>    A large header block ([Section 4.3](https://datatracker.ietf.org/doc/html/rfc7540#section-4.3)) can cause an implementation to
>    commit a large amount of state.  Header fields that are critical for
>    routing can appear toward the end of a header block, which prevents
>    streaming of header fields to their ultimate destination.  This
>    ordering and other reasons, such as ensuring cache correctness, mean
>    that an endpoint might need to buffer the entire header block.  Since
>    there is no hard limit to the size of a header block, some endpoints
>    could be forced to commit a large amount of available memory for
>    header fields.
> 
>    An endpoint can use the SETTINGS_MAX_HEADER_LIST_SIZE to advise peers
>    of limits that might apply on the size of header blocks.  This
>    setting is only advisory, so endpoints MAY choose to send header
>    blocks that exceed this limit and risk having the request or response
>    being treated as malformed.  This setting is specific to a
>    connection, so any request or response could encounter a hop with a
>    lower, unknown limit.  An intermediary can attempt to avoid this
>    problem by passing on values presented by different peers, but they
>    are not obligated to do so.
> 
>    A server that receives a larger header block than it is willing to
>    handle can send an HTTP 431 (Request Header Fields Too Large) status
>    code [[RFC6585](https://datatracker.ietf.org/doc/html/rfc6585)].  A client can discard responses that it cannot
>    process.  The header block MUST be processed to ensure a consistent
>    connection state, unless the connection is closed.



---

### #3261: Add command: DescribeExternalWorkflowExecution

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3261 |
| **State** | OPEN |
| **Author** | lorensr (Loren ‚ò∫Ô∏è) |
| **Created** | 2022-08-23 15:52:26.000 UTC (3y 4m ago) |
| **Updated** | 2023-03-03 20:18:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

In order to describe an external workflow, you currently need to call `workflow.describe()` using a client in an activity.

**Describe the solution you'd like**

Being able to describe an external workflow from inside a workflow.

**Additional context**

User request: https://github.com/temporalio/features/issues/590



---

### #3258: Allow option for reset to terminate any currently pending child workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3258 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2022-08-22 17:06:01.000 UTC (3y 4m ago) |
| **Updated** | 2023-03-03 20:18:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently reset will fail if there are any pending child workflows. 
Allow a flag to terminate the child workflow execution allowing reset command to go through.


---

### #3223: panic: assignment to entry in nil map in `loadClusterInformationFromStore`

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3223 |
| **State** | OPEN |
| **Author** | aybabtme (Antoine Grondin) |
| **Created** | 2022-08-12 19:20:24.000 UTC (3y 4m ago) |
| **Updated** | 2023-03-03 20:18:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | alexshtin |
| **Milestone** | None |

#### Description

## Expected Behavior

The program should return well formatted errors and shouldn't panic, especially not for bug-looking reasons like assigning in nil-maps.

## Actual Behavior

```
panic: assignment to entry in nil map

goroutine 1 [running]:
go.temporal.io/server/temporal.loadClusterInformationFromStore({0x2a5fab0?, 0xc00005a080}, 0xc000578900, {0x2a697a8?, 0xc0005e6000}, {0x2a63ae0, 0xc000962a80})
        /home/runner/work/temporal/temporal/temporal/fx.go:733 +0x7ae
go.temporal.io/server/temporal.ApplyClusterMetadataConfigProvider({0x2a63ae0, 0xc000878f10}, 0xc000578900, {0x2a49ba0, 0x3cd9f28}, 0x2633f08, {0x0, 0x0})
        /home/runner/work/temporal/temporal/temporal/fx.go:686 +0x22eb
reflect.Value.call({0x21af600?, 0x2634540?, 0x40d645?}, {0x24c790e, 0x4}, {0xc00031d200, 0x5, 0x30?})
        /opt/hostedtoolcache/go/1.18.4/x64/src/reflect/value.go:556 +0x845
reflect.Value.Call({0x21af600?, 0x2634540?, 0x40d9a7?}, {0xc00031d200, 0x5, 0x5})
        /opt/hostedtoolcache/go/1.18.4/x64/src/reflect/value.go:339 +0xbf
go.uber.org/dig.defaultInvoker({0x21af600?, 0x2634540?, 0xc0000fae60?}, {0xc00031d200?, 0x5?, 0x2a6e588?})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/container.go:220 +0x28
go.uber.org/dig.(*constructorNode).Call(0xc00035a900, {0x2a6e588, 0xc0003580a0})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/constructor.go:154 +0x297
go.uber.org/dig.paramSingle.Build({{0x0, 0x0}, 0x0, {0x2a71500, 0x234fe60}}, {0x2a6e588, 0xc000358000})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:296 +0x2f9
go.uber.org/dig.paramObjectField.Build(...)
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:480
go.uber.org/dig.paramObject.Build({{0x2a71500, 0x245c0e0}, {0xc000366f00, 0x14, 0x20}, {0x0, 0x0, 0x0}}, {0x2a6e588, 0xc000358000})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:407 +0x155
go.uber.org/dig.paramList.BuildList({{0x2a71500, 0x20d58a0}, {0xc000332830, 0x1, 0x1}}, {0x2a6e588, 0xc000358000})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:151 +0xb9
go.uber.org/dig.(*constructorNode).Call(0xc00035a600, {0x2a6e588, 0xc000358000})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/constructor.go:145 +0x132
go.uber.org/dig.paramGroupedSlice.callGroupProviders({{0x1f49fd8, 0x8}, {0x2a71500, 0x20036e0}, 0xc000339bc0}, {0x2a6e588?, 0xc0003580a0?})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:597 +0x192
go.uber.org/dig.paramGroupedSlice.Build({{0x1f49fd8, 0x8}, {0x2a71500, 0x20036e0}, 0xc000339bc0}, {0x2a6e588, 0xc0003580a0})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:624 +0x10c
go.uber.org/dig.paramObjectField.Build(...)
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:480
go.uber.org/dig.paramObject.Build({{0x2a71500, 0x222be80}, {0xc000339c20, 0x1, 0x1}, {0x0, 0x0, 0x0}}, {0x2a6e588, 0xc0003580a0})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:407 +0x155
go.uber.org/dig.paramList.BuildList({{0x2a71500, 0x220fc80}, {0xc000358140, 0xa, 0xa}}, {0x2a6e588, 0xc0003580a0})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:151 +0xb9
go.uber.org/dig.(*constructorNode).Call(0xc00035a0c0, {0x2a6e588, 0xc0003580a0})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/constructor.go:145 +0x132
go.uber.org/dig.paramSingle.Build({{0x0, 0x0}, 0x0, {0x2a71500, 0x2164900}}, {0x2a6e588, 0xc0003580a0})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:296 +0x2f9
go.uber.org/dig.paramList.BuildList({{0x2a71500, 0x2061de0}, {0xc000332430, 0x1, 0x1}}, {0x2a6e588, 0xc0003580a0})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:151 +0xb9
go.uber.org/dig.(*constructorNode).Call(0xc00035a180, {0x2a6e588, 0xc0003580a0})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/constructor.go:145 +0x132
go.uber.org/dig.paramSingle.Build({{0x0, 0x0}, 0x0, {0x2a71500, 0x2193d40}}, {0x2a6e588, 0xc0003580a0})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:296 +0x2f9
go.uber.org/dig.paramList.BuildList({{0x2a71500, 0x207fea0}, {0xc00011d200, 0x2, 0x2}}, {0x2a6e588, 0xc0003580a0})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/param.go:151 +0xb9
go.uber.org/dig.(*Scope).Invoke(0xc0003580a0, {0x207fea0?, 0x2634588}, {0xc0007fee68?, 0x4eb245?, 0xc0007fee98?})
        /home/runner/go/pkg/mod/go.uber.org/dig@v1.14.1/invoke.go:85 +0x288
go.uber.org/fx.runInvoke({0x2a5e0b0?, 0xc0003580a0?}, {{0x207fea0, 0x2634588}, {0xc000344a00, 0x7, 0x8}})
        /home/runner/go/pkg/mod/go.uber.org/fx@v1.17.1/invoke.go:93 +0x1ff
go.uber.org/fx.(*module).executeInvoke(0xc0001f6090, {{0x207fea0, 0x2634588}, {0xc000344a00, 0x7, 0x8}})
        /home/runner/go/pkg/mod/go.uber.org/fx@v1.17.1/module.go:174 +0x133
go.uber.org/fx.(*module).executeInvokes(0xc0001f6090)
        /home/runner/go/pkg/mod/go.uber.org/fx@v1.17.1/module.go:155 +0xf1
go.uber.org/fx.New({0xc0007ff338, 0xc, 0x7?})
        /home/runner/go/pkg/mod/go.uber.org/fx@v1.17.1/app.go:534 +0x887
go.temporal.io/server/temporal.NewServerFx({0xc000334000?, 0x2a63ae0?, 0xc000878f10?})
        /home/runner/work/temporal/temporal/temporal/fx.go:130 +0x716
go.temporal.io/server/temporal.NewServer(...)
        /home/runner/work/temporal/temporal/temporal/server.go:58
main.buildCLI.func2(0xc0004d1320?)
        /home/runner/work/temporal/temporal/cmd/server/main.go:166 +0x1538
github.com/urfave/cli/v2.(*Command).Run(0xc0004d1320, 0xc00040c4c0)
        /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.4.0/command.go:163 +0x5bb
github.com/urfave/cli/v2.(*App).RunContext(0xc000103a00, {0x2a5fab0?, 0xc00005a068}, {0xc00004c1e0, 0x6, 0x6})
        /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.4.0/app.go:313 +0xb48
github.com/urfave/cli/v2.(*App).Run(...)
        /home/runner/go/pkg/mod/github.com/urfave/cli/v2@v2.4.0/app.go:224
main.main()
        /home/runner/work/temporal/temporal/cmd/server/main.go:53 +0x45
```

## Steps to Reproduce the Problem

Use this config file:

```
log:
    stdout: true
    level: info

persistence:
    defaultStore: mysql-default
    visibilityStore: mysql-visibility
    numHistoryShards: 4
    datastores:
        mysql-default:
            sql:
                pluginName: "mysql"
                databaseName: "{{ .Env.TEMPORAL_MYSQL_DEFAULT_DBNAME }}"
                connectAddr: "{{ .Env.TEMPORAL_MYSQL_DEFAULT_HOST }}:{{ .Env.TEMPORAL_MYSQL_DEFAULT_PORT }}"
                connectProtocol: "tcp"
                user: "{{ .Env.TEMPORAL_MYSQL_DEFAULT_USER }}"
                password: "{{ .Env.TEMPORAL_MYSQL_DEFAULT_PASSWORD }}"
                maxConns: 20
                maxIdleConns: 20
                maxConnLifetime: "1h"
        mysql-visibility:
            sql:
                pluginName: "mysql"
                databaseName: "{{ .Env.TEMPORAL_MYSQL_VISIBILITY_DBNAME }}"
                connectAddr: "{{ .Env.TEMPORAL_MYSQL_VISIBILITY_HOST }}:{{ .Env.TEMPORAL_MYSQL_VISIBILITY_PORT }}"
                connectProtocol: "tcp"
                user: "{{ .Env.TEMPORAL_MYSQL_VISIBILITY_USER }}"
                password: "{{ .Env.TEMPORAL_MYSQL_VISIBILITY_PASSWORD }}"
                maxConns: 2
                maxIdleConns: 2
                maxConnLifetime: "1h"

global:
    membership:
        maxJoinDuration: 30s
        broadcastAddress: "127.0.0.1"
    pprof:
        port: 7936
    metrics:
        statsd:
            hostPort: "{{ .Env.DD_AGENT_HOST }}:28125"
            prefix: "temporal"

services:
    frontend:
        rpc:
            grpcPort: 7233
            membershipPort: 6933
            bindOnLocalHost: true

    matching:
        rpc:
            grpcPort: 7235
            membershipPort: 6935
            bindOnLocalHost: true

    history:
        rpc:
            grpcPort: 7234
            membershipPort: 6934
            bindOnLocalHost: true

    worker:
        rpc:
            grpcPort: 7239
            membershipPort: 6939
            bindOnLocalHost: true

clusterMetadata:
    enableGlobalNamespace: false
    failoverVersionIncrement: 10
    masterClusterName: "active"
    currentClusterName: "active"
    clusterInformation:
        active:
            enabled: true
            initialFailoverVersion: 1
            rpcName: "frontend"
            rpcAddress: "localhost:7233"

dcRedirectionPolicy:
    policy: "noop"
    toDC: ""

archival:
    history:
        state: "enabled"
        enableRead: true
        provider:
            filestore:
            fileMode: "0666"
            dirMode: "0766"
            gstorage:
            credentialsPath: "/tmp/gcloud/keyfile.json"
    visibility:
        state: "enabled"
        enableRead: true
        provider:
            filestore:
            fileMode: "0666"
            dirMode: "0766"

namespaceDefaults:
    archival:
        history:
            state: "disabled"
            URI: "file:///tmp/temporal_archival/development"
        visibility:
            state: "disabled"
            URI: "file:///tmp/temporal_vis_archival/development"

publicClient:
    hostPort: "localhost:7233"
```

## Specifications

  - Version: 1.17.2
  - Platform: x86_64 GNU/Linux



---

### #3183: Replace metrics and retryable client with client interceptors

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3183 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2022-08-04 18:47:31.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:18:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

Please check the comment here:
https://github.com/temporalio/temporal/pull/3172#discussion_r935047507

Currently we use code generation for generating metrics/retryable client (a wrapper for basic/raw service clients) for logging/metrics and retry rpc calls. We have the same logic at the server/receiver end but is achieved via grpc interceptors. 

Looks like the same approach can be applied to client side as well, and avoid the complexity of code generation and the cost for maintaining it.


---

### #3168: Matching service optimization - Do not persist sticky workflow task

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3168 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-07-29 23:57:55.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:18:37.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Sticky workflow tasks usually have short timeout, in terms of seconds.

When a sticky workflow task cannot be sync-matched, matching service right now will persist the task into DB and wait for SDK with local cache to pick up that task.

If sticky workflow task cannot be sync-matched, it (sometimes? usually?) means SDK is unavailable and history service will timeout the task few seconds later.
If above case happen, DB IOPS are wasted. (matching service persist the task, & maybe read it back)

Matching service should not persist sticky workflow task, by
1. either when unable to sync match, return error and let history service retry for few times before give up
2. or using longer sync match timeout, if unable to sync match, give up immediately
3. change history service to dispatch task, preferable to the SDK queue (SDK which has local cache) default to normal queue; when sync match timeout for SDK queue, put the task to normal queue

NOTE: history service will timeout the sticky workflow task & create a normal workflow task, workflow will NOT be stuck


---

### #3154: tdbg workflow show does not support pagination

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3154 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2021-12-29 00:38:54.000 UTC (4 years ago) |
| **Updated** | 2023-03-03 20:21:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

See logic: https://github.com/temporalio/temporal/blob/beb9a5a/tools/cli/adminCommands.go#L93-L94


---

### #3145: Per task queue ratelimiting should only count valid activity tasks

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3145 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2022-07-26 22:49:30.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:19:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | maoyi16 |
| **Milestone** | None |

#### Description

## Expected Behavior
Only valid (started) tasks should count towards per task queue ratelimiting. (when user specifies `TaskQueueActivitiesPerSecond ` in worker config)

## Actual Behavior
https://github.com/temporalio/temporal/blob/master/service/matching/matcher.go#L228

All tasks, regardless of whether its expired or failed to start or successfully started will consume token from the rate limiter, since we perform the rate limiting step when attempting a match.

For example when there's a backlog of expired tasks in the task buffer, all tasks will consume the rate limiting token and throttle how fast the backlog can be drained. However, from user/worker perspective, there's no dispatched tasks at all.

## Steps to Reproduce the Problem

  1.
  1.
  1.

## Specifications

  - Version:
  - Platform:



---

### #3143: Improved per task queue metrics

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3143 |
| **State** | OPEN |
| **Author** | robholland (Rob Holland) |
| **Created** | 2022-07-26 16:36:10.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:19:44.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | maoyi16 |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
The current per task queue metrics have a `per_tl` suffix as a hang-over from Cadence which uses the term task list. The `taskqueue` label uses the task queue name for the label on the root partitions but the internal partition task queue name for the partitions. This makes it difficult to sum the metrics across partitions in order to check, for example, polling sync rates for the task queue as a whole.

**Describe the solution you'd like**
I suggest adding new metrics with `_per_taskqueue` naming. This will make it easier to discover the metric as it uses the correct Temporal term (rather than tl for task list). The metrics should have their root taskqueue for the `taskqueue` labels and a `partition` label which records their task queue partition (0 for the root). This will make it easy to sum values for the task queue as a whole, and also viable to know how many partitions a task queue currently has which is not currently exposed in an easy to consume way.

The old metric would continue to be emitted but deprecated after some number of releases.

**Describe alternatives you've considered**
An alternative would be to amend the labels for the existing metric names but this will break existing dashboards/alerting and cause confusion.


---

### #3133: Publish size/number of history events for closed workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3133 |
| **State** | OPEN |
| **Author** | tusharroy25 |
| **Created** | 2022-07-22 18:05:59.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:19:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

We have size/number of history events whenever workflow update happens. For my current and future investigations we need those number at the time of closing workflow. Also size of mutable state at the closing workflow would be another critical metric.




---

### #3062: New error type for workflow already completed?

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3062 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2022-07-05 23:54:37.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:19:39.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
For both workflow not found and workflow already completed case, we return a NotFound error. And caller can't tell the which case it is just by checking the error type if it wants to have different error handling logic for those two cases.

An example is verifying child workflow recorded. If parent not found, we need to keep retrying the verification. If parent already completed, the verification doesn't need to continue. Currently the implementation is return WorkflowNotReady error if workflow not found and return NotFound when workflow already completed. This makes the error handling logic very confusing. 

In this particular case of verification, we probably don't have to return already completed error at all. Basically assume this error will always be ignored by caller. (The idea of current implementation is simply gave more accurate information to the caller and don't make any assumption on the caller). Then NotFound can either be unexpected or only means the real workflow not found.

Is there any other cases where we need to treat not found and already completed differently?

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #3060: Validate num of matching task queue partition can be decreased

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3060 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-07-05 22:37:35.000 UTC (3y 5m ago) |
| **Updated** | 2023-03-03 20:19:38.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Today, if operator need to increase number of task queue partitions, operator can simply increase both dynamic config to target number:
`matching.numTaskqueueReadPartitions`
`matching.numTaskqueueWritePartitions`

If operator need to decrease number of task queue partition, operator need to first decrease num of write partition, wait for all DB tasks to drain, then decrease num of read partitions

We need to validate the decrease scenario



---

### #3025: Ability to define execution order of async activity invocations

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3025 |
| **State** | OPEN |
| **Author** | tsurdilo (Tihomir Surdilovic) |
| **Created** | 2022-06-27 17:33:02.000 UTC (3y 6m ago) |
| **Updated** | 2023-03-03 20:19:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |
| **Reactions** | ‚ù§Ô∏è 3 |

#### Description

Currently Temporal does not guarantee relative ordering of async started activities. 
User [here](https://community.temporal.io/t/async-execution-of-activities-with-guaranteed-order-outside-of-the-current-workflow/5105) requested that for his use case Temporal would provide such guarantee for his scenario.




---

### #3004: Support bypass of autoforwarding with special header

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/3004 |
| **State** | OPEN |
| **Author** | meiliang86 (Liang Mei) |
| **Created** | 2022-06-17 17:13:55.000 UTC (3y 6m ago) |
| **Updated** | 2023-03-03 20:19:32.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently when autoforwarding is on for all apis, it would be hard to investigate standby cluster/workflow issues (because all requests goes to active cluster)

**Describe the solution you'd like**
A clear and concise description of what you want to happen.
Some way to bypass autoforwarding. This is mostly need for debugging/investigation purpose

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #2995: Task Predicate Equivalence Check

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2995 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2022-06-14 18:52:45.000 UTC (3y 6m ago) |
| **Updated** | 2023-03-03 20:19:32.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | performance, backlog |
| **Assignees** | yycptt |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
- https://github.com/temporalio/temporal/pull/2988 added Predicate.Equals() which checks if two predicates have the same form and value. Since the check won't try to re-arrange the predicates (especially when and/or/not operations are involved), the result will contains false negatives even if mathematically two predicates are equivalent.

- At current stage we think ^ check is sufficient as 1. in most cases our predicates are quite simple and should be equal even when form is taken into consideration 2. When false negatives are returned we will simply not merge (attach) two queue slices and there's no cost for correctness and performance. (All it costs is some additional storage costs for storing an additional queue slice scope data, which is negligible)

- If in the future it turns out that we always have a large number of queue slices, we may want to implement a real equivalence check for predicates so that more queue slices can be merged together.

- Creating this issue to track that this is something we can improve in the future.

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #2991: Dynamically generate task type tag value in timer/transfer queue processing

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2991 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2022-06-14 00:51:08.000 UTC (3y 6m ago) |
| **Updated** | 2023-03-03 20:19:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | refactoring, metrics |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Right now we have one task type tag value look up table for each queue processor: active/standby transfer/timer and visibility. Is it possible to get rid of the look up table and dynamically generate the value based on task type (there's an existing proto enum definition for task type)?

See discussion here: https://github.com/temporalio/temporal/pull/2930#discussion_r891459319

**Describe the solution you'd like**
A clear and concise description of what you want to happen.
- If we want the exact same metrics as today, we need to do some string manipulation on the return value of taskType.String(). For example, replace "Transfer" with "TransferActiveTask" or add "TimerActive" to the return value. And there's some special case where taskType.String() does not match the currently emitted task type tag value.
    - We can update proto definition for those enums so that there's no special case. But we still need to add "Active" or "Standby" to the return value.
    - We make a small "breaking" change and change the task tag value emitted. Since our dashboard queries aggregates on task type, the dashboard will continue to worker. Only when the query is for a specific task type, will it be break. But still, "Active" or "Standby" need to be added to the return value.
- Use a new tag for active & standby. This will break existing dashboard and we need to update all dashboard & alert queries. 

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #2968: Support changing system workflow cron schedule

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2968 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2022-06-07 19:23:42.000 UTC (3y 6m ago) |
| **Updated** | 2023-03-03 20:19:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Support changing system workflow cron schedule. Currently the cron schedule is hardcoded and even if the value in code is updated, when deploying the change, the cron schedule for the workflow won't be updated if there's already a workflow running. 

**Describe the solution you'd like**
If describe existing workflow if running & has different cron schedule, terminateAndStart a new one with updated schedule.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #2918: Persistent Coroutine Workflows

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2918 |
| **State** | OPEN |
| **Author** | faucct (Nikita Sokolov) |
| **Created** | 2022-05-29 14:01:19.000 UTC (3y 7m ago) |
| **Updated** | 2023-03-03 20:20:17.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**

It is a sketch of an idea of an alternative persistence approach to for the same problem the Temporal is trying to solve which could be easier to use than Event Sourcing.
I think, the Event Sourcing pattern makes Workflows' versioning harder than it could be.
Also storing and replaying the whole log seems ineffective and limits the Workflows' duration.   

**Describe the solution you'd like**

For that you would need a language with the support for not only coroutines, but their suspension and serialization/deserialization at continuation points.
Then instead of Event Sourcing the framework could persist the Workflows' coroutines states into the persistent storage.

If the Workflows' coroutines parts between the persisting continuation points are idempotent, then the Workflows would be consistent.
For Workflows versioning a serialization format with an explicit schema could be used (e.g. protobuf).
This would require the workflow definition to contain some sort of annotations: for each persistence point and every persisted field/variable, including nested coroutines which could also have their persistence points.  

**Describe alternatives you've considered**

I know that this project is a successor to Orleans.
I am not sure if I got that right, but looks like it also uses Event Sourcing, thus suffers from the same problems ‚Äì https://dotnet.github.io/orleans/docs/grains/event_sourcing/index.html.
Still, its documentation mentions async/await, but it looks like it is in the different context ‚Äì https://dotnet.github.io/orleans/docs/grains/external_tasks_and_grains.html.

It looks like SecondLife was doing something like coroutines persistence: https://tirania.org/blog/archive/2009/Apr-09.html, https://secondlife.blogs.com/babbage/2006/05/microthreading_.html

I understand that you have probably invested a lot into the Event Sourcing approach and won't switch from it that easily, but still I want to know if this idea looks promising to you, considering you are tackling the same problem. I want to try implementing this approach for Kotlin, which does not support the required coroutine serialization at the moment.


---

### #2915: Dynamic config change listener 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2915 |
| **State** | OPEN |
| **Author** | yycptt (Yichao Yang) |
| **Created** | 2022-05-27 18:00:36.000 UTC (3y 7m ago) |
| **Updated** | 2023-03-03 20:20:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Existing dynamic config is built as poll model, meaning when a value is needed, a function call needs to be made to get the latest value. 

This poll model works well for values that are accessed frequently, but not for values that are rarely accessed or even accessed only once. For example, configs that are used only at service start time, like task worker pool size. 

In those cases, either the config updates are completely ignored when the service is running or each component needs to have its own poller to periodically check if the config value is updated or not (e.g. https://github.com/temporalio/temporal/pull/2911). For values that are rarely updated, it is not efficient and also causes a lot of duplicated code for polling value updates. 

**Describe the solution you'd like**
Add support for push model in dynamic config so that a listener can be registered for a certain key. 

**Describe alternatives you've considered**


**Additional context**




---

### #2849: Do not load all persistence libraries unless needed

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2849 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2022-05-14 03:19:38.000 UTC (3y 7m ago) |
| **Updated** | 2023-03-03 20:20:11.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Currently, we load all persistence libraries no matter what persistence the server is configured to run with.
With recent added auth plugin, we are now also pull in aws libraries for "rds-iam-auth". This will quickly become unsustainable. 

We need a better way to be able to load only needed libraries. 
Few options I can think of:
* Build plugins as library and load dynamically at runtime based on persistence config. 
* Use different build flag to include only needed libraries at build time for persistence used. This will result in different binaries for different persistence.




---

### #2800: Do not write repeated workflow task heartbeat to the history

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2800 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2022-05-04 17:08:08.000 UTC (3y 8m ago) |
| **Updated** | 2023-03-03 20:20:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
A very long running local activity can lead to many repeated WorkflowTaskCompleted/Scheduled/Started events written to the history.

**Describe the solution you'd like**
Do not write these events if there are no commands included in WorkflowTaskCompleted and there are no new events.

We already do a similar "deduping" when processing constantly failing/timing out workflow tasks.




---

### #2730: Use broadcast shutdown channel

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2730 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-04-18 17:20:04.000 UTC (3y 8m ago) |
| **Updated** | 2023-03-03 20:20:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | wxing1292 |
| **Milestone** | None |

#### Description

Use broadcast shutdown channel for shard lifecycle management
https://github.com/temporalio/temporal/blob/master/common/channel/shutdown_once.go


---

### #2528: SQL delete workflow execution does not delete all data

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2528 |
| **State** | OPEN |
| **Author** | yux0 (Yu Xia) |
| **Created** | 2022-02-22 19:26:26.000 UTC (3y 10m ago) |
| **Updated** | 2023-03-03 20:20:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | alexshtin |
| **Milestone** | None |

#### Description

## Expected Behavior
Delete all expired data in SQL plugin

## Actual Behavior
Only delete the data in execution table.


## Test PR
https://github.com/temporalio/temporal/pull/2525/files

## Specifications
This is an old issue that could happen to multiple releases.



---

### #2435: Need detailed server-internal resource exhausted metrics

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2435 |
| **State** | OPEN |
| **Author** | paulnpdev (Paul Nordstrom) |
| **Created** | 2022-01-29 00:34:17.000 UTC (3y 11m ago) |
| **Updated** | 2023-03-03 20:20:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | alexshtin, wxing1292 |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Extending the work done in #2412 , we need more detailed resource headroom metrics to improve our ability to detect approaching exhaustion.  These could collapse into simpler errors to expose to Workers.

**Describe the solution you'd like**
Full design pending discussion.

**Describe alternatives you've considered**
Instead of metrics, we could use a "resource monitor" object that we would access on-demand

**Additional context**
Add any other context or screenshots about the feature request here.



---

### #2406: Wire up Transaction.CreateWorkflowExecution

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2406 |
| **State** | OPEN |
| **Author** | yiminc (Yimin Chen) |
| **Created** | 2022-01-21 22:12:18.000 UTC (3y 11m ago) |
| **Updated** | 2023-03-03 20:20:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | wxing1292 |
| **Milestone** | None |

#### Description

The Transaction.CreateWorkflowExecution() is not used anywhere. Shard context directly calls into createWorkflowExecutionWithRetry(). Need to update code to use Transaction.CreateWorkflowExecution().


---

### #2375: Reduce unnecessary & duplicated data encoding

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2375 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2022-01-13 02:57:36.000 UTC (3y 11m ago) |
| **Updated** | 2023-03-03 20:21:15.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

current implementation of logic allow per attribute encoding, e.g. activity have its own encoding, child workflow can have its own encoding. per attribute encoding can in theory allow more flexibility, but in reality, only proto3 is used.
in fact, business logic object (binary) only need one encoding type.

Update the following structs, replace DataBlob with `[]byte`
InternalWorkflowMutableState / InternalWorkflowMutation / InternalWorkflowSnapshot

[InternalWorkflowMutableState](https://github.com/temporalio/temporal/blob/v1.14.2/common/persistence/persistenceInterface.go#L324)
[InternalWorkflowMutation](https://github.com/temporalio/temporal/blob/v1.14.2/common/persistence/persistenceInterface.go#L387)
[InternalWorkflowSnapshot](https://github.com/temporalio/temporal/blob/v1.14.2/common/persistence/persistenceInterface.go#L427)


---

### #2282: Flush transient ActivityTaskStarted events before closing a workflow

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2282 |
| **State** | OPEN |
| **Author** | Spikhalskiy (Dmitry Spikhalsky) |
| **Created** | 2021-12-09 21:10:42.000 UTC (4 years ago) |
| **Updated** | 2023-03-03 20:21:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
On the attached screenshot you can find a workflow execution history that
1. Is Closed - the workflow execution timed out 
2. An activity was started and was never finished, taking too much time. [Irrelevant, but In reality, an activity worker was killed and the activity had too long start_to_close timeout] Which caused the time out.

This history doesn't have an ActivityTaskStarted at all.
The reason is that we delay or buffer ActivityTaskStarted events till completion. Presumably to simplify processing of our state machine.
The problem is that this history looks like the activity was never ever picked up by the worker. Which makes this history very misleading and hard to debug.

**Describe the solution you'd like**
Flush buffered ActivityTaskStarted events before closing the workflow [completion, termination, cancellation, timeout]
 
**Describe alternatives you've considered**
Temporal may utilize marker events, but this is cumbersome if the original solution is possible. 

**Additional context**
![image](https://user-images.githubusercontent.com/532108/145475408-67043e38-2811-495e-ac25-ff71e8363619.png)



---

### #2059: Embed schema content within schema tool

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/2059 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2021-10-15 23:34:26.000 UTC (4y 2m ago) |
| **Updated** | 2023-03-03 20:21:04.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

We should include the schema content within schema tool using `go:embed`

ref: https://github.com/temporalio/temporal/blob/08b4871/schema/sqlite/setup.go#L48


---

### #1966: Split resource limit exceed error into user facing & internal facing error types

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1966 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2021-09-23 03:42:07.000 UTC (4y 3m ago) |
| **Updated** | 2023-03-03 20:21:43.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | yiminc |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Split resource limit exceed into user facing & internal facing error types
* user facing error should not be retried
* internal facing error should be retried, and eventually turned into unavailable


---

### #1903: Add WorkflowSizeInBytes as Visibility Attribute

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1903 |
| **State** | OPEN |
| **Author** | mastermanu |
| **Created** | 2021-09-09 01:02:39.000 UTC (4y 3m ago) |
| **Updated** | 2023-03-03 20:21:41.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, visibility |
| **Assignees** | alexshtin |
| **Milestone** | None |

#### Description

We recently added StateTransition count as a visibility attribute for a Completed Workflow. Can we also add the final size of the Workflow itself?


---

### #1867: temporal-server "hang" during SIGTERM after loss of Cassandra connection

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1867 |
| **State** | OPEN |
| **Author** | leowmjw |
| **Created** | 2021-08-28 07:23:39.000 UTC (4y 4m ago) |
| **Updated** | 2023-03-03 20:21:36.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug |
| **Assignees** | None |
| **Milestone** | None |

#### Description

## Expected Behavior
When send interrupt signal; temporal-server cleanly shutdown

## Actual Behavior
Unable to cleanly shutdown; it "hangs".  Normal SIGTERM is ignored; have to SIGKILL.
Only happens after network ; and it does not retry and "hangs"

## Steps to Reproduce the Problem

  1. Start temporal-server with config below; no need HostLookup as Cassandra only have one endpoint to talk to (managed Cassandra AstraDB)
  1. Turn off network/wifi (simulate loss of connection to DB)
  1. Run an operation 'tctl wf list'; oobserve failure and reconnect network/wifi 
  1. Once network connection is back; confirm normal operation works 'tctl wf list'
  1. Ctrl-C, observe error and "hang", forced to SIGKILL

## Cassandra config

```
    cass-default:
      cassandra:
        hosts: <endpoint>-ap-southeast-1.db.astra.datastax.com
        port: 29042
...
        disableInitialHostLookup: true
        keyspace: "temporal2"
    cass-visibility:
      cassandra:
        hosts: <endpoint>-ap-southeast-1.db.astra.datastax.com
        port: 29042
...
        disableInitialHostLookup: true
        keyspace: "temporal_visibility2"

```
## Observation

It looks as though after hitting "read: connection reset by peer", the node is marked unhealthy.  When there is the config **disableInitialHostLookup=true** it should just retry the known connection after a short while.

## Specifications

  - Version: temporal version 1.12.0
  - Platform: OSX
  - DB: Managed Cassandra AstraDB

## Logs

```
{"level":"error","ts":"2021-08-28T14:42:02.680+0800","msg":"Membership upsert failed.","service":"history","error":"operation UpsertClusterMembership encounter read tcp 192.168.0.104:50393->13.213.36.186:29042: read: connection reset by peer","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
{"level":"error","ts":"2021-08-28T14:42:02.680+0800","msg":"Membership upsert failed.","service":"matching","error":"operation UpsertClusterMembership encounter read tcp 192.168.0.104:50414->13.212.129.122:29042: read: connection reset by peer","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
{"level":"error","ts":"2021-08-28T14:42:02.874+0800","msg":"Operation failed with internal error.","service":"matching","error":"read tcp 192.168.0.104:50410->13.213.36.186:29042: read: connection reset by peer","metric-scope":52,"logging-call-at":"persistenceMetricClients.go:896","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/persistence.(*metadataPersistenceClient).updateErrorMetric\n\t/Users/leow/GOMOD/temporal/common/persistence/persistenceMetricClients.go:896\ngo.temporal.io/server/common/persistence.(*metadataPersistenceClient).GetMetadata\n\t/Users/leow/GOMOD/temporal/common/persistence/persistenceMetricClients.go:874\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshNamespacesLocked\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:435\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshNamespaces\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:425\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshLoop\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:409"}
{"level":"error","ts":"2021-08-28T14:42:02.875+0800","msg":"Error refreshing namespace cache","service":"matching","error":"read tcp 192.168.0.104:50410->13.213.36.186:29042: read: connection reset by peer","logging-call-at":"namespaceCache.go:414","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshLoop\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:414"}
{"level":"error","ts":"2021-08-28T14:42:03.937+0800","msg":"Operation failed with internal error.","service":"history","error":"operation UpdateShard encounter read tcp 192.168.0.104:50395->13.213.36.186:29042: read: connection reset by peer","metric-scope":3,"logging-call-at":"persistenceMetricClients.go:194","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/persistence.(*shardPersistenceClient).updateErrorMetric\n\t/Users/leow/GOMOD/temporal/common/persistence/persistenceMetricClients.go:194\ngo.temporal.io/server/common/persistence.(*shardPersistenceClient).UpdateShard\n\t/Users/leow/GOMOD/temporal/common/persistence/persistenceMetricClients.go:176\ngo.temporal.io/server/service/history/shard.(*ContextImpl).updateShardInfoLocked\n\t/Users/leow/GOMOD/temporal/service/history/shard/context_impl.go:778\ngo.temporal.io/server/service/history/shard.(*ContextImpl).UpdateTimerClusterAckLevel\n\t/Users/leow/GOMOD/temporal/service/history/shard/context_impl.go:309\ngo.temporal.io/server/service/history.newTimerQueueActiveProcessor.func2\n\t/Users/leow/GOMOD/temporal/service/history/timerQueueActiveProcessor.go:70\ngo.temporal.io/server/service/history.(*timerQueueAckMgrImpl).updateAckLevel\n\t/Users/leow/GOMOD/temporal/service/history/timerQueueAckMgr.go:401\ngo.temporal.io/server/service/history.(*timerQueueProcessorBase).internalProcessor\n\t/Users/leow/GOMOD/temporal/service/history/timerQueueProcessorBase.go:320\ngo.temporal.io/server/service/history.(*timerQueueProcessorBase).processorPump\n\t/Users/leow/GOMOD/temporal/service/history/timerQueueProcessorBase.go:195"}
{"level":"error","ts":"2021-08-28T14:42:03.938+0800","msg":"Error updating timer ack level for shard","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"timer-queue-processor","cluster-name":"active","error":"operation UpdateShard encounter read tcp 192.168.0.104:50395->13.213.36.186:29042: read: connection reset by peer","logging-call-at":"timerQueueAckMgr.go:403","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/service/history.(*timerQueueAckMgrImpl).updateAckLevel\n\t/Users/leow/GOMOD/temporal/service/history/timerQueueAckMgr.go:403\ngo.temporal.io/server/service/history.(*timerQueueProcessorBase).internalProcessor\n\t/Users/leow/GOMOD/temporal/service/history/timerQueueProcessorBase.go:320\ngo.temporal.io/server/service/history.(*timerQueueProcessorBase).processorPump\n\t/Users/leow/GOMOD/temporal/service/history/timerQueueProcessorBase.go:195"}
{"level":"error","ts":"2021-08-28T14:42:04.019+0800","msg":"Operation failed with internal error.","service":"worker","error":"read tcp 192.168.0.104:50424->13.213.36.186:29042: read: connection reset by peer","metric-scope":52,"logging-call-at":"persistenceMetricClients.go:896","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/persistence.(*metadataPersistenceClient).updateErrorMetric\n\t/Users/leow/GOMOD/temporal/common/persistence/persistenceMetricClients.go:896\ngo.temporal.io/server/common/persistence.(*metadataPersistenceClient).GetMetadata\n\t/Users/leow/GOMOD/temporal/common/persistence/persistenceMetricClients.go:874\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshNamespacesLocked\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:435\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshNamespaces\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:425\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshLoop\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:409"}
{"level":"error","ts":"2021-08-28T14:42:04.019+0800","msg":"Error refreshing namespace cache","service":"worker","error":"read tcp 192.168.0.104:50424->13.213.36.186:29042: read: connection reset by peer","logging-call-at":"namespaceCache.go:414","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshLoop\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:414"}
{"level":"error","ts":"2021-08-28T14:42:04.051+0800","msg":"Operation failed with internal error.","service":"history","error":"operation UpdateShard encounter read tcp 192.168.0.104:50392->13.213.36.186:29042: read: connection reset by peer","metric-scope":3,"logging-call-at":"persistenceMetricClients.go:194","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/persistence.(*shardPersistenceClient).updateErrorMetric\n\t/Users/leow/GOMOD/temporal/common/persistence/persistenceMetricClients.go:194\ngo.temporal.io/server/common/persistence.(*shardPersistenceClient).UpdateShard\n\t/Users/leow/GOMOD/temporal/common/persistence/persistenceMetricClients.go:176\ngo.temporal.io/server/service/history/shard.(*ContextImpl).updateShardInfoLocked\n\t/Users/leow/GOMOD/temporal/service/history/shard/context_impl.go:778\ngo.temporal.io/server/service/history/shard.(*ContextImpl).UpdateTransferClusterAckLevel\n\t/Users/leow/GOMOD/temporal/service/history/shard/context_impl.go:182\ngo.temporal.io/server/service/history.newTransferQueueActiveProcessor.func3\n\t/Users/leow/GOMOD/temporal/service/history/transferQueueActiveProcessor.go:99\ngo.temporal.io/server/service/history.(*transferQueueProcessorBase).updateAckLevel\n\t/Users/leow/GOMOD/temporal/service/history/transferQueueProcessorBase.go:99\ngo.temporal.io/server/service/history.(*queueAckMgrImpl).updateQueueAckLevel\n\t/Users/leow/GOMOD/temporal/service/history/queueAckMgr.go:222\ngo.temporal.io/server/service/history.(*queueProcessorBase).processorPump\n\t/Users/leow/GOMOD/temporal/service/history/queueProcessor.go:241"}
{"level":"error","ts":"2021-08-28T14:42:04.174+0800","msg":"Error updating ack level for shard","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"transfer-queue-processor","cluster-name":"active","error":"operation UpdateShard encounter read tcp 192.168.0.104:50392->13.213.36.186:29042: read: connection reset by peer","operation-result":"OperationFailed","logging-call-at":"queueAckMgr.go:224","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/service/history.(*queueAckMgrImpl).updateQueueAckLevel\n\t/Users/leow/GOMOD/temporal/service/history/queueAckMgr.go:224\ngo.temporal.io/server/service/history.(*queueProcessorBase).processorPump\n\t/Users/leow/GOMOD/temporal/service/history/queueProcessor.go:241"}
{"level":"error","ts":"2021-08-28T14:42:05.244+0800","msg":"Operation failed with internal error.","service":"worker","error":"read tcp 192.168.0.104:50419->13.212.129.122:29042: read: connection reset by peer","metric-scope":52,"logging-call-at":"persistenceMetricClients.go:896","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/persistence.(*metadataPersistenceClient).updateErrorMetric\n\t/Users/leow/GOMOD/temporal/common/persistence/persistenceMetricClients.go:896\ngo.temporal.io/server/common/persistence.(*metadataPersistenceClient).GetMetadata\n\t/Users/leow/GOMOD/temporal/common/persistence/persistenceMetricClients.go:874\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshNamespacesLocked\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:435\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshNamespaces\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:425\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshLoop\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:409"}
{"level":"error","ts":"2021-08-28T14:42:05.265+0800","msg":"Error refreshing namespace cache","service":"worker","error":"read tcp 192.168.0.104:50419->13.212.129.122:29042: read: connection reset by peer","logging-call-at":"namespaceCache.go:414","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/cache.(*namespaceCache).refreshLoop\n\t/Users/leow/GOMOD/temporal/common/cache/namespaceCache.go:414"}



{"level":"info","ts":"2021-08-28T14:42:21.560+0800","msg":"none","service":"matching","component":"matching-engine","wf-task-queue-name":"/_sys/temporal-sys-add-search-attributes-task-queue/2","wf-task-queue-type":"Activity","lifecycle":"Stopped","logging-call-at":"taskQueueManager.go:259"}
{"level":"info","ts":"2021-08-28T14:43:05.635+0800","msg":"none","service":"matching","component":"matching-engine","lifecycle":"Starting","wf-task-queue-name":"/_sys/temporal-sys-add-search-attributes-task-queue/2","wf-task-queue-type":"Activity","logging-call-at":"matchingEngine.go:206"}
{"level":"info","ts":"2021-08-28T14:43:05.635+0800","msg":"none","service":"matching","component":"matching-engine","lifecycle":"Started","wf-task-queue-name":"/_sys/temporal-sys-add-search-attributes-task-queue/2","wf-task-queue-type":"Activity","logging-call-at":"matchingEngine.go:209"}
^C{"level":"info","ts":"2021-08-28T14:43:51.239+0800","msg":"Received interrupt signal, stopping the server.","value":"interrupt","logging-call-at":"server.go:230"}
{"level":"info","ts":"2021-08-28T14:43:51.240+0800","msg":"Get dynamic config","name":"frontend.shutdownDrainDuration","value":0,"default-value":0,"logging-call-at":"config.go:79"}
{"level":"info","ts":"2021-08-28T14:43:51.240+0800","msg":"ShutdownHandler: Updating rpc health status to ShuttingDown","service":"frontend","logging-call-at":"service.go:402"}
{"level":"info","ts":"2021-08-28T14:43:51.240+0800","msg":"ShutdownHandler: Waiting for others to discover I am unhealthy","service":"frontend","logging-call-at":"service.go:405"}
{"level":"info","ts":"2021-08-28T14:43:51.240+0800","msg":"ShutdownHandler: Draining traffic","service":"frontend","logging-call-at":"service.go:411"}
{"level":"info","ts":"2021-08-28T14:43:51.240+0800","msg":"Get dynamic config","name":"history.shutdownDrainDuration","value":0,"default-value":0,"logging-call-at":"config.go:79"}
{"level":"info","ts":"2021-08-28T14:43:51.241+0800","msg":"ShutdownHandler: Evicting self from membership ring","service":"history","logging-call-at":"service.go:226"}
{"level":"info","ts":"2021-08-28T14:43:51.240+0800","msg":"RuntimeMetricsReporter stopped","service":"worker","logging-call-at":"runtime.go:160"}
{"level":"info","ts":"2021-08-28T14:43:51.240+0800","msg":"ShutdownHandler: Evicting self from membership ring","service":"matching","logging-call-at":"service.go:151"}
{"level":"info","ts":"2021-08-28T14:43:51.241+0800","msg":"RuntimeMetricsReporter stopped","service":"frontend","logging-call-at":"runtime.go:160"}
{"level":"info","ts":"2021-08-28T14:43:51.242+0800","msg":"Received a ring changed event","service":"history","component":"service-resolver","service":"frontend","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.242+0800","msg":"Received a ring changed event","service":"history","component":"service-resolver","service":"matching","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.245+0800","msg":"Current reachable members","service":"history","component":"service-resolver","service":"matching","addresses":[],"logging-call-at":"rpServiceResolver.go:266"}
{"level":"info","ts":"2021-08-28T14:43:51.243+0800","msg":"frontend stopped","service":"frontend","logging-call-at":"service.go:426"}
{"level":"info","ts":"2021-08-28T14:43:51.243+0800","msg":"Received a ring changed event","service":"matching","component":"service-resolver","service":"frontend","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.243+0800","msg":"Received a ring changed event","service":"matching","component":"service-resolver","service":"matching","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.245+0800","msg":"Current reachable members","service":"matching","component":"service-resolver","service":"matching","addresses":[],"logging-call-at":"rpServiceResolver.go:266"}
{"level":"info","ts":"2021-08-28T14:43:51.246+0800","msg":"ShutdownHandler: Waiting for others to discover I am unhealthy","service":"history","logging-call-at":"service.go:229"}
{"level":"info","ts":"2021-08-28T14:43:51.243+0800","msg":"Received a ring changed event","service":"matching","component":"service-resolver","service":"worker","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.247+0800","msg":"ShutdownHandler: Initiating shardController shutdown","service":"history","logging-call-at":"service.go:232"}
{"level":"info","ts":"2021-08-28T14:43:51.244+0800","msg":"Received a ring changed event","service":"history","component":"service-resolver","service":"frontend","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.247+0800","msg":"none","service":"history","component":"shard-controller","address":"127.0.0.1:7234","lifecycle":"Stopping","logging-call-at":"controller_impl.go:383"}
{"level":"info","ts":"2021-08-28T14:43:51.247+0800","msg":"none","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","lifecycle":"Stopping","component":"shard-engine","logging-call-at":"controller_impl.go:460"}
{"level":"info","ts":"2021-08-28T14:43:51.244+0800","msg":"Received a ring changed event","service":"history","component":"service-resolver","service":"matching","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.244+0800","msg":"Received a ring changed event","service":"history","component":"service-resolver","service":"history","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.244+0800","msg":"Received a ring changed event","service":"history","component":"service-resolver","service":"worker","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.244+0800","msg":"ShutdownHandler: Waiting for others to discover I am unhealthy","service":"matching","logging-call-at":"service.go:153"}
{"level":"info","ts":"2021-08-28T14:43:51.245+0800","msg":"Received a ring changed event","service":"history","component":"service-resolver","service":"history","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.245+0800","msg":"Received a ring changed event","service":"history","component":"service-resolver","service":"worker","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.246+0800","msg":"Received a ring changed event","service":"matching","component":"service-resolver","service":"frontend","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.246+0800","msg":"Received a ring changed event","service":"matching","component":"service-resolver","service":"matching","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.246+0800","msg":"Received a ring changed event","service":"matching","component":"service-resolver","service":"history","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.246+0800","msg":"Received a ring changed event","service":"matching","component":"service-resolver","service":"worker","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.243+0800","msg":"Received a ring changed event","service":"matching","component":"service-resolver","service":"history","logging-call-at":"rpServiceResolver.go:219"}
{"level":"info","ts":"2021-08-28T14:43:51.248+0800","msg":"Current reachable members","service":"history","component":"service-resolver","service":"history","addresses":[],"logging-call-at":"rpServiceResolver.go:266"}
{"level":"info","ts":"2021-08-28T14:43:51.248+0800","msg":"none","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"history-engine","lifecycle":"Stopping","logging-call-at":"historyEngine.go:309"}
{"level":"info","ts":"2021-08-28T14:43:51.248+0800","msg":"none","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"transfer-queue-processor","cluster-name":"active","lifecycle":"Stopping","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:166"}
{"level":"info","ts":"2021-08-28T14:43:51.248+0800","msg":"Current reachable members","service":"matching","component":"service-resolver","service":"history","addresses":[],"logging-call-at":"rpServiceResolver.go:266"}
{"level":"info","ts":"2021-08-28T14:43:51.260+0800","msg":"Queue processor pump shut down.","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"transfer-queue-processor","cluster-name":"active","logging-call-at":"queueProcessor.go:249"}
{"level":"info","ts":"2021-08-28T14:43:51.260+0800","msg":"Task processor shutdown.","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"transfer-queue-processor","cluster-name":"active","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.260+0800","msg":"none","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"transfer-queue-processor","cluster-name":"active","lifecycle":"Stopped","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:179"}
{"level":"info","ts":"2021-08-28T14:43:51.260+0800","msg":"Timer queue processor pump shutting down.","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:202"}
{"level":"info","ts":"2021-08-28T14:43:51.260+0800","msg":"Timer processor exiting.","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:203"}
{"level":"info","ts":"2021-08-28T14:43:51.260+0800","msg":"Task processor shutdown.","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.260+0800","msg":"Timer queue processor stopped.","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:183"}
{"level":"info","ts":"2021-08-28T14:43:51.248+0800","msg":"Get dynamic config","name":"matching.shutdownDrainDuration","value":0,"default-value":0,"logging-call-at":"config.go:79"}
{"level":"info","ts":"2021-08-28T14:43:51.261+0800","msg":"none","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"visibility-queue-processor","lifecycle":"Stopping","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:166"}
{"level":"info","ts":"2021-08-28T14:43:51.261+0800","msg":"RuntimeMetricsReporter stopped","service":"matching","logging-call-at":"runtime.go:160"}
{"level":"info","ts":"2021-08-28T14:43:51.261+0800","msg":"Queue processor pump shut down.","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"visibility-queue-processor","logging-call-at":"queueProcessor.go:249"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"Task processor shutdown.","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"visibility-queue-processor","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"visibility-queue-processor","lifecycle":"Stopped","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:179"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","component":"history-engine","lifecycle":"Stopped","logging-call-at":"historyEngine.go:328"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":3,"address":"127.0.0.1:7234","shard-item":"0xc000135c80","lifecycle":"Stopped","component":"shard-engine","logging-call-at":"controller_impl.go:463"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","lifecycle":"Stopping","component":"shard-engine","logging-call-at":"controller_impl.go:460"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"history-engine","lifecycle":"Stopping","logging-call-at":"historyEngine.go:309"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"transfer-queue-processor","cluster-name":"active","lifecycle":"Stopping","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:166"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"Queue processor pump shut down.","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"transfer-queue-processor","cluster-name":"active","logging-call-at":"queueProcessor.go:249"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"Task processor shutdown.","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"transfer-queue-processor","cluster-name":"active","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"transfer-queue-processor","cluster-name":"active","lifecycle":"Stopped","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:179"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"Timer queue processor pump shutting down.","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:202"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"Timer processor exiting.","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:203"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"Task processor shutdown.","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"Timer queue processor stopped.","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:183"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"visibility-queue-processor","lifecycle":"Stopping","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:166"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"Queue processor pump shut down.","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"visibility-queue-processor","logging-call-at":"queueProcessor.go:249"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"Task processor shutdown.","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"visibility-queue-processor","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"visibility-queue-processor","lifecycle":"Stopped","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:179"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","component":"history-engine","lifecycle":"Stopped","logging-call-at":"historyEngine.go:328"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":4,"address":"127.0.0.1:7234","shard-item":"0xc000854000","lifecycle":"Stopped","component":"shard-engine","logging-call-at":"controller_impl.go:463"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","lifecycle":"Stopping","component":"shard-engine","logging-call-at":"controller_impl.go:460"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"history-engine","lifecycle":"Stopping","logging-call-at":"historyEngine.go:309"}
{"level":"info","ts":"2021-08-28T14:43:51.283+0800","msg":"none","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"transfer-queue-processor","cluster-name":"active","lifecycle":"Stopping","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:166"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Queue processor pump shut down.","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"transfer-queue-processor","cluster-name":"active","logging-call-at":"queueProcessor.go:249"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Task processor shutdown.","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"transfer-queue-processor","cluster-name":"active","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"none","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"transfer-queue-processor","cluster-name":"active","lifecycle":"Stopped","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:179"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Timer queue processor pump shutting down.","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:202"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Timer processor exiting.","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:203"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Task processor shutdown.","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Timer queue processor stopped.","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:183"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"none","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"visibility-queue-processor","lifecycle":"Stopping","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:166"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Queue processor pump shut down.","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"visibility-queue-processor","logging-call-at":"queueProcessor.go:249"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Task processor shutdown.","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"visibility-queue-processor","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"none","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"visibility-queue-processor","lifecycle":"Stopped","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:179"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"none","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","component":"history-engine","lifecycle":"Stopped","logging-call-at":"historyEngine.go:328"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"none","service":"history","shard-id":1,"address":"127.0.0.1:7234","shard-item":"0xc000135800","lifecycle":"Stopped","component":"shard-engine","logging-call-at":"controller_impl.go:463"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"none","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","lifecycle":"Stopping","component":"shard-engine","logging-call-at":"controller_impl.go:460"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"none","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"history-engine","lifecycle":"Stopping","logging-call-at":"historyEngine.go:309"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"none","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"transfer-queue-processor","cluster-name":"active","lifecycle":"Stopping","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:166"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Queue processor pump shut down.","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"transfer-queue-processor","cluster-name":"active","logging-call-at":"queueProcessor.go:249"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Task processor shutdown.","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"transfer-queue-processor","cluster-name":"active","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"none","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"transfer-queue-processor","cluster-name":"active","lifecycle":"Stopped","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:179"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Timer queue processor pump shutting down.","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:202"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Timer processor exiting.","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:203"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Task processor shutdown.","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Timer queue processor stopped.","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"timer-queue-processor","cluster-name":"active","component":"timer-queue-processor","logging-call-at":"timerQueueProcessorBase.go:183"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"none","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"visibility-queue-processor","lifecycle":"Stopping","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:166"}
{"level":"info","ts":"2021-08-28T14:43:51.284+0800","msg":"Queue processor pump shut down.","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"visibility-queue-processor","logging-call-at":"queueProcessor.go:249"}
{"level":"info","ts":"2021-08-28T14:43:51.285+0800","msg":"Task processor shutdown.","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"visibility-queue-processor","logging-call-at":"taskProcessor.go:155"}
{"level":"info","ts":"2021-08-28T14:43:51.285+0800","msg":"none","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"visibility-queue-processor","lifecycle":"Stopped","component":"transfer-queue-processor","logging-call-at":"queueProcessor.go:179"}
{"level":"info","ts":"2021-08-28T14:43:51.285+0800","msg":"none","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","component":"history-engine","lifecycle":"Stopped","logging-call-at":"historyEngine.go:328"}
{"level":"info","ts":"2021-08-28T14:43:51.285+0800","msg":"none","service":"history","shard-id":2,"address":"127.0.0.1:7234","shard-item":"0xc000135a00","lifecycle":"Stopped","component":"shard-engine","logging-call-at":"controller_impl.go:463"}
{"level":"info","ts":"2021-08-28T14:43:51.285+0800","msg":"none","service":"history","component":"shard-controller","address":"127.0.0.1:7234","lifecycle":"Stopped","logging-call-at":"controller_impl.go:178"}
{"level":"info","ts":"2021-08-28T14:43:51.285+0800","msg":"ShutdownHandler: Waiting for traffic to drain","service":"history","logging-call-at":"service.go:234"}
{"level":"info","ts":"2021-08-28T14:43:51.285+0800","msg":"ShutdownHandler: No longer taking rpc requests","service":"history","logging-call-at":"service.go:237"}
{"level":"info","ts":"2021-08-28T14:43:51.285+0800","msg":"Replication task fetchers stopped.","service":"history","logging-call-at":"replicationTaskFetcher.go:171"}
{"level":"info","ts":"2021-08-28T14:43:51.285+0800","msg":"RuntimeMetricsReporter stopped","service":"history","logging-call-at":"runtime.go:160"}
{"level":"info","ts":"2021-08-28T14:43:51.285+0800","msg":"history stopped","service":"history","logging-call-at":"service.go:246"}
{"level":"error","ts":"2021-08-28T14:43:54.988+0800","msg":"Membership upsert failed.","service":"history","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
{"level":"error","ts":"2021-08-28T14:43:55.228+0800","msg":"Membership upsert failed.","service":"frontend","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
^C




{"level":"error","ts":"2021-08-28T14:44:08.989+0800","msg":"Membership upsert failed.","service":"history","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
{"level":"error","ts":"2021-08-28T14:44:10.228+0800","msg":"Membership upsert failed.","service":"frontend","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
{"level":"warn","ts":"2021-08-28T14:44:20.663+0800","msg":"Failed to cancel outstanding poller.","service":"frontend","wf-task-queue-name":"/_sys/temporal-sys-history-scanner-taskqueue-0/1","error":"Not enough hosts to serve the request","logging-call-at":"workflowHandler.go:3399"}
{"level":"warn","ts":"2021-08-28T14:44:20.747+0800","msg":"Failed to cancel outstanding poller.","service":"frontend","wf-task-queue-name":"Mojaves-iMac.local:dae73def-31d3-4e33-8ea2-f3e4bb952e32","error":"Not enough hosts to serve the request","logging-call-at":"workflowHandler.go:3399"}
{"level":"error","ts":"2021-08-28T14:44:20.990+0800","msg":"Membership upsert failed.","service":"history","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
{"level":"warn","ts":"2021-08-28T14:44:21.156+0800","msg":"Failed to cancel outstanding poller.","service":"frontend","wf-task-queue-name":"temporal-sys-processor-parent-close-policy","error":"Not enough hosts to serve the request","logging-call-at":"workflowHandler.go:3399"}
{"level":"error","ts":"2021-08-28T14:44:24.229+0800","msg":"Membership upsert failed.","service":"frontend","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
^C^C^C^C^C{"level":"error","ts":"2021-08-28T14:44:30.991+0800","msg":"Membership upsert failed.","service":"history","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
{"level":"error","ts":"2021-08-28T14:44:36.235+0800","msg":"Membership upsert failed.","service":"frontend","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
{"level":"error","ts":"2021-08-28T14:44:42.993+0800","msg":"Membership upsert failed.","service":"history","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
{"level":"error","ts":"2021-08-28T14:44:50.237+0800","msg":"Membership upsert failed.","service":"frontend","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}

{"level":"error","ts":"2021-08-28T14:44:52.995+0800","msg":"Membership upsert failed.","service":"history","error":"operation UpsertClusterMembership encounter session has been closed","logging-call-at":"rpMonitor.go:276","stacktrace":"go.temporal.io/server/common/log.(*zapLogger).Error\n\t/Users/leow/GOMOD/temporal/common/log/zap_logger.go:143\ngo.temporal.io/server/common/membership.(*ringpopMonitor).startHeartbeatUpsertLoop.func1\n\t/Users/leow/GOMOD/temporal/common/membership/rpMonitor.go:276"}
''make: *** [server] Killed: 9

```


---

### #1840: Temporal Elasticsearch support for custom index creation

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1840 |
| **State** | OPEN |
| **Author** | ilblum |
| **Created** | 2021-08-19 21:05:06.000 UTC (4y 4m ago) |
| **Updated** | 2023-03-03 20:21:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | alexshtin |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently temporal only supports a single elastic search index - '[temporal_visibility_v1_dev](https://docs.temporal.io/docs/server/elasticsearch-setup/)'  
Temporal offers users advance search attributes - 'upsert-search-attributes', but with temporal elastic search implementation there is no solution in place to configure these attributes to different elastic search indices.  All of the search attributes can only be accessed by a single elastic index.
If there are 5 customer upsert-search attributes, then the only available elastic search index would be:

Elastic search index: **temporal_visibility_v1_dev**

- Custom Search field 1
- Custom Search field 2
- Custom Search field 3
- Custom Search field 4
- Custom Search field 5

**Describe the solution you'd like**

User should be able to setup multiple, in this example 3 custom indices with customized custom search field mapping and update.
For example: 
  **Custom index 1:**
- Custom Search field 1
- Custom Search field 2
- Custom Search field 3

  **Custom index 2:**
- Custom Search field 2
- Custom Search field 3
- Custom Search field 4

  **Custom index 3:**
- Custom Search field 1
- Custom Search field 4
- Custom Search field 5


**Describe alternatives you've considered**
A large single index covering all of the custom search fields presents access challenges

There is another workaround where Custom elastic indices can be updated directly via workflow activities but then temporal native rollback will not work.



---

### #1757: Document dynamic config behavior

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1757 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2021-07-23 17:53:34.000 UTC (4y 5m ago) |
| **Updated** | 2023-03-03 20:21:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently there are 2 kinds of dynamic configs:
* truly dynamic, e.g. business logic will check the newest value each time
* not truly dynamic, e.g. business logic will only check the value at server start up, e.g. thread pool size

**Describe the solution you'd like**
Document above 2 categories for easy reference.


---

### #1579: Continued submission of tasks to an orphaned task queue can lead to Cassandra tombstone scanning issues

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1579 |
| **State** | OPEN |
| **Author** | rfwagner (Richard Wagner) |
| **Created** | 2021-05-27 22:24:15.000 UTC (4y 7m ago) |
| **Updated** | 2023-03-03 20:22:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | potential-bug, difficulty: medium, operations |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

It is possible for poorly managed user executions to result in scenarios that cause Cassandra tombstone scanning issues.

We ran into one such scenario:

- an integration test ran every hour of every day. It created a cron workflow that fired every minute
- under normal circumstances, the test would verify that the cron workflow ran 2 or 3 times and then terminate the workflow
- but if the test crashed or there was test infrastructure failure, the workflow was never terminated, resulting in a cron that created a new workflow task every minute, forever
- these workflows were being submitted to unique task queues (every run of the test created a task queue with a uuid in the name)
- so there were never any workers to pick up these wf tasks and advance the cursor on the tasks table
- but since new tasks were being submitted by the orphaned cron every minute, the task manager for the task queue remained active and the taskReader would scan over all the tombstones resulting from the TTL'ed tasks on the queue

From speaking with Samar it sounds like one possible approach is to stop using TTL's to remove expired tasks from the tasks table and instead delete them explicitly. This would give the task reader an opportunity to move the cursor when it determined there were no active tasks beyond a certain point.



---

### #1552: Better resource limit exceeded error message

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1552 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2021-05-17 21:40:45.000 UTC (4y 7m ago) |
| **Updated** | 2023-03-03 20:22:08.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, operations |
| **Assignees** | Ardagan |
| **Milestone** | None |

#### Description

Temporal server should include type of throttling limit & throttling QPS within resource limit exceeded error message.


---

### #1080: Use workflow raw history events API 

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1080 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-12-11 22:16:18.000 UTC (5 years ago) |
| **Updated** | 2023-03-03 20:22:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, performance, difficulty: medium |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently the workflow history events will be deserialized then serialized then deserialized again with passing to SDK
We should remove the unnecessary deserialized then serialized logic within frontend

Necessary changes need to be made in frontend and both SDKs


---

### #1041: Ensure major.minor.patch to major.minor+1.patch upgrade

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1041 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-12-01 21:01:28.000 UTC (5y 1m ago) |
| **Updated** | 2023-03-03 20:23:09.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, versioning, difficulty: easy |
| **Assignees** | alexshtin |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Temporal server should ensure major.minor.patch to major.minor+1.patch upgrade path



---

### #1021: Maintenance workflow cleanup stale queue tasks

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/1021 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-11-24 00:05:57.000 UTC (5y 1m ago) |
| **Updated** | 2023-03-03 20:23:06.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, difficulty: easy, planning, operations |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Temporal worker should have background maintenance workflow cleaning stale workflow tasks / activity tasks, even for Cassandra

A task is considered stale if corresponding workflow is finished and no poller is polling the corresponding task queue.

**Describe the solution you'd like**
Similar implementation like: https://github.com/temporalio/temporal/tree/master/service/worker/scanner



---

### #973: Support of batch activity

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/973 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-11-11 07:28:16.000 UTC (5y 1m ago) |
| **Updated** | 2020-11-11 07:28:16.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, performance |
| **Assignees** | wxing1292 |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Add support of batch activity execution and batch wait to same unnecessary command tasks



---

### #972: Have a Deleted state for entities state machine in mutable state

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/972 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-11-11 07:27:17.000 UTC (5y 1m ago) |
| **Updated** | 2021-07-04 20:08:20.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, performance, difficulty: medium, operations |
| **Assignees** | wxing1292 |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Conduct a design discussion about possibility of NOT deleting record from Cassandra, but rather mark the record as deleted, to deal with possible Cassandra tombstone issue.



---

### #940: Unify all persistence TTL handling logic

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/940 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-11-02 02:11:37.000 UTC (5y 2m ago) |
| **Updated** | 2021-07-04 20:18:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, refactoring |
| **Assignees** | yiminc |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Unify all persistence TTL handling logic since currently SQL / NoSQL record TTL are handled differently.

**Describe the solution you'd like**
SQL should already have corresponding worker setup for DB record TTL handling, NoSQL should follow the same

**Describe alternatives you've considered**
N/A

**Additional context**
N/A



---

### #939: Persistence Layer Unified Serialization / Deserialization

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/939 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-11-02 02:07:08.000 UTC (5y 2m ago) |
| **Updated** | 2021-07-04 20:19:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, refactoring |
| **Assignees** | yiminc |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Unify all persistence serialization / deserialization login into one place, instead of letting SQL / NoSQL layer handle separately

**Describe the solution you'd like**
See above

**Describe alternatives you've considered**
N/A

**Additional context**
N/A



---

### #926: Add configurable initialDelay for Cron job setup for the first run

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/926 |
| **State** | OPEN |
| **Author** | doqtri |
| **Created** | 2020-10-30 05:59:23.000 UTC (5y 2m ago) |
| **Updated** | 2021-07-04 06:53:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, API, CRON |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
At the moment, Cronjob configuration automatically calculates initial delay for the first run. It's unpredictable. We should have such configuration to trigger the first run as expectation.

**Describe the solution you'd like**
Client sdk should have one option to configure initialDelay for cron schedule. Like this:
`workflowOptions.setInitialDelay(Long seconds)`

**Describe alternatives you've considered**
N/A

**Additional context**
N/A



---

### #891: History service should pause accepting new signals if worker cannot keep up.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/891 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-10-23 00:16:37.000 UTC (5y 2m ago) |
| **Updated** | 2021-07-04 06:55:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, difficulty: medium |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
History service should pause accepting new signals if worker cannot keep up.

**Describe the solution you'd like**
Pause functionality, in case worker cannot keep up with the incoming events.

**Describe alternatives you've considered**
N/A

**Additional context**
N/A



---

### #881: Matching Service Task Queue Metadata Management

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/881 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-10-21 00:30:14.000 UTC (5y 2m ago) |
| **Updated** | 2022-11-05 03:08:35.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, performance, difficulty: medium |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently, partitioned matching task queue may encounter issue if the following scenario happens:
1. t = 0: task added to task queue `queue` with partition 42
2. t = 1: no poller appears, task is persisted to DB
3. t = 2: matching host with task queue `queue` with partition 42 shutdown

according to the current arch, each task queue manager (within matching service) is responsible for loading tasks from DB and push the task to a common rally point if task queue is partitioned, so poller, can poll existing tasks, without starvation.

the above arch has a flaw, i.e. if the task queue (with partition) is not directly polled, and task queue manager not initialized, then the poller may take a long time picking up some tasks (if num of worker << number of partition & randomization)

**Describe the solution you'd like**
1. common metadata manager / service / component which will notify all matching service to initialize corresponding task queue manager
2. round robin polling

**Describe alternatives you've considered**
N/A

**Additional context**
Possibly related issue: #459



---

### #852: Support in mem TLS configuration / data for PostgreSQL

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/852 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-10-14 01:47:29.000 UTC (5y 2m ago) |
| **Updated** | 2020-10-19 17:51:07.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, config |
| **Assignees** | wxing1292 |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Support in mem key / cert / ca for PostgreSQL
ref: https://github.com/lib/pq/issues/789

**Describe the solution you'd like**
Support in mem key / cert / ca for PostgreSQL

**Describe alternatives you've considered**
N/A

**Additional context**
N/A


---

### #848: Support blue/green cluster deployment with incompatible cluster membership changes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/848 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-10-13 22:54:34.000 UTC (5y 2m ago) |
| **Updated** | 2020-10-13 22:54:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Sometimes a backward-incompatible change in the cluster membership protocol is needed. In this case a newly deployed instances will not be able to reach the old one and the cluster will be partitioned between the old and new versions. This leads to downtime during deployment due to constant shard stealing between the partitions.

**Describe the solution you'd like**
Provide a mechanism to not distribute history and matching shards to the newly added hosts while they are deployed. After the deployment is complete flip a config switch to using only the newly added hosts for shard hosting.  This way the deployment of such an incompatible version would be possible without downtime.




---

### #810: Revisit persistence interface (SQL / NoSQL) before modularization

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/810 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-10-07 22:28:27.000 UTC (5y 2m ago) |
| **Updated** | 2021-07-04 07:04:03.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, refactoring, packaging |
| **Assignees** | wxing1292, yiminc |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Before making the persistence layer modular and ready for public override, revisit the interfaces and naming conventions.

**Describe the solution you'd like**
N/A

**Describe alternatives you've considered**
N/A

**Additional context**
N/A


---

### #791: Revisit matching engine ListTaskQueue functionality

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/791 |
| **State** | OPEN |
| **Author** | wxing1292 (Wenquan Xing) |
| **Created** | 2020-10-03 03:52:19.000 UTC (5y 3m ago) |
| **Updated** | 2021-07-04 07:05:13.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | needs-investigation |
| **Assignees** | wxing1292 |
| **Milestone** | None |

#### Description

Revisit the code logic in matching engine: 
https://github.com/temporalio/temporal/blob/36525fad464757f59d1e963ccba3b3c46aff0e33/common/persistence/sql/sqlTaskManager.go#L262-L274


---

### #741: Support reset starting from an activity of a specific type.

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/741 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-09-19 23:10:02.000 UTC (5y 3m ago) |
| **Updated** | 2021-07-04 07:17:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, architecture, difficulty: hard |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
From a [support topic:](https://community.temporal.io/t/best-way-to-get-the-previous-decisiontaskcompleted-id-from-a-specific-activity/638):

> The other day we had this issue where the team responsible for service C told us to rerun the request again since they were presenting issues and the 200 OK we got was a false positive.
> So we did it manually, They were just a few requests affected so we could do it without problems. But now we wonder what would happen when we got 1000 affected requests and we had to do it again.
> That‚Äôs why we were trying to find a way to get (in a programmatic way) the event id to use in the CLI.

**Describe the solution you'd like**
Support reset that accepts an activity type and status (OK, Failed, Any) and resets workflow from that point. It assumes that reset can recreate the activity task for all activities that completed after the reset point.



---

### #699: History Engine refactoring

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/699 |
| **State** | OPEN |
| **Author** | samarabbas (Samar Abbas) |
| **Created** | 2020-08-31 01:29:29.000 UTC (5y 4m ago) |
| **Updated** | 2021-07-04 07:19:53.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, refactoring, testing |
| **Assignees** | wxing1292 |
| **Milestone** | None |

#### Description

All the unit test for history engine are quite messy specially after 2dc removal.  We need to refactor history engine code base into smaller packages.  Following test suites need lots of attention:
```
TestEngine2Suite
TestEngine3Suite
TestEngineSuite
```


---

### #689: Add dynamic config knob to limit maximum duration for Timers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/689 |
| **State** | OPEN |
| **Author** | mastermanu |
| **Created** | 2020-08-25 23:53:25.000 UTC (5y 4m ago) |
| **Updated** | 2021-07-04 07:22:10.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, API, operations |
| **Assignees** | None |
| **Milestone** | None |

#### Description

Administrators may want to enforcer that Workflow Timers beyond a certain duration are not allowed to be scheduled (and would fail), as perhaps they want to force customers to use ContinueAsNew and ensure that their Workflows never exceed a certain age.


---

### #677: Support for loading history events using size of batches rather than count

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/677 |
| **State** | OPEN |
| **Author** | samarabbas (Samar Abbas) |
| **Created** | 2020-08-14 03:29:02.000 UTC (5y 4m ago) |
| **Updated** | 2021-07-03 23:17:51.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, difficulty: hard, operations |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Temporal writes event to historystore in a batch which can be of variable sizes.  When we query 
for a page of event we set the page size based on number of batches we read from database.
This could be a problem if user is writing very large payloads and single batch is large in size.
We may end of loading unnecessary events which we have to throw away when returning the 
events back to client based on the page size set by the caller. 

**Describe the solution you'd like**
We need to design a solution which allows us to load events based on size rather than count.

**Describe alternatives you've considered**
Another alternate is to still load on count but have throttling in place using size of payloads 
loaded from DB.  Issue #675 




---

### #675: Rate limiting pagination based on history sizes

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/675 |
| **State** | OPEN |
| **Author** | samarabbas (Samar Abbas) |
| **Created** | 2020-08-13 18:01:18.000 UTC (5y 4m ago) |
| **Updated** | 2021-07-03 23:18:42.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, operations |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
We have rate limiting logic at various levels based on API call rps.  This does not takes into
account large number of data we may end up shipping over the wire for certain api calls 
which results in loading huge payloads from database and shipping it over the wire.
GetHistory is one example where api rate limiting does not act as a good protection.

**Describe the solution you'd like**
Have rate limiting logic which also takes into account payload sizes and throttle namespaces 
which goes over configured quota.




---

### #595: Allow configuration of archival config through docker

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/595 |
| **State** | OPEN |
| **Author** | samarabbas (Samar Abbas) |
| **Created** | 2020-07-22 22:04:31.000 UTC (5y 5m ago) |
| **Updated** | 2021-07-03 23:32:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, packaging, docker |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Temporal docker image does not provide a mechanism for users to pass in there own archival config for the server.

**Describe the solution you'd like**
Update [config_template.yaml](https://github.com/temporalio/temporal/blob/master/docker/config_template.yaml#L174) to have ability for overriding archival config.

**Describe alternatives you've considered**

**Additional context**



---

### #589: listTaskQueuePartitions in MatchingService matching engine needs unit tests

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/589 |
| **State** | OPEN |
| **Author** | mastermanu |
| **Created** | 2020-07-22 01:27:41.000 UTC (5y 5m ago) |
| **Updated** | 2022-11-05 03:08:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | testing |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

There was a fairly blatant bug in the implementation of the API (have to go back and see if this was a merge issue or something else). Even the most basic of unit tests would have caught this. We have a PR to fix the bug immediately, but we need to invest in testing here


---

### #574: Background scan to remove unnecessary timers

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/574 |
| **State** | OPEN |
| **Author** | samarabbas (Samar Abbas) |
| **Created** | 2020-07-20 23:03:00.000 UTC (5y 5m ago) |
| **Updated** | 2021-07-03 23:35:50.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, difficulty: medium, operations |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
There are many situations where tasks in Temporal are protected by timeout tasks.  Most of the time these timers end up being redundant when workflows/tasks complete successfully.  
Further user can create user timers in the future and never need them because workflow completes well before them.  

**Describe the solution you'd like**
I'm proposing a background job which scans all timer queues and delete any tasks for expired workflows/tasks.  
One thing to consider is that we use range deletes when cleaning up timer queues and moving cursor.  This may need not be an option to remove tasks individually.

**Describe alternatives you've considered**

**Additional context**




---

### #550: creating a dev environment using podman instead of docker-compose

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/550 |
| **State** | OPEN |
| **Author** | Lercher (Martin Lercher) |
| **Created** | 2020-07-16 18:13:26.000 UTC (5y 5m ago) |
| **Updated** | 2021-07-03 23:39:29.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, packaging, docker |
| **Assignees** | underrun |
| **Milestone** | None |

#### Description

This is more an offer instead of a request: My corporation only allows containers running in user space, so docker is a no-go for me. I wanted to run a dev environment using podman instead of docker-compose. Here is a yaml file I'd like to share with the community for version 0.26 plus cassandra: https://gitlab.com/lercher/wf/-/blob/master/temporal.yaml

After downloading, a local pod can be created and started via `podman play kube temporal.yaml`. Starting, stoping and removing the pod: `podman pod start/stop/rm temporal`. Hoping that helps ppl like me with this kind of "root" problems.

My suggestion is that something like that finds its way into the code base and the documentation, eventually. As I'm quite un-experienced using containers, that's probably a better option compared to what I did.

<hr>

For easier reference, this is the current content of the file mentioned:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: temporal
spec:
  restartPolicy: Always

  containers:

  - name: cassandra
    image: cassandra:3.11
    ports:
    - containerPort: 9042
      hostPort: 9042
    resources: {}
  
  - name: temporal-srv
    image: temporalio/auto-setup:0.26.0
    env:
    - name: CASSANDRA_SEEDS
      value: localhost
    - name: DYNAMIC_CONFIG_FILE_PATH
      value: config/dynamicconfig/development.yaml
    - name: BIND_ON_IP
      value: 127.0.0.1
    ports:
    - containerPort: 7233
      hostPort: 7233
    resources: {}

  - name: temporal-web
    image: temporalio/web:0.26.0
    env:
    - name: TEMPORAL_GRPC_ENDPOINT
      value: localhost:7233
    ports:
    - containerPort: 8088
      hostPort: 8088
    resources: {}
```


---

### #373: Change ParentClosePolicy to struct and add applyOnParentContinueAsNew

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/373 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-05-13 17:15:40.000 UTC (5y 7m ago) |
| **Updated** | 2023-03-03 20:23:34.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, API, difficulty: easy |
| **Assignees** | None |
| **Milestone** | None |

#### Description

**Is your feature request related to a problem? Please describe.**
Currently a ParentClosePolicy is always applied when parent calls continue as new. In the majority of the cases it is not a desired behaviour and the policy should be applied only when a parent completes its execution.

**Describe the solution you'd like**
Change ParentClosePolicy to a structure with two fields: The current ParentClosePolicy enum and 
applyOnParentContinueAsNew. When applyOnParentContinueAsNew is set to true then the policy applies when parent continues as new. When set to false only when parent completes.




---

### #309: Make long poll interval configurable through the service config

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/309 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-04-15 16:00:21.000 UTC (5y 8m ago) |
| **Updated** | 2023-03-03 20:23:33.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | enhancement, API, config |
| **Assignees** | dnr |
| **Milestone** | Initial Temporal Release |

#### Description

Currently long poll intervals are hardcoded and cannot be changed as it is going to break clients which have too short of a deadline.


---

### #294: Return LongPollExpirationInterval on poll call results

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/294 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-04-11 22:10:10.000 UTC (5y 8m ago) |
| **Updated** | 2023-03-03 20:23:31.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | API |
| **Assignees** | dnr |
| **Milestone** | None |

#### Description

Ideally it should be also returned by frontend as part of the timeout error.
This way worker code can adjust automatically for the correct long poll interval.


---

### #293: Fix names of dynamic config properties

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/293 |
| **State** | OPEN |
| **Author** | mfateev (Maxim Fateev) |
| **Created** | 2020-04-11 19:16:12.000 UTC (5y 8m ago) |
| **Updated** | 2023-03-03 20:23:30.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | refactoring, API, config, difficulty: easy |
| **Assignees** | Ardagan |
| **Milestone** | None |

#### Description

Names of dynamic config properties are not consistent. Some of them start from capital letter, some of them from lower case. Some of them use abbreviations as "num", "max", while others use the whole words.
The same applies to the contant names. Some of them are prepended with the component like Worker, Matching, History, but the large number of them is not.
Also sorting them in the alphabetic order per component would make sense.

https://github.com/temporalio/temporal/blob/master/common/service/dynamicconfig/constants.go


---

### #84: Group and categorize tests

| Field | Value |
|-------|-------|
| **URL** | https://github.com/temporalio/temporal/issues/84 |
| **State** | OPEN |
| **Author** | alexshtin (Alex Shtin) |
| **Created** | 2020-01-25 00:48:03.000 UTC (5y 11m ago) |
| **Updated** | 2021-07-03 22:22:46.000 UTC |
| **Upvotes** | 0 |
| **Comments** | 0 |
| **Priority Score** | 0 |
| **Labels** | refactoring, testing |
| **Assignees** | alexshtin |
| **Milestone** | None |

#### Description

We currently have integration tests inside `host` and all other tests are considered to be unit tests. But many of them actually require at least database. We need to categorize all tests:
1. **Pure unit tests**. Doesn't require any external dependency, can be run quickly to verify basic things.
2. **Integration tests, Suite 0**. Requires database and use mocks for other dependencies.
3. **Integration tests, Suite 1**. Doesn't use mocks, but uses `onebox` implementation of server (which needs to be refactored to use as much as possible of production code path).
4. **End to end tests**. Starts real `temporal-server` and make requests to it through API and client.

Every test category should have target in `Makefile` and instruction how to run locally.


