{"assignees":[],"author":{"id":"U_kgDOCS9Z8w","is_bot":false,"login":"tetrakatech","name":"Karla"},"body":"### What are you really trying to do?\r\n\r\nWe have workflows that are routinely cancelled by other (orchestrator) workflows. Usually this works perfectly, and allows our system to implement preemption in line with our business requirements.\r\n\r\nSporadically - a few times a day, out of at least thousands of workflows - the child workflow is failing to cancel correctly, and is instead stalling in a running state, leading to ~infinite retries and blocking of the orchestrator workflow.\r\n\r\n### Describe the bug\r\n\r\nTypically, when cancellation occurs we see a WorkflowTaskCompleted event in the Temporal UI, followed by a WorkflowExecutionCanceledEvent.\r\n\r\n![image](https://github.com/user-attachments/assets/ae57764d-496a-46f9-a39f-6a9cc0e0fe6f)\r\n\r\nIn the buggy cases, we instead see two instances of WorkflowTaskFailed, with the call stack being a `CancelledFailure` inside the Temporal SDK, and the workflow left stuck in a Running state:\r\n<img width=\"1697\" alt=\"image\" src=\"https://github.com/user-attachments/assets/71ed68b6-5dc3-4438-bd33-5fe25f3ecbcd\">\r\n\r\nLogs for a buggy case run ID:\r\n\r\n```\r\n2024-11-28 00:54:22.293 Workflow started\r\n2024-11-28 00:54:22.351 Activity started\r\n2024-11-28 00:54:23.030 Activity completed\r\n2024-11-28 00:54:23.700 Workflow completed as cancelled\r\n2024-11-28 00:54:23.700 Failing workflow task\r\n2024-11-28 00:54:24.105 Workflow completed as cancelled\r\n2024-11-28 00:54:24.108 Failing workflow task\r\n2024-11-28 00:54:24.193 {\"component\":\"matching-engine\", \"error\":\"Workflow task not found.\", \"level\":\"info\", \"logging-call-at\":\"matching_engine.go:613\", \"msg\":\"Workflow task not found\", \"queue-task-id\":-137, \"queue-task-visibility-timestamp\":\"2024-11-27T13:54:24.180Z\", \"ts\":\"2024-11-27T13:54:24.193Z\", \"wf-history-event-id\":16, \"wf-id\":\"9ee39789-3ca2-4fc3-af5c-8bd24a7bbe28-1-378bf7e7-bc20-4b26-b459-c9e7633aa5c6\", \"wf-namespace-id\":\"5e9a725c-d677-4281-8b07-9cd4097060bd\", \"wf-run-id\":\"dd593d4f-3f24-4177-b376-ba022ebb5d10\", \"wf-task-queue-name\":\"chatExecutor\"}\r\n2024-11-28 00:54:24.290999889 Workflow completed as cancelled\r\n2024-11-28 00:54:34.270999908 Workflow completed as cancelled\r\n2024-11-28 00:54:44.298000097 Workflow completed as cancelled\r\n2024-11-28 00:54:59.002000093 Workflow completed as cancelled\r\n2024-11-28 00:55:18.104000091 Workflow completed as cancelled\r\n2024-11-28 00:55:46.848999977 Workflow completed as cancelled\r\n2024-11-28 00:56:30.530999898 Workflow completed as cancelled\r\n2024-11-28 00:57:55.980000019 Workflow completed as cancelled\r\n2024-11-28 01:00:26.433630347 {\"address\":\"10.20.12.44:7234\", \"attempt\":10, \"level\":\"warn\", \"logging-call-at\":\"workflow_task_state_machine.go:1061\", \"msg\":\"Critical attempts processing workflow task\", \"service\":\"history\", \"shard-id\":130, \"ts\":\"2024-11-27T14:00:26.433Z\", \"wf-id\":\"9ee39789-3ca2-4fc3-af5c-8bd24a7bbe28-1-378bf7e7-bc20-4b26-b459-c9e7633aa5c6\", \"wf-namespace\":\"default\", \"wf-run-id\":\"dd593d4f-3f24-4177-b376-ba022ebb5d10\"}\r\n2024-11-28 01:00:26.480000019 Workflow completed as cancelled\r\n2024-11-28 01:05:07.210610845 {\"address\":\"10.20.12.44:7234\", \"attempt\":11, \"level\":\"warn\", \"logging-call-at\":\"workflow_task_state_machine.go:1061\", \"msg\":\"Critical attempts processing workflow task\", \"service\":\"history\", \"shard-id\":130, \"ts\":\"2024-11-27T14:05:07.209Z\", \"wf-id\":\"9ee39789-3ca2-4fc3-af5c-8bd24a7bbe28-1-378bf7e7-bc20-4b26-b459-c9e7633aa5c6\", \"wf-namespace\":\"default\", \"wf-run-id\":\"dd593d4f-3f24-4177-b376-ba022ebb5d10\"}\r\n```\r\n\r\nand then progressively more retries that are backed off.\r\n\r\n### Minimal Reproduction\r\n\r\nI've attached a (redacted) copy of the JSON events for one of the buggy cases. I'm hoping this is sufficient to repro, since the issue repeats on workflow re-attempts, and the example history is quite short (< 20 events).\r\n[dd593d4f-3f24-4177-b376-ba022ebb5d10_events.json](https://github.com/user-attachments/files/18001967/dd593d4f-3f24-4177-b376-ba022ebb5d10_events.json)\r\n\r\nThe workflow implementation to hit this case is trivially simple:\r\n\r\n```typescript\r\nconst { handleCustomerMessage } =\r\n  proxyActivities<IHandleCustomerMessageActivity>({\r\n  startToCloseTimeout: '1 minute',\r\n  retry: {\r\n    maximumAttempts: 3,\r\n  },\r\n  cancellationType: ActivityCancellationType.TRY_CANCEL,\r\n})\r\n\r\nexport async function processChatTicket({\r\n  data,\r\n}: {\r\n  data: ProcessTicketInput\r\n}): Promise<ProcessTicketOutput> {\r\n  try {\r\n    const temporalWorkflowId = workflowInfo().workflowId\r\n\r\n    const handleCustomerMessageOutput = await handleCustomerMessage({\r\n      temporalWorkflowId,\r\n      data,\r\n    })\r\n\r\n    // More logic below...\r\n   } catch (error) {\r\n    // If the error is just a cancellation, or a termination, just re-throw\r\n    // immediately. The former is expected (e.g. a parent will cancel this\r\n    // workflow if a new message is received; the latter is triggered manually\r\n    // in the UI by a human in very limited circumstances)\r\n    if (isCancellation(error) || error instanceof TerminatedFailure) {\r\n      throw error\r\n    }\r\n\r\n    // Otherwise, kick off a \"cleanup\" workflow\r\n    await startChild(cleanup, {\r\n      // Other arguments\r\n      parentClosePolicy: ParentClosePolicy.PARENT_CLOSE_POLICY_ABANDON,\r\n    })\r\n\r\n    throw error\r\n  }\r\n}\r\n```\r\n\r\n### Environment/Versions\r\n\r\n<!-- Please complete the following information where relevant. -->\r\n\r\n- OS and processor: Kubernetes running `node:lts-bookworm-slim` (workers)\r\n- Temporal Version: Kubernetes running `temporalio/server:1.24.1` (server), Typescript SDK `1.11.3` (workers)\r\n- Are you using Docker or Kubernetes or building Temporal from source? Kubernetes\r\n\r\n### Additional context\r\n\r\nBased on the log lines, it looks to me like something is getting confused between the Typescript and Core SDKs as to whether the cancellation error thrown represents a WorkflowTask failure or not.\r\n\r\nNotably, the logs around the task cancellation in Typescript are run in both buggy/working cases: https://github.com/temporalio/sdk-typescript/blob/be37851670c5c43ae4dc2ea1929797fbbcbffc26/packages/workflow/src/logs.ts#L99\r\n\r\nBut the log around the workflow task failing is over here, https://github.com/temporalio/sdk-core/blob/4a2368d19f57e971ca9b2465f1dbeede7a861c34/core/src/worker/workflow/mod.rs#L423, and is only called in the buggy case.\r\n\r\nWith my limited knowledge of how this code works, is it possible that `this.cancelled` (https://github.com/temporalio/sdk-typescript/blob/main/packages/workflow/src/internals.ts#L945) isn't being set correctly in all cases, especially when an Activity runs and successfully completes in parallel with a ActivityTaskCancelRequested event being generated? If that were the case, the Typescript SDK would I think indicate a workflow failure on cancellation, rather than cancelling cleanly, but would still emit the logs we're seeing.\r\n\r\nHappy to provide any other details if they'd be useful :)\r\n","closedAt":null,"comments":[{"id":"IC_kwDOEujx186ZuiNg","author":{"login":"tetrakatech"},"authorAssociation":"NONE","body":"@mjameswh any chance you've had a chance to look at this? (tagging because you're the person who's been looking at other bugs filed around the same time)\r\n\r\nI'm fine to be told that we need to work harder to get a repro case, but if you could validate whether the last event in [the attached JSON history file](https://github.com/user-attachments/files/18001967/dd593d4f-3f24-4177-b376-ba022ebb5d10_events.json) looks valid, that would be super helpful for me to at least know where to focus my efforts.\r\n\r\nIs it correct that throwing a `CancelledFailure` error should lead to an event type of `EVENT_TYPE_WORKFLOW_TASK_FAILED`, rather than a pair of `EVENT_TYPE_WORKFLOW_TASK_COMPLETED` and `EVENT_TYPE_WORKFLOW_EXECUTION_CANCELED` events?","createdAt":"2025-01-09T03:31:01Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-typescript/issues/1580#issuecomment-2579112800","viewerDidAuthor":false},{"id":"IC_kwDOEujx186_JhFI","author":{"login":"bkanzariya-abstract"},"authorAssociation":"NONE","body":"@tetrakatech Did you get any solution/workaround to this? we kind of having bit similar warn logs `Critical attempts processing workflow task`","createdAt":"2025-08-20T15:44:57Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-typescript/issues/1580#issuecomment-3206943048","viewerDidAuthor":false}],"createdAt":"2024-12-04T04:58:26Z","labels":[{"id":"MDU6TGFiZWwyNTQ1Mzg0MDA2","name":"bug","description":"Something isn't working","color":"d73a4a"}],"milestone":null,"number":1580,"reactionGroups":[],"state":"OPEN","title":"[Bug] Workflows getting stuck after cancellation (spurious WorkflowTaskFailed errors)","updatedAt":"2025-08-20T15:44:58Z","url":"https://github.com/temporalio/sdk-typescript/issues/1580"}
