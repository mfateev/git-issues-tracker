{"assignees":[],"author":{"id":"MDQ6VXNlcjkyMDc4NA==","is_bot":false,"login":"leowmjw","name":""},"body":"## Expected Behavior\r\nNo panic should occur when querying a registered QueryHandler; even when the workflow has failed and is retrying\r\n\r\n## Actual Behavior\r\nAfter max attempt for Activity has passed; the whole Workflow has failed and is restarted.\r\n\r\nObserve the panic \"Attempt to generate a command before processing WorkflowTaskStarted event\" due to \"WorkerType WorkflowWorker Error operation GetWorkflowExecution encounter not found\"\r\n\r\n## Logs\r\n$ go run main.go\r\n...\r\n2021/06/21 23:31:45 ERROR Activity error. Namespace default TaskQueue onboarding.queue.name WorkerID 11259@Mojaves-iMac.local@ WorkflowID mleow-1 RunID a7f324da-ccf1-4f1a-97d3-f43df8af9260 ActivityType FailActivities Attempt 2 Error FAIL mleow-1 (type: TEMP, retryable: true)\r\nWID:  mleow-1  CTX_ATTEMPT:  2\r\n\r\n2021/06/21 23:31:45 ERROR Workflow panic Namespace default TaskQueue onboarding.queue.name WorkerID 11259@Mojaves-iMac.local@ WorkflowType RetryWorkflow WorkflowID mleow-1 RunID 07917e48-4dcf-447f-8e0c-9c55674051c5 Attempt 1 Error Attempt to generate a command before processing WorkflowTaskStarted event StackTrace coroutine root [panic]:\r\n\r\ngo.temporal.io/sdk/internal.(*commandsHelper).getNextID(...)\r\n        /Users/leow/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/internal_decision_state_machine.go:824\r\ngo.temporal.io/sdk/internal.(*workflowEnvironmentImpl).GenerateSequence(...)\r\n        /Users/leow/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/internal_event_handlers.go:453\r\ngo.temporal.io/sdk/internal.(*workflowEnvironmentImpl).ExecuteActivity(0xc00018d560, 0x0, 0x0, 0x1a58fa3, 0x15, 0x0, 0x0, 0xbebc200, 0x0, 0x0, ...)\r\n        /Users/leow/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/internal_event_handlers.go:464 +0x696\r\ngo.temporal.io/sdk/internal.(*workflowEnvironmentInterceptor).ExecuteActivity(0xc000180c30, 0x1b5d9f8, 0xc0004b61e0, 0x1ca2c81, 0xe, 0xc000182950, 0x1, 0x1, 0x0, 0x0)\r\n        /Users/leow/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/workflow.go:491 +0x6bd\r\ngo.temporal.io/sdk/internal.ExecuteActivity(0x1b5d9f8, 0xc0004b61e0, 0x1929b60, 0x1a933b0, 0xc000182950, 0x1, 0x1, 0x0, 0x0)\r\n        /Users/leow/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/workflow.go:440 +0x128\r\ngo.temporal.io/sdk/workflow.ExecuteActivity(...)\r\n        /Users/leow/go/pkg/mod/go.temporal.io/sdk@v1.7.0/workflow/workflow.go:113\r\napp/bug-wf-retry.RetryWorkflow(0x1b5d870, 0xc0001b6080, 0xc000410a20, 0x7, 0x0, 0x0)\r\n        /Users/leow/GOMOD/testcase-temporal/bug-wf-retry/workflow.go:42 +0x487\r\nreflect.Value.call(0x19351e0, 0x1a933b8, 0x13, 0x1a4a44d, 0x4, 0xc0004b6150, 0x2, 0x2, 0x2, 0x18, ...)\r\n        /usr/local/Cellar/go/1.16.5/libexec/src/reflect/value.go:476 +0x8e7\r\nreflect.Value.Call(0x19351e0, 0x1a933b8, 0x13, 0xc0004b6150, 0x2, 0x2, 0x1, 0x2, 0xc000180c30)\r\n        /usr/local/Cellar/go/1.16.5/libexec/src/reflect/value.go:337 +0xb9\r\ngo.temporal.io/sdk/internal.(*workflowEnvironmentInterceptor).ExecuteWorkflow(0xc000180c30, 0x1b5d870, 0xc0001b6080, 0xc0000c0c80, 0xd, 0xc0001828f0, 0x1, 0x1, 0x0, 0x0, ...)\r\n        /Users/leow/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/workflow.go:398 +0x2cb\r\ngo.temporal.io/sdk/internal.(*workflowExecutor).Execute(0xc0001b6000, 0x1b5d870, 0xc0001b6080, 0xc000367d10, 0xc0004e5738, 0x1889225, 0x0)\r\n        /Users/leow/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/internal_worker.go:740 +0x35a\r\ngo.temporal.io/sdk/internal.(*syncWorkflowDefinition).Execute.func1(0x1b5d9f8, 0xc0004b6120)\r\n        /Users/leow/go/pkg/mod/go.temporal.io/sdk@v1.7.0/internal/internal_workflow.go:494 +0xf5\r\n\r\n2021/06/21 23:31:45 WARN  Failed to process workflow task. Namespace default TaskQueue onboarding.queue.name WorkerID 11259@Mojaves-iMac.local@ WorkflowType RetryWorkflow WorkflowID mleow-1 RunID 07917e48-4dcf-447f-8e0c-9c55674051c5 Attempt 1 Error Attempt to generate a command before processing WorkflowTaskStarted event\r\n2021/06/21 23:31:45 INFO  Task processing failed with error Namespace default TaskQueue onboarding.queue.name WorkerID 11259@Mojaves-iMac.local@ WorkerType WorkflowWorker Error operation GetWorkflowExecution encounter not found\r\n\r\n==================\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n  1. Workflow that runs activity with MaxAttempts = 2\r\n  2. Once reach above max, activity fails and cause the workflow itself to fail and restart\r\n  3. When above is happening, query is concurrently running\r\n\r\n## Code Snippet\r\n\r\n### Query\r\n```\r\n// Loop and query continuously ..\r\n\t\t\t\ttime.Sleep(time.Second / 2)\r\n\t\t\t\tres, qerr := c.QueryWorkflow(ctx,\r\n\t\t\t\t\tfmt.Sprintf(\"mleow-%d\", i), \"\",\r\n\t\t\t\t\t\"CORE/current_state\")\r\n\t\t\t\tif qerr != nil {\r\n...\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\terr := res.Get(&status)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\tfmt.Println(\"GET_ERR:\", err.Error())\r\n\t\t\t\t}\r\n\t\t\t\tfmt.Println(\"STATUS: \", status)\r\n```\r\n======\r\n\r\n### Worker\r\n```\r\n...\r\n\t\t\t// Start workflow\r\n\t\t\twid := fmt.Sprintf(\"mleow-%d\", id)\r\n\t\t\twfo := client.StartWorkflowOptions{\r\n\t\t\t\tID:                       wid,\r\n\t\t\t\tTaskQueue:                onboarding_patient.WorkflowQueue,\r\n\t\t\t\tWorkflowExecutionTimeout: 2 * time.Minute,\r\n\t\t\t\tWorkflowRunTimeout:       time.Minute,\r\n\t\t\t\tWorkflowTaskTimeout:      time.Second,\r\n\t\t\t\tRetryPolicy: &temporal.RetryPolicy{\r\n\t\t\t\t\tInitialInterval: time.Second,\r\n\t\t\t\t\tMaximumAttempts: 2,\r\n\t\t\t\t},\r\n\t\t\t}\r\n\t\t\twfr, err := c.ExecuteWorkflow(context.Background(), wfo,\r\n\t\t\t\tbug_wf_retry.RetryWorkflow,\r\n\t\t\t\twid,\r\n\t\t\t)\r\n\t\t\tif err != nil {\r\n\t\t\t\tpanic(err)\r\n\t\t\t}\r\n\t\t\tfmt.Println(\"WFIF: \", wfr.GetID(), \"RID: \", wfr.GetRunID())\r\n...\r\n```\r\n=======\r\n\r\n### Workflow + Activity\r\n```\r\nfunc RetryWorkflow(ctx workflow.Context, wid string) error {\r\n\tfmt.Println(\"WID: \", wid, \" CTX_ATTEMPT: \", workflow.GetInfo(ctx).Attempt)\r\n\t// Status to keep track; we return as we go along\r\n\tstatus := \"Starting RetryWorkflow ...\"\r\n\therr := workflow.SetQueryHandler(ctx, \"CORE/current_state\", func() (string, error) {\r\n\t\t// Once completed/failed goes into replay state\r\n\t\tif workflow.IsReplaying(ctx) {\r\n\t\t\tfmt.Println(\"REPLAYING ...\")\r\n\t\t}\r\n\t\treturn status, nil\r\n\t})\r\n\tif herr != nil {\r\n\t\tstatus = \"Failed to set QueryHandler!\"\r\n\t\treturn herr\r\n\t}\r\n\t// Activities\r\n\tao := workflow.ActivityOptions{\r\n\t\tTaskQueue:           onboarding_patient.WorkflowQueue,\r\n\t\tStartToCloseTimeout: time.Millisecond * 200,\r\n\t\tRetryPolicy: &temporal.RetryPolicy{\r\n\t\t\tInitialInterval: time.Second,\r\n\t\t\tMaximumAttempts: 2,\r\n\t\t},\r\n\t}\r\n\tctx = workflow.WithActivityOptions(ctx, ao)\r\n\tf := workflow.ExecuteActivity(ctx, FailActivities, wid)\r\n\tf.Get(ctx, nil)\r\n\tstatus = \"After failed activity ..\"\r\n\r\n\treturn fmt.Errorf(\"UNKNOWN!\")\r\n}\r\n\r\nfunc FailActivities(wid string) error {\r\n\tfmt.Println(\"WID: \" + wid + \" FAIL :( :(\")\r\n\treturn temporal.NewApplicationError(\r\n\t\tfmt.Sprintf(\"FAIL %s\", wid), \"TEMP\",\r\n\t)\r\n}\r\n```\r\n==============\r\n\r\n## Specifications\r\n\r\n  - Version: SDK v.1.7.0\r\n  - Platform: Linux, docker-compose\r\n","closedAt":null,"comments":[{"id":"MDEyOklzc3VlQ29tbWVudDg2NzE4NTI4NQ==","author":{"login":"vitarb"},"authorAssociation":"CONTRIBUTOR","body":"Are you seeing this error persistently (e.g. workflow is getting blocked) or is it cleared after the retry?","createdAt":"2021-06-23T21:51:52Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-go/issues/475#issuecomment-867185285","viewerDidAuthor":false},{"id":"MDEyOklzc3VlQ29tbWVudDg2NzUyNzU4MA==","author":{"login":"leowmjw"},"authorAssociation":"NONE","body":"@vitarb The workflow is not blocked and retry happens as expected; but this is just a toy case to reproduce. \r\n\r\nI submitted as per requested  by @samarabbas just to make sure this is not an edge case that might affect production/more complex scenarios.","createdAt":"2021-06-24T10:32:00Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-go/issues/475#issuecomment-867527580","viewerDidAuthor":false},{"id":"IC_kwDODN1w6847OQJU","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"I have replicated this. In cases where a query is sent to a not-yet-started workflow (can happen during restart, cron, etc), the task polled has only a history with `WorkflowExecutionStarted` with the query and the SDK expects queries to only be handled after `WorkflowTaskStarted`. I am checking with @mfateev, @yiminc, @wxing1292, and others to determine the possible route to take here.","createdAt":"2021-12-14T14:23:05Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-go/issues/475#issuecomment-993591892","viewerDidAuthor":false},{"id":"IC_kwDODN1w6847Vp2M","author":{"login":"tminusplus"},"authorAssociation":"NONE","body":"Have also experienced a similar panic. It was for a cron workflow which spawns parallel activity executions and then waits for all of them to finish. Looks like the workflow is able to retry successfully later. Here is a sample, I removed two service specific frames in this backtrace for privacy:\r\n```\r\nexternal/io_temporal_go_sdk/internal/internal_workflow.go:499 +0xd2\r\ngo.temporal.io/sdk/internal.(*syncWorkflowDefinition).Execute.func1({0x2745f38, 0xc000439bc0})\r\n\texternal/io_temporal_go_sdk/internal/internal_worker.go:741 +0x292\r\ngo.temporal.io/sdk/internal.(*workflowExecutor).Execute(0xc0034d6580, {0x2745de8, 0xc0034d6600}, 0x25)\r\n\texternal/io_temporal_go_sdk/internal/workflow.go:415 +0x166\r\ngo.temporal.io/sdk/internal.(*workflowEnvironmentInterceptor).ExecuteWorkflow(0xc00b7ab9f0, {0x2745de8, 0xc0034d6600}, 0xc0045cb7e8)\r\n\texternal/io_temporal_go_sdk/internal/internal_worker.go:1512 +0x136\r\ngo.temporal.io/sdk/internal.executeFunction({0x11802e0, 0xc0003f1ee0}, {0xc001471e08, 0x1, 0xc001420000})\r\n\tGOROOT/src/reflect/value.go:339 +0xc5\r\nreflect.Value.Call({0x11802e0, 0xc0003f1ee0, 0x403c6c}, {0xc0045cb800, 0x1, 0x1})\r\n\tGOROOT/src/reflect/value.go:543 +0x814\r\n... service backtrace for workflow ...\r\ngo.temporal.io/sdk/workflow.ExecuteActivity(...)\r\n\texternal/io_temporal_go_sdk/internal/workflow.go:455 +0x185\r\ngo.temporal.io/sdk/internal.ExecuteActivity({0x2745f38, 0xc000439d70}, {0x11cb080, 0x254c158}, {0xc000474fc0, 0x1, 0x1})\r\n\texternal/io_temporal_go_sdk/internal/workflow.go:511 +0x782\r\ngo.temporal.io/sdk/internal.(*workflowEnvironmentInterceptor).ExecuteActivity(0xc00b7ab9f0, {0x2745f38, 0xc000439dd0}, {0x29d3360, 0x6}, {0xc000474fc0, 0x1, 0x1})\r\n\texternal/io_temporal_go_sdk/internal/internal_event_handlers.go:463 +0x545\r\ngo.temporal.io/sdk/internal.(*workflowEnvironmentImpl).ExecuteActivity(0xc000100e00, {{{0x0, 0x0}, {0xc00004bb00, 0xf}, 0x0, 0x0, 0x1a3185c5000, 0x0, 0x0, ...}, ...}, ...)\r\n\texternal/io_temporal_go_sdk/internal/internal_event_handlers.go:452\r\ngo.temporal.io/sdk/internal.(*workflowEnvironmentImpl).GenerateSequence(...)\r\n\texternal/io_temporal_go_sdk/internal/internal_decision_state_machine.go:833\r\ngo.temporal.io/sdk/internal.(*commandsHelper).getNextID(...)\r\n```","createdAt":"2021-12-16T08:04:52Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-go/issues/475#issuecomment-995532172","viewerDidAuthor":false},{"id":"IC_kwDODN1w6847W8oL","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"Are you regularly querying the workflow and getting \"Attempt to generate a command before processing WorkflowTaskStarted event\"?\r\n\r\nIt is currently a bug in the SDK where querying a workflow while it is waiting to start (via retry, cron, etc) is causing a panic. Once https://github.com/temporalio/temporal/issues/2300 is solved, the query should fail from the server side and not reach the worker to cause this panic.","createdAt":"2021-12-16T14:31:31Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-go/issues/475#issuecomment-995871243","viewerDidAuthor":false},{"id":"IC_kwDODN1w6847YG7J","author":{"login":"tminusplus"},"authorAssociation":"NONE","body":"We are not querying the workflow at all. Essentially the workflow is doing the same as https://github.com/temporalio/samples-go/blob/main/branch/workflow.go#L34 where it spawns multiple concurrent activities.\r\n\r\nOne potential issue that might be causing it, is that we for-loop through the futures twice. This could be improved, but the intention was that we'd potentially spawn hundreds of activities from this workflow, so we wanted to batch them and ensure we only spawn a limited number of activities per a batch. I reworked the example to replicate our workflow:\r\n```\r\nfunc SampleBranchWorkflow(ctx workflow.Context, totalBranches int) (result []string, err error) {\r\n\tao := workflow.ActivityOptions{\r\n\t\tStartToCloseTimeout: 10 * time.Second,\r\n\t}\r\n\tctx = workflow.WithActivityOptions(ctx, ao)\r\n\r\n        var futures []workflow.Future\r\n        for batch := 0; batch < 10; batch++ {\r\n\t    for i := 1; i <= totalBranches; i++ {\r\n\t\tfuture := workflow.ExecuteActivity(ctx, SampleActivity, activityInput)\r\n\t\tfutures = append(futures, future)\r\n\t    }\r\n \r\n            // Block until this batch finishes processing\r\n            for _, future := range futures {\r\n\t\terr = future.Get(ctx, nil)\r\n\t\tif err != nil {\r\n\t\t\treturn\r\n\t\t}\r\n\t    }\r\n        }\r\n\r\n\t// One last sweep to ensure all scheduled activities are complete\r\n\tfor _, future := range futures {\r\n            err = future.Get(ctx, nil)\r\n\t    if err != nil {\r\n\t\treturn\r\n\t    }\r\n\t}\r\n}\r\n```\r\n","createdAt":"2021-12-16T20:36:05Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-go/issues/475#issuecomment-996175561","viewerDidAuthor":false},{"id":"IC_kwDODN1w6847YILB","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"Ah, that may be different (this issue is for queries during retry in particular). Can you open a new issue? Also, you are looping through issues more than twice, e.g. the second batch will loop through the first batch's, etc. Also, based on that code, that last for loop provides no value since you are doing the exact same thing as the last step in the previous loop that is guaranteed to have run 10 times before it got there. Maybe you want to add `futures = futures[:0]` as the first step of your first loop and remove the last loop?","createdAt":"2021-12-16T20:44:37Z","includesCreatedEdit":true,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/temporalio/sdk-go/issues/475#issuecomment-996180673","viewerDidAuthor":false},{"id":"IC_kwDODN1w684_EQYP","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"Update for those reading this issue. The primary issue here (last few comments a different thing) is https://github.com/temporalio/temporal/issues/2300.","createdAt":"2022-03-03T14:11:42Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-go/issues/475#issuecomment-1058080271","viewerDidAuthor":false},{"id":"IC_kwDODN1w685EKkJ_","author":{"login":"cretz"},"authorAssociation":"MEMBER","body":"This may be solved with https://github.com/temporalio/temporal/pull/2826. Once released, we will write a test confirming that queries can be issued during workflow retry or cron or pending continue as new.","createdAt":"2022-06-01T13:34:13Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/temporalio/sdk-go/issues/475#issuecomment-1143620223","viewerDidAuthor":false}],"createdAt":"2021-06-21T16:47:49Z","labels":[{"id":"LA_kwDODN1w687UGgkJ","name":"external dependency","description":"Waiting on something externally","color":"c5def5"}],"milestone":null,"number":475,"reactionGroups":[],"state":"OPEN","title":"Panic when querying during Workflow failure + retry ","updatedAt":"2022-06-01T13:34:14Z","url":"https://github.com/temporalio/sdk-go/issues/475"}
